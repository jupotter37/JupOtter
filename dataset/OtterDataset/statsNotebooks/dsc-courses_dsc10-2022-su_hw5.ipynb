{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Simulation, Sampling, and Hypothesis Testing\n",
    "\n",
    "## Due Saturday, Aug 13th at 11:59PM PST\n",
    "\n",
    "Welcome to Homework 5! This homework will cover:\n",
    "- Simulations (see [Note 18](https://notes.dsc10.com/04-probability_and_simulation/probability_and_simulation.html))\n",
    "- Populations and samples (see [Note 19](https://notes.dsc10.com/04-probability_and_simulation/1_populations_and_samples.html))\n",
    "- Parameters and statistics (see [Note 20](https://notes.dsc10.com/04-probability_and_simulation/2_parameters_and_statistics.html))\n",
    "- Models and Hypothesis Testing (see [Note 21](https://notes.dsc10.com/05-hypothesis_testing/1_hypothesis_tests.html) and [CIT 11.2](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This assignment is due on Saturday, Aug 13th at 11:59pm. You are given six slip days throughout the quarter to extend deadlines. See the syllabus for more details. With the exception of using slip days, late work will not be accepted unless you have made special arrangements with your instructor.\n",
    "\n",
    "**Important**: For homeworks, the `otter` tests don't usually tell you that your answer is correct. More often, they help catch careless mistakes. It's up to you to ensure that your answer is correct. If you're not sure, ask someone (not for the answer, but for some guidance about your approach). These are great questions for office hours (see the schedule on the [Calendar](https://dsc10.com/calendar)) or Campuswire. Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please don't change this cell, but do make sure to run it.\n",
    "import babypandas as bpd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import otter\n",
    "grader = otter.Notebook()\n",
    "\n",
    "%reload_ext pandas_tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lucky Triton Lotto, Continued  🔱 🎱 🧜\n",
    "\n",
    "In the last homework, we calculated the probability of winning the grand prize (free housing) on a Lucky Triton Lotto lottery ticket, and found that it was quite low 😭."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_housing_chance = (1 / 62) * (1 / 61) * (1 / 60) * (1 / 59) * (1 / 58) * (1 / 16)\n",
    "free_housing_chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we'll approach the same question not using math, but using simulation. \n",
    "\n",
    "It's important to remember how this lottery game works:\n",
    "- When you buy a Lucky Triton Lotto ticket, you first pick five different numbers, one at a time, from 1 to 62. Then you separately pick a number from 1 to 16, which may or may not be the same as one of the first five. These are **your numbers**. For example, you may select (15, 1, 13, 3, 61, 8). This is a sequence of six numbers - **order matters**!.\n",
    "- The **winning numbers** are chosen by King Triton drawing five balls, one at a time, **without replacement**, from a pot of white balls numbered 1 to 62. Then, he draws a gold ball, the Tritonball, from a pot of gold balls numbered 1 to 16. Both pots are completely separate, hence the different ball colors. For example, maybe the winning numbers are (13, 15, 62, 3, 5, 8).\n",
    "\n",
    "We’ll assume for this problem that in order to win the grand prize (free housing), all six of your numbers need to match the winning numbers and be in the **exact same positions**. In other words, your entire sequence of numbers must be exactly the same. However, if some numbers in your sequence match up with the corresponding number in the winning sequence, you will still win some Triton Cash. \n",
    "\n",
    "Suppose again that your numbers are (15, 1, 13, 3, 61, 8) and the winning numbers are (13, 15, 62, 3, 5, 8). In this case, two of your numbers are considered to match two of the winning numbers. Notice that although both sequences include the number 15 within the first five numbers (representing a white ball), since they are in different positions, that's not considered a match.\n",
    "\n",
    "- Your numbers: (15, 1, 13, **3**, 61, **8**)\n",
    "- Winning numbers: (13, 15, 62, **3**, 5, **8**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1.** Write a function called `simulate_one_ticket`. It should take no arguments, and it should return an array with 6 random numbers, simulating how the numbers are selected for a single Lucky Triton Lotto ticket. The first five numbers should all be randomly chosen without replacement, from 1 to 62. The last number should be between 1 and 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_ticket():\n",
    "    \"\"\"Simulate one Lucky Triton Lotto ticket.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2.** It's draw day. You checked the winning numbers King Triton drew, which happened to be **(55, 12, 3, 51, 23, 5)**. You didn't win free housing, and you are quite sad.\n",
    "\n",
    "Suppose you want to remind yourself how unlikely it is to win the grand prize. Call the function `simulate_one_ticket` 100,000 times. (It would cost a fortune if you were to buy that many! It's pretty nice to be able to simulate this experiment instead of doing it in real life). In your 100,000 tickets, **how many times did you win the grand prize (free housing)?** Assign your answer to `count_free_housing`.\n",
    "\n",
    "**_Hint 1:_** Try it first with only buying 10 tickets. Once you are sure you have that figured out, change it to 100,000 tickets. It may take a little while (up to a minute) for Python to perform the calculations when you are buying 100,000 tickets.\n",
    "\n",
    "**_Hint 2:_** You'll have to count how many of the numbers you chose match the numbers that were drawn. One way to do this involves [`np.count_nonzero`](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html). Remember you need **all** the numbers to match to win the grand prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_free_housing = ...\n",
    "...\n",
    "count_free_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the mathematical probability of winning free housing is quite low, on the order of $10^{-11}$. That's a lot lower than than 1 in 100,000, which is $10^{-5}$.\n",
    "\n",
    "**Question 1.3.** As we've seen, you would need to be extremely lucky to win the grand prize. To encourage more students to buy Lucky Triton Lotto tickets, students can win Triton Cash if some of their numbers match the corresponding winning numbers, as described above. Again simulate buying 100,000 tickets, but this time find **the greatest number of matches achieved by any of your tickets**, and assign this to `most_matches`. \n",
    "\n",
    "The winning numbers are the same from the previous part: **(55, 12, 3, 51, 23, 5)**\n",
    "\n",
    "For example, if 90,000 of your tickets matched 1 winning number and 10,000 of your tickets matched 2 winning numbers, then you would set `most_matches` to 2. If 99,999 of your tickets matched 1 winning number and one of your tickets matched 4 winning numbers, you would set `most_matches` to 4. If you happened to win the grand prize on one of your tickets, you would set `most_matches` to 6. Remember, order matters.\n",
    "\n",
    "**_Hint:_** There are several ways to approach this; one way involves storing the number of matches per ticket in an array and finding the largest number in that array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_matches = ...\n",
    "...\n",
    "most_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4.** Suppose one Lucky Triton Lotto ticket costs $2.\n",
    "\n",
    "The Lucky Triton Lotto advertisement on Instagram promises you will never lose money because of the following generous prizes:\n",
    "\n",
    "- Win $10 with a 1-number match\n",
    "\n",
    "- Win $25 with a 2-number match\n",
    "\n",
    "- Win $100 with a 3-number match\n",
    "\n",
    "- Win $1,000 with a 4-number match\n",
    "\n",
    "- Win $5,000 with a 5-number match\n",
    "\n",
    "- Win $20,000 with a 6-number match (Free Housing!)\n",
    "\n",
    "If you had the money to buy 100,000 tickets, what would be your net winnings from buying these tickets? Since this is net winnings, this should account for the prizes you win and the cost of buying the tickets. Assign the amount to `net_winnings`. Note that a positive value means you won money overall, and a negative value means you lost money overall. Do you believe the advertisement's claims?\n",
    "\n",
    "The winning numbers are the same from the previous part: **(55, 12, 3, 51, 23, 5)**\n",
    "\n",
    "**_Hint:_** Again, there are a few ways you could approach this problem. One way involves generating another 100,000 random tickets and counting the amount earned per ticket, adding to a running total. Alternatively, if you created an array of the number of matches per ticket in Question 1.3, you could loop through that array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_winnings = ...\n",
    "...\n",
    "net_winnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling with Netflix 🍿\n",
    "\n",
    "In this question, we will use a dataset consisting of information about all Netflix Original movies to get some practice with sampling. Run the cell below to load the data into a DataFrame, indexed by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "movie_data = bpd.read_csv('data/netflix_originals.csv').set_index('Title')\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a function called `compute_statistics` that takes as input a DataFrame with two columns, `'Runtime'` and `'IMDB Score'`, and then:\n",
    "- draws a histogram of `'Runtime'`,\n",
    "- draws a histogram of `'IMDB Score'`, and\n",
    "- returns a two-element array containing the mean `'Runtime'` and mean `'IMDB Score'`.\n",
    "\n",
    "Run the cell below to define the `compute_statistics` function, and a helper function called `histograms`. Don't worry about how this code works, and please don't change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, just run it.\n",
    "def histograms(df):\n",
    "    runtimes = df.get('Runtime').values\n",
    "    ratings = df.get('IMDB Score').values\n",
    "    \n",
    "    plt.subplots(1, 2, figsize=(15, 4), dpi=100)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(runtimes, density=True, alpha=0.5, color='blue', ec='w', bins=np.arange(0, 250, 10))\n",
    "    plt.title('Distribution of Runtimes')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(ratings, density=True, alpha=0.5, color='blue', ec='w', bins=np.arange(0, 10, 0.4))\n",
    "    plt.title('Distribution of IMDB Scores')\n",
    "    \n",
    "def compute_statistics(runtimes_and_ratings_data, draw=True):\n",
    "    if draw:\n",
    "        histograms(runtimes_and_ratings_data)\n",
    "    avg_runtime = np.average(runtimes_and_ratings_data.get('Runtime').values)\n",
    "    avg_rating = np.average(runtimes_and_ratings_data.get('IMDB Score').values)\n",
    "    avg_array = np.array([avg_runtime, avg_rating]) \n",
    "    return avg_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this `compute_statistics` function to show the distribution of `'Runtime'` and `'IMDB Score'` and compute their means, for any collection of movies. \n",
    "\n",
    "Run the next cell to show these distributions and compute the means for all Netflix Original movies. Notice that an array containing the mean `'Runtime'` and mean `'IMDB Score'` values is displayed before the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stats = compute_statistics(movie_data)\n",
    "movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that instead of having access to the full *population* of movies, we had only gotten data on a smaller subset of the movies, or a *sample*.  For 584 movies, it's not so unreasonable to expect to see all the data, but usually we aren't so lucky.  Instead, we often make *statistical inferences* about a large underlying population using a smaller sample.\n",
    "\n",
    "A **statistical inference** is a statement about some characteristic of the underlying population, such as \"the average IMDB rating for Netflix Original movies is 6.3\". You may have heard the word \"inference\" used in other contexts.  It's important to keep in mind that statistical inferences can be wrong.\n",
    "\n",
    "A common strategy for inference using samples is to estimate parameters of the population by computing the same statistics on a sample.  This strategy sometimes works well and sometimes doesn't.  The degree to which it gives us useful answers depends on several factors.\n",
    "\n",
    "One very important factor in the utility of samples is how they were gathered. Let's look at some different sampling strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience sampling\n",
    "One sampling methodology, which is **generally a bad idea**, is to choose movies which are somehow convenient to sample.  For example, you might choose movies that you have personally watched, since it's easier to collect information about them.  This is called, somewhat pejoratively, *convenience sampling*.\n",
    "\n",
    "**Question 2.1.**  Suppose you love scary movies 👻 and you decide to manually look up information on all Netflix Original movies in the following genres:\n",
    "- `'Horror'`\n",
    "- `'Thriller'`\n",
    "- `'Horror thriller'`\n",
    "\n",
    "Assign `convenience_sample` to a subset of `movie_data` that contains only the rows for movies that are in one of these genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convenience_sample = ...\n",
    "convenience_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2.** Assign `convenience_stats` to an array of the mean `'Runtime'` and mean `'IMDB Score'` of your convenience sample.  Since they're computed on a sample, these are called *sample means*. \n",
    "\n",
    "**_Hint_**: Use the function `compute_statistics`; it's okay if histograms are displayed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "convenience_stats = ...\n",
    "convenience_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compare the distribution of runtimes in our convenience sample with distribution of runtimes for all the movies in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "def compare_runtimes(first, second, first_title, second_title):\n",
    "    \"\"\"Compare the runtimes in two DataFrames.\"\"\"\n",
    "    bins = np.arange(0, 250, 10)\n",
    "    \n",
    "    plt.subplots(1, 2, figsize=(15, 4), dpi=85)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(first.get('Runtime'), bins=bins, density=True, ec='w', color='blue', alpha=0.5)\n",
    "    plt.title(f'Runtimes ({first_title})')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(second.get('Runtime'), bins=bins, density=True, ec='w', color='blue', alpha=0.5)\n",
    "    plt.title(f'Runtimes ({second_title})')\n",
    "\n",
    "compare_runtimes(movie_data, convenience_sample, 'All Movies', 'Convenience Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3.** \n",
    "\n",
    "From what you see in the histogram above, did the convenience sample give us an accurate picture of the runtimes for the full population of movies?  Why or why not?\n",
    "\n",
    "Assign either 1, 2, 3, or 4 to the variable `sampling_q3` below. \n",
    "1. Yes. The sample is large enough, so it is an accurate representation of the population.\n",
    "2. No. Normally convenience samples give us an accurate representation of the population, but only if the sample size is large enough. Our convenience sample here was too small.\n",
    "3. No. Normally convenience samples give us an accurate representation of the population, but we just got unlucky.\n",
    "4. No. Convenience samples generally don't give us an accurate representation of the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple random sampling\n",
    "A more principled approach is to sample uniformly at random from the movies.  If we ensure that each movie is selected at most once, this is a **random sample without replacement**, sometimes abbreviated to \"**simple random sample**\" or \"**SRS**\".  Imagine writing down each movie's title on a card, putting the cards in a hat, and shuffling the hat.  To sample, pull out cards one by one and set them aside, stopping when the specified *sample size* is reached.\n",
    "\n",
    "We've produced two simple random samples of `ratings_data`: the variable `small_srs_data` contains a SRS of size 70, and the variable `large_srs_data` contains a SRS of size 180."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the same analyses on the small simple random sample, the large simple random sample, and the convenience sample. The subsequent code draws the histograms and computes the means for `'Runtime'` and `'IMDB Score'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell, but do run it.\n",
    "\n",
    "small_srs_data = bpd.read_csv('data/small_srs_rating.csv').set_index('Title')\n",
    "large_srs_data = bpd.read_csv('data/large_srs_rating.csv').set_index('Title')\n",
    "\n",
    "small_stats = compute_statistics(small_srs_data, draw=False);\n",
    "large_stats = compute_statistics(large_srs_data, draw=False);\n",
    "convenience_stats = compute_statistics(convenience_sample, draw=False);\n",
    "\n",
    "print('Full data stats:                 ', movie_stats)\n",
    "print('Small SRS stats:', small_stats)\n",
    "print('Large SRS stats:', large_stats)\n",
    "print('Convenience sample stats:        ', convenience_stats)\n",
    "\n",
    "color_dict = {\n",
    "    'small SRS': 'blue',\n",
    "    'large SRS': 'green',\n",
    "    'convenience sample': 'orange'\n",
    "}\n",
    "\n",
    "plt.subplots(3, 2, figsize=(15, 15), dpi=100)\n",
    "i = 1\n",
    "\n",
    "for df, name in zip([small_srs_data, large_srs_data, convenience_sample], color_dict.keys()):\n",
    "    plt.subplot(3, 2, i)\n",
    "    i += 2\n",
    "    plt.hist(df.get('Runtime'), density=True, alpha=0.5, color=color_dict[name], ec='w', \n",
    "             bins=np.arange(0, 250, 10))\n",
    "    plt.title(f'Runtimes ({name})');\n",
    "\n",
    "i = 2\n",
    "for df, name in zip([small_srs_data, large_srs_data, convenience_sample], color_dict.keys()):\n",
    "    plt.subplot(3, 2, i)\n",
    "    i += 2\n",
    "    plt.hist(df.get('IMDB Score'), density=True, alpha=0.5, color=color_dict[name], ec='w', \n",
    "             bins=np.arange(0, 10, 0.4))\n",
    "    plt.title(f'IMDB Ratings ({name})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing simple random samples\n",
    "Often it's useful to take random samples even when we have a larger dataset available.  One reason is that doing so can help us understand how inaccurate other samples are.\n",
    "\n",
    "DataFrames provide the method `.sample` for producing simple random samples.  Note that its default is to sample **without** replacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4.** Produce a simple random sample *without replacement* of size 70 from `movie_data`. Store an array containing the mean `'Runtime'` and mean `'IMDB Score'` of your SRS in `my_small_stats`. Again, it's fine if histograms are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_small_stats = ...\n",
    "my_small_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell in which `my_small_srs_data` is defined many times, to collect new samples and compute their sample means.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, recall, `small_stats` is an array containing the mean `'Runtime'` and mean `'IMDB Score'` for the one small SRS that we provided you with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Answer the following two-fold question:\n",
    "- Are the values in `my_small_stats` (the mean `'Runtime'` and `'IMDB Score'` for **your** small SRS) similar to the values in `small_stats` (the mean `'Runtime'` and `'IMDB Score'` for the small SRS **we provided you with**)? \n",
    "- Each time you collect a new sample – i.e. each time you re-run the cell where `my_small_stats` is defined – do the values in `my_small_stats` change a lot?\n",
    "\n",
    "Assign either 1, 2, 3, or 4 to the variable `sampling_q4` below.\n",
    "1. The values in `my_small_stats` are very different from the values in `small_stats`, and don't change at all each time a new sample is collected.\n",
    "2. The values in `my_small_stats` are identical to the values in `small_stats`, and change a bit each time a new sample is collected.\n",
    "3. The values in `my_small_stats` are slightly different from the values in `small_stats`, and change a bit each time a new sample is collected.\n",
    "4. The values in `my_small_stats` are identical to the values in `small_stats`, and don't change at all each time a new sample is collected.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5.** Similarly, create a simple random sample of size 180 from `movie_data` and store an array of the sample's mean `'Runtime'` and mean `'IMDB Score'` in `my_large_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_large_stats = ...\n",
    "my_large_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell in which `my_large_stats` is defined many times. Do the histograms and  mean statistics (mean `'Runtime'` and mean `'IMDB Score'`) seem to change more or less across samples of size 180 than across samples of size 70?\n",
    "\n",
    "Assign either 1, 2, or 3 to the variable `sampling_q5` below. \n",
    "\n",
    "1. The statistics change *less* across samples of size 180 than across samples of size 70.\n",
    "2. The statistics change an *equal amount* across samples of size 180 and across samples of size 70.\n",
    "3. The statistics change *more* across samples of size 180 than across samples of size 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_q5 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. COVID Politics 🐘 🐎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 8 of the Midterm Project, we analyzed COVID positivity rates for different states based on the party affiliation of voters in that state, as determined by their votes in the 2020 presidential election. We have the relevant data in the DataFrame`covid_politics` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "covid_politics = bpd.read_csv('data/covid_politics.csv')\n",
    "covid_politics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, each row in the DataFrame represents a state in the United States. The columns are\n",
    "- `state`,\n",
    "- `endPositiveRate` (the total number of positive tests per 100,000 people for that state as of December 31, 2020), and \n",
    "- `popParty` (the popular political party, according to votes in the 2020 election)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we'll think of the dataset of 50 states as a _population_ and see what we can learn (infer) about the population by looking at data in a _sample_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the project you calculated a variable called `difference_by_residents`, defined as the difference in mean COVID positivity rates between `'Republican'` and `'Democratic'` states (in the order `'Republican'` minus `'Democratic'`). We've recalculated it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_residents = covid_politics[covid_politics.get('popParty')=='Republican'] \n",
    "democratic_residents = covid_politics[covid_politics.get('popParty')=='Democratic']\n",
    "difference_by_residents = republican_residents.get('endPositiveRate').mean() - democratic_residents.get('endPositiveRate').mean() \n",
    "difference_by_residents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.** Create a function called `mean_diff` that takes as input a DataFrame of states with columns `'endPositiveRate'` and `'popParty'`, and returns the difference between the mean COVID positivity rate for `'Republican'` states and the mean COVID positivity rate for `'Democratic'` states (again, calculate `'Republican'` minus `'Democratic'`).\n",
    "\n",
    "When called on the input `covid_politics`, the output should be the same as `difference_by_residents`, however, this function should work on *any* DataFrame of states, provided there are at least some `'Republican'` states and some `'Democratic'` states in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_diff(state_df):\n",
    "    ...\n",
    "\n",
    "# This should be the same as difference_by_residents. It's okay if the last few decimal places are off.\n",
    "mean_diff(covid_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2.** The value of `difference_by_residents` uses data from all 50 states in the `covid_politics` dataset. Let's suppose, as is often the case in reality, that you couldn't access information about all of the states in the dataset at once, but instead you could **only look at 15 states at a time**. You want to look at **15 random states, sampled without replacement**, to get a representative sample of the full dataset. Write a function called `pick_15` that simulates this. Specifically, the function should take *no* arguments and should return a DataFrame of 15 randomly selected states from `covid_politics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pick_15():\n",
    "    \"\"\"Randomly select 15 different states from covid_politics.\"\"\"\n",
    "    ...\n",
    "pick_15()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even without access to the full `covid_politics` dataset, you can get an idea of the difference between mean COVID positivity rates of `'Republican'` and `'Democratic'` states, based on the 15 states in a random sample. The `mean_diff` function you wrote should be able to calculate the difference in mean COVID positivity rates for a random sample of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff(pick_15())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you'd picked a different random 15 states for your sample? Surely, you'd get a different answer, but how different? Run the cell above a few times. You should get different results each time. If not, check for a mistake in your `mean_diff` function or your `pick_15` function.\n",
    "\n",
    "To answer this question of how the mean difference changes as our sample changes, let's repeat our experiment.\n",
    "\n",
    "**Question 3.3.** 500 times, randomly select 15 states and calculate the difference of mean COVID positivity rates between `'Republican'` and `'Democratic'` states (do `'Republican'` minus `'Democratic'`). Record the 500 differences of mean COVID positivity rates in an array called `experiment_differences`.\n",
    "\n",
    "**_Hint:_** Feel free to use previously defined functions. First try simulating 10 trials. Once you are sure you have that figured out, change it to 500 trials. It may take about a minute to run with 500 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_differences = ...\n",
    "experiment_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4.** When you ran your experiment 500 times, you got 500 different estimates for the difference of mean COVID positivity rates between `'Republican'` and `'Democratic'` states, and you stored those estimates in `experiment_differences`. These estimates are statistics because they come from samples. Create a density histogram showing the distribution of these statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_4\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your histogram here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "**Question 3.5.** Compute the average value of the 500 statistics in `experiment_differences` and store your average in `approximate_difference`. This average is **also** an estimate of the difference in mean COVID positivity rates for the full data set, which is the population parameter you're trying to estimate here. Further, it's probably a better estimate that any individual statistic, because it comes from an average, which balances out the statistics that are too high with the statistics that are too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_difference = ...\n",
    "approximate_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6.**  Now you have an estimate for the difference in mean COVID positivity rates between `'Republican'` and `'Democratic'` states, but you'd like to know how good of an estimate it is. How far is `approximate_difference`, calculated from your sample statistics, from `difference_by_residents`, the parameter calculated from the full `covid_politics` population? Compute the absolute difference between the two values and store it in the variable `error`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = ...\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to explore this some more, try taking samples of different sizes, and calculating the error in your corresponding estimates. Do estimates derived from bigger samples tend to be more accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis 📰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text analysis, data scientists can identify positive, negative, and neutral expressions in text. This is known as  sentiment analysis.\n",
    "\n",
    "Suppose that you consider UCSD's student-run newspaper, [The Guardian](https://ucsdguardian.org/), to be a fairly positive publication. You estimate that about 10% of sentences are negative, 35% are neutral, and 55% are positive. We'll represent that as an array called `guardian_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_model = np.array([0.1, 0.35, 0.55]) \n",
    "guardian_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a hypothesis test to check if our model is accurate. Suppose you run a sentiment analysis program on the most recent issue of The Guardian, and find that out of 6600 sentences, 3570 are positive. \n",
    "\n",
    "**Question 4.1.** Complete the implementation of the function `one_simulation`, which has no arguments and returns the proportion of sentences with a positive sentiment, out of 6600 sentences whose sentiments are **randomly generated** according to our model.\n",
    "\n",
    "**_Hint:_** Use `np.random.multinomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_simulation():\n",
    "    ...\n",
    "\n",
    "one_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2.** The test statistic for our hypothesis test will be the **absolute difference between the proportion of positive sentences in a given simulation and the expected proportion of positive sentences in our model**, i.e.\n",
    "\n",
    "$$| \\text{proportion of positive sentences in simulated sample} - 0.55 |$$\n",
    "\n",
    "\n",
    "Let's conduct 5000 simulations. Create an array named `proportion_diffs` containing 5000 simulated values of the test statistic described above. Utilize the function created in the previous question to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_diffs = ...\n",
    "\n",
    "# Visualize with a histogram\n",
    "bpd.DataFrame().assign(absolute_differences=proportion_diffs).plot(kind='hist', bins=np.arange(0, 0.03, 0.002), density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=abs(3570 / 6600 - 0.55), color='red', label='observed statistic')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3.** Recall that our null hypothesis is that the proportion of positive sentences is 0.55, and that our sentiment analysis program found 3570 out of 6600 sentences to be positive. Use this information to calculate the p-value for this hypothesis test, which is the **proportion of times in our simulation that we saw a test statistic as or more extreme than our observed test statistic**. Assign the result to `guardian_p`.\n",
    "\n",
    "**_Hint:_** Do large values of our test statistic favor the alternative hypothesis, or do small values of our test statistic favor the alternative hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_p = ...\n",
    "guardian_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4.** Assign the variable `guardian_conclusion` to the best conclusion of this hypothesis test, based on the standard 0.05 significance level.\n",
    "   \n",
    "   1. We should reject the null hypothesis because it is unlikely that we'd see the observed number of positive sentences if our model were correct. \n",
    "    \n",
    "   2. We should accept the null hypothesis because our observed data is consistent with our model.\n",
    "    \n",
    "   3. We should neither reject nor accept the null hypothesis because we haven't seen any evidence that our model is wrong, but we also don't know that it's accurate.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardian_conclusion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Cracking Wordle 🟨 ⬛ 🟨 🟩 ⬛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you're a really competitive [Wordle](https://www.nytimes.com/games/wordle) player and you're looking for some tips to guess the answer word more quickly. Online, you find a _model_ for the proportion of times each letter in the alphabet is the first letter of the answer word in Wordle. (For example, in the words `\"ALOOF\"`, `\"TRACE\"`, and `\"POINT\"`, the letters in the first position of the word are `\"A\"`, `\"T\"`, and `\"P\"`, respectively.)\n",
    "\n",
    "The model you found is:\n",
    "\n",
    "<table>\n",
    "    <tr><th>Letter</th><th>Estimated Chance of Being First Letter</th></tr>\n",
    "    <tr><td>A</td><td>7%</td></tr>\n",
    "        <tr><td>B</td><td>9%</td></tr>\n",
    "        <tr><td>C</td><td>10%</td></tr>\n",
    "        <tr><td>D</td><td>4%</td></tr>\n",
    "       <tr> <td>E</td><td>2%</td></tr>\n",
    "        <tr><td>F</td><td>5%</td></tr>\n",
    "        <tr><td>G</td><td>3%</td></tr>\n",
    "        <tr><td>H</td><td>5%</td></tr>\n",
    "        <tr><td>I</td><td>2%</td></tr>\n",
    "        <tr><td>J</td><td>1%</td></tr>\n",
    "        <tr><td>K</td><td>3%</td></tr>\n",
    "        <tr><td>L</td><td>2%</td></tr>\n",
    "        <tr><td>M</td><td>2%</td></tr>\n",
    "        <tr><td>N</td><td>4%</td></tr>\n",
    "        <tr><td>O</td><td>3%</td></tr>\n",
    "        <tr><td>P</td><td>7%</td></tr>\n",
    "        <tr><td>Q</td><td>0.25%</td></tr>\n",
    "        <tr><td>R</td><td>2%</td></tr>\n",
    "        <tr><td>S</td><td>16%</td></tr>\n",
    "        <tr><td>T</td><td>8%</td></tr>\n",
    "        <tr><td>U</td><td>1%</td></tr>\n",
    "        <tr><td>V</td><td>2%</td></tr>\n",
    "        <tr><td>W</td><td>1%</td></tr>\n",
    "        <tr><td>X</td><td>0.25%</td></tr>\n",
    "        <tr><td>Y</td><td>0.25%</td></tr>\n",
    "        <tr><td>Z</td><td>0.25%</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store these values in an array called `wordle_distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "wordle_distribution = np.array([0.07, 0.09, 0.10, 0.04, 0.02, 0.05, 0.03, 0.05, 0.02, 0.01, 0.03, 0.02, 0.02, 0.04, 0.03, 0.07, 0.0025, 0.02, 0.16, 0.08, 0.01, 0.02, 0.01, 0.0025, 0.0025, 0.0025])\n",
    "wordle_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that you have seen the letter `\"S\"` as the first letter of the Wordle quite often. The model you found estimates that there is a 16% chance of `\"S\"` being the first letter of the answer word in Wordle. You decide to play Wordle for 100 straight days, and `\"S\"` is the first letter of the answer word exactly 9 times. You start to suspect that 16% might be **too high** of an estimate, and that the model is wrong. \n",
    "\n",
    "**Question 5.1.** Using the model in which there is a 16% chance of `\"S\"` being the first letter, write a simulation that runs 100 games and keeps track of the **difference** between: \n",
    "- the number of Wordles in which `\"S\"` is the first letter, and \n",
    "- the number of times you'd expect `\"S\"` to be the first letter in 100 Wordles according to the model.\n",
    "\n",
    "In other words, you will be calculating the observed (empirical) minus expected (theoretical) number of times `\"S\"` is the first letter in 100 Wordles. Note that there are no absolute values involved, unlike in Question 4.\n",
    "\n",
    "Run your simulation 5000 times. Keep track of the differences in an *array* called `wordle_differences`.\n",
    "\n",
    "**_Hint:_** If A is the 1st letter in the alphabet, then S is the 19th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordle_differences = ...\n",
    "\n",
    "# Visualize with a histogram\n",
    "bpd.DataFrame().assign(differences=wordle_differences).plot(kind='hist', density=True, bins=np.arange(-15, 15, 1), ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=-7, color='red', label='observed statistic')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.2.** Recall, your null hypothesis was that there is a 16% chance of `\"S\"` being the first letter of the Wordle, but you observed `\"S\"` being the first letter 9 times out of 100. Compute the p-value for this hypothesis test, and save the result to `wordle_p_value`.\n",
    "\n",
    "**_Hint:_** Remember, the reason you ran a hypothesis test at all was that you thought 16% was too high of an estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordle_p_value = ...\n",
    "wordle_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.3.** Based on the histogram and the p-value, set the variable `wordle_null_hypothesis` below to `True` if you think your model is plausible or `False` if it should be rejected at the standard 0.05 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordle_null_hypothesis = ...\n",
    "wordle_null_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.4.** In this question, we chose as our test statistic the difference (signed, not absolute) between the number of times out of 100 `\"S\"` was the first letter of the Wordle and the number of times out of 100 you would expect this to happen. But this is not the only statistic we could have chosen; there are many that could have worked here. \n",
    "\n",
    "From the options below, choose the test statistic that would **not** have worked for this hypothesis test, and save your choice in the variable `wordle_bad_choice`. \n",
    "\n",
    "1. The number of times out of 100 that `\"S\"` was the first letter of the Wordle.\n",
    "2. The proportion of times that `\"S\"` was the first letter of the Wordle.\n",
    "3. The absolute difference between the number of times out of 100 that `\"S\"` was the first letter of the Wordle and the theoretical number of times out of 100. ($\\text{statistic} = |\\text{empirical} - \\text{theoretical}|$)\n",
    "4. The sum of the number of times out of 100 that `\"S\"` was the first letter of the Wordle and the theoretical number of times out of 100 that `\"S\"` is the first letter. ($\\text{statistic} = \\text{empirical} + \\text{theoretical}$)\n",
    "\n",
    "**_Hint:_** Our goal is to find a test statistic that will help us determine whether the number of times `\"S\"` is the first letter of the Wordle is **less** than the expected number of 16.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordle_bad_choice = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <span style='color:#FF1480'> Surprise Mini Brands!</span>  🍭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you buy a Surprise Mini Brands toy, you open it up to reveal tiny replicas of branded supermarket products. Here are some of the possible items you may see when opening a Surprise Mini Brands toy:\n",
    "<img src='data/minibrand.png' width='650'>\n",
    "\n",
    "No, that is not real pasta sauce!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of replicas in a Surprise Mini Brands toy: `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`. The first three are \"rare\" types, which are made of special materials.\n",
    "\n",
    "Unfortunately, Zuru, the company behind Surprise Mini Brands, doesn't make public the probability of getting any of the four types of replicas. A DSC 10 tutor proposed the following probability distribution:\n",
    "\n",
    "| Type | Estimated Probability of Type |\n",
    "| --- | --- |\n",
    "| Gold | $\\frac{1}{15}$ |\n",
    "| Metallic | $\\frac{1}{15}$ |\n",
    "| Glow in the Dark | $\\frac{1}{30}$ |\n",
    "| Common | $\\frac{5}{6}$ |\n",
    "\n",
    "We'll store this distribution in an array, in the order `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "type_distribution_tutor = np.array([1 / 15, 1 / 15, 1 / 30, 5 / 6])\n",
    "type_distribution_tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the validity of their model, the tutor surveyed many individuals who purchased Surprise Mini Brands toys and asked them for the types of replicas they received. In total, they were given information about 15525 replicas, out of which:\n",
    "- 818 were `'Gold'`,\n",
    "- 976 were `'Metallic'`,\n",
    "- 412 were `'Glow in the Dark'`, and\n",
    "- the rest were `'Common'`.\n",
    "\n",
    "We can calculate the **empirical** type distribution using survey data and store it in an array as well (in the same order as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not change it!\n",
    "empirical_type_distribution = np.array([818, 976, 412, (15525 - 818 - 976 - 412)]) / 15525\n",
    "empirical_type_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.1.** Let's perform a hypothesis test to determine whether the tutor's model is accurate. Note that this hypothesis test is different than the ones performed in Questions 4 and 5, since we aren't just looking at one number or one proportion, but rather four proportions – one for each of `'Gold'`, `'Metallic'`, `'Glow in the Dark'`, and `'Common'`.\n",
    "\n",
    "Which of the following is **not** a reasonable choice of test statistic for this hypothesis test? Save your choice in the variable `unreasonable_test_statistic`. You may only choose one.\n",
    "1. The total variation distance between the proposed distribution (expected proportion of types) and the empirical distribution (actual proportion of types).\n",
    "2. The sum of the absolute difference between the proposed distribution (expected proportion of types) and the empirical distribution (actual proportion of types).\n",
    "3. The absolute difference between the sum of the proposed distribution (expected proportion of types) and the sum of the empirical distribution (actual proportion of types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreasonable_test_statistic = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.2.** We'll use the TVD, i.e. **total variation distance**, as our test statistic. Below, complete the implementation of the function `total_variation_distance`, which takes in two distributions (stored as arrays) as arguments and returns the total variation distance between the two arrays.\n",
    "\n",
    "Then, use the function `total_variation_distance` to determine the TVD between the type distribution proposed by the tutor and the empirical type distribution observed. Assign this TVD to `observed_tvd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_distance(first_distrib, second_distrib):\n",
    "    '''Computes the total variation distance between two distributions.'''\n",
    "    ...\n",
    "\n",
    "observed_tvd = ...\n",
    "observed_tvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.3.** Now, we'll calculate 5000 simulated TVDs to see what a typical TVD between the proposed distribution and an empirical distribution would look like if the tutor's model were accurate. Since our real-life data includes 15525 replicas, in each trial of the simulation, we'll:\n",
    "- draw 15525 replicas at random from the tutor's proposed distribution, then \n",
    "- calculate the TVD between the **type distribution proposed by the tutor** and the **empirical type distribution from the simulated sample**. \n",
    "\n",
    "Store these 5000 simulated TVDs in an array called `simulated_tvds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_tvds = ...\n",
    "\n",
    "# Visualize the distribution of TVDs with a histogram\n",
    "bpd.DataFrame().assign(simulated_tvds=simulated_tvds).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=observed_tvd, color='red', label='observed TVD')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.4.** Now, we check the p-value of our test by computing the proportion of times in our simulation that we saw a TVD greater than or equal to our observed TVD. Assign your result to `type_p_value`.\n",
    "\n",
    "Additionally, conclude whether we should reject the null hypothesis at the standard 0.05 significance level. Set the variable `type_null` below to `True` if you think we should fail to reject the null hypothesis or `False` if you think the null hypothesis should be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_p_value = ...\n",
    "type_null = ...\n",
    "type_p_value, type_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our tutor didn't do such a good job at proposing a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Line 🏁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit your assignment:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using `File -> Download as -> Notebook (.ipynb)`, then upload your notebook to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
