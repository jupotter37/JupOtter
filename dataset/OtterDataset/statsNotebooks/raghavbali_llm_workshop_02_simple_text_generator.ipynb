{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_workshop/blob/main/module_02/solutions/02_simple_text_generator.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Accelerator Device=cpu\n"
     ]
    }
   ],
   "source": [
    "# if you are on apple silicon execute the below command before starting jupyter\n",
    "#export PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "# Some Causal Modeling Ops are not available on MPS yet \n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "#     Tensor = torch.FloatTensor\n",
    "#     LongTensor = torch.LongTensor\n",
    "#     DEVICE_ID = 0\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from numpy import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1177d40f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.42.3\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1177d40f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# colab/gpu systems\n",
    "!nvidia-smi\n",
    "# htop or activity monitor for linux based systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "We will fine-tune a pre-trained model GPT-2 model on our earlier dataset itself. But wait, what do you mean pre-trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-28 00:53:52--  http://www.gutenberg.org/files/1661/1661-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 2610:28:3090:3000:0:bad:cafe:47, 152.19.134.47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/files/1661/1661-0.txt [following]\n",
      "--2024-07-28 00:53:52--  https://www.gutenberg.org/files/1661/1661-0.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|2610:28:3090:3000:0:bad:cafe:47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607504 (593K) [text/plain]\n",
      "Saving to: ‘sherlock_homes.txt’\n",
      "\n",
      "sherlock_homes.txt  100%[===================>] 593.27K  1.21MB/s    in 0.5s    \n",
      "\n",
      "2024-07-28 00:53:53 (1.21 MB/s) - ‘sherlock_homes.txt’ saved [607504/607504]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O sherlock_homes.txt http://www.gutenberg.org/files/1661/1661-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sherlock_homes.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "text = raw_text [1450:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation & Pre-trained Models\n",
    "\n",
    "**Foundation models** are the models that are trained from scratch on a large corpus of data. In the context of NLP, these models are designed to learn the fundamental patterns, structures, and representations of natural language. Foundation models are typically trained using unsupervised learning objectives, such as language modeling or autoencoding, where the model predicts the next word in a sentence or reconstructs the original sentence from a corrupted version/masked version.\n",
    "Models such as GPT, BERT, T5, etc are typical examples of Foundation Models\n",
    "\n",
    "\n",
    "Instances of foundation models that have been trained on specific downstream tasks or datasets are termed as **Pre-Trained Models**. Pretrained models leverage the knowledge learned from foundation models and are fine-tuned on task-specific data to perform well on specific NLP tasks, such as text classification, named entity recognition, machine translation, sentiment analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = '<|sot|>'\n",
    "EOS_TOKEN = '<|eot|>'\n",
    "PAD_TOKEN = '<|pad|>'\n",
    "MODEL_NAME = \"raghavbali/gpt2_ft_sherlock_holmes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let us get the tokenizer object\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME,\n",
    "                                          bos_token=BOS_TOKEN,\n",
    "                                          eos_token=EOS_TOKEN,\n",
    "                                          pad_token=PAD_TOKEN\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "\n",
    "      encodings_dict = tokenizer(\n",
    "          BOS_TOKEN + txt + EOS_TOKEN, #TODO\n",
    "          truncation=True,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\"\n",
    "          )\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)#TODO: return size of input_ids\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size to work it out on colab\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,949 training samples\n",
      "  217 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = GPT2Dataset(text.split('\\n'),\n",
    "                      tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = BATCH_SIZE#TODO: set batch-size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = BATCH_SIZE#TODO: set batch-size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "epochs = 1 #3 seems good if you train from gpt2 checkpoint\n",
    "learning_rate = 5e-4\n",
    "# to speed up learning\n",
    "warmup_steps = 1e2\n",
    "epsilon = 1e-8\n",
    "\n",
    "# generate output after N steps\n",
    "sample_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Config\n",
    "configuration = GPT2Config.from_pretrained(MODEL_NAME,\n",
    "                                           output_hidden_states=False)\n",
    "\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME, config=configuration,)\n",
    "\n",
    "# NOTE: This is important to imply that we have updated BOS, EOS, etc\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      ">> Epoch 1 / 1 \n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # Training\n",
    "    print(\"*\"*25)\n",
    "    print('>> Epoch {:} / {:} '.format(epoch_i + 1, epochs))\n",
    "    print(\"*\"*25)\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    #TODO: call model's training interface\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[0].to(DEVICE)\n",
    "        b_masks = batch[1].to(DEVICE)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(  b_input_ids,\n",
    "                          labels=b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Training Loss: {:>5,}.   Time Taken: {:}.'.format(step,\n",
    "                                                                                     len(train_dataloader),\n",
    "                                                                                     batch_loss,\n",
    "                                                                                     elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_outputs = model.generate(\n",
    "                                    do_sample=True,\n",
    "                                    top_k=50,\n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95,\n",
    "                                    num_return_sequences=1,\n",
    "                                    pad_token_id=tokenizer.eos_token_id\n",
    "                                )\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Average Loss\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # training time\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch time: {:}\".format(training_time))\n",
    "\n",
    "    # Validation\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[0].to(DEVICE)\n",
    "        b_masks = batch[1].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(b_input_ids,#TODO: pass batch's ids,\n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation time: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_oss': avg_val_loss,\n",
    "            'train_ime': training_time,\n",
    "            'val_ime': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Training Completed\")\n",
    "print(\"Total training time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './model_save/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: the King of England, and he was as good a queen, as if she had\n",
      "\n",
      "\n",
      "1: the King of England, with a face of a royal-fancy one, and a thick,\n",
      "\n",
      "\n",
      "2: the King of England.”\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"the King of England\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(BOS_TOKEN+prompt)).unsqueeze(0)\n",
    "generated = generated.to(DEVICE)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated,\n",
    "                                do_sample=True,\n",
    "                                top_k=50,\n",
    "                                max_length = len(generated) + 50,\n",
    "                                top_p=0.92,\n",
    "                                num_return_sequences=3,\n",
    "                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                temperature=0.8,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare output to foundation model\n",
    "pre_trainedtokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "pre_trainedmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pre_trainedtokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = pre_trainedmodel.generate(\n",
    "    input_ids,\n",
    "    bos_token_id=random.randint(1,30000),\n",
    "    max_length=len(input_ids[0]) + 50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,  # Adjust the sampling parameters as needed\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the King of England was a good fellow, for I was the better'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trainedtokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "\n",
    "The ``generate()`` utility we used above used every output prediction as input for the next time step. This method of using the highest probability prediction as output is called __Greedy Decoding__. Greeding decoding is fast and simple but is marred with issues we saw in samples we just generated.\n",
    "\n",
    "Focusing on only highest probability output narrows our model's focus to just the next step which inturn may result in inconsistent or non-dictionary terms/words.\n",
    "\n",
    "### Beam Search\n",
    "Beam search is the obvious next step to improve the output predictions from the model. Instead of being greedy, beam search keeps track of n paths at any given time and selects the path with overall higher probability.\n",
    "\n",
    "<img src=\"../assets/beamsearch_nb_2.png\">\n",
    "\n",
    "### Other Key Decoding Strategies:\n",
    "- Sampling\n",
    "- Top-k Sampling\n",
    "- Nucleus Sampling\n",
    "\n",
    "### Temperature\n",
    "Though sampling helps bring in required amount of randomness, it is not free from issues. Random sampling leads to gibberish and incoherence at times. To control the amount of randomness, we introduce __temperature__. This parameter helps increase the likelihood of high probability terms reduce the likelihood of low probability ones. This leads to sharper distributions. \n",
    "\n",
    "> High temperature leads to more randomness while lower temperature brings in predictability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"the King of England\"\n",
    "\n",
    "generated = tokenizer.encode(BOS_TOKEN+prompt,return_tensors='pt')\n",
    "generated = generated.to(DEVICE)\n",
    "\n",
    "beam_output = model.generate(\n",
    "    **generated,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and What Next?\n",
    "- Long Range Context\n",
    "- Scalability\n",
    "- Instruction led generation\n",
    "- Benchmarking\n",
    "- Halucination / Dreaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
