{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 4: controlling for observable factors and causal trees\n",
    "\n",
    "In this Exercise Set 4 we will try out different techniques for using matching and try an implementation of causal trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4.1 Survival and passenger class\n",
    "\n",
    "We revisit a classic dataset: Titanic. We are interested in analyzing whether the passengers on First class had a higher survival probability. \n",
    "\n",
    "The code below loads the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic').dropna(subset=['age'])\n",
    "\n",
    "\n",
    "X = pd.get_dummies(df.drop(['pclass','class', 'alive'],axis=1), drop_first=True).astype('float')\n",
    "D = (df['pclass'] < 3).rename('high_class')\n",
    "y = (df['alive']=='yes').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.1:** Compute the ATE of not travelling on a 3rd class ticket, assuming the CIA holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE assuming randomization of D: 0.33159402095021384\n"
     ]
    }
   ],
   "source": [
    "avg_treated = y[D].mean()\n",
    "avg_untreated = y[~D].mean()\n",
    "\n",
    "print(f'ATE assuming randomization of D: {avg_treated - avg_untreated}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2:** Compute the share of males, the proportion travelling alone, and the mean age, by treatment status. Then modify the code below to try out coarsened exact matching on `exact_cols = ['age_group', 'alone','sex_male']` with age bins of size 2, 5, 10 and 15 years. \n",
    ">\n",
    "> Comment on the result. Does coarse matching seem like a feasible approach in this \n",
    "```python\n",
    "age_diff =  2\n",
    "X['age_group'] =  (X.age // age_diff)\n",
    "match_count = \\\n",
    "    pd.DataFrame({'treat':X[D].groupby(exact_cols).size(), \n",
    "                  'control':X[~D].groupby(exact_cols).size()})\\            \n",
    "n_obs_matched = int(match_count.dropna().sum().sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 611 with a maximum age difference of 2\n",
      "Matched 662 with a maximum age difference of 5\n",
      "Matched 679 with a maximum age difference of 10\n",
      "Matched 687 with a maximum age difference of 15\n"
     ]
    }
   ],
   "source": [
    "#age_diff =  2\n",
    "exact_cols = ['age_group', 'alone', 'sex_male']\n",
    "\n",
    "for age_diff in (2,5,10,15):\n",
    "    X['age_group'] =  (X.age // age_diff)\n",
    "    match_count = \\\n",
    "        pd.DataFrame({'treat':X[D].groupby(exact_cols).size(), \n",
    "                      'control':X[~D].groupby(exact_cols).size()})           \n",
    "    n_obs_matched = int(match_count.dropna().sum().sum())\n",
    "    print(f'Matched {n_obs_matched} with a maximum age difference of {age_diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with age difference = 5.\n",
    "\n",
    "> **Ex. 4.1.3:** Compute the average treatment effect by using (coarsened) exact matching on `age` (i.e. on `age_group`). \n",
    ">\n",
    ">Comment on the result. How does the group treatment effects compare to the ATE you found in 4.1.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qsd161\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:362: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n",
      "C:\\Users\\qsd161\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:362: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n",
      "  warnings.warn(empty_warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2959026678780339"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "\n",
    "# 1. fit RadiusNeighborRegressor(radius = 0) to the treated \n",
    "# individuals. This model will predict the average Y(1) of \n",
    "# people with an exact match on covariates.\n",
    "# Use this model to predict counterfactual Y(1) on the control group.\n",
    "y_treated, y_control = y[D], y[~D]\n",
    "X_treated, X_control = X[D], X[~D]\n",
    "\n",
    "model_tgroup = RadiusNeighborsRegressor(radius = 0)\n",
    "model_tgroup.fit(X_treated[exact_cols], y_treated)\n",
    "imputed_Y1 = model_tgroup.predict(X_control[exact_cols])\n",
    "\n",
    "# Reverse and repeat for the control group\n",
    "model_cgroup = RadiusNeighborsRegressor(radius = 0)\n",
    "model_cgroup.fit(X_control[exact_cols], y_control)\n",
    "imputed_Y0 = model_cgroup.predict(X_treated[exact_cols])\n",
    "\n",
    "\n",
    "ITE_treated = y_treated - imputed_Y0\n",
    "ITE_control = imputed_Y1 - y_control\n",
    "\n",
    "ITE = np.r_[ITE_treated, ITE_control]\n",
    "ITE[~np.isnan(ITE)].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.4:** Estimate a logistic regression model for predicting the passenger class variable (i.e. `D`, the treatment indicator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5:** What other models might we have chosen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.6:** What is the overlap of predicted probabilities? What happens if you estimate the model without `fare` and `deck`? Comment\n",
    ">\n",
    "> Why do `fare` and `deck` matter a lot in this setting, try to draw a causal diagram that might illuminate your discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.7:** Use a 5-nearest-neighbors matching in propensity space to compute the average treatment effect. Bootstrap the 95 pct. confidence interval of the ATE. What happens if you select only propensity score values with high common support, i.e. between 0.2 and 0.8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.7:** (BONUS) How might we improve on the approach above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Honest trees\n",
    "\n",
    "In this problem we will try to implement and understand some of the ideas used in [Athey, Imbens (2015)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf) to develop _Honest Inference_ in desicion tree models. The paper begins by covering honesty in a setting of population averages, and for estimating conditional means; so you will need to look towards the second half of the paper to get an impression of it's use for treatment-effect estimation.\n",
    "\n",
    "> **Ex. 4.2.1:** What does it mean that a tree is _honest?_ In particular what are the implications in terms of \n",
    "> * The intuition for why honesty is required in order to get good local treatment effect estimates?\n",
    "> * The practical implementation of the DT algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.4.2.2:** Use the `load_42_data` function to load the boston house-price dataset. Split your dataset in two. A 50% test set and a 50% train set using `sklearn.model_selection.train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_42_data():\n",
    "    from sklearn.datasets import load_boston\n",
    "    df = load_boston()\n",
    "    df = pd.DataFrame(np.c_[df['data'], df['target']], columns = list(df['feature_names']) + ['y'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 4.2.3:** Identify the column and value in `X_train` that minimizes the (cross split weighted) sum of squared errors in the training data. Split the test data according to this value and report the mean and standard deviation of `y` in both subsamples for both the train and test data.\n",
    ">\n",
    "> Comment on your results. How different are the two subsamples from the overall mean and standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 4.2.4:** Redo your analysis from 4.2.3, but this time split in a 66% train dataset and a 33% test dataset. Split the train data once more 50/50 to get a train and an estimation dataset. \n",
    ">\n",
    "> Focus only on one of the subsets (i.e. either the left or right leaf). \n",
    ">\n",
    "> Report the same statistics as before, but for train, estimation and test samples. This time, show your results as density plots graphing 5.000 bootstrap replications of the whole procedure. If your pc is slow, you might need to reduce the number of replications to 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
