{
 "metadata": {
  "name": "",
  "signature": "sha256:6794dea5166465a3089915b602fe3fe7d7c5b06b649c968cdaacb4c1db5aaa44"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The General idea of posterior inference#\n",
      "\n",
      "If the posterior distribution is tractable, then we can use exact inference methods. If instead the posterior distribution is intractable, there are two general approaches we can take to resolve this:\n",
      "\n",
      "a) Deterministic approximate inference (classical variational inference)\n",
      "\n",
      "b) Stochastic approximate inference (MCMC, stochastic variational inference).\n",
      "\n",
      "##Classical (mean field) variational inference###\n",
      "\n",
      "While there are other algorithms available, the most popular method for variational inference uses the mean-field approximation. The crux of the approximation is this: we assume the hidden variables are independent. Of course, this does not work for many models. However, it provides a quick and easy approximation. For this reason, many researchers have cited variational methods as being faster than MCMC (but this is a controversial statement). We construct a function that is close to the true posterior in terms of KL divergence, then perform inference on this nice, known function. Results are approximate.\n",
      "\n",
      "##Markov chain Monte Carlo##\n",
      "\n",
      "Posterior inference is carried out using samples from the posterior. We construct a Markov chain that has a stationary distribution equal to the posterior distribution, and run the chain to \"convergence\" (which is a blurry term - convergence diagnostics are still under development). Then we sample from the stationary distribution of the Markov chain, which gives us posterior samples. MCMC will yield exact results if the process is allowed to run forever. These chains, though, often exhibit slow mixing, which means that MCMC can take a long time to yield good results.\n",
      "\n",
      "##Stochastic variational inference##\n",
      "\n",
      "Classical variational inference uses the entire dataset in each iteration of the algorithm. Thus, classical variational methods do not scale to large datasets. Stochastic variational inference solves this problem, but adds others. Instead of looking at the entire dataset to perform a parameter update, we sample one datapoint uniformly from the set of observations and use just that observation to inform parameter changes. The stochasticity here comes entirely from sampling noise. While the algorithm as described is entirely online, extensions to minibatches have been implemented. Full batch stochastic variational inference is equivalent to classical variational inference. One new problem introduced by this method is the requirement of a step size schedule, to be described later (one solution has been provided by Ranganath et al, ICML 2013). Future work includes extending the methodology to time series models (started by Johnson and Willsky, ICML 2014), and to non-exponential family models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mean field variational inference ##\n",
      "\n",
      "### The setup ###\n",
      "\n",
      "Suppose we have data $X = \\left\\{x_1, \\ldots, x_n \\right\\}$, global hidden variables $\\beta$ and local hidden variables $Z = \\left\\{z_1, \\ldots, z_n\\right\\}$. Notice we have one local hidden variable for each observation $x_i$ and the global hidden variables do not necessarily have dimension $n$. This setup can be shown using a plate diagram (below)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/PlateModel.png\" alt=\"A plate model with N observations\" style=\"width:250px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plate model (above) is a schematic for a model distribution, showing the conditional dependencies. In a plate model, a rectangle with a total (n) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to n. So we have a pictoral representation of $z_i : i = 1, \\ldots, n$ and $x_i : i = 1, \\ldots, n$. The notation for unobserved variables (hidden units, in this case $z_i$ and $\\beta$) in a plate diagram is an unfilled circle. Observed variables (visible units, in this case $x_i$) are filled circles. Outside the plate are variables or parameters that are not repeated n times. That is, $\\beta$ is a vector of global hidden variables, which means the length of the vector is not necessarily $n$. The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $\\beta$ to $z_i$ indicates that the value of $z_i$ dependes on the value of $\\beta$. Taken as a whole, the plate model shows that the joint (model) distribution $P(x, z, \\beta)$, and illustrates the dependencies in the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, in a Gaussian mixture model, the global parameters $\\beta$ would be the means and precision matrices of the components, and the local parameters $z$ would be the identity of the component that the observation belongs to. (http://www-users.cs.york.ac.uk/adrian/Papers/Journals/TSMC-B06.pdf)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model then looks like this:\n",
      "    \n",
      "    \n",
      "\\begin{align}\n",
      "P(x, z, \\beta) &= P(\\beta) \\prod_{i=1}^n P(z_i \\mid \\beta) P(x_i \\mid z_i, \\beta) %P(x_i, z_i \\mid \\beta)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that given the global hidden parameters, $\\beta$, the local hidden variables $z$ are conditionally independent. While the assumption of total independence (the mean field assumption) doesn't really hold, there is some independence structure here that we are exploiting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal, as usual, is to infer the posterior distribution over the hidden variables, given the observed data:\n",
      "\n",
      "$$P(z, \\beta \\mid x)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, the problem is that the posterior distribution is intractable, because the normalization constant $P(x)$ (aka the partition function) is intractable. Recall the relationship from Bayesian statistics:\n",
      "\n",
      "$$P(A, B \\mid C) = \\frac{P(A, B, C)}{P(C)}$$\n",
      "\n",
      "Using this relationship, we can show the formula needed to calculate the posterior distribution:\n",
      "\n",
      "$$P(z, \\beta \\mid x) = \\frac{P(z, \\beta, x)}{\\int_z \\int_\\beta P(z, \\beta, x)\\ d\\beta\\ dz}$$\n",
      "\n",
      "\n",
      "\n",
      "where the denominator comes from marginalizing over the global hidden variables $\\beta$ and the local hidden variables $z$, which means the denominator becomes a function of the data alone (hence calling it the marginal likelihood)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have the joint distribution from our model, and what we're lacking is the marginal likelihood, $P(x)$.\n",
      "\n",
      "<!--\n",
      "Denote the incomplete data log-likelihood by $P(x \\mid \\beta)$ and the complete data log likelihood by $P(x, z \\mid \\beta)$. Assuming $z$ to be given is equivalent to assuming the cluster assignment is given for each observation in the case of a Gaussian mixture model.\n",
      "\n",
      "Often we do not have the cluster labels, so they are considered a nuisance parameter. We integrate over them to obtain the incomplete data log-likelihood:\n",
      "\n",
      "$${\\rm{ln\\ }} P(x \\mid \\beta) = {\\rm{ln\\ }} \\int_{z} P(x, z \\mid \\beta) dz$$\n",
      "-->\n",
      "\n",
      "We can derive a lower bound for the marginal likelihood \n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(x) &= {\\rm{ln}} \\int_z \\int_\\beta P(z, \\beta, x)\\ d\\beta\\ dz \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_z \\int_\\beta Q(z, \\beta) \\frac{P(z, \\beta, x)}{Q(z, \\beta)}\\ d\\beta\\ dz \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(z, \\beta, x)}{Q(z, \\beta)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(\\beta, z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(z, \\beta, x)}{Q(z, \\beta)} \\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(z, \\beta, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(z, \\beta)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q) \\\\\n",
      "\\end{align}\n",
      "\n",
      "<!--\n",
      "Suppose we have the complete data (that is, we have the cluster assignment $z_i$ for each data point $x_i$, and we have the observations $x_i$). Since the likelihood is a function of $x$ and $z$, denote it $P(x, z \\mid \\beta)$. We want to maximize the likelihood given the model parameters.\n",
      "\n",
      "$$P(\\beta \\mid x, z) = \\frac{P(x, z \\mid \\beta) P(\\beta)}{ \\int_\\beta P(x, z \\mid \\beta) P(\\beta)d\\beta}$$\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The posterior distribution is intractable because the normalization constant (marginal likelihood) is intractable. \n",
      "\n",
      ": HERE! This is weird because my model distribution would change, right? \n",
      "\n",
      "The likelihood function for the observed data and the hidden variables \n",
      "\n",
      "$$P(x \\mid \\beta, z) = \\int_z \\int_\\beta P(x, z, \\beta)dzd\\beta$$\n",
      "\n",
      "\n",
      "$$P(x \\mid \\beta, z) = \\int_z \\int_\\beta P(x, z \\mid \\beta)dzd\\beta$$\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have $${\\rm{\\ ln\\ }} P(x) = \\mathcal{L}(Q) + {\\rm{KL}}(Q || P)$$ (I can send you the proof of this one).\n",
      "\n",
      "To maximize the marginal likelihood, ${\\rm{\\ ln\\ }} P(x)$, we need to maximize the lower bound or minimize the KL divergence between the distribution $Q(z, \\beta)$ and the joint posterior distribution $P(z , \\beta \\mid x)$. Remember the KL divergence is never less than 0, so the log likelihood will always be at least $\\mathcal{L}(Q)$ (so $\\mathcal{L}(Q )$ is a lower bound on the log likelihood). For later, note:\n",
      "\n",
      "$$\\mathcal{L}(Q) =  {\\rm{\\ ln\\ }} P(x) - {\\rm{KL}}(Q || P)$$\n",
      "\n",
      "so $\\mathcal{L}(Q)$ is maximized when ${\\rm{KL}}(Q || P)$ is 0."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "The distribution $Q$ is important here and in the variational inference version of the EM algorithm.\n",
      "\n",
      "In the EM algorithm, we have to evaluate the expectation of the complete data log likelihood with respec to $Q$, since\n",
      "\n",
      "$${\\mathcal{L}}(Q , \\beta) = E_Q \\left[{\\rm{ln\\ }} P(x, z \\mid \\beta)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(z)}\\right]$$\n",
      "\n",
      "The problem is that we often don't have the complete data log likelihood. We usually don't know the values of the $z_i$ variables. All of the information we have about them, though, can be quantified in the posterior for $z$:\n",
      "\n",
      "$$P(z \\mid x, \\beta)$$\n",
      "-->\n",
      "\n",
      "The classical EM algorithm, using this decomposition for the log likelihood, is:\n",
      "\n",
      "#### Algorithm for General EM using the log likelihood decomposition####\n",
      "\n",
      "\n",
      "1. Initialize model parameters $\\beta_c$\n",
      "2. E-step: holding $\\beta_c$ constant, maximize $\\mathcal{L}(Q)$ with respect to $Q(z, \\beta)$ (note because of the decomposition, the maximizer will be exactly the posterior distribution $P(z , \\beta \\mid x)$.\n",
      "3. M-step: holding $Q(z, \\beta_c)$ constant, maximize $\\mathcal{L}(Q)$ with respect to $\\beta$. \n",
      "4. Continue until convergence.\n",
      "\n",
      "\n",
      "\n",
      "<!--\n",
      "We get a value $\\theta_{new}$, which means $KL(Q || P)$ is now nonzero because Q is being held fixed under $\\theta_c$, and the new posterior distribution will be $P(Z \\mid X, \\theta_{new})$. So the log likelihood ${\\rm{\\ ln\\ }} P(X \\mid \\theta)$ will increase\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### What happens when the posterior distribution is intractable? ####\n",
      "\n",
      "In the E-step of the EM algorithm, the distribution $Q$ that maximized the lower bound on the log likelihood was the posterior distribution $P(z, \\beta \\mid x)$. \n",
      "\n",
      "Often, the posterior distribution is intractable.\n",
      "\n",
      "In mean field variational inference, the function $Q$ is assumed to factor over the latent variables and parameters. That is, \n",
      "\n",
      "$$Q(z, \\beta) = Q(z)Q(\\beta)$$\n",
      "\n",
      "Recall there are $n$ values of $z$, so we have:\n",
      "\n",
      "\\begin{align}\n",
      "Q(z, \\beta) &= \\prod\\limits_{j=1}^n Q(z_j)Q(\\beta) \\\\[0.5em]\n",
      "&= Q(\\beta)\\prod\\limits_{j=1}^n Q(z_j)\\\\[0.5em]\n",
      "&= Q(\\beta \\mid \\lambda) \\prod\\limits_{j=1}^n Q(z_j \\mid \\phi_j)\\\\[0.5em]\n",
      "\\end{align}\n",
      "\n",
      "where each variational distribution $Q(\\beta)$ and $Q(z_j)$ has its own set of (variational) parameters, $\\lambda$ for $Q(\\beta)$ and $\\phi_j$ for $Q(z_j)$. Then we minimize the KL divergence between the factorized distribution $Q(z, \\beta)$ and the true posterior distribution $P(z \\mid x, \\beta$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to optimize the variational parameteres $\\lambda$, $\\phi$ alternately to yield the closest possible distribution, within this factorized family, to the true posterior distribution. This looks very similar to the steps in the EM algorithm (we hold one unknown quantity fixed and maximize with respect to the other unknown quantity, then the other way around). After some derivation, we can arrive at the iterative updates for the $Q(z, \\beta)$ distribution (Bishop 2006):\n",
      "\n",
      "####The complete algorithm is:####\n",
      "\n",
      "1. Initialize the parameters of all of the variational distributions $Q_j(Z_j),\\ j = 1, \\ldots, n$\n",
      "2. For j in 1:n\n",
      "    $$Q^{new}_j(Z_j, \\beta) = \\displaystyle{\\frac{{\\rm{exp}}\\left\\{ E_{Q_{i \\ne j, \\beta}} \\left[ {\\rm{\\ln\\ }} P(X, Z, \\beta) \\right]\\right\\}}{\\int_{Z_j} \\rm{exp}\\left\\{ E_{Q_{i \\ne j, \\beta}} \\left[ {\\rm{\\ln\\ }} P(X, Z, \\beta) \\right]\\right\\} d{Z_j}}}$$\n",
      "    \n",
      "3. Iterate until convergence.\n",
      "\n",
      "\n",
      "#### A quick note on the relationship between EM and VB ####\n",
      "\n",
      "Variational EM is a particular algorithm associated with variational inference. While the description above is rather general, the only difference between the EM algorithm and variational EM arises in the M step (Ghahramani, 2003). Instead of maximizing with respect to the unknown parameter $\\beta$ as in the traditional EM algorithm, the maximization is with respect to the parameter's distribution, $q(\\beta)$. If the distribution is restricted to be a point mass, using a Dirac delta function, we recover the original EM algorithm (Ghahramani and Beal, 2001).\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "## Example: Gaussian mixture models using VB ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will work regardless of your Python version\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline \n",
      "\n",
      "#Get 50 samples from each of 4 separate 2D Multivariate Normal distributions\n",
      "x0 = np.random.multivariate_normal([0, 0], [[2, 0], [0, 0.1]], size=50)\n",
      "x1 = np.random.multivariate_normal([0, 0], [[0.1, 0], [0, 2]], size=50)\n",
      "x2 = np.random.multivariate_normal([2, 2], [[2, -1.5], [-1.5, 2]], size=50)\n",
      "x3 = np.random.multivariate_normal([-2, -2], [[0.5, 0], [0, 0.5]], size=50)\n",
      "\n",
      "\n",
      "#concatenate all of the observations together\n",
      "obs = np.vstack([x0, x1, x2, x3]) #this version is used in the online example\n",
      "obsplot = np.r_[x0, x1, x2, x3]  #this version is for my plot. The only difference is formatting.\n",
      "\n",
      "#and plot them\n",
      "plt.plot(obs[:,0][0:49], obs[:,1][0:49], 'ro', obs[:,0][50:99], obs[:,1][50:99], 'bo', obs[:,0][100:149], obs[:,1][100:149], 'rx', obs[:,0][150:199], obs[:,1][150:199], 'bx',)\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('y')\n",
      "min0 = min(obs[:,0]) - 1\n",
      "min1 = min(obs[:,1]) - 1\n",
      "max0 = max(obs[:,0]) + 1\n",
      "max1 = max(obs[:,1]) + 1\n",
      "plt.axis([min0, max0, min1, max1])\n",
      "plt.title('Example data from four MVN distributions')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEZCAYAAAB7HPUdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x106443150>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now suppose we didn't know the cluster labels. That is, we don't know which of our 4 MVN distributions an observation comes from. We can visualize this with a plot without the colours - just the raw data:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline1.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Model####\n",
      "\n",
      "For clarity, let us denote the number of the data vectors with N\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 200"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the dimensionality of the data vectors with D:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we don't know how many clusters our dataset has, we'll start with some large number of clusters, and the algorithm will pare this number down to a 'best guess' as to the true number of clusters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember when we have more than 2 possible clusters (we're assuming 10 here to be safe), we use a Categorical distribution. So the cluster assignments Z will have a Categorical distribution, and the prior for the cluster assignmenat probabilities $P(\\pi)$ will have a Dirichlet distribution:\n",
      "\n",
      "$$P(\\pi) = Dir(\\pi \\mid \\alpha)$$\n",
      "\n",
      "(More about Dirichlet distributions [here](http://en.wikipedia.org/wiki/Dirichlet_distribution), but for this example you only need to know that it's the conjugate prior of the Categorical distribution)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Dirichlet, Categorical\n",
      "\n",
      "#Create a Dirichlet process prior for the cluster assignments our P(\\pi) \n",
      "alpha = Dirichlet(1e-5*npones(K), name = 'alpha') \n",
      "\n",
      "#The cluster assignments themselves follow a Categorical distribution\n",
      "#Since there are N observations, there will be N cluster assignments\n",
      "Z = Categorical(alpha, plates=(N,), name='z')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also want to find the mean vectors and precision matrices of the clusters themselves (the precision matrix is the inverse of the covariance matrix, and is arguably nicer to work with when deriving the math behind this process). The conjugate prior of the mean of a Gaussian distribution is another Gaussian, and we will have exactly K elements in our mean vector, $\\mu$ (each cluster will have its own mean). \n",
      "\n",
      "The conjugate prior of the precision matrix $\\Lambda$ (inverse of the covariance matrix) is a Wishart distribution. (More about [Wishart distributions](http://en.wikipedia.org/wiki/Wishart_distribution)).\n",
      "\n",
      "(Side note: Some people like to put all of this together and talk about a Normal-inverse-Wishart distribution as a conjugate prior for a multivariate Normal distribution with unknown mean and covariance matrix. If you're interested, here's the [Wikipedia article](http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution).)\n",
      "\n",
      "So we have:\n",
      "\n",
      "$$P(\\Lambda) = \\prod_{k = 1}^{K} \\mathcal{W}(\\lambda_k \\mid W_0, \\eta_0)$$\n",
      "\n",
      "$$P(\\mu \\mid \\Lambda) = \\prod_{k = 1}^{K} \\mathcal{N}(\\mu_k \\mid m_0, (\\beta_0 \\lambda_k)^{-1})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Gaussian, Wishart\n",
      "\n",
      "mu = Gaussian(np.zeros(D), 1e-5*np.identity(D), plates=(K,), name='mu')\n",
      "\n",
      "Lambda = Wishart(D, 1e-5*np.identity(D), plates=(K,), name='Lambda')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that our data come from a Gaussian mixture distribution, but we don't know anything else. We create a Gaussian mixture distribution object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "from bayespy.nodes import Mixture\n",
      "\n",
      " Y = Mixture(Z, Gaussian, mu, Lambda, name='Y') #This is the object\n",
      " Y.observe(obs) #and these are the model observations - the data."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we randomly initialize our cluster assignments $z$ to start everything off:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "Z.initialize_from_random()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to perform inference (get distributional estimates) on the hidden values, given the data:\n",
      "\n",
      "Z (cluster assignments), \n",
      "\n",
      "$\\Lambda$ (the precision matrix), \n",
      "\n",
      "$\\mu$ (the vector of means),\n",
      "\n",
      "$K$ (the number of clusters), and\n",
      "\n",
      "$\\pi$ (the probability of being in a cluster)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Inference####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create a Variational Bayes object (same as variational inference in this case). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "from bayespy.inference import VB\n",
      "\n",
      "Q = VB(Y, mu, Lambda, Z, alpha)\n",
      "\n",
      "Q.update(repeat=1000)\n",
      "\n",
      "#This is the output:\n",
      "#Iteration 1: loglike=-1.402345e+03 (... seconds)\n",
      "#...\n",
      "#Iteration 61: loglike=-8.888464e+02 (... seconds)\n",
      "#Converged at iteration 61.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the results can be visualized in the following plot:\n",
      "    \n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline2.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the algorithm has found all four clusters, and has figured out the cluster means and covariances, visualized by the ellipsoids. It has also figured out which observations belong to each cluster, with a few expections. It's clear that it would be hard to figure out the cluster assignment for the data point on the far right, for example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Summary ###\n",
      "\n",
      "Variational inference is useful when the posterior distribution is intractable. It is similar to several methods, like the EM (expectation-maximization) algorithm, especially for mixtures of Gaussians. According to Bishop (2006), there are some advantages to variational inference over the EM algorithm:\n",
      "\n",
      "1. The singularities that we see in the (frequentist) EM algorithm do not arise in this Bayesian treatment.\n",
      "\n",
      "2. If we choose a large number of initial clusters (here we chose K = 10), we don't have to worry about overfitting. The EM algorithm cannot find the number of clusters.\n",
      "\n",
      "3. Finding the optimal value of K did not require the extra step of cross-validation.\n",
      "\n",
      "Variational methods have been extended beyond the mean-field assumption, and have been extended to stochastic versions (see Hoffman et al 2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Markov chain Monte Carlo ##\n",
      "\n",
      "The general process of MCMC is to construct a Markov chain that converges to the unknown posterior distribution, then draw samples from the Markov chain once it reaches equilibrium. These samples can be regarded as dependent samples from the posterior distribution.\n",
      "\n",
      "One of the most popular methods of MCMC is the random walk Metropolis-Hastings version:\n",
      "\n",
      "### RWMH MCMC ###\n",
      "\n",
      "1. Initialize parameters from their prior distributions\n",
      "2. Calculate the log likelihood of the data under the current parameter settings.\n",
      "3. Propose new candidate values for the parameters, using a symmetric proposal distribution and the current values.\n",
      "4. Calculate the log likelihood of the data under the candidate parameter values.\n",
      "5. Calculate the acceptance probability.\n",
      "6. Stochastically accept the candidate values of the parameters according to the acceptance probability.\n",
      "7. Go to 2) until the maximum number of iterations has been reached.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variational inference differs fundamentally from this approach as there is no stochasticity in the classical variational inference algorithm. Also, in stochastic variational inference, the only randomness introduced is through sampling noise. By contrast, in MCMC, the act of moving from one set of parameter values to another is governed by a transition probability.\n",
      "\n",
      "A simpler algorithm for MCMC is obtained through straightforward Gibbs sampling. While the algorithm is slightly different, the result remains the same: we use a Markov chain to stand in for the posterior distribution and sample from the Markov chain at equilibrium. Again, the parameter values are chosen stochastically, not updated deterministically, which constitutes the major difference between variational inference and Gibbs sampling.\n",
      "\n",
      "###Algorithm for Gibbs Sampling### \n",
      "\n",
      "1. Initialize parameters $\\beta_j, j = 1, \\ldots n$ from their prior distributions\n",
      "2. Draw a new value for the $j^{th}$ parameter, given the current settings of all other parameters. Do this for all $n$ parameters.\n",
      "3. Iterate until convergence.\n",
      "\n",
      "([Reference for Gibbs Sampling algorithm](https://pymc-devs.github.io/pymc/theory.html#monte-carlo-methods-in-bayesian-analysis) )"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### RWMH MCMC for Gaussian mixture model ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Copyright 2011 Tom SF Haines\n",
      "\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
      "\n",
      "#   http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
      "\n",
      "#https://code.google.com/p/haines/source/browse/gcp/wishart.py\n",
      "\n",
      "\n",
      "import math\n",
      "import random\n",
      "import numpy\n",
      "import numpy.linalg\n",
      "import numpy.random\n",
      "import scipy.special\n",
      "\n",
      "\n",
      "\n",
      "class Wishart:\n",
      "  \"\"\"Simple Wishart distribution class, quite basic really, but has caching to avoid duplicate computation.\"\"\"\n",
      "  def __init__(self, dims):\n",
      "    \"\"\"dims is the number of dimensions - it initialises with the dof set to 1 and the scale set to the identity matrix. Has copy constructor support.\"\"\"\n",
      "    if isinstance(dims, Wishart):\n",
      "      self.dof = dims.dof\n",
      "      self.scale = dims.scale.copy()\n",
      "      self.invScale = dims.invScale.copy() if dims.invScale!=None else None\n",
      "      self.norm = dims.norm\n",
      "      self.cholesky = dims.cholesky.copy() if dims.cholesky!=None else None\n",
      "    else:\n",
      "      self.dof = 1.0\n",
      "      self.scale = numpy.identity(dims, dtype=numpy.float32)\n",
      "      self.invScale = None\n",
      "      self.norm = None\n",
      "      self.cholesky = None\n",
      "\n",
      "  def setDof(self, dof):\n",
      "    \"\"\"Sets the degrees of freedom of the distribution.\"\"\"\n",
      "    self.dof = dof\n",
      "    self.norm = None\n",
      "\n",
      "  def setScale(self, scale):\n",
      "    \"\"\"Sets the scale matrix, must be symmetric positive definite\"\"\"\n",
      "    ns = numpy.array(scale, dtype=numpy.float32)\n",
      "    assert(ns.shape==self.scale.shape)\n",
      "    self.scale = ns\n",
      "    self.invScale = None\n",
      "    self.norm = None\n",
      "    self.cholesky = None\n",
      "\n",
      "  def getDof(self):\n",
      "    \"\"\"Returns the degrees of freedom.\"\"\"\n",
      "    return self.dof\n",
      "\n",
      "  def getScale(self):\n",
      "    \"\"\"Returns the scale matrix.\"\"\"\n",
      "    return self.scale\n",
      "\n",
      "  def getInvScale(self):\n",
      "    \"\"\"Returns the inverse of the scale matrix.\"\"\"\n",
      "    if self.invScale==None:\n",
      "      self.invScale = numpy.linalg.inv(self.scale)\n",
      "    return self.invScale\n",
      "\n",
      "\n",
      "  def getNorm(self):\n",
      "    \"\"\"Returns the normalising constant of the distribution, typically not used by users.\"\"\"\n",
      "    if self.norm==None:\n",
      "      d = self.scale.shape[0]\n",
      "      self.norm  = math.pow(2.0,-0.5*self.dof*d)\n",
      "      self.norm *= math.pow(numpy.linalg.det(self.scale),-0.5*self.dof)\n",
      "      self.norm *= math.pow(math.pi,-0.25*d*(d-1))\n",
      "      for i in xrange(d):\n",
      "        self.norm /= scipy.special.gamma(0.5*(n-i))\n",
      "    return self.norm\n",
      "\n",
      "  def prob(self, mat):\n",
      "    \"\"\"Returns the probability of the provided matrix, which must be the same shape as the scale matrix and also symmetric and positive definite.\"\"\"\n",
      "    d = self.scale.shape[0]\n",
      "    val  = math.pow(numpy.linalg.det(mat),0.5*(n-1-d))\n",
      "    val *= math.exp(-0.5 * numpy.linalg.trace(numpy.dot(mat,self.getInvScale())))\n",
      "    return self.getNorm() * val\n",
      "\n",
      "\n",
      "  def sample(self):\n",
      "    \"\"\"Returns a draw from the distribution - will be a symmetric positive definite matrix.\"\"\"\n",
      "    if self.cholesky==None:\n",
      "      self.cholesky = numpy.linalg.cholesky(self.scale)\n",
      "    d = self.scale.shape[0]\n",
      "    a = numpy.zeros((d,d),dtype=numpy.float32)\n",
      "    for r in xrange(d):\n",
      "      if r!=0: a[r,:r] = numpy.random.normal(size=(r,))\n",
      "      a[r,r] = math.sqrt(random.gammavariate(0.5*(self.dof-d+1),2.0))\n",
      "    return numpy.dot(numpy.dot(numpy.dot(self.cholesky,a),a.T),self.cholesky.T)\n",
      "\n",
      "\n",
      "  def __str__(self):\n",
      "    return '{dof:%f, scale:%s}'%(self.dof, str(self.scale))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymc as pm\n",
      "\n",
      "#Lambda = Wishart(D, 1e-5*np.identity(D), plates=(K,), name='Lambda')\n",
      "mu = np.random.multivariate_normal(np.zeros(D), 1e-5*np.identity(D), 4)\n",
      "L1 = Wishart(4)\n",
      "Lambda = L1.sample\n",
      "print Lambda\n",
      "#Lambda = pm.Wishart(\"cov_matrix_inv\", 200 ,np.linalg.inv(cov_matrix))\n",
      "\n",
      "#print mu\n",
      "#mu = pm.Normal('mu', np.zeros(D), 1e-5*np.identity(D), size=4)\n",
      "#centers = pm.Normal('centers', [0.3, 0.7], [1/(0.1)**2, 1/(0.1)**2], size=2)\n",
      "\n",
      "\n",
      "#print 1e-5*np.identity(D)\n",
      "\n",
      "#observations = pm.Normal('samples_model', mu=mu, tau=tau, value=samples, observed=True)\n",
      "#model = pm.Model([obs, mu, Lambda, Z, alpha])\n",
      "\n",
      "#mcmc = pm.MCMC(model)\n",
      "#mcmc.sample(100000, 30000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'function' object has no attribute 'value'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-57-04c2eaeeac79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWishart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#Lambda = pm.Wishart(\"cov_matrix_inv\", 200 ,np.linalg.inv(cov_matrix))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'value'"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stochastic variational inference ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}