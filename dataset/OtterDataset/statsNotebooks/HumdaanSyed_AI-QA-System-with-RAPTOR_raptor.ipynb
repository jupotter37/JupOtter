{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and instantiating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI Projects\\Long Context RAPTOR\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from getpass import getpass\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_milvus.vectorstores import Milvus\n",
    "from collections import defaultdict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Gemini API key from user\n",
    "# Instantiating LLM\n",
    "\n",
    "api_key = getpass()\n",
    "model = GoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash-latest\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Embedding Model\n",
    "# normalize_embeddings is set True for cosine-similarity\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, tokenizing and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF Directory\n",
    "\n",
    "document_loader = PyPDFDirectoryLoader(\"./data\")\n",
    "documents = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Function that returns token count\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "docs_texts = [d.page_content for d in documents] # Store content of each page in list\n",
    "\n",
    "# Visualize token count in docs_texts\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 561253\n"
     ]
    }
   ],
   "source": [
    "# Get total token count \n",
    "d_sorted = sorted(documents, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text splits generated: 956\n"
     ]
    }
   ],
   "source": [
    "# Chunking the data\n",
    "\n",
    "chunk_size_tok = 1000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)\n",
    "#\n",
    "print(f\"Number of text splits generated: {len(texts_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAPTOR Tree Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is information contained in the books, Artificial Intelligence for Dummies, Data Science from Scratch and Python for Data Science for Dummies. \n",
    "    \n",
    "    These books serve to give a basic knowledge of their topics to readers.\n",
    "    \n",
    "    Give a detailed summary of the books provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 238 clusters--\n",
      "--Generated 49 clusters--\n",
      "--Generated 9 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree (Around 30 mins)\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vector stores and instantiate retrievers (Around 4 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    \n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Use all_texts to build vector store using MILVUS\n",
    "vectorstore_raptor = Milvus.from_texts(\n",
    "    texts=all_texts,\n",
    "    embedding=embd,\n",
    "    collection_name=\"RaptorCollection\",\n",
    "    connection_args={\n",
    "        \"uri\": \"http://localhost:19530\",\n",
    "    },\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "#Use documents and build vector store for retrieving metadata (Page and Source)\n",
    "vectorstore_metad = Milvus.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embd,\n",
    "    collection_name=\"MetaData\",\n",
    "    connection_args={\n",
    "        \"uri\": \"http://localhost:19530\",\n",
    "    },\n",
    "    drop_old=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## from langchain_community.retrievers import BM25Retriever\n",
    "## bm25_retriever = BM25Retriever.from_texts(texts = all_texts) => Subpar retrieval when compared to vectore-store retriever\n",
    "\n",
    "\n",
    "# Instantiate response retriever\n",
    "answer_retriever = vectorstore_raptor.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "#Instatiate metadata retriever\n",
    "metadata_retriever = vectorstore_metad.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building RAG chain, generating response and source for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prompt\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an assistant for question-answering tasks.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": answer_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_metadata(query):\n",
    "\n",
    "    # Retrieve 4 documents that match the query and get their metadata\n",
    "    metadata_docs = metadata_retriever.invoke(query) \n",
    "    metadata_list = [doc.metadata for doc in metadata_docs]\n",
    "\n",
    "    final_metadata = defaultdict(list)\n",
    "\n",
    "    for d in metadata_list:\n",
    "        final_metadata[d['source']].append(d['page'])\n",
    "\n",
    "    print(\"Source for reference:\" )\n",
    "    for source, pages in final_metadata.items():\n",
    "        print(f\"Book: {source} => Page numbers: {sorted(pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing data is crucial because it can significantly impact the accuracy and reliability of data analysis and machine learning models. If left unaddressed, missing data can lead to biased results, inaccurate predictions, and flawed conclusions.\n",
      "\n",
      "Here are the common methods for managing missing values in Python:\n",
      "\n",
      "**1. Deletion:**\n",
      "\n",
      "* **Listwise Deletion:**  Removes entire rows containing missing values. This is simple but can lead to significant data loss, especially if missing values are frequent.\n",
      "* **Pairwise Deletion:**  Excludes cases with missing values only for specific calculations, but this can introduce bias if missingness is not random.\n",
      "\n",
      "**2. Imputation:**\n",
      "\n",
      "* **Mean/Median/Mode Imputation:** Replaces missing values with the mean, median, or mode of the respective column. This is simple but can distort the distribution of the data.\n",
      "* **K-Nearest Neighbors (KNN) Imputation:**  Uses the values of nearest neighbors to predict missing values. This can be more accurate than simple mean/median imputation but can be computationally expensive.\n",
      "* **Regression Imputation:**  Uses a regression model to predict missing values based on other variables. This can be effective if there is a strong relationship between the missing variable and other variables.\n",
      "\n",
      "**3. Interpolation:**\n",
      "\n",
      "* **Linear Interpolation:**  Estimates missing values based on a linear relationship between neighboring values. This is suitable for data with a clear linear trend.\n",
      "* **Spline Interpolation:**  Uses a smooth curve to estimate missing values, which can be more accurate than linear interpolation for data with non-linear trends.\n",
      "\n",
      "**Difference between Imputation and Interpolation:**\n",
      "\n",
      "* **Imputation:** Replaces missing values with estimated values based on the existing data. It's suitable for situations where the missing values are scattered randomly.\n",
      "* **Interpolation:**  Estimates missing values based on the relationship between neighboring values. It's suitable for data with a clear trend or pattern.\n",
      "\n",
      "**When to prefer one over the other:**\n",
      "\n",
      "* **Imputation:**  Preferable when missing values are scattered randomly and there's no clear trend in the data.\n",
      "* **Interpolation:**  Preferable when there's a clear trend or pattern in the data and missing values are sequential.\n",
      "\n",
      "The choice of method depends on the nature of the data, the extent of missing values, and the specific analysis goals. It's important to carefully consider the potential biases and limitations of each method before applying it. \n",
      "\n",
      "Source for reference:\n",
      "Book: data\\Python_for_Data_Science_For_Dummies.pdf => Page numbers: [132, 133, 134]\n",
      "Book: data\\Artificial_Intelligence_For_Dummies.pdf => Page numbers: [53]\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you explain the importance of handling missing data in a dataset, and describe the different methods available in Python for managing missing values? Additionally, how do techniques like imputation and interpolation differ, and in what scenarios would you prefer one over the other?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Print Response\n",
    "print(str(response))\n",
    "\n",
    "# Print source\n",
    "get_metadata(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are a type of machine learning model inspired by the structure and function of the human brain. They consist of interconnected nodes, called neurons, organized in layers. Each neuron receives input from other neurons, performs a simple calculation, and then outputs a value to other neurons.\n",
      "\n",
      "Here's a breakdown of key components:\n",
      "\n",
      "* **Neurons:** The basic building blocks of a neural network. They receive input signals, perform a weighted sum of those inputs, apply an activation function, and output a value.\n",
      "* **Activation Functions:** These functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include sigmoid, ReLU, and tanh.\n",
      "* **Layers:** Neurons are organized into layers. The first layer is the input layer, which receives data. The last layer is the output layer, which produces the network's predictions. Hidden layers lie between the input and output layers and perform intermediate computations.\n",
      "* **Weights:** Each connection between neurons has a weight associated with it. These weights represent the strength of the connection and are adjusted during training to improve the network's performance.\n",
      "* **Backpropagation:** This algorithm is used to train neural networks. It calculates the error between the network's predictions and the actual values and uses this error to adjust the weights.\n",
      "\n",
      "**Example: The Perceptron**\n",
      "\n",
      "The perceptron is a simple neural network that can be used to implement simple logic gates. It consists of a single neuron with multiple inputs and a single output. The neuron calculates a weighted sum of its inputs and applies a step function to determine its output.\n",
      "\n",
      "For example, consider a perceptron with two inputs, x1 and x2, and a single output, y. The neuron's output is calculated as follows:\n",
      "\n",
      "```\n",
      "y = step_function(w1 * x1 + w2 * x2 + b)\n",
      "```\n",
      "\n",
      "where w1 and w2 are the weights, b is the bias, and step_function is a function that outputs 1 if its input is greater than or equal to 0, and 0 otherwise.\n",
      "\n",
      "By adjusting the weights and bias, the perceptron can be trained to implement different logic gates, such as AND, OR, and XOR.\n",
      "\n",
      "**Example: Deep Learning**\n",
      "\n",
      "Deep learning is a type of machine learning that uses deep neural networks with multiple hidden layers to learn complex patterns from data. These networks can be used for a wide range of tasks, including image recognition, natural language processing, and machine translation.\n",
      "\n",
      "For example, a deep neural network can be trained to recognize handwritten digits from images. The network would have an input layer that receives the image data, multiple hidden layers that perform intermediate computations, and an output layer that predicts the digit.\n",
      "\n",
      "**Key Advantages of Neural Networks:**\n",
      "\n",
      "* **Non-linearity:** They can learn complex patterns that are not linearly separable.\n",
      "* **Feature Learning:** They can automatically learn features from data, reducing the need for manual feature engineering.\n",
      "* **Generalization:** They can generalize well to new data, even if it is different from the data they were trained on.\n",
      "\n",
      "**Limitations of Neural Networks:**\n",
      "\n",
      "* **Black Box:** It can be difficult to understand how they make predictions.\n",
      "* **Data Requirements:** They require large amounts of data to train effectively.\n",
      "* **Computational Cost:** Training and running large neural networks can be computationally expensive.\n",
      "\n",
      "Overall, neural networks are a powerful tool for machine learning, but they have limitations that need to be considered. \n",
      "\n",
      "Source for reference:\n",
      "Book: data\\Artificial_Intelligence_For_Dummies.pdf => Page numbers: [191, 195, 197]\n",
      "Book: data\\Data_Science_from_Scratch.pdf => Page numbers: [291]\n"
     ]
    }
   ],
   "source": [
    "query = \"Give a detailed explanation of neural networks. Give examples when possible.\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Print Response\n",
    "print(str(response))\n",
    "\n",
    "# Print source\n",
    "get_metadata(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pk': 451348365454083351}, page_content='Chapter 5. Statistics\\nFacts ar e stubborn, but statistics ar e mor e pliable.\\n— Mark T wain\\nStatistics  refers  to the mathematics and techniques with which we\\nunderstand data. It is a rich, enormous field, more suited to a shelf (or\\nroom) in a library than a chapter in a book, and so our discussion will\\nnecessarily not be a deep one. Instead, I’ll try to teach you just enough to be\\ndangerous, and pique your interest just enough that you’ll go of f and learn\\nmore.\\nD e s c r i b i n g  a  S i n g l e  S e t  o f  D a t a\\nThrough  a combination of word of mouth and luck, DataSciencester has\\ngrown to dozens of members, and the VP of Fundraising asks you for some\\nsort of description of how many friends your members have that he can\\ninclude in his elevator pitches.\\nUsing techniques from Chapter 1 , you are easily able to produce this data.\\nBut now you are faced with the problem of how to describe  it.\\nOne obvious description of any dataset is simply the data itself:\\nnum_friends = [100, 49, 41, 40, 25, \\n               # ... and lots more \\n              ]\\nFor a small enough dataset, this might even be the best description. But for\\na lar ger dataset, this is unwieldy and probably opaque. (Imagine staring at a\\nlist of 1 million numbers.) For that reason, we use statistics to distill and\\ncommunicate relevant features of our data.'),\n",
       " Document(metadata={'pk': 451348365454083433}, page_content='Chapter 10. W orking with Data\\nExperts often possess mor e data than judgment.\\n— Colin Powell\\nW orking with data is both an art and a science. W e’ve mostly been talking\\nabout the science part, but in this chapter we’ll look at some of the art.\\nE x p l o r i n g  Y o u r  D a t a\\nAfter  you’ve identified the questions you’re trying to answer and have\\ngotten your hands on some data, you might be tempted to dive in and\\nimmediately start building models and getting answers. But you should\\nresist this ur ge. Y our first step should be to explor e  your data.\\nExploring One-Dimensional Data\\nThe simplest case is when you have a  one-dimensional dataset, which is just\\na collection of numbers. For example, these could be the daily average\\nnumber of minutes each user spends on your site, the number of times each\\nof a collection of data science tutorial videos was watched, or the number of\\npages of each of the data science books in your data science library .\\nAn obvious first step is to compute a few summary statistics. Y ou’d like to\\nknow how many data points you have, the smallest, the lar gest, the mean,\\nand the standard deviation.\\nBut even these don’ t necessarily give you a great understanding. A good\\nnext step is to create a histogram, in which you group your data into\\ndiscrete buckets  and count how many points fall into each bucket:\\nfrom typing import List, Dict \\nfrom collections import Counter \\nimport math \\n '),\n",
       " Document(metadata={'pk': 451348365454084367}, page_content='You\\'ve provided a snippet of code from a book, but you haven\\'t given me the actual summaries of the books \"Artificial Intelligence for Dummies,\" \"Data Science from Scratch,\" and \"Python for Data Science for Dummies.\" \\n\\nTo give you a detailed summary, I need the actual content of those books. \\n\\nHowever, I can tell you that the code snippet you provided is likely from a section on word embeddings or word vectors, a fundamental concept in natural language processing (NLP). \\n\\nHere\\'s a breakdown of what the code does:\\n\\n1. **Creating a Toy Dataset:** The code creates a small dataset of words categorized into different parts of speech (colors, nouns, verbs, adverbs, adjectives). This is a simplified example to illustrate the concept of word vectors.\\n2. **Generating Sentences:** The `make_sentence()` function randomly selects words from each category to create sentences with a consistent structure.\\n3. **Creating a Corpus:** The code generates 50 sentences using the `make_sentence()` function, forming a small corpus of text.\\n\\nThe purpose of this code is to demonstrate how word vectors can be learned from a corpus of text. The idea is that words that appear in similar contexts (like \"red\" and \"blue\" in the example) should have similar word vectors. \\n\\n**To get a detailed summary of the books, please provide the following:**\\n\\n* **The actual content of the books:**  You can either copy and paste relevant sections or provide links to the books.\\n* **Specific topics you\\'re interested in:**  Are you looking for an overview of the entire book, or are you interested in specific chapters or sections?\\n\\nOnce I have this information, I can provide you with a detailed summary of the books. \\n'),\n",
       " Document(metadata={'pk': 451348365454083798}, page_content='12\\nPart I: Getting Started with Python for Data Science  \\nHowever, the mathematical basis behind data science is centuries old\\xa0because \\ndata science is essentially a method of viewing and analyzing  stati stics and \\nprobability. The first essential use of statistics as a term comes in 1749, but  statistics are certainly much older than that. People have used statis\\xad\\ntics\\xa0to recognize patterns for thousands of years. For example, the  historian \\nThucydides (in his History of the Peloponnesian War) describes how the \\nAthenians calculated the height of the wall of Platea in fifth century BC by \\ncounting bricks in an unplastered section of the wall. Because the count \\nneeded to be accurate, the Athenians took the average of the count by several \\nsolders.\\nThe process of quantifying and understanding statistics is relatively new, \\nbut the science itself is quite old. An early attempt to begin documenting the \\n importance of statistics appears in the ninth century when Al‐Kindi wrote \\nManuscript on Deciphering Cryptographic Messages. In this paper, Al‐Kindi \\ndescribes how to use a combination of statistics and frequency analysis to \\n decipher encrypted messages. Even in the beginning, statistics saw use in \\n practical application of science to tasks that seemed virtually impossible to \\ncomplete. Data science continues this process, and to some people it might \\nactually seem like magic.\\nOutlining the core competencies  \\nof a data scientist\\nLike most complex trades today, the data scientist requires knowledge of a broad range of skills in order to perform the required tasks. In fact, so \\nmany different skills are required that data scientists often work in teams. \\nSomeone who is good at gathering data might team up with an analyst and \\nsomeone gifted in presenting information. It would be hard to find a single \\nperson with all the required skills. With this in mind, the following list \\ndescribes areas in which a data scientist could excel (with more compet\\xad\\nencies being better):\\n ✓Data capture: It doesn’t matter what sort of math skills you have if you \\ncan’t obtain data to analyze in the first place. The act of capturing data \\nbegins by managing a data source using database management skills. \\nHowever, raw data isn’t particularly useful in many situations — you \\nmust also understand the data domain so that you can look at the \\ndata and begin formulating the sorts of questions to ask. Finally, you \\nmust have data‐modeling skills so that you understand how the data is \\n connected and whether the data is structured.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_retriever.invoke(\"Generate an answer which summarizes statistics in 200 words. You can use examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\Artificial_Intelligence_For_Dummies.pdf', 'page': 197, 'pk': 451348365454084675}, page_content='182      PART 3  Working with Software-Based AI Applicationsapplies, also renders nonlinear the resulting recombination of the inputs received \\nby the connections. Both of these neural network components enable the algo -\\nrithm to learn complex target functions that represent the relationship between \\nthe input features and the target outcome.\\nUnderstanding the role of backpropagation\\nLearning occurs in a human brain because of the formation and modification of \\nsynapses between neurons, based on stimuli received by trial-and-error experi-\\nence. Neural networks provide a way to replicate this process as a mathematical \\nformulation called backpropagation. Here’s how this architecture of interconnected \\ncomputing units can solve problems: The units receive an example, and if they \\ndon’t guess correctly, they retrace the problem in the system of existing weights \\nusing backpropagation and fix it by changing some values. This process goes on \\nfor many iterations before a neural network can learn. Iterations in a neural net -\\nwork are called epochs, a name that fits perfectly because a neural network may \\nneed days or weeks of training to learn complex tasks.\\nBackpropagation math is quite advanced and requires knowledge of concepts such \\nas derivatives. You can read a detailed but accessible math description in Machine \\nLearning For Dummies,  2nd Edition, by John Paul Mueller and Luca Massaron \\n(Wiley) and get an overview of the necessary calculations. Backpropagation as a \\nconcept is intuitive enough to grasp and convey because it resembles what people \\ndo when performing a task using iterated approximate trial and error. Since the \\nappearance of the backpropagation algorithm in the 1970s, developers have fixed \\nit many times and are currently discussing whether to rethink it. (You can read the \\nopinion of Geoffrey Hinton, one of the coauthors of the method, at https://\\ntinyurl.com/rrea42wz .) Backpropagation is at the core of the present AI renais -\\nsance. In the past, each neural network learning process improvement resulted in \\nnew applications and a renewed interest in the technique. Also, the current deep \\nlearning revolution, which involves a revival of neural networks (abandoned at \\nthe beginning of the 1990s), resulted from key advances in the way neural net -\\nworks learn from their errors.\\nIntroducing Deep Learning\\nAfter backpropagation, the next improvement in neural networks led to deep \\nlearning. Research continued in spite of the AI winter, and neural networks over-\\ncame technical problems, such as the vanishing gradient,  which limits the '),\n",
       " Document(metadata={'source': 'data\\\\Artificial_Intelligence_For_Dummies.pdf', 'page': 195, 'pk': 451348365454084673}, page_content='180       PART 3  Working with Software-Based AI ApplicationsThe figure shows a simple neural network architecture. Note how the layers filter \\nand process information in a progressive way. This is a feed-forward input  because \\ndata feeds one direction into the network. Connections exclusively link units in \\none layer with units in the following layer (information flows from left to right). \\nNo connections exist between units in the same layer or with units outside the \\nnext layer. Moreover, the information pushes forward (from the left to the right). \\nProcessed data never returns to previous neuron layers.\\nIn more advanced neural network applications, you also have to decide on the \\nlayer types you need and the large number of parameters that will influence the \\nlayers’ behavior. Neural networks are extremely flexible, and that aspect is a  \\ndouble-edged sword: You increase the power of the machine learning tool as \\ncomplexity skyrockets.\\nUsing a neural network is like using a stratified filtering system for water: You \\npour the water from above, and the water is filtered at the bottom. The water has \\nno way to go back up; it just goes forward and straight down, and never laterally. \\nIn the same way, neural networks force data features to flow through the network \\nand mix with each other as dictated by the network’s architecture. By using the \\nbest architecture to mix features, the neural network creates newly composed \\nfeatures at every layer and helps achieve better predictions. Unfortunately, in \\nspite of the efforts of academics to discover a theoretical rule, you have no way to \\ndetermine the best architecture without empirically trying different solutions and \\ntesting whether output data helps predict your target values after flowing through \\nthe network. This need for manual configuration illustrates the no-free-lunch the -\\norem (which you can read about in Chapter\\xa010) in action. The gist of it is that an \\narchitecture that works the best on one task won’t necessarily perform success -\\nfully on other problems.\\nSometimes concepts can be understood better if directly tested in reality. Google \\noffers a Neural Network Playground ( http://playground.tensorflow.org ) in \\nwhich you can actually test how a neural network works in an intuitive manner, \\nas shown in Figure\\xa011-3. You see how the neural network builds a neural network \\nby adding or removing layers and changing kinds of activations.\\nFiguring out the secret is in the weights\\nNeural networks have different layers, with each one having its own weights. \\nWeights  represent the strength of the connection between neurons in the network. \\nWhen the weight of the connection between two layers is small, it means that the \\nnetwork dumps values flowing between them and signals that taking this route \\nisn’t likely to influence the final prediction. Likewise, a large positive or negative \\nvalue affects the values that the next layer receives, thus determining certain pre-\\ndictions. This approach is analogous to brain cells, which don’t stand alone but '),\n",
       " Document(metadata={'source': 'data\\\\Data_Science_from_Scratch.pdf', 'page': 291, 'pk': 451348365454085140}, page_content='Chapter 18. Neural Networks\\nI like nonsense; it wakes up the brain cells.\\n— Dr . Seuss\\nAn  artificial neural network  (or neural network for short)  is a predictive\\nmodel motivated by the way the brain operates. Think of the brain as a\\ncollection of neurons wired together . Each neuron looks at the outputs of\\nthe other neurons that feed into it, does a calculation, and then either fires\\n(if the calculation exceeds some threshold) or doesn’ t (if it doesn’ t).\\nAccordingly , artificial neural networks  consist of artificial neurons, which\\nperform similar calculations over their inputs. Neural networks can solve a\\nwide variety of problems like handwriting recognition and face detection,\\nand they are used heavily in deep learning, one of the trendiest subfields of\\ndata science. However , most neural networks are “black boxes”—\\ninspecting their details doesn’ t give you much understanding of how  they’re\\nsolving a problem. And lar ge neural networks can be dif ficult to train. For\\nmost problems you’ll encounter as a budding data scientist, they’re\\nprobably not the right choice. Someday , when you’re trying to build an\\nartificial intelligence to bring about the Singularity , they very well might be.\\nP e r c e p t r o n s\\nPretty  much the simplest neural network is the per ceptr on , which\\napproximates a single neuron with n  binary inputs. It computes a weighted\\nsum of its inputs and “fires” if that weighted sum is 0 or greater:\\nfrom scratch.linear_algebra import Vector, dot \\n \\ndef step_function(x: float) -> float: \\n    return 1.0 if x >= 0 else 0.0 \\n \\ndef perceptron_output(weights: Vector, bias: float, x: Vector) -> float: '),\n",
       " Document(metadata={'source': 'data\\\\Artificial_Intelligence_For_Dummies.pdf', 'page': 191, 'pk': 451348365454084669}, page_content='176      PART 3  Working with Software-Based AI ApplicationsShaping Neural Networks Similar  \\nto the Human Brain\\nThe following sections present a family of learning algorithms that derive inspi -\\nration from how the brain works. They’re neural networks, the core algorithm of \\nthe connectionists’ tribe that best mimics neurons inside human brains at a \\nsmaller scale. (See Chapter\\xa01 for an overview of the five tribes of machine learning \\nemployed by various scientists.)\\nConnectionism is the machine learning approach based on neuroscience, as well \\nas the example of biologically interconnected networks.\\nIntroducing the neuron\\nHuman brains have billions of neurons, which are cells that receive, process, and \\ntransmit electric and chemical signals. Each neuron possesses a nucleus with fila -\\nments that act as inputs; dendrites that receive signals from other neurons; and a \\nsingle output filament, the axon, that terminates with synapses devoted to outside \\ncommunication. Neurons connect to other neurons and transmit information \\nbetween them using chemicals, whereas information inside the neuron itself is \\nelectrically processed. You can read more about neuronal structure in “What’s the \\nBasic Structure of Nerves?” at Dummies.com (or in Neuroscience For Dummies,  2nd \\nEdition, by Frank Amthor (Wiley).\\nReverse-engineering how a brain processes signals helps the connectionists \\ndefine neural networks based on biological analogies and their components. Con-\\nnectionists thus use an abundance of brain terms such as neurons, activation, and \\nconnections as names for mathematical operations. Yet, in spite of the biological \\nterms, neural networks resemble nothing more than a series of multiplications \\nand summations when you check their math formulations. These algorithms are \\nextraordinarily effective at solving complex problems such as image and sound \\nrecognition or machine language translation; using specialized hardware, they \\ncan execute prediction computations quickly.\\nStarting with the miraculous perceptron\\nThe core of a neural network algorithm is the neuron (also called a unit). Many \\nneurons arranged in an interconnected structure make up a neural network, with \\neach neuron linking to the inputs and outputs of other neurons. Thus, a neuron \\ncan input data from examples or transmit the results of other neurons, depending \\non its location in the neural network.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_retriever.invoke(\"Give a detailed explanation of neural networks. Give examples when possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'pk': 451348365454084255}, page_content='## Summary of \"Artificial Intelligence for Dummies\", \"Data Science from Scratch\", and \"Python for Data Science for Dummies\"\\n\\nThis summary focuses on the sections provided, which primarily deal with neural networks and deep learning. \\n\\n**\"Artificial Intelligence for Dummies\"**\\n\\n* **Chapter 11: Improving AI with Deep Learning:** This chapter introduces the concept of neural networks, drawing parallels to the human brain\\'s structure and function. It explains the basic components of a neural network, including neurons, activation functions, and layers. \\n* **Key Concepts:**\\n    * **Neural Networks:** Inspired by the human brain, these networks consist of interconnected neurons that process information through layers.\\n    * **Activation Functions:** These functions determine whether a neuron \"fires\" based on the input it receives.\\n    * **Backpropagation:** This algorithm allows neural networks to learn by adjusting weights based on errors in their predictions.\\n    * **Deep Learning:** This is a type of neural network with multiple hidden layers, enabling it to learn complex patterns from large datasets.\\n    * **Vanishing Gradient:** This problem arises in deep networks where signals can become too small to propagate through layers, hindering learning.\\n    * **GPUs:** These specialized processors are crucial for training deep networks due to their ability to handle complex calculations.\\n\\n**\"Data Science from Scratch\"**\\n\\n* **Chapter 18: Neural Networks:** This chapter provides a more practical approach to neural networks, focusing on their implementation in Python. It covers the basic building blocks of neural networks, including perceptrons and feed-forward networks.\\n* **Key Concepts:**\\n    * **Perceptron:** A simple neural network that simulates a single neuron with binary inputs.\\n    * **Feed-Forward Networks:** These networks consist of multiple layers of neurons, with information flowing in one direction.\\n    * **Sigmoid Function:** A smooth approximation of the step function used in neural networks to activate neurons.\\n\\n**\"Python for Data Science for Dummies\"**\\n\\n* **No specific sections on neural networks or deep learning are provided.**\\n\\n**Overall Summary:**\\n\\nThe provided sections offer a basic introduction to neural networks and deep learning, emphasizing their biological inspiration and practical applications. They explain the core concepts, including neuron structure, activation functions, backpropagation, and the challenges of training deep networks. The \"Artificial Intelligence for Dummies\" chapter provides a more conceptual overview, while the \"Data Science from Scratch\" chapter focuses on practical implementation in Python. \\n\\n**Note:** This summary is based on the provided excerpts and may not cover all aspects of the books. For a comprehensive understanding, it is recommended to read the full books. \\n'),\n",
       " Document(metadata={'pk': 451348365454083086}, page_content='182      PART 3  Working with Software-Based AI Applicationsapplies, also renders nonlinear the resulting recombination of the inputs received \\nby the connections. Both of these neural network components enable the algo -\\nrithm to learn complex target functions that represent the relationship between \\nthe input features and the target outcome.\\nUnderstanding the role of backpropagation\\nLearning occurs in a human brain because of the formation and modification of \\nsynapses between neurons, based on stimuli received by trial-and-error experi-\\nence. Neural networks provide a way to replicate this process as a mathematical \\nformulation called backpropagation. Here’s how this architecture of interconnected \\ncomputing units can solve problems: The units receive an example, and if they \\ndon’t guess correctly, they retrace the problem in the system of existing weights \\nusing backpropagation and fix it by changing some values. This process goes on \\nfor many iterations before a neural network can learn. Iterations in a neural net -\\nwork are called epochs, a name that fits perfectly because a neural network may \\nneed days or weeks of training to learn complex tasks.\\nBackpropagation math is quite advanced and requires knowledge of concepts such \\nas derivatives. You can read a detailed but accessible math description in Machine \\nLearning For Dummies,  2nd Edition, by John Paul Mueller and Luca Massaron \\n(Wiley) and get an overview of the necessary calculations. Backpropagation as a \\nconcept is intuitive enough to grasp and convey because it resembles what people \\ndo when performing a task using iterated approximate trial and error. Since the \\nappearance of the backpropagation algorithm in the 1970s, developers have fixed \\nit many times and are currently discussing whether to rethink it. (You can read the \\nopinion of Geoffrey Hinton, one of the coauthors of the method, at https://\\ntinyurl.com/rrea42wz .) Backpropagation is at the core of the present AI renais -\\nsance. In the past, each neural network learning process improvement resulted in \\nnew applications and a renewed interest in the technique. Also, the current deep \\nlearning revolution, which involves a revival of neural networks (abandoned at \\nthe beginning of the 1990s), resulted from key advances in the way neural net -\\nworks learn from their errors.\\nIntroducing Deep Learning\\nAfter backpropagation, the next improvement in neural networks led to deep \\nlearning. Research continued in spite of the AI winter, and neural networks over-\\ncame technical problems, such as the vanishing gradient,  which limits the '),\n",
       " Document(metadata={'pk': 451348365454083084}, page_content='180       PART 3  Working with Software-Based AI ApplicationsThe figure shows a simple neural network architecture. Note how the layers filter \\nand process information in a progressive way. This is a feed-forward input  because \\ndata feeds one direction into the network. Connections exclusively link units in \\none layer with units in the following layer (information flows from left to right). \\nNo connections exist between units in the same layer or with units outside the \\nnext layer. Moreover, the information pushes forward (from the left to the right). \\nProcessed data never returns to previous neuron layers.\\nIn more advanced neural network applications, you also have to decide on the \\nlayer types you need and the large number of parameters that will influence the \\nlayers’ behavior. Neural networks are extremely flexible, and that aspect is a  \\ndouble-edged sword: You increase the power of the machine learning tool as \\ncomplexity skyrockets.\\nUsing a neural network is like using a stratified filtering system for water: You \\npour the water from above, and the water is filtered at the bottom. The water has \\nno way to go back up; it just goes forward and straight down, and never laterally. \\nIn the same way, neural networks force data features to flow through the network \\nand mix with each other as dictated by the network’s architecture. By using the \\nbest architecture to mix features, the neural network creates newly composed \\nfeatures at every layer and helps achieve better predictions. Unfortunately, in \\nspite of the efforts of academics to discover a theoretical rule, you have no way to \\ndetermine the best architecture without empirically trying different solutions and \\ntesting whether output data helps predict your target values after flowing through \\nthe network. This need for manual configuration illustrates the no-free-lunch the -\\norem (which you can read about in Chapter\\xa010) in action. The gist of it is that an \\narchitecture that works the best on one task won’t necessarily perform success -\\nfully on other problems.\\nSometimes concepts can be understood better if directly tested in reality. Google \\noffers a Neural Network Playground ( http://playground.tensorflow.org ) in \\nwhich you can actually test how a neural network works in an intuitive manner, \\nas shown in Figure\\xa011-3. You see how the neural network builds a neural network \\nby adding or removing layers and changing kinds of activations.\\nFiguring out the secret is in the weights\\nNeural networks have different layers, with each one having its own weights. \\nWeights  represent the strength of the connection between neurons in the network. \\nWhen the weight of the connection between two layers is small, it means that the \\nnetwork dumps values flowing between them and signals that taking this route \\nisn’t likely to influence the final prediction. Likewise, a large positive or negative \\nvalue affects the values that the next layer receives, thus determining certain pre-\\ndictions. This approach is analogous to brain cells, which don’t stand alone but '),\n",
       " Document(metadata={'pk': 451348365454084356}, page_content='## Summary of \"Artificial Intelligence for Dummies\", \"Data Science from Scratch\", and \"Python for Data Science for Dummies\"\\n\\nThis summary focuses on the sections of the books related to neural networks and recurrent neural networks, as those are the topics highlighted in the provided text excerpt.\\n\\n**\"Artificial Intelligence for Dummies\"**\\n\\nThis book likely provides a high-level introduction to artificial intelligence (AI), including neural networks. It might cover:\\n\\n* **Basic concepts of AI:** What is AI, different types of AI, and its applications.\\n* **Neural networks:** A simplified explanation of how neural networks work, including concepts like neurons, layers, activation functions, and training.\\n* **Types of neural networks:**  A brief overview of different neural network architectures like feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\\n* **Applications of neural networks:** Examples of how neural networks are used in various fields like image recognition, natural language processing, and robotics.\\n\\n**\"Data Science from Scratch\"**\\n\\nThis book likely delves deeper into the implementation of neural networks, focusing on building them from scratch using Python. It might cover:\\n\\n* **Mathematical foundations:**  Explaining the mathematical concepts behind neural networks, including linear algebra, calculus, and probability.\\n* **Implementation details:**  Providing code examples and explanations for building neural networks using Python libraries like NumPy and TensorFlow.\\n* **Training and optimization:**  Discussing different training algorithms like gradient descent and backpropagation, as well as techniques for optimizing model performance.\\n* **Practical applications:**  Demonstrating how to apply neural networks to real-world data science problems.\\n\\n**\"Python for Data Science for Dummies\"**\\n\\nThis book likely focuses on using Python for data science tasks, including working with neural networks. It might cover:\\n\\n* **Python libraries for data science:**  Introducing popular Python libraries like Pandas, Scikit-learn, and Keras for data manipulation, analysis, and machine learning.\\n* **Neural network libraries:**  Explaining how to use libraries like Keras and TensorFlow to build and train neural networks in Python.\\n* **Preprocessing and feature engineering:**  Discussing techniques for preparing data for neural network training.\\n* **Model evaluation and selection:**  Explaining how to evaluate the performance of neural networks and choose the best model for a given task.\\n\\n**The Text Excerpt**\\n\\nThe provided text excerpt focuses on the implementation of a simple recurrent neural network (RNN) in Python. It highlights:\\n\\n* **Linear layer:**  Explaining how a linear layer, which represents the dot product of weights and inputs, is implemented.\\n* **Dropout layer:**  Describing how a dropout layer is used to prevent overfitting by randomly dropping out neurons during training.\\n* **Simple RNN:**  Presenting a basic implementation of an RNN, which uses a hidden state to store information from previous inputs.\\n* **Limitations of simple RNNs:**  Discussing the limitations of simple RNNs, such as their difficulty in learning long-range dependencies.\\n* **Character-level RNNs:**  Illustrating how RNNs can be used to generate text by learning patterns from character sequences.\\n\\n**Overall**\\n\\nThe three books provide a comprehensive introduction to AI, data science, and neural networks. \"Artificial Intelligence for Dummies\" offers a high-level overview, \"Data Science from Scratch\" focuses on implementation details, and \"Python for Data Science for Dummies\" emphasizes using Python for data science tasks, including neural networks. The text excerpt provides a practical example of implementing a simple RNN in Python, highlighting key concepts and limitations. \\n')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_retriever.invoke(\"Give a detailed explanation of neural networks. Give examples when possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics is the science of collecting, analyzing, interpreting, and presenting data. It's a powerful tool for understanding patterns, trends, and relationships within data.  \n",
      "\n",
      "For example, imagine you want to understand the average height of people in a city. You could collect data on the heights of a sample of people, calculate the average height, and use this information to estimate the average height of the entire city's population.\n",
      "\n",
      "Statistics is used in many different fields, including:\n",
      "\n",
      "* **Business:** To track sales, analyze customer behavior, and make informed decisions about marketing and product development.\n",
      "* **Healthcare:** To study the effectiveness of treatments, monitor disease outbreaks, and improve patient care.\n",
      "* **Science:** To analyze experimental data, test hypotheses, and make discoveries.\n",
      "\n",
      "Statistics is a vast field with many different techniques and methods. Some common statistical concepts include:\n",
      "\n",
      "* **Mean:** The average of a set of numbers.\n",
      "* **Standard deviation:** A measure of how spread out a set of numbers is.\n",
      "* **Correlation:** A measure of the relationship between two variables.\n",
      "* **Regression:** A statistical technique used to predict the value of one variable based on the value of another variable.\n",
      "\n",
      "By understanding statistics, we can gain valuable insights from data and make better decisions in our personal and professional lives. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 5\n",
    "response =rag_chain.invoke(\"Generate an answer which summarizes statistics in 200 words. You can use examples.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashing trick is a technique used to convert text data into numerical vectors. It involves using a hash function to map each word to a unique index within a predefined range. This allows you to represent text data as a sparse vector, where most entries are zero, and only the indices corresponding to the words present in the text are set to one.\n",
      "\n",
      "Here's a code example demonstrating the hashing trick:\n",
      "\n",
      "```python\n",
      "def hashing_trick(input_string, vector_size=20):\n",
      "    feature_vector = [0] * vector_size\n",
      "    for word in input_string.split(' '):\n",
      "        index = abs(hash(word)) % vector_size\n",
      "        feature_vector[index] = 1\n",
      "    return feature_vector\n",
      "\n",
      "input_string = 'Python for data science'\n",
      "feature_vector = hashing_trick(input_string, vector_size=20)\n",
      "print(feature_vector)\n",
      "```\n",
      "\n",
      "This code defines a function `hashing_trick` that takes an input string and a vector size as arguments. It then iterates over each word in the input string, calculates its hash value, and uses the modulo operation to map it to an index within the specified vector size. Finally, it creates a feature vector of the specified size, where the indices corresponding to the hashed words are set to 1.\n",
      "\n",
      "For example, if the input string is \"Python for data science\" and the vector size is 20, the resulting feature vector will be a list of 20 elements, with 1s at the indices corresponding to the hashed values of \"Python\", \"for\", \"data\", and \"science\", and 0s at all other indices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 5\n",
    "response =rag_chain.invoke(\"Demonstrate the hashing trick and give a specific code example\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics is the science of collecting, analyzing, interpreting, and presenting data. It's a powerful tool for understanding patterns, making predictions, and drawing conclusions from information. \n",
      "\n",
      "For example, imagine you want to know the average height of students in a school. You could collect data by measuring the height of every student. Then, you could use statistical methods to calculate the average height, which would give you a representative measure of the students' heights.\n",
      "\n",
      "Statistics is used in many different fields, including:\n",
      "\n",
      "* **Business:** To analyze sales data, track customer behavior, and make informed decisions about marketing and pricing.\n",
      "* **Healthcare:** To study the effectiveness of treatments, track disease outbreaks, and improve patient care.\n",
      "* **Science:** To analyze experimental results, test hypotheses, and make discoveries.\n",
      "\n",
      "Statistics is a vast and complex field, but even a basic understanding can be incredibly useful in everyday life. By learning how to collect, analyze, and interpret data, you can make more informed decisions and better understand the world around you. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 4\n",
    "response =rag_chain.invoke(\"Generate an answer which summarizes statistics in 200 words. You can use examples.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashing trick is a technique used to convert text data into numerical feature vectors. It involves using a hash function to map each word to a unique index within a predefined range. This allows you to represent text data as a sparse vector, where most entries are zero, and only the indices corresponding to the words present in the text are set to one.\n",
      "\n",
      "Here's a code example demonstrating the hashing trick:\n",
      "\n",
      "```python\n",
      "def hashing_trick(input_string, vector_size=20):\n",
      "    feature_vector = [0] * vector_size\n",
      "    for word in input_string.split(' '):\n",
      "        index = abs(hash(word)) % vector_size\n",
      "        feature_vector[index] = 1\n",
      "    return feature_vector\n",
      "\n",
      "input_string = 'Python for data science'\n",
      "feature_vector = hashing_trick(input_string, vector_size=20)\n",
      "print(feature_vector)  # Output: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "```\n",
      "\n",
      "In this example, the `hashing_trick` function takes an input string and a vector size as arguments. It splits the input string into words, hashes each word using the built-in `hash` function, and then calculates an index within the specified vector size using the modulo operator (`%`). The function then sets the corresponding index in the feature vector to 1.\n",
      "\n",
      "The output of the code is a sparse vector of length 20, where the indices corresponding to the words \"Python\", \"for\", \"data\", and \"science\" are set to 1, and all other indices are 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 4\n",
    "response =rag_chain.invoke(\"Demonstrate the hashing trick and give a specific code example\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are a type of machine learning algorithm inspired by the structure and function of the human brain. They consist of interconnected nodes, called neurons, organized in layers. Each neuron receives input from other neurons, processes it using a mathematical function called an activation function, and then outputs a value to other neurons.\n",
      "\n",
      "Here's a breakdown of key components:\n",
      "\n",
      "* **Neurons:** The basic building blocks of a neural network. They receive input signals, perform a weighted sum of these inputs, and apply an activation function to produce an output.\n",
      "* **Activation Functions:** These functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include sigmoid, ReLU, and tanh.\n",
      "* **Layers:** Neurons are organized into layers. The first layer is the input layer, which receives data. The last layer is the output layer, which produces the network's predictions. Hidden layers lie between the input and output layers and perform complex computations.\n",
      "* **Weights:** Each connection between neurons has a weight associated with it. These weights represent the strength of the connection and are adjusted during training to improve the network's performance.\n",
      "* **Backpropagation:** This algorithm is used to train neural networks. It calculates the error between the network's predictions and the actual values and then adjusts the weights to minimize this error.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Imagine you want to build a neural network to classify images of cats and dogs.\n",
      "\n",
      "1. **Input Layer:** The input layer would receive the image data, which could be represented as a matrix of pixel values.\n",
      "2. **Hidden Layers:** These layers would process the image data, extracting features like edges, shapes, and textures.\n",
      "3. **Output Layer:** The output layer would produce a probability score for each class (cat or dog). The class with the highest probability would be the network's prediction.\n",
      "\n",
      "**Training:**\n",
      "\n",
      "During training, the network is fed with labeled images (images with known classifications). The network adjusts its weights based on the error between its predictions and the actual labels. This process is repeated for many iterations until the network achieves a desired level of accuracy.\n",
      "\n",
      "**Types of Neural Networks:**\n",
      "\n",
      "* **Feedforward Networks:** Information flows in one direction, from the input layer to the output layer.\n",
      "* **Convolutional Neural Networks (CNNs):** Specialized for image recognition, they use convolutional filters to extract features from images.\n",
      "* **Recurrent Neural Networks (RNNs):** Designed for sequential data, they have feedback loops that allow them to remember past information.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "Neural networks are used in a wide range of applications, including:\n",
      "\n",
      "* **Image Recognition:** Identifying objects in images, such as faces, cars, and animals.\n",
      "* **Natural Language Processing:** Understanding and generating human language, such as machine translation and text summarization.\n",
      "* **Speech Recognition:** Converting spoken words into text.\n",
      "* **Robotics:** Controlling robots and enabling them to perform tasks.\n",
      "* **Financial Modeling:** Predicting stock prices and other financial indicators.\n",
      "\n",
      "Neural networks are powerful tools for solving complex problems, and their applications continue to expand as research and development progress.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 4\n",
    "response =rag_chain.invoke(\"Give a detailed explanation of neural networks. Give examples when possible.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing data is crucial because it can significantly impact the accuracy and reliability of data analysis and machine learning models.  Missing data can lead to biased results, inaccurate predictions, and flawed conclusions.\n",
      "\n",
      "Here are the methods available in Python for managing missing values:\n",
      "\n",
      "* **Finding Missing Data:** The `isnull()` method in Pandas identifies missing values in a Series or DataFrame.\n",
      "* **Encoding Missingness:**\n",
      "    * **`fillna()`:** This method fills missing values with a specified value, such as the mean, median, or a constant.\n",
      "    * **`dropna()`:** This method removes rows or columns containing missing values.\n",
      "* **Imputing Missing Data:**\n",
      "    * **`Imputer` from Scikit-learn:** This class provides various strategies for replacing missing values, including mean, median, or most frequent value.\n",
      "\n",
      "**Imputation vs. Interpolation:**\n",
      "\n",
      "* **Imputation:** Replaces missing values with estimated values based on existing data. It's suitable for handling missing values in numerical data.\n",
      "* **Interpolation:**  Estimates missing values based on the relationship between existing data points. It's often used for time series data or data with a clear trend.\n",
      "\n",
      "**When to use Imputation vs. Interpolation:**\n",
      "\n",
      "* **Imputation:** Use when the missing values are likely to be similar to the existing values in the dataset. For example, if you have a dataset of customer ages and some ages are missing, you can impute the missing ages using the mean or median age of existing customers.\n",
      "* **Interpolation:** Use when the data has a clear trend or pattern. For example, if you have a dataset of stock prices over time and some prices are missing, you can use interpolation to estimate the missing prices based on the trend of the existing prices.\n",
      "\n",
      "In summary, handling missing data is essential for accurate data analysis. Python provides various methods for managing missing values, including imputation and interpolation. The choice between these techniques depends on the nature of the data and the specific requirements of the analysis. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## k = 4\n",
    "response =rag_chain.invoke(\"Can you explain the importance of handling missing data in a dataset, and describe the different methods available in Python for managing missing values? Additionally, how do techniques like imputation and interpolation differ, and in what scenarios would you prefer one over the other?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics is a branch of mathematics that deals with the collection, analysis, interpretation, presentation, and organization of data. It helps us understand patterns, trends, and relationships within data sets. \n",
      "\n",
      "For example, imagine you want to know the average height of students in a school. You could collect data on the height of each student, then calculate the average height. This average height represents a statistical measure that summarizes the entire data set. \n",
      "\n",
      "Statistics is used in many fields, including healthcare, finance, marketing, and social sciences. It helps us make informed decisions based on data, rather than relying on intuition or guesswork. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 retriever\n",
    "response =rag_chain.invoke(\"Generate an answer which summarizes statistics in 200 words. You can use examples.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import sklearn.feature_extraction.text as txt\n",
      "sklearn_hashing_trick = txt.HashingVectorizer(\n",
      "     n_features=20, binary=True, norm=None) \n",
      "enconder = txt.CountVectorizer()\n",
      "texts = ['Python for data science',         'Python for machine learning']\n",
      "\n",
      "%timeit enconded = enconder.fit_transform(texts)\n",
      "# 1000 loops, best of 3: 1.27 ms per loop\n",
      "\n",
      "%timeit  hashing = sklearn_hashing_trick.transform(texts)\n",
      "# 10000 loops, best of 3: 158 µs per loop\n",
      "```\n",
      "\n",
      "The hashing trick is faster than one hot encoder, and it’s possible to explain the difference by noting that the latter is an optimized algorithm that keeps track of how the words are encoded, something that the hashing trick doesn’t do. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 retriever\n",
    "response =rag_chain.invoke(\"Demonstrate the hashing trick and give a specific code example\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context doesn't contain information about neural networks. Therefore, I cannot provide a detailed explanation of them. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 retriever\n",
    "response =rag_chain.invoke(\"Give a detailed explanation of neural networks. Give examples when possible.\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data can significantly skew analysis results, leading to inaccurate conclusions.  Here's why:\n",
      "\n",
      "* **Bias:** Missing data can introduce bias into your analysis, as the missing values may not be randomly distributed. This can lead to misleading results and incorrect inferences.\n",
      "* **Reduced Sample Size:** Missing data reduces the effective sample size, which can decrease the statistical power of your analysis and make it harder to detect significant relationships.\n",
      "* **Incorrect Model Fitting:** Missing data can lead to incorrect model fitting, as the model may be trained on incomplete information. This can result in poor predictions and inaccurate insights.\n",
      "\n",
      "**Methods for Handling Missing Data in Python:**\n",
      "\n",
      "1. **Deletion:**\n",
      "   * **Listwise Deletion:**  Removes entire rows with missing values. This is simple but can lead to significant data loss, especially if missing values are frequent.\n",
      "   * **Pairwise Deletion:**  Uses only complete cases for each analysis, but this can lead to different sample sizes for different analyses.\n",
      "\n",
      "2. **Imputation:**\n",
      "   * **Mean/Median/Mode Imputation:** Replaces missing values with the mean, median, or mode of the respective column. This is simple but can introduce bias if the missing values are not randomly distributed.\n",
      "   * **K-Nearest Neighbors (KNN) Imputation:**  Uses the values of the k nearest neighbors to predict the missing value. This can be more accurate than mean/median/mode imputation, but it can be computationally expensive.\n",
      "   * **Regression Imputation:** Uses a regression model to predict the missing value based on other variables in the dataset. This can be more accurate than KNN imputation, but it requires careful model selection and validation.\n",
      "\n",
      "3. **Interpolation:**\n",
      "   * **Linear Interpolation:**  Estimates missing values by drawing a straight line between the known values. This is suitable for data with a linear trend.\n",
      "   * **Spline Interpolation:**  Uses a smooth curve to estimate missing values. This is suitable for data with a non-linear trend.\n",
      "\n",
      "**Imputation vs. Interpolation:**\n",
      "\n",
      "* **Imputation:**  Replaces missing values with estimated values based on the existing data. It is suitable for handling missing values in numerical data.\n",
      "* **Interpolation:**  Estimates missing values based on the trend of the data. It is suitable for handling missing values in time series data or data with a clear trend.\n",
      "\n",
      "**Choosing the Right Method:**\n",
      "\n",
      "The best method for handling missing data depends on the specific dataset and the analysis goals. Consider the following factors:\n",
      "\n",
      "* **Nature of the Missing Data:**  Is the missing data random or systematic?\n",
      "* **Amount of Missing Data:**  How much data is missing?\n",
      "* **Type of Data:**  Is the data numerical or categorical?\n",
      "* **Analysis Goals:**  What are you trying to achieve with the analysis?\n",
      "\n",
      "For example, if you have a small amount of missing data in a numerical variable and the missing values are likely to be randomly distributed, mean/median/mode imputation might be sufficient. However, if you have a large amount of missing data in a time series dataset, interpolation might be a better choice. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BM25 retriever\n",
    "response =rag_chain.invoke(\"Can you explain the importance of handling missing data in a dataset, and describe the different methods available in Python for managing missing values? Additionally, how do techniques like imputation and interpolation differ, and in what scenarios would you prefer one over the other?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
