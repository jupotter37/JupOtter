{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)\n",
    "\n",
    "This notebook introduces basic text mining capabilities in the NLTK library and brings these capabilities together to process text data from movie reviews to build a sentiment analysis model\n",
    "\n",
    "* [Text Mining Capabilities in NLTK](#first-bullet)\n",
    "* [Sentiment Analysis with NLTK](#second-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random, re, os\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining Capabilities in NLTK <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization\n",
    "Sentence tokenizer breaks text paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#text = 'An excellent documentry. I personally remember this growing up in NYC in the early 80\\'s. This movie is for anyone that wasn\\'t around during that time period.'\n",
    "#text = 'A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.'\n",
    "text='FORBIDDEN PLANET is the best SF film from the golden age of SF cinema and what makes it a great film is its sense of wonder . As soon as the spaceship lands the audience - via the ships human crew - travels through an intelligent and sometimes terrifying adventure'\n",
    "tokenized_text=sent_tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization\n",
    "Word tokenizer breaks text paragraph into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "Stopwords are considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.  We want to remove these stopwords from our analysis\n",
    "\n",
    "To add additional stopwords to the NLTK corpus, `stopwords.append('newWord')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of default stopwords in the NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from tokenized text\n",
    "tokenized_filtered_sent=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        tokenized_filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokenized_word)\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Filterd Sentence:\",tokenized_filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Distribution\n",
    "\n",
    "Plot the distribution of the most frequently occurring words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokenized_filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Normalization\n",
    "Lexicon normalization reduces related words to a common root word.  For example, the words \"*connection*\", \"*connected*\", \"*connecting*\" are reduced to a common root word \"connect\".\n",
    "\n",
    "The two techniques for lexicon normalization are **Stemming** and **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. \n",
    "\n",
    "Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in tokenized_filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",tokenized_filtered_sent)\n",
    "print(\"-----------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "\n",
    "# the default part of speech extracted is nouns, the result could be a verb, noun, adjective, or adverb:\n",
    "\n",
    "print(\"Lemmatized Word(verb): \",lem.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "print(\"Lemmatized Word(noun): \", lem.lemmatize(word, pos=\"n\"))\n",
    "\n",
    "print(\"Lemmatized Word(adjective): \",lem.lemmatize(word, pos=\"a\"))\n",
    "\n",
    "print(\"Lemmatized Word(adverb): \", lem.lemmatize(word, pos=\"r\"))\n",
    "\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonyms\n",
    "WordNet is a lexical database for the English language, which was created by Princeton, and is part of the NLTK corpus.  You can use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the synonyms of a specific word\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"delighted\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging\n",
    "The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "Reference to interpret [POS tags](https://www.guru99.com/pos-tagging-chunking-nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with NLTK <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "In this section we will apply the NLTK text mining capabilities to extract a bag of words(BOW) from a set of labeled movie reviews, positive or negative reviews.  These BOW will serve as input features to build a model to predict the sentiments for future reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labeled movie reviews are in the pos_reviews.txt and neg_reviews.txt files. The reviews are from 25000 IMBD reviews found [here](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "<font color='red'>Action Required:</font> Add the files,  *pos_reviews.txt* and *neg_reviews.txt* files, into this project before you proceed to execute the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from project_lib import Project\n",
    "project = Project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this code to read data from the project in WSD\n",
    "pos_reviews=pd.read_csv(project.get_file('pos_reviews.txt'), sep='\\t', names=['text'], header=None)\n",
    "neg_reviews=pd.read_csv(project.get_file('neg_reviews.txt'), sep='\\t', names=['text'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the reviews to speed up the processing.  Adjust the numbers lower for speedier processing and lower accuracy, it is a tradeoff\n",
    "pos_reviews=pos_reviews[0:8000]\n",
    "neg_reviews=neg_reviews[0:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews=neg_reviews.values.tolist()\n",
    "pos_reviews=pos_reviews.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_reviews[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NLTK text mining capabilities to extact BOW from positive reviews\n",
    "\n",
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# for the scope of our analysis, we will only extract adjectives\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in  pos_reviews:\n",
    "    # create a list of tuples where the first element is a review and the second element is the label, \"pos\"\n",
    "    documents.append( (p[0], \"pos\") )\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(p[0])\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped=[]\n",
    "    for w in tokenized:\n",
    "        if w not in stop_words:\n",
    "            stopped.append(w)\n",
    "    \n",
    "    # normalize words\n",
    "    stemmed_words=[]\n",
    "    for k in stopped:\n",
    "        stemmed_words.append(ps.stem(k))\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stemmed_words)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NLTK text mining capabilities to extact BOW from negative reviews\n",
    "\n",
    "for n in neg_reviews:\n",
    "    # create a list of tuples where the first element is a review and the second element is the label, \"neg\"\n",
    "    documents.append( (n[0], \"neg\") )\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(n[0])\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped=[]\n",
    "    for w in tokenized:\n",
    "        if w not in stop_words:\n",
    "            stopped.append(w)\n",
    "            \n",
    "    # normalize words\n",
    "    stemmed_words=[]\n",
    "    for k in stopped:\n",
    "        stemmed_words.append(ps.stem(k))\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    neg = nltk.pos_tag(stemmed_words)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in neg:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "all_words.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the 500 most frequent words.  Adjust this number lower for speedier processing and lower accuracy of the model, it is a tradeoff\n",
    "word_features = list(all_words.keys())[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, create a tuple. The first element of the tuple is a dictionary where the keys are each of the 5000 words from BOW and values for each key is either True if the word appears in the review or False if the word does not. The second element is the label, tagged ‘pos’ for positive reviews and ‘neg’ for negative reviews.\n",
    "\n",
    "An example of a tuple feature set for a given review\n",
    "`({'great': True, 'bad': False, 'horrible': False}, 'pos')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the feature_set into training set and testing set.  Adjust the index based on the size of the featuresets\n",
    "training_set = featuresets[:12000]\n",
    "testing_set = featuresets[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List the most informative features**\n",
    "And the ratios associated with them shows how much more often each corresponding word appear in one class of text over others. These ratios are known as likelihood ratios. For example, the word ‘flawless’ is 16 times more likely to occur in a positive review than in a negative review.  The word 'horrid' is 12 times more likely to occur in a negative review than in a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the top 15 most informative features\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional References**:\n",
    "1. [NLTK Reference](https://www.nltk.org/book/)<br/>\n",
    "2. [NLP Tutorial using NLTK](https://likegeeks.com/nlp-tutorial-using-python-nltk/amp/#)\n",
    "3. [Basic Sentiment Analysis using NLTK](https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sidney Phoon <br/>\n",
    "**Date**: Jan 22, 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
