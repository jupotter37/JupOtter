{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/header.png\"></center>\n",
    "\n",
    "<h1><center>Алгоритмы интеллектуальной обработки больших объемов данных</center></h1>\n",
    "<hr>\n",
    "<h2><center>Деревья решений, случайные леса</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quiz Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Деревья решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Пример 1\n",
    "\n",
    "* Некоторая кoмпания открыла курс по разработке приложений на Andriod для (всех) студентов МГУ\n",
    "* Прошло уже несколько наборов\n",
    "* По результатам каждого отбора слушатели отставляют отзыв (`+1`|`-1`)\n",
    "* О слушателях известно\n",
    "    * Пол\n",
    "    * Возраст\n",
    "    * Курс\n",
    "    * Факультет\n",
    "    * Посещаемость\n",
    "    * Оценки за ДЗ\n",
    "    * ...\n",
    "    \n",
    "Компания просит у исследователя описать те группы слушателей, которым курс не понравился, чтобы возможно как-то его улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "В идеальном мире исследователь должен прийти с чем-то вроде\n",
    "\n",
    "* `[Пол = Ж][Возраст > 21][Факультет = Экономика][Посещаемость < 50%]`  - не нравится в 82% случаев\n",
    "* `[Пол = М][Возраст >= 20][Факультет = ВМК][Средний балл > 4.5]` - не нравится в 10% случаев\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Пример 2\n",
    "\n",
    "* Вы приходите в банк за кредитом ~~не дай бог~~, подаете анкету со всеми необходимыми документами\n",
    "* Сотрудник банка проверяет вашу анкету:\n",
    "    1. Если объем сбережений <= 200 тыс., то перейти к шагу 2, иначе - к шагу 3.\n",
    "    2. Если стаж больше года - дать кредит, иначе - не давать\n",
    "    3. Если продолжительность займа < 30 месяцев - не давать кредит, иначе - к шагу 4\n",
    "    4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://www.mapr.com/sites/default/files/blogimages/creditdecisiontree.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://i.stack.imgur.com/KYSy4.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='images/dec_tree_model.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='http://scikit-learn.org/stable/_static/ml_map.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Формально, дерево решений - это связный ациклический граф. В нем можно выделить 3 типа вершин:\n",
    "1. Корневая вершина (root node) -  откуда все начинается\n",
    "2. Внутренние вершины (intermediate nodes)\n",
    "3. Листья (leafs) - самые глубокие вершины дерева, в которых содержится \"ответ\"\n",
    "\n",
    "Во внутренней или коневой вершине признак проверяется на некий логический критерий, по результатам которого мы движемся все глубже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Обобщенный псевдокод алгоритма построения дерева\n",
    "\n",
    "\n",
    "```{python}\n",
    "1. function decision_tree(X, y):\n",
    "\n",
    "2.    if stopping_criterion(X, y) == True:\n",
    "    \n",
    "3.        S = create_leaf_with_prediction(y)\n",
    "        \n",
    "4.    else:\n",
    "    \n",
    "5.        S = create_node()\n",
    "6.        (X_1, y_1) .. (X_L, y_L) = best_split(X, y)\n",
    "        \n",
    "7.        for i in 1..L:\n",
    "8.            C = decision_tree(X_i, y_i)\n",
    "9.            connect_nodes(S, C)\n",
    "10.   return S     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Как определяются лучшие разбиения (best splits)?\n",
    "### Меры неопределенности (impurity measures)\n",
    "\n",
    "По какому мешку лучше классифицировать?\n",
    "\n",
    "<center><img src='images/bins.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Как определяются лучшие разбиения (best splits)?\n",
    "### Меры неопределенности (impurity measures)\n",
    "\n",
    "Пусть $p_k$ - это доля класса $C_k$ в узле дерева $S$.\n",
    "\n",
    "1. Missclassification error  \n",
    "$$I(S) = 1 - \\max\\limits_k p_k $$\n",
    "2. Gini index \n",
    "$$I(S) = 1 - \\sum\\limits_k (p_k)^2 = \\sum\\limits_{k'\\neq k} p_{k'} p_k$$\n",
    "3. Entropy \n",
    "$$I(S) = -\\sum\\limits_k p_k \\log(p_k)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def plot_impurities():\n",
    "    p = np.linspace(0, 1, 100)\n",
    "    p = np.c_[p, 1-p]\n",
    "\n",
    "    missclass = 1 - p.max(axis=1)\n",
    "    plt.plot(p[:,0], missclass, label = 'missclassification error')\n",
    "\n",
    "    gini = 1 - (p ** 2).sum(axis=1)\n",
    "    plt.plot(p[:,0], gini, label = 'gini index')\n",
    "\n",
    "    entropy = - np.nansum((p*np.log2(p)), axis=1)\n",
    "    plt.plot(p[:,0], entropy, label = 'entropy')\n",
    "\n",
    "    plt.xlabel('$p_k$')\n",
    "    plt.ylabel('$I(S)$')\n",
    "    # plt.legend(loc=2, bbox_to_anchor=(0.,0.))\n",
    "    plt.legend(loc=2, bbox_to_anchor=(-0.3,1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_impurities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Как определяются лучшие разбиения (best splits)?\n",
    "### Прирост информации\n",
    "\n",
    "Выберем признак $A$ и пороговое значение $t$ на нем таким образом, чтобы уменьшить неопределенность:\n",
    "\n",
    "**Насколько уменьшится неопределенность:** <br/>\n",
    "$$ Gain(S, A) = I(S) - \\left(\\frac{|S_L|}{|S|}\\cdot I(S_L) + \\frac{|S_R|}{|S|}\\cdot I(S_R) \\right),$$ где $S_R$ и $S_L$ - это потомки узла $S$ c объектами, удовлетворяющим соответствующим условиям.\n",
    "\n",
    "* Стратегия выбора - жадная\n",
    "* Как определяется порог при вещественных признаках?\n",
    "* Что если категорий у признака много? [Breiman, et al](https://www.amazon.co.uk/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)\n",
    "* Локальная оптимизация - уменьшение Impurity внутри узла\n",
    "* Результаты не сильно зависят от выбора самой меры неопределенности\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def wine_demo():\n",
    "\n",
    "    df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    df_wine.loc[:, 'quality_cat'] = (df_wine.loc[:, 'quality'] > 5).astype(int) \n",
    "    idx = df_wine.loc[:, 'quality_cat'] == 1\n",
    "    df_wine.loc[idx, 'alcohol'].hist(label='good quality', bins=20, alpha = 0.4, ax=ax[0])\n",
    "    df_wine.loc[~idx, 'alcohol'].hist(label='bad quality', bins=20, alpha = 0.4, ax=ax[0])\n",
    "    ax[0].set_xlabel('alcohol')\n",
    "\n",
    "    p = np.array([df_wine.quality_cat.mean(), 1-df_wine.quality_cat.mean()])\n",
    "\n",
    "    init_impurity = impurity(p)\n",
    "\n",
    "    G = []\n",
    "    t_range = np.linspace(df_wine.alcohol.min(), df_wine.alcohol.max(), 100)\n",
    "\n",
    "    for t in t_range:\n",
    "        idx = df_wine.alcohol < t\n",
    "        p1 = np.array([df_wine.loc[idx, 'quality_cat'].mean(), 1-df_wine.loc[idx, 'quality_cat'].mean()])\n",
    "        p2 = np.array([df_wine.loc[~idx, 'quality_cat'].mean(), 1-df_wine.loc[~idx, 'quality_cat'].mean()])\n",
    "\n",
    "        G.append(init_impurity - (idx.mean()*impurity(p1) + (1-idx.mean())*impurity(p2)))\n",
    "\n",
    "    ax[1].plot(t_range, G)\n",
    "    ax[1].set_xlabel('alcohol')\n",
    "    ax[1].set_ylabel('Gain')\n",
    "\n",
    "    mG = np.nanmax(G)\n",
    "    mt = t_range[np.nanargmax(G)]\n",
    "\n",
    "    ax[0].vlines(mt, 0, 150, label='best threshold (%.2f)' % mt)\n",
    "    ax[1].vlines(mt, 0, mG, label='best threshold\\n(gain = %.4f)' % mG)\n",
    "    \n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def impurity(p): \n",
    "    # p - массив из долей каждого из классов\n",
    "    # Имплементируйте любую меру неопределенности\n",
    "    return 1 - (p**2).sum()\n",
    "#     return 1 - np.max(p)\n",
    "#     return -np.sum(p*np.log2(p))\n",
    "wine_demo() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from ipywidgets import interact, IntSlider\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def demo_dec_tree(depth=1):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    C = np.array([[0., -0.7], [1.5, 0.7]])\n",
    "    gauss1 = np.dot(np.random.randn(200, 2) + np.array([4, 2]), C)\n",
    "    gauss2 = np.dot(np.random.randn(300, 2), C)\n",
    "\n",
    "    X = np.vstack([gauss1, gauss2])\n",
    "    y = np.r_[np.ones(200), np.zeros(300)]\n",
    "\n",
    "    ax[1].scatter(X[:,0], X[:, 1], c=y)\n",
    "    ax[1].set_xlabel('$x_1$')\n",
    "    ax[1].set_ylabel('$x_2$')\n",
    "\n",
    "    # Dec Tree Stuff\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=123)\n",
    "    tree.fit(X,y)\n",
    "\n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "\n",
    "    Y = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    Y = Y.reshape(xx1.shape)\n",
    "\n",
    "    ax[1].contourf(xx1, xx2, Y, alpha=0.3)\n",
    "    ax[1].scatter(X[:,0], X[:,1],c=y)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with open('tree.dot', 'w') as fout:\n",
    "            export_graphviz(tree, out_file=fout, feature_names=['x1', 'x2'], class_names=['0', '1'])\n",
    "        command = [\"dot\", \"-Tpng\", \"tree.dot\", \"-o\", \"tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[0].imshow(plt.imread('tree.png'))\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        print('Скорее всего не установлен graphviz')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = interact(demo_dec_tree, depth=IntSlider(min=1, max=5, value=1))\n",
    "except:\n",
    "    print('Что-то не так. Посмотрите на доску')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Критерии останова (регуляризация)\n",
    "\n",
    "* Никогда\n",
    "* Задать порог по мере неопределенности: $I(S) \\leq \\theta$\n",
    "* Задать порог по размеру узла: $|S| \\leq n$\n",
    "* Задать порог на глубину: $Depth(S) = d$\n",
    "* Задать порог на размер потомков: $|S_L| \\leq n_1 \\& |S_R| \\leq n_2$\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Регрессия\n",
    "\n",
    "Для задачи регрессии в качестве меры неопределенности могут выступать\n",
    "\n",
    "* Среднее квадратичное отклонение от среднего\n",
    "$$ I(S) = \\frac{1}{|S|}\\sum\\limits_{i \\in S}(y_i - \\bar{y_S})^2 $$\n",
    "* Среднее абсолютное отклонение от медианы\n",
    "$$ I(S) = \\frac{1}{|S|}\\sum\\limits_{i \\in S}|y_i - \\bar{y_S}| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "x_true = np.arange(-5, 5, 0.2)\n",
    "x = x_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "y_true = np.sin(x_true)+x_true/3\n",
    "y = y_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "\n",
    "def plot_dec_reg(depth=1, criterion='mse', ):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figheight(5)\n",
    "    \n",
    "    tree = DecisionTreeRegressor(criterion=criterion, max_depth=depth)\n",
    "    tree.fit(x.reshape(-1,1), y)\n",
    "    y_hat = tree.predict(x_true.reshape(-1,1))\n",
    "    \n",
    "    ax[1].plot(x_true, y_true, c='g', label='$f(x)$')\n",
    "    ax[1].scatter(x, y, label='actual data')\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel('y')\n",
    "    ax[1].plot(x_true, y_hat, c='r', label='decision tree \\nregression')\n",
    "    ax[1].legend(loc=2)\n",
    "    \n",
    "    try:\n",
    "        with open('tree.dot', 'w') as fout:\n",
    "            export_graphviz(tree, out_file=fout, feature_names=['x1'], class_names=['0', '1'])\n",
    "        command = [\"dot\", \"-Tpng\", \"tree.dot\", \"-o\", \"tree.png\"]\n",
    "        subprocess.check_call(command)\n",
    "        ax[0].imshow(plt.imread('tree.png'))\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        print('Скорее всего не установлен graphviz')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = interact(plot_dec_reg, depth=IntSlider(min=1, max=5, value=1), criterion=['mse', 'mae'])\n",
    "except:\n",
    "    print('Что-то не так. Посмотрите на доску')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Как определяется ответ?\n",
    "\n",
    "* Классификация\n",
    "    * Класс с большинством в листе\n",
    "    * Доли каждого из классов в листе\n",
    "* Регрессия\n",
    "    * Среднее (медиана) целевой переменной в листе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Важность признаков\n",
    "\n",
    "В деревьях решений производится автоматический отбор признаков.\n",
    "\n",
    "Пусть $v(S)$ - это признак, который использовался для ветвления в узле $S$\n",
    "\n",
    "$$ \\text{imp}(A) = \\sum\\limits_{i: v(S_i) = A} \\frac{|S_i|}{|S|} Gain(S_i, A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Работа с пропусками\n",
    "\n",
    "1. Удалить объекты\\признаки с пропусками\n",
    "2. Пропущенное значение = отдельная категория\n",
    "3. Вычисление impurity без учета пропуска\n",
    "4. Surrogate split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Вычислительная сложность\n",
    "\n",
    "#### Обучение\n",
    "* Расчет мер неопределенности для $n$ объектов с $d$ признаками на одном уровне:\n",
    "    * $O(dn)$\n",
    "* Надо делать на каждом уровне дерева. Глубина дерева в сбалансированном случае случае - $\\log{(n)}$\n",
    "    * $O(dn \\log{(n)})$\n",
    "    \n",
    "#### Применение\n",
    "* В худшем случае $O(n)$\n",
    "* В сбалансированном случае $O(\\log{(n)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Специальные алгоритмы построения деревьев\n",
    " \n",
    " \n",
    "** ID 3 **\n",
    "* Только категориальные признаки\n",
    "* Количество потомков = количеству значений признака\n",
    "* Строится до максимальной глубины\n",
    "\n",
    "** С 4.5 **\n",
    "* Поддержка вещественных признаков\n",
    "* Категриальные как в ID3\n",
    "* При пропуске значения переход по всем потомкам\n",
    "* Удаляет избыточные ветвления\n",
    "\n",
    "** СART **\n",
    "* В основном сегодняшнее занятие про него\n",
    "* Специальная процедура усещения дерева после построения (post prunning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Преимущества / Недостатки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** Преимущества **\n",
    "* Простота построения\n",
    "* Интерпретируемость (при небольшой глубине)\n",
    "* Требуются минимальная предобработка признаков\n",
    "* Встроенный отбор признаков\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Недостатки **\n",
    "* Границы строяется только параллельно или перпендикулярно осям признаков\n",
    "* При изменении набора данных надо полностью перестраивать и результат может получится совершенно иным\n",
    "* Жадность построения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Случайный лес (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Дерево решений очень чувствительно к данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "def demo_2dec_tree():\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    X, y = make_moons(noise=0.3, random_state=123)\n",
    "    \n",
    "    # Dec Tree Stuff\n",
    "    for i in range(2):\n",
    "        idx = np.random.randint(0, X.shape[0], int(0.9*X.shape[0]))\n",
    "\n",
    "        X1 = X[idx, :]\n",
    "        y1 = y[idx]\n",
    "\n",
    "        ax[i].scatter(X1[:,0], X1[:, 1], c=y1)\n",
    "        ax[i].set_xlabel('$x_1$')\n",
    "        ax[i].set_ylabel('$x_2$')\n",
    "\n",
    "        # Dec Tree Stuff 2\n",
    "        tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=123)\n",
    "        tree.fit(X1,y1)\n",
    "\n",
    "        x_range = np.linspace(X.min(), X.max(), 100)\n",
    "        xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "\n",
    "        Y = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        Y = Y.reshape(xx1.shape)\n",
    "\n",
    "        ax[i].contourf(xx1, xx2, Y, alpha=0.3)\n",
    "        ax[i].scatter(X[:,0], X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demo_2dec_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bagging - это параллельный способ построения ансамбля.<br/>\n",
    "1. Обучающая выборка сэмплируется $k$ раз с помощью *bootstrap'a* (выборка с возвратом)\n",
    "2. На каждом сэмпле обучается отдельная **базовая модель**\n",
    "3. Ответы моделей усредняются (возможно с весом)\n",
    "<center><img src='https://api.ning.com/files/PljwbnoaWerfeDW8ck54xrq1d3AVIf518M0tLFD08F0oFg76*kz1qP9-Ndi1qXF50EfGAFDvcHOILJgdHPBRZzUedJMQmYe-/Capture.PNG' width='750'></center>\n",
    "\n",
    "*Вопрос: Какая доля объектов в среднем попадает в один bootstrap сэмпл?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Почему усреднение работает?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Какая высота у Эйфелевой башни?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Кто здесь леопард?\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\"><img width=400 src='images/cat1.jpg'></th>\n",
    "    <th class=\"tg-031e\"><img width=500 src='images/cat2.jpg'></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_demo(n_est=5):\n",
    "    rf = RandomForestClassifier(random_state=123, n_estimators=n_est)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    X, y = make_moons(noise=0.3, random_state=123)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "    \n",
    "    \n",
    "    for tree in rf.estimators_:\n",
    "        y_hat = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        y_hat = y_hat.reshape(xx1.shape)\n",
    "\n",
    "        plt.contourf(xx1, xx2, y_hat, alpha=1.0/n_est)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.title('N estimators = %d' % n_est)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Так же есть некоторые обобщения бэггинга:\n",
    "\n",
    "* Метод случайных подпространств - на шаге 1. сэмплируются не только объекты, но и подпространство признаков\n",
    "* Метод случайного леса - на каждом узле сэмплируется подпространство признаков\n",
    "\n",
    "В данном случае, на каждом сэмпле базовой моделью является дерево решений.<br/>\n",
    "Если вам нужно за минимальное время построить достаточно точную и устойчивую модель - это ваш вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = interact(rf_demo, n_est=IntSlider(min=1, max=101, value=1, step=5))\n",
    "except:\n",
    "    print('Что-то не так. Посмотрите на доску')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Полезные ссылки\n",
    "* [Как работают деревья \"на пальцах\"](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "* [Про визуализацию деревьев решений](http://explained.ai/decision-tree-viz/index.html)\n",
    "* [Гайд про энтропию](http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx)\n",
    "* [Гайд про энтропию 2](http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Доп. Материалы\n",
    "* Mohammed J. Zaki. Data Mining and Analysis: Fundamental Concepts and Algorithms. Ch19\n",
    "* Trevor Hastie, et al. The Elements of Statistical Learning. Ch9.2\n",
    "* [Random Forests Home](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вопросы?\n",
    "\n",
    "## Пожалуйста, оставьте отзыв о лекции"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave",
   "width": "1280px"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "toc_position": {
   "height": "973px",
   "left": "0px",
   "right": "1708px",
   "top": "109px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
