{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from math import log2\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy import spatial\n",
    "from scipy import sparse as sp\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy import sparse\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import time\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Performance metrics** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generalized_average(U, V, average_method):\n",
    "    \"\"\"Return a particular mean of two numbers.\"\"\"\n",
    "    if average_method == \"min\":\n",
    "        return min(U, V)\n",
    "    elif average_method == \"geometric\":\n",
    "        return np.sqrt(U * V)\n",
    "    elif average_method == \"arithmetic\":\n",
    "        return np.mean([U, V])\n",
    "    elif average_method == \"max\":\n",
    "        return max(U, V)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"'average_method' must be 'min', 'geometric', 'arithmetic', or 'max'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def contingency_matrix(\n",
    "    labels_true, labels_pred, *, eps=None, sparse=False, dtype=np.int64\n",
    "):\n",
    "  \n",
    "\n",
    "    if eps is not None and sparse:\n",
    "        raise ValueError(\"Cannot set 'eps' when sparse=True\")\n",
    "\n",
    "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "    n_classes = classes.shape[0]\n",
    "    n_clusters = clusters.shape[0]\n",
    "    # Using coo_matrix to accelerate simple histogram calculation,\n",
    "    # i.e. bins are consecutive integers\n",
    "    # Currently, coo_matrix is faster than histogram2d for simple cases\n",
    "    contingency = sp.coo_matrix(\n",
    "        (np.ones(class_idx.shape[0]), (class_idx, cluster_idx)),\n",
    "        shape=(n_classes, n_clusters),\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    if sparse:\n",
    "        contingency = contingency.tocsr()\n",
    "        contingency.sum_duplicates()\n",
    "    else:\n",
    "        contingency = contingency.toarray()\n",
    "        if eps is not None:\n",
    "            # don't use += as contingency is integer\n",
    "            contingency = contingency + eps\n",
    "    return contingency\n",
    "\n",
    "\n",
    "# clustering measures\n",
    "\n",
    "\n",
    "def pair_confusion_matrix(labels_true, labels_pred):\n",
    "   \n",
    "    #labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = np.int64(labels_true.shape[0])\n",
    "\n",
    "    # Computation using the contingency data\n",
    "    contingency = contingency_matrix(\n",
    "        labels_true, labels_pred, sparse=True, dtype=np.int64\n",
    "    )\n",
    "    n_c = np.ravel(contingency.sum(axis=1))\n",
    "    n_k = np.ravel(contingency.sum(axis=0))\n",
    "    sum_squares = (contingency.data**2).sum()\n",
    "    C = np.empty((2, 2), dtype=np.int64)\n",
    "    C[1, 1] = sum_squares - n_samples\n",
    "    C[0, 1] = contingency.dot(n_k).sum() - sum_squares\n",
    "    C[1, 0] = contingency.transpose().dot(n_c).sum() - sum_squares\n",
    "    C[0, 0] = n_samples**2 - C[0, 1] - C[1, 0] - sum_squares\n",
    "    return C\n",
    "\n",
    "\n",
    "def rand_score(labels_true, labels_pred):\n",
    "  \n",
    "    contingency = pair_confusion_matrix(labels_true, labels_pred)\n",
    "    numerator = contingency.diagonal().sum()\n",
    "    denominator = contingency.sum()\n",
    "\n",
    "    if numerator == denominator or denominator == 0:\n",
    "        # Special limit cases: no clustering since the data is not split;\n",
    "        # or trivial clustering where each document is assigned a unique\n",
    "        # cluster. These are perfect matches hence return 1.0.\n",
    "        return 1.0\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def adjusted_rand_score(labels_true, labels_pred):\n",
    "    \n",
    "    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
    "    # convert to Python integer types, to avoid overflow or underflow\n",
    "    tn, fp, fn, tp = int(tn), int(fp), int(fn), int(tp)\n",
    "\n",
    "    # Special cases: empty data or full agreement\n",
    "    if fn == 0 and fp == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return 2.0 * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) + (tp + fp) * (fp + tn))\n",
    "\n",
    "\n",
    "def mutual_info_score(labels_true, labels_pred, *, contingency=None):\n",
    "  \n",
    "\n",
    "    if isinstance(contingency, np.ndarray):\n",
    "        # For an array\n",
    "        nzx, nzy = np.nonzero(contingency)\n",
    "        nz_val = contingency[nzx, nzy]\n",
    "    elif sp.issparse(contingency):\n",
    "        # For a sparse matrix\n",
    "        nzx, nzy, nz_val = sp.find(contingency)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type for 'contingency': %s\" % type(contingency))\n",
    "\n",
    "    contingency_sum = contingency.sum()\n",
    "    pi = np.ravel(contingency.sum(axis=1))\n",
    "    pj = np.ravel(contingency.sum(axis=0))\n",
    "\n",
    "    # Since MI <= min(H(X), H(Y)), any labelling with zero entropy, i.e. containing a\n",
    "    # single cluster, implies MI = 0\n",
    "    if pi.size == 1 or pj.size == 1:\n",
    "        return 0.0\n",
    "\n",
    "    log2_contingency_nm = np.log2(nz_val)\n",
    "    contingency_nm = nz_val / contingency_sum\n",
    "    # Don't need to calculate the full outer product, just for non-zeroes\n",
    "    outer = pi.take(nzx).astype(np.int64, copy=False) * pj.take(nzy).astype(\n",
    "        np.int64, copy=False\n",
    "    )\n",
    "    log2_outer = -np.log2(outer) + log2(pi.sum()) + log2(pj.sum())\n",
    "    mi = (\n",
    "        contingency_nm * (log2_contingency_nm - log2(contingency_sum))\n",
    "        + contingency_nm * log2_outer\n",
    "    )\n",
    "    mi = np.where(np.abs(mi) < np.finfo(mi.dtype).eps, 0.0, mi)\n",
    "    return np.clip(mi.sum(), 0.0, None)\n",
    "\n",
    "\n",
    "def adjusted_mutual_info_score(\n",
    "    labels_true, labels_pred, *, average_method=\"arithmetic\"\n",
    "):\n",
    "   \n",
    "    #labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = labels_true.shape[0]\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "\n",
    "    # Special limit cases: no clustering since the data is not split.\n",
    "    # It corresponds to both labellings having zero entropy.\n",
    "    # This is a perfect match hence return 1.0.\n",
    "    if (\n",
    "        classes.shape[0] == clusters.shape[0] == 1\n",
    "        or classes.shape[0] == clusters.shape[0] == 0\n",
    "    ):\n",
    "        return 1.0\n",
    "\n",
    "    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n",
    "    contingency = contingency.astype(np.float64, copy=False)\n",
    "    # Calculate the MI for the two clusterings\n",
    "    mi = mutual_info_score(labels_true, labels_pred, contingency=contingency)\n",
    "    # Calculate the expected value for the mutual information\n",
    "    emi = expected_mutual_information(contingency, n_samples)\n",
    "    # Calculate entropy for each labeling\n",
    "    h_true, h_pred = entropy(labels_true), entropy(labels_pred)\n",
    "    normalizer = _generalized_average(h_true, h_pred, average_method)\n",
    "    denominator = normalizer - emi\n",
    "    # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.\n",
    "    # normalizer should always be >= emi, but because of floating-point\n",
    "    # representation, sometimes emi is slightly larger. Correct this\n",
    "    # by preserving the sign.\n",
    "    if denominator < 0:\n",
    "        denominator = min(denominator, -np.finfo(\"float64\").eps)\n",
    "    else:\n",
    "        denominator = max(denominator, np.finfo(\"float64\").eps)\n",
    "    ami = (mi - emi) / denominator\n",
    "    return ami\n",
    "\n",
    "\n",
    "def normalized_mutual_info_score(\n",
    "    labels_true, labels_pred, *, average_method=\"arithmetic\"\n",
    "):\n",
    "  \n",
    "    #labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    classes = np.unique(labels_true)\n",
    "    clusters = np.unique(labels_pred)\n",
    "\n",
    "    # Special limit cases: no clustering since the data is not split.\n",
    "    # It corresponds to both labellings having zero entropy.\n",
    "    # This is a perfect match hence return 1.0.\n",
    "    if (\n",
    "        classes.shape[0] == clusters.shape[0] == 1\n",
    "        or classes.shape[0] == clusters.shape[0] == 0\n",
    "    ):\n",
    "        return 1.0\n",
    "\n",
    "    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n",
    "    contingency = contingency.astype(np.float64, copy=False)\n",
    "    # Calculate the MI for the two clusterings\n",
    "    mi = mutual_info_score(labels_true, labels_pred, contingency=contingency)\n",
    "\n",
    "    # At this point mi = 0 can't be a perfect match (the special case of a single\n",
    "    # cluster has been dealt with before). Hence, if mi = 0, the nmi must be 0 whatever\n",
    "    # the normalization.\n",
    "    if mi == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate entropy for each labeling\n",
    "    h_true, h_pred = entropy(labels_true), entropy(labels_pred)\n",
    "\n",
    "    normalizer = _generalized_average(h_true, h_pred, average_method)\n",
    "    return mi / normalizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def entropy(labels):\n",
    " \n",
    "    if len(labels) == 0:\n",
    "        return 1.0\n",
    "    label_idx = np.unique(labels, return_inverse=True)[1]\n",
    "    pi = np.bincount(label_idx).astype(np.float64)\n",
    "    pi = pi[pi > 0]\n",
    "\n",
    "    # single cluster => zero entropy\n",
    "    if pi.size == 1:\n",
    "        return 0.0\n",
    "\n",
    "    pi_sum = np.sum(pi)\n",
    "    # log2(a / b) should be calculated as log2(a) - log2(b) for\n",
    "    # possible loss of precision\n",
    "    return -np.sum((pi / pi_sum) * (np.log2(pi) - log2(pi_sum)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Privacy Analysis using DP** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.005, 0.01, 0.015, 0.02, 0.025, 0.030]\n",
    "\n",
    "delta1 = 0.0005\n",
    "epsilon_del1 = [(2*delta1)/lambd for lambd in lambdas]\n",
    "\n",
    "delta2 = 0.001\n",
    "epsilon_del2 = [(2*delta2)/lambd for lambd in lambdas]\n",
    "\n",
    "delta3 = 0.005\n",
    "epsilon_del3 = [(2*delta3)/lambd for lambd in lambdas]\n",
    "\n",
    "fig = plt.figure(figsize=(6,6)) \n",
    "default_x_ticks = lambdas\n",
    "plt.plot(default_x_ticks, epsilon_del1 , 'o--', linewidth=2)\n",
    "plt.plot(default_x_ticks, epsilon_del2, 'd--', linewidth=2)\n",
    "plt.plot(default_x_ticks, epsilon_del3, 's--', linewidth=2)\n",
    "\n",
    "#customization\n",
    "\n",
    "#plt.xticks(default_x_ticks, [0,10,30,50,70])\n",
    "plt.xlabel('λ', fontsize = 14)\n",
    "plt.ylabel('ε', fontsize = 14)\n",
    "plt.legend(title='Datasets', title_fontsize = 13, labels=['δ = 0.0005', 'δ = 0.001', 'δ = 0.005'])\n",
    "#plt.savefig(\"C:/Users/nimes/OneDrive/Desktop/noise_f-score.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Privacy Preserving Framework** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for constructing Incomplete Distance Matrix for PPDA\n",
    "\n",
    "def dist_approx(X_na,X_a):\n",
    "\n",
    "  DA = cdist(X_a,X_a, metric='euclidean')                   #anchor to anchor distance\n",
    "  DNA = cdist(X_na,X_a, metric='euclidean')                 #non-anchor to anchor distance \n",
    "  \n",
    "  #for k in attackers:\n",
    "      #DNA[k] += np.full(DNA[k].shape, 2)\n",
    "#       DNA[k] += 0.3*np.random.uniform(0,1,DNA[k].shape)\n",
    "  #Distances with uniform random noise\n",
    "  #DNA = cdist(X_na,X_a, metric='euclidean') + 0.1*np.random.uniform(0,1,(X_na.shape[0], X_a.shape[0]))\n",
    "\n",
    "#   #Distance with LDP Noise\n",
    "#   delta = 0.2\n",
    "#   for i in range(DNA.shape[0]):\n",
    "#     DNA[i] = DNA[i] * delta / (np.linalg.norm(DNA[i]))\n",
    "#     DNA[i] += np.random.laplace(loc = 1, scale = 0.05, size = DNA[i].shape)\n",
    "#   for i in range(DA.shape[0]):\n",
    "#     DA[i] = DA[i]*delta / (np.linalg.norm(DA[i]))\n",
    "  zero_mat = np.eye((X_na.shape[0]))\n",
    "  ones_mat = np.ones((X_a.shape[0],X_na.shape[0]))\n",
    "  ones_mat2 =  np.ones((X_a.shape[0],X_a.shape[0]))\n",
    "  D = np.array(np.vstack((np.hstack((np.zeros((X_na.shape[0],X_na.shape[0])), DNA)), np.hstack((DNA.T, DA)))))    #Incomplete distance matrix\n",
    "  \n",
    "  W_1 = np.array(np.vstack((np.hstack((zero_mat,ones_mat.T)), np.hstack((ones_mat, ones_mat2)))))          #Weight matrix\n",
    "\n",
    "\n",
    "  V = np.array(np.diag(np.matmul(W_1,np.ones(W_1.shape[0]))) - W_1)      #V matrix required for SMACOF\n",
    " \n",
    "\n",
    "  V1=V[:X_na.shape[0],:X_na.shape[0]]\n",
    "  V2=V[:X_na.shape[0],X_na.shape[0]:]\n",
    "\n",
    "  return D, V, V1, V2, W_1, DA, DNA\n",
    "\n",
    "#Function for learning embeddings through MDS\n",
    "\n",
    "def classical_MDS_X(D, V, W_1, n,d):\n",
    "\n",
    "  epsilon= 1e-3\n",
    " \n",
    "  #D_inv = np.reciprocal(D,  out = np.zeros_like(D), where=(D!=0))\n",
    "\n",
    "  #L_D_inv = np.diag(np.matmul(D_inv,np.ones(D_inv.shape[0]))) - D_inv\n",
    "  #print(L_D_inv) \n",
    "  #Zu = np.random.multivariate_normal(np.zeros(n), np.linalg.pinv(L_D_inv), 50).T\n",
    "  np.random.seed(10)  \n",
    "  Zu = np.random.normal(0,1,(n, 500))               #Initializing Embeddings\n",
    "  epochs = 2000\n",
    "  loss = []\n",
    "  V_inv = np.linalg.pinv(V)\n",
    "\n",
    "  W_new = np.multiply(W_1,D)\n",
    "  print(W_new)\n",
    "\n",
    "  D_new = cdist(Zu,Zu, metric='euclidean')\n",
    "    \n",
    "  #SMACOF implementation\n",
    "\n",
    "  for t in tqdm(range(epochs)):\n",
    "    \n",
    "   \n",
    "    W_final = np.divide(W_new,D_new, out = np.zeros_like(W_new), where=(D_new!=0))\n",
    "    B_Z = np.diag(np.matmul(W_final,np.ones(W_final.shape[0]))) - W_final \n",
    "    X_final = np.matmul(np.matmul(V_inv, B_Z), Zu)\n",
    "    D_new = cdist(X_final,X_final, metric='euclidean')\n",
    "    D_inv_new = np.reciprocal(D_new ,  out = np.zeros_like(D_new), where=(D_new!=0))\n",
    "    W_upper_triag = np.array(D_inv_new[np.triu_indices(D_inv_new.shape[0], k = 1)])\n",
    "    C = np.square(D - D_new)\n",
    "    D_upper_triag = C[np.triu_indices(C.shape[0], k = 1)]\n",
    "    stress = np.dot(W_upper_triag, D_upper_triag)\n",
    "    loss.append(stress)\n",
    "    Zu = X_final\n",
    "    if t % 10 == 0:\n",
    "       print(stress)\n",
    "    \n",
    "    if t!=0:\n",
    "      if abs(loss[t]-loss[t-1]) < epsilon:\n",
    "        break\n",
    "    \n",
    "  return X_final, loss\n",
    "\n",
    "#Function for learning embeddings through Anchored-MDS \n",
    "\n",
    "def MDS_X(D, V1, V2, W_1, DA,X_na, X_a, n,d):\n",
    "    \n",
    "  print(D)\n",
    "  D_inv = np.reciprocal(D,  out = np.zeros_like(D), where=(D!=0))\n",
    "\n",
    "  L_D_inv = np.diag(np.matmul(D_inv,np.ones(D_inv.shape[0]))) - D_inv\n",
    "\n",
    "  np.random.seed(32)\n",
    "  Zu_samples = np.random.multivariate_normal(np.zeros(X_na.shape[0] + X_a.shape[0]), np.linalg.pinv(L_D_inv),d).T\n",
    "  Zu = Zu_samples[:n,:]                    #Intializing Embeddings\n",
    "\n",
    "\n",
    "  \n",
    "  epsilon= 1e-3\n",
    "  epochs = 2000\n",
    "  loss = []\n",
    "  V1_inv = np.linalg.pinv(V1)\n",
    "\n",
    "  W_new = np.multiply(W_1,D)\n",
    "\n",
    "  DNA_new = cdist(Zu,X_a, metric='euclidean')\n",
    "  D_new = np.array(np.vstack((np.hstack((np.zeros((X_na.shape[0],X_na.shape[0])), DNA_new)), np.hstack((DNA_new.T, DA)))))\n",
    "  for t in tqdm(range(epochs)):\n",
    "   \n",
    "    W_final = np.divide(W_new,D_new, out = np.zeros_like(W_new), where=(D_new!=0))\n",
    "\n",
    "    B_Z = np.diag(np.matmul(W_final,np.ones(W_final.shape[0]))) - W_final \n",
    "\n",
    "    BZ1 = B_Z[:X_na.shape[0],:X_na.shape[0]]\n",
    "    BZ2 = B_Z[:X_na.shape[0],X_na.shape[0]:]\n",
    "\n",
    "    term1 = np.matmul(BZ1,Zu)\n",
    "    term2_temp = BZ2 - V2\n",
    "    term2 = np.matmul(term2_temp, X_a)\n",
    "\n",
    "\n",
    "    X_final = np.matmul(V1_inv,(term1 + term2))\n",
    "\n",
    "    DNA_new = cdist(X_final,X_a, metric='euclidean')   \n",
    "    D_new = np.array(np.vstack((np.hstack((np.zeros((X_na.shape[0],X_na.shape[0])), DNA_new)), np.hstack((np.transpose(DNA_new), DA)))))\n",
    "    D_inv_new = np.reciprocal(D_new ,  out = np.zeros_like(D_new), where=(D_new!=0))\n",
    "    W_upper_triag = np.array(D_inv_new[np.triu_indices(D_inv.shape[0], k = 1)])\n",
    "\n",
    "    C = np.square(D - D_new)\n",
    "    \n",
    "    D_upper_triag = C[np.triu_indices(C.shape[0], k = 1)]\n",
    "\n",
    "    \n",
    "    stress = np.dot(W_upper_triag, D_upper_triag)\n",
    "\n",
    "    loss.append(stress)\n",
    "    Zu = X_final\n",
    "    if t % 10 == 0:\n",
    "       print(stress)\n",
    "    \n",
    "    if t!=0:\n",
    "      if abs(loss[t]-loss[t-1]) < epsilon:\n",
    "        break\n",
    "    \n",
    "  return X_final, loss\n",
    "\n",
    "#Function for computing error in estimation of distances\n",
    "\n",
    "def dist_error(X_na, X_final):\n",
    "  \n",
    "  D_true = cdist(X_na, X_na, metric='euclidean')\n",
    "  z_true = spatial.distance.squareform(D_true)\n",
    "\n",
    "  D_esti = cdist(X_final, X_final, metric='euclidean')\n",
    "  z_esti = spatial.distance.squareform(D_esti) \n",
    "  Error = np.linalg.norm((D_true - D_esti), 'fro')/ np.linalg.norm((D_true), 'fro')\n",
    "  \n",
    "  return Error, D_true, D_esti, z_true, z_esti\n",
    "\n",
    "#Function for checking F-score for neighborhood structure preservation, where k is the number of nearest neighbor\n",
    "#Input (k+1)  for k-NN as we are not considering distance of node from itself.\n",
    "\n",
    "def check_score(D_true, D_approx,k):\n",
    "      f_scores = []\n",
    "      for i in range(D_true.shape[0]):\n",
    "        list1 =  np.argsort(D_true[i])\n",
    "        list2 =  np.argsort(D_approx[i])\n",
    "        newlist1 = list1[1:k]\n",
    "        newlist2 = list2[1:k]\n",
    "        count = 0\n",
    "        for p in range(k-1):\n",
    "          for q in range(k-1):\n",
    "            if(newlist1[p] == newlist2[q]):\n",
    "              count += 1\n",
    "              break;\n",
    "        \n",
    "        f_score = 2*count/(2*count + (k- 1 - count))\n",
    "        f_scores.append(f_score)\n",
    "        #print(\"Node:{}, newlist1:{}, newlist2:{}\".format(i+1, newlist1, newlist2))\n",
    "        #print(\"Relative F-score for node: {} = {}\".format(i+1, f_score))\n",
    "      avg_f_score = sum(f_scores)/len(f_scores)\n",
    "\n",
    "      return avg_f_score\n",
    "    \n",
    "#Function for checking similarity between graph structures obtained in non-private and private manner\n",
    "\n",
    "def check_F_score(A_esti, A_org):\n",
    "      temp1 = spatial.distance.squareform(A_esti)\n",
    "      temp2 = spatial.distance.squareform(A_org)\n",
    "      print(temp2)\n",
    "      print(temp1)\n",
    "      TP = 0\n",
    "      FP = 0\n",
    "      FN = 0\n",
    "      FP_elements = []\n",
    "      TP_elements = []\n",
    "      for i in range(temp1.shape[0]):\n",
    "        if(temp2[i] > 0 and temp1[i] > 0):\n",
    "          TP+=1\n",
    "          TP_elements.append(temp1[i])\n",
    "        elif(temp2[i] == 0 and temp1[i] > 0):\n",
    "          FP+=1\n",
    "          FP_elements.append(temp1[i])\n",
    "        elif(temp2[i] > 0 and temp1[i] == 0):\n",
    "          FN+=1\n",
    "        \n",
    "      print(\"TP: {}, FP: {}, FN: {}\".format(TP, FP, FN))\n",
    "      F_score = (2*TP)/(2*TP + FP + FN)\n",
    "\n",
    "      return F_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Dataset Loaders** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human Activity Recognition (Large)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/train.csv\", sep = ',')\n",
    "df['Activity'].replace(['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
    "       'WALKING_UPSTAIRS'],[1, 2,3,4,5,6], inplace=True)\n",
    "#print(df.head())\n",
    "X_na = df.to_numpy()\n",
    "Y_na = X_na[:,-1]\n",
    "X_na = X_na[:,:561]\n",
    "df_test = pd.read_csv(\"/test.csv\", sep = ',')\n",
    "df_test['Activity'].replace(['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
    "       'WALKING_UPSTAIRS'],[1, 2,3,4,5,6], inplace=True)\n",
    "X_a = df_test.to_numpy()\n",
    "X_a = X_a[:X_a.shape[1] - 1, :561]\n",
    "\n",
    "\n",
    "n = X_na.shape[0]\n",
    "d = X_na.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human Activity Recognition (Moderate)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/final_X_train.txt\", sep = ',', header = None)\n",
    "#print(df.head())\n",
    "X_na = df.to_numpy()\n",
    "df_test = pd.read_csv(\"/final_X_test.txt\", sep = ',', header = None)\n",
    "df_y_train = pd.read_csv(\"/final_y_train.txt\", sep = ',', header = None)\n",
    "df_y_test = pd.read_csv(\"/final_y_test.txt\", sep = ',', header = None)\n",
    "X_a = df_test.to_numpy()\n",
    "X_a = X_a[:X_a.shape[1] - 1]\n",
    "Y_na = df_y_train.to_numpy()\n",
    "Y_na = [item for sublist in Y_na for item in sublist]\n",
    "Y_a = df_y_test.to_numpy()\n",
    "\n",
    "n = X_na.shape[0]\n",
    "d = X_na.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PANCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv(\"/data.csv\", sep = \",\")\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "X_na = X[:,1:]\n",
    "df_label = pd.read_csv(\"/labels.csv\", sep = \",\")\n",
    "Y = df_label.to_numpy()\n",
    "Y_temp = Y[:,1:]\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y_temp)\n",
    "Y_na = le.transform(Y_temp)\n",
    "Y_na = Y_na + 1\n",
    "print(Y_na)\n",
    "\n",
    "#Y_na = np.ones((150,))\n",
    "#Y_na[50:100] = 2\n",
    "#Y_na[100:150] = 3\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"3\"> **Iris** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/bezdekIris.data.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "X_na = X[:,:4]\n",
    "Y_na = np.ones((150,))\n",
    "Y_na[50:100] = 2\n",
    "Y_na[100:150] = 3\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/glass.data.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,1:10]\n",
    "Y_na = X[:, 10]\n",
    "Y_na[Y_na == 7] = 4\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "print(X_na)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na, axis=1, ddof=1)\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/wine.data.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,1:14]\n",
    "Y_na = X[:, 0]\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na, axis=1, ddof=1)\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WDBC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/wdbc.data.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,2:32]\n",
    "Y = X[:, 1]\n",
    "#print(Y)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "la = LabelEncoder()\n",
    "labels = la.fit_transform(Y)\n",
    "Y_na = labels + 1\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype(float), axis=1, ddof=1)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Control Chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/synthetic_control.data.txt\", delim_whitespace=True, header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "X_na = X[:,:]\n",
    "c = np.array([1,2,3,4,5,6])\n",
    "Y_na = np.tile(c,(100,1))\n",
    "Y_na = Y_na.flatten('F')\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype('float'), axis=1, ddof=1)\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parkinsons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/parkinsons.data.txt\", sep = \",\")\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "cols = np.r_[1:17, 18:24]\n",
    "X_na = X[:][:,cols]\n",
    "Y_na = X[:, 17] + 1\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype('float'), axis=1, ddof=1)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vertebral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/column_3C.dat\",  delim_whitespace=True, header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,:6]\n",
    "Y = X[:,6]\n",
    "#print(Y_na)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "la = LabelEncoder()\n",
    "labels = la.fit_transform(Y)\n",
    "Y_na = labels + 1\n",
    "print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype('float'), axis=1, ddof=1)\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breast tissue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(\"/breast-tissue.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,2:11]\n",
    "Y = X[:, 1]\n",
    "#print(Y)\n",
    "\n",
    "la = LabelEncoder()\n",
    "labels = la.fit_transform(Y)\n",
    "Y_na = labels + 1\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype(float), axis=1, ddof=1)\n",
    "n_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seeds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/seeds_dataset.txt\", sep = \"\t\", header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "print(X.shape)\n",
    "X_na = X[:,:7]\n",
    "Y_na = X[:, 7]\n",
    "print(Y_na)\n",
    "print(X_na.shape)\n",
    "\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df1 = pd.read_csv(\"/segmentation.data.txt\", sep = \",\", header = None)\n",
    "df2 = pd.read_csv(\"/segmentation.test.txt\", sep = \",\", header = None)\n",
    "print(df.head())\n",
    "X1 = df1.to_numpy()\n",
    "X2 = df2.to_numpy()\n",
    "#print(X.shape)\n",
    "X = np.vstack((X1,X2))\n",
    "#print(X.shape)\n",
    "X_na = X[:,1:]\n",
    "Y = X[:, 0]\n",
    "#print(Y)\n",
    "\n",
    "la = LabelEncoder()\n",
    "labels = la.fit_transform(Y)\n",
    "Y_na = labels + 1\n",
    "#print(Y_na.shape)\n",
    "#print(X_na.shape)\n",
    "from scipy import stats\n",
    "X_na = stats.zscore(X_na.astype(float), axis=1, ddof=1)\n",
    "n_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yeast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(\"/yeast.data.txt\", delim_whitespace=True , header = None)\n",
    "print(df.head())\n",
    "X = df.to_numpy()\n",
    "#print(X.shape)\n",
    "X_na = X[:,1:9]\n",
    "Y = X[:, 9]\n",
    "#print(Y)\n",
    "\n",
    "la = LabelEncoder()\n",
    "labels = la.fit_transform(Y)\n",
    "Y_na = labels + 1\n",
    "#print(Y_na)\n",
    "print(X_na.shape)\n",
    "from scipy import stats\n",
    "#X_na = stats.zscore(X_na.astype(float), axis=1, ddof=1)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "executionInfo": {
     "elapsed": 839,
     "status": "error",
     "timestamp": 1671432582392,
     "user": {
      "displayName": "Nimesh Agrawal",
      "userId": "11423357928396443155"
     },
     "user_tz": -330
    },
    "id": "tW31Eyk_zmLn",
    "outputId": "61b3df07-67e3-44ce-ece2-5bd4e23970bd"
   },
   "outputs": [],
   "source": [
    "X_na, X_a, Y_na, Y_a = train_test_split(X_na, Y_na, test_size=0.10, random_state=42)\n",
    "\n",
    "\n",
    "n = X_na.shape[0]               #number of non-anchor nodes (clients)\n",
    "\n",
    "d = X_na.shape[1]               #dimensionality of data\n",
    "\n",
    "X_a = X_a[:(d-1), :]\n",
    "print(\"Size of reference nodes(anchors)\", X_a.shape)\n",
    "\n",
    "X = np.vstack((X_na, X_a))\n",
    "\n",
    "nodes = X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = np.random.uniform(0, 1, size = (81, d))\n",
    "#X_a = np.random.rand((d-1),d)\n",
    "X = np.vstack((X_na, X_a))\n",
    "\n",
    "nodes = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_na.shape[0]):\n",
    "    X_na[i] = X_na[i]/np.linalg.norm(X_na[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "indices = np.arange(0, X_na.shape[0]).tolist()\n",
    "num_attackers = int((0.50)*(X_na.shape[0]))\n",
    "attackers_list = random.sample(indices, num_attackers)\n",
    "print(attackers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in attackers_list:\n",
    "    X_na[k] += 0.3*np.random.uniform(0,1,X_na[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.2\n",
    "X_na_noisy = np.zeros((X_na.shape[0], X_na.shape[1]))\n",
    "for i in range(DNA.shape[0]):\n",
    "    X_na_noisy[i] = X_na[i] * delta / (np.linalg.norm(X_na[i]))\n",
    "    X_na_noisy[i] += np.random.laplace(loc = 0, scale = 0.005, size = X_na_noisy[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Using Anchored MDS:\n",
    "\n",
    "#D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'), attackers_list)\n",
    "D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'))\n",
    "X_final, loss = MDS_X(D, V1, V2, W_1, DA, X_na.astype('float'), X_a.astype('float'), n, d)\n",
    "error, D_true, D_esti, z_true, z_esti = dist_error(X_na.astype('float'), X_final)\n",
    "print(\"Error in distance approximation: \", error)\n",
    "fscore = check_score(D_true, D_esti, 11)\n",
    "print(\"F-score: \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta = 0.0005\n",
    "# for i in range(DNA.shape[0]):\n",
    "#     DNA[i] = DNA[i] * delta / (np.linalg.norm(DNA[i]))\n",
    "#     DNA[i] += np.random.laplace(loc = 0, scale = 0.005, size = DNA[i].shape)\n",
    "# for i in range(DNA.shape[0]):\n",
    "#     print(np.linalg.norm(DNA[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_na, X_a))\n",
    "\n",
    "nodes = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Using classical MDS\n",
    "\n",
    "D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'))\n",
    "X_final, loss = classical_MDS_X(D, V, W_1, nodes,d)\n",
    "error, D_true, D_esti, z_true, z_esti = dist_error(X_na.astype('float'), X_final[:X_na.shape[0], :])\n",
    "print(\"Error in distance approximation: \", error)\n",
    "fscore = check_score(D_true, D_esti, 11)\n",
    "print(\"F-score: \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating Results \n",
    "\n",
    "print(X_final.shape)\n",
    "#embed_anc = X_final[X_na.shape[0] : , :]\n",
    "embed_non_anc = X_final[:X_na.shape[0], :]\n",
    "#embed_non_anc = X_final\n",
    "print(\"Embeddings:\",embed_non_anc)\n",
    "print(\"Original data:\",X_na)\n",
    "print(\"Estimated distance:\", cdist(embed_non_anc,embed_non_anc, metric='euclidean'))\n",
    "print(\"True distance:\", D_true)\n",
    "print(\"Anchor to Anchor Distance: \", DA)\n",
    "print(\"Anchor to Non-anchor distance: \", DNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Distance plot** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1669714073794,
     "user": {
      "displayName": "Nimesh Agrawal",
      "userId": "11423357928396443155"
     },
     "user_tz": -330
    },
    "id": "mf4rTkEqTyd9",
    "outputId": "2a0c01e1-5265-4221-8583-8abab5373ac8"
   },
   "outputs": [],
   "source": [
    "# True euclidean distance (vertical axis) vs. predicted euclidean distance (horizontal axis) plot for PPDA\n",
    "fig = plt.figure(figsize=(6,6)) \n",
    "plt.scatter( z_esti,z_true, s = 5, c = 'red', alpha = 0.6)\n",
    "plt.plot( [2,4.5], [2,4.5], color='black', linestyle = '-' )\n",
    "plt.xlabel(\"Predicted distance\", fontsize = 12)\n",
    "plt.ylabel(\"Real distance\",fontsize = 12)\n",
    "#plt.savefig(\"C:/Users/nimes/OneDrive/Desktop/Project_Results/ER_n = {}_d = {}_p = {}_learnt.jpeg\".format(n,d,param))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1669701815988,
     "user": {
      "displayName": "Nimesh Agrawal",
      "userId": "11423357928396443155"
     },
     "user_tz": -330
    },
    "id": "yowQfLYQhcc5",
    "outputId": "d4ad1473-c517-4e52-8587-696602dfd8db"
   },
   "outputs": [],
   "source": [
    "#Plot of Loss vs epochs:\n",
    "fig = plt.figure(figsize=(10,10)) \n",
    "plt.plot(np.arange(0, len(loss) - 10), loss[10:] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "X_na_noisy = np.zeros((X_na.shape[0], X_na.shape[1]))\n",
    "for i in range(X_na.shape[0]):\n",
    "    X_na_noisy[i] = X_na[i] * delta / (np.linalg.norm(X_na[i]))\n",
    "    X_na_noisy[i] += np.random.laplace(loc = 0, scale = 0.01,size = X_na_noisy[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **T-SNE visualization for graph structure positioning** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = visualize(X_na_noisy,Y_na)\n",
    "z = visualize(X_na,Y_na)\n",
    "#z_private = visualize(X_final, Y_na)                                   #For Anchored MDS\n",
    "z_private = visualize(X_final[:X_na.shape[0]], Y_na)                    #For Classical MDS\n",
    "print(type(z))\n",
    "pos = {}\n",
    "for i in range(X_na.shape[0]):\n",
    "    pos[i] = z[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "AS = kneighbors_graph(X_na,10,mode='distance')\n",
    "AS_1 = kneighbors_graph(X_final,10,mode='distance')\n",
    "AS=AS.toarray()\n",
    "AS_1 = AS_1.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Graph Clustering** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Constrained Laplacian Rank (CLR) Algorithm** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse import csr_matrix\n",
    "from networkx import *\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from sklearn.cluster import KMeans\n",
    "from quadprog import solve_qp\n",
    "\n",
    "#Function for clustering graph into k-components\n",
    "\n",
    "def cluster_k_component_graph(Y, k = 1, m = 5, lmd = 100, eigtol = 1e-9, edgetol = 1e-2, maxiter = 1000):\n",
    "  A = build_initial_graph(Y, m)\n",
    "  #print(check_symmetric(A))\n",
    "  n = A.shape[0]\n",
    "  S = np.full((n, n), 1/n)\n",
    "  DS = np.diag(0.5 * np.sum((S + S.T), axis = 0))\n",
    "  LS =  DS - .5 * (S + S.T)\n",
    "  #print(LS[0])\n",
    "  DA = np.diag(0.5 * np.sum((A + A.T), axis = 0))\n",
    "  LA = DA - .5 * (A + A.T)\n",
    "  #print(LA[0])\n",
    "  if k == 1:\n",
    "    F = eigvec_sym(LA)\n",
    "    F = F[:, 0:k]\n",
    "  else:\n",
    "    F = eigvec_sym(LA)\n",
    "    F = F[:, 0:k]\n",
    "  # bounds for variables in the QP solver\n",
    "  x = np.array([1, 0])\n",
    "  bvec = np.repeat(x, [1, n], axis=0)\n",
    "  Amat = np.hstack((np.repeat(1,n, axis = 0).reshape(n,1), np.eye(n)))\n",
    "  lmd_seq = [lmd]\n",
    "\n",
    "  for t in tqdm(range(maxiter)):\n",
    "    \n",
    "    V =  cdist(F,F, metric='sqeuclidean')\n",
    "    print(V)\n",
    "    for i in range(n): \n",
    "      p = A[i,:] - .5 * lmd * V[i,:]\n",
    "      qp,_,_,_,_,_ = solve_qp(G = np.eye(n).astype('double'), a = p.astype('double'), C = Amat.astype('double'), b = bvec.astype('double'))\n",
    "      #print(qp)\n",
    "      S[i] = qp\n",
    "    \n",
    "    DS = np.diag(0.5 * np.sum((S + S.T), axis = 0))\n",
    "    LS = DS - .5 * (S + S.T)\n",
    "    F = eigvec_sym(LS)\n",
    "    F = F[:, 0:k]\n",
    "    eig_vals = eigval_sym(LS)\n",
    "    n_zero_eigenvalues = np.sum(np.abs(eig_vals) < eigtol)\n",
    "    \n",
    "    if k < n_zero_eigenvalues:\n",
    "      lmd = .5 * lmd\n",
    "    elif k > n_zero_eigenvalues:\n",
    "      lmd = 2 * lmd\n",
    "    else:\n",
    "      break\n",
    "    lmd_seq.append(lmd)\n",
    "  \n",
    "  LS[np.abs(LS) < edgetol] = 0\n",
    "  AS = np.diag(np.diag(LS)) - LS\n",
    "  #AS[np.abs(AS) < edgetol] = 0\n",
    "\n",
    "  return LS, AS, eig_vals,lmd_seq\n",
    "\n",
    "def eigvec_sym(L):\n",
    "    Lambdas, V = np.linalg.eig(L)\n",
    "   \n",
    "    ind = np.argsort(np.linalg.norm(np.reshape(Lambdas, (1, len(Lambdas))), axis=0))\n",
    "    #Lambdas = Lambdas[np.argsort(Lambdas)]\n",
    "    #print(ind)\n",
    "    #print(Lambdas)\n",
    "    #print(V[:,0])\n",
    "    V = np.real(V[:,np.argsort(Lambdas)])\n",
    "    return V\n",
    "def eigval_sym(L):\n",
    "    Lambdas, V = np.linalg.eig(L)\n",
    "    Lambdas = Lambdas[np.argsort(Lambdas)]\n",
    "    return Lambdas\n",
    "\n",
    "\n",
    "#Function for building intial graph structure\n",
    "\n",
    "def build_initial_graph(Y,m):\n",
    "    \n",
    "      n = Y.shape[0]\n",
    "      A = np.zeros((n,n))\n",
    "      E = cdist(Y,Y, metric='sqeuclidean')\n",
    "      for i in range(n):\n",
    "        sorted_index = np.argsort(E[i,:])\n",
    "        j_sweep = sorted_index[1:(m+1)]\n",
    "        den = m * E[i, sorted_index[m+1]] - np.sum(E[i, j_sweep])\n",
    "        ei = E[i, sorted_index[m+1]]\n",
    "        for j in j_sweep: \n",
    "          A[i, j] = (ei - E[i, j]) / den\n",
    "        \n",
    "      #S = 0.5*(A + A.T)\n",
    "        \n",
    "      \n",
    "      return A\n",
    "\n",
    "#Function for checking quality metrics of clustering\n",
    "\n",
    "def metrics(Y_na, assignment):\n",
    "    nmi = normalized_mutual_info_score(Y_na, assignment, average_method='max')\n",
    "    ari = adjusted_rand_score(Y_na, assignment)\n",
    "    \n",
    "    return nmi,ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LS, AS, eig_vals,lmd_seq = cluster_k_component_graph(X_na_noisy.astype('float'),6,5)\n",
    "#LS, AS, eig_vals,lmd_seq = cluster_k_component_graph(X_na.astype('float'),6,5)\n",
    "LS_1, AS_1, eig_vals_1,lmd_seq_1 = cluster_k_component_graph(X_final.astype('float'),3,7)                             #For Anchored MDS\n",
    "#LS_1, AS_1, eig_vals_1,lmd_seq_1 = cluster_k_component_graph(X_final[:X_na.shape[0]].astype('float'),1,5)              #For classical MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lmd_seq,lmd_seq_1)\n",
    "print(eig_vals, eig_vals_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(AS)\n",
    "plt.colorbar()\n",
    "print(AS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assinging colours for clusters for different number of classes\n",
    "\n",
    "print(Y_na)\n",
    "#colors = [\n",
    "#\"lightcoral\", \"gray\", \"lightgray\", \"firebrick\", \"red\", \"chocolate\", \"darkorange\", \"moccasin\", \"gold\", \"yellow\", \"darkolivegreen\", \"chartreuse\", \"forestgreen\", \"lime\", \"mediumaquamarine\", \"turquoise\", \"teal\", \"cadetblue\",\"magenta\", \"blue\"]\n",
    "\n",
    "#2 classes:\n",
    "# colors = [\"blue\", \"red\"]\n",
    "# C =  [colors[0] if i == 1 else colors[1] for i in Y_na]\n",
    "\n",
    "#3 classes:\n",
    "colors = [\"lightcoral\", \"blue\", \"red\"]\n",
    "C =  [colors[0] if i == 1 else colors[1] if i == 2 else colors[2] for i in Y_na]\n",
    "\n",
    "#5 classes:\n",
    "#colors = [\"lightcoral\", \"gray\", \"blue\", \"red\", \"chocolate\"]\n",
    "#C = [colors[0] if i == 1 else colors[1] if i == 2 else colors[2] if i == 3 else colors[3] if i == 4 else colors[4] for i in Y_na]\n",
    "\n",
    "#6 classes:\n",
    "#colors = [\"lightcoral\", \"gray\", \"blue\", \"red\", \"chocolate\", \"darkorange\"]\n",
    "#C = [colors[0] if i == 1 else colors[1] if i == 2 else colors[2] if i == 3 else colors[3] if i == 4 else colors[4] if i == 5 else colors[5] for i in Y_na]\n",
    "\n",
    "#7 classes:\n",
    "#C = [colors[0] if i == 1 else colors[1] if i == 2 else colors[2] if i == 3 else colors[3] if i == 4 else colors[4] if i == 5 else colors[5] if i == 6 else colors[6] for i in Y_na]\n",
    "\n",
    "#10 classes:\n",
    "#colors = [ \"gray\", \"blue\", \"red\", \"chocolate\", \"darkorange\",\"yellow\",\"darkolivegreen\", \"chartreuse\", \"cadetblue\",\"magenta\"]\n",
    "#C = [colors[0] if i == 1 else colors[1] if i == 2 else colors[2] if i == 3 else colors[3] if i == 4 else colors[4] if i == 5 else colors[5] if i == 6 else colors[6] if i == 7 else colors[7] if i == 8 else colors[8] if i == 9 else colors[9] for i in Y_na]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Adjacency_private_ActivityLarge', AS_1)\n",
    "np.save('Adjacency_non_private_ActivityLarge', AS)\n",
    "np.save('X_final_ActivityLarge', X_final)\n",
    "np.save('X_na_ActivityLarge', X_na)\n",
    "np.save('Y_na_ActivityLarge', Y_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_final_Iris.csv\", X_final, delimiter=\",\")\n",
    "np.savetxt(\"Adjacency_Iris_private.csv\", AS_1, delimiter=\",\")\n",
    "np.savetxt(\"Adjacency_Iris_non_private.csv\", AS, delimiter=\",\")\n",
    "np.savetxt(\"Y_na_Iris.csv\", Y_na, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Plotting graph structures for non-private and private cases** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_org = nx.from_numpy_matrix(AS)\n",
    "G_private = nx.from_numpy_matrix(AS_1)\n",
    "#print('Graph statistics(shared):')\n",
    "#print('Nodes: ', G_org.number_of_nodes(), 'Edges: ', G_org.number_of_edges())\n",
    "\n",
    "#print('Graph statistics(private):')\n",
    "#print('Nodes: ', G_private.number_of_nodes(), 'Edges: ', G_private.number_of_edges())\n",
    "#pos1 = nx.spring_layout(G_org)\n",
    "pos2 = nx.spring_layout(G_private)\n",
    "# # # normalize edge weights to plot edges strength\n",
    "fig = plt.figure(figsize=(12,12)) \n",
    "# # # # plot graph\n",
    "#plt.subplot(221)\n",
    "#fig = plt.figure(figsize=(20,20)) \n",
    "\n",
    "#nx.draw_networkx(G_org,pos1,node_color = C, node_size = 50, with_labels = False, width = 0.2)\n",
    "\n",
    "#plt.subplot(222)\n",
    "plt.box(False)\n",
    "nx.draw_networkx(G_private,pos2,node_color = C, node_size = 50, with_labels = False, width = 0.2)\n",
    "#plt.savefig(\"C:/Users/nimes/OneDrive/Desktop/PANCAN_non_private.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Assigning clusters** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Non-private graph clustering** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [i for i in range(X_na.shape[0])]\n",
    "cluster1 = list(nx.node_connected_component(G_org,0))\n",
    "print(set(z) - set(cluster1))\n",
    "cluster2 = list(nx.node_connected_component(G_org, 66))\n",
    "print((set(z) - set(cluster1)) - set(cluster2))\n",
    "\n",
    "\n",
    "cluster3 = list(nx.node_connected_component(G_org, 260))\n",
    "print(set(z) - set(cluster1) - set(cluster2) - set(cluster3))\n",
    "\n",
    "cluster4 = list(nx.node_connected_component(G_org, 107))\n",
    "print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4))\n",
    "\n",
    "cluster5 = list(nx.node_connected_component(G_org, 204))\n",
    "print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5))\n",
    "\n",
    "cluster6 = list(nx.node_connected_component(G_org, 334))\n",
    "print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5) - set(cluster6))\n",
    "# cluster7 = list(nx.node_connected_component(G_org, 53))\n",
    "#print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5) - set(cluster6) - set(cluster7))\n",
    "# cluster8 = list(nx.node_connected_component(G_org, 53))\n",
    "#print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5) - set(cluster6) - set(cluster7) - set(cluster8))\n",
    "# cluster9 = list(nx.node_connected_component(G_org, 53))\n",
    "#print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5) - set(cluster6) - set(cluster7) - set(cluster8) - set(cluster9))\n",
    "# cluster10 = list(nx.node_connected_component(G_org, 53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = X_na.shape[0] \n",
    "#assignment = [1 if i in cluster1 else 2 for i in range(nodes)]\n",
    "#assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 for i in range(nodes)]\n",
    "assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 if i in cluster3  else 4 if i in cluster4 else 5 if i in cluster5 else 6 for i in range(nodes)]\n",
    "#assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 if i in cluster3  else 4 if i in cluster4 else 5 for i in range(nodes)]\n",
    "print(assignment)\n",
    "print(normalized_mutual_info_score(Y_na, assignment, average_method = 'geometric'))\n",
    "print(adjusted_rand_score(Y_na, assignment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Private graph clustering** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [i for i in range(X_na.shape[0])]\n",
    "cluster1 = list(nx.node_connected_component(G_private, 0))\n",
    "print(set(z) - set(cluster1))\n",
    "\n",
    "cluster2 = list(nx.node_connected_component(G_private, 1))\n",
    "print((set(z) - set(cluster1)) - set(cluster2))\n",
    "\n",
    "cluster3 = list(nx.node_connected_component(G_private,2))\n",
    "print(set(z) - set(cluster1) - set(cluster2) - set(cluster3))\n",
    "\n",
    "# cluster4 = list(nx.node_connected_component(G_private, 513))\n",
    "# print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4))\n",
    "\n",
    "# cluster5 = list(nx.node_connected_component(G_private,387))\n",
    "# print(set(z) - set(cluster1) - set(cluster2) - set(cluster3) - set(cluster4) - set(cluster5))\n",
    "\n",
    "# cluster6 = list(nx.node_connected_component(G_private, 260))\n",
    "\n",
    "#print(\"cluster6\",cluster6)\n",
    "# cluster7 = list(nx.node_connected_component(G_private, 53))\n",
    "# cluster8 = list(nx.node_connected_component(G_private, 53))\n",
    "# cluster9 = list(nx.node_connected_component(G_private, 53))\n",
    "# cluster10 = list(nx.node_connected_component(G_private, 53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = X_na.shape[0] \n",
    "#assignment = [1 if i in cluster1 else 2 for i in range(nodes)]\n",
    "assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 for i in range(nodes)]\n",
    "#assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 if i in cluster3  else 4 if i in cluster4 else 5 if i in cluster5 else 6 for i in range(nodes)]\n",
    "#assignment = [1 if i in cluster1 else 2 if i in cluster2 else 3 if i in cluster3  else 4 if i in cluster4 else 5 for i in range(nodes)]\n",
    "print(assignment)\n",
    "print(normalized_mutual_info_score(Y_na, assignment, average_method = 'geometric'))\n",
    "print(adjusted_rand_score(Y_na, assignment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Animals** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/nimes/Downloads/Project_codes/animal_data.txt', 'r') as f:\n",
    "    data = [[int(num) for num in line.split(',')] for line in f]\n",
    "data = np.asarray(data)\n",
    "n = data.shape[0]\n",
    "d = data.shape[1]\n",
    "with open('C:/Users/nimes/Downloads/Project_codes/animal_name.txt', 'r') as f:\n",
    "    names = [[str(num) for num in line.split(',')] for line in f]\n",
    "names = names[0]\n",
    "print(data.shape)\n",
    "\n",
    "anchors = np.random.randint(2, size=(d-1,d))\n",
    "#anchors = np.random.rand(d-1,d)\n",
    "#anchors = np.random.randn(d-1, d)\n",
    "print(anchors.shape)\n",
    "\n",
    "X_na = data\n",
    "X_a = anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'))\n",
    "X_final, loss = MDS_X(D, V1, V2, W_1, DA, X_na.astype('float'), X_a.astype('float'), n, d)\n",
    "error, D_true, D_esti, z_true, z_esti = dist_error(X_na.astype('float'), X_final)\n",
    "print(\"Error in distance approximation: \", error)\n",
    "fscore = check_score(D_true, D_esti, 11)\n",
    "print(\"F-score: \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_na, X_a))\n",
    "\n",
    "nodes = X.shape[0]\n",
    "\n",
    "#2) Using classical MDS\n",
    "\n",
    "D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'))\n",
    "X_final, loss = classical_MDS_X(D, V, W_1, nodes,d)\n",
    "error, D_true, D_esti, z_true, z_esti = dist_error(X_na.astype('float'), X_final[:X_na.shape[0], :])\n",
    "print(\"Error in distance approximation: \", error)\n",
    "fscore = check_score(D_true, D_esti, 11)\n",
    "print(\"F-score: \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_na_animal.csv\", X_na, delimiter=\",\")\n",
    "np.savetxt(\"X_final_animal_50.csv\",X_final, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **Synthetic Data (SGL with low dimension embedding)** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy example of two-moon dataset\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.datasets import make_circles\n",
    "'''Visual results on two moon dataset \n",
    "\n",
    "'''\n",
    "np.random.seed(0)\n",
    "n = 100  # number of nodes per cluster\n",
    "k = 5   # number of components\n",
    "#X, y = make_moons(n_samples=n*k, noise=.05, shuffle=True)\n",
    "#X, y = make_circles(n_samples=n*k, factor=0.3, noise=0.05, random_state=0)\n",
    "X, y = make_blobs(n_samples=n*k, centers=k, n_features=500, random_state=0)\n",
    "y = y + 1\n",
    "# dict to store position of nodes\n",
    "pos = {}\n",
    "for i in range(n*k):\n",
    "    pos[i] = X[i,:2]\n",
    "# Visualization of original data\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.title(\"Two moon dataset\")\n",
    "plt.xlabel('x-coordinate')\n",
    "plt.ylabel('y-coordinate')\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_na, X_a, Y_na, Y_a = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "\n",
    "n = X_na.shape[0]               #number of non-anchor nodes (clients)\n",
    "\n",
    "d = X_na.shape[1]               #dimensionality of data\n",
    "\n",
    "print(\"Size of reference nodes(anchors)\", X_a.shape)\n",
    "\n",
    "X = np.vstack((X_na, X_a))\n",
    "\n",
    "nodes = X.shape[0]\n",
    "\n",
    "#2) Using classical MDS\n",
    "\n",
    "D, V, V1, V2, W_1, DA, DNA = dist_approx(X_na.astype('float'), X_a.astype('float'))\n",
    "X_final, loss = classical_MDS_X(D, V, W_1, nodes,d)\n",
    "error, D_true, D_esti, z_true, z_esti = dist_error(X_na.astype('float'), X_final[:X_na.shape[0], :])\n",
    "print(\"Error in distance approximation: \", error)\n",
    "fscore = check_score(D_true, D_esti, 11)\n",
    "print(\"F-score: \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.load('X_final_CC.npy')\n",
    "#X_na = np.load('X_na_Iris.npy')\n",
    "AS_1 = np.load('Adjacency_Private_CC.npy')\n",
    "AS = np.load('Adjacency_non_private_CC.npy')\n",
    "Y_na = np.load('Y_na_CC.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AS_1_sparse = sparse.csr_matrix(AS_1) \n",
    "AS_sparse = sparse.csr_matrix(AS) \n",
    "X_na_tensor = torch.from_numpy(X_na.astype('float'))\n",
    "X_final_tensor = torch.from_numpy(X_final)                                 #For anchored MDS\n",
    "#X_final_tensor = torch.from_numpy(X_final[:X_na.shape[0]])                 #For classical MDS\n",
    "Y_na_tensor = torch.from_numpy(Y_na - 1).type(torch.LongTensor)               #(-1) is done for considering 5 classes from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "sss = ShuffleSplit(n_splits=10, test_size=0.20) \n",
    "#train_index, test_index = next(sss.split(X_final[:X_na.shape[0]], Y_na))         #For classical MDS \n",
    "train_index, test_index = next(sss.split(X_final, Y_na))                         #For anchored MDS \n",
    "train_mask = np.zeros((X_na.shape[0],),dtype=bool)\n",
    "train_mask[train_index] = True\n",
    "train_mask = torch.from_numpy(train_mask)\n",
    "\n",
    "test_mask = np.zeros((X_na.shape[0],),dtype=bool)\n",
    "test_mask[test_index] = True\n",
    "test_mask = torch.from_numpy(test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **GNN with private graph** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    #plt.savefig(\"C:/Users/nimes/OneDrive/Desktop/ActivityLarge_after_GCN.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(X_final.shape[1], hidden_channels)             #Change number of features of input nodes\n",
    "        self.conv2 = GCNConv(hidden_channels, 6)                                #Change number of classes\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "edge_info = from_scipy_sparse_matrix(AS_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "model.double()\n",
    "model.eval()\n",
    "\n",
    "out = model(X_na_tensor, edge_info[0], edge_info[1])\n",
    "visualize(out, color=Y_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(X_na_tensor, edge_info[0], edge_info[1])  # Perform a single forward pass.\n",
    "      loss = criterion(out[train_mask], Y_na_tensor[train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(X_na_tensor, edge_info[0], edge_info[1])\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[test_mask] == Y_na_tensor[test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "acc = []\n",
    "for t in range(10):\n",
    "    sss = ShuffleSplit(n_splits=10, test_size=0.20) \n",
    "    #train_index, test_index = next(sss.split(X_final[:X_na.shape[0]], Y_na))         #For classical MDS \n",
    "    train_index, test_index = next(sss.split(X_final, Y_na))                         #For anchored MDS \n",
    "    train_mask = np.zeros((X_na.shape[0],),dtype=bool)\n",
    "    train_mask[train_index] = True\n",
    "    train_mask = torch.from_numpy(train_mask)\n",
    "\n",
    "    test_mask = np.zeros((X_na.shape[0],),dtype=bool)\n",
    "    test_mask[test_index] = True\n",
    "    test_mask = torch.from_numpy(test_mask)\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    test_acc = test()\n",
    "    acc.append(test_acc)\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "print(\"Average Accuracy over 10 runs:\", np.average(acc), \" ± \", np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "out = model(X_na_tensor, edge_info[0], edge_info[1])\n",
    "visualize(out, color=Y_na_tensor)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07ac4082f8b04fe2a75420b6aeb0de5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "099b6f777fc545aebd73b57abfea1bc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15c6217ec99c4ac48ff867f537a18a6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c39b146a61f84d919020985c250370fb",
      "placeholder": "​",
      "style": "IPY_MODEL_fba9d86b84304127b852fec68f520253",
      "value": " 1000/1000 [00:28&lt;00:00, 38.34it/s]"
     }
    },
    "1dba9196045d46d8b1fdf595e31d3c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "26a33910bc5f48d2ad4ab280cb2a344e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da5bade41a7f47b789cd14113eeb406e",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1dba9196045d46d8b1fdf595e31d3c8b",
      "value": 1000
     }
    },
    "48d3fb46d934428487f1174506e1897b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c79872bcc0c40979a562f54e4c94824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8adfe11c67014805969420c7f2b25ac7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9377e85bf2f5480daa8a2aa202f67133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "945162b5da9e4783ab89135d96b46096": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9646a8336a8d4052bfc642f342e4311d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad71995e703e4783a792540ce3cbd9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8adfe11c67014805969420c7f2b25ac7",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_48d3fb46d934428487f1174506e1897b",
      "value": 1000
     }
    },
    "b40c7bc4a9544db4873f3c224b54254d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9377e85bf2f5480daa8a2aa202f67133",
      "placeholder": "​",
      "style": "IPY_MODEL_9646a8336a8d4052bfc642f342e4311d",
      "value": "100%"
     }
    },
    "b6f351485dac4b2783227a17fafa16c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c39b146a61f84d919020985c250370fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c63ac589e1694c2780d8dd62c8232fe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da8542530db04661ba05b5e5ad895222",
       "IPY_MODEL_ad71995e703e4783a792540ce3cbd9cf",
       "IPY_MODEL_15c6217ec99c4ac48ff867f537a18a6e"
      ],
      "layout": "IPY_MODEL_e013541b25744487a441394a1eae1493"
     }
    },
    "d215b0a30c824be596b6a721f27f70d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b40c7bc4a9544db4873f3c224b54254d",
       "IPY_MODEL_26a33910bc5f48d2ad4ab280cb2a344e",
       "IPY_MODEL_ea41c99b97e1446b941c18157f755e2f"
      ],
      "layout": "IPY_MODEL_099b6f777fc545aebd73b57abfea1bc8"
     }
    },
    "da5bade41a7f47b789cd14113eeb406e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da8542530db04661ba05b5e5ad895222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07ac4082f8b04fe2a75420b6aeb0de5f",
      "placeholder": "​",
      "style": "IPY_MODEL_b6f351485dac4b2783227a17fafa16c5",
      "value": "100%"
     }
    },
    "e013541b25744487a441394a1eae1493": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea41c99b97e1446b941c18157f755e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_945162b5da9e4783ab89135d96b46096",
      "placeholder": "​",
      "style": "IPY_MODEL_7c79872bcc0c40979a562f54e4c94824",
      "value": " 1000/1000 [00:30&lt;00:00, 37.28it/s]"
     }
    },
    "fba9d86b84304127b852fec68f520253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
