{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures,OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import shap\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "\n",
    "\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "\n",
    "# Improved EDA\n",
    "def improved_eda(data):\n",
    "    print(\"Dataset Information:\")\n",
    "    print(data.info())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(data.isnull().sum())\n",
    "    \n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(data.describe())\n",
    "    \n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(data['DEATH_EVENT'].value_counts(normalize=True))\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution plots for numerical features\n",
    "    num_features = data.select_dtypes(include=[np.number]).columns\n",
    "    n_features = len(num_features)\n",
    "    fig, axes = plt.subplots(n_features // 3 + 1, 3, figsize=(20, 5 * (n_features // 3 + 1)))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(num_features):\n",
    "        sns.histplot(data=data, x=col, hue='DEATH_EVENT', kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plots for numerical features\n",
    "    fig, axes = plt.subplots(n_features // 3 + 1, 3, figsize=(20, 5 * (n_features // 3 + 1)))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(num_features):\n",
    "        sns.boxplot(data=data, x='DEATH_EVENT', y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Box Plot of {col} by DEATH_EVENT')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for outliers using IQR method\n",
    "    def detect_outliers(df, column):\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        return outliers\n",
    "    \n",
    "    print(\"\\nOutliers Detection:\")\n",
    "    for col in num_features:\n",
    "        outliers = detect_outliers(data, col)\n",
    "        print(f\"Outliers in {col}: {len(outliers)}\")\n",
    "\n",
    "improved_eda(data)\n",
    "\n",
    "\n",
    "# Advanced EDA\n",
    "def plot_feature_distributions(data):\n",
    "    n_features = len(data.columns)\n",
    "    fig, axes = plt.subplots(n_features // 3 + 1, 3, figsize=(20, 5 * (n_features // 3 + 1)))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(data.columns):\n",
    "        sns.histplot(data=data, x=col, hue='DEATH_EVENT', kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plot_feature_distributions(data)\n",
    "    \n",
    "# Outlier detection and handling function\n",
    "def detect_and_handle_outliers(data, columns, method='iqr'):\n",
    "    for col in columns:\n",
    "        if method == 'iqr':\n",
    "            Q1 = data[col].quantile(0.25)\n",
    "            Q3 = data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs(stats.zscore(data[col]))\n",
    "            lower_bound = data[col].mean() - 3 * data[col].std()\n",
    "            upper_bound = data[col].mean() + 3 * data[col].std()\n",
    "        \n",
    "        print(f\"Outliers in {col}:\")\n",
    "        print(data[(data[col] < lower_bound) | (data[col] > upper_bound)][col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "detect_and_handle_outliers(data, data.select_dtypes(include=[np.number]).columns, method='iqr')\n",
    "\n",
    "def one_hot_encoding(df,columns):\n",
    "        for column in columns:\n",
    "            df = pd.concat([df,pd.get_dummies(df[column],prefix=column)],axis=1)\n",
    "            df.drop(column,axis=1,inplace=True)\n",
    "        return df\n",
    "\n",
    "def feature_engineering(data):\n",
    "    # Add age_group categorical feature\n",
    "    data['age_group'] = pd.cut(data['age'], bins=[0, 30, 45, 60, 75, 100], labels=['Young', 'Middle-aged', 'Senior', 'Elderly', 'Very Elderly'])\n",
    "\n",
    "    data = one_hot_encoding(data,['age_group'])\n",
    "\n",
    "    # Add anemia and diabetes interaction feature\n",
    "    data['anemia_diabetes_interaction'] = data['anaemia'] * data['diabetes']\n",
    "    \n",
    "    # If you are high blood pressure and/or smoke diabetes and senior or elder, you are at higher risk of heart failure. Convert this to a binary feature (1 or 0)\n",
    "    data['risk_factor'] = ((data['high_blood_pressure'] == 1) | (data['smoking'] == 1)) & ((data['diabetes'] == 1)) \n",
    "    data['risk_factor'] = data['risk_factor'].astype(int)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Handle Class Imbalance using SMOTE\n",
    "def handle_class_imbalance(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Feature Importance Analysis\n",
    "def compute_feature_importance(name,best_model_clf,model,X):\n",
    "    if name in [\"randomforest\",\"gradientboosting\",\"adaboost\",\"extratrees\"]:\n",
    "        #analyze_feature_importance(best_model_clf_, X_train)    \n",
    "        importances = best_model_clf.named_steps[model[0][0]].feature_importances_\n",
    "    \n",
    "    if name in [\"logistic\"]:\n",
    "        #analyze_feature_importance(best_model_clf_, X_train)\n",
    "        importances = best_model_clf.named_steps[model[0][0]].coef_\n",
    "    \n",
    "    feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    feature_importance = feature_importance[feature_importance['importance'] > 0]\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Feature Importance for ' + name)\n",
    "    plt.show()\n",
    "\n",
    "# Model Interpretability using SHAP\n",
    "def interpret_model_with_shap(best_model_clf, X_train,X_test):\n",
    "\n",
    "    # explain all the predictions in the test set\n",
    "    explainer = shap.KernelExplainer(best_model_clf.predict_proba, X_train[:30])\n",
    "    shap_values = explainer.shap_values(X_test[:30])\n",
    "    shap.force_plot(explainer.expected_value[0], shap_values[..., 0], X_test[:30])\n",
    "\n",
    "# Feature Selection using Recursive Feature Elimination\n",
    "def perform_feature_selection(X, y, model, n_features_to_select=5):\n",
    "    rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    return X.columns[rfe.support_]\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(y_true, y_pred, y_prob):\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_true, y_prob)}\")\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    print(f\"Average Precision Score: {average_precision_score(y_true, y_prob)}\")\n",
    "\n",
    "    # ROC-AUC Score\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(['DEATH_EVENT'], axis=1)\n",
    "y = data['DEATH_EVENT']\n",
    "\n",
    "# Feature Engineering\n",
    "X = feature_engineering(X)\n",
    "\n",
    "display(X.head())\n",
    "\n",
    "\n",
    "search_space_svc = [{'svc__kernel': ['linear', 'poly'],\n",
    "                    'svc__gamma': ['scale'],\n",
    "                    'svc__C': [0.1, 1]}]\n",
    "\n",
    "search_space_lr = [{'logisticregression__C': [0.1, 1],\n",
    "                    'logisticregression__penalty': ['l1', 'l2']}]\n",
    "\n",
    "search_space_ridge = [{'ridge__alpha': [0.1, 1]}]\n",
    "\n",
    "search_space_decisiontree = [{'decisiontreeclassifier__max_depth': [2, 4, 6]}]\n",
    "search_space_randomforest = [{'randomforestclassifier__n_estimators': [10,50,100,200],\n",
    "                                'randomforestclassifier__max_features': [1,5,10]}]\n",
    "\n",
    "search_space_gradientboosting = [{'gradientboostingclassifier__n_estimators': [10,25,50],\n",
    "                                'gradientboostingclassifier__max_features': [1, 2, 5,10]}]\n",
    "\n",
    "search_space_adaboost = [{'adaboostclassifier__n_estimators': [10,25,50]}]\n",
    "\n",
    "search_space_bagging = [{'baggingclassifier__n_estimators': [10,25,50]}]\n",
    "\n",
    "search_space_extratrees = [{'extratreesclassifier__n_estimators': [10,25,50]}]\n",
    "\n",
    "search_space_knn = [{'kneighborsclassifier__n_neighbors': [5, 10, 15,25]}]\n",
    "\n",
    "\n",
    "dict_models = {\n",
    "    \"svc\": [('svc', SVC(probability=True)),\n",
    "            search_space_svc,\n",
    "            True],\n",
    "\n",
    "    \"logistic\": [('logisticregression', LogisticRegression()),\n",
    "                    search_space_lr,\n",
    "                    True],\n",
    "    \n",
    "    \"decisiontree\": [('decisiontreeclassifier', DecisionTreeClassifier()),\n",
    "                     search_space_decisiontree,\n",
    "                     False],\n",
    "    \"randomforest\": [('randomforestclassifier', RandomForestClassifier()),\n",
    "                     search_space_randomforest,\n",
    "                     False],\n",
    "    \"gradientboosting\": [('gradientboostingclassifier', GradientBoostingClassifier()),\n",
    "                         search_space_gradientboosting,\n",
    "                         False],\n",
    "    \"adaboost\": [('adaboostclassifier', AdaBoostClassifier()),\n",
    "                 search_space_adaboost,\n",
    "                 False],\n",
    "    \"bagging\": [('baggingclassifier', BaggingClassifier()),\n",
    "                search_space_bagging, \n",
    "                False],\n",
    "    \"extratrees\": [('extratreesclassifier', ExtraTreesClassifier()),\n",
    "                   search_space_extratrees, \n",
    "                   False],\n",
    "    \"knn\": [('kneighborsclassifier', KNeighborsClassifier()),\n",
    "            search_space_knn,\n",
    "            True],\n",
    "}\n",
    "\n",
    "polynomial = False\n",
    "subset = False\n",
    "subset_features = X.columns\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Handle class imbalance\n",
    "#X_train, y_train = handle_class_imbalance(X_train, y_train)\n",
    "\n",
    "results = {}\n",
    "already_scaled = False\n",
    "for name, model in dict_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    if model[2] and not already_scaled: # this means the model requires scaling\n",
    "       \n",
    "        standard_scaler = StandardScaler()\n",
    "        if polynomial:\n",
    "            X_train = X_train.to_numpy()\n",
    "\n",
    "            poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=True)\n",
    "\n",
    "            binary_columns = [col for col in X_train.columns if len(X_train[col].unique()) == 2]\n",
    "            #use sets to get the difference between the two lists\n",
    "            numeric_features = list(set(X_train.columns) - set(binary_columns_subset))\n",
    "\n",
    "            X_train_poly = poly.fit_transform(X_train[[numeric_features]])\n",
    "            X_test_poly = poly.fit_transform(X_test[[numeric_features]])\n",
    "\n",
    "            X_train_scaled = standard_scaler.fit_transform(X_train_poly)\n",
    "\n",
    "            # Apply the scaler to the test data\n",
    "            X_test_scaled = standard_scaler.transform(X_test_poly)\n",
    "            X_test = np.concatenate((X_test_scaled, X_test[binary_columns]), axis=1)\n",
    "\n",
    "            # Append the binary columns to the normalized data\n",
    "            X_train = np.concatenate((X_train_scaled, X_train[categorical_features]), axis=1)\n",
    "            X_train = pd.DataFrame(X_train)\n",
    "\n",
    "        else:\n",
    "            if subset:\n",
    "                #get the subset of features\n",
    "                X_train = X_train[[subset_features]]\n",
    "                X_test = X_test[[subset_features]] \n",
    "                \n",
    "            #check which columns are binary\n",
    "            binary_columns_subset = [col for col in X_train.columns if len(X_train[col].unique()) == 2]\n",
    "            #use sets to get the difference between the two lists\n",
    "            numeric_features_subset = list(set(X_train.columns) - set(binary_columns_subset))\n",
    "\n",
    "            #print(numeric_features_subset)\n",
    "            #print(binary_columns_subset)\n",
    "\n",
    "            X_train_scaled = standard_scaler.fit_transform(X_train[numeric_features_subset])\n",
    "            X_train = np.concatenate((X_train_scaled, X_train[binary_columns_subset]), axis=1)\n",
    "            X_train = pd.DataFrame(X_train, columns=numeric_features_subset+binary_columns_subset)\n",
    "\n",
    "            X_test_scaled = standard_scaler.transform(X_test[numeric_features_subset])\n",
    "            X_test = np.concatenate((X_test_scaled, X_test[binary_columns_subset]), axis=1)\n",
    "\n",
    "            already_scaled = True\n",
    "\n",
    "\n",
    "    pipe = Pipeline([model[0]])\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, model[1], cv=5, verbose=0,scoring=\"roc_auc\",return_train_score=True,n_jobs=-1)\n",
    "    \n",
    "    # Fit grid search\n",
    "    best_model_clf_ = grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # visualize the best hyperparameters without dictionary format \n",
    "\n",
    "    print(\"Best hyperparameters %s\" % (best_model_clf_.best_params_))\n",
    "\n",
    "    # get the best estimator\n",
    "    best_model_clf = best_model_clf_.best_estimator_\n",
    "\n",
    "    # Compute feature importances\n",
    "    #compute_feature_importances(name,X_train,best_model_clf)\n",
    "    if name in [\"logisticregression\",\"randomforest\",\"gradientboosting\",\"adaboost\",\"extratrees\"]:\n",
    "        compute_feature_importance(name,best_model_clf,model,X_train)\n",
    "\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = best_model_clf.predict(X_test)\n",
    "    y_prob = best_model_clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    evaluate_model(y_test, y_pred, y_prob)\n",
    "\n",
    "    # Interpret model using SHAP\n",
    "    #if name in [\"randomforest\",\"gradientboosting\",\"adaboost\",\"extratrees\"]:\n",
    "        #interpret_model_with_shap(best_model_clf, X_train,X_test)\n",
    "    \n",
    "    \n",
    "    results[name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_score': grid_search.best_score_,\n",
    "        'test_score': roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "\n",
    "    #Convert the results to a dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "display(results_df)\n",
    "# Compare models\n",
    "cv_scores = [result['cv_score'] for result in results.values()]\n",
    "test_scores = [result['test_score'] for result in results.values()]\n",
    "\n",
    "\n",
    "# Feature importance analysis\n",
    "best_model = max(results, key=lambda x: results[x]['test_score'])\n",
    "best_model_cv = max(results, key=lambda x: results[x]['cv_score'])\n",
    "print(f\"Best Model cv: {best_model_cv}\")\n",
    "print(f\"\\nBest Model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance for the best model\n",
    "compute_feature_importance(best_model, results[best_model]['model'],X_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
