{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emotion_detection.py\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np  #this will be used later in the process\n",
    "\n",
    "imgpath = \"sad face.jpg\"  #put the image where this file is located and put its name here\n",
    "image = cv2.imread(imgpath)\n",
    "\n",
    "analyze = DeepFace.analyze(img_path = imgpath, \n",
    "        actions = ['emotion']\n",
    ")\n",
    "print(analyze[0].get('dominant_emotion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104682a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c230778",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = DeepFace.analyze(img_path = \"sad face.jpg\", \n",
    "        actions = ['age', 'gender', 'race', 'emotion']\n",
    ")\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1b478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d2657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef11bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca8463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "\n",
    "face_cascade_name = cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'  #getting a haarcascade xml file\n",
    "face_cascade = cv2.CascadeClassifier()  #processing it for our project\n",
    "if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):  #adding a fallback event\n",
    "    print(\"Error loading xml file\")\n",
    "\n",
    "video=cv2.VideoCapture(0)  #requisting the input from the webcam or camera\n",
    "\n",
    "while True:  \n",
    "    ret,frame = video.read()\n",
    "\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  #changing the video to grayscale to make the face analisis work properly\n",
    "    face=face_cascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5)\n",
    "\n",
    "    for x,y,w,h in face:\n",
    "        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),1)  #making a recentangle to show up and detect the face and setting it position and colour\n",
    "   \n",
    "      #making a try and except condition in case of any errors\n",
    "        try:\n",
    "            analyze = DeepFace.analyze(frame, actions = ['emotion'])\n",
    "          #analyze = DeepFace.analyze(frame,actions=['emotions'])  #same thing is happing here as the previous example, we are using the analyze class from deepface and using ‘frame’ as input\n",
    "            print(analyze[0].get('dominant_emotion'))  #here we will only go print out the dominant emotion also explained in the previous example\n",
    "        except:\n",
    "            print(\"no face\")\n",
    "\n",
    "      #this is the part where we display the output to the user\n",
    "      \n",
    "    cv2.imshow('frame', frame)\n",
    "      \n",
    "    key=cv2.waitKey(1) & 0xFF\n",
    "    if key==ord('q'):# here we are specifying the key which will stop the loop and stop all the processes going\n",
    "        cv2.destroyAllWindows()\n",
    "        video.release()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fc215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0235c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras.preprocessing.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = 'models/_mini_XCEPTION.102-0.66.hdf5'\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]\n",
    "\n",
    "\n",
    "#feelings_faces = []\n",
    "#for index, emotion in enumerate(EMOTIONS):\n",
    "   # feelings_faces.append(cv2.imread('emojis/' + emotion + '.png', -1))\n",
    "\n",
    "# starting video streaming\n",
    "cv2.namedWindow('your_face')\n",
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    frame = camera.read()[1]\n",
    "    #reading the frame\n",
    "    frame = imutils.resize(frame,width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
    "    frameClone = frame.copy()\n",
    "    if len(faces) > 0:\n",
    "        faces = sorted(faces, reverse=True,\n",
    "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "        (fX, fY, fW, fH) = faces\n",
    "                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare\n",
    "            # the ROI for classification via the CNN\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "        roi = cv2.resize(roi, (64, 64))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        \n",
    "        preds = emotion_classifier.predict(roi)[0]\n",
    "        emotion_probability = np.max(preds)\n",
    "        label = EMOTIONS[preds.argmax()]\n",
    "    else: continue\n",
    "\n",
    " \n",
    "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                # construct the label text\n",
    "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                # draw the label + probability bar on the canvas\n",
    "               # emoji_face = feelings_faces[np.argmax(preds)]\n",
    "\n",
    "                \n",
    "                w = int(prob * 300)\n",
    "                cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
    "                (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                (255, 255, 255), 2)\n",
    "                cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                              (0, 0, 255), 2)\n",
    "#    for c in range(0, 3):\n",
    "#        frame[200:320, 10:130, c] = emoji_face[:, :, c] * \\\n",
    "#        (emoji_face[:, :, 3] / 255.0) + frame[200:320,\n",
    "#        10:130, c] * (1.0 - emoji_face[:, :, 3] / 255.0)\n",
    "\n",
    "\n",
    "    cv2.imshow('your_face', frameClone)\n",
    "    cv2.imshow(\"Probabilities\", canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe662ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307e317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe731be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "USE_WEBCAM = True # If false, loads video file source\n",
    "\n",
    "# parameters for loading data and images\n",
    "emotion_model_path = './models/emotion_model.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "\n",
    "# hyper-parameters for bounding boxes shape\n",
    "frame_window = 10\n",
    "emotion_offsets = (20, 40)\n",
    "\n",
    "# loading models\n",
    "face_cascade = cv2.CascadeClassifier('./models/haarcascade_frontalface_default.xml')\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "# starting lists for calculating modes\n",
    "emotion_window = []\n",
    "\n",
    "# starting video streaming\n",
    "\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Select video or webcam feed\n",
    "cap = None\n",
    "if (USE_WEBCAM == True):\n",
    "    cap = cv2.VideoCapture(0) # Webcam source\n",
    "else:\n",
    "    cap = cv2.VideoCapture('./demo/dinner.mp4') # Video file source\n",
    "\n",
    "while cap.isOpened(): # True:\n",
    "    ret, bgr_image = cap.read()\n",
    "\n",
    "    #bgr_image = video_capture.read()[1]\n",
    "\n",
    "    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5,\n",
    "\t\t\tminSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for face_coordinates in faces:\n",
    "\n",
    "        x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
    "        gray_face = gray_image[y1:y2, x1:x2]\n",
    "        try:\n",
    "            gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gray_face = preprocess_input(gray_face, True)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "        emotion_probability = np.max(emotion_prediction)\n",
    "        emotion_label_arg = np.argmax(emotion_prediction)\n",
    "        emotion_text = emotion_labels[emotion_label_arg]\n",
    "        emotion_window.append(emotion_text)\n",
    "\n",
    "        if len(emotion_window) > frame_window:\n",
    "            emotion_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_window)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if emotion_text == 'angry':\n",
    "            color = emotion_probability * np.asarray((255, 0, 0))\n",
    "        elif emotion_text == 'sad':\n",
    "            color = emotion_probability * np.asarray((0, 0, 255))\n",
    "        elif emotion_text == 'happy':\n",
    "            color = emotion_probability * np.asarray((255, 255, 0))\n",
    "        elif emotion_text == 'surprise':\n",
    "            color = emotion_probability * np.asarray((0, 255, 255))\n",
    "        else:\n",
    "            color = emotion_probability * np.asarray((0, 255, 0))\n",
    "\n",
    "        color = color.astype(int)\n",
    "        color = color.tolist()\n",
    "\n",
    "        draw_bounding_box(face_coordinates, rgb_image, color)\n",
    "        draw_text(face_coordinates, rgb_image, emotion_mode,\n",
    "                  color, 0, -45, 1, 1)\n",
    "\n",
    "    bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('window_frame', bgr_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2da4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ed492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer import Video\n",
    "from fer import FER\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Put in the location of the video file that has to be processed\n",
    "location_videofile = \"/content/Video_One.mp4\"\n",
    "\n",
    "# Build the Face detection detector\n",
    "face_detector = FER(mtcnn=True)\n",
    "# Input the video for processing\n",
    "input_video = Video(location_videofile)\n",
    "\n",
    "# The Analyze() function will run analysis on every frame of the input video. \n",
    "# It will create a rectangular box around every image and show the emotion values next to that.\n",
    "# Finally, the method will publish a new video that will have a box around the face of the human with live emotion values.\n",
    "processing_data = input_video.analyze(face_detector, display=False)\n",
    "\n",
    "# We will now convert the analysed information into a dataframe.\n",
    "# This will help us import the data as a .CSV file to perform analysis over it later\n",
    "vid_df = input_video.to_pandas(processing_data)\n",
    "vid_df = input_video.get_first_face(vid_df)\n",
    "vid_df = input_video.get_emotions(vid_df)\n",
    "\n",
    "# Plotting the emotions against time in the video\n",
    "pltfig = vid_df.plot(figsize=(20, 8), fontsize=16).get_figure()\n",
    "\n",
    "# We will now work on the dataframe to extract which emotion was prominent in the video\n",
    "angry = sum(vid_df.angry)\n",
    "disgust = sum(vid_df.disgust)\n",
    "fear = sum(vid_df.fear)\n",
    "happy = sum(vid_df.happy)\n",
    "sad = sum(vid_df.sad)\n",
    "surprise = sum(vid_df.surprise)\n",
    "neutral = sum(vid_df.neutral)\n",
    "\n",
    "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotions_values = [angry, disgust, fear, happy, sad, surprise, neutral]\n",
    "\n",
    "score_comparisons = pd.DataFrame(emotions, columns = ['Human Emotions'])\n",
    "score_comparisons['Emotion Value from the Video'] = emotions_values\n",
    "score_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7331a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "78b3c8e8b1c69b9698898f323dba1fce1c2f465c5a24064388ed16c4183972ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
