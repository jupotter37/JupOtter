{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting options for the plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats={'retina', 'svg'}\n",
    "%config InlineBackend.rc={'savefig.dpi': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out warnings from third-party libraries to prevent them\n",
    "# from showing up in the notebooks in multiple places\n",
    "import warnings\n",
    "\n",
    "# warning from shap about tqdm \n",
    "from tqdm import TqdmExperimentalWarning\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=TqdmExperimentalWarning, \n",
    "                        message=\"Using `tqdm.autonotebook.tqdm` .*\", \n",
    "                        module=\"shap.explainers._linear\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a118166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import platform\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from os.path import abspath, relpath, exists, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "# allow older versions of pandas to work\n",
    "try:\n",
    "    from pandas.io.common import DtypeWarning\n",
    "except ImportError:\n",
    "    from pandas.errors import DtypeWarning\n",
    "\n",
    "from IPython import sys_info\n",
    "from IPython.display import display, HTML, Image, Javascript, Markdown, SVG\n",
    "from rsmtool.reader import DataReader\n",
    "from rsmtool.writer import DataWriter\n",
    "from rsmtool.utils.files import parse_json_with_comments\n",
    "from rsmtool.utils.notebook import (float_format_func,\n",
    "                                    int_or_float_format_func,\n",
    "                                    compute_subgroup_plot_params,\n",
    "                                    bold_highlighter,\n",
    "                                    color_highlighter,\n",
    "                                    show_thumbnail)\n",
    "\n",
    "from rsmtool.fairness_utils import (get_fairness_analyses,\n",
    "                                    write_fairness_results)\n",
    "\n",
    "from rsmtool.version import VERSION as rsmtool_version\n",
    "\n",
    "# turn off interactive plotting\n",
    "plt.ioff()\n",
    "\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c13e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_report_dir = os.environ.get('RSM_REPORT_DIR', None)\n",
    "if rsm_report_dir is None:\n",
    "    rsm_report_dir = os.getcwd()\n",
    "\n",
    "rsm_environ_config = join(rsm_report_dir, '.environ.json')\n",
    "if not exists(rsm_environ_config):\n",
    "    raise FileNotFoundError('The file {} cannot be located. '\n",
    "                            'Please make sure that either (1) '\n",
    "                            'you have set the correct directory with the `RSM_REPORT_DIR` '\n",
    "                            'environment variable, or (2) that your `.environ.json` '\n",
    "                            'file is in the same directory as your notebook.'.format(rsm_environ_config))\n",
    "    \n",
    "environ_config = parse_json_with_comments(rsm_environ_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbf1eb",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "  div.prompt.output_prompt { \n",
    "    color: white; \n",
    "  }\n",
    "  \n",
    "  span.highlight_color {\n",
    "    color: red;\n",
    "  }\n",
    "  \n",
    "  span.highlight_bold {\n",
    "    font-weight: bold;  \n",
    "  }\n",
    "    \n",
    "  @media print {\n",
    "    @page {\n",
    "      size: landscape;\n",
    "      margin: 0cm 0cm 0cm 0cm;\n",
    "    }\n",
    "\n",
    "    * {\n",
    "      margin: 0px;\n",
    "      padding: 0px;\n",
    "    }\n",
    "\n",
    "    #toc {\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    span.highlight_color, span.highlight_bold {\n",
    "        font-weight: bolder;\n",
    "        text-decoration: underline;\n",
    "    }\n",
    "\n",
    "    div.prompt.output_prompt {\n",
    "      display: none;\n",
    "    }\n",
    "    \n",
    "    h3#Python-packages, div#packages {\n",
    "      display: none;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you will need to set the following manually\n",
    "# if you are using this notebook interactively.\n",
    "experiment_id = environ_config.get('EXPERIMENT_ID')\n",
    "description = environ_config.get('DESCRIPTION')\n",
    "context = environ_config.get('CONTEXT')\n",
    "train_file_location = environ_config.get('TRAIN_FILE_LOCATION')\n",
    "test_file_location = environ_config.get('TEST_FILE_LOCATION')\n",
    "output_dir = environ_config.get('OUTPUT_DIR')\n",
    "figure_dir = environ_config.get('FIGURE_DIR')\n",
    "model_name = environ_config.get('MODEL_NAME')\n",
    "model_type = environ_config.get('MODEL_TYPE')\n",
    "skll_fixed_parameters = environ_config.get('SKLL_FIXED_PARAMETERS')\n",
    "skll_objective = environ_config.get('SKLL_OBJECTIVE')\n",
    "file_format = environ_config.get('FILE_FORMAT')\n",
    "length_column = environ_config.get('LENGTH_COLUMN')\n",
    "second_human_score_column = environ_config.get('H2_COLUMN')\n",
    "use_scaled_predictions = environ_config.get('SCALED')\n",
    "min_score = environ_config.get(\"MIN_SCORE\")\n",
    "max_score = environ_config.get(\"MAX_SCORE\")\n",
    "standardize_features = environ_config.get('STANDARDIZE_FEATURES')\n",
    "exclude_zero_scores = environ_config.get('EXCLUDE_ZEROS')\n",
    "feature_subset_file = environ_config.get('FEATURE_SUBSET_FILE', ' ')\n",
    "min_items = environ_config.get('MIN_ITEMS')\n",
    "use_thumbnails = environ_config.get('USE_THUMBNAILS')\n",
    "predict_expected_scores = environ_config.get('PREDICT_EXPECTED_SCORES')\n",
    "rater_error_variance = environ_config.get(\"RATER_ERROR_VARIANCE\")\n",
    "\n",
    "# groups for analysis by prompt or subgroup.\n",
    "groups_desc = environ_config.get('GROUPS_FOR_DESCRIPTIVES') \n",
    "groups_eval = environ_config.get('GROUPS_FOR_EVALUATIONS') \n",
    "\n",
    "# min number of n for group to be displayed in the report\n",
    "min_n_per_group = environ_config.get('MIN_N_PER_GROUP')\n",
    "\n",
    "if min_n_per_group is None:\n",
    "    min_n_per_group = {}\n",
    "\n",
    "# javascript path\n",
    "javascript_path = environ_config.get(\"JAVASCRIPT_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008da71",
   "metadata": {},
   "source": [
    "# Experiment Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ac5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counter for thumbnail IDs\n",
    "id_generator = itertools.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(javascript_path, \"sort.js\"), \"r\", encoding=\"utf-8\") as sortf:\n",
    "    display(Javascript(data=sortf.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown('''This report presents the analysis for **{}**: {}'''.format(experiment_id, description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "if use_thumbnails:\n",
    "    markdown_str += (\"\"\"\\n  - Images in this report have been converted to \"\"\"\n",
    "                     \"\"\"clickable thumbnails.\"\"\")\n",
    "if predict_expected_scores:\n",
    "    markdown_str += (\"\"\"\\n  - Predictions analyzed in this report are *expected scores*, \"\"\"\n",
    "                     \"\"\"i.e., probability-weighted averages over all score points.\"\"\")\n",
    "\n",
    "if markdown_str:\n",
    "    markdown_str = '**Notes**:' + markdown_str\n",
    "    display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de643089",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(time.strftime('%c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ed2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ca862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training and testing features, both raw and pre-processed\n",
    "# Make sure that the `spkitemid` and `candidate` columns are read as strings \n",
    "# to preserve any leading zeros\n",
    "# We filter DtypeWarnings that pop up mostly in very large files\n",
    "\n",
    "string_columns = ['spkitemid', 'candidate']\n",
    "converter_dict = {column: str for column in string_columns}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', category=DtypeWarning)\n",
    "    if exists(train_file_location):\n",
    "        df_train_orig = DataReader.read_from_file(train_file_location)\n",
    "\n",
    "    train_file = join(output_dir, '{}_train_features.{}'.format(experiment_id,\n",
    "                                                                file_format))\n",
    "    if exists(train_file):\n",
    "        df_train = DataReader.read_from_file(train_file, converters=converter_dict)\n",
    "\n",
    "    train_metadata_file = join(output_dir, '{}_train_metadata.{}'.format(experiment_id,\n",
    "                                                                         file_format))    \n",
    "    if exists(train_metadata_file):\n",
    "        df_train_metadata = DataReader.read_from_file(train_metadata_file, converters=converter_dict)\n",
    "\n",
    "    train_other_columns_file = join(output_dir, '{}_train_other_columns.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_other_columns_file):\n",
    "        df_train_other_columns = DataReader.read_from_file(train_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    train_length_file = join(output_dir, '{}_train_response_lengths.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(train_length_file):\n",
    "        df_train_length = DataReader.read_from_file(train_length_file, converters=converter_dict)\n",
    "\n",
    "    train_excluded_file = join(output_dir, '{}_train_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(train_excluded_file):\n",
    "        df_train_excluded = DataReader.read_from_file(train_excluded_file, converters=converter_dict)\n",
    "\n",
    "    train_responses_with_excluded_flags_file = join(output_dir, '{}_train_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                   file_format))\n",
    "    if exists(train_responses_with_excluded_flags_file):\n",
    "        df_train_responses_with_excluded_flags = DataReader.read_from_file(train_responses_with_excluded_flags_file,\n",
    "                                                                           converters=converter_dict)\n",
    "\n",
    "    train_preproc_file = join(output_dir, '{}_train_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                     file_format))    \n",
    "    if exists(train_preproc_file):\n",
    "        df_train_preproc = DataReader.read_from_file(train_preproc_file, converters=converter_dict)\n",
    "\n",
    "    if exists(test_file_location):\n",
    "        df_test_orig = DataReader.read_from_file(test_file_location)\n",
    "\n",
    "    test_file = join(output_dir, '{}_test_features.{}'.format(experiment_id,\n",
    "                                                              file_format))\n",
    "    if exists(test_file):\n",
    "        df_test = DataReader.read_from_file(test_file, converters=converter_dict)\n",
    "\n",
    "    test_metadata_file = join(output_dir, '{}_test_metadata.{}'.format(experiment_id,\n",
    "                                                                       file_format))    \n",
    "    if exists(test_metadata_file):\n",
    "        df_test_metadata = DataReader.read_from_file(test_metadata_file, converters=converter_dict)\n",
    "\n",
    "    test_other_columns_file = join(output_dir, '{}_test_other_columns.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_other_columns_file):\n",
    "        df_test_other_columns = DataReader.read_from_file(test_other_columns_file, converters=converter_dict)\n",
    "\n",
    "    test_human_scores_file = join(output_dir, '{}_test_human_scores.{}'.format(experiment_id,\n",
    "                                                                               file_format))\n",
    "    if exists(test_human_scores_file):\n",
    "        df_test_human_scores = DataReader.read_from_file(test_human_scores_file, converters=converter_dict)\n",
    "\n",
    "    test_excluded_file = join(output_dir, '{}_test_excluded_responses.{}'.format(experiment_id,\n",
    "                                                                                 file_format))\n",
    "    if exists(test_excluded_file):\n",
    "        df_test_excluded = DataReader.read_from_file(test_excluded_file, converters=converter_dict)\n",
    "\n",
    "    test_responses_with_excluded_flags_file = join(output_dir, '{}_test_responses_with_excluded_flags.{}'.format(experiment_id,\n",
    "                                                                                                                 file_format))\n",
    "    if exists(test_responses_with_excluded_flags_file):\n",
    "        df_test_responses_with_excluded_flags = DataReader.read_from_file(test_responses_with_excluded_flags_file,\n",
    "                                                                          converters=converter_dict)\n",
    "\n",
    "    test_preproc_file = join(output_dir, '{}_test_preprocessed_features.{}'.format(experiment_id,\n",
    "                                                                                   file_format))\n",
    "    if exists(test_preproc_file):\n",
    "        df_test_preproc = DataReader.read_from_file(test_preproc_file, converters=converter_dict)\n",
    "\n",
    "    pred_preproc_file = join(output_dir, '{}_pred_processed.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "    if exists(pred_preproc_file):\n",
    "        df_pred_preproc = DataReader.read_from_file(pred_preproc_file, converters=converter_dict)\n",
    "\n",
    "    feature_file = join(output_dir, '{}_feature.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "    if exists(feature_file):\n",
    "        df_features = DataReader.read_from_file(feature_file, converters=converter_dict)\n",
    "        features_used = [c for c in df_features.feature.values]\n",
    "        # compute the longest feature name: we'll need if for the plots\n",
    "        longest_feature_name = max(map(len, features_used))\n",
    "\n",
    "    betas_file = join(output_dir, '{}_betas.{}'.format(experiment_id,\n",
    "                                                       file_format))\n",
    "    if exists(betas_file):\n",
    "        df_betas = DataReader.read_from_file(betas_file)\n",
    "\n",
    "    if exists(feature_subset_file):\n",
    "        df_feature_subset_specs = DataReader.read_from_file(feature_subset_file)\n",
    "    else:\n",
    "        df_feature_subset_specs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for continuous human scores in the evaluation set\n",
    "continuous_human_score = False\n",
    "\n",
    "if exists(pred_preproc_file):\n",
    "    if not df_pred_preproc['sc1'].equals(np.round(df_pred_preproc['sc1'])):\n",
    "        continuous_human_score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a141f7",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c17028",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num_excluded_train = len(df_train_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_train = 0\n",
    "\n",
    "try:\n",
    "    num_excluded_test = len(df_test_responses_with_excluded_flags)\n",
    "except NameError:\n",
    "    num_excluded_test = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_excluded_train = round(100*num_excluded_train/len(df_train_orig), 2)\n",
    "pct_excluded_test = round(100*num_excluded_test/len(df_test_orig), 2)\n",
    "\n",
    "if (num_excluded_train != 0 or num_excluded_test != 0):\n",
    "    display(Markdown(\"### Responses excluded due to flags\"))\n",
    "\n",
    "    display(Markdown(\"Total number of responses excluded due to flags:\"))\n",
    "    if context=='rsmtool':\n",
    "        display(Markdown(\"Training set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_train, pct_excluded_train, len(df_train_orig))))\n",
    "    display(Markdown(\"Evaluation set: {} responses ({:.1f}% of the original {} responses)\".format(num_excluded_test, pct_excluded_test, len(df_test_orig))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a61951",
   "metadata": {},
   "source": [
    "### Responses excluded due to non-numeric feature values or scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494edf0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    num_missing_rows_train = len(df_train_excluded)\n",
    "except NameError:\n",
    "    num_missing_rows_train = 0\n",
    "\n",
    "if context == 'rsmtool':\n",
    "    pct_missing_rows_train = 100*num_missing_rows_train/len(df_train_orig)\n",
    "\n",
    "try:\n",
    "    num_missing_rows_test = len(df_test_excluded)\n",
    "except:\n",
    "    num_missing_rows_test = 0\n",
    "pct_missing_rows_test = 100*num_missing_rows_test/len(df_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206a5fe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_candidates_note = Markdown(\"Note: if a candidate had less than {} responses left for analysis after applying all filters, \"\n",
    "                                    \"all responses from that \"\n",
    "                                    \"candidate were excluded from further analysis.\".format(min_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b72b4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown(\"**Training set**\"))\n",
    "    display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_train, pct_missing_rows_train, len(df_train_orig))))\n",
    "    if num_missing_rows_train != 0:\n",
    "        train_excluded_analysis_file = join(output_dir, '{}_train_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                                  file_format))\n",
    "        df_train_excluded_analysis = DataReader.read_from_file(train_excluded_analysis_file)\n",
    "        display(HTML(df_train_excluded_analysis.to_html(classes=['sortable'], na_rep='-', float_format=float_format_func, index=False))) \n",
    "        if min_items > 0:\n",
    "            display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2f6b5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('**Evaluation set**'))\n",
    "display(Markdown('Total number of excluded responses: {} ({:.1f}% of the original {})'.format(num_missing_rows_test, pct_missing_rows_test, len(df_test_orig))))\n",
    "if num_missing_rows_test != 0:\n",
    "    test_excluded_analysis_file = join(output_dir, '{}_test_excluded_composition.{}'.format(experiment_id,\n",
    "                                                                                            file_format))\n",
    "    df_test_excluded_analysis = DataReader.read_from_file(test_excluded_analysis_file)\n",
    "    display(HTML(df_test_excluded_analysis.to_html(classes=['sortable'], na_rep='-', float_format=float_format_func, index=False)))\n",
    "    if min_items > 0:\n",
    "        display(excluded_candidates_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18d6ce",
   "metadata": {},
   "source": [
    "The rest of this report is based only on the responses that were not excluded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae08da",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if context == 'rsmtool':\n",
    "    display(Markdown('### Composition of the training and evaluation sets'))\n",
    "elif context == 'rsmeval':\n",
    "    display(Markdown('### Composition of the evaluation set'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcaa1f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the table showing candidate (speaker), prompt \n",
    "# and responses stats for training and test\n",
    "\n",
    "# feature descriptives extra table\n",
    "data_composition_file = join(output_dir, '{}_data_composition.{}'.format(experiment_id, file_format))\n",
    "df_data_desc = DataReader.read_from_file(data_composition_file)\n",
    "display(HTML(df_data_desc.to_html(classes=['sortable'], float_format=float_format_func, index=False)))\n",
    "\n",
    "try:\n",
    "    num_double_scored_responses = len(df_test_human_scores[df_test_human_scores['sc2'].notnull()])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    zeros_included_or_excluded = 'excluded' if exclude_zero_scores else 'included'\n",
    "    display(Markdown(\"Total number of double scored responses in the evaluation set\" \n",
    "                     \" used: {} (zeros {})\".format(num_double_scored_responses,\n",
    "                                                   zeros_included_or_excluded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f9683",
   "metadata": {},
   "source": [
    "## Overall descriptive feature statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442a8090",
   "metadata": {},
   "source": [
    "These values are reported before transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7c9b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives table\n",
    "desc_file = join(output_dir, '{}_feature_descriptives.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_desc = DataReader.read_from_file(desc_file, index_col=0)\n",
    "HTML(df_desc.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa4148",
   "metadata": {},
   "source": [
    "### Prevalence of recoded cases\n",
    "\n",
    "This sections shows the number and percentage of cases truncated to mean +/- 4 SD for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4eb4d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliers_file = join(output_dir, '{}_feature_outliers.{}'.format(experiment_id, file_format))\n",
    "df_outliers = DataReader.read_from_file(outliers_file, index_col=0)\n",
    "df_outliers.index.name = 'feature'\n",
    "df_outliers = df_outliers.reset_index()\n",
    "df_outliers = pd.melt(df_outliers, id_vars=['feature'])\n",
    "df_outliers = df_outliers[df_outliers.variable.str.contains(r'[ulb].*?perc')]\n",
    "\n",
    "\n",
    "# we need to increase the plot height if feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "    \n",
    "# we also need a higher aspect if we have more than 40 features\n",
    "# The aspect defines the final width of the plot (width=aspect*height).\n",
    "# We keep the width constant (9 for plots with many features or 6\n",
    "# for plots with few features) by dividing the expected width\n",
    "# by the height. \n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 3)\n",
    "\n",
    "# what's the largest value in the data frame\n",
    "maxperc = df_outliers['value'].max()\n",
    "\n",
    "# compute the limits for the graph\n",
    "limits = (0, max(2.5, maxperc))\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "    # create a barplot without a legend since we will manually\n",
    "    # add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\", \n",
    "                    palette=colors, data=df_outliers, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', '% cases truncated\\nto mean +/- 4*sd')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "\n",
    "    # add a line at 2%\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=2.0, linestyle='--', linewidth=1.5, color='black')\n",
    "\n",
    "    # add a legend with the right colors\n",
    "    legend=axis.legend(('both', 'lower', 'upper'), title='', frameon=True, fancybox=True, ncol=3)\n",
    "    legend.legend_handles[0].set_color(colors[0])\n",
    "    legend.legend_handles[1].set_color(colors[1])\n",
    "\n",
    "    # we want to try to force `tight_layout()`, but if this \n",
    "    # raises a warning, we don't want the entire notebook to fail\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_outliers.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f1a8b",
   "metadata": {},
   "source": [
    "### Feature value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7c28c",
   "metadata": {},
   "source": [
    "The following table shows additional statistics for the data. Quantiles are computed using type=3 method used in SAS. The mild outliers are defined as data points between [1.5, 3) \\* IQR away from the nearest quartile. Extreme outliers are the data points >= 3 * IQR away from the nearest quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1071b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature descriptives extra table\n",
    "desce_file = join(output_dir, '{}_feature_descriptivesExtra.{}'.format(experiment_id,\n",
    "                                                                       file_format))\n",
    "df_desce = DataReader.read_from_file(desce_file, index_col=0)\n",
    "HTML(df_desce.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9476440",
   "metadata": {},
   "source": [
    "##  Feature distributions and inter-feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98de65",
   "metadata": {},
   "source": [
    "### Training set distributions\n",
    "\n",
    "The following plot shows the distributions of the feature values in \n",
    "the training set, after transformation (if applicable), truncation \n",
    "and standardization (if applicable). The line shows the kernel density estimate. The \n",
    "human score (`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if you specified `length_column` in the config file, unless\n",
    "the column had missing values or a standard deviation <= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = features_used + ['sc1', 'spkitemid']\n",
    "df_train_preproc_selected_features = df_train_preproc[selected_columns]\n",
    "try:\n",
    "    df_train_preproc_selected_features = df_train_preproc_selected_features.merge(df_train_length, on='spkitemid')\n",
    "except NameError:\n",
    "    column_order = sorted(features_used) + ['sc1']\n",
    "else:\n",
    "    column_order = sorted(features_used) + ['sc1', 'length']\n",
    "\n",
    "df_train_preproc_melted = pd.melt(df_train_preproc_selected_features, id_vars=['spkitemid'])\n",
    "df_train_preproc_melted = df_train_preproc_melted[['variable', 'value']]\n",
    "\n",
    "# we need to reduce col_wrap and increase width if the feature names are too long\n",
    "if longest_feature_name > 10:\n",
    "    col_wrap = 2\n",
    "    # adjust height to allow for wrapping really long names. We allow 0.25 in per line\n",
    "    height = 2+(math.ceil(longest_feature_name/30)*0.25)\n",
    "    aspect = 4/height\n",
    "else:\n",
    "    col_wrap = 3\n",
    "    aspect = 1\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    with sns.axes_style('white'):\n",
    "        g = sns.FacetGrid(col='variable', data=df_train_preproc_melted, col_wrap=col_wrap, \n",
    "                          col_order=column_order, sharex=False, sharey=False, height=height, \n",
    "                          aspect=aspect)\n",
    "        g.map(sns.distplot, \"value\", color=\"grey\", kde=False)\n",
    "        for ax, cname in zip(g.axes, g.col_names):\n",
    "            labels = ax.get_xticks()\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_xticks(ax.get_xticks(), labels, rotation=90)\n",
    "            plot_title = '\\n'.join(wrap(str(cname), 30))\n",
    "            ax.set_title(plot_title)\n",
    "\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "        imgfile = join(figure_dir, '{}_distrib.svg'.format(experiment_id))\n",
    "        plt.savefig(imgfile)\n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e2092",
   "metadata": {},
   "source": [
    "### Inter-feature correlations\n",
    "\n",
    "The following table shows the Pearson correlations between all the training features\n",
    "after transformation (if applicable), truncation and standardization (if applicable). The human score \n",
    "(`sc1`) is also included. \n",
    "\n",
    "Response length (`length`) is included if \n",
    "you specified `length_column` in the config file, unless the column had missing \n",
    "values or a standard deviation <= 0. \n",
    "\n",
    "The following values are <span class=\"highlight_color\">highlighted</span>:\n",
    "- inter-feature correlations above 0.7, and\n",
    "- `sc1`-feature correlations lower than 0.1 or higher than 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c80f8a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cors_file = join(output_dir, '{}_cors_processed.{}'.format(experiment_id,\n",
    "                                                           file_format))\n",
    "df_cors = DataReader.read_from_file(cors_file, index_col=0)\n",
    "if 'length' in df_cors.columns:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c not in ['sc1', 'length']])\n",
    "    order = ['sc1', 'length'] + feature_columns\n",
    "else:\n",
    "    feature_columns = sorted([c for c in df_cors.columns if c != 'sc1'])\n",
    "    order = ['sc1'] + feature_columns\n",
    "    \n",
    "df_cors = df_cors.reindex(index=order, columns=order)\n",
    "\n",
    "# wrap the column names if the feature names are very long\n",
    "if longest_feature_name > 10:\n",
    "    column_names = ['\\n'.join(wrap(c, 10)) for c in order]\n",
    "else:\n",
    "    column_names = order\n",
    "    \n",
    "df_cors.columns = column_names\n",
    "\n",
    "# apply two different formatting to the columns according\n",
    "# to two different thresholds. The first one highlights all\n",
    "# inter-feature correlations > 0.7 (so, not including sc1)\n",
    "# and the second highlights all sc1-X correlations lower\n",
    "# than 0.1 and higher than 0.7. We will use red for the\n",
    "# first formatting and blue for the second one. \n",
    "formatter1 = partial(color_highlighter, low=-1, high=0.7)\n",
    "formatter2 = partial(color_highlighter, low=0.1, high=0.7)\n",
    "\n",
    "formatter_dict = {c: formatter1 for c in column_names if not c == 'sc1'}\n",
    "formatter_dict.update({'sc1': formatter2})\n",
    "\n",
    "HTML(df_cors.to_html(classes=['sortable'], formatters=formatter_dict, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355126ea",
   "metadata": {},
   "source": [
    "### Marginal and partial correlations\n",
    "\n",
    "The plot below shows correlations between truncated and standardized (if applicable) values of each feature against human score. The first bar (`Marginal`) in each case shows Pearson's correlation. The second bar (`Partial - all`) shows partial correlations after controlling for all other variables. If you specified `length_column` in the config file, a third bar (`Partial - length`) will show partial correlations of each feature against the human score after controlling for length. The dotted lines correspond to *r* = 0.1 and *r* = 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ccc3f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and merge the score correlations \n",
    "margcor_file = join(output_dir, '{}_margcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "pcor_file = join(output_dir, '{}_pcor_score_all_data.{}'.format(experiment_id, file_format))\n",
    "\n",
    "df_margcor = DataReader.read_from_file(margcor_file, index_col=0)\n",
    "df_pcor = DataReader.read_from_file(pcor_file, index_col=0)\n",
    "\n",
    "# check if we have length partial correlations\n",
    "pcor_no_length_file = join(output_dir, '{}_pcor_score_no_length_all_data.{}'.format(experiment_id,\n",
    "                                                                                    file_format))\n",
    "with_length = exists(pcor_no_length_file)\n",
    "if with_length:\n",
    "    df_pcor_no_length = DataReader.read_from_file(pcor_no_length_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data'], \n",
    "                             df_pcor_no_length.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all', 'partial_length']\n",
    "    num_entries = 3\n",
    "    labels = ('Marginal', 'Partial - all', 'Partial - length')\n",
    "\n",
    "else:\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], \n",
    "                             df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.columns = ['marginal', 'partial_all']\n",
    "    num_entries = 2\n",
    "    labels = ('Marginal', 'Partial (all)')\n",
    "\n",
    "df_mpcor.index.name = 'feature'\n",
    "df_mpcor = df_mpcor.reset_index()\n",
    "df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "# we need to change the plot height if the feature names are long\n",
    "if longest_feature_name > 10:\n",
    "    height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "else:\n",
    "    height = 3\n",
    "        \n",
    "# we need a higher aspect if we have more than 40 features\n",
    "aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", num_entries)\n",
    "\n",
    "# check for any negative correlations\n",
    "limits = (0, 1)\n",
    "if len(df_mpcor[df_mpcor.value < 0]):\n",
    "    limits = (-1, 1)\n",
    "\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # generate a bar plot but without the legend since we will\n",
    "    # manually add one later\n",
    "    p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_mpcor, height=height, \n",
    "                    aspect=aspect, legend=False)\n",
    "    p.set_axis_labels('', 'Correlation with score')\n",
    "    p.set_xticklabels(rotation=90)\n",
    "    p.set(ylim=limits)\n",
    "    \n",
    "    # add a line at 0.1 and 0.7\n",
    "    axis = p.axes[0][0]\n",
    "    axis.axhline(y=0.1, linestyle='--', linewidth=0.5, color='black');\n",
    "    axis.axhline(y=0.7, linestyle='--', linewidth=0.5, color='black');\n",
    "\n",
    "    # create the legend manually with the right colors\n",
    "    legend = axis.legend(labels=labels, title='', frameon=True, \n",
    "                         fancybox=True, ncol=num_entries)\n",
    "    for i in range(num_entries):\n",
    "        legend.legend_handles[i].set_color(colors[i]);\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_cors_score.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ddbcb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_margcor_file = join(output_dir, '{}_margcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                           file_format))\n",
    "len_pcor_file = join(output_dir, '{}_pcor_length_all_data.{}'.format(experiment_id,\n",
    "                                                                     file_format))\n",
    "if exists(len_margcor_file) and exists(len_pcor_file):\n",
    "\n",
    "    if standardize_features:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"standardized values of each feature against length.\"))\n",
    "    else:\n",
    "        display(Markdown(\"The plot below shows the same correlations between truncated and \"\n",
    "                         \"un-standardized values of each feature against length.\"))\n",
    "\n",
    "    df_margcor = DataReader.read_from_file(len_margcor_file, index_col=0)\n",
    "    df_pcor = DataReader.read_from_file(len_pcor_file, index_col=0)\n",
    "    df_mpcor = pd.DataFrame([df_margcor.loc['All data'], df_pcor.loc['All data']]).transpose()\n",
    "    df_mpcor.index.name = 'feature'\n",
    "    df_mpcor.columns = ['marginal', 'partial']\n",
    "    df_mpcor = df_mpcor.reset_index()\n",
    "    df_mpcor = pd.melt(df_mpcor, id_vars=['feature'])\n",
    "\n",
    "    # we need to change the plot height if the feature names are long\n",
    "    if longest_feature_name > 10:\n",
    "        height = 3 + math.ceil((longest_feature_name - 10)/10)\n",
    "    else:\n",
    "        height = 3\n",
    "        \n",
    "    # we need a higher aspect if we have more than 40 features\n",
    "    aspect = 9/height if len(features_used) > 40 else 6/height\n",
    "\n",
    "\n",
    "    # check for any negative correlations\n",
    "    limits = (0, 1)\n",
    "    if len(df_mpcor[df_mpcor.value < 0]):\n",
    "        limits = (-1, 1)\n",
    "\n",
    "    # get the colors for the plot\n",
    "    colors = sns.color_palette(\"Greys\", 2)\n",
    "        \n",
    "    with sns.axes_style('whitegrid'):\n",
    "        \n",
    "        # create a barplot but without the legend since\n",
    "        # we will manually add one later\n",
    "        p = sns.catplot(x=\"feature\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                        palette=colors, data=df_mpcor, height=height, \n",
    "                        aspect=aspect, legend=False)\n",
    "        p.set_axis_labels('', 'Correlation with length')\n",
    "        p.set_xticklabels(rotation=90)\n",
    "        p.set(ylim=limits)\n",
    "\n",
    "        # create the legend manually with the right colors\n",
    "        axis = p.axes[0][0]\n",
    "        legend = axis.legend(labels=('Marginal', 'Partial  - all'), title='', \n",
    "                             frameon=True, fancybox=True, ncol=2)\n",
    "        legend.legend_handles[0].set_color(colors[0]);\n",
    "        legend.legend_handles[1].set_color(colors[1]);\n",
    "        imgfile = join(figure_dir, '{}_cors_length.svg'.format(experiment_id))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            plt.tight_layout(h_pad=1.0)\n",
    "\n",
    "        plt.savefig(imgfile) \n",
    "        if use_thumbnails:\n",
    "            show_thumbnail(imgfile, next(id_generator))\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1c4cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = ['## Consistency']\n",
    "\n",
    "consistency_file = join(output_dir, '{}_consistency.{}'.format(experiment_id, file_format))\n",
    "degradation_file = join(output_dir, '{}_degradation.{}'.format(experiment_id, file_format))\n",
    "disattenuation_file = join(output_dir, '{}_disattenuated_correlations.{}'.format(experiment_id, file_format))\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id,\n",
    "                                                 file_format))\n",
    "confmat_h1h2_file = join(output_dir, '{}_confMatrix_h1h2.{}'.format(experiment_id, file_format))\n",
    "\n",
    "if exists(consistency_file) and exists(degradation_file) and exists(disattenuation_file):\n",
    "    df_consistency = DataReader.read_from_file(consistency_file, index_col=0)\n",
    "    df_degradation = DataReader.read_from_file(degradation_file, index_col=0)\n",
    "    df_dis_corrs = DataReader.read_from_file(disattenuation_file, index_col=0)\n",
    "    df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "\n",
    "    markdown_strs.append('*Note: this section assumes that the score used for evaluating machine scores '\n",
    "                         'is the score assigned by the first rater.*')\n",
    "    markdown_strs.append('### Human-human agreement')\n",
    "    markdown_strs.append(\"This table shows the human-human agreement on the \"\n",
    "                         \"double-scored evaluation data.\")\n",
    "    if continuous_human_score:\n",
    "        markdown_strs.append('For the computation of `kappa` and `wtkappa` '\n",
    "                             'human scores have beeen rounded to the nearest integer.')\n",
    "        \n",
    "    markdown_strs.append(\"The following are <span class='highlight_color'>highlighted </span>: \")\n",
    "    markdown_strs.append(' - Exact agreement (`exact_agr`) < 50%')\n",
    "    markdown_strs.append(' - Adjacent agreement (`adj_agr`) < 95%')\n",
    "    markdown_strs.append(' - Quadratic weighted kappa (`wtkappa`) < 0.7')\n",
    "    markdown_strs.append(' - Pearson correlation (`corr`) < 0.7')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_exact_agr = partial(color_highlighter, low=50, high=100)\n",
    "    formatter_adj_agr = partial(color_highlighter, low=95, high=100)\n",
    "    formatter_wtkappa_corr = partial(color_highlighter, low=0.7)\n",
    "    formatter_dict = {'exact_agr': formatter_exact_agr, \n",
    "                      'adj_agr': formatter_adj_agr,\n",
    "                      'wtkappa': formatter_wtkappa_corr, \n",
    "                      'corr': formatter_wtkappa_corr}\n",
    "    display(HTML(df_consistency.to_html(index=False,\n",
    "                                        escape=False,\n",
    "                                        float_format=float_format_func,\n",
    "                                        formatters=formatter_dict)))\n",
    "    \n",
    "    markdown_strs = ['### Degradation']\n",
    "    markdown_strs.append('The next table shows the degradation in the evaluation metrics '\n",
    "                         '(`diff`) when comparing the machine (`H-M`) to a second human (`H-H`). '\n",
    "                         'A positive degradation value indicates better human-machine performance. '\n",
    "                         'Note that the human-machine agreement is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'agreement is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following degradation values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `corr` < -0.1')\n",
    "    markdown_strs.append(' - `wtkappa` < -0.1')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    df_eval_for_degradation = df_eval[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation = pd.concat([df_consistency]*len(df_eval), sort=True)\n",
    "    df_consistency_for_degradation = df_consistency_for_degradation[df_degradation.columns].copy()\n",
    "    df_consistency_for_degradation.index = df_eval_for_degradation.index\n",
    "\n",
    "    df_consistency_for_degradation['type'] = 'H-H'\n",
    "    df_eval_for_degradation['type'] = 'H-M'\n",
    "    df_degradation['type'] = 'diff'\n",
    "\n",
    "    df = pd.concat([df_consistency_for_degradation,\n",
    "                    df_eval_for_degradation,\n",
    "                    df_degradation], sort=True)\n",
    "    df = df[['type','corr', 'kappa', 'wtkappa', 'exact_agr', 'adj_agr', 'SMD']]\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index(['index', 'type']).sort_index(level='index')\n",
    "    df.index.names = [None, None]\n",
    "    \n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_corr = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_wtkappa = partial(color_highlighter, low=-0.1, high=100)\n",
    "    formatter_dict = {'corr': formatter_corr, 'wtkappa': formatter_wtkappa}\n",
    "    display(HTML(df.to_html(float_format=float_format_func, \n",
    "                            formatters=formatter_dict, escape=False)))\n",
    "\n",
    "    if exists(confmat_h1h2_file):\n",
    "        markdown_strs = ['### Human-human confusion matrix']\n",
    "        markdown_strs.append(\"Confusion matrix using the two human scores (rows=human #1, columns=human #2).\"\n",
    "                             \" Note that the matrix is computed on the subset of responses that were \"\n",
    "                             \"double-scored.\")\n",
    "        if continuous_human_score:\n",
    "            markdown_strs.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        df_confmat_h1h2 = DataReader.read_from_file(confmat_h1h2_file, index_col=0)\n",
    "        display(HTML(df_confmat_h1h2.to_html(float_format=float_format_func, \n",
    "                                             formatters=formatter_dict, escape=False)))\n",
    "\n",
    "    markdown_strs = ['### Disattenuated correlations']\n",
    "    markdown_strs.append('The next table shows the correlations between human and machine scores, '\n",
    "                         'the correlations between two human scores, '  \n",
    "                         'and disattenuated correlations between human and machine scores computed as '\n",
    "                         'human-machine correlations divided by the square root of human-human '\n",
    "                         'correlation. '\n",
    "                         'Note that the human-machine correlation is computed on the full '\n",
    "                         'dataset (to get a reliable estimate) whereas the human-human '\n",
    "                         'correlation is computed on the subset of responses that were double-scored.')\n",
    "    markdown_strs.append(\"\\nThe following values are \"\n",
    "                         \"<span class='highlight_color'>highlighted</span>\")\n",
    "    markdown_strs.append(' - `disattenuated_corr` < 0.9')\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    # display the HTML for the table with the various formatters\n",
    "    formatter_dis_corr = partial(color_highlighter, low=0.9)\n",
    "    formatter_dict = {'corr_disattenuated': formatter_dis_corr}\n",
    "    display(HTML(df_dis_corrs.to_html(index=True,\n",
    "                                      escape=False,\n",
    "                                      classes=['sortable'],\n",
    "                                      float_format=float_format_func,\n",
    "                                      formatters=formatter_dict)))\n",
    "else:  \n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` which is necessary to compute \"\n",
    "                         \"consistency metrics.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d4d48",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31267842",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objective_used = skll_objective if skll_objective else 'neg_mean_squared_error'\n",
    "intro_markdown_str = f\"\"\"### Characteristics \n",
    "\n",
    "- Model: **{model_name}**\n",
    "- Number of features: **{len(features_used)}**\n",
    "- Tuning objective: **{objective_used}**\n",
    "\"\"\"\n",
    "\n",
    "if skll_fixed_parameters:\n",
    "    intro_markdown_str += \"- Custom fixed parameters:\"\n",
    "    for parameter, value in skll_fixed_parameters.items():\n",
    "        intro_markdown_str += f\"\\n    - `{parameter}` : **{value!r}**\"\n",
    "\n",
    "display(Markdown(intro_markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eabc684",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skll_linear_models = ['Ridge', 'BayesianRidge', 'LinearSVR', 'Lasso', 'Lars', 'HuberRegressor', 'TheilSenRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21986d19",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_markdown_str = \"\"\"### Feature weights\n",
    "\n",
    "Here are the feature weights as learned by the model.\"\"\"\n",
    "\n",
    "if model_name in skll_linear_models:\n",
    "    display(Markdown(weights_markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01d037",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if model_name in skll_linear_models:\n",
    "\n",
    "    from rsmtool.modeler import Modeler\n",
    "    from six import iteritems\n",
    "    from sklearn.svm import SVR\n",
    "    \n",
    "    model_file = join(output_dir, '{}.model'.format(experiment_id))\n",
    "    learner = Modeler.load_from_file(model_file).learner\n",
    "    \n",
    "    # get the coefficients and the intercept\n",
    "    weights = {}\n",
    "    coef = learner.model.coef_\n",
    "    intercept = {'_intercept_': learner.model.intercept_}\n",
    "\n",
    "    # convert SVR coefficient format (1 x matrix) to array\n",
    "    if isinstance(learner._model, SVR):\n",
    "        coef = coef.toarray()[0]\n",
    "\n",
    "    # inverse transform to get indices for before feature selection\n",
    "    coef = learner.feat_selector.inverse_transform(coef.reshape(1, -1))[0]\n",
    "    for feat, idx in iteritems(learner.feat_vectorizer.vocabulary_):\n",
    "        if coef[idx]:\n",
    "            weights[feat] = coef[idx]\n",
    "\n",
    "    # Some learners (e.g. LinearSVR) may return a list of intercepts\n",
    "    if isinstance(intercept['_intercept_'], np.ndarray):\n",
    "        intercept_list = [\"%.12f\" % i for i in intercept['_intercept_']]\n",
    "        print(\"intercept = {}\".format(intercept_list))\n",
    "    else:\n",
    "        print(\"intercept = {:.12f}\".format(intercept['_intercept_']))\n",
    "    print()\n",
    "        \n",
    "    print(\"Number of nonzero features:\", len(weights))\n",
    "    weight_items = iteritems(weights)\n",
    "    for feat, val in sorted(weight_items, key=lambda x: -abs(x[1])):\n",
    "        print(\"{:.12f}\\t{}\".format(val, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec5d76",
   "metadata": {},
   "source": [
    "## Evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ecd62",
   "metadata": {},
   "source": [
    "### Overall association statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ec072",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = (\"The tables in this section show the standard association metrics between \"\n",
    "                \"*observed* human scores and different types of machine scores. \"\n",
    "                \"These results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                \"are truncated to [{}, {}]. `raw_trim_round` scores are computed by first truncating \"\n",
    "                \"and then rounding the predicted score. Scaled scores are computed by re-scaling \"\n",
    "                \"the predicted scores using mean and standard deviation of human scores as observed \"\n",
    "                \"on the training data and mean and standard deviation of machine scores as predicted \"\n",
    "                \"for the training set.\".format(min_score, max_score))\n",
    "display(Markdown(markdown_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cea94",
   "metadata": {},
   "source": [
    "#### Descriptive holistic score statistics\n",
    "\n",
    "The table shows distributional properties of human and system scores. SMD values lower then -0.15 or higher than 0.15 are <span class=\"highlight_color\">highlighted</span>.\n",
    "\n",
    "*Please note that for raw scores, SMD values are likely to be affected by possible differences in scale.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd11bc5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "eval_file = join(output_dir, '{}_eval.{}'.format(experiment_id, file_format))\n",
    "df_eval = DataReader.read_from_file(eval_file, index_col=0)\n",
    "distribution_columns = ['N', 'h_mean', 'sys_mean', 'h_sd',  'sys_sd', 'h_min', 'sys_min', 'h_max', 'sys_max', 'SMD']\n",
    "association_columns = ['N'] + [column for column in df_eval.columns if not column in distribution_columns]\n",
    "df_distribution = df_eval[distribution_columns]\n",
    "df_association = df_eval[association_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc9f17",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "formatter = partial(color_highlighter, low=-0.15, high=0.15)\n",
    "HTML('<span style=\"font-size:95%\">'+ df_distribution.to_html(classes=['sortable'], \n",
    "                                                             escape=False,\n",
    "                                                             formatters={'SMD': formatter},\n",
    "                                                             float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3959052",
   "metadata": {},
   "source": [
    "#### Association statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ['The table shows the standard association metrics between human scores and machine scores.']\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` both human and machine scores are rounded.\")\n",
    "else:\n",
    "    markdown_str.append(\"Note that for computation of `kappa` all machine scores are rounded.\")\n",
    "\n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fc0dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width=10\n",
    "HTML('<span style=\"font-size:95%\">'+ df_association.to_html(classes=['sortable'], \n",
    "                                                            escape=False,\n",
    "                                                            float_format=float_format_func) + '</span>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3259418",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = [\"Confusion matrix using {}, trimmed, and rounded scores and human scores (rows=human, columns=system).\".format(raw_or_scaled)]\n",
    "\n",
    "if continuous_human_score:\n",
    "    markdown_str.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "            \n",
    "Markdown('\\n'.join(markdown_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e094585",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confmat_file = join(output_dir, '{}_confMatrix.{}'.format(experiment_id, file_format))\n",
    "df_confmat = DataReader.read_from_file(confmat_file, index_col=0)\n",
    "df_confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef50fc",
   "metadata": {},
   "source": [
    "### Distribution of human and machine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba00407",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "markdown_strs = [\"The histogram and the table below show the distibution of \"\n",
    "                 \"human scores and {}, trimmed, and rounded machine scores \"\n",
    "                 \"(as % of all responses).\".format(raw_or_scaled)]\n",
    "markdown_strs.append(\"Differences in the table between human and machine distributions \"\n",
    "                     \"larger than 5 percentage points are <span class='highlight_color'>highlighted</span>.\")\n",
    "if continuous_human_score:\n",
    "    markdown_strs.append(\"Note: Human scores have beeen rounded to the nearest integer.\")\n",
    "    \n",
    "display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267007e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoredist_file = join(output_dir, '{}_score_dist.{}'.format(experiment_id, file_format))\n",
    "df_scoredist = DataReader.read_from_file(scoredist_file, index_col=0)\n",
    "df_scoredist_melted = pd.melt(df_scoredist, id_vars=['score'])\n",
    "df_scoredist_melted = df_scoredist_melted[df_scoredist_melted['variable'] != 'difference']\n",
    "\n",
    "# get the colors for the plot\n",
    "colors = sns.color_palette(\"Greys\", 2)\n",
    "\n",
    "with sns.axes_style('whitegrid'):\n",
    "\n",
    "    # make a barplot without a legend since we will \n",
    "    # add one manually later\n",
    "    p = sns.catplot(x=\"score\", y=\"value\", hue=\"variable\", kind=\"bar\",\n",
    "                    palette=colors, data=df_scoredist_melted, \n",
    "                    height=3, aspect=2, legend=False)\n",
    "    p.set_axis_labels('score', '% of responses')\n",
    "    \n",
    "    # add a legend with the right colors\n",
    "    axis = p.axes[0][0]\n",
    "    legend = axis.legend(labels=('Human', 'Machine'), title='', frameon=True, fancybox=True)\n",
    "    legend.legend_handles[0].set_color(colors[0])\n",
    "    legend.legend_handles[1].set_color(colors[1])\n",
    "\n",
    "    imgfile = join(figure_dir, '{}_score_dist.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab511c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formatter = partial(color_highlighter, low=0, high=5, absolute=True)\n",
    "df_html = df_scoredist.to_html(classes=['sortable'], index=False, \n",
    "                               escape=False, formatters={'difference': formatter})\n",
    "display(HTML(df_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12995f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_strs = ['### True score evaluations']\n",
    "\n",
    "raw_or_scaled = \"scaled\" if use_scaled_predictions else \"raw\"\n",
    "true_eval_file = join(output_dir, '{}_true_score_eval.{}'.format(experiment_id, file_format))\n",
    "if exists(true_eval_file): \n",
    "    df_true_eval = DataReader.read_from_file(true_eval_file, index_col=0)\n",
    "    df_true_eval.replace({np.nan: '-'}, inplace=True)\n",
    "    prmse_columns = ['N','N raters', 'N single', 'N multiple', \n",
    "                     'Variance of errors', 'True score var',\n",
    "                     'MSE true', 'PRMSE true']\n",
    "    df_prmse = df_true_eval[prmse_columns]\n",
    "\n",
    "    markdown_strs.append(\"The tables in this section show how well system scores can \"\n",
    "                         \"predict *true* scores. According to Test theory, a *true* score \"\n",
    "                         \"is a score that would have been obtained if there were no errors \"\n",
    "                         \"in measurement. While true scores cannot be observed, the variance \"\n",
    "                         \"of true scores and the prediction error can be estimated using observed \"\n",
    "                         \"human scores when multiple human ratings are available for a subset of \"\n",
    "                         \"responses.\")\n",
    "\n",
    "    if rater_error_variance is None: \n",
    "        \n",
    "        rater_variance_source = 'estimated'\n",
    "        # if we estimated rater error variance from the data,\n",
    "        # we display the variance of the two human raters\n",
    "        # so that the user can verify there is no bias\n",
    "        # We get that data from existing analyses\n",
    "        df_human_variance = df_consistency[['N', 'h1_sd', 'h2_sd']].copy()\n",
    "        df_human_variance['N_double'] = df_human_variance['N']\n",
    "        df_human_variance['h1_var (double)'] = df_human_variance['h1_sd']**2\n",
    "        df_human_variance['h2_var (double)'] = df_human_variance['h2_sd']**2\n",
    "        df_human_variance['N_total'] = df_eval.iloc[0]['N']\n",
    "        df_human_variance['h1_var (single)'] = df_eval.iloc[0]['h_sd']**2\n",
    "        df_human_variance.index = ['human']\n",
    "\n",
    "\n",
    "        if context == 'rsmtool':\n",
    "            label_column = \"test_label_column\"\n",
    "        else:\n",
    "            label_column = \"human_score_column\"\n",
    "\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores is estimated using \"\n",
    "                             \"the human ratings available for \"\n",
    "                             \"responses in the evaluation set. Note that the analyses in this \"\n",
    "                             \"section assume that the values \"\n",
    "                            \"in `{}` and `second_human_score_column` are independent scores \"\n",
    "                            \"from different raters or groups of raters. These analyses are \"\n",
    "                            \"not applicable to a situation where `{}` contains an average \"\n",
    "                            \"score from multiple raters\".format(label_column, label_column))\n",
    "\n",
    "        markdown_strs.append(\"#### Variance of human scores\")\n",
    "        markdown_strs.append(\"The table below shows variance of both sets of human scores \"\n",
    "                            \"for the whole evaluation set and for the subset of responses \"\n",
    "                            \"that were double-scored. Large differences in variance between \"\n",
    "                            \"the two human scores require further investigation.\")\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        pd.options.display.width=10\n",
    "        column_order = ['N_total', 'N_double', 'h1_var (single)', 'h1_var (double)', 'h2_var (double)']\n",
    "        display(HTML('<span style=\"font-size:95%\">'+ df_human_variance[column_order].to_html(classes=['sortable'], \n",
    "                                                                                            escape=False,\n",
    "                                                                                            float_format=float_format_func) + '</span>'))\n",
    "    else:\n",
    "        markdown_strs.append(\"In this notebook the variance of true scores was \"\n",
    "                            \"estimated using the value of rater error variance \"\n",
    "                            \"supplied by the user ({})\".format(rater_error_variance))\n",
    "        display(Markdown('\\n'.join(markdown_strs)))\n",
    "        rater_variance_source = 'supplied'\n",
    "    \n",
    "    \n",
    "    markdown_strs = [\"#### Proportional reduction in mean squared error (PRMSE)\"]\n",
    "    markdown_strs.append(\"The table shows {} variance of human rater errors, \"\n",
    "                         \"true score variance, mean squared error (MSE) and \"\n",
    "                         \"proportional reduction in mean squared error (PRMSE) for \"\n",
    "                         \"predicting a true score with system score. As for other evaluations, \"\n",
    "                         \"these results are computed on the evaluation set. `raw_trim` scores \"\n",
    "                         \"are truncated to [{}, {}]. `raw_trim_round` scores are computed \"\n",
    "                         \"by first truncating and then rounding the predicted score. Scaled scores \"\n",
    "                         \"are computed by re-scaling the predicted scores using mean and standard \"\n",
    "                         \"deviation of human scores as observed on the training data and mean and \"\n",
    "                         \"standard deviation of machine scores as predicted for the training set.\".format(rater_variance_source,\n",
    "                                                                                                          min_score,\n",
    "                                                                                                          max_score))\n",
    "    display(Markdown('\\n'.join(markdown_strs)))\n",
    "    pd.options.display.width=10\n",
    "    display(HTML('<span style=\"font-size:95%\">'+ df_prmse.to_html(classes=['sortable'], \n",
    "                                                               escape=False,\n",
    "                                                               float_format=float_format_func) + '</span>'))\n",
    "else:\n",
    "    markdown_strs.append(\"The configuration file did not specify \"\n",
    "                         \"`second_human_score_column` or `rater_error_variance`. \"\n",
    "                         \"At least one of these must be specified to compute \"\n",
    "                         \"evaluations against true scores.\")\n",
    "    display(Markdown('\\n'.join(markdown_strs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ea764",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "PCA using scaled data and singular value decomposition. This is computed using processed features after the truncation of outliers and other transformations specified in feature config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c01dc4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_file = join(output_dir, '{}_pca.{}'.format(experiment_id, file_format))\n",
    "df_pca = DataReader.read_from_file(pca_file, index_col=0)\n",
    "df_pca.sort_index(inplace=True)\n",
    "HTML(df_pca.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d82a9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcavar_file = join(output_dir, '{}_pcavar.{}'.format(experiment_id, file_format))\n",
    "df_pcavar = DataReader.read_from_file(pcavar_file, index_col=0)\n",
    "df_pcavar.sort_index(inplace=True)\n",
    "HTML(df_pcavar.to_html(classes=['sortable'], float_format=float_format_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297389a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the Scree plot\n",
    "with sns.axes_style('white'):\n",
    "    num_components = len(df_pcavar.columns)\n",
    "    labels = list(df_pcavar.columns)\n",
    "    ax = df_pcavar.transpose().plot(y='Eigenvalues', kind='line', \n",
    "                                    color='black', linestyle='dashed', marker='o', legend=False,\n",
    "                                    linewidth=1, use_index=True, xticks=range(num_components),\n",
    "                                    figsize=(11, 5), title='Scree Plot: Principal Component Analysis')\n",
    "    ax.set_ylabel('Variances')\n",
    "    ax.set_xticks(ax.get_xticks(), labels, rotation=90)\n",
    "    imgfile = join(figure_dir, '{}_pca.svg'.format(experiment_id))\n",
    "    plt.savefig(imgfile)\n",
    "    if use_thumbnails:\n",
    "        show_thumbnail(imgfile, next(id_generator))\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab832eb",
   "metadata": {},
   "source": [
    "## Links to intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically set the documentation link based on context\n",
    "if context == \"rsmtool\":\n",
    "    documentation_link = \"https://rsmtool.readthedocs.io/en/main/usage_rsmtool.html#intermediate-files\"\n",
    "else:\n",
    "    # this is rsmeval\n",
    "    documentation_link = \"https://rsmtool.readthedocs.io/en/main/advanced_usage.html#intermediate-files\"\n",
    "\n",
    "markdown_text = (\"The following intermediate files were generated as part of this experiment.\"\n",
    "\"To examine a file, click on the link. Note that the descriptions provided here are only brief \"\n",
    "\"summaries. For more detailed descriptions, please refer to the \"\n",
    "f\"[relevant RSMTool documentation]({documentation_link}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfff057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsmtool.utils.notebook import show_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files(context, output_dir, experiment_id, file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc560f3",
   "metadata": {},
   "source": [
    "## System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346f4e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "\n",
    "# People might not know what 'Darwin' is, so we should replace that with 'Mac OS X'\n",
    "if system_name == 'Darwin':\n",
    "    system_name = 'Mac OS X'\n",
    "    \n",
    "# get the architecture\n",
    "architecture = platform.architecture()[0]\n",
    "\n",
    "# get the rsmtool version\n",
    "rsmtool_version_str = '.'.join(map(str, rsmtool_version))\n",
    "\n",
    "display(Markdown('This report was generated using rsmtool v{} on a '\n",
    "                 '{} computer running {}.'.format(rsmtool_version_str, \n",
    "                                                  architecture, \n",
    "                                                  system_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8552e01",
   "metadata": {},
   "source": [
    "### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921d93f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib.metadata import distributions\n",
    "package_names = '\\n'.join(sorted([\"%s==%s\" % (d.metadata['name'], d.version) for d in distributions()]))\n",
    "display(HTML('<div id=\"packages\"><pre>{}</pre></div>'.format(package_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33328a8a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Code to dynamically generate table of contents at the top of the HTML file\n",
    "var tocEntries = ['<ul>'];\n",
    "var anchors = $('a.anchor-link');\n",
    "var headingTypes = $(anchors).parent().map(function() { return $(this).prop('tagName')});\n",
    "var headingTexts = $(anchors).parent().map(function() { return $(this).text()});\n",
    "var subList = false;\n",
    "\n",
    "$.each(anchors, function(i, anch) {\n",
    "    var hType = headingTypes[i];\n",
    "    var hText = headingTexts[i];\n",
    "    hText = hText.substr(0, hText.length - 1);\n",
    "    if (hType == 'H2') {\n",
    "        if (subList) {\n",
    "            tocEntries.push('</ul>')\n",
    "            subList = false;\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "    else if (hType == 'H3') {\n",
    "        if (!subList) {\n",
    "            subList = true;\n",
    "            tocEntries.push('<ul>')\n",
    "        }\n",
    "        tocEntries.push('<li><a href=\"' + anch + '\"</a>' + hText + '</li>')\n",
    "    }\n",
    "});\n",
    "tocEntries.push('</ul>')\n",
    "$('#toc').html(tocEntries.join(' '))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
