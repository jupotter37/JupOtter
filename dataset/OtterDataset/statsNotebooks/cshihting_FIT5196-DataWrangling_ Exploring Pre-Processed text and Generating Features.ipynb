{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Pre-Processed text and Generating Features\n",
    "One of the challenges of text analysis is to convert unstructured and semi-structured text into a structured representation. This must be done prior to carrying out any text analysis tasks. This chapter will show you \n",
    "how to put some of those basic steps discussed in the previous chapter together to generate different vector\n",
    "representations for some given text. You will learn how to compute some basic statistics for text, and how to extract features rather than unigrams.\n",
    "\n",
    "\n",
    "## 1. Counting Vocabulary by Selecting Tokens of Interest\n",
    "Two important concepts that should be mentioned first are **type** and **token**.\n",
    "Here are the definitions of the two terms, quoted from \"[tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\",  \n",
    ">a **token** is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing;\n",
    "\n",
    "> a **type** is the class of all tokens containing the same character sequence. \n",
    "\n",
    "A *type* is also a vocabulary entry. In other words, a vocabulary consists of a number of word types.\n",
    "The distinction between a type and its tokens is a distinction that separates a descriptive concept from\n",
    "its particular concrete instances. \n",
    "This is quite similar to the distinction in object-oriented programming between classes and objects.\n",
    "In this section, you are going to learn how to count types in a given corpus by further processing the text.\n",
    "\n",
    "The document collection that we are going to use is a set of Reuters articles that comes with NLTK.\n",
    "It contains 10788 Reuters articles in total and has been split into two subsets, training and testing.\n",
    "Although this collection has already been pre-processed (e.g., you can access the text at different levels, like raw text, tokens, and sentences),\n",
    "we would still like to demonstrate how to put some of the basis text preprocessing steps together and process the raw Reuters articles step by step.\n",
    "First, import the main Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tokenizer works on a per document level, we can parallelize the process of tokenization with Python's multi-processing module. Please refer to its official documentation [here](https://docs.python.org/2/library/multiprocessing.html).\n",
    "In the following code, we wrap tokenization in a Python function, and then\n",
    "create a pool of four worker processes with the Python Pool class.\n",
    "The <font color=\"blue\">Pool.map()</font>, a parallel equivalent of the  built-in  <font color=\"blue\">map()</font> function, takes one iterable argument.\n",
    "The iterable will be split into a number of chunks, each of which will be submitted to a process in the process pool.\n",
    "Each process will apply a callable function to each element in the chunk it has received.\n",
    "Note that you can replace the NLTK tokenizer with the one you implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizeRawData(fileid):\n",
    "    \"\"\"\n",
    "        This function tokenizes a raw text document.\n",
    "    \"\"\"\n",
    "    raw_article = reuters.raw(fileid).lower() # cover all words to lowercase\n",
    "    tokenised_article = nltk.tokenize.word_tokenize(raw_article) # tokenize each Reuters articles\n",
    "    return (fileid, tokenised_article) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/eileen/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "nltk.download('reuters')\n",
    "pool = mp.Pool(processes=4) # Build a pool of 4 processess \n",
    "tokenized_reuters = dict(pool.map(tokenizeRawData, reuters.fileids()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above parallelized code does not work on your computer, you can try the following code running with a signle thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenized_reuters =  dict(tokenizeRawData(fileid) for fileid in reuters.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Removing Words with Non-alphabetic Characters\n",
    "The NLTK's built-in  <font color=\"blue\">word_tokenize</font> function tokenizes a string to split off punctuation other than periods.\n",
    "Not only does it return words with alphanumerical characters, but also punctuations. \n",
    "Let's take a look at one Reuters articles,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partnership',\n",
       " 'cuts',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'international',\n",
       " '&',\n",
       " 'lt',\n",
       " ';',\n",
       " 'erc',\n",
       " '>',\n",
       " 'parsow',\n",
       " 'partnership',\n",
       " 'ltd',\n",
       " ',',\n",
       " 'a',\n",
       " 'nevada',\n",
       " 'investment',\n",
       " 'partnership',\n",
       " ',',\n",
       " 'said',\n",
       " 'it',\n",
       " 'lowered',\n",
       " 'its',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'international',\n",
       " 'inc',\n",
       " 'to',\n",
       " '343,500',\n",
       " 'shares',\n",
       " 'or',\n",
       " '8.3',\n",
       " 'pct',\n",
       " 'of',\n",
       " 'the',\n",
       " 'total',\n",
       " 'outstanding',\n",
       " 'common',\n",
       " 'stock',\n",
       " ',',\n",
       " 'from',\n",
       " '386,300',\n",
       " 'shares',\n",
       " ',',\n",
       " 'or',\n",
       " '9.3',\n",
       " 'pct',\n",
       " '.',\n",
       " 'in',\n",
       " 'a',\n",
       " 'filing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'securities',\n",
       " 'and',\n",
       " 'exchange',\n",
       " 'commission',\n",
       " ',',\n",
       " 'parsow',\n",
       " 'said',\n",
       " 'it',\n",
       " 'sold',\n",
       " '42,800',\n",
       " 'erc',\n",
       " 'common',\n",
       " 'shares',\n",
       " 'between',\n",
       " 'jan',\n",
       " '9',\n",
       " 'and',\n",
       " 'march',\n",
       " '2',\n",
       " 'at',\n",
       " 'prices',\n",
       " 'ranging',\n",
       " 'from',\n",
       " '12.125',\n",
       " 'to',\n",
       " '14.50',\n",
       " 'dlrs',\n",
       " 'each',\n",
       " '.',\n",
       " 'the',\n",
       " 'partnership',\n",
       " 'said',\n",
       " 'its',\n",
       " 'dealings',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'stock',\n",
       " 'are',\n",
       " 'for',\n",
       " 'investment',\n",
       " 'purposes',\n",
       " 'and',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'intention',\n",
       " 'of',\n",
       " 'seeking',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reuters['training/1684']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Assume that we are interested in words containing alphabetic characters only \n",
    "and would like to remove all the other tokens\n",
    "that contain digits, punctuation and the other symbols.\n",
    "Removing all the non-alphabetic words from the vocabulary is\n",
    "usually required in some text analysis tasks, such as Topic Modelling that\n",
    "learns the semantic meaning of documents.\n",
    "It can be easily done with the  <font color=\"blue\">isalpha()</font> function.\n",
    " <font color=\"blue\">isalpha()</font>\n",
    "checks whether the string consists of alphabetic characters only or not.\n",
    "This method returns true if all characters in the string are in the alphabet and there \n",
    "is at least one character, false otherwise.\n",
    "If you would like to keep all words with alphanumeric characters, you can use\n",
    " <font color=\"blue\">isalnum()</font>. Refer to Python's [built-in types](https://docs.python.org/2/library/stdtypes.html) for more detail.\n",
    "Indeed, you can construct your tokenizer in a way such that the tokenizer only extracts words with either \n",
    "alphabetic or alphanumerical characters, as we discussed in the previous chapter.\n",
    "We will leave this as a simple exercise for you to do on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partnership',\n",
       " 'cuts',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'international',\n",
       " 'lt',\n",
       " 'erc',\n",
       " 'parsow',\n",
       " 'partnership',\n",
       " 'ltd',\n",
       " 'a',\n",
       " 'nevada',\n",
       " 'investment',\n",
       " 'partnership',\n",
       " 'said',\n",
       " 'it',\n",
       " 'lowered',\n",
       " 'its',\n",
       " 'stake',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'international',\n",
       " 'inc',\n",
       " 'to',\n",
       " 'shares',\n",
       " 'or',\n",
       " 'pct',\n",
       " 'of',\n",
       " 'the',\n",
       " 'total',\n",
       " 'outstanding',\n",
       " 'common',\n",
       " 'stock',\n",
       " 'from',\n",
       " 'shares',\n",
       " 'or',\n",
       " 'pct',\n",
       " 'in',\n",
       " 'a',\n",
       " 'filing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'securities',\n",
       " 'and',\n",
       " 'exchange',\n",
       " 'commission',\n",
       " 'parsow',\n",
       " 'said',\n",
       " 'it',\n",
       " 'sold',\n",
       " 'erc',\n",
       " 'common',\n",
       " 'shares',\n",
       " 'between',\n",
       " 'jan',\n",
       " 'and',\n",
       " 'march',\n",
       " 'at',\n",
       " 'prices',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'to',\n",
       " 'dlrs',\n",
       " 'each',\n",
       " 'the',\n",
       " 'partnership',\n",
       " 'said',\n",
       " 'its',\n",
       " 'dealings',\n",
       " 'in',\n",
       " 'erc',\n",
       " 'stock',\n",
       " 'are',\n",
       " 'for',\n",
       " 'investment',\n",
       " 'purposes',\n",
       " 'and',\n",
       " 'it',\n",
       " 'has',\n",
       " 'no',\n",
       " 'intention',\n",
       " 'of',\n",
       " 'seeking',\n",
       " 'control',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, v in tokenized_reuters.items():\n",
    "    tokenized_reuters[k] = [word for word in v if word.isalpha()]\n",
    "\n",
    "tokenized_reuters['training/1684']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have derived much cleaner text for each Reuters article.\n",
    "Let's check how many types we have in the whole corpus and the lexical diversity (i.e., the average number \n",
    "of times a type apprearing in the collection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  27944 \n",
      "Total number of tokens:  1274688 \n",
      "Lexical diversity:  45.61580303464071\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from itertools import chain\n",
    "\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 1.27 million word tokens in the tokenized Reuters corpus.\n",
    "The vocabulary size is 27,944, which is still quite large according to our knowledge of this corpus.\n",
    "The lexical diversity tells us that words occur on average about 46 times each.\n",
    "You might think that\n",
    "there could still be words that occur very frequently, such as stopwords,\n",
    "and those that only occur once or twice.\n",
    "For example, if an article \"the\" appears in almost\n",
    "every document in a corpus,\n",
    "it might not help you at all and would only contribute noise.\n",
    "Similarly if a word appears only once in a corpus or only in one document of the corpus,\n",
    "it could carry little useful information for downstream analysis.\n",
    "Therefore, we would better remove those words from the vocabulary, which\n",
    "will benefit the text analysis algorithms in terms of reducing running time and\n",
    "memory requirement, and improving their performance.\n",
    "To do so, we need to further explore the corpus by computing some simple\n",
    "statistics.\n",
    "\n",
    "Note that we introduced two new Python libraries in the code above.\n",
    "They are\n",
    "[`__future__`](https://docs.python.org/2/library/__future__.html) \n",
    "and [`itertools`](https://docs.python.org/2/library/itertools.html). \n",
    "The first statement in the code makes sure that Python switches to \n",
    "always yielding a real result.\n",
    "Thus if you divide two integer values, you will not get for example. \n",
    "````\n",
    "    1/2 = 0\n",
    "    3/2 = 1\n",
    "````\n",
    "Instead, you will have\n",
    "```\n",
    "    1/2 = 0.5\n",
    "    3/2 = 1.5\n",
    "```\n",
    "The second statement imported a  <font color=\"blue\">chain()</font> iterator from the  <font color=\"blue\">itertools</font> module.\n",
    "We use the iterator to join all the words in all the Reuters articles together.\n",
    "It works as\n",
    "```python\n",
    "   for wordList in tokenized_reuters.values():\n",
    "       for word in wordList:\n",
    "           yield word\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Removing the Most and Less Frequent Words\n",
    "It is quite useful for us to identify the words that are most informative about the sematic \n",
    "meaning of the text regardless of syntax.\n",
    "One common statistics often used in text processing is frequency distribution.\n",
    "It can tell us how frequent a word is in a given corpus in terms of either term frequency or document frequency.\n",
    "Term frequency counts the number of times a word occurs in the whole corpus regardless which document it is in.\n",
    "Frequency distribution based on term frequency tells us how the total number of word tokens are distributed across all the types.\n",
    "NLTK provides a built-in function `FreqDist` to compute this distribution directly from a set of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "fd_1 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words in the corpus?\n",
    "we can use the  <font color=\"blue\">most_common</font> function to print out the most frequent words together with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69245),\n",
       " ('of', 36749),\n",
       " ('to', 36275),\n",
       " ('in', 29217),\n",
       " ('and', 25616),\n",
       " ('said', 25381),\n",
       " ('a', 24723),\n",
       " ('mln', 18598),\n",
       " ('vs', 14332),\n",
       " ('for', 13420),\n",
       " ('dlrs', 12329),\n",
       " ('it', 11087),\n",
       " ('pct', 9771),\n",
       " ('on', 9094),\n",
       " ('lt', 8696),\n",
       " ('cts', 8308),\n",
       " ('from', 8217),\n",
       " ('is', 7673),\n",
       " ('that', 7538),\n",
       " ('its', 7402),\n",
       " ('by', 7082),\n",
       " ('at', 7014),\n",
       " ('net', 6986),\n",
       " ('year', 6687),\n",
       " ('be', 6354)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_1.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above contains the 25 most frequent words.\n",
    "You can see that it is mostly dominated by the little words of the English language which have important grammatical roles.\n",
    "Those words are articles, prepositions, pronouns, auxiliary webs, conjunctions, etc.\n",
    "They are usually referred to as function words in linguistics, which tell us nothing about \n",
    "the meaning of the text.\n",
    "What proportion of the text is taken up with such words?\n",
    "We can generate a cumulative frequency plot for them\n",
    "using  <font color=\"blue\">fd.plot(25, cumulative=True)</font>.\n",
    "If you set  <font color=\"blue\">cumulative</font> to  <font color=\"blue\">False</font>, \n",
    "it will plot the frequencies of these 25 words.\n",
    "These 25 words account for about 33% of the while Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEXCAYAAAB76ulbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a210c36d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd_1.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words in terms of document frequency?\n",
    "Here we are going to count how many documents a word appears in, which is referred to as document frequency.\n",
    "Instead of writing nested FOR loops to count the document frequency for each word,\n",
    "we can use  <font color=\"blue\">FreqDist()</font> jointly with  <font color=\"blue\">set()</font> as follows:\n",
    "1. Apply  <font color=\"blue\">set()</font> to each Reuters article to generate a set of unique words in the article and save all sets in a list\n",
    "```python\n",
    "    [set(value) for value in tokenized_reuters.values()]\n",
    "```\n",
    "2. Similar to what we have done before, we put all the words in a list using  <font color=\"blue\">chain.from_iterable</font> and past\n",
    "it to  <font color=\"blue\">FreqDist</font>.\n",
    "\n",
    "The first step makes sure that each word in an article appears only once, thus the total number of \n",
    "times a word appears in all the sets is equal to the number of documents containing that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 7621),\n",
       " ('the', 6950),\n",
       " ('to', 6911),\n",
       " ('said', 6784),\n",
       " ('and', 6761),\n",
       " ('in', 6580),\n",
       " ('a', 6222),\n",
       " ('lt', 6069),\n",
       " ('for', 5415),\n",
       " ('mln', 4845),\n",
       " ('it', 4788),\n",
       " ('dlrs', 4193),\n",
       " ('from', 4006),\n",
       " ('on', 3987),\n",
       " ('its', 3761),\n",
       " ('is', 3569),\n",
       " ('by', 3511),\n",
       " ('year', 3416),\n",
       " ('at', 3392),\n",
       " ('with', 3253),\n",
       " ('pct', 3212),\n",
       " ('cts', 3068),\n",
       " ('inc', 3017),\n",
       " ('vs', 2982),\n",
       " ('be', 2927)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_2 = list(chain.from_iterable([set(value) for value in tokenized_reuters.values()]))\n",
    "fd_2 = FreqDist(words_2)\n",
    "fd_2.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will find is that the majority of the most frequent words according to their document frequecy are still functional words.\n",
    "Therefore, the next step is to remove all the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignoring Stopwords\n",
    "As discussed in section 3 of chapter 1, we often remove function words from the text completely for most text analysis tasks.\n",
    "Instead of using the built-in stopword list of NLTK, we use a much rich stopword list that has been downloaded in Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reuters_1 = {}\n",
    "for fileid in reuters.fileids():\n",
    "    tokenized_reuters_1[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list comprehension \n",
    "```python\n",
    "    [w for w in tokenized_reuters[fileid] if w not in stopwords]\n",
    "```\n",
    "says: For each word in each Reuters article, keep the word if the word is not contained in the stopword list.\n",
    "\n",
    "Checking for membership of a value in a list takes time proportional to the list's length in the average and worst cases. \n",
    "It causes the above code to run quite slow as we need to do the check for every word in each Reuters article\n",
    "and the size of the stopword list is large.\n",
    "However, if you have hashable items, which means both the item order and duplicates are disregarded, \n",
    "Python `set` is better choice than `list`. The former runs much faster than the latter in terms of searching\n",
    "a large number of hashable items. Indeed, `set` takes constant time to check the membership.\n",
    "Let's try converting the stopword list into a stopword set, then search to remove all the stopwords.\n",
    "Please also note that if you try to perform iteration, `list` is much better than `set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordsSet = set(stopwords)\n",
    "for fileid in reuters.fileids():\n",
    "    tokenized_reuters[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwordsSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above stopping process, 481 stopwords have been removed from the vocabulary. You might wonder what those removed words are. It is quite easy to check those words by differentiating the vocabulary before and after stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['namely',\n",
       " 'zero',\n",
       " 'alone',\n",
       " 'my',\n",
       " 'described',\n",
       " 'appropriate',\n",
       " 'really',\n",
       " 'allow',\n",
       " 'knows',\n",
       " 'lately',\n",
       " 'inc',\n",
       " 'nine',\n",
       " 'for',\n",
       " 'trying',\n",
       " 'among',\n",
       " 'regardless',\n",
       " 'self',\n",
       " 'p',\n",
       " 'another',\n",
       " 'particular',\n",
       " 'using',\n",
       " 'by',\n",
       " 'through',\n",
       " 'non',\n",
       " 'soon',\n",
       " 'still',\n",
       " 'x',\n",
       " 'k',\n",
       " 'than',\n",
       " 'yet',\n",
       " 'itself',\n",
       " 'new',\n",
       " 'what',\n",
       " 'against',\n",
       " 'get',\n",
       " 'besides',\n",
       " 'upon',\n",
       " 'onto',\n",
       " 'use',\n",
       " 'ever',\n",
       " 'nothing',\n",
       " 'course',\n",
       " 's',\n",
       " 'thorough',\n",
       " 'somehow',\n",
       " 'herself',\n",
       " 'seen',\n",
       " 'serious',\n",
       " 'they',\n",
       " 'may',\n",
       " 'unfortunately',\n",
       " 'will',\n",
       " 'tell',\n",
       " 'thus',\n",
       " 'some',\n",
       " 'definitely',\n",
       " 'throughout',\n",
       " 'me',\n",
       " 'ones',\n",
       " 'themselves',\n",
       " 'whatever',\n",
       " 'various',\n",
       " 'greetings',\n",
       " 'tends',\n",
       " 'very',\n",
       " 'forth',\n",
       " 'j',\n",
       " 'sure',\n",
       " 'outside',\n",
       " 'none',\n",
       " 'please',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'nowhere',\n",
       " 'as',\n",
       " 'indicated',\n",
       " 'being',\n",
       " 'moreover',\n",
       " 'says',\n",
       " 'keep',\n",
       " 'via',\n",
       " 'c',\n",
       " 'downwards',\n",
       " 'up',\n",
       " 'also',\n",
       " 'becoming',\n",
       " 'novel',\n",
       " 'likely',\n",
       " 'associated',\n",
       " 'reasonably',\n",
       " 'and',\n",
       " 'okay',\n",
       " 'available',\n",
       " 'much',\n",
       " 'specified',\n",
       " 'one',\n",
       " 'exactly',\n",
       " 'nearly',\n",
       " 'placed',\n",
       " 'inward',\n",
       " 'therefore',\n",
       " 'above',\n",
       " 'perhaps',\n",
       " 'happens',\n",
       " 'well',\n",
       " 'uses',\n",
       " 'six',\n",
       " 'then',\n",
       " 'different',\n",
       " 'thanks',\n",
       " 'am',\n",
       " 'rather',\n",
       " 'unlikely',\n",
       " 'goes',\n",
       " 'entirely',\n",
       " 'along',\n",
       " 'got',\n",
       " 'd',\n",
       " 'looking',\n",
       " 'every',\n",
       " 'seriously',\n",
       " 'elsewhere',\n",
       " 'second',\n",
       " 'unless',\n",
       " 'b',\n",
       " 'somewhat',\n",
       " 'whereby',\n",
       " 'old',\n",
       " 'several',\n",
       " 'across',\n",
       " 'at',\n",
       " 'overall',\n",
       " 'how',\n",
       " 'while',\n",
       " 'hi',\n",
       " 'indeed',\n",
       " 'each',\n",
       " 'be',\n",
       " 'look',\n",
       " 'less',\n",
       " 'something',\n",
       " 'anything',\n",
       " 'sensible',\n",
       " 'per',\n",
       " 'thoroughly',\n",
       " 'oh',\n",
       " 'sometimes',\n",
       " 'fifth',\n",
       " 'consequently',\n",
       " 'anybody',\n",
       " 'given',\n",
       " 'ourselves',\n",
       " 'though',\n",
       " 'or',\n",
       " 'gone',\n",
       " 'com',\n",
       " 'myself',\n",
       " 'h',\n",
       " 'u',\n",
       " 'hence',\n",
       " 'a',\n",
       " 'saw',\n",
       " 'specifying',\n",
       " 'which',\n",
       " 'should',\n",
       " 'become',\n",
       " 'right',\n",
       " 'o',\n",
       " 'were',\n",
       " 'both',\n",
       " 'came',\n",
       " 'four',\n",
       " 'th',\n",
       " 'former',\n",
       " 'awfully',\n",
       " 'could',\n",
       " 'sometime',\n",
       " 't',\n",
       " 'enough',\n",
       " 'want',\n",
       " 'mainly',\n",
       " 'been',\n",
       " 'probably',\n",
       " 'whether',\n",
       " 'own',\n",
       " 'in',\n",
       " 'know',\n",
       " 'seeing',\n",
       " 'beforehand',\n",
       " 'thereby',\n",
       " 'why',\n",
       " 'provides',\n",
       " 'contains',\n",
       " 'just',\n",
       " 'done',\n",
       " 'more',\n",
       " 'relatively',\n",
       " 'kept',\n",
       " 'say',\n",
       " 'was',\n",
       " 'are',\n",
       " 'within',\n",
       " 'around',\n",
       " 'whenever',\n",
       " 'comes',\n",
       " 'actually',\n",
       " 'looks',\n",
       " 'the',\n",
       " 'cause',\n",
       " 'whole',\n",
       " 'away',\n",
       " 'theirs',\n",
       " 'etc',\n",
       " 'anywhere',\n",
       " 'sorry',\n",
       " 'merely',\n",
       " 'plus',\n",
       " 'latter',\n",
       " 'almost',\n",
       " 'whereas',\n",
       " 'f',\n",
       " 'l',\n",
       " 'go',\n",
       " 'when',\n",
       " 'ask',\n",
       " 'often',\n",
       " 'now',\n",
       " 'i',\n",
       " 'keeps',\n",
       " 'way',\n",
       " 'other',\n",
       " 'n',\n",
       " 'twice',\n",
       " 'everybody',\n",
       " 'corresponding',\n",
       " 'ok',\n",
       " 'where',\n",
       " 'formerly',\n",
       " 'nevertheless',\n",
       " 'tries',\n",
       " 'seemed',\n",
       " 'whose',\n",
       " 'same',\n",
       " 'appear',\n",
       " 'already',\n",
       " 'z',\n",
       " 'ignored',\n",
       " 'containing',\n",
       " 'rd',\n",
       " 'inner',\n",
       " 'little',\n",
       " 'later',\n",
       " 'vs',\n",
       " 'wants',\n",
       " 'particularly',\n",
       " 'v',\n",
       " 'brief',\n",
       " 'believe',\n",
       " 'appreciate',\n",
       " 'near',\n",
       " 'ex',\n",
       " 'into',\n",
       " 'ought',\n",
       " 'hereby',\n",
       " 'until',\n",
       " 'necessary',\n",
       " 'certainly',\n",
       " 'such',\n",
       " 'beyond',\n",
       " 'it',\n",
       " 'to',\n",
       " 'himself',\n",
       " 'otherwise',\n",
       " 'towards',\n",
       " 'see',\n",
       " 'over',\n",
       " 'do',\n",
       " 'its',\n",
       " 'seven',\n",
       " 'nor',\n",
       " 'ltd',\n",
       " 'these',\n",
       " 'who',\n",
       " 'come',\n",
       " 'must',\n",
       " 'else',\n",
       " 'regarding',\n",
       " 'normally',\n",
       " 'them',\n",
       " 'down',\n",
       " 'example',\n",
       " 'furthermore',\n",
       " 'sent',\n",
       " 'obviously',\n",
       " 'seeming',\n",
       " 'contain',\n",
       " 'anyone',\n",
       " 'here',\n",
       " 'w',\n",
       " 'changes',\n",
       " 'third',\n",
       " 'she',\n",
       " 'nobody',\n",
       " 'about',\n",
       " 'somebody',\n",
       " 'least',\n",
       " 'concerning',\n",
       " 'off',\n",
       " 'without',\n",
       " 'r',\n",
       " 'quite',\n",
       " 'certain',\n",
       " 'thereafter',\n",
       " 'below',\n",
       " 'on',\n",
       " 'further',\n",
       " 'take',\n",
       " 'aside',\n",
       " 'if',\n",
       " 'shall',\n",
       " 'secondly',\n",
       " 'wish',\n",
       " 'is',\n",
       " 'had',\n",
       " 'accordingly',\n",
       " 'somewhere',\n",
       " 'eg',\n",
       " 'before',\n",
       " 'let',\n",
       " 'whoever',\n",
       " 'saying',\n",
       " 'especially',\n",
       " 'said',\n",
       " 'taken',\n",
       " 'everything',\n",
       " 'two',\n",
       " 'has',\n",
       " 'he',\n",
       " 'value',\n",
       " 'last',\n",
       " 'having',\n",
       " 'becomes',\n",
       " 'but',\n",
       " 'g',\n",
       " 'yourself',\n",
       " 'we',\n",
       " 'maybe',\n",
       " 'lest',\n",
       " 'ours',\n",
       " 'need',\n",
       " 'us',\n",
       " 'liked',\n",
       " 'although',\n",
       " 'always',\n",
       " 'useful',\n",
       " 'him',\n",
       " 'too',\n",
       " 'there',\n",
       " 'toward',\n",
       " 'this',\n",
       " 'afterwards',\n",
       " 'neither',\n",
       " 'presumably',\n",
       " 'seems',\n",
       " 'gives',\n",
       " 'never',\n",
       " 'think',\n",
       " 'between',\n",
       " 'apart',\n",
       " 'm',\n",
       " 'would',\n",
       " 'like',\n",
       " 'causes',\n",
       " 'all',\n",
       " 'un',\n",
       " 'either',\n",
       " 'et',\n",
       " 'gets',\n",
       " 'known',\n",
       " 'behind',\n",
       " 'specify',\n",
       " 'few',\n",
       " 'went',\n",
       " 'his',\n",
       " 'currently',\n",
       " 're',\n",
       " 'allows',\n",
       " 'co',\n",
       " 'according',\n",
       " 'meanwhile',\n",
       " 'eight',\n",
       " 'welcome',\n",
       " 'name',\n",
       " 'hopefully',\n",
       " 'during',\n",
       " 'however',\n",
       " 'once',\n",
       " 'everywhere',\n",
       " 'seem',\n",
       " 'from',\n",
       " 'does',\n",
       " 'considering',\n",
       " 'others',\n",
       " 'everyone',\n",
       " 'regards',\n",
       " 'usually',\n",
       " 'three',\n",
       " 'her',\n",
       " 'not',\n",
       " 'despite',\n",
       " 'whom',\n",
       " 'even',\n",
       " 'only',\n",
       " 'those',\n",
       " 'tried',\n",
       " 'your',\n",
       " 'indicate',\n",
       " 'many',\n",
       " 'became',\n",
       " 'can',\n",
       " 'wonder',\n",
       " 'getting',\n",
       " 'an',\n",
       " 'first',\n",
       " 'hardly',\n",
       " 'five',\n",
       " 'help',\n",
       " 'possible',\n",
       " 'since',\n",
       " 'doing',\n",
       " 'amongst',\n",
       " 'e',\n",
       " 'instead',\n",
       " 'following',\n",
       " 'after',\n",
       " 'have',\n",
       " 'far',\n",
       " 'needs',\n",
       " 'with',\n",
       " 'because',\n",
       " 'except',\n",
       " 'try',\n",
       " 'that',\n",
       " 'out',\n",
       " 'follows',\n",
       " 'able',\n",
       " 'so',\n",
       " 'immediate',\n",
       " 'better',\n",
       " 'someone',\n",
       " 'best',\n",
       " 'q',\n",
       " 'used',\n",
       " 'mostly',\n",
       " 'again',\n",
       " 'hers',\n",
       " 'gotten',\n",
       " 'took',\n",
       " 'consider',\n",
       " 'next',\n",
       " 'asking',\n",
       " 'respectively',\n",
       " 'our',\n",
       " 'any',\n",
       " 'clearly',\n",
       " 'under',\n",
       " 'most',\n",
       " 'indicates',\n",
       " 'mean',\n",
       " 'of',\n",
       " 'anyway',\n",
       " 'going',\n",
       " 'followed',\n",
       " 'truly',\n",
       " 'y',\n",
       " 'might',\n",
       " 'together',\n",
       " 'willing',\n",
       " 'you',\n",
       " 'did',\n",
       " 'their']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_3 = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "fd_3 = FreqDist(words_3)\n",
    "list(vocab - set(fd_3.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside stopwords, there might some other words that occur quite often as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mln', 18598),\n",
       " ('dlrs', 12329),\n",
       " ('pct', 9771),\n",
       " ('lt', 8696),\n",
       " ('cts', 8308),\n",
       " ('net', 6986),\n",
       " ('year', 6687),\n",
       " ('billion', 5809),\n",
       " ('loss', 5115),\n",
       " ('company', 4593)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_3.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we decide to remove those words from our vocabulary, it might be worth checking what \n",
    "those words mean and the context of those words. Fortunately NLTK provides a `concordance`\n",
    "function in the `nltk.text` module. A concordance view shows us every occurrence of a given \n",
    "word, together with the corresponding context. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      "e U . S . Has said it will impose 300 mln dlrs of tariffs on imports of Japanes\n",
      ". It also said that each year 1 . 575 mln tonnes , or 25 pct , of China ' s fru\n",
      "it output are left to rot , and 2 . 1 mln tonnes , or up to 30 pct , of its veg\n",
      "ergy supplies in the year 2000 to 550 mln kilolitres ( kl ) from 600 mln , they\n",
      "to 550 mln kilolitres ( kl ) from 600 mln , they said . The decision follows th\n",
      "totalled 9 , 595 tonnes , worth 6 . 9 mln dlrs FOB , plus 184 . 3 mln rupiah fo\n",
      "rth 6 . 9 mln dlrs FOB , plus 184 . 3 mln rupiah for rubber delivered locally ,\n",
      "thern Territory at a cost of about 21 mln dlrs . The mine , to be known as the \n",
      "umitomo last August agreed to pay 500 mln dlrs for a 12 . 5 pct limited partner\n",
      " . It reported a net loss of 976 . 38 mln pesos in the year ending December 198\n",
      "ld acquire Atlas ' total loans of 275 mln dlrs , to be repaid by the mining com\n",
      "increase in authorised capital to 175 mln shares from 125 mln at a general meet\n",
      "ed capital to 175 mln shares from 125 mln at a general meeting on May 1 , it sa\n",
      "WATER 1986 PRETAX PROFITS RISE 15 . 6 MLN STG Shr 27 . 7p vs 20 . 7p Div 6 . 0p\n",
      "s 1 . 29 billion Pretax profit 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9\n",
      "on Pretax profit 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company n\n",
      "t 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company name is Bowater \n",
      "vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company name is Bowater Industries Pl\n",
      "lt ; BWTR . L > Trading profit 63 . 4 mln vs 45 . 1 mln Trading profit includes\n",
      "> Trading profit 63 . 4 mln vs 45 . 1 mln Trading profit includes - Packaging a\n",
      "kaging and associated products 23 . 2 mln vs 14 . 2 mln Merchanting and service\n",
      "ociated products 23 . 2 mln vs 14 . 2 mln Merchanting and services 18 . 4 mln v\n",
      "2 mln Merchanting and services 18 . 4 mln vs 9 . 6 mln Tissue and timber produc\n",
      "ting and services 18 . 4 mln vs 9 . 6 mln Tissue and timber products 9 . 0 mln \n",
      " mln Tissue and timber products 9 . 0 mln vs 5 . 8 mln Interest debit 15 . 4 ml\n"
     ]
    }
   ],
   "source": [
    "nltk.Text(reuters.words()).concordance('mln')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      "ndesbank is effectively withdrawing a net 8 . 1 billion marks from the market w\n",
      "d world copper prices . It reported a net loss of 976 . 38 mln pesos in the yea\n",
      "nding December 1986 , compared with a net loss of 1 . 53 billion in 1985 . The \n",
      " U . S .- based bank , said it made a net loss of just over six mln crowns in 1\n",
      "ng program , reported a 198 mln franc net loss , after 187 mln francs in provis\n",
      "plant , compared with a 250 mln franc net profit in 1985 . VIEILLE MONTAGNE REP\n",
      "REPORTS LOSS , DIVIDEND NIL 1986 Year Net loss after exceptional charges 198 ml\n",
      "on francs vs 20 . 20 billion Proposed net dividend on ordinary shares nil vs 11\n",
      "ently reported first - half 1986 / 87 net fell to 15 . 02 mln dlrs from 17 . 09\n",
      "eso sales in 1985 . It said unaudited net profit was in the neighbourhood of 70\n",
      "id in a statement that parent company net profit last year will rise from the 7\n",
      " DSM & lt ; DSMN . AS > said its 1986 net profit rose to 412 mln guilders from \n",
      "ARCH INC & lt ; CORE > 2ND QTR FEB 28 NET Shr 14 cts vs nine cts Net 217 , 572 \n",
      "QTR FEB 28 NET Shr 14 cts vs nine cts Net 217 , 572 vs 153 , 454 Revs 2 , 530 ,\n",
      "8 , 924 1st half Shr 19 cts vs 11 cts Net 299 , 838 vs 174 , 739 Revs 4 , 865 ,\n",
      "ENT CORP & lt ; ELRC > 3RD QTR FEB 28 NET Shr 20 cts vs 32 cts Net 1 , 358 , 00\n",
      "D QTR FEB 28 NET Shr 20 cts vs 32 cts Net 1 , 358 , 000 vs 2 , 476 , 000 Revs 2\n",
      "0 Nine mths Shr 68 cts vs 1 . 05 dlrs Net 4 , 957 , 000 vs 8 , 129 , 000 Revs 8\n",
      "0 RUBBERMAID INC & lt ; RBD > 1ST QTR NET Shr 28 cts vs 22 cts Net 20 . 6 mln v\n",
      "BD > 1ST QTR NET Shr 28 cts vs 22 cts Net 20 . 6 mln vs 16 . 1 mln Sales 238 . \n",
      "IONAL INC & lt ; WAF > 4TH QTR FEB 28 NET Shr profit 13 cts vs loss 33 cts Net \n",
      " NET Shr profit 13 cts vs loss 33 cts Net profit 1 , 149 , 000 vs loss 2 , 833 \n",
      "Year Shr profit 24 cts vs loss 18 cts Net profit 2 , 050 , 000 vs loss 1 , 551 \n",
      "GA SAVINGS BANK & lt ; CAYB > 1ST QTR NET Shr 55 cts vs 41 cts Net 494 , 000 vs\n",
      "YB > 1ST QTR NET Shr 55 cts vs 41 cts Net 494 , 000 vs 204 , 000 Avg shrs 896 ,\n"
     ]
    }
   ],
   "source": [
    "nltk.Text(reuters.words()).concordance('net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing those words, you might also want to remove them from the vocabulary. \n",
    "We will leave it as an excersie for you to do on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Less Frequent Words\n",
    "If the most common words do not benefit the downstream text analysis tasks, except for contributing noises,\n",
    "how about the words that occur once or twice?\n",
    "Here another interesting statistic to look at is the frequency of the frequencies of word types in a given corpus.\n",
    "We would like to see how many words appear only once, how many words appear twice, how many\n",
    "words appear three times, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEKCAYAAADNSVhkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xm4XFWV9/HvjzBDmARimITI1AgK\nyCjqG0AQ0RbtZhSbQWyHRgQEXqHtFrVfWmxxQHFCRhGZoY0okIhGFCGBkBDCJCgRwhRoxgACgfX+\ncXaR4nbVvZV765xTt/bv8zz15NQ+p6rWqgpZnF2n1lZEYGZmVqUl6g7AzMzy4+JjZmaVc/ExM7PK\nufiYmVnlXHzMzKxyLj5mZlY5Fx8zM6uci4+ZmVXOxcfMzCq3ZN0B9KpVVlklNtxww7rDqMVzzz3H\nCiusUHcYtcg5d8g7/5xzh+7lP2PGjMcjYo2hjnPxaWPcuHHcfPPNdYdRi6lTpzJx4sS6w6hFzrlD\n3vnnnDt0L39Jf+3kOE+7mZlZ5Vx8zMysci4+ZmZWORcfMzOrnIuPmZlVzsXHzMwq5+JjZmaVc/Ex\nM7PK+Uembbzw8iusf/wvW+6be/L7K47GzKy/+MzHzMwqNyqLj6RzJN0naVa6bZnGJek7ku6VNFvS\n1k2POVjSPel2cH3Rm5lZT067SVo1Ip4c4rDjIuLSAWPvAzZKt+2BHwDbS1oNOBHYBghghqRJHbyG\nmZmVoFfPfG6W9DNJu0jSYjxuL+AnUbgRWEXSeOC9wJSIeCIVnCnAHiXEbWZmHejV4rMx8DPgM8Ad\nkv5V0loDjjkpTa19S9IyaWxt4IGmY+alsXbjZmZWg56cdouIV4ArgSslrQF8Fbhf0jsiYjpwAvAI\nsDRwOvB54CtAq7OkGGT8dSR9AvgEwOqrr8EXt1jYMr6pU6cuZkajy4IFC/o+x3Zyzh3yzj/n3KH6\n/Huy+ABIWhnYDzgUeBk4DJgNEBEPp8NelHQ2cGy6Pw9Yt+lp1gEeSuMTB4xPHfiaEXE6RTFjvQkb\nxjdua/32zD1wYsvxfpHzuiY55w55559z7lB9/j057Sbpp8AtwATgoIh4d0ScGxF/S/vHpz8FfAiY\nkx46CTgoXfW2A/B0KlTXALtLWlXSqsDuaczMzGrQq2c+FwOHRETreS84P03HCZgFfCqN/wrYE7gX\neJ7irImIeELSfwA3peO+EhFPlBW8mZkNrieLT0RMGmL/Lm3GAzi8zb6zgLNGHp2ZmY1UT067mZlZ\nf+vJM59esNxSY7jbPdzMzErhMx8zM6uci4+ZmVXO025tDLakQhm8TIOZ5cRnPmZmVrnSio+ksyTN\nlzSnaWw1SVPSsgZT0g8+kbSppBskvSjp2EGec6Kkp5uWUvhi0749JN2dllM4vml8A0nT0mteJGnp\nsnI2M7POlHnmcw7/u3P08cC1EbERcG26D/AE8FnglA6e9/cRsWW6fQVA0hjgexRLKmwGHCBps3T8\n14Bvpdd8kqJNj5mZ1ai04hMR11EUlWZ7Aeem7XMpWuMQEfMj4iaKHm7DsR1wb0T8JSJeAi4E9krt\nd3YBGuv+vPaaZmZWn6ovOBjXaAoaEQ9LWnMYz7GjpFspGoYeGxG303rJhO2BNwBPNbXpGXQphU67\nWpehl7rp5tzdN+fcIe/8c84d3NV6KLcAb4qIBZL2BP6bYtXSES2l8NqODrtal6GXOmXn3N0359wh\n7/xzzh36v6v1o00dqccD8wc7WNLhTRcXrBURz0TEAoCI+BWwlKTVab+UwuMUq5kuOWDczMxqVHXx\nmQQcnLYPBn4+2MER8b2miwsekvTGxrLakrajiP9/KLpVb5SubFsa2B+YlBqN/hbYu9PXNDOz8pU2\nryTpAooF3FaXNA84ETgZuFjSYcD9wD7p2DcCNwMrAa9KOgrYLCKeGfC0ewOflrQQeAHYPxWYhZI+\nQ7FGzxjgrPRdEBSrnF4o6f8BM4Ezy8rZzMw6U1rxiYgD2uzatcWxj1BMiQ31nKcBp7XZ9yuK9XwG\njv+F4mo4MzPrEaPtgoPKuKu1mVl53F7HzMwq5+JjZmaV87RbG1V2tXZHazPLjc98zMyscrUUH0lz\nJd2Wfjx6cxrbR9Ltkl6VtM0gj217nKQTUlfruyW9t2m8ZcdrMzOrR53TbjtHxONN9+cA/wD8aIjH\ntTwudbHeH3gLsBbwa0kbp93fA3aj6IRwk6RJEXHHyFMwM7Ph6JnvfCLiToDUwGA4x+0FXBgRLwL3\nSbqXRb/vuTf93gdJF6ZjXXzMzGpS13c+AUyWNCN1ku6GVp2t1x5k3MzMalLXmc9OqVfbmsAUSXel\n9X9Gol0H61YFtmVn67qWVOi1Nu45t5bPOXfIO/+cc4dMllSIiIfSn/MlXUExPday+Eg6G9gKeCgi\n9hzkadt1tmaQ8YFx1bKkQi8tpwB5t5bPOXfIO/+cc4f+X1IBSStIGtvYBnanuIigpYg4NHW1Hqzw\nQNExe39Jy0jagGKdn+m06XjdjVzMzGx46vjOZxzwh7Qa6XTglxFxtaQPp+7XOwK/lHRNqwe3Oy51\nsb6Y4kKCq4HDI+KVtIppo+P1ncDFTR2vzcysBpVPu6Wrzt7WYvwK4IoOHt/2uIg4CTipxXjLjtdm\nZlaPnrnUute4q7WZWXncXsfMzCrn4mNmZpXztFsbVXa17jXHbLGQQzLL3Z3FzarlMx8zM6tcTxUf\nSWdJmi9pTtPYapKmSLon/blqm8eeI+m+1Cl7lqQt07gkfSd1tJ4taeuq8jEzs9Z6qvgA5wB7DBg7\nHrg2IjYCrk332zku/SB1y4iYlcbeR/GD040oWuf8oLshm5nZ4uqp4pP6uz0xYHgv4Ny0fS7wocV8\n2r2An0ThRmAVSeNHFqmZmY1ETxWfNsZFxMMA6c81Bzn2pDS19i1Jy6Qxd7U2M+sx/XS12wnAI8DS\nFM1BPw98hfbdrv+Xurpa95pxyxVXvOWk0c3XnY3zzT/n3CGTrtaL6VFJ4yPi4TRdNh8g9XQbB9wc\nER9vnB0BL6ZO2Mem+4N1u36durpa95pjtlhIbrk3Oou7s3G++eecO2TQ1XoYJgEHp+2DgZ8DRMR7\n04UFHwdofI+jYonTD7GoU/Yk4KB01dsOwNNNhcrMzGrQU/97K+kCYCKweupcfSJwMnCxpMOA+4F9\n2jz8fElrUEyzzQI+lcZ/BewJ3As8DxxaWgJmZtaRnio+EXFAm127dvDYXdqMB3D4SOIyM7PuGg3T\nbmZm1md66synl+S8pMLUqVN7bmlvM+svPvMxM7PKufiYmVnlPO3WxuIsqeB2/GZmi2fIMx9Jl0l6\nvySfJZmZWVd0UlB+AHwEuEfSyZI2LTmmYRvJkgxmZladIYtPRPw6Ig4EtgbmAlMk/VHSoZKWKjvA\nxXQOI1uSwczMKtDRVJqkNwCHAB8HZgKnUhSjKaVFNgwlLclgZmZdpqIBwCAHSJcDmwLnAec090WT\ndHNEbFNuiItH0vrAlRGxebr/VESs0rT/yYhotxpqc1frt3/x2z/u6DW3WHvlEUbdWxYsWMCKK65Y\ndxi1yDl3yDv/nHOH7uW/8847z+ikLnRytdtpEfGbVjt6rfCM1HC7WvfbDzJz7u6bc+6Qd/455w69\n2dX67yQ1nzmsKulfSoyp2x5t6nj92pIMZmZWn06Kzz9HxFONOxHxJPDP5YXUdS2XZDAzs/p0UnyW\nSGvkACBpDMVqoT0nLclwA7CJpHlpGYaTgd0k3QPslu6bmVmNOvlS4xqK9XR+SLH89KeAq0uNaphG\nsiSDmZlVp5Pi83ngk8CnKRZqmwycUWZQvSDnrtZmZmUbsvhExKsUXQ5+UH44ZmaWgyGLj6SdgC8B\nb0rHi2KB0AnlhmZmZv2qk2m3M4GjgRnAK+WG0zsWp6t1g7tbm5l1ppPi83REXFV6JGZmlo1OLrX+\nraSvS9pR0taNW+mRdZmkIyXNkXS7pKPqjsfMLGednPlsn/5sbqUTwC7dD6cckjan+GHsdsBLwNWS\nfhkR99QbmZlZnjq52m3nKgIp2d8BN0bE8wCSfgd8GPivWqMyM8tUJyuZjpN0pqSr0v3NUueA0WQO\n8G5Jb5C0PLAnsG7NMZmZZauTJRWuAs4GvhARb5O0JDAzIraoIsBuSQXzcGABcAfwQkQcPeCYYS2p\n0NAvSyvk3Fo+59wh7/xzzh2qX1Khk+JzU0RsK2lmRGyVxmZFxJYjjrImkv4TmBcR3293zHoTNowl\n9j11sZ63Xy61zrm1fM65Q97555w7dC9/SV1bz+e5tJJppCfeAXh6hPFVTtKaETFf0nrAPwA71h2T\nmVmuOik+n6NYluDNkq4H1gD2LjWqclyWiujLwOFpaQgzM6tBJ1e73SLp/wCbULTWuTsiXi49si6L\niHfVHYOZmRU66e120IChrSURET8pKaae4K7WZmbl6WTabdum7WUp1sa5Bejr4mNmZuXpZNrtiOb7\nklYGzistIjMz63udnPkM9DywUbcD6TXD6Wrd0C+XXJuZlaWT73x+QbrMmqIjwmbAxWUGZWZm/a2T\nM59TmrYXAn+NiHklxVMaSUcDH6copLcBh0bE3+qNyswsT5185/O7KgIpk6S1gc8Cm0XEC5IuBvYH\nzqk1MDOzTHUy7fYsi6bdXreLYjntlboeVTmWBJaT9DKwPPBQzfGYmWWrk2m3bwGPUFzhJuBAYGxE\njJrlCCLiQUmnAPcDLwCTI2JyzWGZmWWrk8ai0yJi+6HGepmkVYHLgP2Ap4BLgEsj4qcDjhtRV+uG\n0d7dOufuvjnnDnnnn3PuUH1X607OfF6RdCBwIcX02wHAKyOMr2rvAe6LiMcAJF0OvAN4XfGJiNOB\n06Hoav2N24ZzJTrMPXDiSGKtXc7dfXPOHfLOP+fcofr8h1xMDvgIsC/waLrtk8ZGk/uBHSQtL0kU\nXRrurDkmM7NsdXK121xgr/JDKU9ETJN0KUVboIXATNIZjpmZVa+TZbQ3lnStpDnp/lsl/Vv5oXVX\nRJwYEZtGxOYR8U8R8WLdMZmZ5aqTabcfAydQrINDRMym+I2MmZnZsHTyjfryETG9+KrkNQtLiqdn\neEkFM7PydHLm87ikN7NoGe29gYdLjcrMzPpaJ2c+h1N8Ob+ppAeB+yh+aGpmZjYsgxYfSUsA20TE\neyStACwREc9WE1q9RrKkQiteZsHMbJFBp90i4lXgM2n7uVwKj5mZlauT73ymSDpW0rqSVmvcSo+s\niyRtImlW0+0ZSUfVHZeZWa46+c7nY+nPw5vGApjQ/XDKERF3A1sCSBoDPAhcUWtQZmYZ66TDwQZV\nBFKhXYE/R8Rf6w7EzCxXbbtaS/rPiPjXtL1bREypNLKSSDoLuCUiTmuxrytdrVsZTZ2uc+7um3Pu\nkHf+OecO1Xe1Hqz43BIRWw/cHs0kLU2xiNxbIuLRwY5db8KGscS+p3bttUfT1W45d/fNOXfIO/+c\nc4fu5S+po+LTyQUH/eR9FGc9gxYeMzMr12Df+awp6XMUq5c2tl8TEd8sNbJyHABcUHcQZma5G6z4\n/BgY22J7VJK0PLAb8Mm6YzEzy13b4hMRX64ykLJFxPPAG+qOw8zMOvudT5bc1drMrDy5XXBgZmY9\nwMXHzMwq13babeDVbQON0qvdOtbtrtatjKbf/piZddNg3/k0rm7bBNgWmJTu/z1wXZlBmZlZfxvy\najdJk4GtG8spSPoScEkl0XWRpFWAM4DNKRqjfiwibqg3KjOzPHVytdt6wEtN918C1i8lmnKdClwd\nEXunNjvL1x2QmVmuOik+5wHTJV1BccbwYeDcUqPqMkkrAe8GDgGIiJd4fUE1M7MKdbKkwkmSrgLe\nlYYOjYiZ5YbVdROAx4CzJb0NmAEcGRHP1RuWmVme2na1BpC0BDA7IjavLqTuk7QNcCOwU0RMk3Qq\n8ExE/PuA40pbUqGVXl1mIefW8jnnDnnnn3PuUP2SCoOe+UTEq5JulbReRNw/4qjqMw+YFxHT0v1L\ngeMHHhQRpwOnQ7GkwjduK7cBxNwDJ5b6/MOVc2v5nHOHvPPPOXeoPv9O/nUdD9wuaTrw2jRVRHyw\ntKi6LCIekfSApE3Sktq7AnfUHZeZWa46KT790mD0COD8dKXbX4BDa47HzCxbnVxw8DtJ4yh+aAow\nPSLmlxtW90XELGDIeUgzMyvfkMVH0r7A14GpFAvLfVfScRFxacmx1cpdrc3MytPJtNsXgG0bZzuS\n1gB+TfGlvZmZ2WLrpKv1EgOm2f6nw8eZmZm11MmZz9WSrgEuSPf3A35VXki9oYqu1oNxx2sz62ed\nXHBwnKR/BHai+M7n9Ii4ovTIzMysbw22ns9RwPXAzIi4DLissqhKIGku8CzwCrCwk1/gmplZOQY7\n81mHohP0ppJmA3+kKEY3RMQTVQRXgp0j4vG6gzAzy91g6/kcC5B+lLkN8A7gY8CPJT0VEZtVE6KZ\nmfWbTq5aWw5YCVg53R4Cpg36iN4UwGRJM1IDUTMzq0nbrtaSTgfeQvE9yTSKrtA3RsST1YXXPZLW\nioiHJK0JTAGOiIjrBhxTaVfrwdTZ8Trn7r455w55559z7tBbXa3XA5YB7gEepOgM/dSII6tJRDyU\n/pyfFsbbDrhuwDGVdrUeTJ0dr3Pu7ptz7pB3/jnnDtXn33baLSL2oOjndkoaOga4SdJkSaOq2aik\nFSSNbWwDuwNz6o3KzCxfQ63nE8AcSU8BT6fbByjOGk4sP7yuGQdcIQmKnH8WEVfXG5KZWb4G+53P\nZymucNsJeJl0mTVwFnBbJdF1SUT8BXhb3XGYmVlhsDOf9Smahx4dEQ9XE46ZmeVgsN/5fK7KQHqN\nl1QwMyuPu1ObmVnl6ruWuMfV3dW6G9wZ28x6lc98zMysci4+ZmZWuayKj6QxkmZKurLuWMzMcpZV\n8QGOBO6sOwgzs9xlU3wkrQO8Hzij7ljMzHLXtqt1v5F0KfBVYCxwbER8oMUxPdPVuhuG2xk75+6+\nOecOeeefc+7QW12t+4akDwDzI2KGpIntjuulrtbdMNzO2Dl39805d8g7/5xzhx7qat1ndgI+KGku\ncCGwi6Sf1huSmVm+sig+EXFCRKwTEesD+wO/iYiP1hyWmVm2sig+ZmbWW0b3lxrDEBFTgak1h2Fm\nlrXsik+n3NXazKw8nnYzM7PKufiYmVnlPO3WRj8sqTBcx2yxkEMGyd1LNZjZSPnMx8zMKpdF8ZG0\nrKTpkm6VdLukL9cdk5lZznKZdnsR2CUiFkhaCviDpKsi4sa6AzMzy1EWxSeK7qkL0t2l0i2Pjqpm\nZj0oi2k3eG0huVnAfGBKREyrOyYzs1xls6RCg6RVgCuAIyJizoB9fbWkwnCNWw4efaH9/uEu1TAa\nuK1+vvnnnDt4SYXSRcRTkqYCewBzBuzrqyUVhuuYLRYyWO7DXaphNHBb/Xzzzzl38JIKpZC0Rjrj\nQdJywHuAu+qNyswsX7n8r/144FxJYygK7sURcWXNMZmZZSuL4hMRs4Gt6o7DzMwKWRSf4ci5q/XU\nqVP7+nsdM6tfFt/5mJlZb3HxMTOzynnarQ13tc4vd3frNquOz3zMzKxyWRQfSetK+q2kO1NX6yPr\njsnMLGe5TLstBI6JiFskjQVmSJoSEXfUHZiZWY6yOPOJiIcj4pa0/SxwJ7B2vVGZmeUri+LTTNL6\nFD84dVdrM7OaZNXVWtKKwO+AkyLi8hb73dWaobta96st1l7ZnY0zzj/n3KH6rtbZFJ+0gumVwDUR\n8c2hjl9vwoaxxL6nlh9YDxqqq3W/mnvy+93ZOOP8c84dupe/pI6KTxbTbpIEnAnc2UnhMTOzcmVR\nfICdgH8CdpE0K932rDsoM7NcZTG3EhF/AFR3HGZmVsjlzMfMzHpIFmc+w+ElFSbWHYaZ9TGf+ZiZ\nWeWyudR6cflS6zxPinPOHfLOP+fcYVH+I+3u7kutzcysZ7n4mJlZ5bIoPpLOkjRf0py6YzEzs0yK\nD3AOsEfdQZiZWSGL4hMR1wFP1B2HmZkVsrnaLS2lcGVEbD7IMe5qTb5drSHv3CHv/HPOHRblv8Xa\nK4/oeTrtap3vdYUtRMTpwOlQXGqd62WXOV9ymnPukHf+OecOTZdaV/QD8yym3czMrLe4+JiZWeWy\nKD6SLgBuADaRNE/SYXXHZGaWsywmOCPigLpjMDOzRbIoPsPhrtYT6w6jFjnnDnnnn3PuUH3+WUy7\nmZlZb3HxMTOzyrn4mJlZ5Vx8zMysci4+ZmZWORcfMzOrnIuPmZlVzsXHzMwq5+JjZmaVy2Y9n8Ul\n6Vng7rrjqMnqwON1B1GTnHOHvPPPOXfoXv5viog1hjrI7XXau7uTBZH6kaSbnXuecs4/59yh+vw9\n7WZmZpVz8TEzs8q5+LR3et0B1Mi55yvn/HPOHSrO3xccmJlZ5XzmY2ZmlXPxGUDSHpLulnSvpOPr\njqcbJK0r6beS7pR0u6Qj0/hqkqZIuif9uWoal6TvpPdgtqStm57r4HT8PZIOriunxSVpjKSZkq5M\n9zeQNC3lcZGkpdP4Mun+vWn/+k3PcUIav1vSe+vJZPFJWkXSpZLuSn8Hdszls5d0dPo7P0fSBZKW\n7efPXtJZkuZLmtM01rXPWtLbJd2WHvMdSRp2sBHhW7oBY4A/AxOApYFbgc3qjqsLeY0Htk7bY4E/\nAZsB/wUcn8aPB76WtvcErgIE7ABMS+OrAX9Jf66atletO78O34PPAT8Drkz3Lwb2T9s/BD6dtv8F\n+GHa3h+4KG1vlv4+LANskP6ejKk7rw5zPxf4eNpeGlglh88eWBu4D1iu6TM/pJ8/e+DdwNbAnKax\nrn3WwHRgx/SYq4D3DTvWut+sXrqlN/WapvsnACfUHVcJef4c2I3iR7Tj09h4it82AfwIOKDp+LvT\n/gOAHzWNv+64Xr0B6wDXArsAV6b/cB4Hlhz4uQPXADum7SXTcRr4d6H5uF6+ASulf4A1YLzvP/tU\nfB5I/4gumT779/b7Zw+sP6D4dOWzTvvuahp/3XGLe/O02+s1/rI2zEtjfSNNJWwFTAPGRcTDAOnP\nNdNh7d6H0fr+fBv4v8Cr6f4bgKciYmG635zHazmm/U+n40dr7hOAx4Cz07TjGZJWIIPPPiIeBE4B\n7gcepvgsZ5DPZ9/Qrc967bQ9cHxYXHxer9X8Zd9cDihpReAy4KiIeGawQ1uMxSDjPUvSB4D5ETGj\nebjFoTHEvlGXe7IkxTTMDyJiK+A5iqmXdvom//Tdxl4UU2VrASsA72txaL9+9kNZ3Hy7+j64+Lze\nPGDdpvvrAA/VFEtXSVqKovCcHxGXp+FHJY1P+8cD89N4u/dhNL4/OwEflDQXuJBi6u3bwCqSGu2l\nmvN4Lce0f2XgCUZn7lDEPS8ipqX7l1IUoxw++/cA90XEYxHxMnA58A7y+ewbuvVZz0vbA8eHxcXn\n9W4CNkpXwyxN8aXjpJpjGrF0RcqZwJ0R8c2mXZOAxpUsB1N8F9QYPyhdDbMD8HQ6Xb8G2F3Squn/\nKndPYz0rIk6IiHUiYn2Kz/M3EXEg8Ftg73TYwNwb78ne6fhI4/unK6I2ADai+PK1p0XEI8ADkjZJ\nQ7sCd5DBZ08x3baDpOXTfwON3LP47Jt05bNO+56VtEN6Pw9qeq7FV/eXY712o7gC5E8UV7R8oe54\nupTTOylOj2cDs9JtT4r57GuBe9Kfq6XjBXwvvQe3Ads0PdfHgHvT7dC6c1vM92Eii652m0DxD8i9\nwCXAMml82XT/3rR/QtPjv5Dek7sZwVU+NeS9JXBz+vz/m+IKpiw+e+DLwF3AHOA8iivW+vazBy6g\n+H7rZYozlcO6+VkD26T38s/AaQy4kGVxbu5wYGZmlfO0m5mZVc7Fx8zMKufiY2ZmlXPxMTOzyrn4\nmJlZ5Vx8rC9IekXSrKbb+nXHVIXUqXm2pKMHjH9J0oNN78fJdcVo1oovtba+IGlBRKw4yP4lY1E/\nr74g6Y0UnYjf1GLfl4AFEXHKII8fExGvlBiiWVs+87G+JekQSZdI+gUwOY0dJ+mmdLbw5aZjv5DW\navl1Ops4No1PlbRN2l49telprA/09abn+mQan5ge01g/5/zGmieStpX0R0m3Spouaayk30vasimO\n6yW9dUAey0o6O62jMlPSzmnXZGDNdGbzrg7fk7mSvijpD8A+kt4s6WpJM1Ism6bjNpB0Q8rvPyQt\naMrvyqbnO03SIWn77ZJ+l57rmqaWLlMlfS3l/KdGrOk9PCXlNVvSEZJ2lXRF0/PvJulyrO8sOfQh\nZqPCcpJmpe37IuLDaXtH4K0R8YSk3Slao2xH8evuSZLeTdFsc3+Kbt9LArdQdD8ezGEU7Ui2lbQM\ncL2kyWnfVsBbKPpeXQ/sJGk6cBGwX0TcJGkl4AXgDIo1Zo6StDHFr+1nD3itwwEiYotUHCanYz9I\n0bFhS1o7WtJH0/bnI6LRDudvEfFOAEnXAp+KiHskbQ98n6L/3akUzUh/IunwId6LRu/A7wJ7RcRj\nkvYDTqL4pTwUSxhsJ2lP4ESKvmufoGj6uVVELJS0GvAk8D1Ja0TEY8ChwNlDvb6NPi4+1i9eaPOP\n8JSIeCJt755uM9P9FSmK0Vjgioh4HkBSJ/38dgfeKqnRI2zl9FwvAdMjYl56rlkU66s8DTwcETcB\nROoqLukS4N8lHUfxD/U5LV7rnRT/sBMRd0n6K7AxMFhncoBvtZl2uyi99ooUjTYv0aIFKZdJf+4E\n/GPaPg/42hCvtQmwOTAlPdcYijYvDY2zlxkU7wcUBeiHjenQxuck6Tzgo5LOpvifh4OGeG0bhVx8\nrN8917Qt4KsR8aPmAyQdRfvW8AtZND297IDnOqLpbKLxXBOBF5uGXqH470ytXiMinpc0haL1/74U\nvbMGGv5Sxa013pMlKNa2aXfm1Oo9aX4/YNF7IuD2iNixzXM13pPG+9F4TKvXOBv4BfA34JJ++67O\nCv7Ox3JyDfCx9H/8SFpb0prAdcCHJS0naSzw902PmQu8PW3vPeC5Pp2mm5C0sYpF2tq5C1hL0rbp\n+LFa1Nb/DOA7wE1NZ2nNrgMObLwOsB5Fg8sRSWdf90naJz23JL0t7b6eYiqSxmsnfwU2U9HheWWK\nTtGkeNaQtGN6rqUkvWWIECZ1fJPcAAABJ0lEQVQDn2q8D2najYh4iGLK8t9ofSZofcDFx7IREZOB\nnwE3SLqNYm2bsRFxC8VU1CyKNY9+3/SwUyiKzB+B1ZvGz6Boz3+LpDkUSw23nUmIiJeA/YDvSroV\nmEI6a4hiobtnaP/dxveBMSnmi4BDIuLFNscurgOBw1JMt1OcgQEcCRwu6SaKKcVGHg8AF1N0yD6f\nNIWZ8tsb+Fp6rlkUU3qDOYNi2YPZ6TEfadp3PvBARNwxsvSsV/lSa7MB1MFlyl1+vbWAqcCmEfHq\nEIfXQkNcyl7C650GzIyIM6t6TauWz3zMaiTpIGAaxdpRPVl4qiZpBvBW4Kd1x2Ll8ZmPmZlVzmc+\nZmZWORcfMzOrnIuPmZlVzsXHzMwq5+JjZmaVc/ExM7PK/X8iUNUW63zjQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a20bd6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ffd = FreqDist(fd_3.values())\n",
    "from pylab import *\n",
    "y = [0]*14\n",
    "for k, v in ffd.items():\n",
    "     if k <= 10:\n",
    "        y[k-1] = v\n",
    "     elif k >10 and k <= 50:\n",
    "        y[10] =  y[10] + v\n",
    "     elif k >50 and k <= 100:\n",
    "        y[11] =  y[11] + v\n",
    "     elif k > 100 and k <= 500:\n",
    "        y[12] =  y[12] + v\n",
    "     else:\n",
    "        y[13] =  y[13] + v\n",
    "x = range(1, 15) # generate integer from 1 to 14\n",
    "ytks =list(map(str, range(1, 11))) # covert a integer list to a string list\n",
    "ytks.append('10-50')\n",
    "ytks.append('51-100')\n",
    "ytks.append('101-500')\n",
    "ytks.append('>500')\n",
    "barh(x,y, align='center')\n",
    "yticks(x, ytks)\n",
    "xlabel('Frequency of Frequency')\n",
    "ylabel('Word Frequency')\n",
    "grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bar chart generated above shows how many word types occur with a certain frequency.\n",
    "There are 241 types occurring over 500 times and therefore individually accounting for about 1% of\n",
    "the vocabulary. \n",
    "However, on the other extreme, more than one-third of the word types occur only once in the Reuters corpus.\n",
    "Note that the majority of word types occur quite infrequently given the size of the whole corpus (i.e., 721,371 word tokens):\n",
    "about 78% of the word types occur 10 times or less.\n",
    "Similarly, you can also look at the bar chart based on the document frequency. Try it by yourself!\n",
    "\n",
    "Let's further remove those words that occur only once. \n",
    "To get those words, you can write the code like\n",
    "```python\n",
    "    lessFreqWords = set([k for k, v in fdist.items() if v < 2])\n",
    "```\n",
    "or choose to use `hapaxes()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lessFreqWords = set(fd_3.hapaxes())\n",
    "\n",
    "def removeLessFreqWords(fileid):\n",
    "    return (fileid, [w for w in tokenized_reuters[fileid] if w not in lessFreqWords])\n",
    "\n",
    "#pool = mp.Pool(4)\n",
    "#tokenized_reuters = dict(pool.map(removeLessFreqWords, reuters.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_reuters = dict(removeLessFreqWords(fileid) for fileid in reuters.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should have a pretty clean set of Reuters articles, each of which is stored as a list of word tokens.\n",
    "Let's further print out some statistics that summarize this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  17403\n",
      "Total number of tokens:  711311\n",
      "Lexical diversity:  45.61580303464071\n",
      "Total number of articles: 10788\n",
      "Average document length: 65.93539117538005\n",
      "Maximun document length: 703\n",
      "Minimun document length: 0\n",
      "Standard deviation of document length: 65.71555877530848\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "vocab = set(words)\n",
    "print (\"Vocabulary size: \",len(vocab))\n",
    "print (\"Total number of tokens: \", len(words))\n",
    "print (\"Lexical diversity: \", lexical_diversity)\n",
    "print (\"Total number of articles:\", len(tokenized_reuters))\n",
    "lens = [len(value) for value in tokenized_reuters.values()]\n",
    "print (\"Average document length:\", np.mean(lens))\n",
    "print (\"Maximun document length:\", np.max(lens))\n",
    "print (\"Minimun document length:\", np.min(lens))\n",
    "print (\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the minimun document length is 0. There must be some Reuters articles that are extremely short,\n",
    "after tokenization and stopping, there are no words left. Can you check those documents to see what they look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Vector Representation\n",
    "\n",
    "After text pre-processing has been completed, each individual document needs to be transformed into \n",
    "some kind of numeric representation that can be input into most NLP and text mining algorithms.\n",
    "For example, classification algorithms, such as Support Vector Machine, can only take data in a \n",
    "structured and numerical form. They do not accept free languge text.\n",
    "The most popular structured representation of text is the vector-space model, which represents text\n",
    "as a vector where the elements of the vector indicate the occurence of words within the text.\n",
    "The vector-space model makes an implicit assumption that \n",
    "the order of words in a text document are not as\n",
    "important as words themselves, and thus disregarded.\n",
    "This assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Given a set of documents and a pre-defined list of words appearing \n",
    "in those documents (i.e., a vocabulary), you can compute a vector representation for each document.\n",
    "This vector representation can take one of the following three forms:\n",
    "* a binary representation,\n",
    "* an integer count,\n",
    "* and a float-valued weighted vector.\n",
    "\n",
    "To highlight the difference among the three approaches, we use a very simple example as follows:\n",
    "```\n",
    "    document_1: \"Data analysis is important.\"\n",
    "    document_2: \"Data wrangling is as important as data analysis.\"\n",
    "    document_3: \"Data science contains data analysis and data wrangling.\"\n",
    "```\n",
    "The three documents contain 20 tokens and 9 unique words.\n",
    "Those unique words are sorted alphabetically with total counts:\n",
    "```\n",
    "     'analysis': 3,\n",
    "     'and': 1,\n",
    "     'as': 2,\n",
    "     'contains': 1,\n",
    "     'data': 6,\n",
    "     'important': 2,\n",
    "     'is': 2,\n",
    "     'science': 1,\n",
    "     'wrangling': 2\n",
    "```\n",
    "Given the vocabulary above, \n",
    "both the binary and the integer count vectors are easy to compute.\n",
    "A binary vector stores 1s for the word that appears in a document and 0s for the other words in\n",
    "the vocabulary,\n",
    "whereas a count vector stores the frequency of each word appearing in the document.\n",
    "Thus, the binary vector representations for the three documents above are\n",
    "   \n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|1|0|1|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|1|0|0|1|1|\n",
    "\n",
    "The count vector representations for the same documents would look as follows:\n",
    "\n",
    "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
    "   |document 2:|1|0|2|0|2|1|1|0|1|\n",
    "   |document 3:|1|1|0|1|3|0|0|1|1|\n",
    "\n",
    "Instead of using the two vector representations above, \n",
    "most existing text analysis algorithms, like document classification and information retrieval, \n",
    "prefer representing documents as weighted vectors.\n",
    "The raw term frequency is often replaced with a weighted term frequency\n",
    "that indicates how important a word is in a particular document.\n",
    "There are many different term weighting schemes online.\n",
    "To store each document as a weighted vector, we first need to choose a weighting scheme. \n",
    "The most popular scheme is the TF-IDF weighting approach. \n",
    "TF-IDF stands for term frequency-inverse document frequency. \n",
    "The term frequency for a word is the number of times the word appears in a document. \n",
    "In the preceding example, the term frequency in Document 2 for data is 2, since it appears twice in the document. Document frequency for a word is the number of documents that contain the word; \n",
    "it would also be 3 for data in the collection of the three preceding documents. \n",
    "The Wikipidia entry on [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) lists \n",
    "a number of variants of TF-IDF. \n",
    "One variant is reproduced here\n",
    "$$tf\\cdot idf(w,d) = tf(w, d) * idf(w)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$tf(w,d)\\,=\\, \\sum_{i}^{|d|} 1_{w = w_{d,i}}$$\n",
    "and\n",
    "$$idf(w) = log\\left(\\frac{|D|}{|d \\in D: w \\in d |}\\right)$$\n",
    "\n",
    "The assumption behind TF-IDF is that words with high term frequency should receive high weight unless they also have high document frequency. \n",
    "Stopwords are the most commonly occurring words in the English language. They often occur many times within a single document, but they also occur in nearly every document. \n",
    "These two competing effects cancel out to give them low weights,\n",
    "as those very common words carry very little meaningful information about the actual contents of the document.\n",
    "Therefore, the TF-IDF weights for stopwords are almost always 0.\n",
    "With the TF-DF formulas above,\n",
    "the weighted vector representations for the example documents are computed as\n",
    "\n",
    "||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
    "   |-|-|-|-|-|-|-|-|-|\n",
    "   |document 1:|0|0|0|0|0|0.176|0.176|0|0|\n",
    "   |document 2:|0|0|0.954|0|0|0.176|0.176|0|0.176|\n",
    "   |document 3:|0|0.477|0|0.477|0|0|0|0.477|0.176|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the cleaned up Reuters documents, how can we generate those vectors for each documents? \n",
    "Unfortunately, NLTK does not implement methods that directly produce those vectors.\n",
    "Therefore, we will either write our own code to compute them or appeal to other data analysis libraries.\n",
    "Here we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine \n",
    "learning library for Python.\n",
    "If you use Anaconda, you should already have scikit-learn installed, otherwise you will need to \n",
    "[install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n",
    "\n",
    "Although scikit-learn features various classification, regression and clustering algorithms\n",
    "we are particularly interested in its feature extraction module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\n",
    "This module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\" Please refer to its documentation on text feature extraction,\n",
    "section 4.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). We will demonstrate the usage of the following two classes:\n",
    "* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of text documents to a matrix of token counts. \n",
    "* [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "It converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "### 2.1 Creating Count Vectors\n",
    "Let's start with generating the count vector representation for each Reuters document.\n",
    "Initialise the \"CountVector\" object: since we have pre-processed all the Reuters documents, \n",
    "the parameters, \"tokenizer\", \"preprocessor\" and \"stop_words\" are set to their default value, i.e., None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transform Reuters articles into feature vectors. `fit_transform` does two things: First, it fits the model and learns the vocabulary; second it transforms the text data into feature vectors. \n",
    "Please note the input to `fit_transform` should be a list of strings. \n",
    "Since we have stored each tokenised article as a list of words, we concatenate all the words in the list and separate\n",
    "them with white spaces. \n",
    "The following code will do that:\n",
    "```python\n",
    "[' '.join(value) for value in tokenized_reuters.values()]\n",
    "```\n",
    "Then, we input this list of strings into `fit_transform`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10788, 17403)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\n",
    "print (data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of document-by-word matrix should be 10788 * 17403. \n",
    "However, in order to save such a matrix in memory but also to speed up algebraic operations on the matrix,\n",
    "scikit-learn implements matrix/vector in a sparse representation.\n",
    "Let's check the count vector for the first article, i.e., 'training/1684'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bancorporation : 1\n",
      "index : 1\n",
      "bwrlf : 1\n",
      "glimpse : 2\n",
      "cannon : 1\n",
      "msu : 1\n",
      "omt : 1\n",
      "amounted : 1\n",
      "cohen : 2\n",
      "distributes : 1\n",
      "kock : 1\n",
      "brett : 2\n",
      "slowing : 1\n",
      "life : 1\n",
      "intermediaries : 1\n",
      "londrina : 1\n",
      "cultural : 1\n",
      "vismara : 1\n",
      "reiterated : 5\n",
      "mcgroarty : 1\n",
      "ibn : 2\n",
      "varieties : 1\n",
      "switzerland : 1\n",
      "guinness : 1\n",
      "syracuse : 4\n",
      "macandrews : 2\n",
      "allocates : 1\n",
      "buchbinder : 1\n",
      "stems : 1\n",
      "waree : 1\n",
      "volt : 1\n",
      "creek : 1\n",
      "idling : 1\n",
      "allotment : 1\n",
      "sheikh : 1\n",
      "acc : 1\n",
      "districts : 1\n",
      "crafts : 1\n",
      "euromarket : 1\n",
      "regime : 1\n",
      "treybig : 1\n",
      "psa : 1\n",
      "keating : 2\n",
      "rabt : 1\n",
      "zhejiang : 2\n",
      "spread : 1\n",
      "modulaire : 2\n",
      "tape : 1\n",
      "quartz : 1\n",
      "insisted : 1\n",
      "celebrating : 2\n",
      "jmy : 1\n",
      "directorate : 1\n",
      "propaganda : 1\n",
      "willa : 2\n",
      "pel : 6\n",
      "peltier : 1\n",
      "disincentive : 1\n",
      "alleviate : 2\n",
      "quel : 2\n",
      "clothier : 1\n",
      "zambian : 1\n",
      "cooke : 4\n",
      "pro : 1\n",
      "larger : 1\n",
      "corportation : 1\n",
      "signup : 1\n",
      "pacer : 1\n",
      "jefferson : 1\n",
      "confronted : 1\n",
      "trim : 3\n",
      "texas : 1\n",
      "marriott : 6\n",
      "head : 1\n",
      "equitorial : 1\n",
      "pom : 1\n",
      "borealis : 1\n",
      "spotty : 1\n",
      "assam : 1\n",
      "contracts : 1\n",
      "internal : 1\n",
      "chem : 1\n",
      "bancroft : 1\n",
      "sk : 1\n",
      "kurlack : 1\n",
      "gulfstream : 1\n",
      "reinvested : 2\n",
      "neighbours : 1\n",
      "mousavi : 1\n",
      "unit : 1\n",
      "mexican : 1\n",
      "markedly : 1\n",
      "sows : 1\n",
      "productivity : 4\n",
      "solita : 1\n",
      "relationships : 1\n",
      "fertiliser : 1\n",
      "ball : 5\n",
      "aero : 1\n",
      "represents : 1\n",
      "revenues : 1\n",
      "food : 3\n",
      "setbacks : 1\n",
      "safeguards : 1\n",
      "conable : 1\n",
      "condemned : 12\n",
      "inspections : 4\n",
      "invacare : 1\n",
      "cooperation : 1\n",
      "auto : 4\n",
      "transamerica : 3\n",
      "ra : 1\n",
      "owned : 1\n",
      "forstman : 2\n",
      "efp : 1\n",
      "yan : 2\n",
      "gluck : 1\n",
      "tip : 1\n",
      "americas : 1\n",
      "collect : 1\n",
      "lead : 3\n",
      "unseasonal : 2\n",
      "eldorado : 1\n",
      "dramatically : 1\n",
      "siebe : 1\n",
      "fostered : 1\n",
      "upset : 1\n",
      "mrs : 2\n",
      "intervenes : 1\n",
      "annualized : 1\n",
      "insurances : 1\n",
      "personnel : 2\n",
      "persuasive : 1\n",
      "amicable : 1\n",
      "btx : 1\n",
      "ads : 3\n",
      "drifted : 1\n",
      "unleaded : 1\n",
      "monopolises : 1\n",
      "madeira : 1\n",
      "affairs : 1\n",
      "float : 1\n",
      "efficient : 1\n",
      "ronnskar : 1\n",
      "thermo : 1\n",
      "jcp : 1\n",
      "triumph : 1\n",
      "lacy : 1\n",
      "samaila : 2\n",
      "soymeal : 2\n",
      "aggravated : 1\n",
      "mays : 1\n",
      "bethesda : 1\n",
      "caisse : 1\n",
      "concentrating : 1\n",
      "fighting : 1\n",
      "landslide : 1\n",
      "undercutting : 1\n",
      "performances : 1\n",
      "incredible : 2\n",
      "morita : 1\n",
      "misunderstood : 2\n",
      "borrowers : 1\n",
      "legended : 1\n",
      "disagreed : 1\n",
      "centigrade : 1\n",
      "congestion : 1\n",
      "relating : 3\n",
      "oapec : 1\n",
      "receivership : 1\n",
      "neighbour : 1\n",
      "revalued : 1\n",
      "dissemination : 1\n",
      "relevance : 1\n",
      "doj : 1\n",
      "enforcing : 1\n",
      "stays : 1\n",
      "separate : 1\n",
      "paranavai : 1\n",
      "target : 1\n",
      "translation : 1\n",
      "shandwick : 1\n",
      "subordinated : 1\n",
      "optimum : 1\n",
      "frictions : 1\n",
      "catastrophe : 2\n",
      "adequate : 1\n",
      "pronounced : 1\n",
      "impatience : 1\n",
      "squeeze : 1\n",
      "reorganisation : 1\n",
      "mohler : 1\n",
      "vongarlem : 1\n",
      "montedision : 1\n",
      "cocoas : 3\n",
      "clothes : 2\n",
      "bahamas : 1\n",
      "undeveloped : 1\n",
      "atwell : 1\n",
      "releated : 1\n",
      "heck : 1\n",
      "pipes : 1\n",
      "neic : 1\n",
      "arrow : 1\n",
      "licensees : 1\n",
      "nps : 3\n",
      "pendleton : 2\n",
      "note : 1\n",
      "abandoning : 1\n",
      "esb : 1\n",
      "reade : 1\n",
      "rob : 1\n",
      "abs : 1\n",
      "mann : 1\n",
      "emphasise : 3\n",
      "treasurers : 1\n",
      "radio : 4\n",
      "procurement : 1\n",
      "reimposing : 1\n",
      "canned : 5\n",
      "depict : 1\n",
      "valley : 1\n",
      "auctions : 1\n",
      "rtc : 1\n",
      "volatile : 2\n",
      "alternatively : 1\n",
      "indian : 1\n",
      "lvi : 1\n",
      "urge : 15\n",
      "nolex : 1\n",
      "mixon : 1\n",
      "cheyenne : 2\n",
      "projection : 1\n",
      "kits : 1\n",
      "vote : 1\n",
      "hasco : 1\n",
      "reversion : 1\n",
      "arabian : 2\n",
      "highs : 1\n",
      "feedgrains : 1\n",
      "psm : 4\n",
      "icahn : 1\n"
     ]
    }
   ],
   "source": [
    "vocab2 = vectorizer.get_feature_names()\n",
    "for word, count in zip(vocab, data_features.toarray()[0]):\n",
    "    if count > 0:\n",
    "        print (word, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get the count list above is to use `FreqDist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'commission': 1,\n",
       "          'common': 2,\n",
       "          'company': 1,\n",
       "          'control': 1,\n",
       "          'cuts': 1,\n",
       "          'dealings': 1,\n",
       "          'dlrs': 1,\n",
       "          'erc': 5,\n",
       "          'exchange': 1,\n",
       "          'filing': 1,\n",
       "          'intention': 1,\n",
       "          'international': 2,\n",
       "          'investment': 2,\n",
       "          'jan': 1,\n",
       "          'lowered': 1,\n",
       "          'lt': 1,\n",
       "          'march': 1,\n",
       "          'nevada': 1,\n",
       "          'outstanding': 1,\n",
       "          'parsow': 2,\n",
       "          'partnership': 4,\n",
       "          'pct': 2,\n",
       "          'prices': 1,\n",
       "          'purposes': 1,\n",
       "          'ranging': 1,\n",
       "          'securities': 1,\n",
       "          'seeking': 1,\n",
       "          'shares': 3,\n",
       "          'sold': 1,\n",
       "          'stake': 2,\n",
       "          'stock': 2,\n",
       "          'total': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(tokenized_reuters['training/1684'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocabulary you just got with `vectorizer.get_feature_names()`  shoud be exactly the same\n",
    "as the one you got in section 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab-set(vocab2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating TF-IDF Vectors\n",
    "Similar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying \n",
    "the value of \"analyzer\", and then covert the Reuters data into a list of strings, each of which corresponds\n",
    "to a Reuters articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 17403)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\")\n",
    "tfs = tfidf.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\n",
    "tfs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the weighted vector for the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting : 0.038235730031632956\n",
      "action : 0.030603756662730266\n",
      "advantage : 0.043914840084456494\n",
      "alleged : 0.08925586636134304\n",
      "american : 0.027671241326262693\n",
      "analyst : 0.034717404878887824\n",
      "april : 0.020867068588964373\n",
      "asia : 0.043780644169735164\n",
      "asian : 0.08810338703203563\n",
      "asked : 0.030564378365548584\n",
      "association : 0.03139802300392408\n",
      "australia : 0.07291864821824522\n",
      "australian : 0.03703556377377631\n",
      "awaiting : 0.048841945274933685\n",
      "aware : 0.045948291554218285\n",
      "barriers : 0.0435198255765529\n",
      "beef : 0.04202909114182352\n",
      "biggest : 0.03991321083427934\n",
      "billion : 0.09865210654969163\n",
      "block : 0.04071770792857356\n",
      "boost : 0.06987132219065047\n",
      "broker : 0.045948291554218285\n",
      "budget : 0.03324004443226463\n",
      "business : 0.026607771245008217\n",
      "businessmen : 0.18102651014822307\n",
      "button : 0.13259386759447706\n",
      "call : 0.036789599291540416\n",
      "canberra : 0.06349481614925918\n",
      "capel : 0.050738543313134155\n",
      "capitals : 0.05671643027940679\n",
      "centred : 0.05590244674236398\n",
      "chairman : 0.0270419853614545\n",
      "chief : 0.032956772300091565\n",
      "coal : 0.0423456750026592\n",
      "commercial : 0.03353542582172308\n",
      "complete : 0.03771932513432333\n",
      "concern : 0.035237823865711976\n",
      "concerns : 0.0435198255765529\n",
      "conflict : 0.047135926761575055\n",
      "continue : 0.028595905355944744\n",
      "correspondents : 0.05671643027940679\n",
      "cost : 0.031375908193189736\n",
      "countries : 0.05481032408618094\n",
      "country : 0.02961217277183705\n",
      "curbs : 0.0938465355064654\n",
      "cut : 0.027645454458816766\n",
      "damage : 0.07745914763527406\n",
      "day : 0.030584039466881263\n",
      "defuse : 0.05517431365662817\n",
      "democratic : 0.045948291554218285\n",
      "deputy : 0.08109828341397356\n",
      "deterioration : 0.047135926761575055\n",
      "diplomatic : 0.04735533796630917\n",
      "disadvantage : 0.05590244674236398\n",
      "dispute : 0.07695648333440674\n",
      "dlrs : 0.08064458082945693\n",
      "domestically : 0.0503840618977123\n",
      "due : 0.025561467598290363\n",
      "economic : 0.054297897098674185\n",
      "economy : 0.0619372763783101\n",
      "effort : 0.03835592214732308\n",
      "electric : 0.03991321083427934\n",
      "electronics : 0.16026716839278932\n",
      "emergency : 0.043268493426119235\n",
      "end : 0.025764316991845566\n",
      "erosion : 0.05391431263142745\n",
      "estimates : 0.03447118999818609\n",
      "exchange : 0.023341662438252864\n",
      "expand : 0.03755542324374333\n",
      "export : 0.02716093618886878\n",
      "exporters : 0.1070283720576721\n",
      "exporting : 0.0425650862073933\n",
      "exports : 0.1607237071113289\n",
      "extended : 0.03744827578804067\n",
      "failure : 0.03983768142175566\n",
      "fear : 0.044191311849579286\n",
      "fears : 0.04182581553269218\n",
      "federation : 0.042677460593225774\n",
      "financial : 0.026188802215869\n",
      "firm : 0.02921335794904132\n",
      "firms : 0.03381251350168583\n",
      "fiscal : 0.030885872260308166\n",
      "foreign : 0.02526337837392109\n",
      "friction : 0.04857089592865102\n",
      "friday : 0.03315383700518558\n",
      "gain : 0.02740516204309047\n",
      "goods : 0.06542278020957684\n",
      "government : 0.023924809522229435\n",
      "group : 0.02198567553036606\n",
      "half : 0.02985590638114546\n",
      "halt : 0.04433380911359572\n",
      "helped : 0.038924259155361275\n",
      "hit : 0.03531547493497943\n",
      "hong : 0.1618655031125596\n",
      "hurt : 0.04182581553269218\n",
      "impact : 0.033875626930809054\n",
      "import : 0.03263149473567427\n",
      "imports : 0.14210468607034749\n",
      "impose : 0.04202909114182352\n",
      "include : 0.029595087787963204\n",
      "industrial : 0.029646469976012446\n",
      "industry : 0.08098491732704524\n",
      "interest : 0.02461963713407169\n",
      "international : 0.023225280458743985\n",
      "james : 0.03486214048218121\n",
      "japan : 0.31539909028696617\n",
      "japanese : 0.12095369531503587\n",
      "john : 0.03811759258295876\n",
      "kind : 0.043780644169735164\n",
      "kong : 0.1635619549575162\n",
      "korea : 0.11906716434130579\n",
      "kuroda : 0.05763924870286683\n",
      "large : 0.031785224201264284\n",
      "largest : 0.06235999265476794\n",
      "lawrence : 0.05111219498344811\n",
      "lead : 0.06794278225997828\n",
      "leading : 0.03447118999818609\n",
      "length : 0.05284899694395097\n",
      "liberal : 0.04671695773243584\n",
      "loss : 0.021092709734134078\n",
      "lt : 0.03265736563001262\n",
      "major : 0.05063659900737532\n",
      "makoto : 0.05763924870286683\n",
      "malaysia : 0.041926705973519975\n",
      "manoeuvres : 0.061506682038322665\n",
      "manufacturers : 0.04097852652175582\n",
      "market : 0.020838172912900032\n",
      "markets : 0.055735544540417875\n",
      "matsushita : 0.061506682038322665\n",
      "matter : 0.04182581553269218\n",
      "measure : 0.039261441757101945\n",
      "measures : 0.06813700201184225\n",
      "meet : 0.03309695707539404\n",
      "michael : 0.04106770030684036\n",
      "mills : 0.044051693516017815\n",
      "minister : 0.08392475917773479\n",
      "miti : 0.048310077335468764\n",
      "mln : 0.012442151455344176\n",
      "months : 0.025254267136665703\n",
      "mounting : 0.04941818493958739\n",
      "move : 0.030332707316447608\n",
      "murtha : 0.059964565415544034\n",
      "nakasone : 0.04509401127059949\n",
      "named : 0.03968905478043526\n",
      "nations : 0.03195118794778464\n",
      "newspapers : 0.04972538540121205\n",
      "office : 0.035237823865711976\n",
      "officers : 0.04671695773243584\n",
      "official : 0.05200735698354712\n",
      "officials : 0.05473594680402399\n",
      "open : 0.03133188974846607\n",
      "outcome : 0.04477982660175362\n",
      "outlined : 0.0469232677532327\n",
      "outweighed : 0.05763924870286683\n",
      "package : 0.03905716464482311\n",
      "pact : 0.03335668639874077\n",
      "partners : 0.03447118999818609\n",
      "party : 0.039330898346337496\n",
      "paul : 0.0396159231725238\n",
      "pct : 0.030564328834757053\n",
      "place : 0.035513507752767765\n",
      "pressure : 0.06725311905281636\n",
      "prevent : 0.038858752764836996\n",
      "prime : 0.033781171622764214\n",
      "problems : 0.03413400739644541\n",
      "produced : 0.036322470677561294\n",
      "producers : 0.03169214707099988\n",
      "products : 0.08105531677455703\n",
      "program : 0.03128814990335975\n",
      "promotion : 0.0503840618977123\n",
      "proposed : 0.02927824924700526\n",
      "protectionist : 0.04054914170698678\n",
      "public : 0.03040907132500602\n",
      "purpose : 0.046321943224532244\n",
      "put : 0.03212123556794458\n",
      "quickly : 0.04022386414256948\n",
      "raised : 0.03176183713171253\n",
      "record : 0.02279753166321871\n",
      "reform : 0.0416283486000593\n",
      "relations : 0.041436367575598765\n",
      "remain : 0.032195413802006335\n",
      "remove : 0.04525662753705577\n",
      "representative : 0.03860276526060169\n",
      "reserves : 0.030428294904737016\n",
      "restraining : 0.05284899694395097\n",
      "retaliation : 0.0799791496856755\n",
      "reuter : 0.04182581553269218\n",
      "rift : 0.061506682038322665\n",
      "row : 0.042791692490817095\n",
      "ruling : 0.0396159231725238\n",
      "safe : 0.05517431365662817\n",
      "sales : 0.02175687322771273\n",
      "sell : 0.027788487968998913\n",
      "selling : 0.03353542582172308\n",
      "semiconductors : 0.13094701357125857\n",
      "senior : 0.06749994248529811\n",
      "sentiment : 0.0435198255765529\n",
      "seriousness : 0.061506682038322665\n",
      "serves : 0.048841945274933685\n",
      "share : 0.0196127002248661\n",
      "significant : 0.03559433759182549\n",
      "similar : 0.036144115683152095\n",
      "smith : 0.04364900452375286\n",
      "solve : 0.04576877828067168\n",
      "sources : 0.028264350500815716\n",
      "south : 0.09406159177270869\n",
      "spending : 0.07319788488536816\n",
      "spokesman : 0.026432803103132972\n",
      "spokesmen : 0.05150720949135171\n",
      "stick : 0.050738543313134155\n",
      "stimulate : 0.04106770030684036\n",
      "stock : 0.021082744523384763\n",
      "subject : 0.03062353027409489\n",
      "supplementary : 0.05590244674236398\n",
      "surplus : 0.09341161323222547\n",
      "swell : 0.06349481614925918\n",
      "taiwan : 0.1551754458674922\n",
      "taiwanese : 0.05336114768756689\n",
      "talks : 0.029476661126991384\n",
      "tariffs : 0.17817554909489694\n",
      "taxes : 0.03912458832554064\n",
      "textile : 0.04735533796630917\n",
      "threat : 0.042677460593225774\n",
      "time : 0.02655261634365745\n",
      "tokyo : 0.07160129522524471\n",
      "told : 0.022216443964373063\n",
      "tom : 0.04972538540121205\n",
      "tough : 0.043780644169735164\n",
      "trade : 0.3403484004161321\n",
      "trading : 0.028056384784645044\n",
      "unofficial : 0.05671643027940679\n",
      "view : 0.07447767876581532\n",
      "virtually : 0.04462793318067152\n",
      "warning : 0.047135926761575055\n",
      "washington : 0.03306869170896893\n",
      "week : 0.02413840757160281\n",
      "works : 0.04651662856225648\n",
      "world : 0.05295226319805183\n",
      "worried : 0.04651662856225648\n",
      "yasuhiro : 0.04559381013879644\n",
      "year : 0.059426982680502526\n",
      "yesterday : 0.02754325854911457\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "for word, weight in zip(vocab, tfs.toarray()[0]):\n",
    "    if weight > 0:\n",
    "        print (word, \":\", weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have converted all the Reuters articles into feature vectors. \n",
    "We can use those vectors to, for example,\n",
    "* compute the similarity between two articles, \n",
    "* search articles for a given query\n",
    "* do other advance text analysis, such as document classification and clustering.\n",
    "\n",
    "Assume that we have a new document, how can we get its TF-IDF vector.\n",
    "We do this by using the transform function as follows.\n",
    "We have randomly chosen a sentence from \n",
    "[a recent Reuters news](http://www.reuters.com/article/us-usa-election-idUSKCN0W346T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win  -  0.307399276859104\n",
      "vermont  -  0.4040204786477111\n",
      "step  -  0.25696902690642953\n",
      "states  -  0.17542070972270207\n",
      "state  -  0.18734841995701546\n",
      "senator  -  0.32522928223627556\n",
      "secretary  -  0.20107422757443014\n",
      "nomination  -  0.39136988850775073\n",
      "hoped  -  0.2667815869522921\n",
      "fight  -  0.3013242256926447\n",
      "democratic  -  0.2923711237015126\n",
      "big  -  0.252077968138518\n"
     ]
    }
   ],
   "source": [
    "str = \"\"\"\n",
    "the former secretary of state hoped to win enough states to take a big step toward wrapping up her nomination fight\n",
    "with a democratic senator from Vermont.\n",
    "\"\"\"\n",
    "response = tfidf.transform([str])\n",
    "for col in response.nonzero()[1]:\n",
    "    print (vocab[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the text above is not included in the trained TF-IDF model with the 'transform' function, unless the `fit_transform` function is called,\n",
    "\n",
    "Both `CountVectorizer` and `TfidfVectorizer` come with their own options to automatically do pre-processing, tokenization, and stop word removal -- for each of these, instead of using their default value (i.e., None),\n",
    "we could customise the two vectorizer classes by either using a built-in method or specifying our own function.\n",
    "See the function documentation for more details.\n",
    "However, we wanted to write our own function for clean the text data in this chapter to show you how \n",
    "it's done step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Saving Pre-processed Text to a File\n",
    "The pre-processed text needs to be saved in a proper format so that it can be easily used by the downstream analysis algorithm. There are a couple of ways of dumping the pre-processed text data into txt files. \n",
    "For example, use one txt file to store the tokenized documents. The tokens in a document are stored in one row in the txt file, and are separated with a given delimiter, e.g., whitespace. In this case, the downstream text analyser needs to re-construct the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_file = open(\"./reuters_1.txt\", 'w')\n",
    "for d in tokenized_reuters.values():\n",
    "    out_file.write(' '.join(d) + '\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save vocabulary in a separate file, and assign a fixed integer id to each word in the vocabulary. What text analysers usually do is to use the index of each word in the vocabulary as its integer id.\n",
    "Given the vocabulary, each document can be represented as a sequence of integers that correspond to the tokens,\n",
    "or in the following sparse form:\n",
    "```\n",
    "    word_index:word count\n",
    "```\n",
    "for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_file = open(\"./reuters_2.txt\", 'w')\n",
    "vocab = list(vocab)\n",
    "\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "for d in tokenized_reuters.values():\n",
    "    d_idx = [vocab_dict[w] for w in d]\n",
    "    for k, v in FreqDist(d_idx).items():\n",
    "        out_file.write(\"{}:{} \".format(k,v))\n",
    "    out_file.write('\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Other Features\n",
    "\n",
    "It is common for most text analysis tasks to treat documents as bags-of-words, which can significantly simplify the inference procedure of text analysis algorithms. \n",
    "However, things always have pros and cons. \n",
    "The bag-of-words representation loses lots of information encoded in either syntax or word order (i.e., dependencies between adjacent words in sentences.). \n",
    "For example, representing a document as a collection of unigrams effectively disregards any word order dependence,\n",
    "which fails to capture phrases and multi-word expressions. A similar issue has been mentioned in section 2.1. of Chapter 2. \n",
    "In this section, we are going to show you how to\n",
    "* use Part-of-Speeching (POS) tagging to extract specific word groups, such as all nouns, verbs, etc.,\n",
    "* extract n-grams,\n",
    "*  and extract collocations\n",
    "\n",
    "These features can be further used to enrich the representation of a document.\n",
    "\n",
    "### 4.1 Extracting Nouns and Verbs\n",
    "It is easy for human to tell the difference between nouns, verbs, \n",
    "adjectives and adverbs, as we have learnt them back in elementary school.\n",
    "However, how can we automatically classify words into their parts of speech (i.e., lexical categories or word classes) \n",
    "and label them accordingly with computer program? \n",
    "This section is not going to discuss how to determine the category of a word from a linguistic perspective.\n",
    "Instead it demonstrates the use of some existing POS taggers to extract words in a specific lexical category.\n",
    "It has been proven that words together with their part-of-speech (POS) are quite useful for many language processing tasks. \n",
    "\n",
    "In NLP, the process of labelling words with their corresponding part-of-speech (POS) tags is known as [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).\n",
    "A POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context. There are many POS taggers available online, such as [Sandford POS tagger](http://nlp.stanford.edu/software/tagger.shtml). \n",
    "We are going to use the one implemented by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - '/Users/eileen/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b715895d9d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtagged_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtagged_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - '/Users/eileen/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "example_sent = 'A POS tagger processes a sequence of words and attaches a POS tag to each \\\n",
    "word based on both its definition and its context'\n",
    "text = nltk.word_tokenize(example_sent)\n",
    "tagged_sent = nltk.tag.pos_tag(text)\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are seeing these tags for the first time, you will wonder what these tags mean. \n",
    "You can find the specification of all the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \n",
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (nltk.help.upenn_tagset('NNP'))\n",
    "print (nltk.help.upenn_tagset('IN'))\n",
    "print (nltk.help.upenn_tagset('PRP$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example sentence has been processed by `pos_tag` into a list of tuples, each of which is a pair of a word and its POS tag. We see that 'a' is 'DT', a determiner; 'its' is 'PRP$', a possessive pronoun; 'and' is 'CC', a coordinating conjunction, 'words' is 'NNS', a noun in the plural form, and so on. Note that several of the corpora included in NLTK have been tagged for their POS. Please click [here](http://www.nltk.org/howto/corpus.html#tagged-corpora) to see how to access those tagged corpora.\n",
    "Here is an example of using the `tagged_words` function to retrieve all words in Brown corpus with their tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the collection of tags is known as a tag set. \n",
    "There are many different conventions for tagging words.\n",
    "Therefore, tag sets can vary among different tasks.\n",
    "What we used above is the Penn Treebank tag set.\n",
    "Let's change the tag set to the Universal POS tag set, and print the Brown corpus again.\n",
    "You will find different tags are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to learn more about POS tagging, please refer to [1].\n",
    "\n",
    "Given the tagged text, you can easily identify all the nouns, verbs, etc.\n",
    "Nouns generally refer to people, places, things, or concepts, e.g., Monash, Melbourne, university, data, and science. \n",
    "Nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
    "Now how can we extract all the nouns from a text?\n",
    "Assume we use the Penn Treebank tag set.\n",
    "Here are all the tags for nouns:\n",
    "```\n",
    "    NN    Noun, singular or mass\n",
    "    NNS   Noun, plural\n",
    "    NNP   Proper noun, singular\n",
    "    NNPS  Proper noun, plural\n",
    "```\n",
    "It is not hard to see all the tags above start with 'NN'.\n",
    "Thus, we can iterate over all the words and check if their tag string starts with 'NN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns = [w for w,t in tagged_sent if t.startswith('NN')]\n",
    "all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you will find that all the verb tags start with 'VB', see\n",
    "```\n",
    "    VB\tVerb, base form\n",
    "    VBD   Verb, past tense\n",
    "    VBG   Verb, gerund or present participle\n",
    "    VBN   Verb, past participle\n",
    "    VBP   Verb, non-3rd person singular present\n",
    "    VBZ   Verb, 3rd person singular present\n",
    "```\n",
    "Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = [w for w,t in tagged_sent if t.startswith('VB')]\n",
    "all_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the Reuters corpus that we have been using, has no built-in POS tags. But you can get sentences from Reuters corpus, and then you can get the POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extracting N-grams and Collocations\n",
    "\n",
    "Besides unigrams that we have been working on so far,\n",
    "N-grams of texts are also extensively used in various text analysis tasks.\n",
    "They are basically contiguous sequences of `n` words from a given sequence of text.\n",
    "When computing the n-grams you typically move a fixed size window of size n\n",
    "words forward.\n",
    "For example, for the sentence\n",
    "\"Laughter is like a windshield wiper.\"\n",
    "if N = 2 (known as bigrams), the n-grams would be:\n",
    "```\n",
    "    Laughter is \n",
    "    is like \n",
    "    like a \n",
    "    a windshield \n",
    "    windshield wiper\n",
    "```\n",
    "So you have 5 bigrams in this case. Notice that the generative process above\n",
    "essentially moves one word forward to generate the next bigram.\n",
    "If N = 3 (known as trigrams), the n-grams would be:\n",
    "```\n",
    "    Laughter is like \n",
    "    is like a \n",
    "    like a  windshield\n",
    "    a  windshield wiper\n",
    "```\n",
    "What are N-grams used for? They can be used to build n-gram language model that\n",
    "can be further used for speech recognition, spelling correction, entity detection, etc.\n",
    "In terms of text mining tasks, n-grams is used for developing features for \n",
    "classification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\n",
    "The idea is to expand the unigram feature space with n-grams.\n",
    "But please notice that\n",
    "the use of bigrams and trigrams in your feature space may not necessarily yield significant performance\n",
    "improvement. The only way to know this is to try it! \n",
    "Extracting from a text a list of n-gram can be easily accomplished with function `ngram()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "bigrams = ngrams(reuters.words(), n = 2)\n",
    "fdbigram = FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdbigram.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words that commonly co-occur. \n",
    "\n",
    ">Finding collocations requires first calculating the frequencies of words and\n",
    "their appearance in the context of other words. Often the collection of words\n",
    "will then requiring filtering to only retain useful content terms. Each ngram\n",
    "of words may then be scored according to some association measure, in order\n",
    "to determine the relative likelihood of each ngram being a collocation. (Quoted from [here](http://www.nltk.org/_modules/nltk/collocations.html))\n",
    "\n",
    "For example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by ranking the bigrams by some measures. A commonly used measure is [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). The following code will find the best 100 bigrams using the PMI scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(reuters.words())\n",
    "finder.nbest(bigram_measures.pmi, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collocations` module implements a number of measures to score collocations or other associations. \n",
    "They include Student's t test, Chi-Square, likelihood ratios, PMI and so on.\n",
    "Here we used PMI scores for finding bigrams.\n",
    "Please read [2] for a detailed tutorial on finding collocations with NLTK.\n",
    "If you would like to know more about collocations, please refer to [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "This chapter has show you how to \n",
    "* generate vocabulary be further exploring the tokenized text with some simple statistics. \n",
    "* convert unstructured text to structured form using the bag-of-words model\n",
    "* compute TF-IDF\n",
    "* extract words in specific lexical categories, n-grams and collocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reference Reading Materials\n",
    "1. \"[Categorizaing and Tagging Words](http://www.nltk.org/book/ch05.html)\", \n",
    "Chapter 5 of \"Natural Language Processing with Python\".\n",
    "2. \"[Collocations](http://www.nltk.org/howto/collocations.html)\": An NTLK tutorial on how to extract collocations  .\n",
    "3. \"[Collocations](http://nlp.stanford.edu/fsnlp/promo/colloc.pdf)\": An introduction to collocation by Manning and Schutze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 . Exercises\n",
    "2. We have shown you how to generate frequency of frequency bar chart with term frequency. Similarly, you can generate the bar chart based on document frequency. \n",
    "2. Remove short words. There are some very short words in the vocabulary, for example, 'aa', 'ab', 'ad', 'ax', etc.\n",
    "Write Python code to explore the distribution of word lengths, and remove those words with less than two characters.\n",
    "3. Write code to tag the Reuters corpus with the Penn Treebank tag set, find the top 10 most common tags, nouns, and verbs.\n",
    "2. There might be some text analysis tasks where the binary occurrence markers might be enough. \n",
    "Please modify the CountVectorizer code to generate binary vectors for all the Reuters articles. \n",
    "2. We have shown you how to generate feature vectors from raw text. As we mentioned in section 3, you can actually customise the two vectorizer classes by specifying, for example, the tokenizer and stopword list. So try\n",
    "to customize either vecotorizer so that it can carry out all the steps in section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
