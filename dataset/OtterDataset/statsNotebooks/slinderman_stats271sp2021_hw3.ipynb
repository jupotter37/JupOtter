{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoYdLutn7dUa"
   },
   "source": [
    "# HW3: Hamiltonian Monte Carlo\n",
    "\n",
    "\n",
    "**STATS271/371: Applied Bayesian Statistics**\n",
    "\n",
    "_Stanford University. Winter, 2021._\n",
    "\n",
    "---\n",
    "\n",
    "**Name:** _Your name here_\n",
    "\n",
    "**Names of any collaborators:** _Names here_\n",
    "\n",
    "*Due: 11:59pm Friday, April 23, 2021 via GradeScope*\n",
    "\n",
    "---\n",
    "\n",
    "In this homework assignment you'll perform MCMC with both Metropolis-Hastings Hamiltonian Monte Carlo. We will investigate the Federalist papers---specifically, modeling the rate at which Hamilton (we're using HMC after all!) uses the word _can_ in his papers. \n",
    "\n",
    "We will fit this model using a negative binomial distribution. That is, for each document $n$ that Hamilton wrote, we have the number of times the word 'can' appears $y_n$ as \n",
    "\\begin{align}\n",
    "y_n \\sim \\text{NB}(\\mu_n, r)\n",
    "\\end{align}\n",
    "where \n",
    "\\begin{align}\n",
    "\\text{NB}(y_n \\mid \\mu_n, r) = \\frac{\\Gamma(y_n+r)}{\\Gamma(r) \\Gamma(y_n+1)} \\left(\\frac{r}{\\mu_n + r}\\right)^r \\left(1 - \\frac{r}{\\mu_n + r}\\right)^{y_n}\n",
    "\\end{align}\n",
    "The mean is given by $\\mathbb{E}[y_n] = \\mu_n$, and $r$ controls the dispersion. Here, we model the mean for document $n$ as\n",
    "\\begin{align}\n",
    "\\mu_n = \\frac{T_n}{1000} \\mu\n",
    "\\end{align}\n",
    "where $\\mu$ is the rate of usage of 'can' per 1000 words and $T_n$ is the number of words in document $n$ (i.e. the document length).\n",
    "\n",
    "For our model, we will use the following prior for the non-negative parameters,\n",
    "\\begin{align}\n",
    "\\log \\mu &\\sim \\mathcal{N}(0, 9) \\\\\n",
    "\\log r &\\sim \\mathcal{N}(0, 9)\n",
    "\\end{align}\n",
    "\n",
    "In a classic paper, Mosteller and Wallace (JASA, 1963) used likelihood ratios under negative binomial models with different mean rates for Alexander Hamilton and James Madison to infer the more likely author of disputed Federalist papers. Spoiler alert: while Hamilton wrote the majority of the papers, the 12 disputed papers appear to be Madison's! A key step in their analysis was estimating the NB parameters. While Mosteller and Wallace used a point estimate for each word and author, you'll do full posterior inference, focusing on Hamilton's use of the word _can_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/slinderman/stats271sp2021/main/assignments/hw3/federalist_can_hamilton.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('federalist_can_hamilton.csv')\n",
    "Ts = np.array(df['Total'])\n",
    "ys = np.array(df['N'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1  [math]: Show that the negative binomial can be expressed as the marginal distribution of a Poisson with gamma prior\n",
    "\n",
    "Similar to how the Student's t distribution is a marginal of an inverse chi-squared and a Gaussian, show that\n",
    "\n",
    "\\begin{align}\n",
    "\\text{NB}(y \\mid \\mu, r) = \\int \\text{Po}(y \\mid \\lambda) \\, \\text{Ga}(\\lambda \\mid \\alpha,\\beta) \\, \\mathrm{d}\\lambda\n",
    "\\end{align}\n",
    "\n",
    "Express the parameters of the negative binomial distribution as a function of the parameters of the gamma distribution. (Assume $\\beta$ is the rate parameter.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK9HU0boZxD0"
   },
   "source": [
    "## Problem 2: Implement the log joint probability of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gm2TLeKqDRdx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following MC implementation problems, sample in $\\log(\\mu), \\log(r)$ space. Initialize with $\\log(\\mu) = 0, \\log(r) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Implement Metropolis-Hastings\n",
    "\n",
    "Implement and run Metropolis-Hastings with a spherical Gaussian proposal. Try various proposal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-CvgtlRZ3dd"
   },
   "source": [
    "## Problem 4: Implement Hamiltonian Monte Carlo\n",
    "\n",
    "Implement the leapfrog step as a function, and run HMC. Try various step sizes and number of leapfrog steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Diagnostics\n",
    "\n",
    "For both algorithms, make trace plots of the parameters and plot histograms of posterior marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3bQq1rFGKSJ",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Effective Sample Size\n",
    "\n",
    "Calculate effective sample size for both chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OnB5kf-k7B0"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "\n",
    "**Formatting:** check that your code does not exceed 80 characters in line width. If you're working in Colab, you can set _Tools &rarr; Settings &rarr; Editor &rarr; Vertical ruler column_ to 80 to see when you've exceeded the limit. \n",
    "\n",
    "Download your notebook in .ipynb format and use the following commands to convert it to PDF:\n",
    "```\n",
    "jupyter nbconvert --to pdf hw3_yourname.ipynb\n",
    "```\n",
    "\n",
    "**Dependencies:**\n",
    "\n",
    "- `nbconvert`: If you're using Anaconda for package management, \n",
    "```\n",
    "conda install -c anaconda nbconvert\n",
    "```\n",
    "\n",
    "**Upload** your .ipynb and .pdf files to Gradescope. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of STATS271 HW1: Bayesian Linear Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
