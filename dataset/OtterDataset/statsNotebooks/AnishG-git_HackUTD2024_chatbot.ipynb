{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (1.54.4)\n",
      "Requirement already satisfied: pandas in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: fastapi in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (0.115.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from fastapi) (0.41.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\vs code\\hackutd2024\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai pandas fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "# Generate synthetic data for the API metrics table\n",
    "num_rows = 250\n",
    "\n",
    "# Possible values for endpoints and methods\n",
    "endpoints = [\n",
    "    \"/auth/login\", \"/auth/logout\", \"/auth/reset\", \n",
    "    \"/data/fetch\", \"/data/upload\", \"/data/sync\",\n",
    "    \"/user/profile\", \"/user/settings\", \"/user/preferences\",\n",
    "    \"/admin/dashboard\", \"/admin/users\", \"/admin/logs\",\n",
    "    \"/api/v1/products\", \"/api/v1/orders\", \"/api/v1/customers\",\n",
    "    \"/search/quick\", \"/search/advanced\",\n",
    "    \"/payments/process\", \"/payments/verify\"\n",
    "]\n",
    "\n",
    "methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"]\n",
    "\n",
    "# Response codes with weighted probabilities\n",
    "response_codes = {\n",
    "    200: 0.55,  # Success (55% probability)\n",
    "    201: 0.1,   # Created\n",
    "    400: 0.1,   # Bad Request\n",
    "    401: 0.05,  # Unauthorized\n",
    "    403: 0.05,  # Forbidden\n",
    "    404: 0.05,  # Not Found\n",
    "    500: 0.08,  # Internal Server Error\n",
    "    503: 0.02   # Service Unavailable\n",
    "}\n",
    "\n",
    "# Generate timestamps within the last 24 hours\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=1)\n",
    "\n",
    "# Function to generate latency with suspicious patterns\n",
    "def generate_latency():\n",
    "    pattern_type = random.random()\n",
    "    \n",
    "    if pattern_type < 0.7:  # 70% normal cases\n",
    "        return random.randint(50, 500)\n",
    "    elif pattern_type < 0.85:  # 15% slower cases\n",
    "        return random.randint(500, 2000)\n",
    "    elif pattern_type < 0.95:  # 10% very slow cases\n",
    "        return random.randint(2000, 10000)\n",
    "    else:  # 5% suspicious/timeout cases\n",
    "        return random.randint(10000, 60000)  # Up to 1 minute\n",
    "\n",
    "# Create the data\n",
    "data = {\n",
    "    \"RequestID\": [str(uuid.uuid4()) for _ in range(num_rows)],\n",
    "    \"Endpoint\": [random.choice(endpoints) for _ in range(num_rows)],\n",
    "    \"Method\": [random.choice(methods) for _ in range(num_rows)],\n",
    "    \"Latency_ms\": [generate_latency() for _ in range(num_rows)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generate timestamps and calculate request/response times with suspicious patterns\n",
    "timestamps = []\n",
    "time_requests = []\n",
    "time_responses = []\n",
    "\n",
    "for _ in range(num_rows):\n",
    "    # Generate base timestamp\n",
    "    timestamp = start_date + timedelta(\n",
    "        seconds=random.randint(0, int((end_date - start_date).total_seconds()))\n",
    "    )\n",
    "    \n",
    "    # Calculate request and response times based on latency\n",
    "    latency = df['Latency_ms'].iloc[_] / 1000  # Convert to seconds\n",
    "    \n",
    "    # Add some suspicious timing patterns\n",
    "    if random.random() < 0.1:  # 10% chance of suspicious timing\n",
    "        # Add extra delay between request and response\n",
    "        extra_delay = random.randint(30, 300)  # 30 seconds to 5 minutes\n",
    "        latency += extra_delay\n",
    "    \n",
    "    request_time = timestamp\n",
    "    response_time = request_time + timedelta(seconds=latency)\n",
    "    \n",
    "    timestamps.append(timestamp)\n",
    "    time_requests.append(request_time)\n",
    "    time_responses.append(response_time)\n",
    "\n",
    "df['Timestamp'] = timestamps\n",
    "df['time_request_to_Server'] = time_requests\n",
    "df['Time_response_from_server'] = time_responses\n",
    "\n",
    "# Add response codes with correlation to latency\n",
    "response_codes_list = []\n",
    "for latency in df['Latency_ms']:\n",
    "    if latency > 10000:  # Very slow responses\n",
    "        # Higher chance of error codes for very slow responses\n",
    "        codes = [500, 503, 504, 408]  # Including timeout status\n",
    "        response_codes_list.append(random.choice(codes))\n",
    "    elif latency > 2000:  # Slower responses\n",
    "        if random.random() < 0.3:  # 30% chance of error\n",
    "            response_codes_list.append(random.choice([500, 503, 408]))\n",
    "        else:\n",
    "            response_codes_list.append(200)\n",
    "    else:  # Normal responses\n",
    "        response_codes_list.append(\n",
    "            random.choices(\n",
    "                list(response_codes.keys()),\n",
    "                weights=list(response_codes.values()),\n",
    "                k=1\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "df['Response_Code'] = response_codes_list\n",
    "\n",
    "# Generate request counts with suspicious patterns\n",
    "df['Request_Count'] = [\n",
    "    int(random.choice([\n",
    "        random.randint(1, 50),       # Low traffic\n",
    "        random.randint(50, 200),     # Medium traffic\n",
    "        random.randint(200, 500),    # High traffic\n",
    "        random.randint(500, 1000),   # Traffic spike\n",
    "        random.randint(2000, 5000)   # Suspicious spike\n",
    "    ]) * random.uniform(0.9, 1.1))\n",
    "    for _ in range(num_rows)\n",
    "]\n",
    "\n",
    "# Sort by timestamp\n",
    "df = df.sort_values('Timestamp')\n",
    "\n",
    "# Reorder columns to match requested format\n",
    "df = df[[\n",
    "    'RequestID',\n",
    "    'Endpoint',\n",
    "    'Method',\n",
    "    'Latency_ms',\n",
    "    'time_request_to_Server',\n",
    "    'Time_response_from_server',\n",
    "    'Response_Code',\n",
    "    'Request_Count',\n",
    "    'Timestamp'\n",
    "]]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst few rows of the generated data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display rows with suspicious timing (latency > 10 seconds)\n",
    "suspicious_rows = df[df['Latency_ms'] > 10000]\n",
    "print(\"\\nSuspicious timing patterns (latency > 10 seconds):\")\n",
    "print(suspicious_rows)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "file_path = \"synthetic_api_data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"\\nData saved to {file_path}\")\n",
    "\n",
    "# Display some basic statistics\n",
    "print(\"\\nLatency statistics:\")\n",
    "print(df['Latency_ms'].describe())\n",
    "print(\"\\nResponse Code distribution:\")\n",
    "print(df['Response_Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Initialize SambaNova LLM client\n",
    "client = openai.OpenAI(api_key=\"9cc5e443-487d-450c-913d-2d027b5ea1eb\", base_url=\"https://api.sambanova.ai/v1\")\n",
    "\n",
    "# Load the synthetic data from the CSV file\n",
    "file_path = \"synthetic_api_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Limit analysis to the specified columns\n",
    "data = data[[\n",
    "    'RequestID',\n",
    "    'Endpoint',\n",
    "    'Method',\n",
    "    'Latency_ms',\n",
    "    'time_request_to_Server',\n",
    "    'Time_response_from_server',\n",
    "    'Response_Code',\n",
    "    'Request_Count',\n",
    "    'Timestamp'\n",
    "]]\n",
    "\n",
    "# Convert time_request_to_Server and Time_response_from_server to numeric values\n",
    "data['time_request_to_Server'] = pd.to_numeric(data['time_request_to_Server'], errors='coerce')\n",
    "data['Time_response_from_server'] = pd.to_numeric(data['Time_response_from_server'], errors='coerce')\n",
    "\n",
    "# Function to get insights from SambaNova LLM\n",
    "def get_llm_insights(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to summarize metrics for an endpoint\n",
    "def summarize_metrics(group):\n",
    "    avg_latency = group['Latency_ms'].mean()\n",
    "    avg_request_to_server = group['time_request_to_Server'].mean()\n",
    "    avg_response_from_server = group['Time_response_from_server'].mean()\n",
    "    total_requests = group['Request_Count'].sum()\n",
    "    response_code_counts = group['Response_Code'].value_counts().to_dict()\n",
    "    methods_used = group['Method'].value_counts().to_dict()\n",
    "    \n",
    "    # Summarize data\n",
    "    summary = (\n",
    "        f\"Average latency: {avg_latency:.2f} ms.\\n\"\n",
    "        f\"Average time request to server: {avg_request_to_server:.2f} ms.\\n\"\n",
    "        f\"Average time response from server: {avg_response_from_server:.2f} ms.\\n\"\n",
    "        f\"Total requests: {total_requests}.\\n\"\n",
    "        f\"Response codes: {response_code_counts}.\\n\"\n",
    "        f\"Methods used: {methods_used}.\"\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "# Generate insights for each endpoint\n",
    "insights = []\n",
    "\n",
    "for endpoint, group in data.groupby(\"Endpoint\"):\n",
    "    # Summarize the metrics\n",
    "    summary = summarize_metrics(group)\n",
    "    \n",
    "    # Use SambaNova LLM to generate analysis and suggestions\n",
    "    llm_response = get_llm_insights(\n",
    "        f\"You are an API analytics expert. Given the following metrics for the endpoint '{endpoint}', \"\n",
    "        f\"provide an analysis and actionable suggestions:\\n\\n{summary}\\n\\n\"\n",
    "        \"Respond in the format: 'Analysis: <analysis>. Suggestion: <suggestion>'.\"\n",
    "    )\n",
    "    \n",
    "    # Split the response into analysis and suggestion\n",
    "    analysis, suggestion = \"\", \"\"\n",
    "    if \"Analysis:\" in llm_response and \"Suggestion:\" in llm_response:\n",
    "        analysis = llm_response.split(\"Analysis:\")[1].split(\"Suggestion:\")[0].strip()\n",
    "        suggestion = llm_response.split(\"Suggestion:\")[1].strip()\n",
    "    else:\n",
    "        analysis = llm_response  # Fallback in case format is not strictly followed\n",
    "    \n",
    "    # Format the output\n",
    "    insights.append({\n",
    "        \"endpoint\": endpoint,\n",
    "        \"analysis\": analysis,\n",
    "        \"suggestion\": suggestion\n",
    "    })\n",
    "\n",
    "# Save insights to a JSON file\n",
    "insights_file = \"insights.json\"\n",
    "with open(insights_file, \"w\") as f:\n",
    "    json.dump(insights, f, indent=4)\n",
    "\n",
    "# Generate overall suggestion\n",
    "general_summary = \"\\n\".join(\n",
    "    [f\"Endpoint: {entry['endpoint']}\\nAnalysis: {entry['analysis']}\\nSuggestion: {entry['suggestion']}\\n\"\n",
    "     for entry in insights]\n",
    ")\n",
    "overall_prompt = (\n",
    "    f\"You are an API analytics expert. Based on the following insights for multiple endpoints, \"\n",
    "    f\"provide a single generalized suggestion:\\n\\n{general_summary}\\n\\n\"\n",
    "    \"Respond in the format: 'Overall Suggestion: <generalized suggestion>'.\"\n",
    ")\n",
    "\n",
    "overall_response = get_llm_insights(overall_prompt)\n",
    "\n",
    "# Extract the overall suggestion\n",
    "overall_suggestion = overall_response.split(\"Overall Suggestion:\")[1].strip() if \"Overall Suggestion:\" in overall_response else overall_response\n",
    "\n",
    "# Save overall suggestion to a JSON file\n",
    "overall_suggestions_file = \"overall_suggestions.json\"\n",
    "with open(overall_suggestions_file, \"w\") as f:\n",
    "    json.dump({\"overall_suggestion\": overall_suggestion}, f, indent=4)\n",
    "\n",
    "print(f\"Insights saved to {insights_file}\")\n",
    "print(f\"Overall suggestion saved to {overall_suggestions_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating initial business report...\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "You can now ask questions about the API metrics data in subsequent cells using:\n",
      "chatbot.answer_question('your question here')\n",
      "**Endpoint Analysis: /api/v1/orders**\n",
      "=====================================\n",
      "\n",
      "**Total Requests:** 75\n",
      "**Average Latency:** 2500.12ms\n",
      "**Success Rate:** 45.33%\n",
      "**Error Count:** 43\n",
      "\n",
      "**Key Observations:**\n",
      "\n",
      "* The /api/v1/orders endpoint received a total of 75 requests, which is a significant portion of the total requests (30%).\n",
      "* The average latency of 2500.12ms is higher than the overall average latency of 2280.23ms, indicating that this endpoint is taking longer to respond to requests.\n",
      "* The success rate of 45.33% is lower than the overall success rate of 51.60%, indicating that a higher percentage of requests to this endpoint are failing.\n",
      "* The error count of 43 is a significant portion of the total errors (35%).\n",
      "\n",
      "**Potential Causes:**\n",
      "\n",
      "* The high average latency may be due to slow database queries or inefficient code in the /api/v1/orders endpoint.\n",
      "* The low success rate may be due to errors in the endpoint or client-side issues.\n",
      "* The high error count may be due to a combination of factors, including errors in the endpoint, client-side issues, or security vulnerabilities.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. **Optimize Database Queries:** Review and optimize database queries in the /api/v1/orders endpoint to improve performance and reduce latency.\n",
      "2. **Review Endpoint Code:** Review the code in the /api/v1/orders endpoint to identify and fix errors or inefficiencies.\n",
      "3. **Implement Error Handling:** Implement robust error handling mechanisms in the /api/v1/orders endpoint to handle errors and exceptions.\n",
      "4. **Monitor Endpoint Performance:** Continuously monitor the performance of the /api/v1/orders endpoint and make adjustments as needed to ensure optimal performance.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare the API metrics data\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\"data.csv\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_business_report(df):\n",
    "    \"\"\"Create a prompt for generating a business report from the data\"\"\"\n",
    "    # Calculate key metrics\n",
    "    metrics = {\n",
    "        \"total_requests\": len(df),\n",
    "        \"avg_latency\": df['Latency_ms'].mean(),\n",
    "        \"success_rate\": (df['Response_Code'].astype(str).str.startswith('2').mean() * 100),\n",
    "        \"total_errors\": len(df[df['Response_Code'].astype(str).str.startswith(('4', '5'))]),\n",
    "        \"suspicious_latency\": len(df[df['Latency_ms'] > 10000]),\n",
    "        \"busiest_endpoint\": df['Endpoint'].mode().iloc[0],\n",
    "        \"date_range\": f\"{df['Timestamp'].min()} to {df['Timestamp'].max()}\"\n",
    "    }\n",
    "    \n",
    "    report_prompt = f\"\"\"\n",
    "    Based on the following API metrics data, generate a greatly comprehensive business report with actionable insights:\n",
    "    Make sure it propely formatted in Markdown.\n",
    "\n",
    "    Time Period: {metrics['date_range']}\n",
    "    Total Requests: {metrics['total_requests']}\n",
    "    Average Latency: {metrics['avg_latency']:.2f}ms\n",
    "    Success Rate: {metrics['success_rate']:.2f}%\n",
    "    Total Errors: {metrics['total_errors']}\n",
    "    Suspicious Response Times: {metrics['suspicious_latency']}\n",
    "    Most Accessed Endpoint: {metrics['busiest_endpoint']}\n",
    "    \n",
    "    Firstly, list the top 10 endpoints that need attention\n",
    "\n",
    "    Please analyze this data and provide:\n",
    "    1. Executive Summary\n",
    "    2. Performance Analysis\n",
    "    3. Security Concerns\n",
    "    4. Recommendations\n",
    "    Make sure to highlight any suspicious patterns or potential issues.\n",
    "    \"\"\"\n",
    "    return report_prompt\n",
    "\n",
    "def process_question(df, question):\n",
    "    \"\"\"Prepare context for answering specific questions about the data\"\"\"\n",
    "    context = f\"\"\"Based on the API metrics data, answer the following question: {question}\n",
    "\n",
    "    Relevant metrics:\n",
    "    - Total requests: {len(df)}\n",
    "    - Average latency: {df['Latency_ms'].mean():.2f}ms\n",
    "    - Success rate: {(df['Response_Code'].astype(str).str.startswith('2').mean() * 100):.2f}%\n",
    "    - Error count: {len(df[df['Response_Code'].astype(str).str.startswith(('4', '5'))])}\n",
    "    - Endpoints: {', '.join(df['Endpoint'].unique())}\n",
    "    \"\"\"\n",
    "    return context\n",
    "\n",
    "class APIMetricsChat:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = api_key\n",
    "        self.df = load_data()\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def generate_initial_report(self):\n",
    "        if self.df is None:\n",
    "            return \"Error: Could not load data\"\n",
    "\n",
    "        report_prompt = generate_business_report(self.df)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model='Meta-Llama-3.1-8B-Instruct',\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a business analyst specializing in API metrics analysis.\"},\n",
    "                    {\"role\": \"user\", \"content\": report_prompt}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                top_p=0.1\n",
    "            )\n",
    "            report = response.choices[0].message.content\n",
    "            self.conversation_history.append({\"role\": \"system\", \"content\": report})\n",
    "            return report\n",
    "        except Exception as e:\n",
    "            return f\"Error generating report: {e}\"\n",
    "\n",
    "    def answer_question(self, question):\n",
    "        if self.df is None:\n",
    "            return \"Error: Could not load data\"\n",
    "\n",
    "        context = process_question(self.df, question)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a business analyst specializing in API metrics analysis.\"},\n",
    "            *self.conversation_history,\n",
    "            {\"role\": \"user\", \"content\": context}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model='Meta-Llama-3.1-8B-Instruct',\n",
    "                messages=messages,\n",
    "                temperature=0.1,\n",
    "                top_p=0.1\n",
    "            )\n",
    "            answer = response.choices[0].message.content\n",
    "            self.conversation_history.append({\"role\": \"user\", \"content\": question})\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            return f\"Error answering question: {e}\"\n",
    "\n",
    "# Initialize the chatbot (run this in a cell)\n",
    "api_key = openai.OpenAI(api_key=\"9cc5e443-487d-450c-913d-2d027b5ea1eb\", base_url=\"https://api.sambanova.ai/v1\")\n",
    "chatbot = APIMetricsChat(api_key)\n",
    "\n",
    "# Generate initial report (run this in a cell)\n",
    "print(\"Generating initial business report...\\n\")\n",
    "report = chatbot.generate_initial_report()\n",
    "\n",
    "# Save the report to a markdown file\n",
    "# with open(\"business_report.md\", \"w\") as file:\n",
    "#     file.write(report)\n",
    "# print(report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"You can now ask questions about the API metrics data in subsequent cells using:\")\n",
    "print(\"chatbot.answer_question('your question here')\")\n",
    "\n",
    "# Ask specific questions (run this in a cell)\n",
    "app = FastAPI()\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(question: Question):\n",
    "    if chatbot.df is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Error: Could not load data\")\n",
    "    \n",
    "    answer = chatbot.answer_question(question.question)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# Example usage:\n",
    "# Run the FastAPI app with: uvicorn filename:app --reload\n",
    "# Then you can ask questions by sending a POST request to /ask with a JSON body: {\"question\": \"your question here\"}\n",
    "\n",
    "# Ask a question\n",
    "question = input(\"You: \")\n",
    "answer = chatbot.answer_question(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
