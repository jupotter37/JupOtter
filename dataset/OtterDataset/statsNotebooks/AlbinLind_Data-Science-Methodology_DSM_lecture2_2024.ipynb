{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Science Methods - week 2\n",
    "\n",
    "\n",
    "\n",
    "*Authors: Cees Diks and Bram Wouters, Faculty Economics and Business, University of Amsterdam (UvA)* \n",
    "\n",
    "*Copyright (C): UvA (2024)* \n",
    "\n",
    "*Credits: some of the examples and formulations are taken from Hastie, Tisbhirani and Friedman (2009)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics week 2:\n",
    "\n",
    "* Model evaluation + selection \n",
    "* Bias-variance trade-off\n",
    "* Cross-validation\n",
    "* Dimension reduction / PCA\n",
    "* Nearest neighbor regression\n",
    "* Kernel regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- Hastie 7.1, Reader 1.1 -->\n",
    "#  Model evaluation\n",
    "\n",
    "Choice between various models (or machine learning methods)\n",
    "\n",
    "<!-- Can be a single model specification or learning method, but parameters need to be set (e.g. number of lags in an AR  model for time series) -->\n",
    "\n",
    "When measuring model performance, we want to use a measure not for how good the model fits the data at hand, but how well the model generalises, i.e. of how well will it perform on new, unseen, data. \n",
    "\n",
    "Broadly speaking, there are two potential objectives for model evaluation.\n",
    "\n",
    "* *Model selection*: comparing the performance of different models (often with varying complexity), in order to identify the best one.\n",
    "* *Model assessment*: having chosen a final model, estimating how well it performs on new data.\n",
    "\n",
    "In practical situations, either one of the objectives or both can play a role.<!-- In the latter case, one usually first performs a model selection and subsequently assesses the performance of the selected model. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression model\n",
    "<!-- Also applies to classification, not considered explicitly today -->\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "Y =  f(X,\\beta) + \\varepsilon,  \n",
    "$$\n",
    "\n",
    "where $X = (X_1, \\ldots, X_p)^T$ is an input vector (taking values in the ***feature space***), $Y$ a single response variable, and $\\varepsilon \\thicksim  \\mathcal{N} (0, \\sigma_{\\varepsilon}^2)$ is Gaussian noise with unknown variance.\n",
    "\n",
    "### Linear regression model\n",
    "\n",
    "The well-known model is obtained for the specification\n",
    "$$ \n",
    "  f(X,\\beta) = \\sum_{m=0}^p \\beta_m X_m\n",
    "$$\n",
    "where $X_0$ is taken to be constant, usually $X_0 = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Extending the feature space\n",
    "\n",
    "Next consider the regression model $Y =  f(X,\\beta) + \\varepsilon$, with $f$ specified as\n",
    "\n",
    "$$\n",
    "\\quad f(X,\\beta) = \\sum_{m=0}^M \\beta_m h_m(X), \n",
    "$$\n",
    "\n",
    "where $h_m (x)$ are functions from $\\mathbb{R}^p$ to $\\mathbb{R}$, $m=1, \\ldots, M$, called ***basis functions*** (we usually set $h_0(X)=1$).\n",
    "\n",
    "In machine learning one often refers to $h_m(X)$ (rather than the elements of $X$) as ***features***. \n",
    "\n",
    "It is more accurate to refer to any of the $h_m(X)$ that are not identical to an element of $X$ as ***derived features***.\n",
    "\n",
    "Note that the model still is a linear regression model if one or more of the basis functions are nonlinear, since the target variable is a linear function of the features.\n",
    "\n",
    "<!-- (The elements $\\beta_m$ of $\\beta$ appear linearly on the r.h.s., we merely allow for nonlinear functions of $X$ in addition to the elements of $X$.) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood function\n",
    "\n",
    "$$\n",
    "L(\\beta, \\sigma_{\\varepsilon}) = P(\\mathbf{y} | \\mathbf{X} , \\beta, \\sigma_\\varepsilon  ) = \\prod_{n=1}^N \\mathcal{N} (y_n | f(x_n, \\beta), \\sigma_\\varepsilon^2) ,\n",
    "$$\n",
    "where $\\mathbf{X}$ is the $N \\times (M+1)$ matrix whose elements are given by $\\mathbf{X}_{nm} = h_m(x_n)$. \n",
    "\n",
    "$\\mathbf{y}$ consists of all values of the target variable in the training set, $\\mathbf{y} = \\left\\{ y_1, y_2, \\ldots, y_N\\right\\}$, and likewise, $\\mathbf{X}$ contains the training set values of the input variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log-likelihood function\n",
    "\n",
    "$$\n",
    "\\ln L(\\beta, \\sigma_{\\varepsilon}) = - N \\ln \\sigma_\\varepsilon - \\frac{N}{2} \\ln(2\\pi) - \\frac{E_D (\\beta)}{\\sigma^2_\\varepsilon},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "E_D (\\beta) = \\frac{1}{2} \\sum_{n=1}^N \\left( y_n - f(x_n, \\beta) \\right)^2 = \\frac{1}{2} \\sum_{n=1}^N \\left( y_n - \\beta^T h(x_n) \\right)^2\n",
    "$$\n",
    "\n",
    "is the sum-of-squared-errors. \n",
    "\n",
    "Note: maximizing the likelihood with respect to the $\\beta_m$ is equivalent to minimizing the sum-of-squared-errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the log-likelihood function\n",
    "\n",
    "Differentiating the log likelihood function with respect to $\\beta_m$ gives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_m} \\ln P(\\mathbf{y} | \\mathbf{X} , \\beta, \\sigma_\\varepsilon  ) = \\frac{1}{\\sigma_\\varepsilon^2} \\sum_{n=1}^N \\left( y_n - \\beta^T h(x_n) \\right) h_m(x_n) \\, .\n",
    "$$\n",
    "\n",
    "Setting this to zero and solving for the $\\beta_m$ leads to the so-called normal equations for the least squares problem,\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T \\mathbf{y}  = \\mathbf{X}^+ \\mathbf{y} \\, , \\label{ML_beta}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y} = (y_1, y_2, \\ldots, y_N)^T$. The quantity $\\mathbf{X}^+ = \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T$ is the Moore-Penrose pseudo inverse and can be seen as a generalization of the inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MLE for $\\sigma^2_{\\varepsilon}$ found by maximizing the log likelihood function also w.r.t. the noise variance:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2_\\varepsilon = \\frac{1}{N} \\sum_{n=1}^N \\left( y_n - \\hat \\beta^T \\mathbf{X}_n\\right)^2 \\, . \\label{ML_sigma}\n",
    "$$\n",
    "\n",
    "The quantities $\\hat{\\beta}_j$ and $\\hat{\\sigma}^2_\\varepsilon$ are the maximum likelihood solution for the linear model, given a training set $\\mathcal{T}.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- Reader 1.1 -->\n",
    "# Example model (regression model, polynomial fit of order $M$)\n",
    "\n",
    "Single continuous input variable $X$ and a single continuous target variable $Y$.\n",
    "\n",
    "Training dataset $\\mathcal{T} = \\left\\{ (x_1, y_1), (x_2, y_2), \\ldots , (x_N, y_N) \\right\\}$. \n",
    " \n",
    "For simplicity we assume that the input variables are fixed on a lattice, $x_i = \\tfrac{i-1}{N-1},$ and the target variables are given by $y_i = \\sin (2\\pi x_i) + \\tilde{\\varepsilon}_i.$ \n",
    "\n",
    "The $\\tilde{\\varepsilon}_i$ are a sample realization of the Gaussian noise random variable $\\tilde{\\varepsilon} \\thicksim \\mathcal{N} (0, \\sigma_{\\tilde{\\varepsilon}}^2 = 0.04).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures_week2/lin_mod_training_set.png\" alt=\"Example training set + underlying regression fn\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>\n",
    "\n",
    "\n",
    "<h5>\n",
    "    <center>\n",
    "        Example training set + underlying true regression function \n",
    "    </center>\n",
    "</h5>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h5><center>\n",
    "Solutions for $M =2, 5$ and $15$ given the training data.\n",
    "</center></h5>\n",
    "\n",
    "<img src=\"figures_week2/lin_mod_solutions.png\" alt=\"Fits for M = 3, 6, 16\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization error\n",
    "\n",
    "<!--Assessing the generalization performance of a model is a crucial aspect of data science.--> \n",
    "Given a model trained on a data set $\\mathcal{T},$ the generalization performance of the model tells us how well it predicts responses for new data independently drawn from the same population distribution as $\\mathcal{T}.$  \n",
    "\n",
    "### Loss function\n",
    "\n",
    "To quantify this predictive ability, one first defines a ***loss function*** that measures the size of the prediction error for a trained regression model $\\hat{f}(X)$. \n",
    "\n",
    "Typical loss functions are\n",
    "\n",
    "$$\n",
    "L(Y, \\hat{f}(X)) =\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "(Y - \\hat{f}(X))^2 & \\qquad \\text{(squared error)}  \\\\\n",
    "& \\\\\n",
    "|Y - \\hat{f}(X)| & \\qquad \\text{(absolute error)} \n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "<!-- Note that the linear model example above is of this form, with $\\hat{f}(X) = f(X,\\hat{\\beta})$. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization error (cntd)\n",
    "\n",
    "Given a loss function, the ***generalization error*** (also called ***test error***) is defined as\n",
    "\n",
    "$$\n",
    "\\text{Err}_\\mathcal{T} = E[L(Y, \\hat{f}(X)) | \\mathcal{T}] \\, , \\label{eq:generalization_error}\n",
    "$$\n",
    "\n",
    "where the expectation is taken with respect to the joint distribution of $(X, Y)$. \n",
    "\n",
    "\n",
    "### Note: \n",
    "\n",
    "Generalization error is defined with respect to a data set $\\mathcal{T}$. \n",
    "\n",
    "For the specific model that is trained based on $\\mathcal{T},$ it measures the average prediction error on unseen data that was not used to train the model. \n",
    "\n",
    "In many practical situations, the generalization error is the quantity of interest. However, it cannot always be estimated accurately. Often if you try to estimate it you get something that is actually closer to $\\text{Err}$ (see next slide).\n",
    "\n",
    "<!--\n",
    "It is informative about the prediction capability of the model trained on $\\mathcal{T}$, when it is applied to independent data drawn randomly from the population.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expected test error \n",
    "A related quantity, called the expected prediction (or test) error,\n",
    "is given by\n",
    "\n",
    "$$\n",
    "\\text{Err} = E[L(Y, \\hat{f}(X))],\n",
    "$$\n",
    "\n",
    "where the expectation is taken with respect to $(X, Y)$, as well as the set $\\mathcal{T}$ of fixed size $N$ to train the model on. \n",
    "\n",
    "The observations in $\\mathcal{T}$ are drawn independently from the same joint distribution as $(X, Y)$. \n",
    "\n",
    "This means that the prediction error is the expectation of the generalization error when averaged over all possible sets of observations $\\mathcal{T}$: \n",
    "\n",
    "$$\\text{Err} = E_\\mathcal{T}[\\text{Err}_\\mathcal{T}].$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both error measures and their relation are visualized for the polynomial model. We generated 100 i.i.d. data sets of size $N=20$ and trained polynomial models of degrees ranging from $0$ until $15$ on each of them. For each of the resulting models we calculated the generalization error by simulating another $1000$ data points (thin red lines). The average generalization error, or average test error, is indicated by red dots. \n",
    "\n",
    "<img src=\"figures_week2/lin_mod_errors.png\" alt=\"Linear model errors\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error\n",
    "\n",
    "A third related quantity is the training error, which is the average loss on the set $\\mathcal{T}$ the model was trained on,\n",
    "\n",
    "$$\n",
    "\\overline{\\text{err}} = \\frac{1}{N} \\sum_{n=1}^N  L(y_n, \\hat{f}(x_n)) \\, .\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training errors for the different simulated training samples (blue lines). \n",
    "\n",
    "<img src=\"figures_week2/lin_mod_errors.png\" alt=\"Linear model errors\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>\n",
    "\n",
    "When the model complexity (i.e. \n",
    "the degree $M$ of the polynomial) is increasing, the (average) training error is decreasing to zero. \n",
    "\n",
    "<!-- Intuitively this makes sense, as increasing complexity => more parameters to fit the model to the training data => *overfitting* -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary regarding performance measures\n",
    "* In most practical applications, one desires a model with a high generalization performance. \n",
    "\n",
    "* $\\Rightarrow$ generalization error should be small. \n",
    "\n",
    "* But directly estimating the generalization error usually is not possible. \n",
    "\n",
    "* In practice we often work with an estimate of the expected prediction error. \n",
    "<!-- This will be discussed further below. -->\n",
    "\n",
    "* A high generalization performance can be associated with a low prediction error. \n",
    "\n",
    "Note: The training error can never be an indicator of the generalization performance, as we can make the training error arbitrarily small without (as we will see) improving generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- Hastie 7.3 -->\n",
    "# Bias-variance decomposition\n",
    "Using the squared-error loss function, the expected prediction error at a specific point $X = x_0$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Err}(x_0) \n",
    "& = E[ ( Y - \\hat{f}(X) )^2 | X = x_0] \\\\\n",
    "& = [E \\hat{f}(x_0) - f(x_0)]^2 + E[\\hat{f}(x_0) - E \\hat{f}(x_0)]^2 + \\sigma_\\varepsilon^2 \\\\\n",
    "& = \\text{Bias}^2 (\\hat{f}(x_0)) + \\text{Var}(\\hat{f}(x_0)) + \\sigma_\\varepsilon^2 \\\\\n",
    "& = \\text{Bias}^2 + \\text{Variance} + \\sigma_\\varepsilon^2 \\, ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where in the second line we have used that $Y = f(X) + \\varepsilon$.\n",
    "\n",
    "The prediction error can be decomposed into three terms: \n",
    "* the bias (squared) of the estimated model, \n",
    "* the variance of the estimated model, \n",
    "* the variance of the Gaussian noise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- 7.2 (Reader 1.3) --> \n",
    "# Bias-variance trade-off\n",
    "\n",
    "Model complexity\n",
    "\n",
    "We want a model with high generalization performance (low expected prediction error). \n",
    "\n",
    "Since we cannot influence the variance of the innovations, we wish to find a model for which the squared bias plus the variance is at its minimum. \n",
    "\n",
    "In doing so, one has to make a trade-off between bias and variance: decreasing the bias squared (often) means increasing the variance, and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To understand this, realize that as a function of the model complexity the bias squared is a decreasing function. \n",
    "\n",
    "Simple models often have a large bias, as it lacks enough degrees of freedom (parameters) to approximate the true underlying model well. \n",
    "\n",
    "Think of the polynomial model with $M=1.$ In that case we are trying to model the sinusoid of the underlying datagenerating process with a straight line, leading to a poor approximation and hence a large bias. \n",
    "\n",
    "A larger $M$ (larger model complexity) gives us more degrees of freedom and would enable us to approximate the sinusoid with a higher accuracy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, an increasing model complexity will also lead to a higher variance. \n",
    "\n",
    "In the polynomial model with $M=1$ all possible data sets $\\mathcal{T}$ would lead to a straight line, while for larger $M$ the model estimate $\\hat{f}(X)$ will fluctuate more wildly, depending on the details of the data. \n",
    "\n",
    "This leads to a larger average deviation of $\\hat{f}(X)$ from its mean and hence a larger variance. \n",
    "\n",
    "Somewhere between a low and high model complexity, there is an optimum where the squared bias plus the variance is minimal. \n",
    "\n",
    "The optimal trade-off between bias and variance will lead to the minimum expected prediction error and a maximum generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To illustrate this trade-off, let's look at the linear model for regression introduced in Section \\ref{sect:lin_mod}. Using that the model estimate is given by \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(X, \\beta) =\\ \\sum_{m=0}^{M} \\hat{\\beta}_m h_m(X) = \\sum_{m=0}^{M} \\sum_{n=1}^{N} \\mathbf{X}^+_{mn} \\, y_n \\, h_m(X) \\, ,\n",
    "\\end{equation}\n",
    "\n",
    "one can show that the prediction error at position $x_0$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Err}(x_0) = [E \\hat{f}(x_0) - f(x_0)]^2 + || \\mathbf{\\chi}(x_0)||^2 \\sigma_\\epsilon^2 + \\sigma_\\epsilon^2 \\, , \\label{eq:pred_error_lin_mod_x0}\n",
    "\\end{equation}\n",
    "\n",
    "where the $N$-dimensional vector $\\mathbf{\\chi}$ has components $\\mathbf{\\chi}_n (x_0) = \\sum_{m=0}^{M} h_m (x_0) \\, \\mathbf{X}^+_{mn}.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The variance term depends on the position $x_0.$ To get the actual prediction error, one needs to average over the distribution of the independent variable $X.$ Assuming the fixed and uniformly distributed $x_n$ of the polynomial model, one finds\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{n=1}^N \\text{Err}(x_n) = \\frac{1}{N} \\sum_{n=1}^N   [E \\hat{f}(x_n) - f(x_n)]^2 + \\frac{M}{N} \\sigma_\\epsilon^2 + \\sigma_\\epsilon^2 \\, . \\label{eq:pred_error_lin_mod}\n",
    "\\end{equation}\n",
    "\n",
    "This leads to the conclusion that for the polynomial model the variance term in the prediction error grows linearly with the degree of the polynomial. As expected, increasing model complexity means increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For real-world data, it is impossible to simulate additional training sets and/or data points to compute the generalization errors and expected prediction error.\n",
    "\n",
    "$\\Rightarrow$ We need alternative procedures to estimate the generalization and prediction errors. \n",
    "\n",
    "<!-- We next describe several of those procedures, with a special focus on $K$-fold cross-validation. -->\n",
    "\n",
    "(Training errors can be calculated directly from the training set and will therefore not be considered here.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, one can distinguish two situations: data-rich situations and cases of insufficient data. \n",
    "\n",
    "In data-rich situations, <!-- the best approach is to --> randomly split the full data set in three parts: a training set, a validation set and a test set:\n",
    "\n",
    "\n",
    "<img src=\"figures_week2/data_split.png\" alt=\"Typical data split\" style=\"display:block; margin-left: auto; margin-right: auto; width: 50%;\"/>\n",
    "\n",
    "The training set is used to estimate models (typically of varying complexity). \n",
    "\n",
    "<!-- Usually, one fits different models that vary in complexity.  -->\n",
    "\n",
    "Select the best model by comparing the models' forecast errors across the data points of the validation set. \n",
    "\n",
    "Having selected the best model, one can assess its performance by calculating the generalization error using the test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: the generalization error calculated on the validation set cannot be used as a proper assessment of the generalization performance of the selected model. \n",
    "\n",
    "The particular model might have been selected because it performs well on the particular validation set. \n",
    "\n",
    "$\\Rightarrow$ error calculated on the validation set expected to underestimate the actual generalization performance.\n",
    "\n",
    "Also note: no general rule for the preferred (relative) size of the three sets, but\n",
    "* the training set needs to be big enough to produce a reliable model estimate, \n",
    "* the other two sets must be big enough to produce a reliable estimate of the generalization error. \n",
    "\n",
    "<!-- Below we sketch a typical split: $50\\%$ for training, $25\\%$ for validation and $25\\%$ for testing. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unfortunately, most practical situations are not data rich. \n",
    "\n",
    "There is (again) no general criterion for when there is sufficient data available to split it into three sets. \n",
    "\n",
    "This can only be answered case-by-case, taking the following (non-exhaustive) list of factors into account: \n",
    "* performance requirements of the model, \n",
    "* signal-to-noise ratio of the data, \n",
    "* dimensionality of the input space, \n",
    "* complexity of the model, \n",
    "* required precision of performance estimates. \n",
    "\n",
    "We next discuss alternative procedures for data-sparse situations. \n",
    "\n",
    "<!-- Note that we will only be concerned with the prediction error, as estimations of the generalization error are generally unfeasible in non-data-rich situations. We will distinguish two types of procedures: calculating errors from data on which the model was trained (in-sample errors) versus calculating errors from data that was excluded from the training set (extra-sample errors). -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 7.4 Optimism of the Training Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 7.12 Conditional versus expected test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 7.5 Estimates of in-sample prediction error \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- 7.5 and 7.7 -->\n",
    "# Information theoretical measures (AIC/BIC)\n",
    "\n",
    "Idea: for each model, the maximum log-likelihood $\\hat L$ (maximized across unknown model parameters) is a measure for how well the model can describe the data at hand (the in-sample model fitness). \n",
    "\n",
    "However, without penalizing for increasing model complexity, this is not a good measure for the generalization error, since the log-likelihood typically is an increasing function of model complexity. \n",
    "\n",
    "To construct useful measures for generalization error based on the log-likelihood, additional penalties are required, taking into account model complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Akaike's information criterion (AIC) and Bayes' information criterion (BIC)\n",
    "\n",
    "Defined by taking minus twice the maximized log-likelihood \n",
    "<!-- (a measure for model unfittness, so it is smaller for better fits), -->\n",
    "with a penalty for model complexity \n",
    "\n",
    "$$\n",
    "\\mathrm {AIC} = -2\\ln(\\hat{L}) + 2 (M+1).\n",
    "$$\n",
    " \n",
    "$$\n",
    " \\mathrm {BIC} = -2\\ln(\\hat {L}) + \\ln(N) (M+1). \n",
    "$$\n",
    "\n",
    "* In practice, model selection across a set of models is performed by choosing the model with the smallest AIC/BIC\n",
    "* These information-theoretical measures are very well suited for sparse data situations, since all the observations can be included in the model estimation step. \n",
    "* The validation is done implicitly, by adding penalty terms, the analytic form of which is motivated using information-theoretical statistical insights.\n",
    "* No observations need to be held back for validation (although one may still hold out a test sample to assess the performance of the selected model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing AIC and BIC\n",
    "\n",
    "Note: the only difference between these two measures is the factor in front of the number of features $M$.\n",
    "\n",
    "for $\\ln(n) > 2$ ($n \\geq 8$ observations) BIC penalizes the addition of an extra parameter more than AIC. \n",
    "\n",
    "Therefore, BIC favors smaller models than AIC.\n",
    "\n",
    "AIC is not consistent, in that it selects complex models too frequently\n",
    "\n",
    "BIC is consistent, but often selects models with small complexity in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "Extra-sample (out-of-sample) errors are calculated on data that was not used to train the model (i.e. excluded from the training sample). \n",
    "\n",
    "$K$-fold cross validation (CV) is an example of an extra-sample error, as is the hold-out method. <!-- and nowadays probably the most widely used method for model evaluation in data science. --> \n",
    "\n",
    "<!-- Before discussing $K$-fold CV, let us mention another extra-sample evaluation method: the holdout method. -->\n",
    "\n",
    "In the holdout procedure, the full data set is randomly split in two subsets. \n",
    "* The first is used to train the model, \n",
    "* the second data set is used to compute the performance of the trained model (defined by the choice of loss function $L$). \n",
    "\n",
    "Because the split is random, the training and test set can be considered independent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The holdout method only provides a reasonable estimate of the expected prediction error, not of the generalization error. \n",
    "<!-- This is mainly because the random choice of the finite-size test set causes a large variance in the calculated error. -->\n",
    "\n",
    "There is no general rule for the relative sizes of the two subsets.\n",
    "* A large training set has the advantage of a small bias with respect to the prediction error.\n",
    "* This goes at the cost of a small test set and hence a large variance of the calculated error. \n",
    "\n",
    "As we will see, $K$-fold CV offers a remedy for the large variance of the calculated holdout error. \n",
    "\n",
    "Some advantages of the holdout method over CV: \n",
    "* Computationally less expensive.\n",
    "* Technically easier to implement.\n",
    "\n",
    "$\\Rightarrow$ holdout method is usually only used for very large data sets or when time (both programmer's time and computation time) is a critical factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The $K$-fold cross-validation estimate\n",
    "We will explain the procedure in the context of the simple linear model and therefore assume a set \n",
    "\n",
    "$$\\left\\{ (x_1, y_1), (x_2, y_2), \\ldots , (x_N, y_N) \\right\\}$$ \n",
    "\n",
    "of $N$ available data points. \n",
    "\n",
    "<!-- Any generalization to other data types and other models is straightforward. -->\n",
    "\n",
    "First step: randomly distribute the available data across $K$ (roughly) equal-sized subsets. \n",
    "\n",
    "<!-- Important that this splitting is performed randomly. -->\n",
    "\n",
    "If the subsets are indexed by $k,$ with $k=1,\\ldots,K,$ then we can define functions $g_k: \\left\\{1,\\ldots,N_k\\right\\} \\to \\left\\{1,\\ldots,N\\right\\}$ that select the $N_k$ datapoints of the $k$-th subset out of the full size-$N$ data set. \n",
    "\n",
    "The subset sizes $N_k$ are taken to be roughly equal. If $K$ is a divisor of $N,$ they can be made exactly equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Singling out a specific subset $k,$ the idea of CV is to train the model on the other $K-1$ subsets (we denote the resulting model by $\\hat{f}^{-k}(X)$) \n",
    "and subsequently test its performance on the subset $k$ (using loss function $L$). \n",
    "\n",
    "This is repeated $K$ times, where each subset acts once as the left-out test set. \n",
    "\n",
    "Taking the average of the $K$ errors, we arrive at the CV estimate of the expected prediction error:\n",
    "\\begin{equation} \\label{eq:CV}\n",
    "\\text{CV} =\\ \\frac{1}{K} \\sum_{k=1}^K \\text{CV}_k \\, , \n",
    "\\end{equation}\n",
    "where the error on test set $k$ is given by\n",
    "\\begin{equation}\n",
    "\\text{CV}_k =\\ \\frac{1}{N_k} \\sum_{i=1}^{N_k} L \\left( y_{g_k (i)}, \\hat{f}^{-k} ( x_{g_k (i)} ) \\right) \\, . \\label{eq:CVk}\n",
    "\\end{equation}\n",
    "<!-- In the next Figure, the computation of the CV estimate is depicted schematically. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The cross-validation estimate depicted schematically\n",
    "\n",
    "<img src=\"figures_week2/CV_data_split.png\" alt=\"Typical data split\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As indicated before, the CV error is only a good estimate of the expected prediction error and not of the generalization error. \n",
    "\n",
    "Intuitively this is not immediately clear, but there is ample empirical evidence that this is the case. See for example Section 7.12 of Hastie *et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimal choice of K\n",
    "\n",
    "<!-- The best choice for the number of subsets $K$ depends on the situation. -->\n",
    "\n",
    "In principle, one can compute a CV estimate for any value of $K$ ranging from $K=2$ to $K=N$. \n",
    "\n",
    "Latter is a special case<!--, in the sense that each left-out test set consists of a single data point -->: leave-one-out cross-validation (LOOCV).\n",
    "\n",
    "Large $K$ advantage: CV estimate relatively unbiased for expected prediction error. \n",
    "\n",
    "(Because the training sets are large, creating model estimates that perform relatively similar to model estimates on the full data set.)\n",
    "\n",
    "Comes at the cost of a larger variance of the CV estimate, because the different training sets are relatively similar for large $K.$ \n",
    "\n",
    "Potential disadvantage of large $K$: computational cost of applying the training method $K$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For small values of $K,$ bias can become a serious problem. \n",
    "\n",
    "To understand this, look at the hypothetical error as a function of the size of the training set shown in the figure below. \n",
    "\n",
    "The expected error decreases with increasing size of the training set, eventually approaching some irreducible error due to Gaussian noise in the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider $5$-fold CV. For $N=200$, training set contains $160$ data points. The expected errors for $160$ and $200$ data points are close (small CV estimate bias). For $N=50$, training sets have size $40,$ and the difference in expected errors is much larger. For the smaller training set, the bias of a $5$-fold CV is larger and to decrease the bias one would need to increase $K.$ \n",
    "\n",
    "<img src=\"figures_week2/err_vs_size.png\" alt=\"Typical data split\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To summarize, the ideal value of $K$ depends on the situation. \n",
    "\n",
    "Some factors to take into consideration are the maximum acceptable bias and variance of the estimate and the computational time it would take to perform the CV procedure. \n",
    "\n",
    "Typical values that are being used in practical applications are $K=5$ or $K=10.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection with cross-validation\n",
    "\n",
    "In the next Figure an example is given of the use of CV for model selection. \n",
    "\n",
    "Using the linear model with $N=50,$ we have plotted the expected prediction error, the average training error and the $K$-fold CV estimates for $K=5$ and $K=25$ as a function of $M.$ \n",
    "\n",
    "We have included error bars for the CV estimates, which represent the standard deviation of the errors of the different folds (i.e. the standard deviation of $\\text{CV}_k$). \n",
    "\n",
    "We have used $N=50$ instead of the usual $N=20,$ because for very small data sets the patterns in CV estimates are less clear due to high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expected prediction error (red), expected training error (blue) and $K$-fold CV estimates for $K=5$ (green) and $K=25$ (purple) as a function of $M,$ for the linear model with $N=50.$ The error bars on the CV estimates represent the standard deviation of $\\text{CV}_k$.\n",
    "\n",
    "<img src=\"figures_week2/lin_mod_CV_errors.png\" alt=\"Typical data split\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, note that the CV estimate for large $K$ is less biased with respect to the prediction error. \n",
    "\n",
    "This makes sense, because the size of the training sets for $K=25$ is 48, whereas for $K=5$ is it 40. \n",
    "\n",
    "Since the prediction error assumes a size of the training set of $50,$ this is closer to the training set size of $25$-fold CV and hence the bias is smaller. \n",
    "\n",
    "<!-- \n",
    "According to Hastie *et al.* the RMSE estimate should be expected to have larger variance for the $25$-fold than for $5$-fold CV, because of the smaller size of the test sets for $25$-fold CV. However, we don't observe this typical behavior in this example, at least as judged by the standard errors.\n",
    "-->\n",
    "Finally, we see that (most of the time) both cross-validations overestimate the prediction error. \n",
    "\n",
    "This is due to the smaller training sets, leading on average to model estimates that deviate more from the underlying data-generating process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Coming back to the issue of model selection with cross-validation, often a ''one-standard-error'' rule is used. \n",
    "\n",
    "This rule dictates that the least complex model whose CV error is no more than one standard deviation above the smallest error is selected. \n",
    "\n",
    "We see that both cross-validations give a minimum RMSE for $M=9$. \n",
    "\n",
    "In both cases the least complex model whose error is not more than one standard deviation above the minimum RMSE, is the model with $M=3.$ \n",
    "\n",
    "So using this rule, both $5$-fold and $25$-fold CV would select the model with $M=3$ as the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!-- Hastie 3.4 ->\n",
    "# Shrinkage methods\n",
    "\n",
    "The idea, dating back to Gauss, behind least-squares estimation of a regression function is that by minimizing (w.r.t. an unknown model parameter $\\beta$) the mean squared error\n",
    "$$\n",
    "\\sum_{n=1}^N \\left( y_n - f(x_n, \\beta) \\right)^2,\n",
    "%L(Y, \\hat{f}(X)) = (Y - \\hat{f}(X))^2,\n",
    "$$\n",
    "we obtain the best 'fit' to the data in the mean-squared-error sense. \n",
    "\n",
    "<!-- \n",
    "Although this may indeed provide the model that best fits\n",
    "the training data set, in practice there are \n",
    "--> \n",
    "\n",
    "But: model selection considerations to be taken into account when aiming for a model that<!--, besides fitness to the estimation sample,--> also has the capability to perform well on new, unseen, data (i.e. a model that has small generalization error). \n",
    "\n",
    "<!-- In choosing the model complexity, the bias/variance trade-off plays an important role. We have seen that models of too high complexity have a tendency to over-fit the estimation data leading to forecasts with high variability across estimation data sets, while models of too low complexity can be seriously biased even for large training sample sizes. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#   Curse of dimensionality 1\n",
    "\n",
    "More features not always better \n",
    "* model complexity can explode with number of features\n",
    "* more features $\\Rightarrow$ more degrees of freedom in model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curse of dimensionality 2\n",
    "\n",
    "Example: polynomial of degree $5$ in $1$-d and $2$-d.\n",
    "\n",
    "In $1$-d, a total of $D+1 = 6$ parameters.\n",
    "\n",
    "In $2$-d, interaction terms enter: $X_1^\\ell X_2 ^m, \\quad$ $\\ell, m \\in 0, \\ldots, D$ \n",
    "\n",
    "Already gives $(D+1)^2 = 36$ coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "## Curse of dimensionality 3\n",
    "\n",
    "\n",
    "Example: Nearest neighbor methods (see below) suffer in high dimensions (no close neighbours)\n",
    "\n",
    "\\# neighbours within distance $r$ from any given feature vector $\\propto \\left(r/r_{max}\\right)^M$\n",
    "\n",
    "$\\Rightarrow$ probability of finding a neighbour within a given distance $r$ rapidly vanishes with $M$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curse of dimensionality 3\n",
    "\n",
    "Take-home message curse of dimensionality:\n",
    "\n",
    "The more flexible your model, the more it will suffer from the curse of dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 1: motivation\n",
    "\n",
    "First method to reduce dimensionality:\n",
    "\n",
    "feature selection = variable subset selection = subset selection\n",
    "\n",
    "discard features that are not informative, or that are redundant\n",
    "\n",
    "Potential advantages:\n",
    "\n",
    "* model becomes simpler\n",
    "* therefore simpler (and faster) to estimate\n",
    "* and easier to interpret,\n",
    "* and generalization error is reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 2: feature selection vs. feature extraction\n",
    "\n",
    "Note: feature selection differs from feature extraction, which is another often-used method.\n",
    "\n",
    "\n",
    "* In feature extraction one creates new (derived) features as functions of the original features,\n",
    "* while in feature selection the aim is to select a subset of the features by removing uninformative and irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 3: when?\n",
    "\n",
    "* feature selection is clearly needed in applications where there are many features and relatively few observations,\n",
    "\n",
    "* such as in DNA micro-array data, where there are many thousands of features, and of the order of 20 to 100 observations,\n",
    "\n",
    "* it may still be useful in cases where there are fewer features and relatively many observations (as in financial applications)\n",
    "\n",
    "* as long as the loss of information by discarding the uninformative/irrelevant features is counterbalanced by an accompanying reduction in generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 4: exhaustive approach (for small $M$)\n",
    "\n",
    "* If $M$ is small (say up to $20$) this can still be done exhaustively\n",
    "\n",
    "* Provides us with a measure of the performance of the best model with $k$ features,\n",
    "*  This is plotted as a function of $k$ (a measure of model complexity) to aid the selection of the optimal model complexity $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 5: forward- and backward stepwise regression (for large $M$)\n",
    "\n",
    "* Iteratively adds the best feature (or deletes the worst feature, respectively) in each round.\n",
    "* The forward approach increases $k$ at each step starting from $k=0$ (no features) and adding the best feature in each round\n",
    "* The backward approach starts at the full set of features, dropping the worst in each round.\n",
    "* The main choice determining the outcome of these algorithms is the criterion that is used for deciding when to stop adding or deleting features.\n",
    "* This is typically done by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection 6: notes\n",
    "\n",
    "  Note 1: stepwise regression approaches are greedy\n",
    "\n",
    " \n",
    "  \n",
    "  They might miss the optimum subset for some intermediate values of $k$ (for $k=0$ and $k=M$ there is only one subset to consider).\n",
    "\n",
    "  \n",
    "\n",
    "  Note 2: Lasso regression can automatically force coefficients to be zero\n",
    "\n",
    " \n",
    "  \n",
    " $\\Rightarrow$ It can be considered as a technique that performs feature selection at estimation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro Principal component analysis 1\n",
    "\n",
    "Alternative dimension reduction method\n",
    "\n",
    "Abbreviation: PCA. Dates back to Pearson, 1901\n",
    "\n",
    "$M$ features $\\mathbf{X}_i = (X_{1,i}, \\ldots, X_{M,i})^T$, $i = 1, \\ldots, N$.\n",
    "\n",
    "$\\mathbf{X}_i$ $\\sim$ some distribution in the $M$-dimensional feature space\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro Principal component analysis 2: motivation\n",
    "\n",
    "Idea:  variation of the features across the\n",
    "feature space might be described well by the variation of the features in a small number of dominant directions.\n",
    "\n",
    "\n",
    "As a result, the variation of the data might take place predominantly in a subspace of dimension $L < M$.\n",
    "\n",
    "\n",
    "\n",
    "$\\Rightarrow$ extract a small number of $L < M$ linear combinations $Z_{\\ell}$, $\\ell=1,\\ldots, L$ of features (or directions) that explain most of the variation in $\\mathbf{X}_i$\n",
    "\n",
    "\n",
    "then continue the data analysis using the $L$ variables $\\mathbf{Z}_i = (Z_{1,i}, \\ldots, Z_{L,i})^T$, $i = 1, \\ldots, N$ rather than the $M$ original variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro Principal component analysis 3: noise reduction\n",
    "\n",
    "\n",
    "Equivalent to projecting the explanatory variables on a lower-dimensional linear sub-space;\n",
    "\n",
    "\n",
    "thus discarding/ignoring the variation of the features in the other (perpendicular) directions.\n",
    "\n",
    "\n",
    "This may be considered as a disadvantage (we are throwing away information)\n",
    "\n",
    "but if the variation in the directions that we ignore happens to be largely due to noise, the projection step may actually help to get rid of this noise,\n",
    "\n",
    "in that sense this may `clean' the input variables (features) to some extent (not completely, since there may still be noise present in the projected variables $\\mathbf{X}'_i$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro Principal component analysis 4: invariance under rescaling\n",
    "\n",
    "\n",
    "A potential disadvantage of PCA:\n",
    "\n",
    "\n",
    "* based on the variance-covariance matrix, rather than the correlation matrix.\n",
    "\n",
    "* $\\Rightarrow$ not invariant under changing units of measurement (i.e. rescaling)\n",
    "\n",
    "* Rescaling will lead to different principal components.\n",
    "\n",
    "* In particular, if one feature has a variance that is much larger than that of the others, it will be nearly identified with the first principal component\n",
    "\n",
    "* This problem can be avoided by standardising each variable to unit sample variance (in addition to zero sample mean) prior to applying PCA.\n",
    "\n",
    "* Using this standardisation is common practice among practitioners now. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 1\n",
    "\n",
    "Consider a bivariate normal distribution centered around (for simplicity) the origin.\n",
    "\n",
    "\n",
    "First consider the case where the covariance between $X_1$ and $X_2$ is zero.\n",
    "\n",
    "\n",
    "If $X_1$ and $X_2$ have identical variance, the distribution is spherically symmetric around the origin (as also showed by contour lines of the density, which are circles centered around the origin)\n",
    "\n",
    "\n",
    "\n",
    "we cannot identify a direction in the plane along which the variation is largest. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contour plot 1\n",
    "\n",
    "<img src=\"figures_week2/contour_1.png\" alt=\"contour 1\" style=\"display:block; margin-left: auto; margin-right: auto; width: 50%;\"/>\n",
    "\n",
    "<!--\n",
    "<h5>\n",
    "    <center>\n",
    "        Example training set + underlying true regression function \n",
    "    </center>\n",
    "</h5>\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 2\n",
    "\n",
    " If, on the other hand, $X_1$ has larger variance than $X_2$ (still for zero covariance), it becomes clear that by ignoring the variation in the $X_2$-direction (by retaining only $X_1$, i.e. by projecting $(X_1,X_2)$ on $X_2$ only) we lose less information (about the fluctuations of $(X_1,X_2)$ in the plane) than when we ignore $X_1$ by projecting $(X_1,X_2)$ onto $X_2$.\n",
    "\n",
    " \n",
    "  \n",
    "Note: the contours of equal density are now ellipses with the main axis oriented along the $X_1$-axis.\n",
    "  \n",
    "  \n",
    "Since it is associated with the principal axis, $X_1$ is called the principal component of the bivariate distribution in this case.\n",
    "\n",
    "  \n",
    "  \n",
    "The short axis of the equal density contours is orthogonal to the principal axis, and hence directed along the vertical axis, associated with variation in $X_2$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contour plot 2\n",
    "\n",
    "<img src=\"figures_week2/contour_2.png\" alt=\"contour 2\" style=\"display:block; margin-left: auto; margin-right: auto; width: 50%;\"/>\n",
    "\n",
    "<!--\n",
    "<h5>\n",
    "    <center>\n",
    "        Example training set + underlying true regression function \n",
    "    </center>\n",
    "</h5>\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 3\n",
    "\n",
    "\n",
    "So far, we have ignored the possibility that the covariance between $X_1$ and $X_2$ is nonzero.\n",
    "\n",
    "Nonzero covariance causes the main axes of the density contours (which are still ellipses) not to be aligned along the $x_1$ or $x_2$ axis\n",
    "\n",
    "They are rotated in the plane (the principal and secondary axes remain orthogonal relative to each other, as the axes of an ellipse are always orthogonal).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contour plot 3\n",
    "\n",
    "\n",
    "<img src=\"figures_week2/contour_3.png\" align=\"left\" style=\" margin-left: auto; margin-right: auto; width: 50%;\"/><img src=\"figures_week2/contour_4.png\" align=\"left\" style=\" margin-left: auto; margin-right: auto; width: 50%;\"/>\n",
    "\n",
    "<!--\n",
    "<img src=\"figures_week4/contour_3.png\" alt=\"contour 2\" style=\"display:block; margin-left: auto; margin-right: auto; width: 30%;\"/> <img src=\"figures_week4/contour_4.png\" alt=\"contour 2\" style=\"display:block; margin-left: auto; margin-right: auto; width: 30%;\"/>\n",
    "-->\n",
    "<!--\n",
    "<h5>\n",
    "    <center>\n",
    "        Example training set + underlying true regression function \n",
    "    </center>\n",
    "</h5>\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 4\n",
    "\n",
    "\n",
    "More formally, the joint pdf of $X_1$ and $X_2$ is\n",
    "$$\n",
    "  f(\\mathbf{x}) = \\frac{1}{2 \\pi |\\Sigma|^{\\frac{1}{2}}} {\\rm e}^{- \\frac{1}{2} \\mathbf{x}^T \\Sigma^{-1} \\mathbf{x}},\n",
    "  $$\n",
    "  \n",
    "\n",
    "  The equal density contours satisfy  $\\mathbf{x}^T \\Sigma^{-1} \\mathbf{x} = {\\rm cnst.}$,\n",
    " \n",
    "  This defines an ellipse with major axes along the eigenvectors of $\\Sigma$.\n",
    " \n",
    "  These eigenvectors are always orthogonal, as they are eigenvectors of a real symmetric (variance-covariance) matrix.\n",
    "  \n",
    "  The variances in the directions of the eigenvectors correspond to the variances of the principal components.\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 5: dimension reduction\n",
    "\n",
    "\n",
    "  \n",
    "  In an $M$-dimensional setting, there are $M$ eigenvalues of the variance-covariance matrix\n",
    "\n",
    "  Assuming they are all different, these define $M$ orthogonal directions that we associate with the principal components.\n",
    "\n",
    "  We rank the principal components descending, with the idea that the variation in our data (features) might be reasonably represented by the variation in the subspace spanned by the first $L$ eigenvectors of the variance-covariance matrix.\n",
    "\n",
    "  After deciding on $L$ (to be discussed later) the features can be mapped to the lower-dimensional space by orthogonal projection onto the principal components.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition PCA 6\n",
    "\n",
    "\n",
    "Note:  the results regarding the decomposition of the variation of the data hold generally for multivariate distributions, not just for multivariate normal distributions\n",
    "\n",
    "\n",
    "    \n",
    "Reason: PCA defines a transformation based on the sample covariance matrix this will also work if the data are not distributed normally.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 1\n",
    "\n",
    "To facilitate linking PCA to the variance-covariance matrix of $\\mathbf{X}$, we assume that $\\mathbf{X}$ has column-wise zero empirical mean (the sample mean of each column has been shifted to zero)\n",
    "\n",
    "Mathematically, the transformation is defined by a set of $M$-dimensional vectors of weights or coefficients $\\mathbf {v} _{k}=(v_{1,k}, \\dots, v_{M,k})^T$ that map each feature vector $ \\mathbf{x}_{i}$ to a new vector of principal component scores ${\\displaystyle \\mathbf {z} _{i}=(z_{1,i},\\dots ,z_{L,i})^T}$, given by\n",
    "\n",
    "$$\n",
    "{{z}_{k,i}}=\\mathbf {x} _{i}\\cdot \\mathbf {v} _{k} \\qquad \\mbox{for} \\qquad i=1, \\dots, N, \\quad k=1,\\dots, L.\n",
    "$$\n",
    "\n",
    "in such a way that the individual variables ${\\displaystyle z_{1}, \\dots, z_{L}}$ of $z$ considered over the data set successively inherit the maximum possible variance from $\\mathbf{x}$, with each coefficient vector $\\mathbf{v}_{k}$ constrained to be a unit vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 2 (First component 1)\n",
    "\n",
    "In order to maximize variance, the first weight vector $\\mathbf{v}_{1}$ thus has to satisfy\n",
    "\n",
    "$$\n",
    "     {\\mathbf {v}}_{{1}}={\\underset {\\Vert {\\mathbf {v}}\\Vert =1}{\\operatorname {\\arg \\,max}}}\\,\\left\\{\\sum _{i=1}^N\\left({z_{1,i}}\\right)^{2}\\right\\}={\\underset {\\Vert {\\mathbf {v}}\\Vert =1}{\\operatorname {\\arg \\,max}}}\\,\\left\\{\\sum _{i=1}^N\\left({\\mathbf {x}}_{{i}}\\cdot {\\mathbf {v}}\\right)^{2}\\right\\}\n",
    "$$\n",
    "\n",
    "Equivalently, writing this in matrix form gives\n",
    "\n",
    "$$\n",
    "     \\mathbf{v} _{1}={\\underset {\\Vert \\mathbf {v} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\{\\Vert \\mathbf {Xv} \\Vert ^{2}\\}={\\underset {\\Vert \\mathbf {v} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\mathbf {v} ^{T}\\mathbf {X^{T}} \\mathbf {Xv} \\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 3 (First component 2)\n",
    "\n",
    "Since $\\mathbf{v}_{1}$ has been defined to be a unit vector, it equivalently also satisfies\n",
    "\n",
    "$$\n",
    "   \\mathbf {v} _{1}={\\underset {\\Vert \\mathbf {v} \\Vert =1}{{\\operatorname {\\arg \\,max} }}}\\,\\left\\{{\\frac {\\mathbf {v} ^{T}\\mathbf {X}^{T} \\mathbf {Xv} }{\\mathbf {v} ^{T}\\mathbf {v} }}\\right\\}\n",
    "$$\n",
    "\n",
    "The quantity to be maximised can be recognised as a ***Rayleigh quotient***.\n",
    "\n",
    "A standard result for a positive semi-definite matrix such as $\\mathbf{X}^T \\mathbf{X}$ is that the quotient's maximum possible value is the ***largest eigenvalue of the matrix***, which occurs when $\\mathbf{v}$\n",
    "is the corresponding eigenvector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 4 (First component 3)\n",
    "\n",
    "With $\\mathbf{v}_{1}$ found, the first principal component of a data vector $\\mathbf{x}_{i}$ can then be given as a score $z_{1,i} = \\mathbf{x}_{i} \\cdot \\mathbf{v}_{1}$ in the transformed coordinates, or as the corresponding vector in the original variables, $\\{ \\mathbf{x}_{i} \\cdot \\mathbf{v}_{1} \\} \\mathbf{v}_{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 5 (Further components 1)\n",
    "\n",
    "The $k$th component can be found after subtracting the first $k-1$ principal components from \n",
    "$ \\mathbf{X}$:\n",
    "$$\n",
    "   {\\mathbf {{\\hat {X}}}}_{{k}}={\\mathbf {X}}-\\sum _{{s=1}}^{{k-1}}{\\mathbf {X}}{\\mathbf {v}}_{s}{\\mathbf {v}}_{s}^{\\rm {T}}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 6 (Further components 2)\n",
    "\n",
    "and then finding the weight vector which extracts the maximum variance from this new data matrix\n",
    "$$\n",
    "    {\\mathbf{v}}_{k}={\\underset {\\Vert {\\mathbf {v}}\\Vert =1}{\\operatorname {arg\\,max}}}\\left\\{\\Vert {{\\hat{\\mathbf {X}}}}_{{k}}{\\mathbf {v}}\\Vert ^{2}\\right\\}={\\underset {\\Vert {\\mathbf {v}}\\Vert =1}{\\operatorname {\\arg \\,max}}}\\,\\left\\{{\\tfrac {{\\mathbf {v}}^{T}{{\\hat{\\mathbf {X}}}}_{{k}}^{T}{{\\hat{\\mathbf {X}}}}_{{k}}{\\mathbf {v}}}{{\\mathbf {v}}^{T}{\\mathbf {v}}}}\\right\\}.\n",
    "$$\n",
    "   It turns out that this gives the remaining eigenvectors of $\\mathbf{X}^T\\mathbf{X}$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues.\n",
    "\n",
    "\n",
    "   Thus the weight vectors are eigenvectors of $\\mathbf{X}^T\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 7 (Further components 3)\n",
    "\n",
    "The $k$th principal component of a data vector $\\mathbf{x}_{i}$ can therefore be given as a score ${z_{k,i}} = \\mathbf{x}_{i} \\cdot \\mathbf{v}_{k}$ in the transformed coordinates, \n",
    "\n",
    "or as the corresponding vector in the space of the original variables, $\\{\\mathbf{x}_{i} \\cdot \\mathbf{v}_{k}\\} \\mathbf{v}_{k}$, where $\\mathbf{v}_{k}$ is the $k$th eigenvector of $\\mathbf{X}^T\\mathbf{X}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 8 (Further components 4)\n",
    "\n",
    "The full principal components decomposition of $X$ can therefore be given as\n",
    "$$\n",
    "     \\mathbf{Z} = \\mathbf{X} \\mathbf{V}\n",
    "$$\n",
    "     where $\\mathbf{V}$ is an $M \\times M$ matrix of weights whose columns are the eigenvectors of $\\mathbf{X}^T \\mathbf{X}$. The transpose of $\\mathbf{V}$ is sometimes called the ***whitening*** or ***sphering transformation***.\n",
    "\n",
    "     \n",
    "Columns of $\\mathbf{V}$ multiplied by the square root of corresponding eigenvalues, i.e. eigenvectors scaled up by the variances, are called ***loadings*** in PCA (this terminology derives from Factor analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 9 (Covariances 1)\n",
    "\n",
    "\n",
    "$\\mathbf{X}^T\\mathbf{X}$ itself can be recognised as proportional to the empirical sample covariance matrix of the dataset $\\mathbf{X}$.\n",
    "\n",
    "The sample covariance $Q$ between two of the different principal components over the dataset is given by:\n",
    "$$\n",
    "   %{\\displaystyle {\\begin{aligned}Q(\\mathrm {PC} _{(j)},\\mathrm {PC} _{(k)})&\\propto (\\mathbf {X} \\mathbf {w} _{(j)})^{T}(\\mathbf {X} \\mathbf {w} _{(k)})\\\\&=\\mathbf {w} _{(j)}^{T}\\mathbf {X} ^{T}\\mathbf {X} \\mathbf {w} _{(k)}\\\\&=\\mathbf {w} _{(j)}^{T}\\lambda _{(k)}\\mathbf {w} _{(k)}\\\\&=\\lambda _{(k)}\\mathbf {w} _{(j)}^{T}\\mathbf {w} _{(k)}\\end{aligned}}} \n",
    "   {\\displaystyle\n",
    "   {\\begin{aligned}\n",
    "   Q(\\mathrm {PC} _{j},\\mathrm {PC} _{k})&\\propto (\\mathbf {X} \\mathbf {v} _{j})^{T}(\\mathbf {X} \\mathbf {v} _{k})\\\\&=\\mathbf {v} _{j}^{T}\\mathbf {X} ^{T}\\mathbf {X} \\mathbf {v} _{k}\\\\&=\\mathbf {v} _{j}^{T}\\lambda _{k}\\mathbf {v} _{k}\\\\&=\\lambda _{k}\\mathbf {v} _{j}^{T}\\mathbf {v} _{k}\\\\\n",
    "   & = \\lambda_k \\delta_{jk}\\end{aligned}}}\n",
    "$$\n",
    "   where the eigenvalue property of $\\mathbf{v}_{k}$ has been used to move from line 2 to line 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 10 (Covariances 2)\n",
    "\n",
    " Another way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.\n",
    "\n",
    "In matrix form, the empirical covariance matrix for the original variables can be written\n",
    "$$\n",
    "    %Q â X T X = V Î V T {\\displaystyle \\mathbf {Q} \\propto \\mathbf {X} ^{T}\\mathbf {X} =\\mathbf {V} \\mathbf {\\Lambda } \\mathbf {V} ^{T}} \n",
    "    \\mathbf{Q} \\propto \\mathbf{X}^T \\mathbf{X} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 11 (Covariances 3)\n",
    "\n",
    "  The empirical covariance matrix between the principal components becomes\n",
    "  \n",
    "$$\n",
    "    \\mathbf{V}^T \\mathbf{Q} \\mathbf{V} \\propto \\mathbf{V}^T \\mathbf{V} \\, \\boldsymbol{\\Lambda} \\, \\mathbf{V}^T \\mathbf{V} = \\boldsymbol{\\Lambda} \n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues $\\lambda_{k}$ of $\\mathbf{X}^T \\mathbf{X}$ being equal to the sum of the squares over the dataset associated with each component $k$:\n",
    "$$\\lambda_{k} = \\sum_{i=1}^N {z_{k,i}}^2 = \\sum_{i=1}^N (\\mathbf{x}_{i} \\cdot  \\mathbf{v}_{k})^2.$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Link with singular value decomposition\n",
    "\n",
    "PCA is closely related to singular value decomposition (SVD), where $\\mathbf{X}$ is decomposed as\n",
    "$$\n",
    " \\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T,\n",
    "$$\n",
    "where the $M \\times M$ matrix $V$ is as above for PCA, \n",
    "\n",
    "$D$ is the $N \\times M$ diagonal matrix with elements $D_{k,k} = \\sqrt{\\lambda_{k}}$, $k = 1, \\ldots, M$, satisfying $\\mathbf{D}^T \\mathbf{D} = \\Lambda$, \n",
    "\n",
    "and $\\mathbf{U}$ is an $N \\times N$ orthogonal matrix satisfying $\\mathbf{Z} = \\mathbf{U} \\mathbf{D}$ with $\\mathbf{Z}$ as for PCA above. \n",
    "\n",
    "Indeed, one finds\n",
    "$$\n",
    "  \\mathbf{X}^T \\mathbf{X} \\propto \\mathbf{V} \\mathbf{D}^T \\mathbf{U}^T \\mathbf{U} \\mathbf{D} \\mathbf{V}^T = \\mathbf{V} \\mathbf{D}^T \\mathbf{D} \\mathbf{V}^T = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^T,\n",
    "$$\n",
    "which is consistent with what was found for PCA above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 12: Dimension reduction 1\n",
    "\n",
    " The transformation $\\mathbf{Z} = \\mathbf{X} \\mathbf{V}$ maps a data vector $\\mathbf{x}_{i}$ from the original space of $M$ variables to a new space of $M$ variables which are uncorrelated over the dataset.\n",
    "\n",
    "\n",
    "However, not all the principal components need to be kept.\n",
    "\n",
    "\n",
    "\n",
    "Keeping only the first $L$ principal components, produced by using only the first $L$ eigenvectors, gives the truncated transformation\n",
    "$$\n",
    " \\mathbf{Z}_L = \\mathbf{X} \\mathbf{V}_L\n",
    "$$\n",
    " where the matrix $\\mathbf{Z}_L$ now has $N$ rows but only $L$ columns ($L < M$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details 13: Dimension reduction 2\n",
    "\n",
    " \n",
    "In other words, PCA learns a linear transformation $z = \\mathbf{V}^T x$ , $x \\in \\mathbb{R}^M$, $z \\in \\mathbb{R}^L$, where the columns of $M \\times L$ matrix $\\mathbf{V}$ form an orthogonal basis for the $L$ features (the components of representation $z$) that are uncorrelated.\n",
    "\n",
    "\n",
    "\n",
    "By construction, of all the transformed data matrices with only $L$ columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error $  \\|\\mathbf {T} \\mathbf {V} ^{T}-\\mathbf {T} _{L}\\mathbf {V} _{L}^{T}\\|_{2}^{2}$ or $ \\|\\mathbf {X} -\\mathbf {X} _{L}\\|_{2}^{2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the number of components 1\n",
    "\n",
    " \n",
    "There are various methods one can use to decide on the number of factors to take into account.\n",
    "\n",
    "\n",
    "\n",
    "The simplest and oldest approach is based on visual expection of the so-called ***scree plot***, which is a plot of the variance of each of the components (ranked from large to small variance) as a function of $L$, usually relative to the the total variance\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^M (X_{ij} - \\bar X_j)^2 =\n",
    "\\sum_{j=1}^M \\lambda_j = {\\rm trace} \\boldsymbol{\\Lambda}.\n",
    "$$\n",
    "<!--\n",
    "% TODO: add scree plot example\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scree plot\n",
    "\n",
    "<img src=\"figures_week2/scree_plot.png\" alt=\"contour 1\" style=\"display:block; margin-left: auto; margin-right: auto; width: 70%;\"/>\n",
    "\n",
    "<!--\n",
    "<h5>\n",
    "    <center>\n",
    "        Example training set + underlying true regression function \n",
    "    </center>\n",
    "</h5>\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the number of components 2\n",
    "\n",
    " \n",
    " The idea is that if this plot shows a few large variances followed by a horizontal plateau beyond some value of $L$, then it is likely that the first (large) components really are meaningful while the remaining ones just describe noise on the data.\n",
    "\n",
    " \n",
    "\n",
    "  Then $L$ would be chosen so that the large variance components are included while the remaining components are discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting the number of components 3\n",
    "\n",
    " \n",
    " This can be a rather subjective way to select the number of principal components.\n",
    "\n",
    "\n",
    "\n",
    "  $\\Rightarrow$ methods to perform the selection in a more objective way.\n",
    "\n",
    "\n",
    "\n",
    "  A relatively recent approach that is suitable for this purpose, consists of using cross-validation, which was discussed last week.\n",
    "\n",
    "\n",
    "  \n",
    "  We propose to use this, together with the one standard error rule, that is: plot the CV estimate of the test error as a function of $L$, together with standard errors, using, say, $10$-fold cross validation.\n",
    "\n",
    "\n",
    "  \n",
    " Subsequently, select the least complex model (smallest $L$) that is within one standard error from the minimum value observed in the scree plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA versus factor analysis (FA) 1\n",
    "\n",
    " \n",
    " A related dimension reduction technique that is often used in statistics and econometrics is factor analysis (FA).\n",
    "\n",
    "  \n",
    "The outcomes of FA and PCA are often very similar in practice and surprisingly there are many practitioners who cannot really tell the difference between the two.\n",
    "\n",
    " \n",
    "Here we briefly explain the differences and similarities between PCA and FA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA versus factor analysis (FA) 2\n",
    "\n",
    " \n",
    " In contrast to PCA, FA assumes an underlying latent factor model. The aim of factor analysis is to identify how many underlying factors there are, and to estimate the directions in which the factors manifest their variation in $\\mathbf{X}$-space (i.e. how the factors relate to the observations $\\mathbf{X}$).\n",
    "\n",
    "    \n",
    "\n",
    "In PCA no assumptions of an underlying model are made. Instead, in PCA we just use the variance-covariance structure of the data to find the directions of largest variation, without identifying each of these directions necessarily with a factor.\n",
    "\n",
    "  \n",
    "In this setting, that is, if the data were indeed generated by an underlying factor model, the application of PCA relative to FA ignores the fact that the observed variables are noisy versions of linear combinations of factors. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA versus factor analysis (FA) 3: note on data standardisation\n",
    "\n",
    " \n",
    "   Recall that in the context of PCA we discussed standardising the data to zero mean and unit variances) prior to analysis.\n",
    "\n",
    "  \n",
    "\n",
    "  Such a standardisation was seen to strongly affect the outcome of PCA (which is why we decided to standardise the data in the first place).\n",
    "\n",
    "  \n",
    "\n",
    "  For FA, standardisation, or in fact any other invertible linear transformation of $\\mathbf{X}$, has no consequences for the factors identified, in that the directions associated with the factors are simply transformed along with the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA versus factor analysis (FA) 4\n",
    "\n",
    " \n",
    " Which method should one use in practice, PCA or FA? This depends on the aim of the exercise.\n",
    "\n",
    "  \n",
    "\n",
    "  If the goal is merely to achieve data reduction, PCA suffices.\n",
    "\n",
    " \n",
    "  \n",
    "  However, interpretation of the factors can be problematic, as the principal components may differ substantially from any true unknown underlying factors.\n",
    "\n",
    " \n",
    "  \n",
    "Therefore, when the interpretation of the factors is important, FA is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Prediction by analogy\n",
    "\n",
    "Nearest-neighbor prediction algorithms are motivated by the notion of `prediction by analogy'. \n",
    "\n",
    "The idea is that if two feature vectors are close, the response variables can be expected to be similar as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# $k$-Nearest neighbors method\n",
    "\n",
    "input variables $X$ \n",
    "\n",
    "response variable $Y$\n",
    "\n",
    "we want to estimate a regression function $f(x) = E(Y|x)$. \n",
    "\n",
    "Earlier we discussed this problem in a parametric (linear regression) context\n",
    "\n",
    "here we want to go one step further and make minimal model assumptions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimal model assumptions\n",
    "\n",
    "Assume that $f(x)$ is continuous $X$-almost everywhere. \n",
    "\n",
    "In addition we assume additive noise:\n",
    "$$\n",
    " Y = f(X) + \\varepsilon,\n",
    "$$\n",
    "where $\\varepsilon$ is a zero mean random noise term. \n",
    "\n",
    "For simplicity also assume that the variance of $\\varepsilon$ doesn't depend on $X$ (not crucial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbor (NN) prediction\n",
    "\n",
    "Simplest implementation of prediction by analogy \n",
    "\n",
    "If you want to predict the response variable $Y$ at $X=x$\n",
    "\n",
    "* look for the nearest neighbor of $x$, $X_{(1)}$, say, among the input vectors in the training set, \n",
    "\n",
    "* use the response $Y_{(1)}$ of that training vector as the prediction of $f(x)$. \n",
    "\n",
    "The estimate becomes\n",
    "$$\n",
    " \\hat f(x) = Y_{(1).}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which distance measure to use?\n",
    "\n",
    "A popular choice is the familiar **Euclidean norm** \n",
    "\n",
    "$$d(x,X_n) = \\sqrt{\\sum_{m=1}^M (x_{m}-X_{n,m})^2}.$$\n",
    "\n",
    "Here \n",
    "\n",
    "* $x_{m}$ denotes the  $m^{\\rm th}$ component of $x$, and \n",
    "\n",
    "* $X_{n,m}$ that of $X_n$. \n",
    "\n",
    "Computationally faster alternative: **supremum norm**\n",
    "\n",
    "$$d_{\\sup}(x, X_n) = \\sup_{m \\in \\{1, \\ldots, M\\} } |x_{m} - X_{n,m}|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NN forecasts can be noisy\n",
    "\n",
    "Nearest-neighbor approach requires nearest neighbours to be sufficiently close\n",
    "\n",
    "$\\Rightarrow$ requires enough observations\n",
    "\n",
    "However, even with enough data, the response of the nearest neighbour is still a noisy prediction, \n",
    "\n",
    "This is because of the noise term $\\varepsilon_{(1)}$ in the observed response value \n",
    "\n",
    "$$Y_{(1)} = f(X_{(1)}) + \\varepsilon_{(1)},$$ which we use as a predictor of $f(x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-nearest neighbors method\n",
    "\n",
    "Noise can be averaged out by using more neighbors\n",
    "\n",
    "$\\Rightarrow$ Take average response over $k$ nearest neighbours of $x$ \n",
    "\n",
    "Forecast variance due to noise decreases by a factor $1/k$\n",
    "\n",
    "The estimator becomes\n",
    "\n",
    "$$\n",
    "  \\hat f(x) =  \\frac{1}{k}\\sum_{i=1}^k Y_{(i)},\n",
    "$$\n",
    "\n",
    "where $Y_{(i)}$ is the response of the observed $i^{\\rm th}$-nearest neighbor of $x$\n",
    "\n",
    "\n",
    "Known as the **$k$-nearest neighbors** ($k$-NN) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-variance trade-off\n",
    "\n",
    "But increasing $k$ also means increasing the distances between $x$ and the additional neighbors included.\n",
    "\n",
    "$\\Rightarrow$ we are averaging the observed responses over regions that are less representative for the target variable $f(x)$. \n",
    "\n",
    "This will typically lead to an increase of the bias.\n",
    "\n",
    "The number of neighbors, $k$, that we use is inversely related to the model complexity. \n",
    "\n",
    "Small values of $k$ lead to a flexible model that can adapt to local variations in $f(x)$ across $x$ well, but will give a high variance contribution to the generalization error. \n",
    "\n",
    "Large values of $k$ lead to a lower variance, at the price of less flexibility, and hence a larger generalization bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 20\n",
    "sigma = 0.2\n",
    "\n",
    "def f(x): return np.sin(2*np.pi*x) # The sinusoidal function underlying the data-generating process\n",
    "\n",
    "X = np.linspace(0, 1, N).reshape(N,1) # Features are stored in a 2-dimensional ndarray\n",
    "eps = np.random.normal(0, sigma, N)\n",
    "y = f(X.reshape(N)) + eps # The target variable is stored in a 1-dimensional ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Plot of regression function and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diks\\AppData\\Local\\Temp\\ipykernel_7268\\743884207.py:5: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"bo\" (-> color='b'). The keyword argument will take precedence.\n",
      "  plt.plot(X, y, 'bo', color='red') # Plotting the data points\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the datapoints and the underlying model:\n",
    "fig1 = plt.figure(1, figsize=(10,6))\n",
    "x_range = np.linspace(0,1,100)\n",
    "plt.plot(x_range, f(x_range)) # Plotting the sinusoid\n",
    "plt.plot(X, y, 'bo', color='red') # Plotting the data points\n",
    "\n",
    "plt.title('Data set of size N={}'.format(N))\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performing $k$-NN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diks\\AppData\\Local\\Temp\\ipykernel_7268\\3455129131.py:15: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"bo\" (-> color='b'). The keyword argument will take precedence.\n",
      "  plt.plot(X, y, 'bo', color='red') # Plotting the data points\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "k = 1\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=k, weights='distance') \n",
    "neigh.fit(X, y) \n",
    "\n",
    "S = 400 # Number of points for the fit plot\n",
    "Z = np.linspace(0, 1, S).reshape(S,1)\n",
    "\n",
    "# Plotting the datapoints, the fit and the underlying model:\n",
    "fig1 = plt.figure(1, figsize=(10,6))\n",
    "\n",
    "plt.plot(x_range, f(x_range)) # Plotting the sinusoid\n",
    "plt.plot(X, y, 'bo', color='red') # Plotting the data points\n",
    "plt.plot(Z, neigh.predict(Z), color='green') # Plotting the fit\n",
    "\n",
    "plt.title('Data set of size N={}'.format(N))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How should we choose $k$?\n",
    "\n",
    "\n",
    "Recall: **training error** is given by\n",
    "\\begin{equation}\n",
    "\\overline{\\text{err}} = \\frac{1}{N} \\sum_{n=1}^N  L(y_n, \\hat{f}(x_n)) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "**In-sample** optimization of the training error w.r.t. $k$ leads to\n",
    "* $k=1$ being optimal, and \n",
    "* an in-sample error equal to zero. \n",
    "\n",
    "We already knew that the training error can be a poor predictor of the generalization error for parametric models, \n",
    "\n",
    "in this nonparametric setting, optimizing the training error gives a useless estimate of the generalization error (=test error).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures_week2/Hastie_fig13.4a.png\" alt=\"Linear model errors\" style=\"display:block; margin-left: auto; margin-right: auto; width: 60%;\"/>\n",
    "<!-- <img src=\"figures_week6/Hastie_fig13.4b.png\" alt=\"Linear model errors\" style=\"display:block; margin-left: auto; margin-right: auto; width: 20%;\"/>\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "<!-- <table><tr><td><img src='figures_week6/Hastie_fig13.4a.png' style=\"display:block; margin-left: auto; margin-right: auto; width: 100%;\"></td><td><img src='figures_week6/Hastie_fig13.4b.png' style=\"display:block; margin-left: auto; margin-right: auto; width: 93%;\"></td></tr></table>\n",
    "-->\n",
    "\n",
    "Fig. 13.4(a) from Hastie et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leave-one-out ($N$-fold) cross-validation\n",
    "\n",
    "\n",
    "$k$ is chosen by minimization of\n",
    "\n",
    "\\begin{equation}\n",
    "CV = \\frac{1}{N} \\sum_{n=1}^N  L(y_n, \\hat{f}^{(n)}(x_n)), \n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{f}^{(n)}$ is the estimated regression function based on the entire sample except the $n^{\\rm th}$ observed $x$-$y$ pair $(x_n, y_n)$.\n",
    "\n",
    "Also $K$-fold cross-validation can be used.\n",
    "\n",
    "But this doesn't save much computational time in the current nonparametric setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using weighted averages\n",
    "\n",
    "Responses are weighted according to their distance to $x$ in the calculation of the average, so\n",
    "\n",
    "$$\n",
    "  \\hat f(x) =  \\frac{\\sum_{i=1}^k Y_{(i)} w_i}{\\sum_{i=1}^k w_i}\n",
    "$$\n",
    "\n",
    "where $Y_{(i)}$ is the $i$th-nearest neighbor. \n",
    "\n",
    "A popular choice is to use the inverse of the distance as the weight $w_i = 1/d(x, X_{(i)})$.\n",
    "\n",
    "Aims at keeping both the bias and the variance small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Curse of dimensionality\n",
    "\n",
    "The $k$-NN algorithm suffers substantially from the curse of dimensionality. \n",
    "\n",
    "For larger dimensions $M$ of the feature space, say $M > 5$.\n",
    "\n",
    "(Depending on the number of observations in the training set and the irregularity of the regression function.)\n",
    "\n",
    "Reason: in high-dimensional feature space, there are very few close neighbors to any point typically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Few close neighbors\n",
    "\n",
    "To see this, consider the case where the $X$-variables are uniformly distributed with uniform density around $x$, then \n",
    "\n",
    "$$\n",
    "  F_R(r) = P(R \\leq r) \\propto r^M, \\qquad \\mbox{for $r$ small.}\n",
    "$$ \n",
    "\n",
    "The expected number of neighbours within a given distance $r$ from $x$ is then proportional to $N (r/r_{\\rm max})^M$, \n",
    "\n",
    "For small $r$ this shrinks to zero fast as $M$ increases. \n",
    "\n",
    "Say we consider observations within distance $r = 0.1 r_{\\rm max}$ from $x$. \n",
    "\n",
    "Then an increase of $M$ by one additional dimension requires $10$ times more observations to retain the same expected number of neighbors within distance $r$ (!)\n",
    "\n",
    "For large $M$, dimension reduction (e.g. **PCA**) and/or **feature selection** should be used before $k$-NN algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $k$-NN is a locally constant model\n",
    "\n",
    "It is **nonparametric**: the number of parameters is proportional to $N$. \n",
    "\n",
    "It uses local averaging of the response across neighbours (either with or without weighting), \n",
    "\n",
    "Such models are known as **locally constant models** \n",
    "\n",
    "Consider, for example, the equally weighted $k$-NN algorithm. \n",
    "\n",
    "* Forecast equals the mean of $Y_n$ across the $k$ nearest neighbors\n",
    "\n",
    "* Can be thought of as regressing $Y_n$ on a constant  $$\\mbox{} \\qquad f(X) = c$$ \n",
    "locally near $x$, using only the $k$ observations $(X_{n}, Y_{n})$ closest to $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel regression\n",
    "\n",
    "Natural extension of KNN. Weights for the estimate of the regression funtction at $X=x$ now determined by a kernel function $K(\\cdot)$.\n",
    "\n",
    "\n",
    "\n",
    "Nadaraya and Watson (NW) have proposed a kernel density estimator for the regression function in this context, given by\n",
    "$$\n",
    " \\hat f(x) = \\frac{\\sum_{i=1}^N K_h(x - X_i) y_i}{\\sum_{j=1}^N K_h(x - X_j)} \\equiv \\sum_{i=1}^N w_i y_i,\n",
    "$$\n",
    "where the (local) weights $w_i$ are given by\n",
    "$$\n",
    " w_i = \\frac{K_h(x-X_i)}{\\sum_{j=1}^N K_h(x-X_j)}.\n",
    "$$\n",
    "\n",
    "Here $K_h(x) = \\frac{1}{h} K(x/h)$ for a fixed kernel function $K$, so that the so-called bandwidth parameter $h$ acts as a scale parameter of the kernel function $K_h(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Popular choice for the kernel function $K$: standard normal (gaussian) kernel function \n",
    "$K(x) = \\frac{1}{\\sqrt{2\\pi} } \\exp(-x^2/2)$. \n",
    "\n",
    "For one-dimensional feature vectors $X$, and upon introducing the bandwidth parameter $h$, this gives\n",
    "$$\n",
    "  K^1_h(s) =\\frac{1}{\\sqrt{2 \\pi} h} \\, {\\rm e}^{-s^2/(2 h^2)}.\n",
    "$$\n",
    "\n",
    "In multivariate cases (dimension, $M$, say) one typically takes the product kernel, given by\n",
    "$$\n",
    "  K^M_h(s) \\equiv \\prod_{m=1}^M \\frac{1}{\\sqrt{2 \\pi} h}  \\, {\\rm e}^{-s_m^2/(2 h^2)}\n",
    "$$\n",
    "which for the gaussian kernel becomes\n",
    "$$\n",
    "  K^M_h(s) \\equiv (2 \\pi h^2)^{-M/2}  \\, {\\rm e}^{-\\|s\\|^2/(2 h^2)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other choices for the kernel are possible. Typically, we require the kernel function $K(\\cdot)$ to have the following properties:\n",
    "\n",
    "* Continuity: $K(s)$ is continuous in $s$.\n",
    "* Positivity: $K(\\cdot)$ is a probability kernel, i.e., it is non-negative and integrating to $1$. \n",
    "* Symmetry: $K(\\cdot)$ is symmetric in its argument, that is, $K(-s) = K(s)$.\n",
    "\n",
    "Additional properties (such as differentiability) are sometimes added as required, for instance, to perform asymptotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The MSE of the NW estimator\n",
    "\n",
    "The bias at a given value of $x_0$ is then given by\n",
    "$$\n",
    "{\\rm bias}(\\hat f(x_0)) = \\frac{h^2}{2}  \\left(f''(x_0) + 2 \\frac{f''(x_0)p'(x_0)}{p(x_0)} \\right) \\int x^2 K(x) {\\rm d} x + o(h^2),\n",
    "$$\n",
    "where we use the small `oh' notation $o(h^2)$ to indicate that the remainder terms (beyond the leading term(s) given) vanish relative to $h^2$ as $h \\rightarrow 0$, or that ${\\rm remainder}(h)/h^2 \\mathop{\\rightarrow}\\limits^{P} 0$ as $h \\rightarrow 0$.\n",
    "\n",
    "The variance is\n",
    "$$\n",
    " {\\rm Var}(\\hat f(x_0)) = \\frac{\\sigma_{\\varepsilon}^2}{N h} \\frac{1}{p(x_0)} \\left(\\int K^2(x) \\, {\\rm d} x \\right)^2 + o\\left(\\frac{1}{Nh}\\right),\n",
    "$$\n",
    "where $\\sigma_{\\varepsilon}^2$ is the variance of the irreducible error term $\\varepsilon$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Squared bias and variance together:\n",
    "$$\n",
    " {\\rm MSE}(\\hat f(x_0)) = \\frac{h^4 \\mu_K}{4} \\left(f''(x_0) + 2 \\frac{f''(x_0)p'(x_0)}{p(x_0)} \\right)^2 + \\frac{\\sigma_{\\varepsilon}^2 \\sigma^2_K}{N h} \\frac{1}{p(x_0)}  + o(h^2) + o\\left(\\frac{1}{Nh}\\right),\n",
    "$$\n",
    "where $\\mu_K = \\int x^2 K(x) \\, {\\rm d} x$ and $\\sigma^2_K = \\left(\\int K^2(x) \\, {\\rm d} x \\right)^2$.\n",
    "\n",
    "A global measure of the expected MSE is the so-called mean integrated squared error is\n",
    "$$\n",
    " {\\rm MISE}(\\hat f) = \\frac{h^4 \\mu_K}{4} \\int \\left(f''(x) + 2 \\frac{f''(x)p'(x)}{p(x)} \\right)^2 \\, {\\rm d} x + \\frac{\\sigma^2_{\\varepsilon} \\sigma^2_K}{N h} \\int \\frac{1}{p(x)} \\, {\\rm d} x  + o(h^2) + o\\left(\\frac{1}{Nh}\\right).\n",
    "$$\n",
    "\n",
    "MISE-optimal bandwidth value for a given sample size $N$ can be found by minimizing the leading MISE terms, the so-called AMISE, with respect to $h$. \n",
    "\n",
    "Leads to\n",
    "$$\n",
    "  h^* = C^* \\times N^{-\\frac{1}{3}},\n",
    "$$\n",
    "where $C^*$ depends on the underlying density $p(x)$ and on the kernel function $K$ used.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local linear models: $k$-NN\n",
    "\n",
    "obtained by performing a **local linear regression** of $Y$ on $X$, locally near $x$, \n",
    "\n",
    "using the $k$ closest neighbours of $x$ in feature space. \n",
    "\n",
    "This may improve the performance in particular if the regression function has smooth derivatives. \n",
    "\n",
    "Note: although a parametric estimation step is involved (a different one for each possible $x$) this is still a **nonparametric** method; the number of parameters is still proportional to $N$. \n",
    "\n",
    "However, we now estimate a local model with $M+1$ regressors (including a constant for the intercept), instead of one\n",
    "\n",
    "$\\Rightarrow$ the local linear approach suffers even more from the **curse of dimensionality** than the locally constant $k$-NN method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local linear models: kernel regression\n",
    "\n",
    "\n",
    "The NW estimator is the result of a locally constant kernel regression near $x=X$. \n",
    "\n",
    "This setup can be generalized to the local linear (and higher-order) regression, by estimating a linear model locally. \n",
    "\n",
    "That is by performing a weighted linear regression for the model\n",
    "$$ \n",
    "  y = \\beta' X + \\varepsilon.\n",
    "$$\n",
    "with weights $w_i$ determined by the kernel function, locally near $X=x$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Test for linearity\n",
    "\n",
    " Local linear models can also be used to test for linearity. \n",
    " \n",
    "The idea is that by choosing $k$ = $N$ for $k$-NN (or $h \\to \\infty$ in kernel regression) you end up with a global linear model. \n",
    " \n",
    " Testing the performance of this global linear model against that obtained with smaller number of neighbors $k$ or smaller kernel bandwidth $h$ provides a test for linearity versus nonlinearity. \n",
    " \n",
    "Suppose that at a given level of confidence, the flexible model with smaller $k$ or $h$ performs significantly better than the global linear model obtained, \n",
    "\n",
    "then the null hypothesis of a global linear model can be rejected, in favor of the nonlinear model with higher complexity."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
