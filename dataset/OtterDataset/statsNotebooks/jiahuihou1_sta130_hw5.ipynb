{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "For an idea to be able to be tested statistically, the data surrounding it must be measurable. Tested ideas needs to have clearly defined, quantifiable parameters that can be compared to using existing data. \n",
    "A GOOD null hypothesis is precise, measurable, and represents clearly the status quo. It should state the assumption clearly, of which is able to be tested and potentially disproven after testing. \n",
    "A null hypothesis assumes no change or effect, while an alternative hypothesis suggests a case that could challenge the null hypothesis. \n",
    "\n",
    "## 2.\n",
    "\n",
    "In hypothesis testing, the focus is on making inferences about the population parameter $\\mu$ (the true mean or average of an entire population), rather than just the sample statistic $\\bar{x}$ (the mean calculated from a sample of that population). The outcome of the test attempts to figure ouit if $\\mu$ likely equals a specific value, $\\mu_0$ (the hypothesized population mean).\n",
    "\n",
    "Explanation for each symbol:\n",
    "- **$x_i$**: These represent individual observations or data points in a **sample** (a subset of the population).\n",
    "- **$\\bar{x}$**: This is the **sample mean**, the average value of the $x_i$s. It estimates $\\mu$ based on the sample data.\n",
    "- **$\\mu$**: This is the **population mean**, which represents the true average of a characteristic across the entire population.\n",
    "- **$\\mu_0$**: This is the **hypothesized population mean** specified in the null hypothesis, which is the assumption we test against using sample data.\n",
    "\n",
    "The point of hypothesis testing is using $\\bar{x}$ to determine whether the evidence supports or contradicts $\\mu_0$, hence helping us make a conclusion about the true value of $\\mu$, essentially moving away from what we observed from the sample, to what we can infer about the entire population. \n",
    "\n",
    "## 3.\n",
    "\n",
    "We imagine a world where the null hypothesis is true because the null hypothesis represents the \"default\", what we assume is true BEFORE seeing any data. We can then calculate how likely our observed data would be if that assumption were true. This probability is what the p-value measures. A small p-value suggets the observed data is unlikely to occur in our \"null world\" (a world where we assume the null hypothesis is true), and the opposite is true for a big p-value. Therefore by imagining that the null is true, we are able to use the p-value to measure the evidence against it. \n",
    "\n",
    "## 4. \n",
    "\n",
    "Based on the answer to question 3 where we established that when we have a null hypothesis, we assume its true; a small p-value means that sticking with the null hypothesis makes the data look surprising or strange, so we start considering $H_1$ (alternate hypothesis).\n",
    "\n",
    "## 5. \n",
    "\n",
    "### Simulating a P-value for the Head Tilt Study\n",
    "\n",
    "**Objective**: To simulate a p-value using a 50/50 coin-flipping model to test the null hypothesis that nobody has a preference towards kissing posture, regarding left or right.\n",
    "\n",
    "**Null Hypothesis ($H_0$)**: The probability of tilting to the right is 0.5 (no preference).\n",
    "\n",
    "**Alternative Hypothesis ($H_1$)**: The probability of tilting to the right is NOT 0.5 (hence preference).\n",
    "\n",
    "**Observed Data**: Out of 124 couples, 80 (or 64.5%) tilted their heads to the right.\n",
    "\n",
    "### Simulation Procedure:\n",
    "\n",
    "1. **Model the Null Hypothesis**: \n",
    "   - We assume each couple's head tilt is a coin flip (50/50 for left or right).\n",
    "   \n",
    "2. **Simulate Many Samples**: \n",
    "   - Flip a \"coin\" (or rng between 1 and 2) 124 times to simulate 1 sample of 124 couples.\n",
    "   - Count how many times the \"coin\" lands as \"right\" (i.e., how many couples tilt right).\n",
    "   - Repeat this simulation, a large number of times (10,000 is generally enough, for my code, i will use 1,000,000 for the sake of accuracy) times, to create a distribution of outcomes. (we can also use bootstrapping which we learned last week)\n",
    "\n",
    "3. **Calculate the P-value**:\n",
    "   - The p-value is the proportion of simulations where the number of \"right\" tilts is at least as extreme as the observed 80 tilts.\n",
    "   - In this case, we look for the proportion of simulations where the number of right tilts is 80 or more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.000823)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_couples = 124  # total number of couples\n",
    "observed_right_tilts = 80  # observed number of right tilts\n",
    "n_simulations = 1000000  # number of simulations\n",
    "\n",
    "# Simulate the number of right tilts under the null hypothesis (50/50 chance)\n",
    "simulated_right_tilts = np.random.binomial(n=n_couples, p=0.5, size=n_simulations)\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = np.mean(simulated_right_tilts >= observed_right_tilts)\n",
    "p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| P-value Range      | Evidence Against $H_0$ |\n",
    "|--------------------|------------------------|\n",
    "| $p > 0.10$         | No evidence            |\n",
    "| $0.05 < p \\leq 0.10$ | Weak evidence         |\n",
    "| $0.01 < p \\leq 0.05$ | Moderate evidence     |\n",
    "| $p \\leq 0.01$      | Strong evidence        |\n",
    "\n",
    "with the p value being approximately 0.0008, we know that there is a very strong evidence agaisnt the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "A smaller p-value **cannot** definitively prove that the null hypothesis is false. A p-value can only show how likely the observed data is if the null hypothesis were true, but it does not provide absolute proof.\n",
    "\n",
    "- **Proving Innocence (Null Hypothesis True)**: A high p-value suggests that the data is consistent with the null hypothesis (e.g., Fido being innocent in the pre lecture video), but it does not definitively prove innocence. It just means that the evidence isn’t strong enough to reject the null hypotheiss.\n",
    "\n",
    "- **Proving Guilt (Null Hypothesis False)**: A low p-value suggests that the data is very unlikely if the null hypothesis were true, providing evidence against it (e.g., suggesting Fido might be guilty). However, it still does not provide absolute proof, as even the smallest possibility is still a possibility nontheless.\n",
    "\n",
    "- **Thresholds for P-values**: There is no p-value threshold that can definitively prove one or the other. Even with very small p-values (like 0.0008 from question 5), we can only say that the data VERY strongly contradicts the null hypothesis, not that it is absolutely false. Hypothesis testing is about weighing evidence, not providing certainty.\n",
    "\n",
    "## 7. \n",
    "\n",
    "(https://chatgpt.com/share/670385ae-48e8-8010-b8c3-a296cfa6cd11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Test Statistic: 0.8\n",
      "P-value (one-sided test): 0.0509\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "patient_data = pd.read_csv('vaccine.csv')\n",
    "\n",
    "# Compute the HealthScoreChange as FinalHealthScore - InitialHealthScore\n",
    "patient_data['HealthScoreChange'] = patient_data['FinalHealthScore'] - patient_data['InitialHealthScore']\n",
    "\n",
    "# Define the null hypothesis value\n",
    "# Under H0, we assume the probability of a positive change is 0.5\n",
    "population_parameter_value_under_H0 = 0.5\n",
    "\n",
    "# Calculate the observed test statistic (proportion of positive changes)\n",
    "observed_test_statistic = (patient_data['HealthScoreChange'] > 0).mean()\n",
    "\n",
    "# Simulate test statistics under the null hypothesis\n",
    "# We'll simulate by assuming a 50% chance of a positive change for each patient\n",
    "number_of_simulations = 10000\n",
    "simulated_test_statistics = np.random.binomial(\n",
    "    n=len(patient_data),\n",
    "    p=population_parameter_value_under_H0,\n",
    "    size=number_of_simulations\n",
    ") / len(patient_data)\n",
    "\n",
    "# For a one-sided test, determine if we are testing for greater or lesser values.\n",
    "# Example: If the alternative hypothesis is that the proportion of positive changes is greater than 0.5:\n",
    "SimTestStats_as_or_more_extreme_than_ObsTestStat = simulated_test_statistics >= observed_test_statistic\n",
    "# use SimTestStats_as_or_more_extreme_than_ObsTestStat = simulated_test_statistics <= observed_test_statistic if you want the other one sided test.\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = SimTestStats_as_or_more_extreme_than_ObsTestStat.mean()\n",
    "\n",
    "print(f\"Observed Test Statistic: {observed_test_statistic}\") # (proportion of positive changes)\n",
    "print(f\"P-value (one-sided test): {p_value}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing from a two sided to one sided hypothesis does affect the interpretation of the p value. The alternative hypothesis for the previously conducted two sided was for any value that was **NOT** 0.5, while now, with our new one sided hypothesis, we are looking at it from a different angle, with the alternative hypothesis now being either **below** or **above** 0.5. \n",
    "\n",
    "As for whether or not the p value will be smaller for our one sided analysis, the answer is yes. This is because we are only looking for one side of the posibility. In a two-sided test, we are testing whether the observed value is significantly different from the null hypothesis value in **either** direction, whikle in a one-tailed test, we are only testing whether the observed value is significantly different in **one** specific direction (either greater than or less than)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8.\n",
    "\n",
    "(https://chatgpt.com/share/67038e29-09a0-8010-bc5a-5e9f34b17281)\n",
    "\n",
    "# Analysis of Fisher's Tea Experiment with STA130 Students\n",
    "\n",
    "### Relationship Between This Experiment and the Original with Fisher and Bristol\n",
    "Fisher's hypothesis test was designed to determine if her friend, Dr. Bristol's ability to distinguish whether milk or tea was poured first was due to chance or if she genuinely had this skill. In this experiment, we are extending a similar concept but to a larger population. Hence making the parameter more abstract, as now we're focusing on a broader population, rather than a personalized context of the original experiment. \n",
    "\n",
    "### Statements of the Null Hypothesis and Alternative Hypothesis\n",
    "- **Null Hypothesis (H₀)**:  \n",
    "  \\( $H_0$: p = 0.5 \\), where \\( p \\) is the true proportion of STA130 students who can correctly identify whether the milk or tea was poured first by random guessing. This means that students are just guessing, with a 50% chance of being correct.\n",
    "\n",
    "- **Alternative Hypothesis (H₁)**:  \n",
    "  \\( $H_1$: p > 0.5 \\), which means that more than half of the STA130 students can correctly identify the pouring order. This suggests that students' ability is better than random guessing, indicating some genuine ability.\n",
    "\n",
    "  # Quantitative Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Proportion (p̂): 0.6125\n",
      "p-value: 0.0283\n",
      "95% Confidence Interval for the proportion: (np.float64(0.5057421109472466), np.float64(0.7192578890527535))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom, norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1111)\n",
    "\n",
    "# Parameters for the experiment\n",
    "total_students = 80  # Total number of students in the sample\n",
    "correct_identifications = 49  # Number of students who correctly identified the pouring order\n",
    "null_hypothesis_probability = 0.5  # Probability under the null hypothesis (random guessing)\n",
    "\n",
    "# Calculate the observed proportion of correct identifications\n",
    "observed_proportion = correct_identifications / total_students\n",
    "\n",
    "# Calculate the p-value using a binomial test\n",
    "# The p-value represents the probability of observing at least this many correct identifications by chance\n",
    "p_value = 1 - binom.cdf(correct_identifications - 1, total_students, null_hypothesis_probability)\n",
    "\n",
    "# Calculate the standard error for the observed proportion\n",
    "standard_error = np.sqrt(observed_proportion * (1 - observed_proportion) / total_students)\n",
    "\n",
    "# Calculate a 95% confidence interval for the observed proportion\n",
    "confidence_interval = (\n",
    "    observed_proportion - 1.96 * standard_error, \n",
    "    observed_proportion + 1.96 * standard_error\n",
    ")\n",
    "\n",
    "# Print the results with explanations\n",
    "print(f\"Observed Proportion (p̂): {observed_proportion:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"95% Confidence Interval for the proportion: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Discussion\n",
    "\n",
    "### Conclusion Regarding the Null Hypothesis\n",
    "- **Analysis of p-value**: The p-value helps determine if the observed results (49 correct identifications) are consistent with the assumption that students are just guessing. A p-value less than 0.05 would indicate that the observed results are unlikely to occur by chance, leading us to reject the null hypothesis.\n",
    "\n",
    "- **Interpretation of the Confidence Interval**: The 95% confidence interval for the observed proportion gives us a range of plausible values for the true proportion of students who can correctly identify the pouring order. If this interval does not include 0.5, it supports rejecting the null hypothesis.\n",
    "\n",
    "- **Final Conclusion**: Based on the p-value and confidence interval, we conclude whether there is sufficient evidence to suggest that STA130 students have an ability to distinguish the pouring order beyond random guessing. If the null hypothesis is rejected, it suggests that the students' ability is more than just random chance, similar to the claim tested in Fisher’s original experiment.\n",
    "\n",
    "\n",
    "## 9. \n",
    "yes, mostly with chatbot and textbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
