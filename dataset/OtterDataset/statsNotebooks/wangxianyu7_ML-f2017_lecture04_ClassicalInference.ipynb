{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=0.6\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'scroll': True,\n",
    "        'width': 1024*factor,\n",
    "        'height': 768*factor\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Inference\n",
    "\n",
    "K. Leighly 2017\n",
    "\n",
    "This lecture was drawn from the following sources:\n",
    " - G. Richards \"Inference\" lecture\n",
    " - Ivezic Chapter 4\n",
    " - \"Data Reduction and Error Analysis in the Physical Sciences\", P. R. Bevington\n",
    " - \"Numerical Recipes: The Art of Scientific Computation\" 3rd Edition, W. H. Press, S. A. Teukolsky, W. T. Vetterling, B. P. Flannery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Statistical *inference* is about drawing conclusions from data, specifically determining the properties of a population by data sampling.  \n",
    "\n",
    "This is a complicated topic, and it would be easy to build a full-semester course on it.  We will spend only one lecture period on it, covering a selection of the most basic results, and some useful applications.\n",
    "\n",
    "In particular, we will discuss: \n",
    " - Maximum likelihood estimation (MLE), a fundamental result that will lead us directly to a discussion of $\\xi^2$ and its application and interpretation.\n",
    " - Bootstrap error analysis\n",
    " - Hypothesis Testing and Comparison of Distributions\n",
    " \n",
    "Ivezic Chapter 4 covers other interesting and useful topics:\n",
    " - The Expectation Maximization Algorithm (we will discuss this when we talk about PCA).\n",
    " - Histograms (we will cover this later, when we talk about cluster analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Maximum Likelihood Estimation is a cornerstone of classical inference.  We have already touched upon some aspects of this in our discussion of Gaussian distributions.\n",
    "\n",
    "If we know the distribution from which our data were drawn, then we can compute the **probability** or **likelihood** of our data.  \n",
    "\n",
    "For example if you know that your data are drawn from a model with a Gaussian distribution, then we've already seen that the probablity of getting a specific value of $x$ is given by\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "If we want to know the total likelihood of our *entire* data set (as opposed to one measurement) then we must compute the *product* of all the individual probabilities (assuming that all the data are drawn independently):\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^n p(x_i|M(\\theta)),$$\n",
    "where $M$ refers to the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can be multi-dimensional.\n",
    "\n",
    "In words, this is *the probability of the data given the model*.  However, note that while the components of $L$ may be normalized pdfs, their product is not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The product can be very small, so we often take the log of $L$.\n",
    "\n",
    "We can write this out as\n",
    "$$L = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "and simplify to\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the product of the exponentials as the exponential of the sum of the arguments, which will make things easier to deal with later.\n",
    "\n",
    "That is, we have done this: $$\\prod_{i=1}^n A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_n) \\exp[-(B_i+B_{i+1}+\\ldots+B_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g.,, doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, maximizing the likelihood is the same as minimizing $\\chi^2$.\n",
    "\n",
    "We will discuss this in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we could have asked instead about the likelihood of the *model* given the *data*:\n",
    "$$L \\equiv p(M(\\theta)|\\{x_i\\}) = \\prod_{i=1}^n p(M(\\theta)|x_i).$$\n",
    "\n",
    "We'll come back to that in the next lecture; that is a fundamental aspect of Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That's the **likelihood**, but what is the **maximum likelihood**?\n",
    "\n",
    "Well, let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters), then MLE is about varying the parameters until we find the maximal value of $L$.  Simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Homoscedastic Gaussian\n",
    "\n",
    "Let's take a look at an example using a Gaussian model where all the measurements have the same error ($\\sigma$).  This is known as having **homoscedastic** errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For an experiment with data $D=\\{x_i\\}$ in 1D with Gaussian errors, we have\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability _of the full data set_, not just one measurement.\n",
    "\n",
    "If $\\sigma$ is both uniform and *known*, then this is a one-parameter model with $k=1$ and $\\theta_1=\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the *log-likelihood function* as ${\\rm lnL} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have $${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "\n",
    "$$L = \\prod_{i=1}^n \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\mu_0} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\mu_o)}{\\sigma^2} = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$, that says \n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\mu_0 = N \\mu_0.$$\n",
    "\n",
    "Thus we find that\n",
    "$$\\mu_0 = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which is just the arithmetic mean of all the measurements.\n",
    "\n",
    "This is a very common result - the most likely value is the arithmetic mean.  Many people, if asked with is the most probable value, or the expected value from the data set, would posit the mean.  But take special note; we have implicitly assumed that the underlying distribution was Gaussian, and that all the errors are the same.  If the underlying distribution is not gaussian, then this may not be a good estimator of the most probable value.\n",
    "\n",
    "(And yes, we already discussed this in the previous lecture, but it is such a fundamental result that it bears repeating.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The uncertainty on our MLEs can be determined from the covariance matrix:\n",
    "$$\\sigma_{jk} = \\left( - \\frac{d^2}{d\\theta_j} \\frac{\\ln L}{d\\theta_k} \\Biggr\\rvert_{\\theta=\\theta_0}\\right)^{-1/2}.$$\n",
    "\n",
    "- The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$.  \n",
    "- If $\\sigma_{jk}=0$ for $j \\ne k$ then the parameters are uncorrelated.  In this case, the $\\sigma_{ii}$ are direct analogs of error bars in the one-dimensional problem.\n",
    "- If $\\sigma_{jk} \\ne 0$ for $j \\ne k$, then the errors for parameters $\\theta_j$ and $\\theta_k$ are correlated.  You can picure this, in the two dimensional space, as a bivariate Gaussian with principal axes that are not aligned with coordinate axes.  One of the things this tells us is that some combination of parameters are better determined than others.\n",
    "\n",
    "(You have already worked with the bivariate Gaussian in the homework.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Where does this come from?  It is derived by expanding $\\ln L$ in a Taylor expansion.  \n",
    "\n",
    "Recall the taylor expansion:\n",
    "\n",
    "$$f(a)+\\frac{f^\\prime}{1!}(x-a)+\\frac{f^{\\prime\\prime}}{2!}(x-a)^2+\\frac{f^{\\prime\\prime\\prime}}{3!}(x-a)^3+\\dots.$$\n",
    "\n",
    "We are evaluating at the maximum, so the odd terms are equal to zero by symmetry.\n",
    "\n",
    "When we truncate to the second term, it means is that we are assuming that the shape of the $\\ln L$ is a parabola.  This may not necessarily be the case, but is if the error distribution is Gaussian (as the higher order terms are zero).  \n",
    "\n",
    "We will talk in more detail in the next lecture about what to do when the likelihood is not parabolic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our example, the uncertainly on the mean is \n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "$$\\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\mu_0} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then $$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is a result that you should be familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Heteroscedastic Gaussian\n",
    "\n",
    "Now let's look a case where the errors are **heteroscedastic**.  For example if we are measuring the length of a rod and have $N$ measurements, $\\{x_i\\}$, where the error for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is not a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\mu_0} = \\sum_{i=1}^N \\frac{(x_i - \\mu_o)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\mu_o}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\mu_0 = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "(Note that this is just the same as taking the weighted mean.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Truncated/Censored Data and Other Cost Functions\n",
    "\n",
    "Note that dealing with missing data points (\"censored data\") adds complications.  There are ways to treat this from a classical statistics point of view, but we will instead look at this question when we talk about linear regression, and use a Bayesian point of view. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Goodness\" of Fit\n",
    "\n",
    "The MLE approach tells us what the \"best\" model parameters are, but _not how good the fit actually is_.  If the model is wrong, \"best\" might not be particularly revealing!  For example, if you have $N$ points drawn from a linear distribution, you can always fit the data perfectly with an $N-1$ order polynomial.  But that won't necessarily perfectly predict future measurements.\n",
    "\n",
    "We can describe the **goodness of fit** in words simply as _whether or not it is likely to have obtained the measured value of $\\ln L_0$ by randomly drawing from the data_.  That means that we need to know the *distribution* of $\\ln L$.  \n",
    "\n",
    "For the Gaussian case we have just described, we can write\n",
    "$$z_i = (x_i-\\mu)/\\sigma,$$ then\n",
    "$$\\ln L = {\\rm constant} - \\frac{1}{2}\\sum_{i=1}^N z^2 = {\\rm constant} - \\frac{1}{2}\\chi^2.$$\n",
    "\n",
    "Here, $\\chi^2$ is the same thing that you may already be familar with and whose distribution we discussed very briefly last week, and will be dicussed in more detail in the next slide.\n",
    "\n",
    "So $\\ln L$ is distributed as $\\chi^2$ (with $N-k$ degrees of freedom).  \n",
    "\n",
    "We define the $\\chi^2$ per degree of freedom, $\\chi^2_{dof}$, as\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-k}\\sum_{i=1}^N z^2_i.$$\n",
    "\n",
    "For a good fit, we would expect that $\\chi^2_{dof}\\approx 1$.  If $\\chi^2_{dof}$ is significantly larger than 1, then it is likely that we are not using the correct model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "\n",
    "then the sum of squares, $Q$ \n",
    "\n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints, i.e., the number of fit parameters).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  \n",
    "\n",
    "Examples of the $\\chi^2$ distribution are shown below.\n",
    "\n",
    "![Ivezic, Figure 3.14](http://www.astroml.org/_images/fig_chi2_distribution_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Understanding $\\chi^2$\n",
    "\n",
    "Let us examine the formula for $\\chi^2$:\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^N \\left(\\frac{y_i-y}{\\sigma_i} \\right)^2$$\n",
    "\n",
    " - The numerator is a measure of the spread of the observations.\n",
    " - The denominator is a measure of the expected spread due to the uncertainty on the data.\n",
    " - One might expect the ratio of the numerator and denominator to be $\\sim 1$ for a single data point, if the model describes the data well.\n",
    " - Then if we sum over $N$ data points, we should obtain about $\\chi^2 \\sim N$.\n",
    " - So reduced $\\chi^2_{dof}\\sim 1$ for a good fit makes sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of Goodness of Fit\n",
    "\n",
    "Let's generate some simulated data, fit it, and examine the goodness of fit.  This follows a portion of the example in Bishop Chapter 1.\n",
    "\n",
    "Specifically, we will generate one cycle of a sine wave, and fit it with various polynomials, and examine the quality of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Initiate python stuff\n",
    "%pylab inline\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Set up an x-vector\n",
    "x=np.linspace(0,1,11)\n",
    "\n",
    "# y is a sine wave\n",
    "y=np.sin(2*np.pi*x)\n",
    "\n",
    "#give y some uncertainty, i.e., apply a Gaussian scatter around \n",
    "#the theoretically determined values.\n",
    "ynew=y+np.random.normal(0,0.15,11)\n",
    "\n",
    "#assign some errors.  These errors don't have any physical interpretation, \n",
    "#but are reasonably appropriate, as they are scattered around the \n",
    "#sigma of the uncertainty gaussian (=0.15)\n",
    "err=np.random.uniform(0.1,0.2,11)\n",
    "\n",
    "#plot ideal y, and data.\n",
    "plt.plot(x,y)\n",
    "plt.errorbar(x, ynew, yerr=err, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now fit with polynomials of different orders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#fit the data with a zeroth order polynomial using polyfit\n",
    "result0=np.polyfit(x,ynew,0,w=1/err)\n",
    "print 'The best fit coefficients ',result0\n",
    "\n",
    "#poly1d will generate the model using the best fit coefficients.  \n",
    "y0=np.poly1d(result0)\n",
    "y0out=y0(x)\n",
    "plt.errorbar(x, ynew, yerr=err, fmt='o')\n",
    "plt.plot(x,y0out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The zeroth order polynomial is a very poor fit to the data.  How poor is it?  We can answer that question by evaluating the $\\chi^2$, i.e., \n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^N \\left(\\frac{y_i-y}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where $y_i$ are the observed points, $\\sigma_i$ are the uncertainties on those points, and $y$ is the model.\n",
    "\n",
    "So $\\chi^2$ is the sum of the weighted squared distance between the data and the model.  \n",
    "\n",
    "We know from above that minimizing $\\chi^2$ is equivalent to maximizing the likelihood, for the case of the Gaussian likelihood.  That is what polyfit is doing.  We will discuss the process of maximizing the likelihood later.\n",
    "\n",
    "The output of the polyfit are the fit coefficients (in result).  Those can be used to evaluate the model at {$x_i$}.  Then the model can be compared with the data using the formula for $\\chi^2$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But now we want to understand what the computed $\\chi^2$ tells us. Section 3.3.7 in Ivezic, \"If {$x_i$) are drawn from a Gaussian distriubtion, and we define $z_i=(x_i-\\mu)/\\sigma$, then the sum of its squares, $Q=\\sum z_i^2$ follows a $\\chi^2$ distribution with $k=N$ degrees of freedom$\\dots$\" (and then an analytical form for the distribution is given; in practice, we can evaluate using the built-in functions from scipy.stats.). \n",
    "\n",
    "Here, the idea is that the model is the model that produced the data, and the data, at each point, are drawn from a Gaussian distribution around the model at that point, characterized by a mean (which is the model) and the standard deviation, which expresses the noise on the data.  \n",
    "\n",
    "Bishop Figure 1.16 illustrates this idea.  Keep in mind that we are not talking about modeling data with a Gaussian, but that the noise on the data is distributed in a Gaussian way around the true value. \n",
    "\n",
    "\n",
    "The expectation value of the $\\chi^2$ distribution is the number of degrees of freedom $N-k$, where $N$ is the number of data points, and $k$ is the number of model parameters.  What this means in practice is that you cannot evaluate the probability of obtaining a certain value alone, you must compare it with that expected for the number of degrees of freedom.  As noted above, the $\\chi^2$ distribution is a function of a single parameter, the number of degrees of freedom (above denoted as $k=N$ but here corrected for the complexity of the model as $N-k$).\n",
    "\n",
    "Moreover, the rule of thumb for $\\chi^2$ is that the reduced $\\chi^2$, also known as $\\chi^2_\\nu$ or $\\chi^2_{dof}$, should be approximately 1 for a good fit. \n",
    "\n",
    "$$\\chi^2_{dof}=\\frac{1}{N-k} \\sum_{i=1}^N z_i^2 \\approx 1.$$\n",
    "\n",
    "Back to our example.  We can compute the $\\chi^2$ for the fit, and estimate the probability that the model provides a good fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Compute chi2\n",
    "chi2=(((ynew-y0out)/err)**2).sum()\n",
    "print 'The value of chi2 is ', chi2\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#number of degrees of freedom equals the number of points minus the number of fit parameters.  \n",
    "#We drew 11 points, and have a single fit parameter.\n",
    "\n",
    "dist=stats.chi2(len(x)-1)\n",
    "print 'The probability obtainning such a high value of chi2 \\\n",
    "if the model were a correct representation of the data ',dist.pdf(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability of obtaining the data given that the model is a zeroth order polynomial is very small.\n",
    "\n",
    "It is not surprising that the zeroth degree polynomial does not provide a good fit to the data.  Let's try a first degree polynomial, i.e., a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result1=np.polyfit(x,ynew,1,w=1/err)\n",
    "print 'The fit coefficients ',result1\n",
    "y1=np.poly1d(result1)\n",
    "y1out=y1(x)\n",
    "plt.errorbar(x, ynew, yerr=err, fmt='o')\n",
    "plt.plot(x,y1out)\n",
    "\n",
    "chi2=(((ynew-y1out)/err)**2).sum()\n",
    "print 'The chi2 is ', chi2\n",
    "\n",
    "from scipy import stats\n",
    "dist=stats.chi2(len(x)-len(result1))\n",
    "print 'The probability that the straight line model could produce the observed data ',dist.pdf(chi2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The probability of obtaining our data set using a linear model is larger than the probability of obtaining our data set using a constant model, but it is still not large.  So let's try increasing the order of the polynomial.\n",
    "\n",
    "\n",
    "\n",
    "Better to write a function rather than repeating the same snippets of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_polyfit(x,ynew,err,degree):\n",
    "    result=np.polyfit(x,ynew,degree,w=1/err)\n",
    "    print 'The fit coefficients ',result\n",
    "    ymodel=np.poly1d(result)\n",
    "    ymodelout=ymodel(x)\n",
    "    plt.errorbar(x,ynew,yerr=err,fmt='o')\n",
    "    plt.plot(x,ymodelout)\n",
    "    chi2=(((ynew-ymodelout)/err)**2).sum()\n",
    "    print 'The chi2 ',chi2\n",
    "    numpnts=len(x)\n",
    "    print 'The reduced chi2',chi2/(numpnts-degree)\n",
    "    dist=stats.chi2(numpnts-degree)\n",
    "    dof=numpnts-degree\n",
    "    print 'The probability that the model could produce the observed data ',dist.pdf(chi2)\n",
    "    return chi2,dof\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=2\n",
    "chi2_2,dof_2=get_polyfit(x,ynew,err,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=3\n",
    "chi2_3,dof_3=get_polyfit(x,ynew,err,degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we obtain a decent probability for a third-order polynomial.\n",
    "\n",
    "But we can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=4\n",
    "chi2_4,dof_4=get_polyfit(x,ynew,err,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=5\n",
    "chi2_5,dof_5=get_polyfit(x,ynew,err,degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F test\n",
    "\n",
    "We observe that increasing the order beyond 3 does not improve the quality of the fit dramatically.  Is there a way to test that quantitatively?  Yes, we can use the F test.\n",
    "\n",
    "If two statistics, $\\chi_1^2$ and $\\chi^2_2$, which follow the $\\chi^2$ distribution have been determined, the ratio of the reduced $\\chi^2$ is distributed according to the F distribution.\n",
    "\n",
    "I.e., \n",
    "\n",
    "$$f=\\frac{\\chi_1^2/\\nu_1}{\\chi^2_2/\\nu_2}$$\n",
    "\n",
    "where $\\nu_1$ and $\\nu_2$ are the respective degrees of freedom.  \n",
    "\n",
    "Note there is some ambiguity about which $\\chi^2$ should appear in the numerator.  \n",
    "\n",
    "So let's experiment with this, first by looking at 3 versus 2 and then 4 versus 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f_2_vs_3=(chi2_2/dof_2)/(chi2_3/dof_3)\n",
    "\n",
    "print 'chi2_3, dof_3, chi2_2, dof_2: ',chi2_3,dof_3,chi2_2,dof_2\n",
    "\n",
    "print 'The value of f: ', f_2_vs_3\n",
    "\n",
    "dist_f_2_vs_3=stats.f(8,9)\n",
    "print 'The probability of exceeding this value of f with chi2_2 dof versus chi2_3 dof: ',1.0-dist_f_2_vs_3.cdf(f_2_vs_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f_4_vs_3=(chi2_4/dof_4)/(chi2_3/dof_3)\n",
    "\n",
    "print 'chi2_4, dof_4, chi2_3, dof_3: ', chi2_4, dof_4, chi2_3, dof_3\n",
    "print 'The value of f: ', f_4_vs_3\n",
    "\n",
    "dist_f_4_vs_3=stats.f(7,8)\n",
    "print 'The probability of exceeding this value of f with chi2_3 dof versus chi2_4 dof: ',1.0-dist_f_4_vs_3.cdf(f_4_vs_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The F test is used to determine whether adding another parameter and therefore an extra degree of freedom is necessary.  \n",
    "\n",
    "The probability that such a large F could have been obtained by accident when we went from two fit parameters to three fit parameters is very small. This means that there is a dramatic improvement in fit (which can be seen in the plots, anyway).\n",
    "\n",
    "The probability that such a large F could have been obtained by accident when we went from three fit parameters to four fit parameters is rather large, approaching 1. This means that there is no significant improvement in fit.\n",
    "\n",
    "We will discuss these concepts more later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $\\chi^2$ minimum\n",
    "\n",
    "\n",
    "Now, the claim has been made that the best fit represents the lowest chi2 (or highest likelihood).  Just for fun, let's investigate this claim.  Specifically, let us return the variances in the fit coefficients, and use one of them to show that it is in fact a minimum in chi2.\n",
    "\n",
    "The result[1] gives the variance of the parameters along the diagonal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=3\n",
    "\n",
    "#Turn on the covariance.\n",
    "result=np.polyfit(x,ynew,degree,w=1/err,cov='True')\n",
    "\n",
    "#Arbitrarily grab the coefficient for the cubic\n",
    "std_coeff0=sqrt(result[1][0][0])\n",
    "\n",
    "#Create a vector with some variance around the value of the cubic coefficient.\n",
    "temp1=np.linspace(-2.0*std_coeff0,2*std_coeff0,101)\n",
    "print temp1.min(),temp1.max()\n",
    "\n",
    "print result[0]\n",
    "print result[1]\n",
    "print result[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#write a function that will compute some chi2s after varying one of the fit coefficients.  \n",
    "#This below varies the linear coefficient.\n",
    "\n",
    "def get_chi2s(x,ynew,err,result):\n",
    "    std_coeff=sqrt(result[1][3][3])\n",
    "    temp1=np.linspace(-std_coeff,std_coeff,101)\n",
    "    chi2_out=np.zeros(101)\n",
    "    plt.errorbar(x,ynew,yerr=err,fmt='o')\n",
    "    for i in arange(101):\n",
    "        ymodel=np.poly1d([result[0][0],result[0][1],result[0][2]+temp1[i],result[0][3]])\n",
    "        ymodelout=ymodel(x)\n",
    "        plt.plot(x,ymodelout)\n",
    "        chi2_out[i]=(((ynew-ymodelout)/err)**2).sum()\n",
    "    return temp1,chi2_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "temp1,chi2_out=get_chi2s(x,ynew,err,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(temp1,chi2_out,'.')\n",
    "print chi2_out.min()\n",
    "test_value=chi2_out.min()+1\n",
    "print test_value\n",
    "result=np.interp(test_value,chi2_out[temp1 > 0],temp1[temp1 > 0])\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This shows indeed that the $\\chi^2$ is minimum at the best fit.  \n",
    "\n",
    "You might be tempted to estimate the errors on this fit parameter by determining the values at which $\\chi^2$ increases by some value.  **Look before you leap!**  Such errors will be incorrect due to covariance, and we will explore that next.\n",
    "\n",
    "\n",
    "## Errors on fit parameters\n",
    "\n",
    "We now have the parameter values for the best fit cubic equation.  These might be something we want to report in a paper.  However, numbers are useless without uncertainties.  So what are the uncertainties on the parameters of the cubic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This question can also be tackled using $\\chi^2$ (within limits to be discussed below).  \n",
    "\n",
    "Consider that the parameter values at the $\\chi^2$ minimum are given by $\\mathbf{a}_{(0)}$, and the value of $\\chi^2$ for those parameters is $\\chi^2_{min}$.  If the parameters $\\mathbf{a}$ are perturbed away from  $\\mathbf{a}_{(0)}$, then $\\chi^2$ increases.  \n",
    "\n",
    "(It may help to think of the $\\chi^2$ surface as a multi-dimensional bowl.)\n",
    "\n",
    "We can define a confidence region by a certain amount $\\Delta\\chi^2$.  Generally speaking, that will be an $M$ dimensional region for $M$ fit parameters.  However, we can also do this for a more limited number of parameters (\"parameters of interest\").  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The information about how to do this is in the covariance matrix, but before we go into those details, we need to consider when this applies.   Generally speaking, these are most applicable when the errors on the data are normally distributed. If they are not normally distributed, $\\chi^2$ can still be used, but the interpretation (e.g., precise values of $\\Delta\\chi^2$ for different levels of confidence) may not hold.\n",
    "\n",
    "In that case, some precepts apply.\n",
    "  - $\\chi^2_{min}$ is distributed as a chi-square distribution with $N-M$ degress of freedom, where $N$ is the number of data points, and $M$ is the number of fit parameters.  (We have already covered this in the $\\chi^2$ minimization discussion above.  \n",
    "  \n",
    "  - Consider $\\mathbf{a}^S_{(j)}$ to be drawn from the universe of data sets with actual (i.e., physical) parameters $\\mathbf{a}_{(0)}$ then the probability distribution of $\\delta\\mathbf{a} = \\mathbf{a}^S_{(j)}- \\mathbf{a}_{(0)}$ is the multivariate normal distribution.\n",
    "  \n",
    "  $$P(\\delta\\mathbf{a}) da_0\\dots d a_{M-1} = const \\times \\exp(-\\frac{1}{2} \\delta\\mathbf{a}\\centerdot \\mathbf{\\alpha} \\centerdot \\delta\\mathbf{a}) da_0\\dots d a_{M-1}$$\n",
    "  where \n",
    "  $$\\mathbf{\\alpha} = \\frac{1}{2} \\frac{\\partial^2\\chi^2}{\\partial a_k \\partial a_l}$$\n",
    "  \n",
    "  shows the curvature of the $\\chi^2$ surfaces.  (This is related to the second partial derivative of the maximum likelihood, as you know, comes from the Taylor expansion of the maximum likelihood that is used to extract the covariance matrix.)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  \n",
    "  - If $\\mathbf{a}^S_{(j)}$ to be drawn from the universe of data sets with actual (i.e., physical) parameters $\\mathbf{a}_{(0)}$, then quantity $\\Delta \\chi^2 = \\chi^2(\\mathbf{a}_{(j)}) - \\chi^2(\\mathbf(a)_{(0)}$ is also distributed as a $\\chi^2$ distribution with $M$ degrees of freedom.  \n",
    "  \n",
    "It is the last precept that makes the connection betweeen particular values of $\\Delta \\chi^2$ and the fraction of the probability distriubtion enclosed, i.e., the confidence levels.\n",
    "\n",
    "  - Suppose that $\\mathbf{a}^S_{(j)}$ is drawn from the universe of simulated data, but this time, the first $\\nu$ components are held fixed, and the remaining $M-\\nu$ components are varied to minimize $\\chi^2$.  Call this new minimum $\\chi^2_\\nu$. Then $\\delta (\\chi^2_\\nu - \\chi^2_{min})$ is distributed as $\\chi^2$ with $\\nu$ degrees of freedom.  \n",
    "  \n",
    "What this gives you is the projected $\\Delta \\chi^2$ on to the $M-\\nu$ dimensional plane.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: Consider $\\nu=1$, where we want to find the confidence of a single parameter. (We call $\\nu$ the number of parameters of interest.)  Then this is distributed as $\\chi^2$ with one degree of freedom.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=1\n",
    "\n",
    "dist_chi_1=stats.chi2(degree)\n",
    "\n",
    "#print out the delta chi^2 for various confidence intervals\n",
    "\n",
    "conf_intervals=[0.6827, 0.90, 0.9545, 0.99, 0.9973, 0.9999]\n",
    "print 'For these confidence levels, and one parameter of interest ', conf_intervals\n",
    "print 'the delta chi2 should be ', dist_chi_1.ppf(conf_intervals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us use this to determine the uncertainty on one of the parameters from our cubic model to the data.  We will use the covariance matrix, which is the inverse of the partial derivatives matrix (the Hessian matrix). For the polyfit, the covariance matrix is in result[1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=3\n",
    "result=np.polyfit(x,ynew,degree,w=1/err,cov='True')\n",
    "print result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use $$\\Delta \\chi^2 = \\delta\\mathbf{a}\\centerdot \\mathbf{\\alpha} \\centerdot \\delta\\mathbf{a}$$ \n",
    "and solve for $\\delta\\mathbf{a}$.  \n",
    "\n",
    "We can write $\\delta\\mathbf{a}$ as:\n",
    "\n",
    "$$\\delta\\mathbf{a}=\\mathbf{\\alpha}^{-1} \\centerdot \\left(\\begin{align}c\\\\ 0\\\\ \\vdots \\\\ 0 \\end{align} \\right) $$\n",
    "$$= \\mathbf{C} \\centerdot   \\left( \\begin{align}c\\\\ 0\\\\ \\vdots \\\\ 0 \\end{align} \\right)$$\n",
    "\n",
    "where $c$ is the constant that will be adjusted to make the $\\Delta\\chi^2$ equal the desired value, and $C$ is the covariance matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining these two equations gives \n",
    "$$ c=\\frac{\\partial a_0}{C_{00}}$$ \n",
    "and therefore \n",
    "$$ \\Delta \\chi^2_\\nu = \\frac{(\\partial a_0)^2}{ C_{00}}$$\n",
    "so \n",
    "$$ \\delta a_o = \\pm \\sqrt{\\Delta \\chi^2_{\\nu}} \\sqrt{C_{00}}.$$\n",
    "\n",
    "So, what this means that, given the uncertainty level desired (e.g., $\\Delta \\chi^2 = 1$ for 68.7% for one parameter of interest), the uncertainty on the parameter is just the square root of the corresponding term of the covariance matrix times the square root of the corresponding $\\Delta \\chi^2_\\nu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if you are interested in more than one parameter?  Then you must compute the $\\Delta \\chi^2$ for the number of parameters of interest, e.g., for two parameters of interest:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "degree=2\n",
    "\n",
    "dist_chi_2=stats.chi2(degree)\n",
    "\n",
    "#print out the delta chi^2 for various confidence intervals\n",
    "\n",
    "conf_intervals=[0.6827, 0.90, 0.9545, 0.99, 0.9973, 0.9999]\n",
    "print 'For these confidence levels, and one parameter of interest ', conf_intervals\n",
    "print 'the delta chi2 should be ', dist_chi_2.ppf(conf_intervals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, the $\\Delta \\chi^2 = $ some value region will be a $\\nu$-dimensional ellipsoid, since if you have multiple parameters of interest, it means you want to determine the joint confidence region between said parameters.  So for $\\nu=2$, the $\\chi^2$ boundary will be an ellipse.  This makes sense due to the fact that we are assuming (by using the second partials to determine the covariance matrix) and therefore the $\\chi^2$ surface is a paraboloid.  \n",
    "\n",
    "So the general form of the equation that you will want to solve is:\n",
    "\n",
    "$$\\Delta = \\partial \\mathbf{a}^{\\prime} \\centerdot {\\mathbf{C}^{-1}_{proj}} \\centerdot \\partial \\mathbf{a} ^\\prime$$\n",
    "\n",
    "where $\\mathbf{a}^\\prime$ is the subset of the whole $\\mathbf{a}$ that form the parameters of interest (e.g., the constant plus linear coefficient in the example above), and $\\mathbf{C}_{proj}$ is the  $\\nu \\times \\nu$ relevant portion of the covariance matrix (e.g., the $[[C_{22}, C_{23}],[C_{32}, C_{33}]]$ portion of the full covariance matrix above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Homework - spectral fitting and $\\chi^2$\n",
    "\n",
    "The first problem of HW4 will involve fitting a couple of models to some simulated data, evaluating and comparing the $\\chi^2$, then constructing a couple of $\\chi^2$ contours for a couple of pairs of parameters of interest.\n",
    "\n",
    "For further reference for this part, see Numerical Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap Resampling\n",
    "\n",
    "The use of $\\chi^2$ makes some significant demands on the nature of the model, the data, and the uncertainties.  Specifically, as noted above, it assumes that at any point, data is distributed around the model as a Gaussian with width $\\sigma$ which is basically understood to be the data uncertainty.  \n",
    "\n",
    "These assumptions may not be valid, for example when the number of counts is small, then the distribution will be Poisson.  There may be other reasons why the uncertainties are not reliable.  In this case, a parameter may be estimated from the data, but what is the uncertainty on that parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's how the bootstrap works in practice:\n",
    " - Measure the parameter of interest from the data.\n",
    " - Draw a \"random sample with replacement\" from the sample, and measure the parameter of interest.\n",
    " - Do this a large number of times.  How large?  Large enough that there is no difference if you do this process more than once.\n",
    " - The distribution of the measured parameter of interest from the bootstrapped samples yields the uncertainty on the parameter of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This procedure seems a little sketchy, but it should work <i> if the boot strapped sample has the same distribution as the original sample</i>.  \n",
    "\n",
    "According to wikipedia, bootstrap is recommended under the following conditions:\n",
    " - \"When the theoretical distribution of a statistic of interest is complicated or unknown. Since the bootstrapping procedure is distribution-independent it provides an indirect method to assess the properties of the distribution underlying the sample and the parameters of interest that are derived from this distribution.\n",
    " - When the sample size is insufficient for straightforward statistical inference. If the underlying distribution is well-known, bootstrapping provides a way to account for the distortions caused by the specific sample that may not be fully representative of the population.\"\n",
    " \n",
    "Wikipedia also notes:\n",
    "  - \"if one performs a naive bootstrap on the sample mean when the underlying population lacks a finite variance (for example, a power law distribution), then the bootstrap distribution will not converge to the same limit as the sample mean. As a result, confidence intervals on the basis of a Monte Carlo simulation of the bootstrap could be misleading. Athreya states that \"Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap\".\"\n",
    "  \n",
    "Let's look at some examples of the bootstrap usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some simple examples follow. First, let's draw 1000 samples from a gamma function, chosen simply because it is asymmetric and non-Gaussian for low values if $k$.  Here, stats.gamma is a function of the shape parameter k, the location parameter, and the scale parameter $\\theta$. \n",
    "\n",
    "Physically, the idea is similar to measuring the length of a rod multiple times, but for whatever reason, the lengths are biased from a Gaussian distribution and are actually distributed according to a gamma distribution (for whatever reason).  Not very realistic, but that is the idea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "dist=stats.gamma(2,0,1)\n",
    "r=dist.rvs(1000)\n",
    "plt.hist(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can compute the mean, and the uncertainty on the mean as though it were Gaussian, as shown above, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print 'The mean is ',r.mean()\n",
    "print 'The uncertainty on the mean, if r were drawn from a Gaussian distribution ',r.std()/sqrt(len(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's compute the distribution of the means using the bootstrap.  This is implemented in np.random.choice.  If we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = len(r)\n",
    "reps = 10000\n",
    "xb = np.random.choice(r, (n, reps))\n",
    "print xb.shape\n",
    "\n",
    "mb=np.mean(xb,axis=0)\n",
    "print mb.shape\n",
    "plt.hist(mb)\n",
    "mb.sort()\n",
    "\n",
    "print 'The 68% bounds on the bootstrapped means is ',0.5*(np.percentile(mb, [68.0])-np.percentile(mb,[32.0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bootstrap gives a smaller estimate of the uncertainty in the mean than does the Gaussian approximation.  Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I would contend it does make sense.  It turns out that the mean estimator for the Gamma function is the shape times the scale (in this case, $2 \\times 1$).  So it makes sense that it should be tightly clustered around that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "One example of the application of the bootstrap is given here:\n",
    "https://ned.ipac.caltech.edu/level5/Sept01/Bhavsar/frames.html\n",
    "\n",
    "The goal of this paper is to derive slope of the angular two-point correlation function.  \n",
    " - They have a sample of galaxies whose position on the sky is known.\n",
    " - They measure the angular distance between each galaxy.\n",
    " - They bin these in angular distance, and fit the result with a power law (after correcting for the uniform distribution).  They find that that the slope of the power law is $0.77$.\n",
    " - The uncertainty on the number of galaxies in a bin is Poisson, and there may not be enough galaxies per bin for those to be approximately Gaussian.  So this falls into the category of errors that are not well understood.  Moreover, as noted in the article, what they really want is not the uncertainty on the data that they have, but on other putative data sets; if they observed in another region of the sky.\n",
    " - So, assuming that the observed sample is a fair sample, they can draw a large number of bootstrap samples (i.e., random samples with replacement) and perform the same power law fit.  The slopes from all these are then analyzed, to prodice a standard deviation on the slope of 0.13.\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Homework for bootstrap\n",
    "\n",
    "A 1-dimensional analog of the galaxy angular distribution is photon arrival times. \n",
    "\n",
    "You will be given two sets of photon arrival time data, created from lightcurves generated from two different power spectra.  \n",
    "\n",
    "You can compute the difference in time between all the points in the data set.\n",
    "\n",
    "You can then create a vector that spans the range of time differences obtained, and count the number of differences in each bin.  \n",
    "\n",
    "Finally, you fit the results (time differences vs number of differences in each bin, assuming Poission errors) with a power law to obtain a slope.\n",
    "\n",
    "What is the error on the slope?  That can be determined by doing bootstrap resampling on the photon arrival times (note that since you will count differences between all points, it won't matter if the bootstrapped data become out of order).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Often we want to know whether a given sample is consistent with some hypothesis.   \n",
    "\n",
    "For example, you measure a set of data, and it looks interesting.  It will be interesting,  you think, if it is not consistent with a Gaussian distribution.  So you ask the statistical question: is a set of data $\\{x_i\\}$ consistent with a Gaussian distribution $\\mathcal{N}(\\mu,\\sigma)$. In this example, $\\mathcal{N}(\\mu,\\sigma)$ is the _null hypothesis_, and we are trying to reject it.  \n",
    "\n",
    "Astronomical example in Ivezic: consider you are trying to detect a certain object in an image with substantial background.  Because the background fluctuates from pixel to pixel, the contribution of the object in a certain pixel must be larger than the background fluctuation in order to claim a detection.  \n",
    "\n",
    "It is assumed that you can compute the probabilty of a given outcome from the null hypothesis: for example, for a cumulative distribution function $H_0(x)$, the probability that you would get a value at least as large as $x_i$ is $p(x> x_i) = 1-H(x_i)$.  This is called the _p value_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Often a significance level $\\alpha$ is adopted (ahead of time) and the null hypothesis is rejected when $p \\le \\alpha$.  For example, if $\\alpha=0.05$ (i.e., 5%), and $p< 0.05$, the null hypothesis is rejected at a 0.05 significance level.\n",
    "\n",
    "Note that failing to reject a hypothesis is not the same as proving its correctness. \n",
    "\n",
    "Example: if we flip a coin 10 times and get 8 tails, should we reject the hypothesis that the coin is fair?  No, because the binomial distribution predicts that the probability of 8 or more tails is 0.054, and so we cannot reject the hypothesis at the 0.05 significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Use hypothesis testing with care.**\n",
    "\n",
    "There are many pitfalls that fall under the term of _p-hacking_, i.e., manipulating data until you get $p< 0.05$, by, e.g., leaving data out, or trying a variety of statistical tests until you get one where $p<0.05$.  \n",
    "\n",
    "Maybe this happens in soft sciences more than in physics and astronomy?  \n",
    "\n",
    "[This article](https://fivethirtyeight.com/features/science-isnt-broken/#part1) gives a nice discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More jargon associated with this:\n",
    "\n",
    " - Type I error (false positives): cases where the null hypothesis is true but incorrectly rejected\n",
    " - Type II error (false negatives): cases where the null hypthesis is false, but it is not rejected.  This is related to the power of the statistical test.\n",
    " \n",
    "There is a lot more to this than I have mentioned; see Ivezic and other sources.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Distributions\n",
    "\n",
    "Sometimes we would like to answer one of the following questions:\n",
    "\n",
    "- Are two samples drawn from the same distribution?  \n",
    "- Is a sample consistent with being drawn from a known distribution?\n",
    "- Do two sets of measurements imply a difference in the measured quantity?\n",
    "\n",
    "Ivezic notes that these are not necessarily easy questions to formulate. For example, a Gaussian distribution depends on both $\\mu$ and $\\sigma$ - it could be that we are interested in only whether the $\\mu$ are different, and we don't care about the $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Nonparametric Methods for Comparing Distributions\n",
    "\n",
    "A nonparametric method is used when the distribution is not known.  For example, the regular Pearson $r$ correlation coefficient implicitly assumes that both variables should be normally distributed.  If they are not, $r$ can still be computed, but the interpretation in terms of $p-$values is not valid.  In contrast, the Spearman rank correlates the ranks rather than the values, and is more robust in the face of non-normal distributions.\n",
    "\n",
    "One of the most popular nonparametric methos for comparing distributions is the Kolmorgorov-Smirnov (K-S) test, which compares the cumulative distributions of two samples.  \n",
    "\n",
    "It is based on the maximum distance of two cumulative distributions, as seen in the [Wikipedia article](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test#/media/File:KS_Example.png).  Interestingly, this distance has well-understood statistical properties.\n",
    "\n",
    "It is implemented in python using kstest, ks_2sample, and ksone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "Let's return to the question of whether a poission distribution is distinguished from a normal distribution for various values of count rates as in HW3.\n",
    "\n",
    "Remember the idea is to compare the Poisson distribution for input value of $\\lambda$ with the inferred Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_poisson_and_normal_samples(rate,numpnts):\n",
    "    s = np.random.poisson(rate, numpnts)\n",
    "    avg = np.average(s)\n",
    "    std = np.std(s)\n",
    "    t=np.random.normal(avg,std,numpnts)\n",
    "    return s,t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rate=1.0\n",
    "numpnts=100\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "s,t=get_poisson_and_normal_samples(rate,numpnts)\n",
    "\n",
    "bins=-4+np.arange(12)\n",
    "\n",
    "plt.hist(s,bins,histtype='step')\n",
    "plt.hist(t,bins,histtype='step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(s,bins,normed=1,histtype='step',cumulative=True)\n",
    "plt.hist(t,bins,normed=1,histtype='step',cumulative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, compute the ks_two for these two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stats.ks_2samp(s,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that the probability that these two are drawn from the same distribution is low.  Let's try with rate=20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rate=20.0\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "s,t=get_poisson_and_normal_samples(rate,numpnts)\n",
    "\n",
    "bins=5+np.arange(30)\n",
    "\n",
    "plt.hist(s,bins,histtype='step')\n",
    "plt.hist(t,bins,histtype='step')\n",
    "\n",
    "stats.ks_2samp(s,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(s,bins,normed=1,histtype='step',cumulative=True)\n",
    "plt.hist(t,bins,normed=1,histtype='step',cumulative=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The large p-value shows that the two distributions are indistinguishable.\n",
    "\n",
    "Note the dependence on number of draws; if you have a lot of data, you can distinguish between different distributions better than if you have only few data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  More on the KS Test\n",
    "\n",
    "Here we have compared two distributions (note that they don't have to have the same number of points, since it is the normalized cumulative distribution that is compared).  \n",
    "\n",
    "But you can also compare a set of data with a known distribution using kstest, which takes as arguments a distribution from scipy.stats, or can also a function.\n",
    "\n",
    "Regardess, because it is looking for the maximum distance in the cumulative distributions, it doesn't perform well when the differences are out in the tails.\n",
    "\n",
    "Note that KS test will be sensitive to location and scale differences, as well as shape differences, but not to reparameterization (e.g., the same result would be obtained for $\\ln x$ as for $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are other nonparameteric tests including \n",
    " - The Cramer-von Mises criterion\n",
    " - The Watson test\n",
    " - The Anderson-Darling test\n",
    " - The U test\n",
    " - The Wilcoxon test\n",
    " - The Shapiro-Wilk test\n",
    " \n",
    "There are also parametric tests, if the sample is consistent with a normal distribution\n",
    " - The t test\n",
    " - The F test (which we have already come across)\n",
    " \n",
    "See Ivezic and other sources for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
