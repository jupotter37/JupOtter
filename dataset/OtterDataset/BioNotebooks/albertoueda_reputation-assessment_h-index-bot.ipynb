{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import cookielib\n",
    "import sys\n",
    "import re\n",
    "import urllib\n",
    "from  urllib2 import *\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from ghost import Ghost \n",
    "from lxml import html\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Not used\n",
    "def authenticate():\n",
    "    email = raw_input(\"Google username: \")\n",
    "    password = getpass.getpass(\"Password: \")\n",
    "\n",
    "    LOGIN_URL = 'https://accounts.google.com/ServiceLogin?service=grandcentral'\n",
    "    AUTH_URL = 'https://accounts.google.com/ServiceLoginAuth?service=grandcentral'\n",
    "    opener = build_opener(HTTPCookieProcessor()) \n",
    "\n",
    "    # Find GALX value\n",
    "    login_page_contents = opener.open(LOGIN_URL).read()\n",
    "    galx = re.search(r\"name=\\\"GALX\\\"\\s+type=\\\"hidden\\\"\\s+value=\\\"(.+)\\\"\", login_page_contents, re.IGNORECASE).group(1)\n",
    "    # print galx \n",
    "\n",
    "    login_data = urllib.urlencode({\n",
    "        'Email': email,\n",
    "        'Passwd': password,\n",
    "        'GALX': galx,\n",
    "    })\n",
    "\n",
    "    opener.open(AUTH_URL, login_data)\n",
    "\n",
    "# Sleep time to avoid hard hitting Google Scholar\n",
    "def sleep_and_build_opener():\n",
    "    time.sleep(MIN_SLEEP + (random.random() * MAX_RANDOM_MINUTES))  \n",
    "    global opener # TODO maybe is not necessary build every time (must test against GS blocks)\n",
    "    opener = build_opener(HTTPCookieProcessor()) \n",
    "    \n",
    "def open_allowed_page():\n",
    "    time.sleep(MIN_SLEEP)\n",
    "    opener.open('http://scholar.google.com.br/citations?user=JMkfK0sAAAAJ&hl=pt-PT') # Berthier's page\n",
    "    \n",
    "def get_nrc_authors():\n",
    "    nrc_file = pd.read_json(\"../nrc/cs-authors-nrc.json\")\n",
    "    nrc_file = nrc_file[['name', 'group_name']]\n",
    "    groups = nrc_file['group_name'].copy().unique()\n",
    "    groups.sort()\n",
    "    return nrc_file\n",
    "\n",
    "def open_page(url, action):\n",
    "    page = None\n",
    "    try:\n",
    "        sleep_and_build_opener()\n",
    "        page = opener.open(url) \n",
    "        open_allowed_page()\n",
    "    except URLError as e:     \n",
    "        print e\n",
    "        print \"URL Error on: \" + action\n",
    "    except Exception as e:     \n",
    "        print e\n",
    "        print \"Generic Exception on: \" + action\n",
    "    if page != None and page.getcode() != 200:\n",
    "        print 'F**!!! Response: ' + str(page.getcode() + ' (' + action + ')')\n",
    "        return None\n",
    "\n",
    "    return page\n",
    "\n",
    "def extract_element(page, xpath):\n",
    "    tree = html.fromstring(page.read())\n",
    "    return tree.xpath(xpath)\n",
    "\n",
    "def extract_author_info(row, link, author_page):            \n",
    "    tree = html.fromstring(author_page.read())\n",
    "\n",
    "    sc_h_index = tree.xpath(\"(//td[@class='gsc_rsb_std'])[3]/text()\")[0]\n",
    "    sc_n_cits = tree.xpath(\"(//td[@class='gsc_rsb_std'])[1]/text()\")[0]\n",
    "    sc_n_cits_2010 = tree.xpath(\"(//td[@class='gsc_rsb_std'])[2]/text()\")[0]\n",
    "    sc_h_index_2010 = tree.xpath(\"(//td[@class='gsc_rsb_std'])[4]/text()\")[0]\n",
    "\n",
    "    found_author_scholar = tree.xpath(\"//div[@id='gsc_prf_in']/text()\")\n",
    "    found_author_scholar = found_author_scholar[0].encode('ascii', 'ignore')\n",
    "\n",
    "    row.extend((found_author_scholar, sc_h_index, sc_n_cits, sc_h_index_2010, sc_n_cits_2010, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0 authors (0.0 seg)\n",
      "Progress: 10 authors (154.9 seg)\n",
      "Authors Not Found in GSC: 2 of 10 (20.0%)\n",
      "\n",
      "--- Finished after 154.898 seconds. ---\n"
     ]
    }
   ],
   "source": [
    "N_AUTHORS = 10\n",
    "EXCLUDED_LASTNAMES = [\"FILHO\", \"NETO\", \"JUNIOR\", \"JR\"]\n",
    "MIN_SLEEP = 2\n",
    "MAX_RANDOM_MINUTES = 0\n",
    "AUTHOR_URL_XPATH = \"(//h3[@class='gsc_1usr_name']/a)[1]/@href\"\n",
    "\n",
    "start_time = time.time()\n",
    "opener = build_opener(HTTPCookieProcessor()) \n",
    "\n",
    "with open('../h-index/h-index-bot-input.tsv','rb') as input_tsv_file, open('../h-index/h-index-bot-output.tsv', 'wb') as output_tsv:\n",
    "        \n",
    "    input_tsv = csv.reader(input_tsv_file, delimiter='\\t')\n",
    "    output_tsv = csv.writer(output_tsv, delimiter='\\t')\n",
    "    rows = []\n",
    "    i = 0\n",
    "    k = -1\n",
    "    not_found = 0\n",
    "    h_index_not_found = 0\n",
    "    \n",
    "    for row in input_tsv:\n",
    "        if k % 10 == 0: print (\"Progress: \" + str(k) + \" authors (%.1f seg)\" % (time.time() - start_time))    \n",
    "        \n",
    "        # header\n",
    "        if (k == -1):\n",
    "            # header = ('MAS_Found_Author', 'MAS_Id', 'MAS_NPubs', 'MAS_NCits', 'H-index', 'Scholar_Found_Author')\n",
    "            header = ('GS_Name', 'GS_H5', 'GS_Cits', 'GS_H5_2010', 'GS_Cits_2010', 'GS_Link')\n",
    "            row.extend(header)\n",
    "            rows.append(row)\n",
    "            k = 0\n",
    "            continue\n",
    "\n",
    "        k += 1 \n",
    "        if (k > N_AUTHORS): break\n",
    "\n",
    "        # Full and Short Names\n",
    "        full_name = unidecode(row[0].decode('utf-8'))\n",
    "        names = full_name.split()\n",
    "\n",
    "        lastname = names[len(names)-1]        \n",
    "        if (lastname.upper() in EXCLUDED_LASTNAMES and len(names) > 2): lastname = names[len(names)-2]\n",
    "        short_name = names[0] + ' ' + lastname\n",
    "        # print \"\\n\" + short_name\n",
    "        \n",
    "        # Specific Known Cases\n",
    "        if full_name == 'Wagner Meira Junior':\n",
    "            full_name = short_name = 'Wagner Meira'\n",
    "        elif full_name == 'EDMUNDO ALBUQUERQUE DE SOUZA E SILVA':\n",
    "            full_name = short_name = 'EDMUNDO SOUZA SILVA'\n",
    "        \n",
    "        # University\n",
    "        university = unidecode(row[4].decode('utf-8'))\n",
    "        \n",
    "        # 1st try: Fullname search\n",
    "        fullname_search_url = 'http://scholar.google.com.br/citations?view_op=search_authors&mauthors=' + urllib.quote(full_name)\n",
    "        search_page = open_page(fullname_search_url, 'full name search')\n",
    "        if search_page is None: break\n",
    "\n",
    "        author_id = extract_element(search_page, AUTHOR_URL_XPATH)\n",
    "\n",
    "        # 2nd try: Shortname + University search\n",
    "        if not author_id: \n",
    "            shortname_search_url = 'http://scholar.google.com.br/citations?view_op=search_authors&mauthors=' + urllib.quote(short_name + ' ' + university)\n",
    "            search_page = open_page(shortname_search_url, '(Short name + university) search')\n",
    "            if search_page is None: break\n",
    "                \n",
    "            author_id = extract_element(search_page, AUTHOR_URL_XPATH)\n",
    "        \n",
    "        # Giving up, jump to next author        \n",
    "        if not author_id: \n",
    "            h_index_not_found += 1\n",
    "        else: \n",
    "            author_id = author_id[0]\n",
    "            author_url = \"https://scholar.google.com.br\" + author_id\n",
    "            author_page = open_page(author_url, 'opening author profile')\n",
    "            if author_page is None: break\n",
    "            \n",
    "            extract_author_info(row, author_id, author_page)\n",
    "     \n",
    "        # Add row with author info or an empty row if it was not found\n",
    "        rows.append(row)\n",
    "    \n",
    "    # end for \n",
    "    output_tsv.writerows(rows)\n",
    "\n",
    "if (k != 1):\n",
    "    print(\"Authors Not Found in GSC: %d of %d (%.1f%%)\" % (h_index_not_found, k-1, (h_index_not_found * 100.00) / (k-1)))\n",
    "print(\"\\n--- Finished after %.3f seconds. ---\" % (time.time() - start_time))\n",
    "\n",
    "del opener\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAS robot (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "        try:\n",
    "            page = requests.get('http://academic.research.microsoft.com/Search?query=author:(' + full_name + ')')\n",
    "        except:     \n",
    "            print \"[1] Erro de conexão, prosseguindo...\"\n",
    "            continue\n",
    "\n",
    "        tree = html.fromstring(page.text)\n",
    "\n",
    "        # Finding author\n",
    "        # At least three matched names\n",
    "        link = tree.xpath(\"(//a[@class='author-name-tooltip']//b[3])[1]/../@href\")\n",
    "\n",
    "        if not link: \n",
    "            try:\n",
    "                page = requests.get('http://academic.research.microsoft.com/Search?query=author:(' + short_name + ')')\n",
    "            except:     \n",
    "                print \"[2] Erro de conexão, prosseguindo...\"\n",
    "                continue\n",
    "            \n",
    "            tree = html.fromstring(page.text)\n",
    "            link = tree.xpath(\"(//a[@class='author-name-tooltip']//b[2])[1]/../@href\")\n",
    "\n",
    "        if not link: \n",
    "            link = tree.xpath(\"(//a[@class='author-name-tooltip']//b[1])[1]/../@href\")\n",
    "\n",
    "        if not link: \n",
    "            not_found += 1\n",
    "            # rows.append(row)\n",
    "            # output_tsv.writerow(row)\n",
    "            # continue;\n",
    "        else:\n",
    "            link = link[0]\n",
    "\n",
    "            # mas_id\n",
    "            url_parts = link.split('/')\n",
    "            mas_id = url_parts[len(url_parts)-2]\n",
    "            found_author = url_parts[len(url_parts)-1]\n",
    "\n",
    "            # cit and pub\n",
    "            try:\n",
    "                page = requests.get(link)\n",
    "            except:     \n",
    "                print \"[3] Erro de conexão, prosseguindo...\"\n",
    "                continue\n",
    "                \n",
    "            tree = html.fromstring(page.text)\n",
    "\n",
    "            n_pub = tree.xpath('//div[@class=\"author-card\"]//a[@id=\"ctl00_MainContent_AuthorItem_publication\"]/span/text()');\n",
    "            if not n_pub: n_pub = 0\n",
    "            else: n_pub = n_pub[0]         \n",
    "\n",
    "            n_cit = tree.xpath('//div[@class=\"author-card\"]//a[@id=\"ctl00_MainContent_AuthorItem_citedBy\"]/span/text()');    \n",
    "            if not n_cit: n_cit = 0\n",
    "            else: n_cit = n_cit[0]\n",
    "\n",
    "            row.extend((found_author, mas_id, n_pub, n_cit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'<<< artificial reference group >>>', u'Arizona State University',\n",
       "       u'Auburn University', u'Boston University', u'Brandeis University',\n",
       "       u'Brigham Young University', u'Brown University',\n",
       "       u'California Institute of Technology',\n",
       "       u'Carnegie Mellon University', u'Case Western Reserve University',\n",
       "       u'City University of New York Grad. Center', u'Clemson University',\n",
       "       u'College of William and Mary',\n",
       "       u'Columbia University In The City of New York',\n",
       "       u'Cornell University', u'Dartmouth College', u'Duke University',\n",
       "       u'ETH Zurich', u'Florida Institute of Technology',\n",
       "       u'Florida International University', u'Florida State University',\n",
       "       u'George Washington University', u'Georgia Institute of Technology',\n",
       "       u'Georgia State University', u'Harvard University',\n",
       "       u'Illinois Institute of Technology', u'Imperial College London',\n",
       "       u'Indiana University at Bloomington', u'Iowa State University',\n",
       "       u'Johns Hopkins University', u'Kansas State University',\n",
       "       u'Kent State University Main Campus', u'Lehigh University',\n",
       "       u'Louisiana State University and Agricultural and Mechanical College',\n",
       "       u'Massachusetts Institute of Technology',\n",
       "       u'Michigan State University', u'Mississippi State University',\n",
       "       u'New Jersey Institute of Technology',\n",
       "       u'New Mexico State University Main Campus', u'New York University',\n",
       "       u'North Carolina State University',\n",
       "       u'North Dakota State University Main Campus',\n",
       "       u'Northeastern University', u'Northwestern University',\n",
       "       u'Ohio State University Main Campus',\n",
       "       u'Oklahoma State University Main Campus',\n",
       "       u'Old Dominion University', u'Oregon Health and Science University',\n",
       "       u'Oregon State University', u'Penn State University',\n",
       "       u'Pontif\\xedcia Universidade Cat\\xf3lica do Paran\\xe1',\n",
       "       u'Pontif\\xedcia Universidade Cat\\xf3lica do Rio Grande do Sul',\n",
       "       u'Pontif\\xedcia Universidade Cat\\xf3lica do Rio de Janeiro',\n",
       "       u'Princeton University', u'Purdue University Main Campus',\n",
       "       u'Rensselaer Polytechnic Institute', u'Rice University',\n",
       "       u'Rutgers The State University of New Jersey New Brunswick Campus',\n",
       "       u'Southern Methodist University', u'Stanford University',\n",
       "       u'State University of New York at Albany',\n",
       "       u'State University of New York at Binghamton',\n",
       "       u'State University of New York at Buffalo',\n",
       "       u'State University of New York at Stony Brook',\n",
       "       u'Syracuse University Main Campus', u'Temple University',\n",
       "       u'Texas A & M University', u'Texas Tech University',\n",
       "       u'Top 30 authors in databases (MSA)',\n",
       "       u'Top 30 authors in information retrieval (MSA)',\n",
       "       u'Top 30 authors in networks & communications (MSA)',\n",
       "       u'Top authors in algorithms & theory',\n",
       "       u'Top authors in artificial intelligence',\n",
       "       u'Top authors in bioinformatics & computational biology',\n",
       "       u'Top authors in computer education',\n",
       "       u'Top authors in computer vision', u'Top authors in data mining',\n",
       "       u'Top authors in databases',\n",
       "       u'Top authors in distributed & parallel computing',\n",
       "       u'Top authors in graphics',\n",
       "       u'Top authors in hardware & architecture',\n",
       "       u'Top authors in human-computer interaction',\n",
       "       u'Top authors in information retrieval',\n",
       "       u'Top authors in machine learning & pattern recognition',\n",
       "       u'Top authors in multimedia',\n",
       "       u'Top authors in natural language & speech',\n",
       "       u'Top authors in networks & communications',\n",
       "       u'Top authors in networks & communications (revised)',\n",
       "       u'Top authors in operating systems',\n",
       "       u'Top authors in programming languages',\n",
       "       u'Top authors in real-time & embedded systems',\n",
       "       u'Top authors in scientific computing',\n",
       "       u'Top authors in security & privacy', u'Top authors in simulation',\n",
       "       u'Top authors in software engineering',\n",
       "       u'Top authors in world wide web', u'Tufts University',\n",
       "       u'USP - S\\xe3o Carlos', u'Universidade Estadual de Campinas',\n",
       "       u'Universidade Federal Fluminense',\n",
       "       u'Universidade Federal da Bahia',\n",
       "       u'Universidade Federal de Campina Grande',\n",
       "       u'Universidade Federal de Mato Grosso do Sul',\n",
       "       u'Universidade Federal de Minas Gerais',\n",
       "       u'Universidade Federal de Pernambuco',\n",
       "       u'Universidade Federal de Santa Catarina',\n",
       "       u'Universidade Federal de S\\xe3o Carlos',\n",
       "       u'Universidade Federal de Uberl\\xe2ndia',\n",
       "       u'Universidade Federal do Amazonas',\n",
       "       u'Universidade Federal do Cear\\xe1',\n",
       "       u'Universidade Federal do Esp\\xedrito Santo',\n",
       "       u'Universidade Federal do Paran\\xe1',\n",
       "       u'Universidade Federal do Rio Grande de Norte',\n",
       "       u'Universidade Federal do Rio Grande do Sul',\n",
       "       u'Universidade Federal do Rio de Janeiro',\n",
       "       u'Universidade de Bras\\xedlia', u'Universidade de Fortaleza',\n",
       "       u'Universidade de S\\xe3o Paulo',\n",
       "       u'Universidade do Vale do Rio dos Sinos', u'University of Alabama',\n",
       "       u'University of Alabama In Huntsville',\n",
       "       u'University of Alabama at Birmingham', u'University of Arizona',\n",
       "       u'University of Arkansas Main Campus',\n",
       "       u'University of California-Berkeley',\n",
       "       u'University of California-Davis',\n",
       "       u'University of California-Irvine',\n",
       "       u'University of California-Los Angeles',\n",
       "       u'University of California-Riverside',\n",
       "       u'University of California-San Diego',\n",
       "       u'University of California-Santa Barbara',\n",
       "       u'University of California-Santa Cruz', u'University of Cambridge',\n",
       "       u'University of Central Florida', u'University of Chicago',\n",
       "       u'University of Cincinnati Main Campus',\n",
       "       u'University of Colorado at Boulder', u'University of Connecticut',\n",
       "       u'University of Delaware', u'University of Florida',\n",
       "       u'University of Georgia', u'University of Houston',\n",
       "       u'University of Illinois at Chicago',\n",
       "       u'University of Illinois at Urbana-Champaign',\n",
       "       u'University of Iowa', u'University of Kansas',\n",
       "       u'University of Kentucky', u'University of Louisiana at Lafayette',\n",
       "       u'University of Maryland Baltimore County',\n",
       "       u'University of Maryland College Park',\n",
       "       u'University of Massachusetts Amherst', u'University of Memphis',\n",
       "       u'University of Michigan-Ann Arbor',\n",
       "       u'University of Minnesota-Twin Cities',\n",
       "       u'University of Nebraska - Lincoln',\n",
       "       u'University of New Mexico Main Campus',\n",
       "       u'University of North Carolina at Chapel Hill',\n",
       "       u'University of North Carolina at Charlotte',\n",
       "       u'University of North Texas',\n",
       "       u'University of Oklahoma Norman Campus', u'University of Oregon',\n",
       "       u'University of Oxford', u'University of Pennsylvania',\n",
       "       u'University of Pittsburgh Pittsburgh Campus',\n",
       "       u'University of Rochester',\n",
       "       u'University of South Carolina Columbia',\n",
       "       u'University of South Florida',\n",
       "       u'University of Southern California',\n",
       "       u'University of Southern Mississippi', u'University of Tennessee',\n",
       "       u'University of Texas at Austin', u'University of Texas at Dallas',\n",
       "       u'University of Utah', u'University of Virginia',\n",
       "       u'University of Washington', u'University of Wisconsin-Madison',\n",
       "       u'Vanderbilt University',\n",
       "       u'Virginia Polytechnic Institute and State University',\n",
       "       u'Washington State University',\n",
       "       u'Washington University In St. Louis', u'Wayne State University',\n",
       "       u'Western Michigan University',\n",
       "       u'Wright State University Main Campus', u'Yale University',\n",
       "       u'\\xc9cole Polytechnique ParisTech'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# %timeit x = 'a'\n",
    "\n",
    "# for x in nrc_file['name'].tolist():\n",
    "    # print x\n",
    "# cnpq_authors = cnpq_authors.ix[:, ['Name', 'ShortName', 'CNPqLevel']]\n",
    "# fullnames_list = [unidecode(x.decode(UTF8)).upper() for x in cnpq_authors['Name'].tolist()]\n",
    "# shortnames_list = [unidecode(x.decode(UTF8)).upper() for x in cnpq_authors['ShortName'].tolist()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
