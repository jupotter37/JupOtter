{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyDarkCovidNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7KgYlQRXuJH"
      },
      "source": [
        "This notebook is written by Brittaney Everitt 10211957 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sypZtCoSpvAQ",
        "outputId": "cb9b4de0-42cd-4567-a81a-3563fbb48f5c"
      },
      "source": [
        "#mount my google drive files in the notebook \n",
        "#I do this so that I can load the images in the next cell \n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZP7DjQqd1_"
      },
      "source": [
        "# Darknet Model\n",
        "\n",
        "I reimplemented the DarkCovidNet model using keras / tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsabhAanqMFd"
      },
      "source": [
        "Load in all of the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888uN0qKurre",
        "outputId": "c73d6d87-def9-4d7b-9915-2617a6a0e9ee"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#load in the labels\n",
        "allLabels = pd.read_csv('drive/My Drive/All_labels_CSV.csv') #stores the label column in a 2D data frame, with axis labelled 'label'\n",
        "print(allLabels)\n",
        "\n",
        "y = allLabels['Label'].to_numpy()\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Name  Label\n",
            "0         covid      0\n",
            "1         covid      0\n",
            "2         covid      0\n",
            "3         covid      0\n",
            "4         covid      0\n",
            "...         ...    ...\n",
            "1120  pneumonia      2\n",
            "1121  pneumonia      2\n",
            "1122  pneumonia      2\n",
            "1123  pneumonia      2\n",
            "1124  pneumonia      2\n",
            "\n",
            "[1125 rows x 2 columns]\n",
            "[0 0 0 ... 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pizOLlarqWBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a5bc8b-be34-4c07-fa14-c4e5e64a4a70"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#this function loads in all of the x-ray image data from a folder and stores them in a numpy array \n",
        "def load_data(folder):\n",
        "    images = [] #instantiate an empty array to store all image data \n",
        "\n",
        "    count = 0\n",
        "    #loop through every image in the folder/ directory \n",
        "    for file in os.listdir(folder):\n",
        "\n",
        "      #open the current image file and convert it to an array \n",
        "      image = Image.open( \n",
        "        os.path.join(folder, file) #concatentaes folder path name and individual image file name, returns a string which represents the concatenated path of current image file in the loop\n",
        "      ).convert('LA').resize((256, 256)) #converts the image to grayscale and resizes it 256 pixels by 256 pixels (width,height)\n",
        "\n",
        "      arr = np.array(image) #converts the Image object to a NumPy array, the values of each pixel are stored in the array \n",
        "      images.append(arr) #append array of image pixels to array of all images\n",
        "\n",
        "      #print out the number of images loaded in to make sure all images are loaded into the array \n",
        "      print(count)\n",
        "      count+=1\n",
        "\n",
        "    #return a numpy array of all of the images\n",
        "    return np.array(images)\n",
        "\n",
        "allImages = load_data('drive/My Drive/AllImages') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRCDAMJU7Q4_",
        "outputId": "1ea84004-1041-451a-c1ef-7489f3a84656"
      },
      "source": [
        "#Normalize the data\n",
        "import tensorflow as tf\n",
        "\n",
        "allImages = tf.keras.utils.normalize(allImages)\n",
        "\n",
        "print(allImages)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   ...\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]]\n",
            "\n",
            "  [[0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   ...\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]]\n",
            "\n",
            "  [[0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   ...\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.00392154 0.99999231]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.49171966 0.87075357]\n",
            "   [0.52428373 0.85154364]\n",
            "   [0.52910129 0.84855867]\n",
            "   ...\n",
            "   [0.24700371 0.96901454]\n",
            "   [0.21814596 0.97591615]\n",
            "   [0.17003633 0.9854378 ]]\n",
            "\n",
            "  [[0.49430222 0.86929012]\n",
            "   [0.53149107 0.84706389]\n",
            "   [0.53623279 0.84407014]\n",
            "   ...\n",
            "   [0.23625434 0.97169125]\n",
            "   [0.21083787 0.97752104]\n",
            "   [0.16627992 0.98607859]]\n",
            "\n",
            "  [[0.45970712 0.88807058]\n",
            "   [0.49687177 0.86782397]\n",
            "   [0.4994283  0.86635522]\n",
            "   ...\n",
            "   [0.20717044 0.97830486]\n",
            "   [0.1775268  0.98411597]\n",
            "   [0.13979028 0.99018113]]]\n",
            "\n",
            "\n",
            " [[[0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   ...\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]]\n",
            "\n",
            "  [[0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   ...\n",
            "   [0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]]\n",
            "\n",
            "  [[0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]\n",
            "   ...\n",
            "   [0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.44298967 0.89652672]\n",
            "   [0.48651539 0.87367201]\n",
            "   [0.5194153  0.85452194]\n",
            "   ...\n",
            "   [0.61329709 0.78985231]\n",
            "   [0.61329709 0.78985231]\n",
            "   [0.57881561 0.81545845]]\n",
            "\n",
            "  [[0.44861497 0.89372513]\n",
            "   [0.4994283  0.86635522]\n",
            "   [0.52910129 0.84855867]\n",
            "   ...\n",
            "   [0.61713994 0.78685342]\n",
            "   [0.61522399 0.78835236]\n",
            "   [0.58093621 0.81394909]]\n",
            "\n",
            "  [[0.41407187 0.91024419]\n",
            "   [0.46517404 0.88521925]\n",
            "   [0.49171966 0.87075357]\n",
            "   ...\n",
            "   [0.5766832  0.81696786]\n",
            "   [0.57453893 0.81847725]\n",
            "   [0.54325128 0.83957016]]]\n",
            "\n",
            "\n",
            " [[[0.31957014 0.94756262]\n",
            "   [0.33937251 0.94065206]\n",
            "   [0.33937251 0.94065206]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.34263035 0.9394703 ]\n",
            "   [0.35873377 0.93343992]\n",
            "   [0.37139068 0.92847669]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.38384388 0.92339801]\n",
            "   [0.38074981 0.9246781 ]\n",
            "   [0.38692507 0.92211116]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.66490908 0.7469243 ]\n",
            "   [0.66653846 0.74547065]\n",
            "   [0.66327016 0.74838005]\n",
            "   ...\n",
            "   [0.62469505 0.78086881]\n",
            "   [0.62093912 0.78385879]\n",
            "   [0.62469505 0.78086881]]\n",
            "\n",
            "  [[0.67136981 0.74112251]\n",
            "   [0.66327016 0.74838005]\n",
            "   [0.66653846 0.74547065]\n",
            "   ...\n",
            "   [0.65829559 0.75275953]\n",
            "   [0.65996346 0.75129769]\n",
            "   [0.65323326 0.75715673]]\n",
            "\n",
            "  [[0.65323326 0.75715673]\n",
            "   [0.64808178 0.76157075]\n",
            "   [0.64808178 0.76157075]\n",
            "   ...\n",
            "   [0.66490908 0.7469243 ]\n",
            "   [0.65996346 0.75129769]\n",
            "   [0.65829559 0.75275953]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.0548194  0.99849629]\n",
            "   [0.05872202 0.99827437]\n",
            "   [0.0548194  0.99849629]\n",
            "   ...\n",
            "   [0.09370354 0.99560014]\n",
            "   [0.09757142 0.99522853]\n",
            "   [0.09757142 0.99522853]]\n",
            "\n",
            "  [[0.0548194  0.99849629]\n",
            "   [0.05872202 0.99827437]\n",
            "   [0.05872202 0.99827437]\n",
            "   ...\n",
            "   [0.09370354 0.99560014]\n",
            "   [0.10143489 0.99484218]\n",
            "   [0.09757142 0.99522853]]\n",
            "\n",
            "  [[0.0548194  0.99849629]\n",
            "   [0.05872202 0.99827437]\n",
            "   [0.05872202 0.99827437]\n",
            "   ...\n",
            "   [0.08983141 0.99595699]\n",
            "   [0.09370354 0.99560014]\n",
            "   [0.08983141 0.99595699]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.04309718 0.99907089]\n",
            "   [0.04309718 0.99907089]\n",
            "   [0.04309718 0.99907089]\n",
            "   ...\n",
            "   [0.0548194  0.99849629]\n",
            "   [0.0548194  0.99849629]\n",
            "   [0.05091427 0.99870303]]\n",
            "\n",
            "  [[0.04309718 0.99907089]\n",
            "   [0.04309718 0.99907089]\n",
            "   [0.04309718 0.99907089]\n",
            "   ...\n",
            "   [0.0548194  0.99849629]\n",
            "   [0.05091427 0.99870303]\n",
            "   [0.05091427 0.99870303]]\n",
            "\n",
            "  [[0.03918557 0.99923195]\n",
            "   [0.03918557 0.99923195]\n",
            "   [0.03918557 0.99923195]\n",
            "   ...\n",
            "   [0.0470068  0.99889457]\n",
            "   [0.0470068  0.99889457]\n",
            "   [0.0470068  0.99889457]]]\n",
            "\n",
            "\n",
            " [[[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.13216372 0.9912279 ]\n",
            "   [0.15496777 0.98791953]\n",
            "   [0.08983141 0.99595699]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.1775268  0.98411597]\n",
            "   [0.1775268  0.98411597]\n",
            "   [0.10143489 0.99484218]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.14359434 0.98963663]\n",
            "   [0.23984733 0.97081062]\n",
            "   [0.19980944 0.97983477]\n",
            "   ...\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]]\n",
            "\n",
            "  [[0.13216372 0.9912279 ]\n",
            "   [0.22903933 0.97341717]\n",
            "   [0.19241446 0.98131375]\n",
            "   ...\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]]\n",
            "\n",
            "  [[0.11299711 0.99359532]\n",
            "   [0.19611614 0.98058068]\n",
            "   [0.17003633 0.9854378 ]\n",
            "   ...\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]]]\n",
            "\n",
            "\n",
            " [[[0.42288547 0.90618314]\n",
            "   [0.43158067 0.90207435]\n",
            "   [0.48389365 0.87512681]\n",
            "   ...\n",
            "   [0.03527216 0.99937774]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.25056688 0.96809929]]\n",
            "\n",
            "  [[0.35232976 0.93587592]\n",
            "   [0.37764289 0.92595132]\n",
            "   [0.39912051 0.91689848]\n",
            "   ...\n",
            "   [0.02744064 0.99962343]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.24700371 0.96901454]]\n",
            "\n",
            "  [[0.34263035 0.9394703 ]\n",
            "   [0.34263035 0.9394703 ]\n",
            "   [0.33610241 0.94182544]\n",
            "   ...\n",
            "   [0.0235229  0.9997233 ]\n",
            "   [0.00392154 0.99999231]\n",
            "   [0.24343049 0.96991835]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.69863934 0.71547402]\n",
            "   [0.69719882 0.71687782]\n",
            "   [0.69863934 0.71547402]\n",
            "   ...\n",
            "   [0.64634457 0.76304567]\n",
            "   [0.63929414 0.76896229]\n",
            "   [0.65493054 0.75568908]]\n",
            "\n",
            "  [[0.69863934 0.71547402]\n",
            "   [0.69719882 0.71687782]\n",
            "   [0.69719882 0.71687782]\n",
            "   ...\n",
            "   [0.64808178 0.76157075]\n",
            "   [0.64283978 0.76600067]\n",
            "   [0.65829559 0.75275953]]\n",
            "\n",
            "  [[0.69863934 0.71547402]\n",
            "   [0.69863934 0.71547402]\n",
            "   [0.69719882 0.71687782]\n",
            "   ...\n",
            "   [0.65152607 0.75862625]\n",
            "   [0.64634457 0.76304567]\n",
            "   [0.65996346 0.75129769]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM1z3uIpqTa1"
      },
      "source": [
        "Split the code into training and testing sets  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMWcBsZ1D4ix",
        "outputId": "07deb9ce-38b9-47af-e9b7-f18578dfda1d"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into training and validation set: 80% training and 20% validation like in the paper\n",
        "X_train, X_test, y_train, y_test = train_test_split(allImages,y, test_size=0.2, random_state=27, stratify=y)\n",
        "\n",
        "#visualize the data to ensure distributions of each class are the same \n",
        "print(np.unique(y, return_counts=True))\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "print(np.unique(y_test, return_counts=True))\n",
        "print(X_train)\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([0, 1, 2]), array([125, 500, 500]))\n",
            "(array([0, 1, 2]), array([100, 400, 400]))\n",
            "(array([0, 1, 2]), array([ 25, 100, 100]))\n",
            "[[[[0.5524351  0.83355592]\n",
            "   [0.58722762 0.80942185]\n",
            "   [0.58722762 0.80942185]\n",
            "   ...\n",
            "   [0.59545356 0.80338973]\n",
            "   [0.59545356 0.80338973]\n",
            "   [0.56142206 0.82752962]]\n",
            "\n",
            "  [[0.59341436 0.80489714]\n",
            "   [0.62655691 0.77937567]\n",
            "   [0.62655691 0.77937567]\n",
            "   ...\n",
            "   [0.61522399 0.78835236]\n",
            "   [0.61135917 0.79135325]\n",
            "   [0.57881561 0.81545845]]\n",
            "\n",
            "  [[0.59545356 0.80338973]\n",
            "   [0.62840811 0.77788383]\n",
            "   [0.62655691 0.77937567]\n",
            "   ...\n",
            "   [0.59949768 0.80037649]\n",
            "   [0.59136366 0.806405  ]\n",
            "   [0.5547002  0.83205029]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.35873377 0.93343992]\n",
            "   [0.39304872 0.91951765]\n",
            "   [0.38074981 0.9246781 ]\n",
            "   ...\n",
            "   [0.17378533 0.98478356]\n",
            "   [0.1775268  0.98411597]\n",
            "   [0.1775268  0.98411597]]\n",
            "\n",
            "  [[0.42869544 0.90344907]\n",
            "   [0.45418745 0.89090615]\n",
            "   [0.44861497 0.89372513]\n",
            "   ...\n",
            "   [0.20717044 0.97830486]\n",
            "   [0.21449642 0.97672477]\n",
            "   [0.21083787 0.97752104]]\n",
            "\n",
            "  [[0.44580892 0.89512815]\n",
            "   [0.46788772 0.88378792]\n",
            "   [0.46517404 0.88521925]\n",
            "   ...\n",
            "   [0.21814596 0.97591615]\n",
            "   [0.22541754 0.97426225]\n",
            "   [0.21449642 0.97672477]]]\n",
            "\n",
            "\n",
            " [[[0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]\n",
            "   [0.01960407 0.99980782]\n",
            "   ...\n",
            "   [0.13598002 0.99071158]\n",
            "   [0.45140781 0.89231776]\n",
            "   [0.46244717 0.88664684]]\n",
            "\n",
            "  [[0.0235229  0.9997233 ]\n",
            "   [0.0235229  0.9997233 ]\n",
            "   [0.0235229  0.9997233 ]\n",
            "   ...\n",
            "   [0.12451362 0.9922179 ]\n",
            "   [0.47058824 0.88235294]\n",
            "   [0.49687177 0.86782397]]\n",
            "\n",
            "  [[0.0235229  0.9997233 ]\n",
            "   [0.0235229  0.9997233 ]\n",
            "   [0.01960407 0.99980782]\n",
            "   ...\n",
            "   [0.09370354 0.99560014]\n",
            "   [0.44861497 0.89372513]\n",
            "   [0.49171966 0.87075357]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.53149107 0.84706389]\n",
            "   [0.56363833 0.82602169]\n",
            "   [0.56142206 0.82752962]\n",
            "   ...\n",
            "   [0.66976877 0.74256972]\n",
            "   [0.67136981 0.74112251]\n",
            "   [0.64283978 0.76600067]]\n",
            "\n",
            "  [[0.53623279 0.84407014]\n",
            "   [0.5680346  0.82300467]\n",
            "   [0.5658425  0.82451335]\n",
            "   ...\n",
            "   [0.6729615  0.73967751]\n",
            "   [0.6745439  0.73823474]\n",
            "   [0.64808178 0.76157075]]\n",
            "\n",
            "  [[0.4994283  0.86635522]\n",
            "   [0.52910129 0.84855867]\n",
            "   [0.52910129 0.84855867]\n",
            "   ...\n",
            "   [0.63570725 0.77193024]\n",
            "   [0.63929414 0.76896229]\n",
            "   [0.6094102  0.7928551 ]]]\n",
            "\n",
            "\n",
            " [[[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01568434 0.99987699]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.03918557 0.99923195]\n",
            "   [0.05872202 0.99827437]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.01176389 0.9999308 ]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.02744064 0.99962343]\n",
            "   [0.04309718 0.99907089]\n",
            "   [0.06262195 0.99803732]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.03135712 0.99950824]\n",
            "   [0.0470068  0.99889457]\n",
            "   [0.06651901 0.99778516]]\n",
            "\n",
            "  [[0.01176389 0.9999308 ]\n",
            "   [0.01568434 0.99987699]\n",
            "   [0.01176389 0.9999308 ]\n",
            "   ...\n",
            "   [0.03527216 0.99937774]\n",
            "   [0.05091427 0.99870303]\n",
            "   [0.07041303 0.99751792]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.65829559 0.75275953]\n",
            "   [0.69574981 0.71828421]\n",
            "   [0.69282609 0.72110471]\n",
            "   ...\n",
            "   [0.69574981 0.71828421]\n",
            "   [0.69863934 0.71547402]\n",
            "   [0.66162164 0.74983786]]\n",
            "\n",
            "  [[0.66162164 0.74983786]\n",
            "   [0.69863934 0.71547402]\n",
            "   [0.69429225 0.71969318]\n",
            "   ...\n",
            "   [0.69863934 0.71547402]\n",
            "   [0.70149509 0.71267429]\n",
            "   [0.66490908 0.7469243 ]]\n",
            "\n",
            "  [[0.63024869 0.77639332]\n",
            "   [0.66815834 0.74401911]\n",
            "   [0.66653846 0.74547065]\n",
            "   ...\n",
            "   [0.66815834 0.74401911]\n",
            "   [0.67136981 0.74112251]\n",
            "   [0.63207871 0.77490419]]]\n",
            "\n",
            "\n",
            " [[[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.54325128 0.83957016]\n",
            "   [0.54786795 0.83656483]\n",
            "   [0.5194153  0.85452194]]\n",
            "\n",
            "  [[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.59136366 0.806405  ]\n",
            "   [0.59341436 0.80489714]\n",
            "   [0.56142206 0.82752962]]\n",
            "\n",
            "  [[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.59341436 0.80489714]\n",
            "   [0.59341436 0.80489714]\n",
            "   [0.56363833 0.82602169]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.57021468 0.82149572]\n",
            "   [0.5658425  0.82451335]\n",
            "   [0.5547002  0.83205029]]\n",
            "\n",
            "  [[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.57453893 0.81847725]\n",
            "   [0.57453893 0.81847725]\n",
            "   [0.56363833 0.82602169]]\n",
            "\n",
            "  [[0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   [0.         1.        ]\n",
            "   ...\n",
            "   [0.53386823 0.84556769]\n",
            "   [0.5385848  0.84257131]\n",
            "   [0.52669885 0.85005195]]]\n",
            "\n",
            "\n",
            " [[[0.5658425  0.82451335]\n",
            "   [0.59341436 0.80489714]\n",
            "   [0.58722762 0.80942185]\n",
            "   ...\n",
            "   [0.12068014 0.99269144]\n",
            "   [0.12068014 0.99269144]\n",
            "   [0.1091479  0.99402552]]\n",
            "\n",
            "  [[0.60349641 0.79736572]\n",
            "   [0.63024869 0.77639332]\n",
            "   [0.62469505 0.78086881]\n",
            "   ...\n",
            "   [0.14739204 0.98907815]\n",
            "   [0.14739204 0.98907815]\n",
            "   [0.13598002 0.99071158]]\n",
            "\n",
            "  [[0.6015027  0.79887077]\n",
            "   [0.62655691 0.77937567]\n",
            "   [0.62093912 0.78385879]\n",
            "   ...\n",
            "   [0.17378533 0.98478356]\n",
            "   [0.17003633 0.9854378 ]\n",
            "   [0.15874549 0.98731954]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.1091479  0.99402552]\n",
            "   [0.07041303 0.99751792]\n",
            "   [0.08207509 0.99662615]\n",
            "   ...\n",
            "   [0.61522399 0.78835236]\n",
            "   [0.61904496 0.78535555]\n",
            "   [0.5680346  0.82300467]]\n",
            "\n",
            "  [[0.09757142 0.99522853]\n",
            "   [0.07041303 0.99751792]\n",
            "   [0.08207509 0.99662615]\n",
            "   ...\n",
            "   [0.62093912 0.78385879]\n",
            "   [0.62093912 0.78385879]\n",
            "   [0.54556582 0.83806798]]\n",
            "\n",
            "  [[0.07819125 0.99693838]\n",
            "   [0.06651901 0.99778516]\n",
            "   [0.07430383 0.99723565]\n",
            "   ...\n",
            "   [0.58304505 0.81243982]\n",
            "   [0.58514217 0.81093072]\n",
            "   [0.4994283  0.86635522]]]]\n",
            "[2 1 1 1 1 2 2 2 2 1 2 1 2 2 0 2 1 2 1 2 1 0 2 2 2 2 1 2 1 2 2 2 1 1 2 1 1\n",
            " 1 2 2 2 0 1 2 1 2 2 1 2 0 2 1 1 1 0 2 2 1 2 1 2 0 1 1 0 2 2 2 2 0 2 2 1 1\n",
            " 1 2 1 1 2 1 1 2 2 2 1 1 1 1 1 2 1 1 2 2 2 1 1 2 1 2 2 1 0 0 2 2 0 0 1 1 2\n",
            " 2 1 2 1 2 2 1 1 2 1 2 2 1 1 2 1 2 1 2 0 1 1 0 2 1 2 1 1 1 0 1 2 1 1 2 1 1\n",
            " 2 2 1 1 1 1 1 1 1 1 1 1 1 0 1 2 0 0 2 1 2 1 1 2 2 2 0 1 1 1 2 2 1 2 1 1 2\n",
            " 1 1 2 2 2 0 0 2 2 2 1 1 2 1 2 2 2 2 2 2 0 2 2 1 0 2 1 1 2 1 2 2 1 2 2 1 0\n",
            " 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLHAekMLricu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8b4e9e-9bd6-4493-9ec1-663a3d8df765"
      },
      "source": [
        "#one_hot encode labels \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#one hot encode the training labels so that it can be used in the weight update calculation\n",
        "encodedLabels = to_categorical(y_train)\n",
        "\n",
        "print(encodedLabels)\n",
        "\n",
        "#One hot encode the testing labels\n",
        "encodedLabelsVal = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCQPseBQqaTi"
      },
      "source": [
        "Model \n",
        "\n",
        "- This model is a modified version of DarkCovideNet code from the author's Github at: https://github.com/muhammedtalo/COVID-19\n",
        "\n",
        "- Ozturk T, Talo M, Yildirim EA, Baloglu UB, Yildirim O, Acharya UR. Automated detection of COVID-19 cases using deep neural networks with X-ray images. Computers in biology and medicine. 2020 Jun 1;121:103792.\n",
        "\n",
        "- The authors use PyTorch and fastai. This code uses Keras and Tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwZEGMChzSE4"
      },
      "source": [
        "#create the padding size rule \n",
        "for_pad = lambda s: s if s > 2 else 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4S44_IrakXd"
      },
      "source": [
        "#this function defines one DarkCovidNet block \n",
        "def conv_block(input_layer,output_size,kernel_size=3):\n",
        "  p1 = tf.keras.layers.ZeroPadding2D(padding=(for_pad(kernel_size)-1)//2)(input_layer)\n",
        "  c1 = tf.keras.layers.Conv2D(output_size, kernel_size, strides = (1,1), padding='valid', use_bias = False)(p1)\n",
        "  b1 = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=0.00001)(c1)\n",
        "  a1 = tf.keras.layers.LeakyReLU(alpha = 0.1)(b1)\n",
        "  return a1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlsXeq87b2yz"
      },
      "source": [
        "#this function defines one triple block from the DarkCovidNet model \n",
        "def triple_block(input_layer,output_size):\n",
        "  c1 = conv_block(input_layer,output_size)\n",
        "  c2 = conv_block(c1,output_size/2,kernel_size=1)\n",
        "  c3 = conv_block(c2,output_size)\n",
        "  return c3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-dFd1raqbZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75392f7f-0256-4a2a-95a7-fe448a431e93"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, Input \n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "#The layers are arranged in \"sequential\" order\n",
        "def build():\n",
        "    #the shape of the expected input is the 256x256 image, with two channels  \n",
        "    #the channel 2 is there potentially because some images have a bit-depth of more than 8 \n",
        "    img_in = Input(shape=(256, 256, 2)) #instantiates a Keras tensor, the input node \n",
        "    \n",
        "    #first convolution block \n",
        "    c1 = conv_block(img_in,8)\n",
        "    #first max pool \n",
        "    p1 = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=2)(c1)\n",
        "\n",
        "    #second conv block \n",
        "    c2 = conv_block(p1,16)\n",
        "\n",
        "    #second max pool \n",
        "    p2 = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=2)(c2)\n",
        "\n",
        "    #third conv\n",
        "    c3 = triple_block(p2,32)\n",
        "\n",
        "    #third max pool \n",
        "    p3 = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=2)(c3)\n",
        "\n",
        "    #fourth conv\n",
        "    c4 = triple_block(p3,64)\n",
        "\n",
        "    #fourth max pool \n",
        "    p4 = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=2)(c4)\n",
        "\n",
        "    #fifth conv\n",
        "    c5 = triple_block(p4,128)\n",
        "\n",
        "    #fifth max pool \n",
        "    p5 = tf.keras.layers.MaxPool2D(pool_size = (2,2), strides=2)(c5)\n",
        "\n",
        "    #sixth conv\n",
        "    c6 = triple_block(p5,256)\n",
        "\n",
        "    #seventh conv\n",
        "    c7 = conv_block(c6,128,kernel_size=1)\n",
        "    #eighth conv\n",
        "    c8 = conv_block(c7,256)\n",
        "    #ninth conv\n",
        "    c9 = tf.keras.layers.Conv2D(3, kernel_size=3, strides = (1,1), padding = 'same',activation = 'relu')(c8)\n",
        "\n",
        "    #final batch normalization\n",
        "    batch = tf.keras.layers.BatchNormalization(momentum=0.1,epsilon=0.00001)(c9)\n",
        "\n",
        "    #Flattened layer \n",
        "    flattened = Flatten()(batch) \n",
        "    \n",
        "    #final fully connected layer\n",
        "    output = Dense(3,activation='softmax')(flattened)\n",
        "\n",
        "    #create Model using input tensor and output layer \n",
        "    model = tf.keras.Model(inputs=img_in, outputs=output) \n",
        "    return model\n",
        "\n",
        "\n",
        "model = build() #build the fully connected neural network \n",
        "#compile the model \n",
        "model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.003), #adam optimizer\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(), #categorical cross entropy loss \n",
        "        metrics=['CategoricalAccuracy','AUC'] \n",
        "        )\n",
        "\n",
        "#displays a summary of the neural network model \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 256, 256, 2)]     0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 258, 258, 2)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 256, 256, 8)       144       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256, 256, 8)       32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 256, 256, 8)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 128, 128, 8)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 130, 130, 8)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 16)      1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128, 128, 16)      64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 16)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 16)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPaddin (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64, 64, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPaddin (None, 66, 66, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 66, 66, 16)        512       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 66, 66, 16)        64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 66, 66, 16)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPaddin (None, 68, 68, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 66, 66, 32)        4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 66, 66, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 66, 66, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 33, 33, 32)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPaddin (None, 35, 35, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 33, 33, 64)        18432     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 33, 33, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 33, 33, 64)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPaddin (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 35, 35, 32)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 35, 35, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 35, 35, 32)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 35, 35, 64)        18432     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 35, 35, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_8 (ZeroPaddin (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 17, 17, 128)       73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 17, 17, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_9 (ZeroPaddin (None, 19, 19, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 19, 19, 64)        8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 19, 19, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPaddi (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 19, 19, 128)       73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 19, 19, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 19, 19, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 9, 9, 128)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 9, 9, 256)         294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 9, 9, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 11, 11, 128)       32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPaddi (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 256)       294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPaddi (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 13, 13, 128)       32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 13, 13, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_15 (ZeroPaddi (None, 15, 15, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 13, 13, 256)       294912    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 13, 13, 3)         6915      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 13, 13, 3)         12        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 507)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 1524      \n",
            "=================================================================\n",
            "Total params: 1,170,739\n",
            "Trainable params: 1,167,517\n",
            "Non-trainable params: 3,222\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeKZ8l_cqx0p"
      },
      "source": [
        "Run the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRp9hyhVtqqO"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Biwp0S3qy58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481a5430-29b7-4720-c63d-2a50bb2ecb0f"
      },
      "source": [
        "#Train the model on the training data\n",
        "history = model.fit(x = X_train, #training data\n",
        "                    y = encodedLabels, #label \n",
        "                    verbose = 1,\n",
        "                    batch_size = 32,\n",
        "                    epochs=100,\n",
        "                    validation_split = 0.2\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 38s 149ms/step - loss: 1.6200 - categorical_accuracy: 0.4546 - auc: 0.6258 - val_loss: 1.0191 - val_categorical_accuracy: 0.4333 - val_auc: 0.6587\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 2s 70ms/step - loss: 0.9463 - categorical_accuracy: 0.4655 - auc: 0.6929 - val_loss: 1.0362 - val_categorical_accuracy: 0.4444 - val_auc: 0.6872\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 2s 70ms/step - loss: 0.9568 - categorical_accuracy: 0.4963 - auc: 0.6966 - val_loss: 1.0703 - val_categorical_accuracy: 0.4278 - val_auc: 0.6652\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9877 - categorical_accuracy: 0.4562 - auc: 0.6848 - val_loss: 0.9872 - val_categorical_accuracy: 0.4722 - val_auc: 0.6891\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 2s 70ms/step - loss: 0.9634 - categorical_accuracy: 0.4872 - auc: 0.7024 - val_loss: 0.9796 - val_categorical_accuracy: 0.5222 - val_auc: 0.7082\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9927 - categorical_accuracy: 0.4568 - auc: 0.6853 - val_loss: 0.9846 - val_categorical_accuracy: 0.4667 - val_auc: 0.6953\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9327 - categorical_accuracy: 0.5055 - auc: 0.7354 - val_loss: 0.9740 - val_categorical_accuracy: 0.5222 - val_auc: 0.7077\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9697 - categorical_accuracy: 0.4969 - auc: 0.7028 - val_loss: 0.9633 - val_categorical_accuracy: 0.5500 - val_auc: 0.7194\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9663 - categorical_accuracy: 0.4972 - auc: 0.7061 - val_loss: 1.0916 - val_categorical_accuracy: 0.4778 - val_auc: 0.6696\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9914 - categorical_accuracy: 0.4983 - auc: 0.6981 - val_loss: 0.9789 - val_categorical_accuracy: 0.5222 - val_auc: 0.7121\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9536 - categorical_accuracy: 0.5278 - auc: 0.7219 - val_loss: 1.1186 - val_categorical_accuracy: 0.4611 - val_auc: 0.6735\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.9386 - categorical_accuracy: 0.5455 - auc: 0.7341 - val_loss: 0.9662 - val_categorical_accuracy: 0.5611 - val_auc: 0.7167\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9252 - categorical_accuracy: 0.5223 - auc: 0.7283 - val_loss: 0.9978 - val_categorical_accuracy: 0.5111 - val_auc: 0.7059\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9350 - categorical_accuracy: 0.5242 - auc: 0.7342 - val_loss: 1.0332 - val_categorical_accuracy: 0.5222 - val_auc: 0.7094\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9178 - categorical_accuracy: 0.5506 - auc: 0.7494 - val_loss: 1.0277 - val_categorical_accuracy: 0.5111 - val_auc: 0.7010\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8830 - categorical_accuracy: 0.5596 - auc: 0.7656 - val_loss: 0.9932 - val_categorical_accuracy: 0.4667 - val_auc: 0.6896\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9025 - categorical_accuracy: 0.5715 - auc: 0.7610 - val_loss: 1.1545 - val_categorical_accuracy: 0.4556 - val_auc: 0.6773\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9136 - categorical_accuracy: 0.5632 - auc: 0.7501 - val_loss: 0.9636 - val_categorical_accuracy: 0.5222 - val_auc: 0.7165\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.8938 - categorical_accuracy: 0.5743 - auc: 0.7632 - val_loss: 1.1268 - val_categorical_accuracy: 0.4889 - val_auc: 0.6912\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9292 - categorical_accuracy: 0.5418 - auc: 0.7459 - val_loss: 0.9735 - val_categorical_accuracy: 0.5000 - val_auc: 0.7126\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 2s 71ms/step - loss: 0.8969 - categorical_accuracy: 0.5540 - auc: 0.7556 - val_loss: 1.0024 - val_categorical_accuracy: 0.5278 - val_auc: 0.7064\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.8884 - categorical_accuracy: 0.5691 - auc: 0.7645 - val_loss: 1.0360 - val_categorical_accuracy: 0.4833 - val_auc: 0.6786\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.9056 - categorical_accuracy: 0.5098 - auc: 0.7413 - val_loss: 1.0276 - val_categorical_accuracy: 0.4389 - val_auc: 0.6760\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.9092 - categorical_accuracy: 0.5319 - auc: 0.7461 - val_loss: 1.0282 - val_categorical_accuracy: 0.5111 - val_auc: 0.7033\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8692 - categorical_accuracy: 0.6035 - auc: 0.7781 - val_loss: 1.0559 - val_categorical_accuracy: 0.5111 - val_auc: 0.6912\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8640 - categorical_accuracy: 0.5878 - auc: 0.7825 - val_loss: 1.1453 - val_categorical_accuracy: 0.4833 - val_auc: 0.6717\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8478 - categorical_accuracy: 0.5946 - auc: 0.7950 - val_loss: 1.0813 - val_categorical_accuracy: 0.4778 - val_auc: 0.6823\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8237 - categorical_accuracy: 0.6188 - auc: 0.7998 - val_loss: 1.2552 - val_categorical_accuracy: 0.5056 - val_auc: 0.6964\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8407 - categorical_accuracy: 0.5895 - auc: 0.7838 - val_loss: 1.1132 - val_categorical_accuracy: 0.4778 - val_auc: 0.6683\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8286 - categorical_accuracy: 0.6210 - auc: 0.7964 - val_loss: 1.0037 - val_categorical_accuracy: 0.5444 - val_auc: 0.7073\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8858 - categorical_accuracy: 0.5677 - auc: 0.7603 - val_loss: 0.9920 - val_categorical_accuracy: 0.4778 - val_auc: 0.7005\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.7876 - categorical_accuracy: 0.6659 - auc: 0.8240 - val_loss: 1.0068 - val_categorical_accuracy: 0.4833 - val_auc: 0.6910\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8017 - categorical_accuracy: 0.6402 - auc: 0.8122 - val_loss: 1.0962 - val_categorical_accuracy: 0.5278 - val_auc: 0.7008\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.7878 - categorical_accuracy: 0.6282 - auc: 0.8209 - val_loss: 1.0599 - val_categorical_accuracy: 0.4389 - val_auc: 0.6454\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.8132 - categorical_accuracy: 0.5973 - auc: 0.8062 - val_loss: 1.2373 - val_categorical_accuracy: 0.4389 - val_auc: 0.6442\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 2s 72ms/step - loss: 0.7847 - categorical_accuracy: 0.6397 - auc: 0.8236 - val_loss: 1.5686 - val_categorical_accuracy: 0.5111 - val_auc: 0.6719\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.7278 - categorical_accuracy: 0.6829 - auc: 0.8489 - val_loss: 1.2299 - val_categorical_accuracy: 0.4889 - val_auc: 0.6545\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.7131 - categorical_accuracy: 0.6766 - auc: 0.8558 - val_loss: 1.2553 - val_categorical_accuracy: 0.5167 - val_auc: 0.6808\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.7147 - categorical_accuracy: 0.6681 - auc: 0.8564 - val_loss: 1.1919 - val_categorical_accuracy: 0.4444 - val_auc: 0.6462\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.6077 - categorical_accuracy: 0.7259 - auc: 0.8987 - val_loss: 1.2928 - val_categorical_accuracy: 0.4444 - val_auc: 0.6685\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.6087 - categorical_accuracy: 0.7199 - auc: 0.8929 - val_loss: 1.5721 - val_categorical_accuracy: 0.4722 - val_auc: 0.6547\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.7216 - categorical_accuracy: 0.6626 - auc: 0.8467 - val_loss: 1.2859 - val_categorical_accuracy: 0.4778 - val_auc: 0.6702\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.5724 - categorical_accuracy: 0.7230 - auc: 0.9059 - val_loss: 1.4445 - val_categorical_accuracy: 0.4889 - val_auc: 0.6577\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.5684 - categorical_accuracy: 0.7552 - auc: 0.9098 - val_loss: 1.4224 - val_categorical_accuracy: 0.4833 - val_auc: 0.6713\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.5276 - categorical_accuracy: 0.7620 - auc: 0.9215 - val_loss: 1.8384 - val_categorical_accuracy: 0.5111 - val_auc: 0.6492\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.5144 - categorical_accuracy: 0.7745 - auc: 0.9249 - val_loss: 1.4393 - val_categorical_accuracy: 0.5000 - val_auc: 0.6603\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.4627 - categorical_accuracy: 0.8001 - auc: 0.9417 - val_loss: 1.5564 - val_categorical_accuracy: 0.4778 - val_auc: 0.6672\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.4742 - categorical_accuracy: 0.7928 - auc: 0.9359 - val_loss: 1.6054 - val_categorical_accuracy: 0.4278 - val_auc: 0.6366\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.4622 - categorical_accuracy: 0.7961 - auc: 0.9406 - val_loss: 1.6721 - val_categorical_accuracy: 0.5444 - val_auc: 0.6790\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.4124 - categorical_accuracy: 0.8070 - auc: 0.9525 - val_loss: 1.6761 - val_categorical_accuracy: 0.4444 - val_auc: 0.6437\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.2882 - categorical_accuracy: 0.8956 - auc: 0.9789 - val_loss: 2.1296 - val_categorical_accuracy: 0.4167 - val_auc: 0.6022\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.3107 - categorical_accuracy: 0.8826 - auc: 0.9729 - val_loss: 2.3478 - val_categorical_accuracy: 0.4556 - val_auc: 0.6171\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.2513 - categorical_accuracy: 0.8906 - auc: 0.9828 - val_loss: 1.8075 - val_categorical_accuracy: 0.4722 - val_auc: 0.6600\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.2365 - categorical_accuracy: 0.9084 - auc: 0.9850 - val_loss: 2.3922 - val_categorical_accuracy: 0.4778 - val_auc: 0.6621\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.1555 - categorical_accuracy: 0.9365 - auc: 0.9934 - val_loss: 2.1613 - val_categorical_accuracy: 0.4389 - val_auc: 0.6322\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.1501 - categorical_accuracy: 0.9551 - auc: 0.9941 - val_loss: 2.9595 - val_categorical_accuracy: 0.4722 - val_auc: 0.6302\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.2132 - categorical_accuracy: 0.9137 - auc: 0.9862 - val_loss: 2.5286 - val_categorical_accuracy: 0.4278 - val_auc: 0.6028\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.2850 - categorical_accuracy: 0.8861 - auc: 0.9761 - val_loss: 3.1248 - val_categorical_accuracy: 0.4944 - val_auc: 0.6333\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 2s 76ms/step - loss: 0.4294 - categorical_accuracy: 0.8296 - auc: 0.9516 - val_loss: 1.9771 - val_categorical_accuracy: 0.4556 - val_auc: 0.6206\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 2s 76ms/step - loss: 0.2978 - categorical_accuracy: 0.9076 - auc: 0.9731 - val_loss: 2.6137 - val_categorical_accuracy: 0.4111 - val_auc: 0.6034\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.1589 - categorical_accuracy: 0.9435 - auc: 0.9925 - val_loss: 2.3492 - val_categorical_accuracy: 0.5000 - val_auc: 0.6600\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.1604 - categorical_accuracy: 0.9443 - auc: 0.9916 - val_loss: 2.6787 - val_categorical_accuracy: 0.4833 - val_auc: 0.6481\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.1273 - categorical_accuracy: 0.9625 - auc: 0.9961 - val_loss: 2.5496 - val_categorical_accuracy: 0.3889 - val_auc: 0.6122\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0827 - categorical_accuracy: 0.9667 - auc: 0.9985 - val_loss: 3.1887 - val_categorical_accuracy: 0.4111 - val_auc: 0.5985\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0785 - categorical_accuracy: 0.9819 - auc: 0.9978 - val_loss: 3.1014 - val_categorical_accuracy: 0.4111 - val_auc: 0.6010\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.0557 - categorical_accuracy: 0.9892 - auc: 0.9964 - val_loss: 3.1836 - val_categorical_accuracy: 0.4556 - val_auc: 0.6228\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0447 - categorical_accuracy: 0.9860 - auc: 0.9994 - val_loss: 3.2358 - val_categorical_accuracy: 0.4333 - val_auc: 0.6146\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0462 - categorical_accuracy: 0.9844 - auc: 0.9995 - val_loss: 2.9860 - val_categorical_accuracy: 0.4111 - val_auc: 0.5837\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0529 - categorical_accuracy: 0.9876 - auc: 0.9987 - val_loss: 3.3151 - val_categorical_accuracy: 0.3944 - val_auc: 0.5902\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0431 - categorical_accuracy: 0.9880 - auc: 0.9994 - val_loss: 3.3555 - val_categorical_accuracy: 0.4444 - val_auc: 0.6105\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0239 - categorical_accuracy: 0.9949 - auc: 0.9999 - val_loss: 3.3979 - val_categorical_accuracy: 0.4556 - val_auc: 0.6204\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0549 - categorical_accuracy: 0.9795 - auc: 0.9989 - val_loss: 3.4220 - val_categorical_accuracy: 0.4611 - val_auc: 0.6181\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0645 - categorical_accuracy: 0.9796 - auc: 0.9985 - val_loss: 3.7895 - val_categorical_accuracy: 0.4278 - val_auc: 0.5848\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.1868 - categorical_accuracy: 0.9304 - auc: 0.9910 - val_loss: 3.9402 - val_categorical_accuracy: 0.4722 - val_auc: 0.6290\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.3128 - categorical_accuracy: 0.8933 - auc: 0.9728 - val_loss: 2.3549 - val_categorical_accuracy: 0.4333 - val_auc: 0.6127\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.1552 - categorical_accuracy: 0.9374 - auc: 0.9929 - val_loss: 2.7307 - val_categorical_accuracy: 0.3722 - val_auc: 0.5813\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0873 - categorical_accuracy: 0.9654 - auc: 0.9981 - val_loss: 2.6506 - val_categorical_accuracy: 0.4222 - val_auc: 0.6081\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.0273 - categorical_accuracy: 0.9953 - auc: 0.9999 - val_loss: 3.2797 - val_categorical_accuracy: 0.4500 - val_auc: 0.6271\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0178 - categorical_accuracy: 0.9936 - auc: 0.9999 - val_loss: 3.6198 - val_categorical_accuracy: 0.4444 - val_auc: 0.6151\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0116 - categorical_accuracy: 0.9986 - auc: 1.0000 - val_loss: 4.0778 - val_categorical_accuracy: 0.4500 - val_auc: 0.6183\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0103 - categorical_accuracy: 0.9970 - auc: 1.0000 - val_loss: 3.8823 - val_categorical_accuracy: 0.4333 - val_auc: 0.6305\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.0052 - categorical_accuracy: 0.9977 - auc: 1.0000 - val_loss: 3.7387 - val_categorical_accuracy: 0.4722 - val_auc: 0.6353\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0110 - categorical_accuracy: 0.9932 - auc: 1.0000 - val_loss: 3.6241 - val_categorical_accuracy: 0.4333 - val_auc: 0.6165\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0093 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 3.2901 - val_categorical_accuracy: 0.4833 - val_auc: 0.6305\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0061 - categorical_accuracy: 0.9974 - auc: 1.0000 - val_loss: 3.8117 - val_categorical_accuracy: 0.4333 - val_auc: 0.6202\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0048 - categorical_accuracy: 0.9981 - auc: 1.0000 - val_loss: 3.8933 - val_categorical_accuracy: 0.4222 - val_auc: 0.6128\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0044 - categorical_accuracy: 0.9992 - auc: 1.0000 - val_loss: 3.8336 - val_categorical_accuracy: 0.4167 - val_auc: 0.6043\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0021 - categorical_accuracy: 0.9989 - auc: 1.0000 - val_loss: 3.5642 - val_categorical_accuracy: 0.4167 - val_auc: 0.6049\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0030 - categorical_accuracy: 0.9977 - auc: 1.0000 - val_loss: 3.7612 - val_categorical_accuracy: 0.4167 - val_auc: 0.6065\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.0019 - categorical_accuracy: 0.9998 - auc: 1.0000 - val_loss: 3.6539 - val_categorical_accuracy: 0.4222 - val_auc: 0.6213\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0037 - categorical_accuracy: 0.9988 - auc: 1.0000 - val_loss: 3.8006 - val_categorical_accuracy: 0.4722 - val_auc: 0.6281\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0068 - categorical_accuracy: 0.9981 - auc: 1.0000 - val_loss: 3.9134 - val_categorical_accuracy: 0.4222 - val_auc: 0.5951\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0032 - categorical_accuracy: 0.9976 - auc: 1.0000 - val_loss: 3.8092 - val_categorical_accuracy: 0.4278 - val_auc: 0.6021\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0020 - categorical_accuracy: 0.9996 - auc: 1.0000 - val_loss: 4.2129 - val_categorical_accuracy: 0.4056 - val_auc: 0.6077\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0035 - categorical_accuracy: 0.9991 - auc: 1.0000 - val_loss: 3.9742 - val_categorical_accuracy: 0.4722 - val_auc: 0.6295\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0036 - categorical_accuracy: 0.9966 - auc: 1.0000 - val_loss: 4.0318 - val_categorical_accuracy: 0.4389 - val_auc: 0.6146\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0027 - categorical_accuracy: 0.9997 - auc: 1.0000 - val_loss: 3.7592 - val_categorical_accuracy: 0.4389 - val_auc: 0.6180\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0034 - categorical_accuracy: 0.9992 - auc: 1.0000 - val_loss: 4.0430 - val_categorical_accuracy: 0.4111 - val_auc: 0.6061\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.0030 - categorical_accuracy: 0.9973 - auc: 1.0000 - val_loss: 4.0265 - val_categorical_accuracy: 0.3944 - val_auc: 0.5880\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 2s 74ms/step - loss: 0.0090 - categorical_accuracy: 0.9966 - auc: 1.0000 - val_loss: 4.0926 - val_categorical_accuracy: 0.4444 - val_auc: 0.6119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8wLh4cGrqlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e76c762-2164-4904-aace-9692a428bd63"
      },
      "source": [
        "#Display the results of the training data\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "#apply the trained neural network to the training data\n",
        "y_predTrain = model.predict(X_train)\n",
        "\n",
        "#get the predicted labels of the data\n",
        "y_predLabelTrain = np.argmax(y_predTrain,axis = 1) \n",
        "\n",
        "#display accuracy, f1-score, precision and recall\n",
        "print(classification_report(y_train,y_predLabelTrain))\n",
        "print(\"Overall precision\",round(precision_score(y_train,y_predLabelTrain, average='micro'),2))\n",
        "print(\"Overall recall\",round(recall_score(y_train,y_predLabelTrain, average='micro'),2))\n",
        "print(\" \")\n",
        "\n",
        "#print the confusion matrix\n",
        "print(confusion_matrix(y_train,y_predLabelTrain))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.78      0.83       100\n",
            "           1       0.79      0.93      0.85       400\n",
            "           2       0.92      0.78      0.84       400\n",
            "\n",
            "    accuracy                           0.85       900\n",
            "   macro avg       0.87      0.83      0.84       900\n",
            "weighted avg       0.86      0.85      0.85       900\n",
            "\n",
            "Overall precision 0.85\n",
            "Overall recall 0.85\n",
            " \n",
            "[[ 78  19   3]\n",
            " [  3 372  25]\n",
            " [  6  82 312]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ZSnhWCwKBkuG",
        "outputId": "6ed7dc00-2a00-49a9-d8a2-168c6ed0b455"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#display the loss vs epochs \n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv-Mb1PjtrtP"
      },
      "source": [
        "Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlmbbeadrwmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17e8244-15ef-4987-fd49-691335814c6e"
      },
      "source": [
        "#apply the neural network model to the testing data \n",
        "y_pred = model.predict(X_test) \n",
        "\n",
        "#get the predicted labels of the data \n",
        "y_predLabel = np.argmax(y_pred,axis = 1) \n",
        "\n",
        "\n",
        "#display the results of the testing data\n",
        "#display accuracy, f1-score, precision and recall\n",
        "print(classification_report(y_test,y_predLabel))\n",
        "print(\"Overall precision\",round(precision_score(y_test,y_predLabel, average='micro'),2))\n",
        "print(\"Overall recall\",round(recall_score(y_test,y_predLabel, average='micro'),2))\n",
        "print(\" \")\n",
        "\n",
        "#print the confusion matrix\n",
        "print(confusion_matrix(y_test,y_predLabel))\n",
        "\n",
        "#confusion matrix\n",
        "#from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "classes = ['Covid-19','Pneumonia','No_findings']\n",
        "print(y_test)\n",
        "print(y_predLabel)\n",
        "\n",
        "confusion_matrix2 = confusion_matrix(y_test,y_predLabel)\n",
        "training_mat = ConfusionMatrixDisplay(confusion_matrix2,display_labels = classes)\n",
        "training_mat.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.24      0.27        25\n",
            "           1       0.43      0.64      0.51       100\n",
            "           2       0.48      0.27      0.35       100\n",
            "\n",
            "    accuracy                           0.43       225\n",
            "   macro avg       0.41      0.38      0.38       225\n",
            "weighted avg       0.44      0.43      0.41       225\n",
            "\n",
            "Overall precision 0.43\n",
            "Overall recall 0.43\n",
            " \n",
            "[[ 6 16  3]\n",
            " [10 64 26]\n",
            " [ 3 70 27]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}