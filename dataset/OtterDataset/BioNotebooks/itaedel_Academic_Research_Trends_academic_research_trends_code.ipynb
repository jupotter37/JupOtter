{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#A Needle in a Data Haystack (67978) - Final Project\n",
        "#Analyzing and Predicting Trends in Academic Research\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Noa Ben Gallim (noa.bengallim@mail.huji.ac.il)\n",
        "\n",
        "Itamar Edelstein (itamar.edelstein@mail.huji.ac.il)\n",
        "\n",
        "Idan Hippach (idan.hippach@mail.huji.ac.il)\n"
      ],
      "metadata": {
        "id": "UZRjKwiyphRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G2OkQ-G2pSdI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import ast\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from tqdm import tqdm\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import defaultdict, Counter\n",
        "from networkx.algorithms.community import louvain_communities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "TiFhlm_YpxpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(filename):\n",
        "    \"\"\"\n",
        "    Preprocess on each txt file individually and after that we combine them\n",
        "    \"\"\"\n",
        "    total_lines = sum(1 for _ in open(filename, 'r', encoding='utf-8'))\n",
        "    data = []\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in tqdm(file, total=total_lines, desc=\"Processing file\"):\n",
        "            record = json.loads(line.strip())\n",
        "            data.append(record)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    df = df.drop_duplicates(subset='id')\n",
        "    df.drop(columns=['venue', 'page_start', 'page_end', 'doc_type', 'publisher',\n",
        "                     'issue', 'volume', 'url', 'doi', 'indexed_abstract',\n",
        "                     'references', 'abstract'], inplace=True, errors='ignore')\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Filter by year\n",
        "    df = df[df['year'] >= 2000]\n",
        "    df = df[df['year'] < 2020]\n",
        "\n",
        "    # Check if all authors have both 'org' and 'org_id'\n",
        "    def all_authors_have_name_and_id(author_list):\n",
        "        return all('org' in author and 'org_id' in author for author in author_list)\n",
        "\n",
        "    df = df[df['authors'].apply(all_authors_have_name_and_id)]\n",
        "\n",
        "    df.to_csv(f'{filename}_preprocessed.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "wZD0GFpPp0yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_dataframes(folder_path, df_names):\n",
        "    \"\"\"\n",
        "    Combine all the mag papers preprecessed dataframes into one dataframe\n",
        "    \"\"\"\n",
        "    df_list = []\n",
        "\n",
        "    for df_name in df_names:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        df = pd.read_csv(file_path)\n",
        "        df = df.dropna(subset=['authors', 'fos'])  # Remove rows where 'authors' or 'fos' contain NaN\n",
        "        df_list.append(df)\n",
        "\n",
        "    # Concatenate all DataFrames into one DataFrame\n",
        "    combined_df = pd.concat(df_list, ignore_index=True)\n",
        "    combined_df.to_csv(f'{folder_path}/mag_papers_combined.csv')\n",
        "    return combined_df\n"
      ],
      "metadata": {
        "id": "pa_vC5WPp6VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have downloaded the MAG papers files, follow these steps:  \n",
        "1. Process each file individually using the `preprocess` function.  \n",
        "2. After preprocessing, merge all the resulting files into a single DataFrame.  "
      ],
      "metadata": {
        "id": "TF0dKfGKrBSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mag_papers_files = ['mag_papers.txt']  # Replace with the names of your MAG papers files\n",
        "\n",
        "for file_name in mag_papers_files:\n",
        "    preprocess(file_name)"
      ],
      "metadata": {
        "id": "IpUq1rZMsPYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_mag_papers_files = ['mag_papers_preprocessed.csv']  # Replace with the names of your preprocessed files\n",
        "mag_papers = combine_dataframes('your_mag_papers_folder_path', preprocessed_mag_papers_files)"
      ],
      "metadata": {
        "id": "2noxPwhguegV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will focus exclusively on the 'year' and 'fos' fields. To simplify processing, we'll extract the 'fos' names into a string format for easier manipulation:"
      ],
      "metadata": {
        "id": "6tMnsXzNu5t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fos_names(fos_list):\n",
        "    \"\"\"\n",
        "    Extracts fields of science (FoS) names from a string containing a list-like\n",
        "    structure and returns a list of extracted FoS names as strings\n",
        "    \"\"\"\n",
        "    reg = r\"'[a-zA-Z ]+',\"\n",
        "    names = re.findall(reg, fos_list)\n",
        "    return [name[1:-2] for name in names]"
      ],
      "metadata": {
        "id": "Za_wv7UTvUrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mag_papers = mag_papers.dropna(subset=['id', 'title', 'authors', 'year', 'fos'])\n",
        "mag_papers['fos_names'] = mag_papers['fos'].apply(get_fos_names)\n",
        "\n",
        "fos_data = mag_papers[['year', 'fos_names']]\n",
        "fos_data['fos_names'] = fos_data['fos_names'].apply(lambda x: ', '.join(x))"
      ],
      "metadata": {
        "id": "nX6zTEuzu1NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Visualizing Top Fields of Study"
      ],
      "metadata": {
        "id": "evqZo92EqIDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_fos_bar_graph(fos_data, top_n=10):\n",
        "    \"\"\"\n",
        "    Visualize the bar graph of the top n fields of study\n",
        "    \"\"\"\n",
        "    all_fos = ', '.join(fos_data['fos_names']).split(', ')\n",
        "    fos_counter = Counter(all_fos)\n",
        "\n",
        "    fos_df = pd.DataFrame(fos_counter.items(), columns=['fos_name', 'count'])\n",
        "    fos_df = fos_df.sort_values(by='count', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(fos_df['fos_name'][:top_n],\n",
        "            fos_df['count'][:top_n],\n",
        "            color='skyblue')\n",
        "\n",
        "    plt.xlabel('Fields of Study')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Top {top_n} Fields of Study Across All Years')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kAu85U-XqTEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to visualize the top fields of study:"
      ],
      "metadata": {
        "id": "6NAs2cAZqwSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_fos_bar_graph(fos_data)"
      ],
      "metadata": {
        "id": "ROwh03pZq2--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Research Trends Over Time"
      ],
      "metadata": {
        "id": "0SB4ENYTvjPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fos_per_year(fos_data):\n",
        "    \"\"\"\n",
        "    Calculate FoS counts per year\n",
        "    \"\"\"\n",
        "    fos_per_year = {}\n",
        "\n",
        "    for year, group in fos_data.groupby('year'):\n",
        "        fos_names = [fos for fos_list in group['fos_names'] for fos in fos_list.split(', ')]\n",
        "\n",
        "        if year not in fos_per_year:\n",
        "            fos_per_year[year] = Counter(fos_names)\n",
        "        else:\n",
        "            fos_per_year[year].update(fos_names)\n",
        "\n",
        "    return fos_per_year"
      ],
      "metadata": {
        "id": "J2JVso6nvotk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fos_trends(fos_data, top_n=10):\n",
        "    \"\"\"\n",
        "    Plot the trends for the top n Fields of Study over time.\n",
        "    \"\"\"\n",
        "    fos_per_year = get_fos_per_year(fos_data)\n",
        "    total_fos_counter = Counter()\n",
        "\n",
        "    for year_counter in fos_per_year.values():\n",
        "        total_fos_counter.update(year_counter)\n",
        "\n",
        "    top_fos = [fos for fos, count in total_fos_counter.most_common(top_n)]\n",
        "\n",
        "    year_range = sorted(map(int, fos_per_year.keys()), reverse=True)\n",
        "    year_range = year_range[:-1]\n",
        "    fos_trends = {fos: [] for fos in top_fos}\n",
        "\n",
        "    for year in year_range:\n",
        "        year_counter = fos_per_year.get(year, Counter())\n",
        "        for fos in top_fos:\n",
        "            fos_trends[fos].append(year_counter.get(fos, 0))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for fos in top_fos:\n",
        "        plt.plot(year_range, fos_trends[fos], label=fos)\n",
        "\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Trends for Top {top_n} Fields of Study Over Time')\n",
        "    plt.xticks(year_range)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Neu-arrTvtcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to visualize the research trends over the years:"
      ],
      "metadata": {
        "id": "6tB72OvCv2Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_fos_trends(fos_data)"
      ],
      "metadata": {
        "id": "8EONAtO5vwTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Clustering Research Fields with K-Means"
      ],
      "metadata": {
        "id": "vIPHJf8hv75e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_clustering(data, X, n_clusters):\n",
        "    \"\"\"\n",
        "    Clusters the data using K-means and visualizes the clusters with word clouds.\n",
        "    X is a vectorized representation of the fos_names column from the data.\n",
        "    \"\"\"\n",
        "    fos_list = data['fos_names']\n",
        "\n",
        "    # K-means clustering to group fields into general categories\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    data[f'category_cluster_{n_clusters}'] = kmeans.fit_predict(X)\n",
        "\n",
        "    clusters = kmeans.labels_  # cluster labels for each keyword\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=int(n_clusters/2),\n",
        "                             ncols=2,\n",
        "                             figsize=(20, 40))\n",
        "\n",
        "    for cluster, ax in enumerate(axes.flatten()):\n",
        "        terms_in_cluster = ', '.join(fos_list[data[f'category_cluster_{n_clusters}'] == cluster])\n",
        "        terms_dict = Counter(terms_in_cluster.split(', '))\n",
        "\n",
        "        # Plot a wordcloud for this cluster\n",
        "        wordcloud = WordCloud(width=800,\n",
        "                              height=400,\n",
        "                              background_color='white').generate_from_frequencies(terms_dict)\n",
        "\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.set_title(f'Cluster {cluster}', fontsize=12)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "m6TpHE8qwHs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer(text):\n",
        "    \"\"\"\n",
        "    A custom tokenizer for splitting text based on commas.\n",
        "    \"\"\"\n",
        "    return [token.strip() for token in text.split(', ')]\n",
        "\n",
        "\n",
        "# Use a TfidfVectorizer to transform the fos_names column into a TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, token_pattern=None)\n",
        "X = vectorizer.fit_transform(fos_data['fos_names'])"
      ],
      "metadata": {
        "id": "G1mGSjxawLgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering(fos_data, n_clusters):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, token_pattern=None)\n",
        "    X = vectorizer.fit_transform(fos_data['fos_names'])\n",
        "\n",
        "    for n in n_clusters:\n",
        "        kmeans_clustering(fos_data, X, n_clusters=n)"
      ],
      "metadata": {
        "id": "d7eJPO3XwYR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Execute the code below to perform K-Means clustering on research fields and visualize the clusters using word clouds:"
      ],
      "metadata": {
        "id": "-FyBhfl2ws-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = []  # Specify the numbers of clusters you want to explore\n",
        "clustering(fos_data, n_clusters)"
      ],
      "metadata": {
        "id": "jPPf2jQ0woIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Community Detection in Research Fields"
      ],
      "metadata": {
        "id": "vvZbJ554w_ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collaboration_graph(data):\n",
        "    \"\"\"\n",
        "    Constructs a collaboration graph where each node is a field of science (FoS),\n",
        "    and an edge between nodes represents co-occurrence of FoS in the same paper.\n",
        "    The weight of the edge corresponds to the number of co-occurrences.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        fields_of_study = row['fos_names'].split(', ')\n",
        "\n",
        "        # Add edges between all fields in this paper\n",
        "        for i in range(len(fields_of_study)):\n",
        "            for j in range(i + 1, len(fields_of_study)):\n",
        "                G.add_edge(fields_of_study[i],\n",
        "                           fields_of_study[j],\n",
        "                           weight=G.get_edge_data(fields_of_study[i],\n",
        "                                                  fields_of_study[j],\n",
        "                                                  default={'weight': 0})['weight'] + 1)\n",
        "    return G"
      ],
      "metadata": {
        "id": "-c_4Bi6PxGQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aggregated_graph(G, communities):\n",
        "    \"\"\"\n",
        "    Aggregates the original collaboration graph by collapsing nodes\n",
        "    into their respective communities\n",
        "    \"\"\"\n",
        "    aggregated_graph = nx.Graph()\n",
        "    community_map = {}\n",
        "\n",
        "    # Assign nodes to communities\n",
        "    for i, community in enumerate(communities):\n",
        "        aggregated_graph.add_node(i, size=len(community))\n",
        "        for node in community:\n",
        "            community_map[node] = i\n",
        "\n",
        "    # Add edges between communities\n",
        "    for u, v in G.edges():\n",
        "        u_community = community_map[u]\n",
        "        v_community = community_map[v]\n",
        "        if u_community != v_community:\n",
        "            if aggregated_graph.has_edge(u_community, v_community):\n",
        "                aggregated_graph[u_community][v_community]['weight'] += 1\n",
        "            else:\n",
        "                aggregated_graph.add_edge(u_community, v_community, weight=1)\n",
        "\n",
        "    return aggregated_graph"
      ],
      "metadata": {
        "id": "-yNJB4c2xJUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_communities(G, communities, title, labels):\n",
        "    \"\"\"\n",
        "    Visualizes the aggregated graph of communities\n",
        "    \"\"\"\n",
        "    pos = nx.circular_layout(G)\n",
        "\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(communities)))\n",
        "    node_colors = [colors[i] for i in range(len(communities))]\n",
        "    sizes = [G.nodes[node]['size'] for node in G.nodes()]\n",
        "    node_color_map = [node_colors[node] for node in G.nodes()]\n",
        "    weights = [G[u][v]['weight'] * 0.0001 for u, v in G.edges()]\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=node_color_map)\n",
        "    nx.draw_networkx_edges(G, pos, width=weights)\n",
        "    nx.draw_networkx_labels(G, pos, labels, font_size=12)\n",
        "\n",
        "    plt.title(title, fontsize=20)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HBZ9AlnlxOfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_community_labels(communities):\n",
        "    \"\"\"\n",
        "    Generates labels for each community based on the fields of s\n",
        "    cience present within the community\n",
        "    \"\"\"\n",
        "    labels = dict()\n",
        "    for community_idx, community in enumerate(communities):\n",
        "        fields = []\n",
        "        if 'mathematics' in community:\n",
        "            fields.append('Mathematics')\n",
        "        if 'physics' in community:\n",
        "            fields.append('Physics')\n",
        "        if 'chemistry' in community:\n",
        "            fields.append('Chemistry')\n",
        "        if 'materials science' in community:\n",
        "            fields.append('Materials Science')\n",
        "        if 'medicine' in community:\n",
        "            fields.append('Medicine')\n",
        "        if 'biology' in community:\n",
        "            fields.append('Biology')\n",
        "        if 'humanities' in community:\n",
        "            fields.append('Social Science\\nand Humanities')\n",
        "        if 'engineering' in community:\n",
        "            fields.append('Engineering')\n",
        "        if 'geology' in community:\n",
        "            fields.append('Geology')\n",
        "        if 'environmental science' in community:\n",
        "            fields.append('Environmental Science')\n",
        "        if 'computer science' in community:\n",
        "            fields.append('Computer Science')\n",
        "\n",
        "        if len(fields) == 0:\n",
        "            labels[community_idx] = ',\\n'.join(list(community)[:3])\n",
        "        else:\n",
        "            labels[community_idx] = '\\n'.join(fields)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "XOHImjqNxQV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fos_counts_per_community(communities, data):\n",
        "    \"\"\"\n",
        "    Count the frequency of FOS terms for each community\n",
        "\n",
        "    This function is used to determine the labels for the communities by printing\n",
        "    the top 10 most common FOS terms and their counts for each community.\n",
        "    Although it is not part of the community detection pipeline, it is left here for\n",
        "    documentation purposes to help in understanding and interpreting community labels.\n",
        "    \"\"\"\n",
        "    community_fos_counts = []\n",
        "\n",
        "    for community_idx, community in enumerate(communities):\n",
        "        fos_in_community = []\n",
        "        for index, row in data.iterrows():\n",
        "            fields_of_study = row['fos_names'].split(', ')\n",
        "\n",
        "            # Add to the community if the FOS terms belong to the community\n",
        "            for fos in fields_of_study:\n",
        "                if fos in community:\n",
        "                    fos_in_community.append(fos)\n",
        "\n",
        "        fos_count = Counter(fos_in_community)  # Count the occurrences of each FOS in the community\n",
        "        community_fos_counts.append(fos_count)\n",
        "\n",
        "    return community_fos_counts\n",
        "\n",
        "\n",
        "def print_most_common_fos_in_communities(community_fos_counts):\n",
        "    \"\"\"\n",
        "    Print the most common FOS terms for each community.\n",
        "\n",
        "    This function is used to determine the labels for the communities by printing\n",
        "    the top 10 most common FOS terms and their counts for each community.\n",
        "    Although it is not part of the community detection pipeline, it is left here for\n",
        "    documentation purposes to help in understanding and interpreting community labels.\n",
        "    \"\"\"\n",
        "    for community_idx, fos_counter in enumerate(community_fos_counts):\n",
        "        common_fos = fos_counter.most_common(10)\n",
        "        common_fos_str = ', '.join([f'{fos} ({count})' for fos, count in common_fos])\n",
        "        print(f\"Community {community_idx}: {common_fos_str}\")"
      ],
      "metadata": {
        "id": "Xrbx6G0vxSkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the code below to identify communities in research fields for your chosen time range and visualize them:"
      ],
      "metadata": {
        "id": "ml3jIDulxWem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_year, end_year = 2000, 2020  # Set the desired start and end years\n",
        "\n",
        "for year in range(start_year, end_year):\n",
        "    collaboration_G = collaboration_graph(fos_data[fos_data['year'] == year])\n",
        "    communities = louvain_communities(collaboration_G)\n",
        "    aggregated_graph = get_aggregated_graph(collaboration_G, communities)\n",
        "    labels = get_community_labels(communities)\n",
        "    plot_communities(aggregated_graph,\n",
        "                     communities,\n",
        "                     title=f'Louvain Communities for Year {year}',\n",
        "                     labels=labels)"
      ],
      "metadata": {
        "id": "BGtR8GSGxUZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}