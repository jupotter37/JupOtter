{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b32c9af",
   "metadata": {},
   "source": [
    "# Network Neuroscience\n",
    "\n",
    "##### Authors: Mauricio Barahona and Robert Peach\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "\n",
    "The field of neuroscience is simulanteously being blessed and cursed with a rapid expansion in the size, scope and complexity of neural data drawn from multiple levels of spatial and temporal organisation. Large portions of this data are relational, describing the interconnections between many individual elements of neurobiological systems. Examples include, but are not limited to:\n",
    "\n",
    "- Protein interaction networks\n",
    "- Genetic regulatory networks\n",
    "- Synaptic connections\n",
    "- Dynamical patterns of neural signalling\n",
    "- Interactions between brain networks and environment\n",
    "\n",
    "The data is not only multi-scale, but involve different domains of biology or data types, posing challenges in analysis. From the intersection between (i) the development of empirical methods for mapping and recording neurobiological data and (ii) the theoretical and computational advances in data analysis and modeling of brain networks, there is an emerging trend of research which falls under the umbrella of *network neuroscience*.\n",
    "We can ask how ideas and tools from network science may bring changes and advances to the types of questions that we can ask and the hypotheses that we can test about neuroscience.\n",
    "\n",
    "\n",
    "##### What to expect?\n",
    "There is no doubt that many of you in this room are more experienced with network neuroscience than us. That being said, we wanted to touch on a couple of topics for those that haven't!\n",
    "1. Network analysis of connectome\n",
    "2. Topological analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ee17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6bb0bb",
   "metadata": {},
   "source": [
    "## Functional connectivity analysis\n",
    "\n",
    "Functional neuroimaging techniques are used widely in cognitive neuroscience to investigate aspects of functional specialization and functional integration in the human brain. First we must define our choice of nodes, then using data we must define the links between our nodes.\n",
    "\n",
    "![title](images/network_neuroscience.png)\n",
    "<p style=\"text-align:center\">Connectome pipeline.</p> \n",
    "\n",
    "Here will just perform a short analysis of some functional connectome data. The resting state fMRI (resting-state fMRI) matrices used here (i.e., based in correlation values of time series) were obtained from the The UCLA multimodal connectivity database (1000_Functional_Connectomes dataset http://fcon_1000.projects.nitrc.org/fcpClassic/FcpTable.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4e34e",
   "metadata": {},
   "source": [
    "Lets load in the connectivity matrix that I have already averaged across the full dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f13b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/1000_functional_connectomes/'\n",
    "\n",
    "matrix = np.genfromtxt(folder + \"connectome_average.csv\",delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773cc658",
   "metadata": {},
   "source": [
    "Code for running over full dataset of the individual connectomes is available below! Don't run these elements if you don't have the data :)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d66d29f0",
   "metadata": {},
   "source": [
    "\n",
    "# loading all individual connectomes\n",
    "matrices = [pd.read_csv(file, header = None, delim_whitespace=True) for file in glob.glob(folder + '/connectomes/*_matrix_file.txt')]\n",
    "filenames = [file for file in glob.glob(folder + '/connectomes/*_matrix_file.txt')]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e778fd",
   "metadata": {},
   "source": [
    "# averaging across individuals\n",
    "to_remove = []\n",
    "for i, m in enumerate(matrices):\n",
    "    if m.shape!=(177,177):\n",
    "        to_remove.append(i)\n",
    "\n",
    "for index in sorted(to_remove, reverse=True):\n",
    "    del matrices[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a4ed0",
   "metadata": {},
   "source": [
    "When working with fMRI brain network data, it is useful to generate some plots (e.g., the heatmaps for matrix visualisation, and distribution plots of edge weights) to facilitate data comprehension and flag potential artefacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed091c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_diagnan = matrix.copy()\n",
    "np.fill_diagonal(matrix_diagnan,np.nan) # remove diagonal\n",
    "\n",
    "# plotting heatmap\n",
    "plt.figure(figsize=(20,16))\n",
    "sns.heatmap(matrix_diagnan, cmap='coolwarm', cbar=True, square=False, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4f45c",
   "metadata": {},
   "source": [
    "In brain networks, we expect mostly weak edges and a smaller proportion of strong ones. When plotted as a probability density of log10, we expect the weight distribution to have a Gaussian-like form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed385ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "# Distribution of absolute raw weights\n",
    "rawdist = sns.distplot(abs(matrix_diagnan.flatten()),  kde=False, ax=axes[0], norm_hist=True)\n",
    "rawdist.set(xlabel='Correlation Values', ylabel = 'Density Frequency')\n",
    "\n",
    "# Probability density of log10\n",
    "log10dist = sns.distplot(np.log10(matrix_diagnan).flatten(), kde=False, ax=axes[1], norm_hist=True)\n",
    "log10dist.set(xlabel='log(weights)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af0b2c",
   "metadata": {},
   "source": [
    "Not quite Gaussian on the right... but hey ho."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2645c2",
   "metadata": {},
   "source": [
    "Lets take the absolute (turning negative correlations into positive) and then sparsify (remove low correlation edges). Then we can generate a networkx graph object from our correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98697437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take absolute \n",
    "matrix = abs(matrix)\n",
    "\n",
    "# Create sparser graphs for visualisation and easier analysis\n",
    "matrix_filtered = matrix.copy()\n",
    "matrix_filtered[matrix_filtered<=0.4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a graph object\n",
    "G = nx.from_numpy_matrix(matrix_filtered)\n",
    "\n",
    "# Removing self-loops\n",
    "G.remove_edges_from(list(nx.selfloop_edges(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff0707",
   "metadata": {},
   "source": [
    "Load additional data and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30804fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the xyz coordinates of each region\n",
    "\n",
    "path_pos = folder + '/HCP_positions.txt'\n",
    "positions = pd.read_csv(path_pos, header = None, delim_whitespace=True)\n",
    "\n",
    "# defining coordinates\n",
    "pos = {}\n",
    "for node in G.nodes:\n",
    "    pos[node] = np.array([positions.loc[node,0],positions.loc[node,1]])\n",
    "    \n",
    "# lets create a simple plot\n",
    "nx.draw(G,pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4de4d7",
   "metadata": {},
   "source": [
    "This visualisation isn't very nice, nor is it informative. Lets trying something 3D with a nice trace of the brain instead...\n",
    "\n",
    "The below cell has some functions to help us plot. We don't need to understand this part in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_brain import plot_brain_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e2975",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_brainobj = folder +  '/brain.obj'\n",
    "path_pos = folder + '/HCP_positions.txt'\n",
    "\n",
    "fig, node_size = plot_brain_network(G, path_pos, path_brainobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd75e7a",
   "metadata": {},
   "source": [
    "## Pose estimation and topology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42316022",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings. Extracting particular aspects of a behavior for further analysis isn't easy, but there are various tools such as deep lab cut, that allow us to track body parts. For example, here we are quite interested in examining motor disorders.\n",
    "\n",
    "![title](images/pose_networks.gif)\n",
    "<p style=\"text-align:center\">Figure: Pose estimation of humans from mediapipe.</p> \n",
    "\n",
    "However, given the large data sets for tracking body parts, how do we then analyse the data? Of course, this depends on the experiment at hand, but here we explore the topology and geometry of the movements.\n",
    "\n",
    "#### Persistent homology\n",
    "\n",
    "To analyse this data, we will touch on an area closely linked to networks (and their extensions into higher order networks). \n",
    "Persistent homology is a method for computing topological features of a space at different spatial resolutions. With it, we can track homology cycles across simplicial complexes (higher order networks), and determine whether there were homology classes that \"persisted\" for a long time. The basic idea is summarized in the illustration below.\n",
    "\n",
    "![title](images/persistent_homology.jpeg)\n",
    "Figure: Topological data analysis. (A) Illustration of simplexes. (B) Representation of simplexes/cliques of different order being formed in the system (e.g., in the brain) across the filtration process. (C) Barcode respective to panel B, representing the filtration across distances (i.e., the inverse of weights in a correlation matrix). Line A represents cycle A in B. H0-2 indicates the homology groups. (H0 = connected components, H1 = one-dimensional holes, H2 = 2-dimensional holes). (D) Circular projection of how the system (e.g., the brain) would be connected. (E) Persistence diagram (or Birth/Death plot) obtained from real resting-state fMRI brain data. In this plot, it is also possible to identify a phase transition between H1 and H2.[1]\n",
    "\n",
    "[1] Centeno, Eduarda Gervini Zampieri, et al. \"A hands-on tutorial on network and topological neuroscience.\" Brain Structure and Function 227.3 (2022): 741-762.\n",
    "\n",
    "\n",
    "\n",
    "##### Data\n",
    "\n",
    "Here, we are using some data kindly donated by Alex Grotemeyer (unpublished so please don't share further than this room!). The data is of rats running on a treadmill - see example image below. We have both ventral and lateral views taken in simultaneous videos.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/rat_gait.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<p style=\"text-align:center\">Figure: Ventral view of rat on treadmill task. The markers have been predicted using DLC.</p> \n",
    "\n",
    "We are going to use ideas from topological data analysis to construct our network and then analyse it with persistent homology!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5bda2",
   "metadata": {},
   "source": [
    "Lets load the cleaned and processed data that I generated earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa184531",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/gait/gait_data.pickle', 'rb') as handle:\n",
    "    all_data, labels = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd38b4",
   "metadata": {},
   "source": [
    "The next set of functions are simply to load and clean the DLC data which are stored in CSV files. You don't need to run these, but I am leaving them here for your future interest/usage."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8f39dd7",
   "metadata": {},
   "source": [
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import re"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bf76a27",
   "metadata": {},
   "source": [
    "# removing low likelihood values from the DLC csv files\n",
    "def remove_low_likelihood(data, likelihood=0.9):\n",
    "    \"\"\" Setting low likelihood samples to zero \"\"\"\n",
    "    \n",
    "    markers = list(data.columns.get_level_values(level=0).unique())\n",
    "    \n",
    "    for marker in markers:\n",
    "        data.loc[(data[marker]['likelihood']<likelihood),(marker,'x')] = np.nan\n",
    "        data.loc[(data[marker]['likelihood']<likelihood),(marker,'y')] = np.nan\n",
    "        \n",
    "    data = data.drop('likelihood', axis=1, level=1)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "530097ab",
   "metadata": {},
   "source": [
    "meta_data_folder = './data/gait/'\n",
    "\n",
    "meta_data = pd.read_excel(meta_data_folder + 'metadata.xlsx')\n",
    "meta_data=meta_data.set_index('id')\n",
    "\n",
    "\n",
    "#%% load patient data\n",
    "directory = './data/gait/cohort_1/'\n",
    "folders1 = [x[0] for x in os.walk(directory)][1:]\n",
    "directory = './data/gait/cohort_2/'\n",
    "folders2 = [x[0] for x in os.walk(directory)][1:]\n",
    "directory = './data/gait/cohort_3/'\n",
    "folders3 = [x[0] for x in os.walk(directory)][1:]\n",
    "directory = './data/gait/cohort_4/'\n",
    "folders4 = [x[0] for x in os.walk(directory)][1:]\n",
    "folders = folders1 + folders2 + folders3 + folders4\n",
    "\n",
    "\n",
    "likelihood = 0.9\n",
    "\n",
    "all_data = []\n",
    "virus_label = []\n",
    "speed_label = []\n",
    "week_label = []\n",
    "exercise_label = []\n",
    "cohort_label = []\n",
    "patient_ids = []\n",
    "filenames = []\n",
    "folders_ = []\n",
    "\n",
    "for k,folder in enumerate(folders):\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".csv\"):   \n",
    "                     \n",
    "            # getting patient details\n",
    "            info = re.findall(r'\\d+',file)\n",
    "                    \n",
    "            patient_id = int(info[0])\n",
    "            speed = int(info[1])\n",
    "            \n",
    "            # try finding metadata, otherwise control\n",
    "            virus = meta_data['virus'].loc[patient_id]                \n",
    "            \n",
    "            # try finding metadata, otherwise control\n",
    "            exercise = meta_data['exercise'].loc[patient_id]                \n",
    "            \n",
    "            week = os.path.basename(folder).split(';')[0]\n",
    "            cohort = os.path.basename(os.path.dirname(folder)).split(';')[0]\n",
    "            \n",
    "            virus_label.append(virus)\n",
    "            speed_label.append(speed)\n",
    "            week_label.append(week)\n",
    "            exercise_label.append(exercise)\n",
    "            cohort_label.append(cohort)\n",
    "            patient_ids.append(patient_id)      \n",
    "            filenames.append(file)\n",
    "            folders_.append(folder)\n",
    "            \n",
    "            \n",
    "labels = pd.DataFrame(data=np.vstack([virus_label, speed_label, week_label, exercise_label, cohort_label,  filenames, folders_,]).T,index=patient_ids,\n",
    "                      columns=['virus','speed','week','exercise','cohort', 'filename','folders'])\n",
    "\n",
    "labels = labels.sort_index()\n",
    "\n",
    "index_to_keep = (pd.Series(labels.index).value_counts()==2)\n",
    "labels = labels.loc[index_to_keep,:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ef06e6c",
   "metadata": {},
   "source": [
    "for idx in range(labels.shape[0]):\n",
    "    data = pd.read_csv(labels['folders'].iloc[idx] + '/' + labels['filename'].iloc[idx],header=[1,2],index_col=0)                \n",
    "    data = remove_low_likelihood(data, likelihood)            \n",
    "    data = data.drop(['T1 ventral', 'T2 ventral', 'T3 ventral'], axis=1, level=0)    \n",
    "    all_data.append(data)            \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84b2ad77",
   "metadata": {},
   "source": [
    "with open('./data/gait/gait_data.pickle', 'wb') as handle:\n",
    "    pickle.dump([all_data,labels], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8dcf8c",
   "metadata": {},
   "source": [
    "Now the fun begins! We have all our DLC files with the most interesting limbs tracked over time. We can now compute correlation matrices between all the limbs in both the x and y directions of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import itertools\n",
    "\n",
    "data = all_data[0]\n",
    "markers = list(data.columns.get_level_values(level=0).unique())\n",
    "marker_pairs = list(itertools.combinations(markers, 2))\n",
    "\n",
    "\n",
    "graphs = []\n",
    "for data in all_data:\n",
    "    correlation_matrix = pd.DataFrame(data=0,columns=markers,index=markers)\n",
    "\n",
    "    # computing correlation as mean of x and y correlations - there are better ways to do this...\n",
    "    correlation_matrix_x =  data.loc[:,data.columns.get_level_values(1)=='x'].corr()\n",
    "    correlation_matrix_y =  data.loc[:,data.columns.get_level_values(1)=='y'].corr()\n",
    "    correlation_matrix = np.dstack([correlation_matrix_x,correlation_matrix_y]).mean(axis=2)\n",
    "        \n",
    "    # converting correlation to a distance measure\n",
    "    distance_matrix = 1 - abs(correlation_matrix)    \n",
    "    distance_matrix[np.isnan(distance_matrix)]=1\n",
    "    \n",
    "    graphs.append(distance_matrix)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817064b",
   "metadata": {},
   "source": [
    "Now we are going to use a package called Giotto-tda to help us compute the persistence homology!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "# Track connected components, loops, and voids\n",
    "homology_dimensions = [0, 1, 2]\n",
    "\n",
    "# Collapse edges to speed up H2 persistence calculation!\n",
    "persistence = VietorisRipsPersistence(\n",
    "    metric=\"precomputed\",\n",
    "    homology_dimensions=homology_dimensions,\n",
    "    n_jobs=6,\n",
    "    collapse_edges=True,\n",
    ")\n",
    "\n",
    "# fit persistence diagram\n",
    "persistence_diagrams = persistence.fit_transform(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "plot_diagram(persistence_diagrams[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485b018",
   "metadata": {},
   "source": [
    "Although persistence diagrams are useful descriptors of the data, they cannot be used directly for machine learning applications. This is because different persistence diagrams may have different numbers of points, and basic operations like the addition and multiplication of diagrams are not well-defined.\n",
    "\n",
    "To overcome these limitations, a variety of proposals have been made to “vectorize” persistence diagrams via embeddings or kernels which are well-suited for machine learning. Here, we use the persistence entropy function in giotto-tda, which measures the entropy of points in a persistence diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ec7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.diagrams import PersistenceEntropy\n",
    "\n",
    "# define persistence entropy object\n",
    "persistence_entropy = PersistenceEntropy()\n",
    "\n",
    "# calculate topological feature matrix\n",
    "X = persistence_entropy.fit_transform(persistence_diagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.plotting import plot_point_cloud\n",
    "\n",
    "# lets visualise our rats. Each point is a rat!\n",
    "plot_point_cloud(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02aca3",
   "metadata": {},
   "source": [
    "Lets now see if there is a statistical difference between the preop and 6 week groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9927d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "x = X[(labels.week=='pre OP')]\n",
    "y = X[(labels.week=='6 weeks')]\n",
    "\n",
    "tvalues, pvalues = st.ttest_rel(x,y)\n",
    "\n",
    "print(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4e970",
   "metadata": {},
   "source": [
    "We notice that there is significance when we consider H2 homology! Can we interpret this? (maybe, with great difficulty).\n",
    "\n",
    "Persistence entropies (each row in our feature matrix) are calculated as the (base 2) Shannon entropies of the collections of differences d - b (“lifetimes”), normalized by the sum of all such differences. A larger entropy in H2 holes means that the lifetimes of H2 holes vary more. We observe a larger entropy in the lifetimes of H2 holes in preOP mice relative to 6 weeks. Potentially this could be a training effect?\n",
    "\n",
    "Thankfully we are at the end of the workshop and I won't need to spend any further time interpreting this horror!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfd42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fc8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
