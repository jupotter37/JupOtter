{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Clustering\n",
    "\n",
    "- **Clustering Definition:** \n",
    "  - Clustering is the process of grouping a set of objects in a dataset into clusters based on similarity, where objects in the same cluster are similar, and those in different clusters are dissimilar.\n",
    "  - It is an **unsupervised learning technique** where the data is unlabeled.\n",
    "\n",
    "**Applications of Clustering:**\n",
    "- **Customer Segmentation:** \n",
    "  - Example: Grouping customers based on characteristics like age, income, interests, etc., to better target marketing efforts.\n",
    "  - Allows businesses to identify and focus on high-profit, low-risk customers.\n",
    "- **Retail Industry:** \n",
    "  - Identify buying patterns and customer behavior based on demographic characteristics.\n",
    "  - Used in recommendation systems (e.g., suggesting books or movies).\n",
    "- **Banking:** \n",
    "  - Fraud detection by identifying clusters of normal and abnormal transactions.\n",
    "  - Segment customers into loyal versus churned customers.\n",
    "- **Insurance Industry:** \n",
    "  - Fraud detection in claims analysis.\n",
    "  - Risk evaluation of customers based on segment analysis.\n",
    "- **Publication Media:** \n",
    "  - Auto-categorize and tag news articles based on content for recommendation purposes.\n",
    "- **Medicine and Biology:** \n",
    "  - Group patients by behavior to identify successful therapies.\n",
    "  - Cluster genes with similar expression patterns or genetic markers for family ties.\n",
    "\n",
    "**Clustering vs. Classification:**\n",
    "- **Classification:** \n",
    "  - Supervised learning with labeled data, used to predict categorical class labels.\n",
    "  - Example: Predicting customer default with decision trees, SVM, or logistic regression.\n",
    "- **Clustering:** \n",
    "  - Unsupervised learning with unlabeled data.\n",
    "  - Example: Using k-means to group customers based on attributes like age and education.\n",
    "\n",
    "**Purposes of Clustering:**\n",
    "- **Exploratory Data Analysis:** \n",
    "  - Understanding patterns and structures within the data.\n",
    "- **Summary Generation:** \n",
    "  - Reducing data scale for easier analysis.\n",
    "- **Outlier Detection:** \n",
    "  - Identifying anomalies, useful in fraud detection or noise removal.\n",
    "- **Finding Duplicates:** \n",
    "  - Identifying similar records in a dataset.\n",
    "- **Pre-processing Step:** \n",
    "  - Used before other data mining tasks or predictions.\n",
    "\n",
    "**Clustering Algorithms:**\n",
    "- **Partition-based Clustering:**\n",
    "  - Produces sphere-like clusters.\n",
    "  - **Examples:** K-Means, K-Medians, Fuzzy c-Means.\n",
    "  - **Characteristics:** Efficient for medium to large datasets.\n",
    "- **Hierarchical Clustering:**\n",
    "  - Produces tree-like clusters.\n",
    "  - **Types:** Agglomerative, Divisive.\n",
    "  - **Characteristics:** Intuitive, good for small datasets.\n",
    "- **Density-based Clustering:**\n",
    "  - Produces arbitrary-shaped clusters.\n",
    "  - **Example:** DBSCAN (good for spatial clusters and noisy datasets).\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Note:** \n",
    "- Clustering is a versatile tool used across various industries for tasks like customer segmentation, fraud detection, and recommendation systems. Understanding different clustering algorithms and their applications is crucial for effectively analyzing and interpreting large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to k-Means\n",
    "\n",
    "- **Customer Segmentation**: A technique to partition a customer base into groups with similar characteristics.\n",
    "- **K-Means Clustering**: A type of partitioning clustering that divides data into K non-overlapping subsets or clusters, with no predefined structure or labels. It's an unsupervised algorithm.\n",
    "\n",
    "#### **Key Concepts**\n",
    "- **Clustering**: Grouping objects such that objects within a cluster are similar, and objects across different clusters are dissimilar.\n",
    "- **Similarity/Dissimilarity**: K-Means often uses dissimilarity metrics (e.g., Euclidean distance) to measure how different two samples are. The goal is to minimize intra-cluster distances and maximize inter-cluster distances.\n",
    "\n",
    "#### **Steps in K-Means Clustering**\n",
    "1. **Initialize K (Number of Clusters)**:\n",
    "   - Choose a random point as the center (centroid) for each cluster.\n",
    "   - **Centroids**: Representative points for clusters; initially selected randomly.\n",
    "  \n",
    "2. **Assign Data Points to Clusters**:\n",
    "   - Calculate the distance of each data point from the centroids using a distance matrix.\n",
    "   - Assign each point to the closest centroid, forming initial clusters.\n",
    "\n",
    "3. **Update Centroids**:\n",
    "   - Calculate the new centroid as the mean of all data points in a cluster.\n",
    "   - Move the centroid to the new position based on the mean of the cluster members.\n",
    "\n",
    "4. **Iterate**:\n",
    "   - Repeat the assignment and centroid update steps until centroids no longer move.\n",
    "   - This process continues until the algorithm converges (i.e., no further changes in centroids).\n",
    "\n",
    "#### **Convergence and Optimization**\n",
    "- **Convergence**: The algorithm continues until centroids stabilize, but it may not reach a global optimum (best possible clusters).\n",
    "- **Local Optimum**: The result might be a local optimum; different initializations can lead to different results.\n",
    "- **Multiple Runs**: To improve results, the algorithm can be run multiple times with different starting points.\n",
    "\n",
    "#### **Key Points to Remember**\n",
    "- **K-Means**: Iterative and heuristic, with potential variations in results based on initial centroid selection.\n",
    "- **Distance Metrics**: The choice of distance metric (e.g., Euclidean) is crucial and should align with the data type and domain knowledge.\n",
    "- **Fast Convergence**: Despite its potential for local optima, K-Means is fast and efficient, making it practical for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on k-Means\n",
    "\n",
    "#### **Algorithm Overview**\n",
    "- **Initialization**: Randomly place K centroids, each representing a cluster.\n",
    "- **Distance Measurement**: Use Euclidean distance (or other distance metrics) to calculate how far each data point is from each centroid.\n",
    "- **Assignment**: Assign each data point to the nearest centroid, forming clusters.\n",
    "- **Centroid Update**: Recalculate the centroid as the mean of all points in its cluster.\n",
    "- **Iteration**: Repeat the assignment and update steps until centroids stabilize.\n",
    "\n",
    "#### **Evaluating Clustering Accuracy**\n",
    "- **Ground Truth Comparison**: In supervised scenarios, compare clusters with known labels. However, K-Means is unsupervised, so this is often not possible.\n",
    "- **Within-Cluster Sum of Squares (WCSS)**: Measure the average distance between data points and their cluster centroids. A lower WCSS indicates better clustering.\n",
    "\n",
    "#### **Choosing the Number of Clusters (K)**\n",
    "- **Challenge**: The optimal number of clusters (K) is not straightforward and depends on the data distribution.\n",
    "- **Elbow Method**:\n",
    "  - **Procedure**: Run K-Means for different values of K and calculate the clustering error (e.g., WCSS).\n",
    "  - **Plot**: Create a plot of the error metric versus K.\n",
    "  - **Elbow Point**: Identify the point where the rate of decrease in error sharply changes. This point indicates a good balance between the number of clusters and the error metric.\n",
    "\n",
    "#### **Characteristics of K-Means Clustering**\n",
    "- **Efficiency**: Relatively efficient for medium to large datasets.\n",
    "- **Cluster Shape**: Produces spherical clusters around centroids.\n",
    "- **Drawback**: Requires pre-specification of the number of clusters, which can be challenging.\n",
    "\n",
    "#### **Summary**\n",
    "- K-Means clustering is effective and widely used but requires careful consideration of the number of clusters and can be sensitive to initial centroid placements. The elbow method is a practical approach for determining the optimal K by examining changes in clustering error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
