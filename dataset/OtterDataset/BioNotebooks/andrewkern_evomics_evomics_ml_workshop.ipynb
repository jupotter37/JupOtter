{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evomics Machine Learning Workshop\n",
    "The goal of today’s workshop is to give you a flavor of what you can do with machine learning, introduce of few very useful algorithms, and eventually apply this to something we care about– evolutionary genetics.\n",
    "\n",
    "Presumably you already understand the context here: our basic goal is to use existing data (a so-called training set, more on this in a moment) to make predictions about new data that we encounter in the work. Traditionally the way to make such predictions was to rely on what we call a *generative* model, i.e. a probabilitistic model that describes the process from which our observed data was generated. With such a generative model in hand, we could then take new data and learn things like the values of parameters from the model which produced the observations. But what if we are unsure of the model? Can we still make useful predictions? The answer is yes and one very popular way is through machine learning.\n",
    "\n",
    "In this workshop we will focus on a branch of machine learning (ML) called supervised ML. Supervised ML starts with a *labeled* or known dataset that we will call the training set. This training set allows us to teach an algorithm about how independent variables (call them $X$) map to dependent variables ($y$). As such we will focus on the *conditional probability* $p(y | X)$ and the models we will use are said to be *discriminative* models. The key here is that the algorithms that we will train with our training set will focus directly on the mapping from $X \\rightarrow y$ rather than on the structure of the model per se. This has proven extremely useful in practice, and high prediction accuracies can be acheived for very complex problems-- problems that are often to complex to write down a full blown probabilistic model. So let's get cracking.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anderson's iris data \n",
    "For a first example lets use Anderson's classic iris dataset. This is a classic dataset because R. A. Fisher used these data in his 1936 paper entitled *The use of multiple measurements in taxonomic problems*. In that paper Fisher introduced a method called linear discriminant analysis (LDA) that is considered by some to be the first machine learning method. This dataset consists of 4 morphological measurements from 150 flowers belonging to three species, *Iris setosa*, *Iris virginica*, and *Iris versicolor*. \n",
    "Let's take a quick look at the plants \n",
    "\n",
    "# \n",
    "# <div style=\"margin: 20px 0;\">\n",
    "#     <img src=imgs/Iris_setosa.jpg style=\"margin-right: 20px;\" align=\"left\" />\n",
    "#     <img src=imgs/Iris_virginica.jpg style=\"margin-right: 20px;\" align=\"left\" />\n",
    "#     <img src=imgs/Iris_versicolor.jpg align=\"left\" />\n",
    "# </div>\n",
    "# <div style=\"clear: both; margin-bottom: 20px;\"></div>\n",
    "    \n",
    "<div style=\"text-align: center; font-style: italic; margin-bottom: 20px;\">\n",
    "From left to right: I. setosa (with its distinctive narrow petals), I. virginica (with larger, broader petals), and I. versicolor (with intermediate-sized petals).\n",
    "</div>\n",
    "\n",
    "These three iris species look quite similar to the untrained eye. Let's examine the data by first looking at a summary, then visualizing a subset. We'll use the `sklearn` package, which conveniently includes this classic iris dataset, for our analysis.\n",
    "\n",
    "\n",
    "now let's import our data and get moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "df_iris['species'] = iris.target_names[iris.target]\n",
    "\n",
    "# Take a look at df_iris\n",
    "df_iris.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really useful thing to do is to create a pairplot to examine everything. We can do this with the `sns.pairplot` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_iris, hue='species')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in comparing these two *features*, sepal length and sepal width, the species don't look perfectly seperable (i.e. we can't draw firm dividing lines between groups). That's ok, we're gonna solve this problem with ML!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying irises using support vector machines\n",
    "\n",
    "The first ML algorithm we will look at are called Support Vector Machines (SVMs). SVMs were first developed by Vladimir Vapnik and Alexey Chervonenkis in 1963, but only became really popular in the mid 1990s after a few modifications to the original method were made and computing power had advanced sufficiently. The name of the game for SVMs is to draw an optiminally separately hyperplane between classes identified in training data. Such a hyperplane would linearly distinguish among groups in the training set such that new predictions of unlabelled data could be made. Multiple embellishments on the vanilla SVM have been made over the years which allow nonlinear discrimination and even potentially mislabelled training examples (i.e. soft margins). We will use the iris dataset to try to classify individual datapoints into their representative species.\n",
    "\n",
    "In this case we will start by using the complete iris dataset as our training set. First we will fit a linear SVM to it and then visualize the decision surface. We will do this at first with only the sepal features (sepal length and width) and then will include the petal features later.\n",
    "\n",
    "First let me import some helpful stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# bring in some helper functions\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with the iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first two features\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target # target here is 0, 1, 2\n",
    "\n",
    "\n",
    "\n",
    "#Support Vector Machine\n",
    "C = 100.0  # SVM regularization parameter\n",
    "clf = svm.SVC(kernel='linear', C=C)\n",
    "         \n",
    "model = clf.fit(X, y)\n",
    "\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "ax = plt.gca()\n",
    "plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(\"linear SVM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above we have plotted the same data as before, but now having fit a linear SVM to the training set. You can see the data as they fall along the linear, separating hyperplane among classes.\n",
    "\n",
    "So we can see that while setosa is easier to classifiy, versicolor and virginica have a bunch of overlap in the SVM we just trained. Before we move on let's quantify the accuracy of our SVM by making it do prediction. For the sake of brevity we will just do prediction from the training set we already used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "preds = clf.predict(X)\n",
    "cm = confusion_matrix(y, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df_iris.species.unique())\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table we just output is often called the *confusion matrix* in the ML work. On columns we have the true value of the individual data points, on the rows the predicted value. We can see that 98% of setosa examples are correctly classified, but only a much smaller percentage of versicolor and virginica examples are correctly classified. This fits with what we saw on our decision surfaces.\n",
    "\n",
    "### Moar features! Moar kernels!\n",
    "\n",
    "To improve our accuracy a bit lets first use all of the features of the iris dataset, both the sepal measurements and the petal measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the features\n",
    "X = iris.data\n",
    "y = iris.target # target here is 0, 1, 2\n",
    "\n",
    "\n",
    "\n",
    "#Support Vector Machine\n",
    "C = 100.0  # SVM regularization parameter\n",
    "clf = svm.SVC(kernel='linear', C=C)\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "preds = clf.predict(X)\n",
    "cm = confusion_matrix(y, preds, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df_iris.species.unique())\n",
    "disp.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So including the petal measurements improved things quite a bit. We are now seeing classification accuracies of 96% and 98% for versicolor and virginica respectively. \n",
    "\n",
    "Can we do even better? Well there isn't much room for improvement but a simple thing we can try is moving from a linear hyperplane for the SVM to a nonlinear decision surface. In the context of SVMs this can be done by changing what is called the *kernel*. Without getting into the maths behind this, the kernel creates an implicit mapping of the input features to a different coordinate space, one that may allow easier separation of the classes in the data. The most popular choice for a nonlinear kernel in the context of SVMs is the radial basis function kernel. Let's try it out for our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=10) # this version takes two parameters\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "preds = clf.predict(X)\n",
    "cm = confusion_matrix(y, preds, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df_iris.species.unique())\n",
    "disp.plot()\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've gone from a 2% misclassification rate to 0% with the change in kernel. Not bad. At this point I'd encourage you to play around with the other kernels that the `sklearn` SVM implementation has, for instance the sigmoid kernel. Does it do better or worse?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at those decision surface visualizations that we created earlier. Note at the outset that we created the earlier ones based on a SVM that we trained with two features: sepal length and sepal width. Let's step back to that two feature input but visualize what a decision surface looks like using the radial basis function kernel. I'm going to turn up a parameter called gamma which basically controls how wiggly our kernel will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target # target here is 0, 1, 2\n",
    "\n",
    "#Support Vector Machine\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=10) # this version takes two parameters\n",
    "model = clf.fit(X, y)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "ax = plt.gca()\n",
    "plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(\"radial basis SVM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see now that our decision surface is altered in comparison to earlier. In particular with the RBF kernel and high gamma the SVM is doing a bit of craziness in trying to define the decision surface. \n",
    "\n",
    "### Lies and damn lies\n",
    "\n",
    "So far, and mostly out of laziness, we've been using the entire Iris dataset for training AND testing. This is bad ML practice, because by testing our performance on the entire training set we are getting an over estimate of how well we are doing. So let's do the right thing shall we? Let's split our dataset into one half for training and the other half for testing the performance of our trained classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.2, random_state=666)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a *balanced* training and testing sets. They are said to be balanced in that equal numbers of examples from each class are present in the datasets. This is a key component to training a good classifier in that an unbalanced trainingset can yield a falsely accurate classifier that simply guesses the most frequent class of example present. \n",
    "\n",
    "Next let's retrain our svm on the training split of our data and then look at it's accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "clf = svm.SVC(kernel='rbf', C=1, gamma=10) # this version takes two parameters\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "cm = confusion_matrix(y_test, preds, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=df_iris.species.unique())\n",
    "disp.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, doing the proper thing of training with one part of our data and then evaluating our fit on another has affected our performance, but just a little. Seems like our SVM classifier is doing quite well on these data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an SVM to classify genomes to country or origin\n",
    "Now let's try to apply these ideas to genome data. Let's use that arabidopsis dataset we had been messing with before and bring in some meta data that I have lying around that will tell us about where the samples came from. \n",
    "\n",
    "Our goal here will be to treat country of origin of the plant as our target and its associate genotype (genome sequence) as the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"data/araTha.hdf5\", 'r')\n",
    "# metadata\n",
    "meta = pd.read_csv(\"data/araTha_meta.csv\")\n",
    "meta.index = meta.pk\n",
    "\n",
    "# data clean up here\n",
    "#\n",
    "countries = {\n",
    "    'GER':'Germany',\n",
    "    'US':'United States',\n",
    "    'UK':'United Kingdom',\n",
    "    'POR':'Portugal',\n",
    "    'LIB':'Libya',\n",
    "    'SUI':'Switzerland',\n",
    "    'NED':'Netherlands',\n",
    "    'DEN':'Denmark',\n",
    "    'GRE':'Greece',\n",
    "    'BUL':'Bulgaria',\n",
    "    'CRO':'Croatia'\n",
    "}\n",
    "meta.country = meta.country.replace(countries)\n",
    "meta.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are the samples from?\n",
    "let's quickly take a look at where these samples were collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location=[0, 40], zoom_start=1)\n",
    "for index, row in meta.iterrows():\n",
    "    if not np.isnan(row.longitude):\n",
    "        # print([row.longitude, row.latitude])\n",
    "        folium.Marker([row.latitude, row.longitude],popup=row.country).add_to(m)\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so there are many countries that have very few samples. machine learning won't work in this situtation-- indeed we rely on have *big data* i.e, lots of training examples for each class so that our algorithm can find the decision surface to an adequate degree.\n",
    "\n",
    "Here's what the counts look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.country.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's filter this to the top 6 countries and only keep those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_list = list(meta.country.value_counts()[:6].index)\n",
    "top6 = meta[meta.country.isin(keep_list)]\n",
    "keep_dict = {k: v for v, k in enumerate(keep_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next let's match up the genotypes with the metadata, we'll use the `hdf5` file we elluded to earlier. The basic workflow here is that we will load the genotype data into a pandas dataframe, then we will match up the samples in the genotype data with the samples in the metadata. We further thin the genotype data to every 10th SNP as I want it to run fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geno_group = f['genotype']\n",
    "# artifically thinning to every 10th SNP as I want it to run fast\n",
    "# for class\n",
    "thin = 10\n",
    "chromosomes = geno_group['col_header']['chrom'][::thin]\n",
    "positions = geno_group['col_header']['pos'][::thin]\n",
    "geno_df = pd.DataFrame(geno_group['matrix'][:,::thin], columns=positions, \n",
    "                       index=geno_group['row_header']['sample_ID'][:],\n",
    "                      dtype='float64')\n",
    "print(f\"shape of geno_df: {geno_df.shape}\")\n",
    "\n",
    "# get intersection index array\n",
    "sample_idx = geno_df.index.intersection(top6.index)\n",
    "print(f\"shape of intersection is {sample_idx.shape}\")\n",
    "snps = geno_df.loc[sample_idx]\n",
    "print(f\"shape of snp matrix {snps.shape}\")\n",
    "geno_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next let's create the targets for our classifier. We will use the country of origin for each sample as the target, again matching up the samples in the genotype data with the samples in the metadata in our restricted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_targets = top6.country.loc[sample_idx]\n",
    "country_targets.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay now to use machine learning I'm going to have to numerically encode these country labels. I'm going to use that convenient dictionary I created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_targets = [keep_dict[x] for x in country_targets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so I've got my targets and I've got my matrix of SNPs-- let's make the machines learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    snps, number_targets, test_size=0.2, random_state=666)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1,) # this version takes two parameters\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "cm = confusion_matrix(y_test, preds, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=country_targets.unique())\n",
    "disp.plot()\n",
    "\n",
    "plt.xticks(rotation=70)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall curves\n",
    "A great way to characterize the performance of a classifier is with precision and recall. So a couple of definitions\n",
    "\n",
    "Precision = # true positives / (# true positives + # false positives)\n",
    "\n",
    "Recall = # true positives / (# true positives + # false negatives)\n",
    "\n",
    "recall can also be thought of as the true positive rate. We want classifiers to have high precision *and* high recall. `sklearn` will calculate all of this for us from our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(x_test)\n",
    "print(sklearn.metrics.classification_report(y_test, preds, target_names=keep_list))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can wee that we really do have high precision and recall in most cases. This output also gives the so-called F1 score, which is simply the harmonic mean between precision and recall. \n",
    "\n",
    "Let's plot our Precision Recall curve for this classifier. Note the code below is a bit involved, but I want to just focus on the result. \n",
    "\n",
    "(note this code block is a bit slow to run ~3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# Use label_binarize to be multi-label like settings\n",
    "Y = label_binarize(number_targets, classes=list(range(6)))\n",
    "n_classes = Y.shape[1]\n",
    "\n",
    "# Split into training and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(snps, Y, test_size=0.2,\n",
    "                                                    random_state=666)\n",
    "\n",
    "# We use OneVsRestClassifier for multi-label prediction\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Run linear SVM\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', C=1))\n",
    "classifier.fit(x_train, y_train)\n",
    "y_score = classifier.decision_function(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                        y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "    y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_score,\n",
    "                                                     average=\"micro\")\n",
    "\n",
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    f'Average precision score, micro-averaged over all classes: AP={round(average_precision[\"micro\"],2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NICE\n",
    "so this classifier looks very good from a Precision Recall perspective. We have high precision at all recalls basically. Classifying Arabidopsis genomes turns out to be a decently easy problem for SVMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest approaches\n",
    "A completely different approach to supervised machine learning is the Random Forest. Rather than focus on one, good predictor, RFs borrow strength among a number of so-called weak learners and combine those in an ensemble to make one good prediction. As we say in the lecture slides these weak learners are individual, randomized decision trees. We can fit RF models on the SNP data above quite easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    snps, number_targets, test_size=0.2, random_state=666)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, preds, normalize='true'), columns=keep_list, index=keep_list)\n",
    "sns.heatmap(cm, annot=True,cmap=\"plasma\")\n",
    "preds = clf.predict(x_test)\n",
    "print(sklearn.metrics.classification_report(y_test, preds, target_names=keep_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so RFs in the case are doing slightly worse than SVMs. Can we fix this up a bit by changing the number of trees in our ensemble?\n",
    "\n",
    "I'll try changing two things-- the number of trees we use in the forest and the metric for splitting trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "nt = [100,200,500,1000,2000,5000]\n",
    "for n in nt:\n",
    "    clf = RandomForestClassifier(n_estimators=n)\n",
    "    model = clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    r.append(sklearn.metrics.precision_recall_fscore_support(y_test, preds, average='weighted')[2])\n",
    "r2 = []\n",
    "for n in nt:\n",
    "   # print(n)\n",
    "    clf = RandomForestClassifier(n_estimators=n, criterion='entropy')\n",
    "    model = clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_test)\n",
    "    r2.append(sklearn.metrics.precision_recall_fscore_support(y_test, preds, average='weighted')[2])\n",
    "plt.plot(nt,r)\n",
    "plt.plot(nt,r2)\n",
    "plt.legend(['gini','entropy'])\n",
    "plt.xlabel(\"number of trees\")\n",
    "plt.ylabel(\"F1-score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in this case it looks like Gini impurity is doing a bit better than entropy, and that 2000 trees is a good number. Let's try to use this model to classify our Arabidopsis genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    snps, number_targets, test_size=0.2, random_state=666\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=2000, criterion=\"gini\")\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_test, preds, normalize=\"true\"),\n",
    "    columns=keep_list,\n",
    "    index=keep_list,\n",
    ")\n",
    "sns.heatmap(cm, annot=True, cmap=\"plasma\")\n",
    "preds = clf.predict(x_test)\n",
    "print(sklearn.metrics.classification_report(y_test, preds, target_names=keep_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which SNPs are important? Feature importance\n",
    "\n",
    "Which a RF is a tree based model, we can get a sense of which SNPs are important by looking at the feature importance scores.\n",
    "These scores are calculated by looking at how much the tree splits on a given SNP and then averaging over all of the trees in the forest.\n",
    "They give us a sense of which SNPs (or more generally features) are important for the classification task.\n",
    "\n",
    "We can get these scores from the `feature_importances_` attribute of the `RandomForestClassifier` object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature importance scores\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# plot the feature importance scores as a bar plot; add a histogram of the feature importance scores in a separate plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.bar(range(len(importances)), importances)\n",
    "ax1.set_xlabel(\"SNP\")\n",
    "ax1.set_ylabel(\"Feature Importance\")\n",
    "ax2.hist(importances, bins=20)\n",
    "ax2.set_xlabel(\"Feature Importance\") \n",
    "ax2.set_ylabel(\"Count\")\n",
    "# rotate the x-axis labels\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so really, very few SNPs are important for this classification task. This is a good thing, as it means that the RF is not simply memorizing the data, but rather is using the underlying structure of the data to make predictions.\n",
    "\n",
    "Let's look at the relationship between feature importance and frequency in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most important SNPs\n",
    "top10 = np.argsort(importances)[-10:]\n",
    "# plot the top 10 most important SNPs versus frequency in the matrix\n",
    "freqs = snps.sum(axis=0)\n",
    "plt.scatter(importances, freqs)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So we can see that the most important SNPs are actually at intermediate frequencies. This makes sense, as the RF is using the underlying structure of the data to make predictions, and so it is important to have SNPs that are at intermediate frequencies, so that the RF can learn the underlying structure of the data.\n",
    "\n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with simulated training sets -- a population genetics classification problem\n",
    "\n",
    "All of the examples above have been using empirical data (i.e. data that come from the real world and have real world labels). However in evolutionary biology, and especially  population genetics, it's oftern impossible to get ground truth labels for our data. Consider for instance the case of \"knowing\" the population history of a species-- we can't go back in time and know the truth. In this case we can use simulated data to train our classifier! The idea, generally speaking, is that we can simulate data from a variety of models and then use the simulated data to train our classifier. We can then use the classifier to predict the labels of new, unseen data. \n",
    "\n",
    "One of the classic tasks in population genetics is to use genetic variation data to test among competing \n",
    "demographic hypotheses. For instance Tajima’s famous D statistic was originally proferred as a way to look\n",
    "for population expansion (or contraction) through a frequentist based hypothesis test. \n",
    "Rather than do hypothesis testing, we can frame this same sort of task as machine learning classification. In this section we will do exactly that– we will build a RF classifier to distinguish between a population with static population size (sometimes called an equilibrium population) and a population that has undergone population growth. Rather than train our classifier using empirical data from populations, we will be training using coalescent simulations. \n",
    "\n",
    "To do this we will need to simulate some data. We will use the `stdpopsim` package to do this, which is a package that allows us to simulate data from a variety of organisms/demographic models. You can read more about it [here](https://stdpopsim.readthedocs.io/en/stable/). \n",
    "\n",
    "For our purposes we will simulate data from the *Homo sapiens* species. We will use the `stdpopsim` package to simulate data from a population that has undergone a recent population expansion as well as a static population. We will then use the `tskit` package to load the simulated data into a tree sequence, and calulated summary statistics from the tree sequence which will serve as features for our classifier. \n",
    "\n",
    "## Simulation aside -- the joys of `stdpopsim`\n",
    "\n",
    "stdpopsim is a really useful package for simulating data from a variety of organisms. It is built on top of the `msprime` package, which is a really fast coalescent simulator and can simulate data for a wide variety of demographic models with empirical recombination maps, realistic mutation rates, and more. \n",
    "\n",
    "The first thing we do in the standard `stdpopsim` workflow is to get the species we want to simulate data from. Here we will get the *Homo sapiens* species object. We will then get a list of all of the demographic models that are available for this species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stdpopsim\n",
    "\n",
    "species = stdpopsim.get_species(\"HomSap\")\n",
    "for x in species.demographic_models:\n",
    "    print(x.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay there are a lot of models here. We will use the model `Africa_1T12` model, which is for one population that has changed population size quite dramatically. We can plot what this model looks like with the `plot_demography` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demes\n",
    "import demesdraw\n",
    "\n",
    "# Get the Arabidopsis thaliana species and the African model\n",
    "species = stdpopsim.get_species(\"HomSap\")\n",
    "model = species.get_demographic_model(\"Africa_1T12\")\n",
    "\n",
    "# Get the demes graph directly from the model\n",
    "graph = model.model.to_demes()\n",
    "demesdraw.tubes(model.model.to_demes())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets simulate some data from this model. We will simulate 10 diploid individuals from the population, with 1e6 sites. We will then use the `tskit` package to load the simulated data into a tree sequence, and calulated summary statistics from the tree sequence which will serve as features for our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Arabidopsis thaliana species and the African model\n",
    "species = stdpopsim.get_species(\"HomSap\")\n",
    "model = species.get_demographic_model(\"Africa_1T12\")\n",
    "contig = species.get_contig(\n",
    "    \"chr1\", \n",
    "    left=19e6, \n",
    "    right=20e6, \n",
    "    mutation_rate=model.mutation_rate\n",
    ")\n",
    "samples = {\"AFR\": 10}\n",
    "engine = stdpopsim.get_engine(\"msprime\")\n",
    "ts = engine.simulate(model, contig, samples)\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this yield a `TreeSequence` object that we can do all sorts of things with. We can plot the tree sequence, examine it's nodes and edges, and more. Here we will just calculate some summary statistics from the tree sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate these statistics in windows\n",
    "# Let's use 10 windows across the sequence\n",
    "windows = np.linspace(0, ts.sequence_length, num=11)\n",
    "diversity_windows = ts.diversity(windows=windows)\n",
    "tajimas_d_windows = ts.Tajimas_D(windows=windows)\n",
    "segregating_sites_windows = ts.segregating_sites(windows=windows)\n",
    "\n",
    "# concatenate these into a single array\n",
    "stats = np.concatenate([diversity_windows, tajimas_d_windows, segregating_sites_windows])\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's replicate this for 1000 simulations\n",
    "sims = []\n",
    "for i in range(1000):\n",
    "    ts = engine.simulate(model, contig, samples)\n",
    "    windows = np.linspace(0, ts.sequence_length, num=11)\n",
    "    diversity_windows = ts.diversity(windows=windows)\n",
    "    tajimas_d_windows = ts.Tajimas_D(windows=windows)\n",
    "    segregating_sites_windows = ts.segregating_sites(windows=windows)\n",
    "    stats = np.concatenate([diversity_windows, tajimas_d_windows, segregating_sites_windows])\n",
    "    sims.append(stats)\n",
    "sims = np.array(sims)\n",
    "sims.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing for the static population size model. First we will set up the model and then plot the demes graph side by side with the growth model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the same simulation, but with the static population size model\n",
    "model_static = stdpopsim.PiecewiseConstantSize(species.population_size)\n",
    "samples = {\"pop_0\": 10}  # this is for the static population size model,\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# side by side plot of the two models   \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "# Get the demes graph directly from the model\n",
    "graph = model_static.model.to_demes()\n",
    "demesdraw.tubes(model_static.model.to_demes(), ax=ax1)\n",
    "graph = model.model.to_demes()\n",
    "demesdraw.tubes(model.model.to_demes(), ax=ax2)\n",
    "# add a title to each plot\n",
    "ax1.set_title(\"Static Population Size\")\n",
    "ax2.set_title(\"Population Growth\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now simulate 1000 replicates of the static population size model and record the summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_static = []\n",
    "for i in range(1000):\n",
    "    ts = engine.simulate(model_static, contig, samples)\n",
    "    windows = np.linspace(0, ts.sequence_length, num=11)\n",
    "    diversity_windows = ts.diversity(windows=windows)\n",
    "    tajimas_d_windows = ts.Tajimas_D(windows=windows)\n",
    "    segregating_sites_windows = ts.segregating_sites(windows=windows)\n",
    "    stats = np.concatenate(\n",
    "        [diversity_windows, tajimas_d_windows, segregating_sites_windows]\n",
    "    )\n",
    "    sims_static.append(stats)\n",
    "sims_static = np.array(sims_static)\n",
    "sims_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the distribution of the statistics for the two models\n",
    "# we can use seaborn to make this look nicer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# plot the diversity distribution in the first window\n",
    "sns.histplot(sims[:,0], label=\"growth\", alpha=0.5, ax=ax1)\n",
    "sns.histplot(sims_static[:,0], label=\"static\", alpha=0.5, ax=ax1)\n",
    "ax1.set_title(\"Diversity\")\n",
    "ax1.legend()\n",
    "\n",
    "# plot the Tajima's D distribution in the first window\n",
    "sns.histplot(sims[:,11], label=\"growth\", alpha=0.5, ax=ax2)\n",
    "sns.histplot(sims_static[:,11], label=\"static\", alpha=0.5, ax=ax2)\n",
    "ax2.set_title(\"Tajima's D\")\n",
    "ax2.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so obviously the growth model has a much more skewed distribution of the statistics. We can use these statistics as features for our classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we need to to create a label for our data. We will use the `growth` label for the growth model and the `static` label for the static model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a label for our data, 0 for growth and 1 for static\n",
    "labels = np.concatenate([np.zeros(1000), np.ones(1000)])\n",
    "labels\n",
    "\n",
    "# now lets concatenate our statistics into one array\n",
    "data = np.concatenate([sims, sims_static])\n",
    "data.shape\n",
    "\n",
    "# let's split this into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=666)\n",
    "\n",
    "# finally let's examine the shapes of our training and testing data\n",
    "print(f\"training data shape: {x_train.shape}\")\n",
    "print(f\"testing data shape: {x_test.shape}\")\n",
    "print(f\"training labels shape: {y_train.shape}\")\n",
    "print(f\"testing labels shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great! we have our training and testing data. Now we can use a random forest classifier to classify the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a random forest classifier\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = clf.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "# evaluate the classifier\n",
    "print(sklearn.metrics.classification_report(y_test, preds))\n",
    "\n",
    "# let's plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, preds, normalize='true')\n",
    "sns.heatmap(cm, annot=True, cmap=\"plasma\", xticklabels=[\"growth\", \"static\"], yticklabels=[\"growth\", \"static\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obviously that worked well! We can see that the classifier is able to distinguish between the two models with high precision and recall. \n",
    "\n",
    "# A regression problem\n",
    "\n",
    "Let's move on to a repression problem. In this case we aren't interested in classifying the data, but rather in predicting a continuous variable. We will use a similar set up as above, relying on `stdpopsim` to simulate data from a two population, isolatioin with migration model. We will aim to predict the time of the split between the two populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolation with migration model\n",
    "\n",
    "species = stdpopsim.get_species(\"HomSap\")\n",
    "samples = {\"pop1\": 10, \"pop2\": 10}\n",
    "\n",
    "# here's how we set up the IM model\n",
    "model = stdpopsim.IsolationWithMigration(\n",
    "    NA=10000, # ancestral population size\n",
    "    N1=10000, # population 1 size\n",
    "    N2=10000, # population 2 size\n",
    "    T=1, # time of split, in generations\n",
    "    M12=1, # migration rate from pop1 to pop2\n",
    "    M21=1 # migration rate from pop2 to pop1\n",
    ")\n",
    "contig = species.get_contig(\n",
    "    \"chr1\", \n",
    "    left=10e6, \n",
    "    right=20e6, \n",
    "    mutation_rate=model.mutation_rate\n",
    ")\n",
    "ts = engine.simulate(model, contig, samples)\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay let's set up replicates of this simulation, varying the time of the split. We will simulate 1000 random times of the split between 0 and 10000 generations. For summary statistics we will use the joint [allele frequency spectrum](https://en.wikipedia.org/wiki/Allele_frequency_spectrum).\n",
    "\n",
    "The allele frequency spectrum is a way to summarize the genetic variation in a population. It is a vector of length 2N, where N is the number of diploid samples in the population, that gives the frequency of each allele in the sample. In the case of more than one population, we can calculate the joint allele frequency spectrum, which is a matrix of size 2N x 2N. \n",
    "\n",
    "We can calculate the joint allele frequency spectrum for our tree sequence using the `allele_frequency_spectrum` function and make a plot of it. We will do this for two different times of the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "# comparison of allele frequency spectra\n",
    "\n",
    "# T = 1\n",
    "model1 = stdpopsim.IsolationWithMigration(\n",
    "    NA=1000,  # ancestral population size\n",
    "    N1=1000,  # population 1 size\n",
    "    N2=1000,  # population 2 size\n",
    "    T=1,  # time of split, in generations\n",
    "    M12=5e-4,  # migration rate from pop1 to pop2\n",
    "    M21=5e-4,  # migration rate from pop2 to pop1\n",
    ")\n",
    "ts1 = engine.simulate(model, contig, samples)\n",
    "\n",
    "jAFS1 = ts1.allele_frequency_spectrum(\n",
    "    sample_sets=[\n",
    "        ts1.samples(population=0), \n",
    "        ts1.samples(population=1)\n",
    "    ],\n",
    "    span_normalise=False,\n",
    "    polarised=True,\n",
    ")\n",
    "# flip the y axis\n",
    "jAFS1 = jAFS1[::-1, :]\n",
    "\n",
    "# T = 1000\n",
    "model2 = stdpopsim.IsolationWithMigration(\n",
    "    NA=1000,  # ancestral population size\n",
    "    N1=1000,  # population 1 size\n",
    "    N2=1000,  # population 2 size\n",
    "    T=1000,  # time of split, in generations\n",
    "    M12=5e-4,  # migration rate from pop1 to pop2\n",
    "    M21=5e-4,  # migration rate from pop2 to pop1\n",
    ")\n",
    "ts2 = engine.simulate(model2, contig, samples)\n",
    "jAFS2 = ts2.allele_frequency_spectrum(\n",
    "    sample_sets=[ts2.samples(population=0), ts2.samples(population=1)],\n",
    "    span_normalise=False,\n",
    "    polarised=True,\n",
    ")\n",
    "# flip the y axis\n",
    "jAFS2 = jAFS2[::-1, :]\n",
    "\n",
    "# plot the joint allele frequency spectra side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "ax1.imshow(jAFS1, cmap=\"viridis\", norm=LogNorm())\n",
    "ax1.set_title(\"T = 1\")\n",
    "\n",
    "ax2.imshow(jAFS2, cmap=\"viridis\", norm=LogNorm())\n",
    "ax2.set_title(\"T = 1000\")\n",
    "\n",
    "plt.show()\n",
    "# Create tick positions and labels that increment by 5\n",
    "tick_positions = np.arange(0, jAFS1.shape[0], 5)\n",
    "tick_labels = np.arange(jAFS1.shape[0])[::-1][::5]\n",
    "\n",
    "# Set ticks for both axes\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_yticks(tick_positions)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(tick_positions)\n",
    "    ax.set_xticklabels(tick_positions)\n",
    "\n",
    "# Add colorbars\n",
    "plt.colorbar(ax1.images[0], ax=ax1)\n",
    "plt.colorbar(ax2.images[0], ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so these look very different! populations with short split times have very similar allele frequency spectra, while populations with long split times have very different allele frequency spectra. \n",
    "\n",
    "Let's use these allele frequency spectra as features for our regression model. We will simulate 1000 random times of the split between 0 and 10000 generations, generate the joint allele frequency spectra for each simulation, flatten it into a vector, and then use that as our feature vector. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 1000 random times of the split between 0 and 10000 generations\n",
    "times = np.random.randint(0, 10000, size=1000)\n",
    "\n",
    "# simulate the data for each time\n",
    "sims_stats = []\n",
    "for t in times:\n",
    "    model = stdpopsim.IsolationWithMigration(NA=1000, N1=1000, N2=1000, T=t, M12=5e-4, M21=5e-4)\n",
    "    ts = engine.simulate(model, contig, samples)\n",
    "    # calculate the joint allele frequency spectrum\n",
    "    jAFS = ts.allele_frequency_spectrum(\n",
    "        sample_sets=[\n",
    "            ts.samples(population=0),\n",
    "            ts.samples(population=1),\n",
    "        ],\n",
    "        span_normalise=False,\n",
    "        polarised=True,\n",
    "    )\n",
    "    # flatten the joint allele frequency spectrum\n",
    "    jAFS = jAFS.flatten()\n",
    "    sims_stats.append(jAFS)\n",
    "sims_stats = np.array(sims_stats)\n",
    "sims_stats.shape\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit a linear regression model to the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# do a train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(sims_stats, times, test_size=0.2, random_state=666)\n",
    "\n",
    "# fit a linear regression model to the training data\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "preds = model.predict(x_test)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "results_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": preds})\n",
    "\n",
    "# plot the predictions vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=results_df, x=\"Actual\", y=\"Predicted\")\n",
    "plt.plot([0, 10000], [0, 10000], \"r--\")  # Add perfect prediction line\n",
    "plt.xlabel(\"Actual Time\")\n",
    "plt.ylabel(\"Predicted Time\")\n",
    "plt.title(\"Predicted vs Actual Split Times\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print various regression metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "r2 = r2_score(y_test, preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.3f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not bad! We can see that the model is able to predict the time of the split with a reasonable degree of accuracy. let's see if we can do better with a more complex model. \n",
    "\n",
    "\n",
    "We will use a random forest regressor to predict the time of the split. We will use the same summary statistics as before, but this time we will use a random forest regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a SVM regressor to the data\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. First scale your data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train)\n",
    "X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# 2. Then fit the SVR with some basic parameter tuning\n",
    "model = SVR(kernel=\"linear\")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "preds = model.predict(X_test_scaled)\n",
    "\n",
    "# plot the predictions vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plot the actual values, untransformed\n",
    "sns.scatterplot(x=y_test, y=preds)\n",
    "plt.plot([0, 10000], [0, 10000], \"r--\")  # Add perfect prediction line\n",
    "plt.xlabel(\"Actual Time\")\n",
    "plt.ylabel(\"Predicted Time\")\n",
    "plt.title(\"Predicted vs Actual Split Times\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Calculate metrics\n",
    "r2 = r2_score(y_test, preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.3f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is doing slightly better than the linear regression model, which itself was already doing pretty well!\n",
    "\n",
    "Next lets see if you can try to predict the time of the split and the migration rate between the two populations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 1000 random times of the split between 0 and 10000 generations\n",
    "times = np.random.randint(0, 10000, size=1000)\n",
    "migration_rates = np.random.uniform(0, 0.05, size=1000)\n",
    "# simulate the data for each time\n",
    "sims_stats = []\n",
    "for t, m in zip(times, migration_rates):\n",
    "    model = stdpopsim.IsolationWithMigration(\n",
    "        NA=1000, N1=1000, N2=1000, T=t, M12=m, M21=m\n",
    "    )\n",
    "    ts = engine.simulate(model, contig, samples)\n",
    "    # calculate the joint allele frequency spectrum\n",
    "    jAFS = ts.allele_frequency_spectrum(\n",
    "        sample_sets=[\n",
    "            ts.samples(population=0),\n",
    "            ts.samples(population=1),\n",
    "        ],\n",
    "        span_normalise=False,\n",
    "        polarised=True,\n",
    "    )\n",
    "    # flatten the joint allele frequency spectrum\n",
    "    jAFS = jAFS.flatten()\n",
    "    sims_stats.append(jAFS)\n",
    "sims_stats = np.array(sims_stats)\n",
    "sims_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have two targets, the time of the split and the migration rate between the two populations. We can use a multi-task learning approach to predict both of these targets at the same time. Again we will use a SVM regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# First do a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sims_stats,\n",
    "    np.column_stack((times, migration_rates)),\n",
    "    test_size=0.2,\n",
    "    random_state=666,\n",
    ")\n",
    "\n",
    "# Use RobustScaler instead of StandardScaler\n",
    "scaler_X = RobustScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Log transform the targets (adding small constant to avoid log(0))\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "# Scale the log-transformed targets\n",
    "scaler_y = RobustScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_log)\n",
    "\n",
    "# Fit model with RBF kernel and increased regularization\n",
    "model = MultiOutputRegressor(SVR(kernel=\"rbf\", C=1.0, epsilon=0.1, gamma=\"scale\"))\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Make predictions and inverse transform\n",
    "preds_scaled = model.predict(X_test_scaled)\n",
    "preds_log = scaler_y.inverse_transform(preds_scaled)\n",
    "preds = np.expm1(preds_log)  # Inverse of log1p\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(y_test[:, 0], preds[:, 0])\n",
    "ax1.plot([0, 10000], [0, 10000], \"r--\")\n",
    "ax1.set_xlabel(\"Actual Time\")\n",
    "ax1.set_ylabel(\"Predicted Time\")\n",
    "ax1.set_title(\"Predicted vs Actual Split Times\")\n",
    "\n",
    "ax2.scatter(y_test[:, 1], preds[:, 1])\n",
    "ax2.plot([0, 0.05], [0, 0.05], \"r--\")\n",
    "ax2.set_xlabel(\"Actual Migration Rate\")\n",
    "ax2.set_ylabel(\"Predicted Migration Rate\")\n",
    "ax2.set_title(\"Predicted vs Actual Migration Rates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics\n",
    "r2_times = r2_score(y_test[:, 0], preds[:, 0])\n",
    "r2_migration_rates = r2_score(y_test[:, 1], preds[:, 1])\n",
    "\n",
    "print(f\"R² Score for Time: {r2_times:.3f}\")\n",
    "print(f\"R² Score for Migration Rate: {r2_migration_rates:.3f}\")\n",
    "\n",
    "rmse_times = np.sqrt(mean_squared_error(y_test[:, 0], preds[:, 0]))\n",
    "rmse_migration_rates = np.sqrt(mean_squared_error(y_test[:, 1], preds[:, 1]))\n",
    "\n",
    "print(f\"Root Mean Square Error for Time: {rmse_times:.3f}\")\n",
    "print(f\"Root Mean Square Error for Migration Rate: {rmse_migration_rates:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locator -- a deep learning approach to geolocation\n",
    "\n",
    "In 2020 we released a machine learning package called [Locator](https://github.com/kr-colab/locator) that uses deep learning to predict the geographic location of individuals from their DNA sequence data. \n",
    "\n",
    "<!-- insert the locator paper image here -->\n",
    "<img src=\"imgs/locator_paper.jpg\" alt=\"Locator paper image\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper introduced a new way to do geolocation that was much more accurate than anything that had been done before. In particular, it used a fully connected neural network (a.k.a a multi-layer perceptron) to learn the relationship between DNA sequence data and geographic location. \n",
    "\n",
    "The idea was to use a training set of DNA sequences from a set of known geographic locations, and then use a neural network to learn the relationship between the DNA sequence data and the geographic location. \n",
    "This is similar to what we did above using the SVM and RF models, but rather than use classification we are using regression-- that is to say we are predicting the continuous variables of latitude and longitude. \n",
    "\n",
    "Here's what the neural network architecture looks like:\n",
    "\n",
    "<!-- insert the locator neural network architecture image here -->\n",
    "<img src=\"imgs/locator_network.jpg\" alt=\"Locator neural network architecture image\" width=\"500\">\n",
    "\n",
    "A key idea here is that recombination will breakup the genealogical relationships along the chromsomes such that some portions of the genome will have a set of ancestors in some locations and other portions will have a set of ancestors in other locations. We can use this information to derived the _uncertainty_ in the location of an individual, along with predicting where certain portions of the genome are derived from. \n",
    "\n",
    "Let's see how this works in practice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locator in action -- predicting the location of Arabidopsis thaliana\n",
    "\n",
    "We will use the `locator` package to predict the location of Arabidopsis thaliana. \n",
    "\n",
    "I'll add that we are using a brand new version of the `locator` package that I've been working on that is much more modular than the current release. The release version is available on github at https://github.com/kr-colab/locator.git and it is a standalone script that you can run from the command line. \n",
    "\n",
    "The version we will work with is also available on github at https://github.com/kr-colab/locator.git@module and it is a python package that you can import into your python scripts, although user beware, it is still under active development and the API may change. \n",
    "\n",
    "We start by importing the package and some of the functions we will need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from locator import Locator, plot_predictions, plot_error_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next let's set up the Arabidopsis thaliana data that we will use to train the model. This is the same data that we used above to train the SVM and RF models, but we will use a smaller subset of the data for this example due to time constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabidopsis data\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "f = h5py.File(\"data/araTha.hdf5\", \"r\")\n",
    "# metadata\n",
    "meta = pd.read_csv(\"data/araTha_meta.csv\")\n",
    "meta.index = meta.pk\n",
    "\n",
    "# data clean up here\n",
    "countries = {\n",
    "    \"GER\": \"Germany\",\n",
    "    \"US\": \"United States\",\n",
    "    \"UK\": \"United Kingdom\",\n",
    "    \"POR\": \"Portugal\",\n",
    "    \"LIB\": \"Libya\",\n",
    "    \"SUI\": \"Switzerland\",\n",
    "    \"NED\": \"Netherlands\",\n",
    "    \"DEN\": \"Denmark\",\n",
    "    \"GRE\": \"Greece\",\n",
    "    \"BUL\": \"Bulgaria\",\n",
    "    \"CRO\": \"Croatia\",\n",
    "}\n",
    "meta.country = meta.country.replace(countries)\n",
    "\n",
    "# keep subset of european countries only\n",
    "euro_countries_list = [\n",
    "    \"France\",   \n",
    "    \"United Kingdom\",\n",
    "    \"Czech Republic\",\n",
    "    \"Austria\",\n",
    "    \"Sweden\",\n",
    "    \"Germany\",\n",
    "    \"Belgium\",\n",
    "    \"Netherlands\",\n",
    "    \"Norway\",\n",
    "    \"Switzerland\",\n",
    "    \"Denmark\",\n",
    "]\n",
    "\n",
    "\n",
    "keep_list = euro_countries_list\n",
    "euro_meta = meta[meta.country.isin(keep_list)]\n",
    "keep_dict = {k: v for v, k in enumerate(keep_list)}\n",
    "geno_group = f[\"genotype\"]\n",
    "\n",
    "\n",
    "\n",
    "# artifically thinning to every 5th SNP as I want it to run fast\n",
    "# for class\n",
    "thin = 20\n",
    "chromosomes = geno_group[\"col_header\"][\"chrom\"][::thin]\n",
    "positions = geno_group[\"col_header\"][\"pos\"][::thin]\n",
    "geno_df = pd.DataFrame(\n",
    "    geno_group[\"matrix\"][:, ::thin],\n",
    "    columns=positions,\n",
    "    index=geno_group[\"row_header\"][\"sample_ID\"][:],\n",
    "    dtype=\"float64\",\n",
    ")\n",
    "print(f\"shape of geno_df: {geno_df.shape}\")\n",
    "\n",
    "# get intersection index array\n",
    "sample_idx = geno_df.index.intersection(euro_meta.index)\n",
    "print(f\"shape of intersection is {sample_idx.shape}\")\n",
    "\n",
    "euro_meta_with_snps = euro_meta.loc[sample_idx]\n",
    "# First cap each country at 50 samples\n",
    "capped_meta = []\n",
    "for country in euro_meta_with_snps.country.unique():\n",
    "    country_data = euro_meta_with_snps[euro_meta_with_snps.country == country]\n",
    "    if len(country_data) > 50:\n",
    "        country_data = country_data.sample(n=50)\n",
    "    capped_meta.append(country_data)\n",
    "\n",
    "capped_euro_meta = pd.concat(capped_meta)\n",
    "\n",
    "# Then do inverse frequency sampling on the capped data\n",
    "freq = capped_euro_meta.country.value_counts()\n",
    "inv_freq = 1 / freq\n",
    "probs = inv_freq / inv_freq.sum()\n",
    "sample_weights = capped_euro_meta.country.map(probs)\n",
    "euro_meta_with_snps = capped_euro_meta.sample(n=250, weights=sample_weights, replace=False)\n",
    "\n",
    "# reset sample index\n",
    "sample_idx = euro_meta_with_snps.index\n",
    "snps = geno_df.loc[sample_idx]\n",
    "print(f\"shape of snp matrix {snps.shape}\")\n",
    "snps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix is the first 5 rows of the SNP matrix for Arabidopsis thaliana. The row indexes are the sample IDs and the column indexes are the SNP positions. \n",
    "\n",
    "Next we will set up the targets for the model. We will use the latitude and longitude of the samples as the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.DataFrame(\n",
    "    {\n",
    "        \"sampleID\": sample_idx.astype(str).values,  # Add .values to get plain array\n",
    "        \"x\": euro_meta_with_snps.longitude.loc[sample_idx],\n",
    "        \"y\": euro_meta_with_snps.latitude.loc[sample_idx],\n",
    "    }\n",
    ").reset_index(drop=True)\n",
    "coords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`locator` uses the labels `x` and `y` to refer to the longitude and latitude of the samples. The sample IDs are used to match the samples to the SNP data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will set up a `Locator` object. The `Locator` object is the main class in the `locator` package and it is used store configuration information, the data, and the model. \n",
    "\n",
    "We will also load the genotype and sample data into the `Locator` object\n",
    "using the `load_genotypes` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Locator(\n",
    "    {\n",
    "        \"out\": \"araTha\",\n",
    "        \"sample_data\": coords,\n",
    "        \"genotype_data\": snps,\n",
    "        \"keras_verbose\": 0, # suppress training output\n",
    "    }\n",
    ")\n",
    "genotypes, samples = locator.load_genotypes()\n",
    "locator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this returns configuration information about the model, including the number of layers, the number of neurons in each layer, and the batch size. It also includes status information about the model, the sample data, and the genotype data. \n",
    "\n",
    "Next we will train a simple model. We will start with this configuration, and hold out 10 samples for testing. There is a convenience function `train_holdout` that will do this for us. It hides all the details of the training process, including the test/train split, the training loop, and the evaluation of the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.train_holdout(genotypes=genotypes, samples=samples, k=10)\n",
    "locator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that was pretty quick! Let's see how well the model did. The `locator` object automatically outputs a quick summary of the training process, showing the final valiation loss and the loss history. When we use the `train_holdout` function, it also returns the sampleIDs that were held out that we can use to evaluate the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get predictions for the held out samples. We will use the `predict_holdout` function to get the predictions. The `locator` object will automatically saved which samples were held out, so we can use those to evaluate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.predict_holdout(return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a dataframe with the predicted coordinates, as well as a little plot of the predictions in comparison to the true coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improve the model\n",
    "\n",
    "Let's next change the model architecture to see if we can improve the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Locator(\n",
    "    {\n",
    "        \"out\": \"araTha\",\n",
    "        \"sample_data\": coords,\n",
    "        \"genotype_data\": snps,\n",
    "        \"keras_verbose\": 0,  # suppress training output\n",
    "        \"nlayers\": 24,\n",
    "        \"width\": 128,\n",
    "        \"batch_size\": 64,\n",
    "        \"min_mac\": 1,\n",
    "    }\n",
    ")\n",
    "genotypes, samples = locator.load_genotypes()\n",
    "locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator.train_holdout(genotypes=genotypes, samples=samples, k=10)\n",
    "locator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so perhaps that's not a big improvement, but it's doing a bit better. we can definitely play with this if there is time during the workshop. \n",
    "\n",
    "Next let's run this model on the full dataset. To do this we will use the `run_holdouts` function. This function will hold out a set number of samples at a time and then predict the coordinates for those samples. It then will loop through the held out samples and predict the coordinates for each set, until all samples have been predicted without using them during training. This will take a few minutes to run. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Locator(\n",
    "    {\n",
    "        \"out\": \"araTha\",\n",
    "        \"sample_data\": coords,\n",
    "        \"genotype_data\": snps,\n",
    "        \"keras_verbose\": 0,\n",
    "        \"nlayers\": 24,\n",
    "        \"width\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"min_mac\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# load genotypes from geno_df\n",
    "genotypes, samples = locator.load_genotypes()\n",
    "ho_preds = locator.run_holdouts(genotypes, samples, k=50, return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training is now done on all of the held out samples, and predictions are made for all samples. Let's plot the results.\n",
    "\n",
    "To do this we will use the `plot_error_summary` function. This function will plot a histogram of the errors, showing the mean error, the median error, and the standard deviation of the error. \n",
    "\n",
    "It will also output a map of the predictions, showing a point where the true location is, connected to a line that shows the predicted location. Predictions are colored by the size of the error, with red being the largest errors and blue being the smallest errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_summary(\n",
    "    predictions=ho_preds,\n",
    "    sample_data=coords,\n",
    "    plot_map=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We see that the model is doing a pretty good job of predicting the location of the samples, with most samples being predicted within a dozens of kilometers of the true location, however there are some samples that are predicted to be quite far away from the true location-- some of these could be due to recent migration events, others could be due to the model not being able to capture the full signal-- we have thinned out a lot of the data, and we have only used a small subset of the genome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uncertainty in the predictions\n",
    "\n",
    "Next let's look at uncertainty in our predictions. There are a few ways to do this that we have implemented in the `locator` package. The fastest way is to use \"jackknife\" resampling. For `locator` this is done by training the model on all the samples once, and then predicting by sampling the genotypes with replacement. Each sample is then associated with multiple predictions from resampling its genotypes. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = Locator(\n",
    "    {\n",
    "        \"out\": \"araTha\",\n",
    "        \"sample_data\": coords,\n",
    "        \"genotype_data\": snps,\n",
    "        \"keras_verbose\": 0,\n",
    "        \"nlayers\": 24,\n",
    "        \"width\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"min_mac\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# load genotypes from geno_df\n",
    "genotypes, samples = locator.load_genotypes()\n",
    "jacknife_preds = locator.run_jacknife_holdouts(\n",
    "    genotypes,\n",
    "    samples,\n",
    "    return_df=True,\n",
    "    k=10,\n",
    "    jacknife_prop=0.25,\n",
    "    n_replicates=500,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jacknifing is done. now let's plot the results. For this we will use the `plot_predictions` function. This function will plot the predictions in comparison to the true coordinates, and it will also plot the uncertainty in the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacknife_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    predictions=jacknife_preds, \n",
    "    locator=locator,\n",
    "    out_prefix=\"jacknife_example\",\n",
    "    plot_map=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## windowed predictions\n",
    "\n",
    "Next let's look at windowed predictions. Windowed predictions are done by dividing the genome into windows of a given size, and then predicting the coordinates for each window. This is perhaps a better way at getting a sense of the uncertainty in the predictions, as it allows us to see how the model performs across the genome, capturing the uncertainty in both the estimator (the neural network) and the evolutionary process (the random sampling of ancestors going back in time).\n",
    "\n",
    "The `run_windows_holdouts` function will do this for us, again holding out a set number of samples for testing, training on the remaining samples, but doing so in windows across the genome. \n",
    "\n",
    "At the end of the function call, it will return a dataframe with the predictions for each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_preds = locator.run_windows_holdouts(\n",
    "    genotypes=genotypes,\n",
    "    samples=samples,\n",
    "    k=10,\n",
    "    return_df=True,\n",
    "    save_full_pred_matrix=True,\n",
    "    window_size=2.5e6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look quickly at the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    predictions=window_preds,\n",
    "    locator=locator,\n",
    "    out_prefix=\"windowed_example\",\n",
    "    plot_map=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
