{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks and Image Processing\n",
    "\n",
    "\n",
    "The first NN architecture you're going to learn is the Convolutional Neural Network (CNN). We start with this architecture because despite the sound of the name, it is relatively simple, and can be used immediately to solve relevant problems. \n",
    "\n",
    "### What are CNNs\n",
    "\n",
    "A convolutional network refers to both the solution and the problem. CNNs are used in situations where the user (you) know quite a bit about your data, hence the use of human intuition in the planning of the solution is quite significant. In particular you know so much about your data that you are willing say that you know that a given datapoint and its corresponding parameter(s) are directly related to its neighbors.\n",
    "\n",
    "CNNs (say \"convnet\") are neural nets that share parameters across space, thus capturing your previous knowledge about your datapoints, describing the data in terms of the relationship of each point to its neighbors. The clearest example of this is a photograph as discussed in detail in this linked [video](https://www.youtube.com/watch?v=jajksuQW4mc). Why? Because **you**, human, know *a priori* that the data you're putting in is a photograph, and you know that it is a photograph ostensibly *of something*. In *your* mind, the photograph represents a flattened image of something which definitely exists and has boundaries and a shape. Therefore the image pixels have clear relationships to each other, caused by the object of interest.\n",
    "\n",
    "The function of the CNN is to \"filter\" meaningful data sequentially out of the input data by \"squeezing\" the relatively simple input data using a sequence of layers into a small number of highly parameterized output features. These output features are a an extraction of features of the dataset and are what is used to predict. \n",
    "\n",
    "### Components of a CNN \n",
    "\n",
    "Convnets contain one or more of each of the following layers:\n",
    "\n",
    "\n",
    "#### 1) Input layer\n",
    "\n",
    "The input layer functions as it always does. \n",
    "\n",
    "#### 2) Convolution (filter) layer\n",
    "\n",
    "This layer encodes the data by using what is called a \"convolution\" to detect specific features. \n",
    "A CNN processes an image using a matrix of weights called filters (or features) that detect specific attributes such as diagonal edges, vertical edges, etc. Moreover, as the image progresses through each layer, the filters are able to recognize more complex attributes. \n",
    "\n",
    "The output of a convolution layer is something called a feature map (or activation map). In order to generate a feature map, we take an array of weights (this is the filter) and slide it over the image, taking the dot product of the smaller array and the pixel values of the image as we go. This operation is called convolution. The convolution selects out a specific type of relationship amongst the pixels of the image.\n",
    "\n",
    "#### 3) Pooling layer\n",
    "\n",
    "The pooling layer also contributes towards the ability of the convnet to locate features regardless of where they are in the image. In particular, the pooling layer makes the convnet less sensitive to small changes in the location of a feature, i.e. it gives the convnet the property of translational invariance in that the output of the pooling layer remains the same even when a feature is moved a little. Pooling also reduces the size of the feature map, thus simplifying computation in later layers.\n",
    "\n",
    "#### 4) ReLU (rectified linear units) layer\n",
    "\n",
    "The ReLU layer enables the CNN to recognize nonlinear relationships amongst the pixels and can be placed before or after the pooling layer (or in series, as is often the case). Other functions can be added into the layers of the network, but ReLU tends to work better in practice, for reasons that I have suggested in class. \n",
    "\n",
    "#### 5) Fully connected layer(s)\n",
    "\n",
    "The final layer of a CNN functions is actually a stack of layers that function just as a MLP does - There are usually 3 layers, an input, hidden and final layer. This stack provides weighting and captures the necessary inferential logic of the network to make a probabilistic decision. The function applied for the final layer is almost always the highly-used [softmax](https://en.wikipedia.org/wiki/Softmax_function) function, which we can think of as a sort of multi-hypothesis (multinomial) logistic function - a **classifier**, not a **regressor**. On the whole, softmax is not a particularly great prior, but as in all things Deep Learning, many layers and strong optimizers tend to overcome any flaws by brute force.\n",
    "\n",
    "#### 6) Loss layer (during the training process)\n",
    "\n",
    "The loss layer functions just as it does with a MLP. The function most often used here due to the classification process of the network is the venerable [cross-entropy cost function](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/). There aren't a lot of other options as far as this task goes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Convolution Works\n",
    "(with execerpts from a [friendly introduction to CNNs](https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks))\n",
    "\n",
    "The [convolution](https://en.wikipedia.org/wiki/Convolution) of two functions $f$ and $g$ is defined as:\n",
    "\n",
    "$$(f*g)(t) = \\int_{-\\infty}^{\\infty}f(\\tau)g(t-\\tau)\\tau$$ \n",
    "\n",
    "Can be thought of as the resulting function that one obtains by passing the process $f$ through a filtering process $g$ such that features in $f$ only result in $(f*g)(t)$ when those features coincide with similar features in $g$ at the interval $\\tau$. This seems like a high-flown idea until you have a chance to [look at it](http://mathworld.wolfram.com/Convolution.html). Also read Colah's excellent post on convolutional nets [here](https://colah.github.io/posts/2014-07-Understanding-Convolutions/).\n",
    "\n",
    "Convolutions work according to the scheme below. Imagine the filter (center square) rastering block-by-block over the image (the big square to the right). The \n",
    "convolution in this case is the grandsum of the hademard product (cell by cell) of the two matrices, line by line. Why do we do this? Look at the  convolution above. The convolution is the repeated integral, \n",
    "\n",
    "$$(f*g)(t) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}f(\\tau, \\zeta)g(t-\\tau, z-\\zeta)d\\tau d\\zeta$$ \n",
    "\n",
    "We need to sum each point of the raster, going over every interval. What is produced is another matrix, with a number of rows and columns represented by the total number of **strides** available to the filter. We can also use a smaller filter, with a smaller number of strides, if we wish to reduce the size of the response map.\n",
    "\n",
    "![convolution-1](./images/convolution_1.png)\n",
    "\n",
    "#### Convolution and padding\n",
    "\n",
    "There are 3 main parameters that we can change to modify the behavior of the convolutional layer: Filter size, stride and padding.\n",
    "\n",
    "Stride controls how the filter convolves around the input volume. Let’s look at an example. Let’s imagine a 7 x 7 input volume, a 3 x 3 filter, and a stride of 1. The resulting response map is 5 x 5.\n",
    "\n",
    "![stride1](./images/Stride1.png)\n",
    "\n",
    "Now try increasing the stride length to 2. The output is now 3 x 3.\n",
    "\n",
    "![stride2](./images/Stride2.png)\n",
    "\n",
    "Padding is necessary when we want to ensure that the size of the output layer matches the size of the input layer (sometimes you want this, for example, when you want to do a convolution before beginning reduction, e.g. smoothing or preconditioning of the input in some way).  For example, we may wish to apply a 3x3 filter to a 9x9 layer. Doing so with strides of 1 will result in an output layer of 7x7 (try this in class). If we wish to increase the size of the output layer to 9x9, we will need to add a row of 0s all the way around the input layer. This is called **padding** and it is used in many places in data science, here just to maintain the size of a layer. \n",
    "\n",
    "If you have a stride of 1 and you want to obtain the same output dimensions of your input layer you can use the formula:\n",
    "\n",
    "$$P = \\dfrac{K-1}{2}$$ \n",
    "\n",
    "Where $P$ is the number of padding rows needed, and $K$ is the size of the filter. The formula for calculating the output size of any convolved layer is:\n",
    "\n",
    "$$O = \\dfrac{W-K+2P}{S} +1$$\n",
    "\n",
    "Where $O$ is the output dimension (presuming square), $W$ is the input dimension, $K$ filter size, $P$ padding and $S$ stride length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Pooling Works\n",
    "\n",
    "The pooling layer does a very important part, it functions as a feature locator. In particular, the pooling layer makes the convnet less sensitive to small changes in the location of a feature, i.e. it gives the convnet the property of translational invariance in that the output of the pooling layer remains the same even when a feature is moved a little. The most typical pooling applied is max pooling, the selection of the maximum valued feature after convolutional filtration.  \n",
    "\n",
    "![max_pooling](./images/max_pooling_1.jpg)\n",
    "\n",
    "However, some [experts](https://www.youtube.com/watch?v=rTawFwUvnLE) feel that the reliance of most developers on max pooling as a logical routing mechanism is a conceptual stumbling block that holds back the improvement of CNNs. I do not hold an opinion on this matter, as I doubt that the best machine learning tool will be the one that somehow mimics the brain the best, nor do I view machine learning as a lens on biology. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU layer\n",
    "\n",
    "ReLU neurons are used to capture nonlinear relationships that exist among individual weights of the layer below them. The value of the response layer cell $j$, $\\hat{y}_{j}$  is simply a linear sum for each convolution, applying a weight for each of the N cells of the filter:\n",
    "\n",
    "$$ \\hat{y}_{j} = \\sum_{i=0}^{N}w_{i, j}x_{i, j} $$\n",
    "\n",
    "Note that this is simply a linear operation.\n",
    "\n",
    "ReLU neurons are a very simple function:\n",
    "\n",
    "$$ReLU(x) = max(x, 0)$$ \n",
    "\n",
    "And functionally eliminate negative values. \n",
    "\n",
    "![relu](./images/ReLU_layer.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer (Prediction Layer)\n",
    "\n",
    "This is essentially a MLP into which is fed the data conditioned by layers below it. The data from the lower layers is piped in the usual way, to be abstracted and filtered by the decision processes of the perceptron. \n",
    "\n",
    "![prediction](./images/prediction_layer.jpg)\n",
    "\n",
    "The neurons in the output layer correspond to each of the possible classes the convnet is looking for. We train the entire system to optimize for prediction against these classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
