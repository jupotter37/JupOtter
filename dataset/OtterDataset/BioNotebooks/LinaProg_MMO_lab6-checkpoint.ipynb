{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-macosx_10_11_x86_64.whl (195.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 195.6 MB 5.0 MB/s eta 0:00:018     |██████████████████████████▋     | 162.7 MB 7.0 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property; python_version < \"3.8\" in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.31.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (47.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.10.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lina/Documents/University Shit/Master/sem_2/ML/.venv/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Using legacy setup.py install for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for wrapt, since package 'wheel' is not installed.\n",
      "Installing collected packages: wheel, termcolor, numpy, keras-preprocessing, tensorflow-estimator, grpcio, google-pasta, astunparse, h5py, markdown, werkzeug, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, tensorboard-data-server, absl-py, tensorboard-plugin-wit, tensorboard, keras-nightly, wrapt, gast, opt-einsum, flatbuffers, tensorflow\n",
      "    Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.2.1\n",
      "    Uninstalling h5py-3.2.1:\n",
      "      Successfully uninstalled h5py-3.2.1\n",
      "    Running setup.py install for wrapt ... \u001b[?25ldone\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrpoMDnAf7SB",
    "outputId": "3418124c-6d45-4dbc-8031-94bb3b9c9b19"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-90cf044403d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNuSVR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearSVR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "import zipfile\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QFLQbkFBhQhI"
   },
   "outputs": [],
   "source": [
    "categories = [\"talk.politics.guns\", \"alt.atheism\", \"sci.med\", \"rec.autos\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Ua57kGKkFhe"
   },
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyIdFrA8lTKs"
   },
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZaH0hYckRBt",
    "outputId": "5d47db36-0509-47d1-abec-628d5c9a8e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 37176\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(data)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd3kM0l0k6Oh",
    "outputId": "58566ca9-7cdb-4329-9501-d211ae6a54ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thom=33375\n",
      "morgan=23251\n",
      "ucs=34360\n",
      "mun=23527\n",
      "ca=8754\n",
      "thomas=33376\n",
      "clancy=9784\n",
      "subject=32210\n",
      "re=28101\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJSLY9hPk-gA",
    "outputId": "7691ed35-5c79-45db-c7e6-632a33396fba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2214x37176 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 375168 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = vocabVect.transform(data)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9oPpvpalm5i",
    "outputId": "eb899000-d791-4246-c6b6-144234794174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weHqh9hQlucV",
    "outputId": "361a7178-ec63-41bf-eecc-baebd45b2045"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zyg', 'zyklon', 'zz', 'zz_g9q3', 'zzz', 'íålittin']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[37170:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZWZn8Gx9nJq3"
   },
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoKnjKohnNiU",
    "outputId": "12e556a3-1b0a-4f3b-aa3e-a681c49b5f34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy = 0.951219512195122\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy = 0.9543812104787714\n",
      "===========================\n",
      "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None,\n",
      "                vocabulary={'00': 0, '000': 1, '0000': 2, '0000001200': 3,\n",
      "                            '00014': 4, '000152': 5, '000406': 6,\n",
      "                            '0005111312': 7, '0005111312na3em': 8, '000601': 9,\n",
      "                            '000710': 10, '000mi': 11, '000miles': 12,\n",
      "                            '000s': 13, '001': 14, '0010': 15, '001004': 16,\n",
      "                            '001125': 17, '001319': 18, '001642': 19, '002': 20,\n",
      "                            '002142': 21, '002651': 22, '003': 23,\n",
      "                            '003258u19250': 24, '0033': 25, '003522': 26,\n",
      "                            '004': 27, '004021809': 28, '004158': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy = 0.6820234869015357\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6HJYv5KlVUy"
   },
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G33fGtCGr9jm"
   },
   "outputs": [],
   "source": [
    "# Using the stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize the stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iAcfN15JrHO1"
   },
   "outputs": [],
   "source": [
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in newsgroups['data']:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZST6Gz8s3KZ",
    "outputId": "0d66c5fb-0fb1-47b2-b433-99cf2b14141c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thom',\n",
       "  'morgan',\n",
       "  'ucs',\n",
       "  'mun',\n",
       "  'ca',\n",
       "  'thomas',\n",
       "  'clancy',\n",
       "  'subject',\n",
       "  'thrush',\n",
       "  'good',\n",
       "  'grief',\n",
       "  'candida',\n",
       "  'albicans',\n",
       "  'organization',\n",
       "  'memorial',\n",
       "  'university',\n",
       "  'newfoundland',\n",
       "  'lines',\n",
       "  'dyer',\n",
       "  'spdcc',\n",
       "  'com',\n",
       "  'steve',\n",
       "  'dyer',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'apr',\n",
       "  'ucsvax',\n",
       "  'sdsu',\n",
       "  'edu',\n",
       "  'mccurdy',\n",
       "  'ucsvax',\n",
       "  'sdsu',\n",
       "  'edu',\n",
       "  'mccurdy',\n",
       "  'writes',\n",
       "  'dyer',\n",
       "  'beyond',\n",
       "  'rude',\n",
       "  'drink',\n",
       "  'yeah',\n",
       "  'yeah',\n",
       "  'yeah',\n",
       "  'threaten',\n",
       "  'rip',\n",
       "  'lips',\n",
       "  'snort',\n",
       "  'always',\n",
       "  'people',\n",
       "  'blinded',\n",
       "  'knowledge',\n",
       "  'unopen',\n",
       "  'anything',\n",
       "  'already',\n",
       "  'established',\n",
       "  'given',\n",
       "  'medical',\n",
       "  'community',\n",
       "  'know',\n",
       "  'surprised',\n",
       "  'outlook',\n",
       "  'duh',\n",
       "  'nice',\n",
       "  'see',\n",
       "  'steve',\n",
       "  'still',\n",
       "  'high',\n",
       "  'almighty',\n",
       "  'intellectual',\n",
       "  'prowess',\n",
       "  'tact',\n",
       "  'record',\n",
       "  'several',\n",
       "  'outbreaks',\n",
       "  'thrush',\n",
       "  'several',\n",
       "  'past',\n",
       "  'years',\n",
       "  'indication',\n",
       "  'immunosuppression',\n",
       "  'nutritional',\n",
       "  'deficiencies',\n",
       "  'taken',\n",
       "  'antobiotics',\n",
       "  'listen',\n",
       "  'thrush',\n",
       "  'recognized',\n",
       "  'clinical',\n",
       "  'syndrome',\n",
       "  'definite',\n",
       "  'characteristics',\n",
       "  'thrush',\n",
       "  'thrush',\n",
       "  'see',\n",
       "  'lesions',\n",
       "  'culture',\n",
       "  'treat',\n",
       "  'generally',\n",
       "  'responds',\n",
       "  'well',\n",
       "  'otherwise',\n",
       "  'immunocompromised',\n",
       "  'noring',\n",
       "  'anal',\n",
       "  'retentive',\n",
       "  'idee',\n",
       "  'fixe',\n",
       "  'fungal',\n",
       "  'infection',\n",
       "  'sinuses',\n",
       "  'even',\n",
       "  'category',\n",
       "  'walking',\n",
       "  'neurasthenics',\n",
       "  'convinced',\n",
       "  'candida',\n",
       "  'reading',\n",
       "  'quack',\n",
       "  'book',\n",
       "  'yawn',\n",
       "  'dentist',\n",
       "  'sees',\n",
       "  'fair',\n",
       "  'amount',\n",
       "  'thrush',\n",
       "  'recommended',\n",
       "  'acidophilous',\n",
       "  'began',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'daily',\n",
       "  'basis',\n",
       "  'outbreaks',\n",
       "  'ceased',\n",
       "  'quit',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'outbreaks',\n",
       "  'periodically',\n",
       "  'resumed',\n",
       "  'resumed',\n",
       "  'taking',\n",
       "  'acidophilous',\n",
       "  'outbreaks',\n",
       "  'since',\n",
       "  'exactly',\n",
       "  'question',\n",
       "  'steve',\n",
       "  'point',\n",
       "  'person',\n",
       "  'one',\n",
       "  'steve',\n",
       "  'dyer',\n",
       "  'nice',\n",
       "  'see',\n",
       "  'things',\n",
       "  'never',\n",
       "  'change',\n",
       "  'steve',\n",
       "  'ignorant',\n",
       "  'one',\n",
       "  'group',\n",
       "  'alternative',\n",
       "  'another',\n",
       "  'one',\n",
       "  'positive',\n",
       "  'thing',\n",
       "  'came',\n",
       "  'longer',\n",
       "  'bothering',\n",
       "  'folks',\n",
       "  'alternative',\n",
       "  'shame',\n",
       "  'people',\n",
       "  'suffer',\n",
       "  'others',\n",
       "  'may',\n",
       "  'breath',\n",
       "  'freely',\n",
       "  'sorry',\n",
       "  'wasting',\n",
       "  'bandwidth',\n",
       "  'folks',\n",
       "  'forget',\n",
       "  'bow',\n",
       "  'every',\n",
       "  'second',\n",
       "  'day',\n",
       "  'offer',\n",
       "  'first',\n",
       "  'born',\n",
       "  'almight',\n",
       "  'omniscient',\n",
       "  'omnipotent',\n",
       "  'mr',\n",
       "  'steve'],\n",
       " ['kempmp',\n",
       "  'phoenix',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'petri',\n",
       "  'pihko',\n",
       "  'subject',\n",
       "  'concerning',\n",
       "  'god',\n",
       "  'morality',\n",
       "  'long',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'oulu',\n",
       "  'finland',\n",
       "  'x',\n",
       "  'newsreader',\n",
       "  'tin',\n",
       "  'version',\n",
       "  'pl',\n",
       "  'lines',\n",
       "  'kind',\n",
       "  'argument',\n",
       "  'cries',\n",
       "  'comment',\n",
       "  'jbrown',\n",
       "  'batman',\n",
       "  'bmd',\n",
       "  'trw',\n",
       "  'com',\n",
       "  'wrote',\n",
       "  'article',\n",
       "  'apr',\n",
       "  'leland',\n",
       "  'stanford',\n",
       "  'edu',\n",
       "  'galahad',\n",
       "  'leland',\n",
       "  'stanford',\n",
       "  'edu',\n",
       "  'scott',\n",
       "  'compton',\n",
       "  'writes',\n",
       "  'jim',\n",
       "  'originally',\n",
       "  'wrote',\n",
       "  'god',\n",
       "  'create',\n",
       "  'disease',\n",
       "  'responsible',\n",
       "  'maladies',\n",
       "  'newborns',\n",
       "  'god',\n",
       "  'create',\n",
       "  'life',\n",
       "  'according',\n",
       "  'protein',\n",
       "  'code',\n",
       "  'mutable',\n",
       "  'evolve',\n",
       "  'without',\n",
       "  'delving',\n",
       "  'deep',\n",
       "  'discussion',\n",
       "  'creationism',\n",
       "  'vs',\n",
       "  'evolutionism',\n",
       "  'god',\n",
       "  'created',\n",
       "  'original',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'without',\n",
       "  'flaw',\n",
       "  'evidence',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'degraded',\n",
       "  'ever',\n",
       "  'since',\n",
       "  'evidence',\n",
       "  'favour',\n",
       "  'statement',\n",
       "  'perhaps',\n",
       "  'biggest',\n",
       "  'imperfection',\n",
       "  'code',\n",
       "  'full',\n",
       "  'non',\n",
       "  'coding',\n",
       "  'regions',\n",
       "  'introns',\n",
       "  'called',\n",
       "  'intervene',\n",
       "  'coding',\n",
       "  'regions',\n",
       "  'exons',\n",
       "  'impressive',\n",
       "  'amount',\n",
       "  'evidence',\n",
       "  'suggests',\n",
       "  'introns',\n",
       "  'ancient',\n",
       "  'origin',\n",
       "  'likely',\n",
       "  'early',\n",
       "  'exons',\n",
       "  'represented',\n",
       "  'early',\n",
       "  'protein',\n",
       "  'domains',\n",
       "  'number',\n",
       "  'introns',\n",
       "  'decreasing',\n",
       "  'increasing',\n",
       "  'appears',\n",
       "  'intron',\n",
       "  'loss',\n",
       "  'occur',\n",
       "  'species',\n",
       "  'common',\n",
       "  'ancestry',\n",
       "  'usually',\n",
       "  'quite',\n",
       "  'similar',\n",
       "  'exon',\n",
       "  'intron',\n",
       "  'structure',\n",
       "  'genes',\n",
       "  'hand',\n",
       "  'possibility',\n",
       "  'introns',\n",
       "  'inserted',\n",
       "  'later',\n",
       "  'presents',\n",
       "  'several',\n",
       "  'logical',\n",
       "  'difficulties',\n",
       "  'introns',\n",
       "  'removed',\n",
       "  'splicing',\n",
       "  'mechanism',\n",
       "  'would',\n",
       "  'present',\n",
       "  'unused',\n",
       "  'introns',\n",
       "  'inserted',\n",
       "  'moreover',\n",
       "  'intron',\n",
       "  'insertion',\n",
       "  'would',\n",
       "  'required',\n",
       "  'precise',\n",
       "  'targeting',\n",
       "  'random',\n",
       "  'insertion',\n",
       "  'would',\n",
       "  'tolerated',\n",
       "  'since',\n",
       "  'sequences',\n",
       "  'intron',\n",
       "  'removal',\n",
       "  'self',\n",
       "  'splicing',\n",
       "  'mrna',\n",
       "  'conserved',\n",
       "  'besides',\n",
       "  'transposition',\n",
       "  'sequence',\n",
       "  'usually',\n",
       "  'leaves',\n",
       "  'trace',\n",
       "  'long',\n",
       "  'terminal',\n",
       "  'repeats',\n",
       "  'target',\n",
       "  'site',\n",
       "  'duplications',\n",
       "  'found',\n",
       "  'near',\n",
       "  'intron',\n",
       "  'sequences',\n",
       "  'seriously',\n",
       "  'recommend',\n",
       "  'reading',\n",
       "  'textbooks',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'posting',\n",
       "  'theological',\n",
       "  'arguments',\n",
       "  'like',\n",
       "  'try',\n",
       "  'watson',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'gene',\n",
       "  'darnell',\n",
       "  'lodish',\n",
       "  'baltimore',\n",
       "  'molecular',\n",
       "  'biology',\n",
       "  'cell',\n",
       "  'starters',\n",
       "  'remember',\n",
       "  'question',\n",
       "  'posed',\n",
       "  'theological',\n",
       "  'context',\n",
       "  'god',\n",
       "  'cause',\n",
       "  'disease',\n",
       "  'newborns',\n",
       "  'answer',\n",
       "  'likewise',\n",
       "  'theological',\n",
       "  'perspective',\n",
       "  'less',\n",
       "  'valid',\n",
       "  'purely',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'different',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'supported',\n",
       "  'evidence',\n",
       "  'whereas',\n",
       "  'theological',\n",
       "  'perspectives',\n",
       "  'often',\n",
       "  'fail',\n",
       "  'fulfil',\n",
       "  'criterion',\n",
       "  'think',\n",
       "  'misread',\n",
       "  'meaning',\n",
       "  'said',\n",
       "  'god',\n",
       "  'made',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'perfect',\n",
       "  'mean',\n",
       "  'perfect',\n",
       "  'certainly',\n",
       "  'evolved',\n",
       "  'since',\n",
       "  'worse',\n",
       "  'would',\n",
       "  'please',\n",
       "  'cite',\n",
       "  'references',\n",
       "  'support',\n",
       "  'assertion',\n",
       "  'assertion',\n",
       "  'less',\n",
       "  'valid',\n",
       "  'scientific',\n",
       "  'perspective',\n",
       "  'unless',\n",
       "  'support',\n",
       "  'evidence',\n",
       "  'fact',\n",
       "  'claimed',\n",
       "  'parasites',\n",
       "  'diseases',\n",
       "  'perhaps',\n",
       "  'important',\n",
       "  'thought',\n",
       "  'instance',\n",
       "  'sex',\n",
       "  'might',\n",
       "  'evolved',\n",
       "  'defence',\n",
       "  'parasites',\n",
       "  'view',\n",
       "  'supported',\n",
       "  'computer',\n",
       "  'simulations',\n",
       "  'evolution',\n",
       "  'eg',\n",
       "  'tierra',\n",
       "  'perhaps',\n",
       "  'thought',\n",
       "  'higher',\n",
       "  'energy',\n",
       "  'rays',\n",
       "  'like',\n",
       "  'x',\n",
       "  'rays',\n",
       "  'gamma',\n",
       "  'rays',\n",
       "  'cosmic',\n",
       "  'rays',\n",
       "  'caused',\n",
       "  'damage',\n",
       "  'fact',\n",
       "  'thermal',\n",
       "  'energy',\n",
       "  'damage',\n",
       "  'although',\n",
       "  'usually',\n",
       "  'mild',\n",
       "  'easily',\n",
       "  'fixed',\n",
       "  'enzymatic',\n",
       "  'action',\n",
       "  'actually',\n",
       "  'neither',\n",
       "  'us',\n",
       "  'knows',\n",
       "  'atmosphere',\n",
       "  'like',\n",
       "  'time',\n",
       "  'god',\n",
       "  'created',\n",
       "  'life',\n",
       "  'according',\n",
       "  'recollection',\n",
       "  'biologists',\n",
       "  'claim',\n",
       "  'life',\n",
       "  'began',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'would',\n",
       "  'half',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'earth',\n",
       "  'created',\n",
       "  'would',\n",
       "  'still',\n",
       "  'primitive',\n",
       "  'support',\n",
       "  'life',\n",
       "  'seem',\n",
       "  'remember',\n",
       "  'figure',\n",
       "  'like',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'origination',\n",
       "  'life',\n",
       "  'earth',\n",
       "  'anyone',\n",
       "  'better',\n",
       "  'estimate',\n",
       "  'replace',\n",
       "  'created',\n",
       "  'formed',\n",
       "  'since',\n",
       "  'need',\n",
       "  'invoke',\n",
       "  'creator',\n",
       "  'earth',\n",
       "  'formed',\n",
       "  'without',\n",
       "  'one',\n",
       "  'recent',\n",
       "  'estimates',\n",
       "  'age',\n",
       "  'earth',\n",
       "  'range',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'earliest',\n",
       "  'signs',\n",
       "  'life',\n",
       "  'true',\n",
       "  'fossils',\n",
       "  'organic',\n",
       "  'stromatolite',\n",
       "  'like',\n",
       "  'layers',\n",
       "  'date',\n",
       "  'back',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'would',\n",
       "  'leave',\n",
       "  'billion',\n",
       "  'years',\n",
       "  'first',\n",
       "  'cells',\n",
       "  'evolve',\n",
       "  'sorry',\n",
       "  'give',\n",
       "  'references',\n",
       "  'based',\n",
       "  'course',\n",
       "  'evolutionary',\n",
       "  'biochemistry',\n",
       "  'attended',\n",
       "  'dominion',\n",
       "  'great',\n",
       "  'feat',\n",
       "  'satan',\n",
       "  'genetically',\n",
       "  'engineer',\n",
       "  'diseases',\n",
       "  'bacterial',\n",
       "  'viral',\n",
       "  'genetic',\n",
       "  'although',\n",
       "  'forces',\n",
       "  'natural',\n",
       "  'selection',\n",
       "  'tend',\n",
       "  'improve',\n",
       "  'survivability',\n",
       "  'species',\n",
       "  'degeneration',\n",
       "  'genetic',\n",
       "  'code',\n",
       "  'tends',\n",
       "  'offset',\n",
       "  'want',\n",
       "  'true',\n",
       "  'evidence',\n",
       "  'supposed',\n",
       "  'degeneration',\n",
       "  'understand',\n",
       "  'scott',\n",
       "  'reaction',\n",
       "  'excuse',\n",
       "  'far',\n",
       "  'fetched',\n",
       "  'know',\n",
       "  'must',\n",
       "  'jesting',\n",
       "  'know',\n",
       "  'pathogens',\n",
       "  'know',\n",
       "  'point',\n",
       "  'mutations',\n",
       "  'know',\n",
       "  'everything',\n",
       "  'come',\n",
       "  'spontaneously',\n",
       "  'response',\n",
       "  'last',\n",
       "  'statement',\n",
       "  'neither',\n",
       "  'may',\n",
       "  'well',\n",
       "  'believe',\n",
       "  'accept',\n",
       "  'fact',\n",
       "  'cannot',\n",
       "  'know',\n",
       "  'hope',\n",
       "  'forget',\n",
       "  'evidence',\n",
       "  'suggests',\n",
       "  'everything',\n",
       "  'come',\n",
       "  'spontaneously',\n",
       "  'evidence',\n",
       "  'conclusion',\n",
       "  'science',\n",
       "  'one',\n",
       "  'believe',\n",
       "  'anything',\n",
       "  'healthy',\n",
       "  'sign',\n",
       "  'doubt',\n",
       "  'disbelieve',\n",
       "  'right',\n",
       "  'path',\n",
       "  'walk',\n",
       "  'take',\n",
       "  'look',\n",
       "  'evidence',\n",
       "  'present',\n",
       "  'one',\n",
       "  'conclusions',\n",
       "  'prior',\n",
       "  'theology',\n",
       "  'use',\n",
       "  'method',\n",
       "  'therefore',\n",
       "  'seriously',\n",
       "  'doubt',\n",
       "  'could',\n",
       "  'ever',\n",
       "  'come',\n",
       "  'right',\n",
       "  'conclusions',\n",
       "  'human',\n",
       "  'dna',\n",
       "  'complex',\n",
       "  'tends',\n",
       "  'accumulate',\n",
       "  'errors',\n",
       "  'adversely',\n",
       "  'affecting',\n",
       "  'well',\n",
       "  'ability',\n",
       "  'fight',\n",
       "  'disease',\n",
       "  'simpler',\n",
       "  'dna',\n",
       "  'bacteria',\n",
       "  'viruses',\n",
       "  'tend',\n",
       "  'become',\n",
       "  'efficient',\n",
       "  'causing',\n",
       "  'infection',\n",
       "  'disease',\n",
       "  'bad',\n",
       "  'combination',\n",
       "  'hence',\n",
       "  'newborns',\n",
       "  'suffer',\n",
       "  'genetic',\n",
       "  'viral',\n",
       "  'bacterial',\n",
       "  'diseases',\n",
       "  'disorders',\n",
       "  'supposing',\n",
       "  'purpose',\n",
       "  'valid',\n",
       "  'move',\n",
       "  'bacteria',\n",
       "  'viruses',\n",
       "  'exist',\n",
       "  'cause',\n",
       "  'disease',\n",
       "  'another',\n",
       "  'manifests',\n",
       "  'general',\n",
       "  'principle',\n",
       "  'evolution',\n",
       "  'replication',\n",
       "  'saves',\n",
       "  'replicators',\n",
       "  'degradiation',\n",
       "  'efficient',\n",
       "  'method',\n",
       "  'dna',\n",
       "  'survive',\n",
       "  'replicate',\n",
       "  'less',\n",
       "  'efficient',\n",
       "  'methods',\n",
       "  'make',\n",
       "  'present',\n",
       "  'last',\n",
       "  'time',\n",
       "  'please',\n",
       "  'present',\n",
       "  'evidence',\n",
       "  'claim',\n",
       "  'human',\n",
       "  'dna',\n",
       "  'degrading',\n",
       "  'evolutionary',\n",
       "  'processes',\n",
       "  'people',\n",
       "  'claimed',\n",
       "  'opposite',\n",
       "  'true',\n",
       "  'suppressed',\n",
       "  'selection',\n",
       "  'thus',\n",
       "  'bound',\n",
       "  'degrade',\n",
       "  'seen',\n",
       "  'much',\n",
       "  'evidence',\n",
       "  'either',\n",
       "  'claim',\n",
       "  'ask',\n",
       "  'relevant',\n",
       "  'discussion',\n",
       "  'answering',\n",
       "  'john',\n",
       "  'question',\n",
       "  'genetic',\n",
       "  'diseases',\n",
       "  'many',\n",
       "  'bacterial',\n",
       "  'viral',\n",
       "  'diseases',\n",
       "  'require',\n",
       "  'babies',\n",
       "  'develop',\n",
       "  'antibodies',\n",
       "  'god',\n",
       "  'fault',\n",
       "  'original',\n",
       "  'question',\n",
       "  'say',\n",
       "  'course',\n",
       "  'nothing',\n",
       "  'evil',\n",
       "  'god',\n",
       "  'fault',\n",
       "  'explanation',\n",
       "  'work',\n",
       "  'fails',\n",
       "  'miserably',\n",
       "  'may',\n",
       "  'right',\n",
       "  'fact',\n",
       "  'know',\n",
       "  'satan',\n",
       "  'responsible',\n",
       "  'neither',\n",
       "  'suppose',\n",
       "  'powerful',\n",
       "  'evil',\n",
       "  'like',\n",
       "  'satan',\n",
       "  'exists',\n",
       "  'would',\n",
       "  'inconceivable',\n",
       "  'might',\n",
       "  'responsible',\n",
       "  'many',\n",
       "  'ills',\n",
       "  'affect',\n",
       "  'mankind',\n",
       "  'think',\n",
       "  'could',\n",
       "  'done',\n",
       "  'much',\n",
       "  'better',\n",
       "  'job',\n",
       "  'pun',\n",
       "  'intended',\n",
       "  'problem',\n",
       "  'seems',\n",
       "  'satan',\n",
       "  'necessary',\n",
       "  'explain',\n",
       "  'diseases',\n",
       "  'inevitable',\n",
       "  'product',\n",
       "  'evolution',\n",
       "  'say',\n",
       "  'seems',\n",
       "  'like',\n",
       "  'another',\n",
       "  'bad',\n",
       "  'inference',\n",
       "  'actually',\n",
       "  'done',\n",
       "  'oversimplify',\n",
       "  'said',\n",
       "  'point',\n",
       "  'summary',\n",
       "  'words',\n",
       "  'takes',\n",
       "  'new',\n",
       "  'context',\n",
       "  'never',\n",
       "  'said',\n",
       "  'people',\n",
       "  'meant',\n",
       "  'presumably',\n",
       "  'god',\n",
       "  'punished',\n",
       "  'getting',\n",
       "  'diseases',\n",
       "  'say',\n",
       "  'free',\n",
       "  'moral',\n",
       "  'choices',\n",
       "  'attendent',\n",
       "  'consequences',\n",
       "  'mankind',\n",
       "  'chooses',\n",
       "  'reject',\n",
       "  'god',\n",
       "  'people',\n",
       "  'done',\n",
       "  'since',\n",
       "  'beginning',\n",
       "  'expect',\n",
       "  'god',\n",
       "  'protect',\n",
       "  'adverse',\n",
       "  'events',\n",
       "  'entropic',\n",
       "  'universe',\n",
       "  'expecting',\n",
       "  'god',\n",
       "  'exists',\n",
       "  'expect',\n",
       "  'leave',\n",
       "  'us',\n",
       "  'alone',\n",
       "  'would',\n",
       "  'also',\n",
       "  'like',\n",
       "  'hear',\n",
       "  'believe',\n",
       "  'choices',\n",
       "  'indeed',\n",
       "  'free',\n",
       "  'interesting',\n",
       "  'philosophical',\n",
       "  'question',\n",
       "  'answer',\n",
       "  'clear',\n",
       "  'cut',\n",
       "  'seems',\n",
       "  'consequences',\n",
       "  'would',\n",
       "  'expect',\n",
       "  'rejecting',\n",
       "  'allah',\n",
       "  'oh',\n",
       "  'admit',\n",
       "  'perfect',\n",
       "  'yet',\n",
       "  'working',\n",
       "  'good',\n",
       "  'library',\n",
       "  'bookstore',\n",
       "  'good',\n",
       "  'starting',\n",
       "  'point',\n",
       "  'price',\n",
       "  'tea',\n",
       "  'china',\n",
       "  'question',\n",
       "  'provided',\n",
       "  'answer',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'fine',\n",
       "  'subjects',\n",
       "  'important',\n",
       "  'scientific',\n",
       "  'endeavors',\n",
       "  'explain',\n",
       "  'god',\n",
       "  'created',\n",
       "  'set',\n",
       "  'life',\n",
       "  'processes',\n",
       "  'explain',\n",
       "  'behind',\n",
       "  'creation',\n",
       "  'life',\n",
       "  'subsequent',\n",
       "  'evolution',\n",
       "  'behind',\n",
       "  'proposition',\n",
       "  'something',\n",
       "  'supported',\n",
       "  'evidence',\n",
       "  'recommend',\n",
       "  'books',\n",
       "  'need',\n",
       "  'invoke',\n",
       "  'behind',\n",
       "  'prime',\n",
       "  'mover',\n",
       "  'evidence',\n",
       "  'whole',\n",
       "  'universe',\n",
       "  'come',\n",
       "  'existence',\n",
       "  'without',\n",
       "  'intervention',\n",
       "  'recent',\n",
       "  'cosmological',\n",
       "  'theories',\n",
       "  'hawking',\n",
       "  'et',\n",
       "  'al',\n",
       "  'suggest',\n",
       "  'people',\n",
       "  'still',\n",
       "  'insist',\n",
       "  'thanks',\n",
       "  'scotty',\n",
       "  'fine',\n",
       "  'sagely',\n",
       "  'advice',\n",
       "  'highly',\n",
       "  'motivated',\n",
       "  'learn',\n",
       "  'nitty',\n",
       "  'gritty',\n",
       "  'details',\n",
       "  'biology',\n",
       "  'genetics',\n",
       "  'although',\n",
       "  'sure',\n",
       "  'find',\n",
       "  'fascinating',\n",
       "  'subject',\n",
       "  'realize',\n",
       "  'details',\n",
       "  'change',\n",
       "  'big',\n",
       "  'picture',\n",
       "  'god',\n",
       "  'created',\n",
       "  'life',\n",
       "  'beginning',\n",
       "  'ability',\n",
       "  'change',\n",
       "  'adapt',\n",
       "  'environment',\n",
       "  'sorry',\n",
       "  'evidence',\n",
       "  'big',\n",
       "  'picture',\n",
       "  'need',\n",
       "  'create',\n",
       "  'anything',\n",
       "  'capable',\n",
       "  'adaptation',\n",
       "  'come',\n",
       "  'existence',\n",
       "  'without',\n",
       "  'supreme',\n",
       "  'try',\n",
       "  'reading',\n",
       "  'p',\n",
       "  'w',\n",
       "  'atkins',\n",
       "  'creation',\n",
       "  'revisited',\n",
       "  'freeman',\n",
       "  'petri',\n",
       "  'petri',\n",
       "  'pihko',\n",
       "  'kem',\n",
       "  'pmp',\n",
       "  'mathematics',\n",
       "  'truth',\n",
       "  'pihatie',\n",
       "  'c',\n",
       "  'finou',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'physics',\n",
       "  'rule',\n",
       "  'sf',\n",
       "  'oulu',\n",
       "  'kempmp',\n",
       "  'game',\n",
       "  'finland',\n",
       "  'phoenix',\n",
       "  'oulu',\n",
       "  'fi',\n",
       "  'chemistry',\n",
       "  'game'],\n",
       " ['organization',\n",
       "  'university',\n",
       "  'illinois',\n",
       "  'chicago',\n",
       "  'academic',\n",
       "  'computer',\n",
       "  'center',\n",
       "  'jason',\n",
       "  'kratz',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'subject',\n",
       "  'shoot',\n",
       "  'somebody',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'veal',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'says',\n",
       "  'article',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'jason',\n",
       "  'kratz',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'uic',\n",
       "  'edu',\n",
       "  'heard',\n",
       "  'many',\n",
       "  'opinions',\n",
       "  'subject',\n",
       "  'would',\n",
       "  'like',\n",
       "  'hear',\n",
       "  'people',\n",
       "  'net',\n",
       "  'say',\n",
       "  'situation',\n",
       "  'pull',\n",
       "  'gun',\n",
       "  'somebody',\n",
       "  'give',\n",
       "  'chance',\n",
       "  'get',\n",
       "  'away',\n",
       "  'decided',\n",
       "  'continue',\n",
       "  'action',\n",
       "  'anyway',\n",
       "  'end',\n",
       "  'shooting',\n",
       "  'killing',\n",
       "  'question',\n",
       "  'stay',\n",
       "  'wait',\n",
       "  'cops',\n",
       "  'collect',\n",
       "  'brass',\n",
       "  'using',\n",
       "  'semi',\n",
       "  'auto',\n",
       "  'get',\n",
       "  'provided',\n",
       "  'course',\n",
       "  'think',\n",
       "  'seen',\n",
       "  'data',\n",
       "  'point',\n",
       "  'tennessee',\n",
       "  'friend',\n",
       "  'mine',\n",
       "  'police',\n",
       "  'officer',\n",
       "  'essentially',\n",
       "  'recommends',\n",
       "  'fade',\n",
       "  'away',\n",
       "  'even',\n",
       "  'perfectly',\n",
       "  'justified',\n",
       "  'likely',\n",
       "  'great',\n",
       "  'deal',\n",
       "  'hassle',\n",
       "  'side',\n",
       "  'note',\n",
       "  'carrying',\n",
       "  'gun',\n",
       "  'concealed',\n",
       "  'misdemeanor',\n",
       "  'exactly',\n",
       "  'heard',\n",
       "  'fade',\n",
       "  'away',\n",
       "  'nobody',\n",
       "  'saw',\n",
       "  'kind',\n",
       "  'evidence',\n",
       "  'would',\n",
       "  'able',\n",
       "  'get',\n",
       "  'catch',\n",
       "  'assuming',\n",
       "  'either',\n",
       "  'collected',\n",
       "  'brass',\n",
       "  'revolver',\n",
       "  'kind',\n",
       "  'laws',\n",
       "  'books',\n",
       "  'regarding',\n",
       "  'type',\n",
       "  'situation',\n",
       "  'would',\n",
       "  'likely',\n",
       "  'thing',\n",
       "  'happen',\n",
       "  'stayed',\n",
       "  'waited',\n",
       "  'first',\n",
       "  'offense',\n",
       "  'would',\n",
       "  'happen',\n",
       "  'took',\n",
       "  'someone',\n",
       "  'saw',\n",
       "  'caught',\n",
       "  'one',\n",
       "  'state',\n",
       "  'things',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'guess',\n",
       "  'time',\n",
       "  'take',\n",
       "  'trip',\n",
       "  'library',\n",
       "  'look',\n",
       "  'illinois',\n",
       "  'statutes',\n",
       "  'record',\n",
       "  'folks',\n",
       "  'asking',\n",
       "  'curious',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'people',\n",
       "  'read',\n",
       "  'stuff',\n",
       "  'like',\n",
       "  'david',\n",
       "  'veal',\n",
       "  'univ',\n",
       "  'tenn',\n",
       "  'div',\n",
       "  'cont',\n",
       "  'education',\n",
       "  'info',\n",
       "  'services',\n",
       "  'group',\n",
       "  'pa',\n",
       "  'utkvm',\n",
       "  'utk',\n",
       "  'edu',\n",
       "  'still',\n",
       "  'remember',\n",
       "  'way',\n",
       "  'laughed',\n",
       "  'day',\n",
       "  'pushed',\n",
       "  'elevator',\n",
       "  'shaft',\n",
       "  'beginning',\n",
       "  'think',\n",
       "  'love',\n",
       "  'anymore',\n",
       "  'weird',\n",
       "  'al',\n",
       "  'jason',\n",
       "  'u',\n",
       "  'uicvm',\n",
       "  'cc',\n",
       "  'uic',\n",
       "  'edu'],\n",
       " ['geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'subject',\n",
       "  'food',\n",
       "  'related',\n",
       "  'seizures',\n",
       "  'reply',\n",
       "  'geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'organization',\n",
       "  'univ',\n",
       "  'pittsburgh',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'bu',\n",
       "  'edu',\n",
       "  'dozonoff',\n",
       "  'bu',\n",
       "  'edu',\n",
       "  'david',\n",
       "  'ozonoff',\n",
       "  'writes',\n",
       "  'many',\n",
       "  'cereals',\n",
       "  'corn',\n",
       "  'based',\n",
       "  'post',\n",
       "  'looked',\n",
       "  'literature',\n",
       "  'located',\n",
       "  'two',\n",
       "  'articles',\n",
       "  'implicated',\n",
       "  'corn',\n",
       "  'contains',\n",
       "  'tryptophan',\n",
       "  'seizures',\n",
       "  'idea',\n",
       "  'corn',\n",
       "  'diet',\n",
       "  'might',\n",
       "  'potentiate',\n",
       "  'already',\n",
       "  'existing',\n",
       "  'latent',\n",
       "  'seizure',\n",
       "  'disorder',\n",
       "  'cause',\n",
       "  'check',\n",
       "  'see',\n",
       "  'two',\n",
       "  'kellog',\n",
       "  'cereals',\n",
       "  'corn',\n",
       "  'based',\n",
       "  'interested',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'intern',\n",
       "  'obese',\n",
       "  'young',\n",
       "  'woman',\n",
       "  'brought',\n",
       "  'er',\n",
       "  'comatose',\n",
       "  'reported',\n",
       "  'grand',\n",
       "  'mal',\n",
       "  'seizures',\n",
       "  'attending',\n",
       "  'corn',\n",
       "  'festival',\n",
       "  'pumped',\n",
       "  'stomach',\n",
       "  'obtained',\n",
       "  'seemed',\n",
       "  'like',\n",
       "  'couple',\n",
       "  'liters',\n",
       "  'corn',\n",
       "  'much',\n",
       "  'intact',\n",
       "  'kernals',\n",
       "  'hours',\n",
       "  'woke',\n",
       "  'fine',\n",
       "  'tempted',\n",
       "  'sign',\n",
       "  'acute',\n",
       "  'corn',\n",
       "  'intoxication',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'n',\n",
       "  'jxp',\n",
       "  'skepticism',\n",
       "  'chastity',\n",
       "  'intellect',\n",
       "  'geb',\n",
       "  'cadre',\n",
       "  'dsl',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'shameful',\n",
       "  'surrender',\n",
       "  'soon'],\n",
       " ['geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'subject',\n",
       "  'helium',\n",
       "  'non',\n",
       "  'renewable',\n",
       "  'many',\n",
       "  'mris',\n",
       "  'reply',\n",
       "  'geb',\n",
       "  'cs',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'organization',\n",
       "  'univ',\n",
       "  'pittsburgh',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'lines',\n",
       "  'article',\n",
       "  'lsj',\n",
       "  'gdinnkor',\n",
       "  'saltillo',\n",
       "  'cs',\n",
       "  'utexas',\n",
       "  'edu',\n",
       "  'turpin',\n",
       "  'cs',\n",
       "  'utexas',\n",
       "  'edu',\n",
       "  'russell',\n",
       "  'turpin',\n",
       "  'writes',\n",
       "  'helium',\n",
       "  'get',\n",
       "  'consumed',\n",
       "  'would',\n",
       "  'thought',\n",
       "  'failure',\n",
       "  'contain',\n",
       "  'perfectly',\n",
       "  'would',\n",
       "  'result',\n",
       "  'evaporation',\n",
       "  'back',\n",
       "  'atmosphere',\n",
       "  'sounds',\n",
       "  'like',\n",
       "  'cycle',\n",
       "  'obviously',\n",
       "  'takes',\n",
       "  'energy',\n",
       "  'run',\n",
       "  'cycle',\n",
       "  'seriously',\n",
       "  'doubt',\n",
       "  'helium',\n",
       "  'consumption',\n",
       "  'resource',\n",
       "  'issue',\n",
       "  'cycle',\n",
       "  'free',\n",
       "  'helium',\n",
       "  'escape',\n",
       "  'atmosphere',\n",
       "  'due',\n",
       "  'high',\n",
       "  'velocity',\n",
       "  'practical',\n",
       "  'recover',\n",
       "  'mined',\n",
       "  'gordon',\n",
       "  'banks',\n",
       "  'n',\n",
       "  'jxp',\n",
       "  'skepticism',\n",
       "  'chastity',\n",
       "  'intellect',\n",
       "  'geb',\n",
       "  'cadre',\n",
       "  'dsl',\n",
       "  'pitt',\n",
       "  'edu',\n",
       "  'shameful',\n",
       "  'surrender',\n",
       "  'soon']]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TVleXMpplbO",
    "outputId": "2fe41809-5bb5-4cc4-fb30-eec4027c30ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 72.1 ms, total: 8.1 s\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%time model = word2vec.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKnrxBrZqYWQ",
    "outputId": "e31dec1f-be59-4766-c7af-a7cbe220f655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('badlands', 0.9109960794448853), ('reply', 0.9061744213104248), ('bill', 0.9060840606689453), ('itc', 0.9001675844192505), ('conner', 0.8974752426147461)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model.wv.most_similar(positive=['subject'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FHQ10xSftCS0"
   },
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2B263i8TtDVr"
   },
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1vF2XHEtFK8",
    "outputId": "aba52876-1348-4ed4-9838-ebf7d9096aba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.7854984894259819\n",
      "1 \t 0.8188585607940446\n",
      "2 \t 0.639225181598063\n",
      "3 \t 0.7193460490463215\n"
     ]
    }
   ],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]\n",
    "sentiment(EmbeddingVectorizer(model.wv), LogisticRegression(C=5.0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab6_MMO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
