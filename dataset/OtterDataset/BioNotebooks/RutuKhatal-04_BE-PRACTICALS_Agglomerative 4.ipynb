{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce58ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features (4 dimensions: Sepal length, Sepal width, Petal length, Petal width)\n",
    "y = data.target  # True labels (used here just for reference)\n",
    "\n",
    "# =============================\n",
    "# Step 1: Plot Dendrogram\n",
    "# =============================\n",
    "\n",
    "# Generate linkage matrix using Ward's method (minimizes variance within clusters)\n",
    "linked = linkage(X, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "dendrogram(linked)\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Samples\")  # X-axis represents individual data samples\n",
    "plt.ylabel(\"Distance\")  # Y-axis represents the distance between merged clusters\n",
    "plt.show()\n",
    "\n",
    "# Explanation:\n",
    "# - 'linkage': Creates a hierarchical clustering model using the chosen method ('ward').\n",
    "# - 'ward': A linkage criterion that minimizes the variance within clusters when merging.\n",
    "# - The dendrogram shows how clusters are formed by merging data points step by step.\n",
    "# - The height of each merge (y-axis) indicates the dissimilarity (distance) between clusters.\n",
    "\n",
    "# =============================\n",
    "# Step 2: Apply Agglomerative Clustering\n",
    "# =============================\n",
    "\n",
    "# Initialize the Agglomerative Clustering model with 3 clusters\n",
    "clustering = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Fit the model and predict cluster labels for each data point\n",
    "labels = clustering.fit_predict(X)\n",
    "\n",
    "# Explanation:\n",
    "# - 'n_clusters=3': Specifies the number of clusters (since the Iris dataset has 3 known classes).\n",
    "# - 'fit_predict': Fits the clustering model to the data and returns the predicted cluster labels.\n",
    "# - Agglomerative Clustering starts with each point as its own cluster and merges the closest clusters iteratively.\n",
    "\n",
    "# =============================\n",
    "# Step 3: PCA for Dimensionality Reduction\n",
    "# =============================\n",
    "\n",
    "# Apply PCA to reduce the dataset to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Explanation:\n",
    "# - 'n_components=2': We reduce the data to 2 principal components for easier 2D visualization.\n",
    "# - PCA is used because it is unsupervised and focuses on maximizing variance in the data.\n",
    "# - LDA (Linear Discriminant Analysis) is not used here because it requires class labels and focuses on maximizing class separation (supervised learning).\n",
    "\n",
    "# =============================\n",
    "# Step 4: Plot the Clustering Results\n",
    "# =============================\n",
    "\n",
    "# Create a scatter plot of the PCA-transformed data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow', edgecolor='k', s=100)\n",
    "plt.title(\"Agglomerative Clustering (3 Clusters)\")\n",
    "plt.xlabel(\"Principal Component 1\")  # X-axis represents the first principal component\n",
    "plt.ylabel(\"Principal Component 2\")  # Y-axis represents the second principal component\n",
    "plt.show()\n",
    "\n",
    "# Explanation of Scatter Plot:\n",
    "# - 'X_pca[:, 0]': Data points along the x-axis (PCA Component 1).\n",
    "# - 'X_pca[:, 1]': Data points along the y-axis (PCA Component 2).\n",
    "# - 'c=labels': Colors the points based on the predicted cluster labels.\n",
    "# - 'cmap': Color map used to differentiate clusters (e.g., 'rainbow' for distinct colors).\n",
    "# - 'edgecolor': The color of the marker edges ('k' stands for black).\n",
    "# - 's': Size of the markers (100 makes the points larger and more visible).\n",
    "\n",
    "# =============================\n",
    "# Step 5: Plot Clusters for Different Values of k (2 to 5)\n",
    "# =============================\n",
    "\n",
    "# Define a function to plot clusters for a given number of clusters (k)\n",
    "def plot_clusters(X, k):\n",
    "    # Initialize the Agglomerative Clustering model\n",
    "    model = AgglomerativeClustering(n_clusters=k)\n",
    "    # Fit the model and get the predicted labels\n",
    "    labels = model.fit_predict(X)\n",
    "\n",
    "    # Create a scatter plot for the clusters\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', edgecolor='k', s=80)\n",
    "    plt.title(f\"Agglomerative Clustering with {k} Clusters\")\n",
    "    plt.xlabel(\"PCA Component 1\")  # X-axis: First principal component\n",
    "    plt.ylabel(\"PCA Component 2\")  # Y-axis: Second principal component\n",
    "    plt.show()\n",
    "\n",
    "# Loop through values of k from 2 to 5 and plot the clustering results\n",
    "for k in range(2, 6):\n",
    "    plot_clusters(X_pca, k)\n",
    "\n",
    "# Explanation:\n",
    "# - The 'plot_clusters' function visualizes the result of agglomerative clustering for different numbers of clusters (k).\n",
    "# - The loop iterates over k values from 2 to 5 to show how the clustering changes.\n",
    "# - Increasing 'k' typically results in more, smaller clusters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Agglomerative Clustering:\n",
    "# Agglomerative clustering is a type of hierarchical clustering. It starts by treating each data point as its own cluster and then iteratively merges the closest clusters based on a similarity metric. This process continues until all points belong to a single cluster or a specified number of clusters is reached.\n",
    "\n",
    "# Why use Agglomerative Clustering?\n",
    "\n",
    "# It does not require you to specify the number of clusters initially (although you can choose to do so).\n",
    "# It creates a hierarchy of clusters, which can be visualized using a dendrogram.\n",
    "# Suitable for small to medium-sized datasets (like the Iris dataset).\n",
    "# Use Cases:\n",
    "\n",
    "# Gene expression analysis in biology.\n",
    "# Market segmentation.\n",
    "# Image segmentation in computer vision.\n",
    "# 2. Principal Component Analysis (PCA):\n",
    "# PCA is a dimensionality reduction technique that transforms the data into a lower-dimensional space while preserving as much variance as possible.\n",
    "\n",
    "# Why use PCA here?\n",
    "\n",
    "# The Iris dataset has 4 features, making it hard to visualize directly in 2D.\n",
    "# PCA reduces the dataset to 2 components for easier visualization.\n",
    "# LDA (Linear Discriminant Analysis) is another technique that could be used for dimensionality reduction but it requires labels to maximize class separation, making it more suitable for supervised learning scenarios. PCA, on the other hand, is unsupervised and focuses on variance.\n",
    "# 3. Dendrogram:\n",
    "# A dendrogram is a tree-like diagram that shows the hierarchical relationship between data points. It illustrates the process of agglomerative clustering, where data points are merged step by step.\n",
    "\n",
    "# Graph Explanation:\n",
    "\n",
    "# The x-axis represents individual samples or clusters.\n",
    "# The y-axis represents the distance or dissimilarity between clusters.\n",
    "# Each horizontal line represents a merge, and the height of the line shows the distance between the merged clusters.\n",
    "# 4. Scatter Plot Attributes:\n",
    "# In the scatter plot:\n",
    "\n",
    "# x: Values for the x-axis (PCA Component 1).\n",
    "# y: Values for the y-axis (PCA Component 2).\n",
    "# c: Color of the points, determined by the cluster labels.\n",
    "# cmap: Color map used to distinguish different clusters.\n",
    "# edgecolor: Color of the marker edges ('k' stands for black).\n",
    "# s: Size of the markers (larger values make the points bigger).\n",
    "# title: Title of the plot.\n",
    "# xlabel and ylabel: Labels for the x-axis and y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
