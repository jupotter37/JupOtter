{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c668b9-dfe5-4c2c-8d1e-e44d82c2e38e",
   "metadata": {
    "id": "17c668b9-dfe5-4c2c-8d1e-e44d82c2e38e",
    "tags": []
   },
   "source": [
    "<img src=\"https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/iguide_logo.png\" width=200 height=200 />\n",
    "\n",
    "# Examining Hydrological Responses and Shrinkage in the Great Salt Lake\n",
    "\n",
    "**Authors**: Kat Fowler <kff26@nau.edu>, Mallory Jordan <maj0062@auburn.edu>, Hodo Orok <horok2@illinois.edu>, Marian de Orla-Barile <mariandeorla-barile@ucsb.edu>, Daniel Beene <darbeene@salud.unm.edu>, Yen-Yi Wu <ywu10@uwyo.edu>, Irene Garousi-Nejad <igarousi@cuahsi.org>\n",
    "\n",
    "**Last Updated**: 10.01.2023\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "- 1. Project Overview\n",
    "- 2. Study Area and Motivation\n",
    "- 3. Required Libraries to Run this Notebook\n",
    "- 4. Computational Analyses and Results\n",
    "  - Analysis 4.1. Compare the Input Precipitation Data from the National Weather Model Sourced from AORC with PRISM Data for the Three Watersheds\n",
    "    - 4.1.1. Download PRISM Monthly Normals Precipitation Data\n",
    "    - 4.1.2. Use Consistent Projections\n",
    "    - 4.1.3. Subset PRISM Data for the Watershed and Calculate the Spatial Average Precipitation Across the Watershed\n",
    "    - 4.1.4. Plot a Time Series of 30-Year Average Precipitation for One Watershed\n",
    "    - 4.1.5. Import the AORC Forcing Dataset and Aggregate to Monthly and Daily Values\n",
    "    - 4.1.6. Plot PRISM versus AORC\n",
    "  - Analysis 4.2. Compare National Water Model Simulated Streamflow to Observations from USGS\n",
    "    - 4.2.1. Import NWM Streamflow Data\n",
    "    - 4.2.2. Retrieve the NWM Streamflow Data for the Outlet Locations\n",
    "    - 4.2.3. Import USGS Stream Gage Data\n",
    "    - 4.2.4. Create NWM versus USGS Stream Gage Comparison Plots\n",
    "  - Analysis 4.3. Compare the Cumulative Precipitation and Runoff for Each Water Year\n",
    "    - 4.3.1. Import PRISM Daily Precipitation Data\n",
    "    - 4.3.2. Import AORC Daily Precipitation Data\n",
    "    - 4.3.3. Import USGS Daily Streamflow Data\n",
    "    - 4.3.4. Import NWM Daily Runoff Data\n",
    "    - 4.3.5. Merge the Different Datasets for Each River Basin\n",
    "    - 4.3.6. Calculate the Cumulative Values Across the Water Year for Each Watershed\n",
    "    - 4.3.7. Plot the Cumulative Precipitation and Runoff for Each Watershed\n",
    "  - Analysis 4.4. Evaluate Great Salt Lake Shrinkage Using LANDSAT Data\n",
    "    - 4.4.1. Download LANDSAT Data Using Google Earth Engine and Perform a Normalized Difference Water Index Analysis\n",
    "    - 4.4.2. Load LANDSAT CSV Files from GEE into Python and Plot a Time Series of the Annual Changes of NWDI over the Great Salt Lake\n",
    "- 5. Geoethical Considerations\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c1c1d-014d-482d-b7db-ad5583275187",
   "metadata": {},
   "source": [
    "# 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff48f2-7cd5-4bad-bdd8-345757cecf17",
   "metadata": {},
   "source": [
    "**What does this notebook do?**\n",
    "\n",
    "This notebook allows you to perform a set of **four analyses** to explore hydrologic drivers of changes in the Great Salt Lake. Most of these analyses focus on the National Water Model (NWM) from the National Oceanic and Atmospheric Administration (NOAA) as there is currently an effort to make the NWM more robust for predicting water supply. Water managers in the Great Salt Lake are interested in understanding the water balance of the basin, and the NWM plays a crucial role in this effort.    \n",
    "\n",
    "> - <font color='blue'> **Analysis 4.1** </font>: We compare the NWM input precipitation dataset, obtained from the Analysis of Record for Calibration (AORC version 1.0) dataset, with a commonly used precipitation dataset in hydrological studies called PRISM across the three watersheds feeding into the Great Salt Lake. This exercise attempts to assess both how short-term averages for the Great Salt Lake compare to long-term averages, and how AORC might be performing compared to PRISM. \n",
    "> - <font color='blue'> **Analysis 4.2** </font>: We compare NWM simulated streamflow to observations from the United States Geological Survey (USGS). \n",
    "> - <font color='blue'> **Analysis 4.3** </font>: We compare cumulative precipitation and runoff, accumulated from October 1 through September 30 for each water year, between AORC and PRISM, as well as NWM and USGS, for the water years 2016-2019. The second and third analyses explore how accurately the NWM can predict water supply in the main tributaries to the Great Salt Lake, and how that relates to the input precipitation dataset. \n",
    "> - <font color='blue'> **Analysis 4.4** </font>: We use Landsat data to create a numerical time series and a visualization of how the area of the Great Salt Lake has changed between 1984-2022.\n",
    "\n",
    "**Notes on code and system requirements**: This notebook summarizes work performed at the I-GUIDE Summer School 2023 by Team 4, comprised of team lead Dr. Irene Garousi-Nejad and participants listed under Authors. We intend this to be set up as a lesson that is reusable on the I-GUIDE platform by using APIs to pull datasets and conducting the computational steps using computational resources being available by the I-GUIDE platform and computational resources. \n",
    "\n",
    "Since the code is reusable, a user does not *need* prior experience with Python, but it will be most useful for people who have seen Python code before. Some cells use bash (i.e., the Linux-based language you use to interact with your operating system) instead. We make note of this in comments (comments are all preceded by a \"#\" so the system doesn't read them as code). Some familiarity with bash is useful but not necessary since it is a minor part of the code. There is one javascript code block at the end of the notebook that you can copy/paste into Google Earth Engine to recreate the GIF image we use to show shrinkage in the Great Salt Lake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5869e-8d70-40a1-bf52-6013cbfccf49",
   "metadata": {},
   "source": [
    "# 2. Study Area and Motivation\n",
    "[Back to the table of content](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861b6b4-4834-4e99-8ea9-00b97d9e3411",
   "metadata": {},
   "source": [
    "In the northwestern corner of Utah, sits the Great Salt Lake, the largest saltwater lake in the western hemisphere and eighth largest in the world. This lake is fed primarily by surface runoff from rain and snowmelt in the Bear, Weber, and Jordan-Provo River watersheds. No rivers drain from the lake, meaning evaporation is the only outlet. Because of this, the Great Salt Lake has become incredibly saline, at the same time producing a crucial ecosystem for migratory birds who feed on brine shrimp and brine flies – some of the only species who can live there – as well as important industries like salt production and food processing for aquaculture (fish food). Today, the Great Salt Lake faces a crisis as *water levels are rapidly declining*, threatening not only ecologic and economic vitality, but also increasing the potential of human health risks associated with inhaling dust from newly exposed shorelines carrying increased levels of toxic metal mixtures (ATSDR 2023; Thorsen et al., 2017). \n",
    "\n",
    "Aside from evaporation at its surface, the lake loses water from salt evaporation ponds used for mineral extraction, and water destined for the lake is diverted for agriculture, cities, and other uses (Null & Wurtsbaugh, 2020). Therefore, advisory boards comprised of scientists, policymakers, and other stakeholders are interested in understanding the total water budget for the lake and the three watersheds that feed it to inform any actionable change (Eisenhauer & Nicholson, 2005).\n",
    "\n",
    "Thus, our analyses were driven by the following research question: **can the NWM be used to predict water supply in the Great Salt Lake? Our objective was to explore the hydrologic responses affecting the Great Salt Lake’s water availability through these four analyses.**\n",
    "\n",
    "<br />\n",
    "<center><img src=\"https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/study-site.png\" width=500 height=500  /></center>\n",
    "<br />\n",
    "\n",
    "The **Great Salt Lake Basin** (shown above) is comprised of three sub-basins: <font color='slateblue'>**Bear River**</font>, <font color='gray'>**Weber River**</font>, and <font color='sandybrown'>**Jordan-Provo**</font>. USGS streamgages are denoted by black stars. The primary rivers flowing to the Great Salt Lake are denoted by blue lines. \n",
    "\n",
    "\n",
    "**References**:\n",
    "\n",
    "- (ATSDR) Agency for Toxic Substances and Disease Registry. (2023, March 17). Substance Priority List. https://www.atsdr.cdc.gov/spl/index.html\n",
    "- Eisenhauer, B. W., & Nicholson, B. (2005). Using Stakeholders’ Views: A Social Science Methodology for the Inclusive Design of Environmental Communications. Applied Environmental Education & Communication, 4(1), 19–30. https://doi.org/10.1080/15330150590910701\n",
    "- Null, S. E., & Wurtsbaugh, W. A. (2020). Water Development, Consumptive Water Uses, and Great Salt Lake. In B. K. Baxter & J. K. Butler (Eds.), Great Salt Lake Biology: A Terminal Lake in a Time of Change (pp. 1–21). Springer International Publishing. https://doi.org/10.1007/978-3-030-40352-2_1\n",
    "- Thorsen, M. L., Handy, R. G., Sleeth, D. K., Thiese, M. S., & Riches, N. O. (2017). A comparison study between previous and current shoreline concentrations of heavy metals at the Great Salt Lake using portable X-ray fluorescence analysis. Human and Ecological Risk Assessment: An International Journal, 23(8), 1941–1954. https://doi.org/10.1080/10807039.2017.1349541\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4d0a4-19a6-45a3-9bba-04850970f75d",
   "metadata": {},
   "source": [
    "# 3. Required libraries to run this notebook\n",
    "[Back to the table of content](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79e0ae-6b2a-4b07-8f6c-cc589fddd07e",
   "metadata": {},
   "source": [
    "To configure your Jupyter environment effectively, you should employ the `Bash` shell to install the necessary packages by executing the `!pip install <package-name>` command, where the '!' notation designates it as Bash rather than Python syntax. Note that the --quiet flag is used to suppress or reduce the amount of output and messages that are displayed in the terminal or command prompt \n",
    "\n",
    "Confirm that your Jupyter **kernel** is set to utilize the <font color='blue'>conda env: iguide</font> environment. Be aware that the package installation process may take around **1 minute** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb9bee-0082-455c-9728-75cd81c40f87",
   "metadata": {
    "id": "76bb9bee-0082-455c-9728-75cd81c40f87",
    "outputId": "b42a0173-ff98-4c63-f9c0-434d6d82c92e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cartopy --quiet\n",
    "!pip install rasterstats --quiet\n",
    "!pip install geopandas --quiet\n",
    "!pip install unzip --quiet\n",
    "!pip install hsclient --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install zarr --quiet\n",
    "!pip install openpyxl --quiet\n",
    "!mamba install unzip --y --quiet\n",
    "!pip install dataretrieval --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3b308-ce8a-4763-ba6b-1328fd0e187b",
   "metadata": {
    "id": "d8f3b308-ce8a-4763-ba6b-1328fd0e187b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this code block is unnecessary, but we use it to suppress warnings when importing the packages in the code block below to the Python kernel\n",
    "# if you want to see the warnings (dependency conflicts, etc) you can remove this block\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c1d8e-3da5-47f0-88f9-0d8973e3029d",
   "metadata": {
    "id": "721c1d8e-3da5-47f0-88f9-0d8973e3029d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all the libraries we'll need across the analyses\n",
    "\n",
    "# entire libraries\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import subprocess\n",
    "import fsspec\n",
    "import shutil\n",
    "import zipfile\n",
    "import glob\n",
    "import re\n",
    "import dask\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "\n",
    "# specific functions\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from rasterstats import zonal_stats\n",
    "from hsclient import HydroShare\n",
    "from geopandas import GeoSeries, GeoDataFrame, read_file, gpd\n",
    "from shapely.geometry import MultiPolygon\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from matplotlib import colors\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import progress\n",
    "from dataretrieval import nwis\n",
    "from IPython.display import display\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95575e-2ea2-490c-add8-388c84097ecb",
   "metadata": {},
   "source": [
    "# 4. Computational Analyses and Results\n",
    "[Back to the table of content](#Table-of-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efaaf6-e0e4-422d-b269-12380e15db43",
   "metadata": {
    "id": "69efaaf6-e0e4-422d-b269-12380e15db43",
    "tags": []
   },
   "source": [
    "## <font color='blue'> **Analysis 4.1.** </font> Compare the input precipitation data from the National Weather Model sourced from AORC with PRISM data for the three watersheds\n",
    "\n",
    "<div style=\"background-color: whitesmoke; padding: 10px; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2); color: #333;\">\n",
    "\n",
    "This analysis compares the precipitation product that was used as an input to the NWM retrospective simulations to another commonly used precipitation product deveoped at Oregon State University called Parameter-elevation Regressions on Independent Slopes Mode, or [PRISM](https://prism.oregonstate.edu/). The point of this exercise is to determine how the AORC precipitation data compares to other datasets.  \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31817ce5-edd4-4a93-9f9b-3147ecfc8046",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first analysis includes two main steps:  <b/>\n",
    "\n",
    "**Step A.** Prepare the spatially averaged <font color='blue'>PRISM </font> precipitation data for each watershed\n",
    "> 4.1.1. Download PRISM monthly normals precipitation data \\\n",
    "> 4.1.2. Use consistent projections \\\n",
    "> 4.1.3. Subset data and calculate spatial average of the PRISM precipitation across each watershed.  \\\n",
    "> 4.1.4. Plot a time series of 30-year average precipitation for one watershed\n",
    "\n",
    "\n",
    "**Step B.** Prepare the spatially averaged <font color='blue'>AORC</font> precipitation data for each watershed\n",
    "> 4.1.5. Import the AORC forcing dataset and aggregate to monthly and daily values\\\n",
    "> 4.1.6. Plot PRISM vs. AORC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe6b15-a244-483d-b54e-8e6b0004f611",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Delving Deeper: Before delving into the detailed explanations of Steps 4.1.1 and Step 4.1.2, let's first explore and understand the geographical domain of our watershed. This understanding will lay the foundation for calculating the spatial average precipitation.  </b> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877159a-6933-4253-a6e3-68b610c48a6e",
   "metadata": {
    "id": "d877159a-6933-4253-a6e3-68b610c48a6e"
   },
   "source": [
    "In this first section, we'll illustrate some features of one of our watersheds of interest, the <font color='blue'> Weber River </font>. We won't use any of the outputs from this section for later analysis, but it will introduce you to some hydrological concepts and spatial analysis concepts that will be useful for later sections.\n",
    "\n",
    "First, we download shapefiles for plotting, which we stored in a [Hydroshare repository](https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/) to make them persistently available for this analysis. You will need a Hydroshare account to access the files, but it's free to sign up! Make sure you have your username and password on hand. The section below uses the [HydroShare Python Client Library](https://www.hydroshare.org/resource/7561aa12fd824ebb8edbee05af19b910/) (**`hsclient`**) to retrieve the data from the Hydroshare resource containing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1123f3-ec12-4da1-9ffd-39ae19f9dcbc",
   "metadata": {
    "id": "3f1123f3-ec12-4da1-9ffd-39ae19f9dcbc",
    "outputId": "3f0ae666-0e9a-4242-e139-bdc957396c52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initiate access to the Hydroshare API\n",
    "# this will ask for your username and password\n",
    "hs = HydroShare()\n",
    "hs.sign_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4587bb-6f0c-429c-b9b4-7fc81608be3d",
   "metadata": {
    "id": "ed4587bb-6f0c-429c-b9b4-7fc81608be3d",
    "outputId": "c9abb40b-4e69-464d-b512-86f5f28dede1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the resource you want to download by its unique Hydroshare ID - we have already filled this in for you\n",
    "# this can take a few minutes and will output the name of a .zip file as a string\n",
    "res = hs.resource('8e2fa6f38ce142e09fa1eb999cd5f248')\n",
    "res.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950f183-9b37-45c8-ba06-5fca3d743084",
   "metadata": {
    "id": "3950f183-9b37-45c8-ba06-5fca3d743084",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use bash to unzip the files\n",
    "# the -q flag means \"quiet\" so we don't print the very long output, but you can remove it to see all the files that are being unzipped\n",
    "!unzip -q 8e2fa6f38ce142e09fa1eb999cd5f248.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e3dca-653f-4cb1-a72f-99cc826fc932",
   "metadata": {
    "id": "a47e3dca-653f-4cb1-a72f-99cc826fc932"
   },
   "source": [
    "You should see a new folder in your working directory now. Your data are in `8e2fa6f38ce142e09fa1eb999cd5f248/8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/`.\n",
    "\n",
    "Now we'll use the Python libraries `MultiPolygon`, `Shapely`, and `Matplotlib` to format and plot some of the spatial data we've downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e72a5c-6ab2-465a-857b-623d29e1f022",
   "metadata": {
    "id": "80e72a5c-6ab2-465a-857b-623d29e1f022",
    "outputId": "b0a53012-1f8e-4517-e990-2cd339cbb031",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the data\n",
    "data_path = \"./8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/GISBasins/catchment\"\n",
    "\n",
    "# select the shapefile for the Weber River watershed\n",
    "mp = MultiPolygon(Reader(os.path.join(data_path, 'WeberRiverBasin.shp')).geometries())\n",
    "\n",
    "# read the geometries for plotting\n",
    "shape_feature = ShapelyFeature(mp.geoms,\n",
    "                                ccrs.PlateCarree(), facecolor='none')\n",
    "\n",
    "# visualize data on the map\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "\n",
    "shape_feature = ShapelyFeature(mp.geoms,\n",
    "                                ccrs.PlateCarree(), facecolor='none')\n",
    "ax.add_feature(shape_feature, zorder=1)\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--');\n",
    "\n",
    "# modify the x and y limits based on the watershed's bounding box information\n",
    "ax.set_ylim([40.5, 41.5]);\n",
    "ax.set_xlim([-112.35, -110.75]);\n",
    "ax.set_aspect('equal');\n",
    "ax.coastlines();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5307f4-a663-43d1-ab82-1167a28bebf3",
   "metadata": {
    "id": "3c5307f4-a663-43d1-ab82-1167a28bebf3"
   },
   "source": [
    "What you're seeing in the plot above are the two \"HUC8\" watersheds that the Weber River flows through. HUC stands for Hydrologic Unit Code and is a standard nested system to define watershed boundaries. The higher the number defining a HUC code (e.g., 2, 4, 6, 8), the smaller the unit. Each HUC8, for example, is nested within a particular HUC6 watershed, which is nested within a HUC4 watershed, and so on. [Here](https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/images/huc_visual_final.png) is a graphic from the USGS that helps define it.\n",
    "\n",
    "Our data are at HUC8 resolution. We want to calculate spatially averaged precipitation across the drainage areas for each of our three rivers of interest. To do that, we need to \"dissolve\" the HUC8 units into a single unit. The spatial analysis function `zonal_stats` from the `rasterstats` package we use later does this internally, but we want to perform this function manually and plot the output to get an understanding of what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc904d5-9a40-4a57-8d4d-0f55306108c5",
   "metadata": {
    "id": "cfc904d5-9a40-4a57-8d4d-0f55306108c5",
    "outputId": "bcd4e23a-b03b-48f3-a0a7-eb85acad2c2b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-read the Weber River shapefile as a dataframe\n",
    "watershed = read_file(os.path.join(data_path, 'WeberRiverBasin.shp'))\n",
    "\n",
    "# add a column with a constant value that will be used to dissolve the shapefile\n",
    "watershed['temp']=1\n",
    "\n",
    "# dissolve\n",
    "watershed_dis = watershed.dissolve(by = 'temp', aggfunc = 'sum')\n",
    "\n",
    "# read the geometries for plotting\n",
    "shape_feature = ShapelyFeature(watershed_dis.geometry,\n",
    "                                ccrs.PlateCarree(), facecolor='none')\n",
    "\n",
    "# visualize data on the map\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "\n",
    "shape_feature = ShapelyFeature( watershed_dis.geometry,\n",
    "                                ccrs.PlateCarree(), facecolor='none')\n",
    "ax.add_feature(shape_feature, zorder=1)\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--');\n",
    "\n",
    "# modify the x and y limits based on the watershed's bounding box information\n",
    "ax.set_ylim([40.5, 41.5]);\n",
    "ax.set_xlim([-112.35, -110.75]);\n",
    "ax.set_aspect('equal');\n",
    "ax.coastlines();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af5956-df47-4a00-aa80-14cfd97cb71a",
   "metadata": {
    "id": "18af5956-df47-4a00-aa80-14cfd97cb71a"
   },
   "source": [
    "Now you can see we only have one geometry for the Weber River, instead of the two HUC8 units. Again, `zonal_stats` will do this for us for each watershed, but now we understand what is happening \"under the hood.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba5b33-ab6f-4708-bb25-a47409f3e098",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> Note that the watersheds we're analyzing (the Bear, Weber, and Jordan-Provo rivers) have a different number of relevant HUC8 units. The Bear River has seven HUC8s included, while the Jordan-Provo River has three and Weber River has two HUC8 units. We are going to do separate analyses for each river. In geospatial analysis, we often think about the Modifiable Areal Unit problem, which says that spatial statistics you derive at one scale are not valid at other scales, so the resolution of your analysis matters. If you wanted to dissolve the watersheds for all 3 rivers into one big geometry representing all the inflow into the Great Salt Lake, or otherwise statistically compare these three watersheds, you would need to be cautious that they comprise a different number of HUC8s and that can affect the results of your analyses. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8b63d-505b-4db5-ae01-04f25f4f16f4",
   "metadata": {
    "id": "56e8b63d-505b-4db5-ae01-04f25f4f16f4",
    "tags": []
   },
   "source": [
    "### 4.1.1. Download PRISM Monthly Normals Precipitation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380a2f7-958c-44bc-bc3d-136a9de9cbfd",
   "metadata": {
    "id": "8380a2f7-958c-44bc-bc3d-136a9de9cbfd"
   },
   "source": [
    "Now we'll download precipitation data from the PRISM dataset from their web service so we can calculate spatially averaged precipitation over each of the three river catchments. PRISM Climate Group collects climate observations \"from a wide range of monitoring networks, applies sophisticated quality control measures, and develops spatial climate datasets to reveal short- and long-term climate patterns.\" Their precipitation datasets are widely used in hydrology. We download 30-year average precipitation conditions across the continental US (CONUS) for each month from their **\"monthly normals\"** dataset. You can find out more about it here: https://prism.oregonstate.edu/normals/\n",
    "\n",
    "PRISM provides a single file per request, and we have one request per month. The files we're requesting are in BIL format. BIL means “band interleaved by line\" and is an uncompressed file containing the actual pixel values of an image. Run the following bash script to perform a bulk download of multiple BIL files.\n",
    "\n",
    "The script creates a folder called PRISM_monthly_normals to store the results (if it does not already exist in your working directory). Then we print just the names of the files that we want to download to check what's available. Lastly, the code retrieves a file for each month, *_bil.zip, and saves them in PRISM_monthly_normals. The last block unzips the zip files using bash and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329c54e-d06a-439c-9e18-e4f0ba4c40d3",
   "metadata": {
    "id": "9329c54e-d06a-439c-9e18-e4f0ba4c40d3",
    "outputId": "a2987542-a658-4543-f494-c14678b51828",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# this entire cell block is written in bash, so instead of using the ! notation from before, we tell Jupyter\n",
    "# to read the whole cell as a bash script using \"%%\"\n",
    "\n",
    "# define the folder name\n",
    "folder=\"./PRISM_monthly_normals\"\n",
    "\n",
    "# check if the folder already exists or not\n",
    "if [ ! -d \"$folder\" ]; then\n",
    "    mkdir -p \"$folder\"\n",
    "    echo \"Directory created: $folder\"\n",
    "else\n",
    "    echo \"Directory already exists: $folder\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c8ebd-9abf-4051-8268-dc413a5f7b0b",
   "metadata": {
    "id": "b19c8ebd-9abf-4051-8268-dc413a5f7b0b",
    "outputId": "823911f4-d071-4352-e68b-a4f5a7887b1f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for m in {01..12};do #for loop from jan-dec because these are monthly data (doesn't load)\n",
    "    echo https://ftp.prism.oregonstate.edu/normals_800m/ppt/PRISM_ppt_30yr_normal_800mM4_${m}_bil.zip\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88056d-7692-4da9-9240-5db2456ec275",
   "metadata": {
    "id": "3c88056d-7692-4da9-9240-5db2456ec275",
    "outputId": "c5b15dfb-f4e3-4895-9ecf-53237775e931",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# for each month (in 2-digit notation, e.g., 01 for January, 02 for February...), download the PRISM data\n",
    "for m in {01..12};do\n",
    "    echo \"Downloading data for Month: $m\"\n",
    "    url=\"https://ftp.prism.oregonstate.edu/normals_800m/ppt/PRISM_ppt_30yr_normal_800mM4_${m}_bil.zip\"\n",
    "    wget -q \"$url\" -P ./PRISM_monthly_normals\n",
    "    sleep 4\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9645a-b095-416b-922b-abd6a5ccae04",
   "metadata": {
    "id": "2bc9645a-b095-416b-922b-abd6a5ccae04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now we need to unzip the files, so we set the filepath for the folder the PRISM files are in\n",
    "folder=\"./PRISM_monthly_normals\"\n",
    "\n",
    "# loop through the PRISM files and unzip each one (but suppress output with -q)\n",
    "for file in \"$folder\"/*.zip; do\n",
    "    unzip -q \"$file\" -d \"$folder\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda7ded-31c2-4a9e-aa77-044f21b41623",
   "metadata": {
    "id": "3bda7ded-31c2-4a9e-aa77-044f21b41623"
   },
   "source": [
    "You should see a lot of files in your PRISM_monthly_normals folder now.\n",
    "\n",
    "Let's visualize the precipitation data for a single month to see what we're working with. The code bock below uses the Python library rasterio to import the data for January. We use Matplotlib to plot the map, with different colors for different ranges of precipitation in millimeters (mm). You can change the bounds of precipitation included in each color category in the line `bounds=[0, 50, 100, 200, 600, img_precip[0].max()]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c92bc-7f95-4033-9add-424933aa5d63",
   "metadata": {
    "id": "fb3c92bc-7f95-4033-9add-424933aa5d63",
    "outputId": "d01c0611-af0e-465a-8cec-0c029c8c88b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use rasterio to import the data as img\n",
    "with rio.open(\"./PRISM_monthly_normals/PRISM_ppt_30yr_normal_800mM4_01_bil.bil\") as src:\n",
    "    boundary = src.bounds\n",
    "    img_precip = src.read()\n",
    "    nodata = src.nodata\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.title(\"Precipitation\", size=16)\n",
    "cmap = colors.ListedColormap(['cyan', 'skyblue', 'deepskyblue', 'royalblue', 'navy'])\n",
    "cmap.set_under('w')\n",
    "bounds=[0, 50, 100, 200, 600, img_precip[0].max()]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "imgplot = plt.imshow(img_precip[0], cmap=cmap, norm=norm)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Precipitation (mm)', size=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048a5b6-f7e2-4d9d-85fc-b15d411d8576",
   "metadata": {
    "id": "6048a5b6-f7e2-4d9d-85fc-b15d411d8576"
   },
   "source": [
    "As you might expect, we see more precipitation in the Pacific Northwest and in the Southeast, while there is relatively very little precipitation across the Intermountain West. For our time series analysis we'll subset the data to just our watersheds of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94b5e4-35a9-44b9-9ed3-b54f4e7ddc3b",
   "metadata": {
    "id": "dc94b5e4-35a9-44b9-9ed3-b54f4e7ddc3b",
    "tags": []
   },
   "source": [
    "### 4.1.2. Use consistent projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc4777-5ed8-4073-9b0c-8f7ce979bc59",
   "metadata": {
    "id": "4efc4777-5ed8-4073-9b0c-8f7ce979bc59"
   },
   "source": [
    "As is often the case with GIS, there is a need to have consistent coordinate projections. GDAL and OGR are both translation libraries for raster and geospatial data formats and provide many useful, widely-used geospatial analysis tools.\n",
    "\n",
    "The following GDAL and OGR command examines the projections of the precipitation data and shapefile, respectively. Each command prints a lot of metadata, but the line we're interested in for both begins with \"ID[...]\". This is where we see what projection is used. For the PRISM data, it's **ESPG #6269**, but for the shapefile of the river basin it's **ESPG #4269**.\n",
    "\n",
    "If you're familiar with projection information, you might notice that the UNIT is \"Degree\", and the Pixel Size is 0.008333333333333 degree. This is because we haven't projected the data yet, so it only has a geographic coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92ca80-b0f1-4bab-a2e9-b3e65236e5cb",
   "metadata": {
    "id": "4f92ca80-b0f1-4bab-a2e9-b3e65236e5cb",
    "outputId": "711075a9-8cfe-4333-fa45-825903716c7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the projection information of the PRISM data\n",
    "!gdalinfo -proj4 ./PRISM_monthly_normals/PRISM_ppt_30yr_normal_800mM4_01_bil.bil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5feb0-7820-4a60-b59d-0c9b07c2490c",
   "metadata": {
    "id": "1af5feb0-7820-4a60-b59d-0c9b07c2490c",
    "outputId": "9eec604d-5012-40fc-b1ea-8c947a645cd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the projection information for the watershed shapefile (we pipe this into the \"head\" command so we only see the metadata rows)\n",
    "!ogrinfo -al ./8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/GISBasins/catchment/WeberRiverBasin.shp | head -n56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472d89e-6388-41a2-937f-0b4917d9af60",
   "metadata": {
    "id": "4472d89e-6388-41a2-937f-0b4917d9af60"
   },
   "source": [
    "Now that we know the projection of the shapefile, we can remap the PRISM data using the correct projection. We use the **`gdalwarp`** function to assign the projection of the shapefile to each of the PRISM files. We create a new directory to store these results (unless something with the same name already exists, e.g. you ran this notebook previously) and loop through all the PRISM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4198c3-8a3d-4005-85ee-ec5957a5c2d2",
   "metadata": {
    "id": "1e4198c3-8a3d-4005-85ee-ec5957a5c2d2",
    "outputId": "6c566883-09c1-4987-cea1-128e80d7e34b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of all .bil files in the folder\n",
    "bil_files = [file for file in os.listdir(\"./PRISM_monthly_normals\") if file.endswith('.bil')]\n",
    "bil_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422665e-8145-4ce0-9ce3-81884d9a5907",
   "metadata": {
    "id": "f422665e-8145-4ce0-9ce3-81884d9a5907",
    "outputId": "96572819-a7df-4d4f-d029-f3df760ae1d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# we're using bash to create a file for the re-mapped PRISM data if it does not already exist\n",
    "# define the folder name\n",
    "folder=\"./PRISM_monthly_normals/outputs\"\n",
    "\n",
    "# check if the folder already exists or not\n",
    "if [ ! -d \"$folder\" ]; then\n",
    "    mkdir -p \"$folder\"\n",
    "    echo \"Directory created: $folder\"\n",
    "else\n",
    "    echo \"Directory already exists: $folder\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0738a67-ce98-431a-8dbc-719fca37ffc2",
   "metadata": {
    "id": "c0738a67-ce98-431a-8dbc-719fca37ffc2",
    "outputId": "c8f02140-4d46-4801-c835-b517d1f746e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the filepaths for your input and output folders (output folder was created above if necessary)\n",
    "input_folder = './PRISM_monthly_normals'\n",
    "output_folder = './PRISM_monthly_normals/outputs'\n",
    "\n",
    "# loop through each .bil file and use gdalwrap to covert the projection\n",
    "for bil_file in bil_files:\n",
    "\n",
    "    # specify the input and output file paths\n",
    "    input_file = os.path.join(input_folder, bil_file)\n",
    "    output_file = os.path.join(output_folder, bil_file)\n",
    "\n",
    "    # construct the gdalwarp command\n",
    "    gdalwarp_cmd = f'gdalwarp -overwrite -t_srs EPSG:4269 {input_file} {output_file}'\n",
    "\n",
    "    # execute the gdalwarp command using subprocess\n",
    "    subprocess.run(gdalwarp_cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "# we silence the output from the subprocess command above because it is very long, but we want to confirm\n",
    "# it has finished running because it takes a few seconds, so we use a print statement\n",
    "print(\"Subprocess command completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c43532-437f-460b-9ece-0b5464219d50",
   "metadata": {
    "id": "39c43532-437f-460b-9ece-0b5464219d50",
    "tags": []
   },
   "source": [
    "### 4.1.3. Subset PRISM Data for the Watershed and calculate the spatial average precipitation across the watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2bb52-399e-4ce6-9e72-9c2c0135a58b",
   "metadata": {
    "id": "4de2bb52-399e-4ce6-9e72-9c2c0135a58b"
   },
   "source": [
    "Now that we have the precipitation data in the same projection format as our watersheds of interest, we can calculate spatially averaged precipitation for each watershed. To do this, we make a list of the river basin names, then loop through all the watersheds and use **`zonal_stats`** to select the precipitation data we want to use and the geographic boundary we want to clip. `zonal_stats` is the function that is dissolving the smaller HUC8 units, similar to what we showed above.\n",
    "\n",
    "Note that we are interested in the **`mean`** values, so we create a dataframe that contains dates and spatially averaged daily precipitation values. At the end of this section, you will have output three CSV files within the <font color=red> ./PRISM_monthly_normals/outputs </font> folder- one with the average monthly precipitation from January-December for each watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2db0a3-185f-4006-8e80-a54fc3e30441",
   "metadata": {
    "id": "bf2db0a3-185f-4006-8e80-a54fc3e30441",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create list of watershed names as string for accessing files within for loops in the next few sections\n",
    "watershed_lst = ['WeberRiverBasin', 'BearRiverBasin', 'Jordan-ProvoRiverBasin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6adad-8819-40c2-9030-026156c5e845",
   "metadata": {
    "id": "3ae6adad-8819-40c2-9030-026156c5e845",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of all .bil files in the folder\n",
    "bil_files = [file for file in os.listdir(\"./PRISM_monthly_normals/outputs\") if file.endswith('.bil')]\n",
    "\n",
    "# for each watershed, loop through each .bil file\n",
    "for watershed in watershed_lst:\n",
    "\n",
    "    # create a list so we can store the name of the month our rows of data are from\n",
    "    month=[]\n",
    "\n",
    "    # create a list to store spatially averaged precipitation volumes\n",
    "    p=[]\n",
    "\n",
    "    # for the BIL files for all the subcatchment\n",
    "    for bil_file in bil_files:\n",
    "\n",
    "        # clip the data so we only use BIL files for the current watershed\n",
    "        stats=zonal_stats(f\"./8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/GISBasins/catchment/{watershed}.shp\", f\"./PRISM_monthly_normals/outputs/{bil_file}\")\n",
    "        # append the month to the month list\n",
    "        month.append(int(bil_file.split(\"_\")[-2]))\n",
    "        # append the spatially averaged precipitation to the p list\n",
    "        # note this value represents the 30-year average TOTAL precipitation e.g., across January for the entire Weber basin\n",
    "        p.append(stats[0]['mean'])\n",
    "\n",
    "    # convert the month/precipitation lists to a single dataframe\n",
    "    df = pd.DataFrame({'Month': month, 'Precipitation (mm)': p})\n",
    "    # sort the dataframe based on dates\n",
    "    df = df.sort_values(by='Month')\n",
    "    # save the dataframe as a CSV file\n",
    "    df.to_csv(f'./PRISM_monthly_normals/outputs/PRISM_Monthly_Normal_Precipitation_{watershed}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144ecce-e7a0-4c21-977e-51cf8526b28d",
   "metadata": {
    "id": "e144ecce-e7a0-4c21-977e-51cf8526b28d",
    "tags": []
   },
   "source": [
    "### 4.1.4. Plot a time series of 30-year average precipitation for one watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08083021-ba40-4b53-8699-c51244a7719b",
   "metadata": {
    "id": "08083021-ba40-4b53-8699-c51244a7719b"
   },
   "source": [
    "Let's visualize the time series of precipitation across the year for one of the watersheds. The `df` variable has the data saved for the **last watershed** in the `for` loop above, so we can just use that dataframe. We'll plot the time series for all the watersheds, and compare to another dataset, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373eb1c3-b821-403e-98a5-a0fa6769726c",
   "metadata": {
    "id": "373eb1c3-b821-403e-98a5-a0fa6769726c",
    "outputId": "ef84da41-d579-4e3a-f899-cf321b6f73dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "import matplotlib.pyplot as plt\n",
    "ax.plot(df['Month'], df['Precipitation (mm)'], color='b')\n",
    "ax.set_ylabel('Depth (mm)', size=18)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.set_title('PRISM monthly normals precipitation averaged across the watershed', size=16)\n",
    "ax.annotate('Month index starts from Janurary',xy=(0.2, 0.05), xycoords='axes fraction', fontsize=12, ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7e718-47ba-4590-8f53-193453b1d9f7",
   "metadata": {
    "id": "eff7e718-47ba-4590-8f53-193453b1d9f7"
   },
   "source": [
    "As we'd expect for a watershed in the arid Southwest, precipitation is lowest in the summer (with a minimum in July). We now have a time series for 30-year average precipitation for each watershed. In the next sections, we download another precipitation dataset to compare how precipitation patterns have changed in the last five years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cbb3b-2006-4197-b1f7-f99382f98a65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> Results for other watersheds will be plotted in the next section. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51683ed-d641-4dad-a57e-76d6b288f887",
   "metadata": {},
   "source": [
    "In the following section, we download a second precipitation dataset. This is the Analysis of Record for Calibration (AORC) dataset, which is a forcing dataset used in the NWM developed by NOAA. You can read the documentation for the dataset from the Office of Water Prediction [here](https://hydrology.nws.noaa.gov/aorc-historic/Documents/AORC-Version1.1-SourcesMethodsandVerifications.pdf).\n",
    "\n",
    "Essentiallly, this is the precipitation dataset that the NWM uses to model streamflow and other hydrological states and fluxes across the continental U.S. We want to calculate the monthly 5-year averages for each watershed from 2015-2019 and compare it to the 30-year normals we calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f5b95-4a66-4d3c-a1dd-73cb5e0df9ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> To make this comparison more robust, we could calculate 5-year averages from daily PRISM dataset. However, the data processing requirements are much more significant and not possible to accomplish in the 5-day Summer School. Future work could re-run the analysis using PRISM daily values instead of AORC. Another option would be to calculate 30-year averages for AORC and compare the 30-year averages to the PRISM 30-year normals, to assess the input data quality for the NWM compared to the field standard. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61513fbc-b753-4bee-aabb-9ccf2849ce2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1.5. Import the AORC forcing dataset and aggregate to monthly and daily values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf886cb-1b0f-48a7-ae94-6d7b9965a947",
   "metadata": {},
   "source": [
    "We have already obtained the AORC dataset when importing data from our [Hydroshare repository](https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/). Our team leader has previously obtained and processed the hourly precipitation data for each watershed using the codes available in this [GitHub repository](https://github.com/igarousi/gsl-hydrology/tree/master/aorc-precipitation). It's worth noting that these codes are currently in the development stage, and the computational efficiency is not yet optimal. The reason for providing pre-processed data is to save time, avoiding the need for time-consuming data access and processing during the summer school. These are stored in a zipped folder. To proceed, we will utilize Bash to unzip the folders, making the AORC data accessible for each watershed. The data is hourly and in units of millimeters per second **(mm/s)**. \n",
    "\n",
    "Our objective is to calculate **5-year monthly averages** for each watershed. To achieve this, we need to perform a spatial aggregation across all the HUC8s within each basin. Additionally, we will convert the precipitation units from mm/s (a rate) to mm (a depth). We'll also need **daily averages** for each watershed for a later analysis; since the code is the same, we'll do both of these aggregations at once and save different CSV files for the different temporal aggregations. This helps us avoid having unecessary repetition of `for` loops, effectively reducing computational overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871638f-c4ca-4836-8d85-b399517e806c",
   "metadata": {
    "id": "6871638f-c4ca-4836-8d85-b399517e806c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# You only need to run this code block once - don't rerun if you're rerunning other cells in the notebook,\n",
    "# otherwise you'll unzip all the watershed data files multiple times\n",
    "\n",
    "folder=\"./8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/AORC\"\n",
    "\n",
    "for file in \"$folder\"/*.zip; do\n",
    "    unzip -q \"$file\" -d \"$folder\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccac00-8e09-4fdd-a625-f45277c86a06",
   "metadata": {
    "id": "60ccac00-8e09-4fdd-a625-f45277c86a06",
    "outputId": "218636af-afad-4583-e10d-4a177761cdee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# now we'll use bash make an output folder for our aggregated AORC data (if it doesn't already exist)\n",
    "folder=\"./AORC_outputs\"\n",
    "\n",
    "# check if the folder already exists or not\n",
    "if [ ! -d \"$folder\" ]; then\n",
    "    mkdir -p \"$folder\"\n",
    "    echo \"Directory created: $folder\"\n",
    "else\n",
    "    echo \"Directory already exists: $folder\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35873dc-75b2-4885-b480-a71cbfa6e17c",
   "metadata": {
    "id": "b35873dc-75b2-4885-b480-a71cbfa6e17c"
   },
   "source": [
    "The next section includes a very long `for` loop to convert the hourly data for all constituent HUC8s to daily and monthly data for each watershed, so we've included more comment structures to help you see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0b6c8-ad59-4736-80fe-cf794620ce29",
   "metadata": {
    "id": "53d0b6c8-ad59-4736-80fe-cf794620ce29",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of watersheds to loop through\n",
    "watershed_lst = ['WeberRiverBasin', 'BearRiverBasin', 'Jordan-ProvoRiverBasin']\n",
    "\n",
    "############################\n",
    "# Loop over each watershed #\n",
    "############################\n",
    "for watershed in watershed_lst:\n",
    "\n",
    "    # set a string that corresponds to the name of the directory for each watershed in the AORC folder\n",
    "    if watershed == 'WeberRiverBasin':\n",
    "        tmp_str = 'weber'\n",
    "    elif watershed == 'BearRiverBasin':\n",
    "        tmp_str = 'bear'\n",
    "    else:\n",
    "        tmp_str = 'provo'\n",
    "\n",
    "    #######################################################\n",
    "    # Find out the distinct HUC8 codes for this watershed #\n",
    "    #######################################################\n",
    "    # set the filepath for the watershed-specific directory\n",
    "    directory = f'./8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/AORC/{tmp_str}'\n",
    "    # set a regular expression pattern we can use to pull files for each HUC8\n",
    "    pattern = r'(?<=_cat-)\\d{8}(?=.csv)'\n",
    "    # determine the HUC8 codes for the current watershed\n",
    "    huc_lst = set()\n",
    "    for filename in os.listdir(directory):\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            huc_lst.add(match.group(0))\n",
    "    huc_lst = [i for i in huc_lst]\n",
    "\n",
    "    ########################################################\n",
    "    # Loop over each .csv file and did the following:      #\n",
    "    # 1. Convert to unit from mm/s to mm                   #\n",
    "    # 2. Calculate daily and monthly sum in each sub-basin #\n",
    "    # 3. Average the values of the sub-basins              #\n",
    "    # 4. Store the results in one dictionary               #\n",
    "    ########################################################\n",
    "    daily_precip = {}\n",
    "    monthly_precip ={}\n",
    "    final_monthly_precip = pd.DataFrame()\n",
    "\n",
    "    # for each HUC8\n",
    "    for huc in huc_lst:\n",
    "\n",
    "        # get a list of file paths that match the HUC8 code\n",
    "        file_paths = [os.path.join(directory, filename) for filename in os.listdir(directory) if filename.endswith(huc+'.csv')]\n",
    "\n",
    "        # read the CSV files into a list of DataFrames\n",
    "        dataframes = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "        # convert mm/s to mm\n",
    "        for df in dataframes:\n",
    "            df['P_mm'] = df['RAINRATE'] * 3600\n",
    "\n",
    "        # concat the files\n",
    "        combined_dataframe = pd.concat(dataframes, axis=0)\n",
    "\n",
    "        # convert the 'time' column to a datetime object\n",
    "        combined_dataframe['time'] = pd.to_datetime(combined_dataframe['time'])\n",
    "\n",
    "        # remove duplicate time entries\n",
    "        combined_dataframe.drop_duplicates(subset=['time'], keep='first', inplace=True)\n",
    "\n",
    "        # set the 'time' column as the index\n",
    "        combined_dataframe.set_index('time', inplace=True)\n",
    "\n",
    "        # resample to calculate daily, monthly sums\n",
    "        daily_sum_df = combined_dataframe['P_mm'].resample('D').sum()\n",
    "        monthly_sum_df = combined_dataframe['P_mm'].resample('M').sum()\n",
    "\n",
    "        # create a new DataFrame\n",
    "        daily_precip_df = pd.DataFrame({\n",
    "            'Date': daily_sum_df.index.strftime('%Y-%m-%d'),\n",
    "            f'Daily_{huc}': daily_sum_df.values\n",
    "        })\n",
    "\n",
    "        monthly_precip_df = pd.DataFrame({\n",
    "            'Month': monthly_sum_df.index.strftime('%Y-%m'),\n",
    "            f'Monthly_Precip_{huc}': monthly_sum_df.values\n",
    "        })\n",
    "\n",
    "        # store the DataFrame in the dictionary\n",
    "        daily_precip[huc] = daily_precip_df\n",
    "        monthly_precip[huc] = monthly_precip_df\n",
    "\n",
    "    #################################################################\n",
    "    # 1. Concate the results in the dictionary into one dataframe   #\n",
    "    # 2. Calculate water year                                       #\n",
    "    # 3. Save the final output as a csv                             #\n",
    "    #################################################################\n",
    "\n",
    "    # convert the dictionary to a list of DataFrames\n",
    "    daily_precip = list(daily_precip.values())\n",
    "    monthly_precip = list(monthly_precip.values())\n",
    "\n",
    "    # concatenate\n",
    "    final_daily_precip = pd.concat(daily_precip, axis=1)\n",
    "    final_daily_precip = final_daily_precip.loc[:, ~final_daily_precip.columns.duplicated()]\n",
    "\n",
    "    final_monthly_precip = pd.concat(monthly_precip, axis=1)\n",
    "    final_monthly_precip = final_monthly_precip.loc[:, ~final_monthly_precip.columns.duplicated()]\n",
    "\n",
    "    # average across subbasins within the watershed (not averaging across time)\n",
    "    final_daily_precip['avg'] = final_daily_precip.iloc[:, [1, -1]].mean(axis=1)\n",
    "    final_monthly_precip['avg'] = final_monthly_precip.iloc[:, [1, -1]].mean(axis=1)\n",
    "\n",
    "    # calculate Water Year\n",
    "    final_daily_precip['Water_Year'] = pd.to_datetime(final_daily_precip['Date']).map(lambda x: x.year+1 if x.month>9 else x.year)\n",
    "    final_monthly_precip['Water_Year'] = pd.to_datetime(final_monthly_precip['Month']).map(lambda x: x.year+1 if x.month>9 else x.year)\n",
    "\n",
    "    # save the merged DataFrame as a CSV file\n",
    "    output_directory = './AORC_outputs'\n",
    "    filename_daily = f'{tmp_str}_daily_precip.csv'\n",
    "    filename_monthly = f'{tmp_str}_monthly_precip.csv'\n",
    "\n",
    "    final_daily_precip.to_csv(os.path.join(output_directory, filename_daily), index=False)\n",
    "    final_monthly_precip.to_csv(os.path.join(output_directory, filename_monthly), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e06ab-0257-4206-aa5e-3e6c8ef6b6f5",
   "metadata": {
    "id": "ae4e06ab-0257-4206-aa5e-3e6c8ef6b6f5"
   },
   "source": [
    "Now that we have the monthly values for each watershed for each year within the <font color=red> ./AORC_outputs </font> folder, we'll calculate the **5-year average monthly** precipitation. The output of the following code block will be three DataFrames, called `AORC_provo_monthly_precip`, `AORC_weber_monthly_precip`, and `AORC_bear_monthly_precip`. Even though we create them during a `for` loop, we can access them later because we store them as *global* variables instead of *local* variables (only relevant within the `for` loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0af0ed-bd68-4cc9-ab5d-cbc4ac8b9ba8",
   "metadata": {
    "id": "4a0af0ed-bd68-4cc9-ab5d-cbc4ac8b9ba8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a list of all monthly precipitation CSV files in the current directory\n",
    "directory = './AORC_outputs'\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('monthly_precip.csv')]\n",
    "\n",
    "# loop through each CSV file (i.e., each watershed)\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(f'{directory}/'+csv_file)\n",
    "\n",
    "    # extract the \"Month\" and \"avg\" columns\n",
    "    month_col = df['Month']\n",
    "    avg_col = df['avg']\n",
    "\n",
    "    # initialize a dictionary to store monthly averages within each CSV file\n",
    "    monthly_averages = {str(month).zfill(2): [] for month in range(1, 13)}\n",
    "\n",
    "    # update the dictionary with the average values for each month within this CSV file\n",
    "    for month, avg in zip(month_col, avg_col):\n",
    "        month_key = month.split('-')[1]  # get the month part (e.g., '01' from '2015-01')\n",
    "        monthly_averages[month_key].append(avg)\n",
    "\n",
    "    # calculate the average of each month over the 5-year range for this CSV file\n",
    "    result_data = {'Month': [], 'Monthly_Avg': []}\n",
    "    for month, avg_list in monthly_averages.items():\n",
    "        avg = sum(avg_list) / len(avg_list)\n",
    "        result_data['Month'].append(month)\n",
    "        result_data['Monthly_Avg'].append(avg)\n",
    "\n",
    "    # create the final DataFrame for this CSV file\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "\n",
    "    # get the base name of the CSV file (without file extension)\n",
    "    csv_base_name = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # create a variable name and store the result DataFrame in it as a global variable\n",
    "    variable_name = f\"AORC_{csv_base_name}\"\n",
    "    globals()[variable_name] = result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69696642-9a92-45b5-b0a2-08b5019f0f76",
   "metadata": {
    "id": "69696642-9a92-45b5-b0a2-08b5019f0f76",
    "tags": []
   },
   "source": [
    "### 4.1.6. Plot PRISM vs. AORC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d993b-d143-4cac-b326-743cf08d5ef2",
   "metadata": {},
   "source": [
    "Next, we read back in the files with the PRISM 30-year normals we calculated for each watershed above. The resulting PRISM variables are called `PRISM_Monthly_Normal_Precipitation_W` for the Weber River, `PRISM_Monthly_Normal_Precipitation_J` for the Jordan-Provo River, and `PRISM_Monthly_Normal_Precipitation_B` for the Bear River.\n",
    "\n",
    "Then, we merge the PRISM and AORC precipitation data into one DataFrame to facilitate plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7694ca-f368-4d8f-9196-672888b4fd18",
   "metadata": {
    "id": "5b7694ca-f368-4d8f-9196-672888b4fd18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the directory containing the PRISM CSV files\n",
    "directory = \"./PRISM_monthly_normals/outputs\"\n",
    "\n",
    "# get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "\n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "\n",
    "    # find the name of the CSV file to determine which watershed it is\n",
    "    csv_base_name = csv_file[0:36]\n",
    "\n",
    "    # convert month column to a common format (e.g., '01', '02')\n",
    "    if 'Month' in df.columns:\n",
    "        df['Month'] = df['Month'].apply(lambda x: str(x).zfill(2))\n",
    "\n",
    "    # create a variable name and store the DataFrame in it\n",
    "    variable_name = f\"{csv_base_name}\"\n",
    "    globals()[variable_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb63a68-4381-4a57-9d90-626fccd55031",
   "metadata": {
    "id": "cfb63a68-4381-4a57-9d90-626fccd55031",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge the DataFrames based on the \"Month\" column\n",
    "merged_provo = pd.merge(AORC_provo_monthly_precip, PRISM_Monthly_Normal_Precipitation_J, on=\"Month\")\n",
    "merged_bear = pd.merge(AORC_bear_monthly_precip, PRISM_Monthly_Normal_Precipitation_B, on=\"Month\")\n",
    "merged_weber = pd.merge(AORC_weber_monthly_precip, PRISM_Monthly_Normal_Precipitation_W, on=\"Month\")\n",
    "\n",
    "# merging creates an additional column, so we drop it\n",
    "merged_provo = merged_provo.drop(columns=['Unnamed: 0'])\n",
    "merged_bear = merged_bear.drop(columns=['Unnamed: 0'])\n",
    "merged_weber = merged_weber.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a6520-48a0-4ef4-aaaa-8bd08c25e56f",
   "metadata": {
    "id": "8c7a6520-48a0-4ef4-aaaa-8bd08c25e56f"
   },
   "source": [
    "Now that we have all the data for PRISM and AORC in one DataFrame for each watershed, we can plot the 30-year normal from PRISM versus the 5-year average from AORC. Remember that we have all the basin names stored in the variable `watershed_lst`, so we loop through the watersheds in the same order as that variable and title the suplots according to the watershed name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56506132-9f78-4c48-8ab5-608dd28de27b",
   "metadata": {
    "id": "56506132-9f78-4c48-8ab5-608dd28de27b",
    "outputId": "1ae1889e-7798-41e0-b609-cce0a71f6af4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "watershed_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae66fa-9c6b-41f7-a706-725a5428324d",
   "metadata": {
    "id": "64ae66fa-9c6b-41f7-a706-725a5428324d",
    "outputId": "ebcc730a-53e1-4b7a-8f5a-317eae88335e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a figure and three subplots in one row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "# set a counter for subsetting the subplots as we loop through the watersheds\n",
    "count = 0\n",
    "\n",
    "# loop through the DataFrames for each watershed and plot them on a separate subplot\n",
    "for merged in [merged_weber, merged_bear, merged_provo]:\n",
    "    axs[count].plot(merged['Month'], merged['Monthly_Avg'], label='AORC')\n",
    "    axs[count].plot(merged['Month'], merged['Precipitation (mm)'], label='PRISM')\n",
    "    axs[count].set_title(f'{watershed_lst[count]}', fontsize=18)\n",
    "    axs[count].set_xlabel('Month, starting from Jan', fontsize=14)\n",
    "    axs[count].set_ylabel('Precipitation (mm)', fontsize=14)\n",
    "    axs[count].set_ylim(0,100)\n",
    "    axs[count].legend(fontsize=14)\n",
    "    axs[count].grid(alpha=0.5)\n",
    "\n",
    "    # increment the counter for subsetting\n",
    "    count += 1\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454e2ce-d377-44ef-b752-1853b2e5dd58",
   "metadata": {},
   "source": [
    "As we can see above, the 5-year AORC data does not closely match the 30-year long-term average from PRISM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4be0da-bdff-444b-bc4d-cd890f0a89c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Delving Deeper: The two timeseries look so different, we double-checked our workflow above to ensure we were not aggregating the data erroneously, but we were not able to find any errors (we welcome feedback if you find one!). We also checked the PRISM online web interface to evaluate anomolies in the months/years for which we have AORC data, and the monthly values in PRISM appear to be similar to the AORC values. As we discussed above, it would be a better comparison to use PRISM hourly data and then calculate the 5-year average to compare to the PRISM 30-year average, but downloading PRISM hourly data would take several hours per watershed. We leave this for future work, but if we do complete that analysis, we can compare the PRISM 5-year data to the AORC 5-year data to more directly assess AORC data quality as well. </b> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06a84e-77c4-44ee-b807-ae8c95403878",
   "metadata": {
    "id": "fc06a84e-77c4-44ee-b807-ae8c95403878"
   },
   "source": [
    "Regardless, we see much more precipitation variability across the year in the short term than in the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fefde-68c1-43ae-bf4a-30fe726bfb3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color='blue'> **Analysis 4.2.** </font> Compare National Water Model simulated streamflow to observations from USGS\n",
    "\n",
    "<div style=\"background-color: whitesmoke; padding: 10px; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2); color: #333;\">\n",
    "This analysis compares the modeled streamflow to observed streamflow from the USGS gages. The point of this exercise is to determine how accurate the NWM is.  \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5035d-6805-478b-b5a8-82019a1ecf27",
   "metadata": {},
   "source": [
    "The second analysis includes two main steps:  <b/>\n",
    "\n",
    "**Step A.** Prepare the simulated streamflow data from <font color='blue'>NWM</font> for each watershed\n",
    "> 4.2.1. Import NWM streamflow data \\\n",
    "> 4.2.2. Retrieve the NWM streamflow data for the outlet locations\n",
    "\n",
    "\n",
    "**Step B.** Prepare the observed streamflow data from <font color='blue'>USGS</font> for each watershed\n",
    "> 4.2.3. Import USGS stream gage data \\\n",
    "> 4.2.4. Create NWM vs. USGS stream gage comparison plots \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f87f0a-1fea-459c-90b4-1fa50cdac3d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2.1. Import NWM streamflow data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e49a6-7346-42bb-9ad6-36b236bae93e",
   "metadata": {
    "id": "088e49a6-7346-42bb-9ad6-36b236bae93e",
    "tags": []
   },
   "source": [
    "The NWM generates estimates of streamflow, along with lots of other hydrological variables, based on meteorological and geophysiological input datasets and models like groundwater flow, lake and reservoir models, land surface processes, etc. You can find an overview from NOAA's Office of Water Prediction here: https://water.noaa.gov/about/nwm. \n",
    "\n",
    "The output we're interested is called **\"channel routing\"**, since rivers and streams are called channels in the model. This is the same as modeled streamflow or modeled discharge. The modeled discharge is available at the same locations as USGS reference stream gage locations, which collect observational data about streamflow across the continental U.S. NWM also predicts discharge at locations without a reference USGS gage, making it useful for water management in areas that don't have local observational data. Abdelkader and Bravo Mendez (2023) provided a table that shows relationship between NWM feature IDs with the associated USGS gage IDs. You can find metadata about the various model outputs here: https://water.noaa.gov/about/output_file_contents.\n",
    "\n",
    "The NWM data are hosted on an Amazon Web Services (AWS) storage infrastructure called the Simple Storage Service (S3) in a file format called **Zarr**. Zarr is a compressed file format that stores N-dimensional ways in chunks, where each chunk is compressed as a programmatic object. These objects are optimized for storage and retrieval, making it a more efficient way to store large multi-dimensional arrays like the NWM outputs. If you want to learn more about Zarr, check out the explanation and tutorial at https://zarr.readthedocs.io/en/stable/getting_started.html.\n",
    "\n",
    "We're going to access the data from AWS S3 and use the Python package **`xarray`** which we imported earlier in one of our first code chunks, to import and reformat these Zarr files. This is the link to the data on AWS in case you want to look at documentation: https://registry.opendata.aws/nwm-archive/.\n",
    "\n",
    "Reference:\n",
    "- https://github.com/NCAR/rechunk_retro_nwm_v21/blob/main/notebooks/usage_example_streamflow_timeseries.ipynb\n",
    "- Abdelkader, M., J. H. Bravo Mendez (2023). NWM version 2.1 model output data retrieval, HydroShare, https://doi.org/10.4211/hs.c4c9f0950c7a42d298ca25e4f6ba5542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bdb48-00c2-4d99-b780-cedc5b7a0f4b",
   "metadata": {
    "id": "937bdb48-00c2-4d99-b780-cedc5b7a0f4b"
   },
   "source": [
    "First, we'll have our system print all of the files that are available for us to download from AWS S3, without actually downloading anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7e27b-593e-47c8-9cff-ed7e5bb87392",
   "metadata": {
    "id": "dcf7e27b-593e-47c8-9cff-ed7e5bb87392",
    "outputId": "c7764def-c3b2-4126-f9c9-049b6521cc0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use fsspec to get a list of available files within the amazon AWS 3\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "fs.glob('/noaa-nwm-retrospective-2-1-zarr-pds/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ef6a6-b906-4b0b-8fed-e84211fafb7c",
   "metadata": {
    "id": "7c4ef6a6-b906-4b0b-8fed-e84211fafb7c"
   },
   "source": [
    "We can see that there are lots of different NWM files. As we discussed above, we only want channel routing. This is stored in the file ending in \".../**chrtout.zarr**\". Let's download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4aa8a-c41f-4399-ba05-6f5755db177f",
   "metadata": {
    "id": "acf4aa8a-c41f-4399-ba05-6f5755db177f",
    "outputId": "bc5f667f-64de-4a9d-b853-0a95decb3988",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# use xarray to access the chrtout zarr file\n",
    "file = fs.glob('noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr')\n",
    "ds = xr.open_dataset(fs.get_mapper(file[0]), engine='zarr', backend_kwargs={'consolidated': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bfafa-561d-4878-ad4c-d7a42999b9aa",
   "metadata": {
    "id": "971bfafa-561d-4878-ad4c-d7a42999b9aa"
   },
   "source": [
    "If you're curious about that `%%time` command, it's a \"magic command\" in Jupyter notebooks that prints the time taken to execute a cell block. It reports the time taken on the actual computations (CPU times), time taken for system overhead (sys), and total execution time (total), as well as the \"wall clock\" time, i.e., how long you and I would perceive it running for.\n",
    "\n",
    "Now we can start working with the data. The next step is important - when you import a Zarr file/any type of data that is chunked for storage, you need to tell the system how to \"**re-chunk**\" the data. The code block below tells Python to create chunks of 1,000 gages each based on the timestamp, and print how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ea039-8c5e-440a-a847-dfa485ee7b31",
   "metadata": {
    "id": "163ea039-8c5e-440a-a847-dfa485ee7b31",
    "outputId": "c5759f30-6313-48fa-e677-b42cddae43e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create chunks to optimize the memory usage\n",
    "dim_chunk_sizes = {'feature_id': 1000, 'time': len(ds.time)} # treat time as one dimension, but chunk by 1000 gauges (trial and error but there is documentation to determine optimal)\n",
    "ds = ds.chunk(chunks=dim_chunk_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696e4e-cbf3-4120-ace6-9a24bcc08446",
   "metadata": {
    "id": "1e696e4e-cbf3-4120-ace6-9a24bcc08446"
   },
   "source": [
    "We'll also select only the timeframe we want for this analysis, i.e. from the start of water year 2016 (October 1, 2015) to the end of water year 2019 (September 30, 2019, but we will use October 1, 2019 to make sure the last day of water year 2019 is included in our subsetted data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516c8d5-d52a-4041-b4df-cf4c2666c5c2",
   "metadata": {
    "id": "b516c8d5-d52a-4041-b4df-cf4c2666c5c2",
    "outputId": "46c251f2-df1f-4a62-dac3-477cb7ae5ccb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# slice the dataset for a period of interest\n",
    "start_time = f'2015-10-01 00:00'\n",
    "end_time = f'2019-10-01 00:00'\n",
    "\n",
    "# isolate the desired time period of our data\n",
    "ds_subset = ds.sortby('time').sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f'The dataset contains {len(ds_subset.time)} timesteps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196be19-6e50-41b6-93c8-28927c3c5aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ds_subset.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5e914-0a43-4cc7-8766-85a3d2e6649d",
   "metadata": {
    "id": "dfd5e914-0a43-4cc7-8766-85a3d2e6649d",
    "tags": []
   },
   "source": [
    "### 4.2.2. Retrieve the NWM streamflow data for the outlet locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0928c-c2f6-420c-8f95-1aa6f6dab5ae",
   "metadata": {},
   "source": [
    "From Step 3.1., we can see 35065 timesteps are selected. That's a lot of timesteps! But we haven't fully downloaded all the data; `xarray` uses something called \"**lazy reading**\" so it can download the metadata but won't save the actual data to memory until we tell it to. Before we do that, we need to subset by our region of interest. To do that, we need to know the USGS reference stream gage numbers associated with our 3 rivers, and map those to the river names. We use the list of names for saving the files later on. Once we have a list of the USGS gages, we can find the corresponding NWM IDs for those gages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ac702-7a5d-4795-a09d-1b5ba1981917",
   "metadata": {
    "id": "846ac702-7a5d-4795-a09d-1b5ba1981917",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IDs for the outlets of the Bear River, Weber River, and Jordan-Provo River\n",
    "USGS_gages = ['10126000', '10141000', '10168000']\n",
    "\n",
    "# this list is used for file saving purposes only\n",
    "USGS_gage_names = ['bear', 'weber', 'jordan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b42fc1-0fc6-43ea-87f7-ab3c28dcd5ad",
   "metadata": {
    "id": "23b42fc1-0fc6-43ea-87f7-ab3c28dcd5ad"
   },
   "source": [
    "In the next step, we download a Microsoft Excel file from the work of Abdelkader, M., J. H. Bravo Mendez (2023), where they have already mapped the USGS gage IDs to the NWM IDs and made it publicly available on [HydroShare](https://doi.org/10.4211/hs.c4c9f0950c7a42d298ca25e4f6ba5542). You'll need your Hydroshare login and password again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59b949-af65-4354-9d38-08b6ca7e1b57",
   "metadata": {
    "id": "ff59b949-af65-4354-9d38-08b6ca7e1b57",
    "outputId": "99154948-2dc2-4d44-8ea9-6245caf76571",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hs = HydroShare()\n",
    "hs.sign_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5932f-df22-4491-b021-3941038ff72f",
   "metadata": {
    "id": "01a5932f-df22-4491-b021-3941038ff72f",
    "outputId": "4e9a43a2-cf27-46dd-b68f-0b15e066e841",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the resource you want to download using its identifier\n",
    "res = hs.resource('c4c9f0950c7a42d298ca25e4f6ba5542')\n",
    "res.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d303794-b63f-4fd6-9a8e-445c6b32a026",
   "metadata": {
    "id": "8d303794-b63f-4fd6-9a8e-445c6b32a026"
   },
   "source": [
    "The zip file is stored in your working directory. We'll use bash to unzip it (with the `-q` flag to avoid printing all the files it is inflating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9c3a3-d143-4951-94d6-13ff8fcc4aaf",
   "metadata": {
    "id": "6ed9c3a3-d143-4951-94d6-13ff8fcc4aaf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -q c4c9f0950c7a42d298ca25e4f6ba5542.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc8fb7-03af-443b-8024-4dafff4e443e",
   "metadata": {
    "id": "88bc8fb7-03af-443b-8024-4dafff4e443e"
   },
   "source": [
    "Now we'll extract the USGS gage IDs and **corresponding NWM IDs** from the file NWM_USGS_Natural_Flow.xlsx. We use the Python command `read_excel` instead of `read_csv` like we have used before because Excel has some proprietary formatting and needs to be read in differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03a347-6a63-406a-9a6a-2b67b2c08ac0",
   "metadata": {
    "id": "9c03a347-6a63-406a-9a6a-2b67b2c08ac0",
    "outputId": "be4fb96c-cb8a-4cd4-9468-ad826e6ad0c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify the file\n",
    "file = 'c4c9f0950c7a42d298ca25e4f6ba5542/data/contents/NWM_USGS_Natural_Flow.xlsx'\n",
    "usgs_stations = pd.read_excel(os.path.join(os.getcwd(), file))\n",
    "# do the mapping from USGS to NWM\n",
    "usgs_stations['USGS_ID'] = usgs_stations['USGS_ID'].apply(lambda x: '{:0>8}'.format(x))\n",
    "# read the assiciated NWM id\n",
    "NWM_feature_ids = list(usgs_stations[usgs_stations['USGS_ID'].isin(USGS_gages)]['NWM_ID'])\n",
    "NWM_feature_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0accd3-bb28-470a-a8c9-ce15e24f11e0",
   "metadata": {
    "id": "dd0accd3-bb28-470a-a8c9-ce15e24f11e0"
   },
   "source": [
    "Lastly, let's **clip our data** so we only have NWM discharge for our rivers of interest. We'll create an output folder and store the files there.\n",
    "\n",
    "The `try` loop below tells the system how to distribute resources to actually read in the NWM data, instead of the \"lazy reading\" we mentioned above. We set the number of \"workers\" i.e., computational resources, we need and how much memory each of them should have. This is an important optimization step for reading in complicated multi-dimensional data like the NWM arrays; you can customize the resources you are requesting to suit your use case and available resources.\n",
    "\n",
    "We loop through each gage in the last `for` loop and save files based on the river name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2d7aa-173d-43d8-9ba9-fa9fc64601ce",
   "metadata": {
    "id": "3cc2d7aa-173d-43d8-9ba9-fa9fc64601ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an output directory to save results\n",
    "!mkdir nwm_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757f33f-cb92-4aa2-a289-a3ce70ba9573",
   "metadata": {},
   "source": [
    "To enhance computation speed and leverage the parallel processing resources provided by the I-GUIDE platform, we utilize the **`Client`** function from the **Dask Distributed** Python package to establish a local cluster. In this configuration, setting `n_workers=-1` signifies the utilization of all available processing units, each allocated with a memory capacity of 2 GB. You can explore further details about Dask at https://distributed.dask.org/en/latest/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6c0e7-12f5-4bab-b422-4b0fd2319fbc",
   "metadata": {
    "id": "16d6c0e7-12f5-4bab-b422-4b0fd2319fbc",
    "outputId": "ccdf7252-6118-4ae6-99ef-52929c51e03f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(\"\")\n",
    "except:\n",
    "    # the client should be customized to your workstation resources.\n",
    "    client = Client(n_workers=-1, memory_limit='2GB') # per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e3ce-af60-40a7-9bd4-5e89fd963a24",
   "metadata": {
    "id": "04b5e3ce-af60-40a7-9bd4-5e89fd963a24",
    "outputId": "f5289563-7ac7-4b45-8417-5bbf9e456ae6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0,3):\n",
    "\n",
    "    print('Retrieving data for gage: ', USGS_gages[i])\n",
    "\n",
    "    ds_subset_one = ds_subset.sel(feature_id=NWM_feature_ids[i])\n",
    "    ds_subset_one = ds_subset_one.compute()\n",
    "\n",
    "    df = ds_subset_one['streamflow'].to_pandas().to_frame()\n",
    "\n",
    "    df.rename(columns={0: f\"Discharge {ds.streamflow.units}\"}, inplace=True)\n",
    "    df_resampled = df.resample('D').mean()\n",
    "    df_resampled.to_csv(os.path.join('./nwm_outputs/', f'nwm_q_{USGS_gage_names[i]}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58735189-e71f-4b48-a1bc-4e0bc45c528c",
   "metadata": {
    "id": "58735189-e71f-4b48-a1bc-4e0bc45c528c"
   },
   "source": [
    "You should have three CSVs in <font color='red'> ./nwm_output </font> folders, called `nwm_q_bear.csv`, `nwm_q_jordan.csv`, and `nwm_q_weber.csv`. We use \"q\" in the name because in hydrology, discharge/runoff is usually denoted as \"Q\". As a note, since we're using daily values, the time zone is not as important. However, if you wanted to look at hourly data for different gages, you might need to standardize your time zone. USGS local time zones differ from the \"universal\" UTM time zone used by the NWM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343fb0-c991-4aa0-959c-f05839676e45",
   "metadata": {
    "id": "99343fb0-c991-4aa0-959c-f05839676e45",
    "tags": []
   },
   "source": [
    "### 4.2.3. Import USGS stream gage data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f6574-fd02-4452-a109-11f6f23bc80c",
   "metadata": {},
   "source": [
    "The USGS reports streamflow across several timescales (e.g., hourly, daily). We retrieve the **daily average** values for water years 2016-2019 using the `nwis` function from the **`dataretrieval`** package following Horsburgh et al. (2022). To do this you need the parameter code for your feature of interest. Here, the code is \"0060\" for discharge.\n",
    "\n",
    "The data are given in cubic feet per second (cfs), but the NWM data we want to compare to reports data in cubic *meters* per second (cms). We use a conversion parameter **`cfs_2_cms`** to convert to the same unit (cms) for comparison.\n",
    "\n",
    "Reference:\n",
    "- Horsburgh, J. S., A. S. Jones, S. S. Black, T. O. Hodson (2022). USGS dataretrieval Python Package Usage Examples, HydroShare, http://www.hydroshare.org/resource/c97c32ecf59b4dff90ef013030c54264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd936c-9d5f-44c3-80a4-ddd70a1e2d45",
   "metadata": {
    "id": "51bd936c-9d5f-44c3-80a4-ddd70a1e2d45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a folder to store the retrieved data\n",
    "!mkdir usgs_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ef52d-e05e-4509-b9d5-944bdb604dc6",
   "metadata": {
    "id": "0a1ef52d-e05e-4509-b9d5-944bdb604dc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bear River, Weber River, and Jordan-Provo River\n",
    "USGS_gages = ['10126000', '10141000', '10168000']\n",
    "\n",
    "# unit conversion parameter\n",
    "cfs_2_cms = 0.028316846592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4aef51-5dbb-4bd6-a180-367cb3567c40",
   "metadata": {
    "id": "aa4aef51-5dbb-4bd6-a180-367cb3567c40",
    "outputId": "22b65cf1-7299-4f16-df93-37672884437f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the parameters needed to retrieve data\n",
    "parameterCode = \"00060\"  # Discharge\n",
    "startDate = \"2015-10-01\"\n",
    "endDate = \"2019-10-01\"\n",
    "\n",
    "# Retrieve the data\n",
    "dailyStreamflow = nwis.get_dv(sites=USGS_gages, parameterCd=parameterCode, start=startDate, end=endDate) # multiple sites\n",
    "dailyStreamflow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3389895-a5c2-4090-a6f5-5a4ee1ccb921",
   "metadata": {
    "id": "f3389895-a5c2-4090-a6f5-5a4ee1ccb921"
   },
   "source": [
    "The DataFrame above shows the daily average value in cfs for all three gages. We save them to different dataframes and convert to cms below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de46578-2fe0-42b0-877b-ab140b7cb6d6",
   "metadata": {
    "id": "2de46578-2fe0-42b0-877b-ab140b7cb6d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dailyStreamflow_bear = dailyStreamflow[0].loc[USGS_gages[0]]\n",
    "dailyStreamflow_weber = dailyStreamflow[0].loc[USGS_gages[1]]\n",
    "dailyStreamflow_jordan = dailyStreamflow[0].loc[USGS_gages[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2cb17-d0d8-4816-a695-b091b2835758",
   "metadata": {
    "id": "02e2cb17-d0d8-4816-a695-b091b2835758",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dailyStreamflow_bear['00060_Mean cms']=dailyStreamflow_bear['00060_Mean']*cfs_2_cms\n",
    "dailyStreamflow_weber['00060_Mean cms']=dailyStreamflow_weber['00060_Mean']*cfs_2_cms\n",
    "dailyStreamflow_jordan['00060_Mean cms']=dailyStreamflow_jordan['00060_Mean']*cfs_2_cms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97f065-d7c2-46e2-a6c1-fde9351bc5eb",
   "metadata": {
    "id": "7e97f065-d7c2-46e2-a6c1-fde9351bc5eb"
   },
   "source": [
    "Let's plot the observed streamflow across all 3 rivers from water year 2016 to water year 2019, then export our data to CSV to capture this data retrieval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e686b2-4eae-47bf-b4ce-0c615c0ce669",
   "metadata": {
    "id": "c3e686b2-4eae-47bf-b4ce-0c615c0ce669",
    "outputId": "a818bb54-be34-4dee-d1de-87fa739326f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1)\n",
    "plt.subplots_adjust(top=0.95, bottom=0.05, hspace=0.4)\n",
    "\n",
    "dailyStreamflow_bear['00060_Mean cms'].plot(ax=axes[0], color='slateblue')\n",
    "dailyStreamflow_weber['00060_Mean cms'].plot(ax=axes[1], color='gray')\n",
    "dailyStreamflow_jordan['00060_Mean cms'].plot(ax=axes[2], color='sandybrown')\n",
    "\n",
    "axes[0].legend(['Bear River'], fontsize=12, frameon=False, loc='upper left')\n",
    "axes[1].legend(['Weber River'], fontsize=12, frameon=False, loc='upper left')\n",
    "axes[2].legend(['Jordan River'], fontsize=12, frameon=False, loc='upper left')\n",
    "\n",
    "for i in range(0,3):\n",
    "    if i == 2:\n",
    "        axes[i].set_xlabel('Date')\n",
    "    else:\n",
    "        axes[i].set_xlabel('')\n",
    "        \n",
    "axes[1].set_ylabel('Streamflow (cms)', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbf457-53c2-4ab9-8c0f-c8ecb42c24ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Delving Deeper: Take note of the significant variation in streamflow value ranges among the three watersheds. The Bear River dataset exhibits the widest range, reaching a maximum value exceeding 200 cms, whereas the Jordan River dataset presents the narrowest range, with its maximum value hovering around 15 cms.  </b> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4969d-7db9-4c61-a3f2-3cdffb0bf0d1",
   "metadata": {
    "id": "bfe4969d-7db9-4c61-a3f2-3cdffb0bf0d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save dataframes\n",
    "dailyStreamflow_bear.to_csv('./usgs_outputs/obs_q_bear.csv')\n",
    "dailyStreamflow_weber.to_csv('./usgs_outputs/obs_q_weber.csv')\n",
    "dailyStreamflow_jordan.to_csv('./usgs_outputs/obs_q_jordan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521a28a-8cda-41bd-ac55-73e24124dc3d",
   "metadata": {
    "id": "7521a28a-8cda-41bd-ac55-73e24124dc3d",
    "tags": []
   },
   "source": [
    "### 4.2.4. Create NWM vs. USGS stream gage comparison plots \n",
    "\n",
    "In this section we'll read our USGS and NWM data back in from their CSV files and compare them. Our USGS variables are actually still saved in memory so we could avoid importing them, but for consistent notation and simplicity in the `for` loop, we import both CSVs for each watershed at the same time. Input-output (I/O) tends to be the most computationally itnensive task, so in future work we could alter the code below to only import the NWM CSVs, or change the NWM import code to save those as global variables too so we don't have to read either in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db244791-f518-4153-9e60-5e0e2b413585",
   "metadata": {},
   "source": [
    "The comparison plots inlcude:\n",
    "> - Plot #1: A sub-panel timeseries plot comparing the observed and modeled streamflow rates in the Bear, Jordon-Provo, and Weber River basins. One sub-panel is created for each river basin.\n",
    "> - Plot #2: Scatter plot showing comparison between USGS observation and NWM modeled values along a 1:1 line, with root-mean-square-error (RMSE) computed for each watershed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb23a6-de58-4092-ab0e-a121fa88a0c6",
   "metadata": {
    "id": "35cb23a6-de58-4092-ab0e-a121fa88a0c6",
    "outputId": "ba9aa332-53ea-412a-979c-23c42e19d874",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define paths to data source\n",
    "nwm_folder = \"./nwm_outputs/\"\n",
    "usgs_folder = \"./usgs_outputs/\"\n",
    "\n",
    "# use glob to get set of files\n",
    "glob_nwm_gridpoints = sorted(glob.glob(f'{nwm_folder}/*.csv'))\n",
    "glob_usgs_discharge = sorted(glob.glob(f'{usgs_folder}/*.csv'))\n",
    "\n",
    "# print set of files as a sanity check\n",
    "display(glob_nwm_gridpoints)\n",
    "display(glob_usgs_discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8c7c3-5b43-4a05-a483-9036259e54c9",
   "metadata": {
    "id": "0ee8c7c3-5b43-4a05-a483-9036259e54c9",
    "outputId": "191ed750-67a5-4c30-f237-71df53d678ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to round axis values to nearest 5\n",
    "def myround(x, base=5):\n",
    "    return base * round(x/base)\n",
    "\n",
    "# define figure and axis for plotting\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(15,10))\n",
    "\n",
    "# define font dictionaries for plotting\n",
    "fontdict_legend_labels   = {'size': 12, 'weight': 'bold'}\n",
    "fontdict_tick_labels     = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "fontdict_axis_labels     = {'fontsize': 14, 'fontweight': 'bold'}\n",
    "fontdict_title_labels    = {'fontsize': 14, 'fontweight': 'bold'}\n",
    "fontdict_text_color_bar  = {'fontsize': 15, 'fontweight': 'normal'}\n",
    "fontdict_text_annotation = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "\n",
    "# define xtick and xticklabels for subplots\n",
    "# these will be datetime objects that represent each year we are plotting\n",
    "ax_xticks      = pd.to_datetime(['2015-10-01', '2016-04-01', '2016-10-01', '2017-04-01', '2017-10-01', '2018-04-01', '2018-10-01', '2019-04-01', '2019-10-01'])\n",
    "ax_xticklabels = ['2015-10-01', '2016-04-01', '2016-10-01', '2017-04-01', '2017-10-01', '2018-04-01', '2018-10-01', '2019-04-01', '2019-10-01']\n",
    "\n",
    "# for each subplot (watershed), do the following\n",
    "for ax_index, ax in enumerate(axs):\n",
    "\n",
    "    # read in USGS observation file, and NWM gridpoint file, into pandas DataFrame\n",
    "    df_usgs_obs      = pd.read_csv(glob_usgs_discharge[ax_index])\n",
    "    df_nwm_gridpoint = pd.read_csv(glob_nwm_gridpoints[ax_index])\n",
    "\n",
    "    # y-axis customizations\n",
    "    ax_ymin = 0\n",
    "    # ax_ymax = 300\n",
    "    ax_ymax = max(df_nwm_gridpoint['Discharge m3 s-1'])\n",
    "    #ax_ystep = 50\n",
    "    ax_ystep = myround(ax_ymax/5)\n",
    "    ax_yticks = np.arange(ax_ymin, ax_ymax+ax_ystep, ax_ystep)\n",
    "    ax_yticklabels = ax_yticks\n",
    "    ax.set_yticks(ax_yticks)\n",
    "    ax.set_yticklabels(ax_yticklabels, fontdict=fontdict_tick_labels)\n",
    "    ax.set_ylabel('Stream Flow (m$^3$/s)', fontsize=13)\n",
    "\n",
    "    # plot a horizontal line that spands the entire y-axis for each of the x_ticks we have\n",
    "    for x_tick in ax_xticks:\n",
    "        ax.axvline(x=x_tick, ymin=0, ymax=1, color='black', linestyle='--', linewidth=0.5, zorder=2)\n",
    "\n",
    "    # plot a horizontal lines that spans the entire x-axis for each of the y_ticks we have\n",
    "    for y_tick in ax_yticks:\n",
    "        ax.axhline(y=y_tick, xmin=0, xmax=1, color='black', linestyle='--', linewidth=0.5, zorder=2)\n",
    "\n",
    "    # plot USGS observation and corresponding NWM gridpoint for each river\n",
    "    ax.plot(pd.to_datetime(df_usgs_obs['datetime']), df_usgs_obs['00060_Mean cms'], color='black', label='USGS Observation', linestyle='-', linewidth=2, zorder=1)\n",
    "    ax.plot(pd.to_datetime(df_nwm_gridpoint['time']), df_nwm_gridpoint['Discharge m3 s-1'], color='Red', label='National Water Model', linestyle='-', linewidth=2, zorder=1)\n",
    "\n",
    "    # add legend\n",
    "    ax.legend(loc='upper left', ncol=1, fancybox=True, shadow=True, framealpha=0.9, facecolor='snow', edgecolor='black', prop=fontdict_legend_labels)\n",
    "\n",
    "    # add titles to each subplot corresponding to the river it is representing\n",
    "    if ax_index == 0:\n",
    "        ax.set_title('Bear River: USGS Streamflow Observations vs. National Water Model Gridpoint (Water Years: 2016-2019)', **fontdict_title_labels)\n",
    "    if ax_index == 1:\n",
    "        ax.set_title('Jordan-Provo River: USGS Streamflow Observations vs. National Water Model Gridpoint (Water Years: 2016-2019)', **fontdict_title_labels)\n",
    "    if ax_index == len(axs)-1:\n",
    "        ax.set_title('Weber River: USGS Streamflow Observations vs. National Water Model Gridpoint (Water Years: 2016-2019)', **fontdict_title_labels)\n",
    "\n",
    "        # x-axis customizations for last (bottom) axis\n",
    "        ax.set_xlim([ax_xticks[0],ax_xticks[-1]])\n",
    "        ax.set_xticks(ax_xticks)\n",
    "        ax.set_xticklabels(ax_xticklabels, fontdict=fontdict_tick_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fc3b3-cbbf-48fe-ad1a-b88479e5ee39",
   "metadata": {},
   "source": [
    "At first glance, the patterns between observed and modeled streamflow seem similar but slightly offset temporally and NWM looks like the predictions are higher. Let's run one more analysis before we interpret these data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed704173-11d5-4219-be9b-9e6193ca4cc2",
   "metadata": {
    "id": "ed704173-11d5-4219-be9b-9e6193ca4cc2"
   },
   "source": [
    "The code block below computes the root mean sqaure error (RMSE) for predicted vs. observed streamflow using the **`mean_sqaured_error`** function from the `sklearn` Python package. This is an error statistic that tells us how different the NWM is compared to the USGS values. RMSE is typically calculated for the entire dataset; we calculate RMSE this way, and calculate RMSE for points where the NWM is predicting higher streamflow and lower streamflow, respectively. The idea was to look for trends in how the model over- or under-estimates streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdf23b-41fa-416f-afef-7f6567c4d452",
   "metadata": {
    "id": "02cdf23b-41fa-416f-afef-7f6567c4d452",
    "outputId": "ed549c22-655c-490d-8302-9d51abea9f7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define figure and axis for plotting\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(19,6), gridspec_kw=dict(hspace=0.25))\n",
    "\n",
    "# define font dictionaries for plotting\n",
    "fontdict_legend_labels   = {'size': 14, 'weight': 'bold'}\n",
    "fontdict_tick_labels     = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "fontdict_axis_labels     = {'fontsize': 14, 'fontweight': 'bold'}\n",
    "fontdict_title_labels    = {'fontsize': 12, 'fontweight': 'bold'}\n",
    "fontdict_text_annotation = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "\n",
    "# for each subplot (watershed), do the following\n",
    "for ax_index, ax in enumerate(axs):\n",
    "\n",
    "    # read in USGS observation file, and NWM gridpoint file, into pandas DataFrame\n",
    "    df_usgs_obs      = pd.read_csv(glob_usgs_discharge[ax_index])\n",
    "    df_nwm_gridpoint = pd.read_csv(glob_nwm_gridpoints[ax_index])\n",
    "\n",
    "    # compute mean absolute error, root mean square error, and R^2 for each plot\n",
    "    # round values to nearest 2 decimal points for display purposes in legend\n",
    "    rmse_total = np.round(mean_squared_error(df_usgs_obs['00060_Mean cms'], df_nwm_gridpoint['Discharge m3 s-1'], squared=False), 2)\n",
    "    rmse_above = np.round(mean_squared_error(df_usgs_obs['00060_Mean cms'][np.where(df_nwm_gridpoint['Discharge m3 s-1'] > df_usgs_obs['00060_Mean cms'])[0]], df_nwm_gridpoint['Discharge m3 s-1'][np.where(df_nwm_gridpoint['Discharge m3 s-1'] > df_usgs_obs['00060_Mean cms'])[0]], squared=False), 2)\n",
    "    rmse_below = np.round(mean_squared_error(df_usgs_obs['00060_Mean cms'][np.where(df_nwm_gridpoint['Discharge m3 s-1'] < df_usgs_obs['00060_Mean cms'])[0]], df_nwm_gridpoint['Discharge m3 s-1'][np.where(df_nwm_gridpoint['Discharge m3 s-1'] < df_usgs_obs['00060_Mean cms'])[0]], squared=False), 2)\n",
    "\n",
    "    # add legend with RMSE info\n",
    "    legend_title = f'Total RMSE: {rmse_total}\\nAbove RMSE: {rmse_above}\\nBelow RMSE: {rmse_below}'\n",
    "    ax.legend(title=legend_title, loc='upper right', ncol=1, fancybox=True, shadow=True, framealpha=1, facecolor='snow', edgecolor='black', prop=fontdict_legend_labels)\n",
    "\n",
    "    # plot USGS observation and corresponding NWM gridpoint for each river\n",
    "    ax.scatter(df_usgs_obs['00060_Mean cms'], df_nwm_gridpoint['Discharge m3 s-1'], s=10, c='blue', marker='o', zorder=1)\n",
    "\n",
    "    # create variables for title string and axis bounds for each subplot based on its index\n",
    "    if ax_index == 0:\n",
    "        tmp_title = 'Bear River \\n USGS Obs vs. NWM Gridpoint  \\n WY: 2015-2019'\n",
    "        ax_xmin, ax_xmax, ax_xstep, ax_ymin, ax_ymax, ax_ystep = [0,300,25,0,300,25]\n",
    "    if ax_index == 1:\n",
    "        tmp_title = 'Jordan-Provo River \\n  USGS Obs vs. NWM Gridpoint  \\n WY: 2015-2019'\n",
    "        ax_xmin, ax_xmax, ax_xstep, ax_ymin, ax_ymax, ax_ystep = [0,20,2,0,20,2]\n",
    "    if ax_index == 2:\n",
    "        tmp_title = 'Weber River \\n USGS Obs vs. NWM Gridpoint  \\n WY: 2015-2019'\n",
    "        ax_xmin, ax_xmax, ax_xstep, ax_ymin, ax_ymax, ax_ystep = [0,300,25,0,300,25]\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(tmp_title, **fontdict_title_labels)\n",
    "\n",
    "    # x-axis customizations\n",
    "    ax_xticks = np.arange(ax_xmin, ax_xmax+ax_xstep, ax_xstep)\n",
    "    ax_xticklabels = ax_xticks\n",
    "    ax.set_xlim([ax_xmin, ax_xmax])\n",
    "    ax.set_xticks(ax_xticks)\n",
    "    ax.set_xticklabels(ax_xticklabels, fontdict=fontdict_tick_labels)\n",
    "    ax.set_xlabel('Observed Stream Flow (m$^3$/s)', fontsize=14)\n",
    "\n",
    "    # y-axis customizations\n",
    "    ax_yticks = np.arange(ax_ymin, ax_ymax+ax_ystep, ax_ystep)\n",
    "    ax_yticklabels = ax_yticks\n",
    "    ax.set_ylim([ax_ymin, ax_ymax])\n",
    "    ax.set_yticks(ax_yticks)\n",
    "    ax.set_yticklabels(ax_yticklabels, fontdict=fontdict_tick_labels)\n",
    "    ax.set_ylabel('Modeled Stream Flow (m$^3$/s)', fontsize=14)\n",
    "\n",
    "    # plot the x and yticks against each other to have a 1:1 line\n",
    "    ax.plot(ax_xticks, ax_yticks, color='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad315e52-64c5-4f78-8c8c-405272a1a8ee",
   "metadata": {
    "id": "ad315e52-64c5-4f78-8c8c-405272a1a8ee"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Discussion: \n",
    "\n",
    "In general, the **NWM predictions appear to be higher than the observed values**. We see this both in the time series of streamflow above, and in the scatterplots we just made. For the scatterplots, the black line represents a 1:1 correlation between the modeled and observed values (i.e., if they were a perfect match). Points above the line indicate that the NWM values are higher than observed values for the timestep, while points below the line indicate that the NWM values are lower than observed values for the timestep.\n",
    "\n",
    "- This pattern is likely because the NWM is simulating natural streamflow, which does not consider human uses, while the USGS measures the actual water in the river, which is influenced by human consumptive withdrawals and inter-basin transfers. Water from the Bear, Weber, and Jordan-Provo watersheds is diverted for agriculture, cities, and other uses [(Null & Wurtsbaugh, 2020)](https://doi.org/10.1007/978-3-030-40352-2_1). Advisory boards comprised of scientists, policymakers, and other stakeholders around the Great Salt Lake are interested in understanding the total water budget for the lake and the three watersheds that feed it [(Eisenhauer & Nicholson, 2005)](https://doi.org/10.1080/15330150590910701). These groups argue that the human causes of lake levels are being masked by droughts and wet cycles, and that we therefore need a clear water budget to inform any actionable change [(Wurtsbaugh et al., 2017)](https://doi.org/10.1038/ngeo3052). Using another water balance model that can incorporate human use and account for streamflow, or an effort to optimize the NWM for the Great Salt Lake (or at least Intermountain West on the whole) would likely improve predictions.\n",
    "\n",
    "- There is also a mismatch between peak flowss, with variations spanning 1-2 months. One possible explanation for these disparities may be the limitations in the physical representation of snow processes within the NWM model, leading to inaccuracies in snowmelt values [(Garousi-Nejad and Tarboton, 2022)](https://doi.org/10.1002/hyp.14469). These inaccuracies have a substantial impact on the streamflow in this region, given its heavy reliance on snowmelt. This discussion pertains to the applicability of the NWM for water supply prediction. While the NWM has undergone calibration for the entire continental U.S., it lacks specific calibration for local regions. Consequently, the model employs spatially averaged parameters that my not provide accurate predictions for smaller areas, such as the watersheds of interest. Furthermore, the model has most often been used for flood prediction, focusing on short-term forecasts spanning hours, days, or weeks.  Its suitability for long-term water supply prediction remains uncertain. Presently, there is a growing effort among researchers in various regions to use the NWM inputs and code while customizing them to align more closely with local observations. These regional adaptions can then potentially be reported to the Office of Water Prediction, enabling the incorporation of regionally-specific parameters and modifications. Additionally, some researchers are working on modifying the NWM code to include modules capable of accommodating different scenarios, such as variations in human water use.\n",
    "\n",
    "**References**:\n",
    "- Eisenhauer, B. W., & Nicholson, B. (2005). Using Stakeholders’ Views: A Social Science Methodology for the Inclusive Design of Environmental Communications. Applied Environmental Education & Communication, 4(1), 19–30. https://doi.org/10.1080/15330150590910701\n",
    "- Garousi-Nejad, I., & Tarboton, D. G. (2022). A comparison of National Water Model retrospective analysis snow outputs at snow telemetry sites across the Western United States. Hydrological Processes, 36(1), e14469. https://doi.org/10.1002/hyp.14469\n",
    "- Null, S. E., & Wurtsbaugh, W. A. (2020). Water Development, Consumptive Water Uses, and Great Salt Lake. In B. K. Baxter & J. K. Butler (Eds.), Great Salt Lake Biology: A Terminal Lake in a Time of Change (pp. 1–21). Springer International Publishing. https://doi.org/10.1007/978-3-030-40352-2_1\n",
    "- Thorsen, M. L., Handy, R. G., Sleeth, D. K., Thiese, M. S., & Riches, N. O. (2017). A comparison study between previous and current shoreline concentrations of heavy metals at the Great Salt Lake using portable X-ray fluorescence analysis. Human and Ecological Risk Assessment: An International Journal, 23(8), 1941–1954. https://doi.org/10.1080/10807039.2017.1349541\n",
    "- Wurtsbaugh, W. A., Miller, C., Null, S. E., DeRose, R. J., Wilcock, P., Hahnenberger, M., Howe, F., & Moore, J. (2017). Decline of the world’s saline lakes. Nature Geoscience, 10(11), Article 11. https://doi.org/10.1038/ngeo3052\n",
    "\n",
    "**Further Reading:**\n",
    "- Thorsen, M. L., Handy, R. G., Sleeth, D. K., Thiese, M. S., & Riches, N. O. (2017). A comparison study between previous and current shoreline concentrations of heavy metals at the Great Salt Lake using portable X-ray fluorescence analysis. Human and Ecological Risk Assessment: An International Journal, 23(8), 1941–1954. https://doi.org/10.1080/10807039.2017.1349541\n",
    "\n",
    "    \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7f7b5-aade-40fa-9700-c61300b591e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## <font color='blue'> **Analysis 4.3.** </font> Compare the cumulative precipitation and runoff for each water year\n",
    "\n",
    "<div style=\"background-color: whitesmoke; padding: 10px; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2); color: #333;\">\n",
    "This analysis compares cumulative AORC precipitation to cumulative NWM runoff (i.e., discharge converted into depth). The point of this exercise is to make inferences about how the input precipitation data influences outputs. In hydrology, we expect precipitation to be higher than runoff, since water models cannot produce water. This is especially true since the National Water Model version we are using incorporates AORC as a forcing dataset. Let's see if it works out that way. \n",
    "\n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7f4bc-7f51-4170-9091-1632088a2d8a",
   "metadata": {
    "id": "33e7f4bc-7f51-4170-9091-1632088a2d8a"
   },
   "source": [
    "We actually were able to download daily PRISM data by this point in the week, so we incorporated it into this analysis. Here, we compare the cumulative PRISM and AORC precipitation to cumulative USGS and NWM streamflow. This accomplishes several things. It visualized:\n",
    "- how the precipitation datasets relate to one another, \n",
    "- how the streamflow datasets relate to one another, and \n",
    "- how the quality of the AORC input dataset for the NWM might be affecting modeled runoff.\n",
    "\n",
    "We stored the downloaded PRISM daily precipitation files in a [HydroShare repository](https://www.hydroshare.org/resource/efe2b649e65e4966860ae2d935a0a1ec/), so we'll use the same method of accessing HydroShare data we've used before. We should have already downloaded and cleaned all the other data sources from the analyses above, so we just re-import those datasets from repositories we created earlier in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aba434-16f4-4423-92ea-37b00a319ed1",
   "metadata": {},
   "source": [
    "The third analysis includes two main steps:  <b/>\n",
    "\n",
    "**Step A.** Import the datasets\n",
    "> 4.3.1. Import PRISM daily precipitation data from HydroShare \\\n",
    "> 4.3.2. Read in the AORC daily precipitation data from our `AORC_outputs` folder \\\n",
    "> 4.3.3. Read in the daily USGS streamflow data from our `usgs_outputs` folder \\\n",
    "> 4.3.4. Read in the daily NWM runoff data from our `nwm_outputs` folder\n",
    "\n",
    "\n",
    "**Step B.** Calculate and graph the cumulative precipitation and runoff for each river\n",
    "> 4.3.5. Merge the precipitation and runoff datasets \\\n",
    "> 4.3.6. Calculate cumulative precipitation and runoff \\\n",
    "> 4.3.7. Plot the cumulative precipitation and runoff for each watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fd4b6-820c-456d-8203-1e9820a5405f",
   "metadata": {},
   "source": [
    "### 4.3.1. Import PRISM daily precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97961f2-ea52-4fbd-8c86-50591a007572",
   "metadata": {},
   "source": [
    "First we'll download the PRISM daily data from our Hydroshare resource using the unique ID `efe2b649e65e4966860ae2d935a0a1ec`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f023e-fe38-4e37-b33b-d227d34272f7",
   "metadata": {
    "id": "3f5f023e-fe38-4e37-b33b-d227d34272f7",
    "outputId": "410bd638-0c11-4913-af20-585f390fe92a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sign into HydroShare\n",
    "hs = HydroShare()\n",
    "hs.sign_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead8ed2-c26e-44fc-b0cf-19d400048df9",
   "metadata": {
    "id": "6ead8ed2-c26e-44fc-b0cf-19d400048df9",
    "outputId": "f460a595-f03e-4d09-853a-875f134c19b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download resource with its unique identifier\n",
    "res = hs.resource('efe2b649e65e4966860ae2d935a0a1ec')\n",
    "res.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167deeb5-a499-424e-bf71-13d4d9363403",
   "metadata": {
    "id": "167deeb5-a499-424e-bf71-13d4d9363403",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unzip the folder you just imported\n",
    "!unzip -q efe2b649e65e4966860ae2d935a0a1ec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8824e55-8922-44d9-8727-8ad324e0769e",
   "metadata": {},
   "source": [
    "Next, we'll loop through all the CSV files in the `PRISM_daily` folder inside our Hydroshare resource and save the contents as global dataframe variables. \"Global variable\" means that the variable will be accessible outside the `for` loop, as opposed to a local variable (like `df` below) that is only accessible inside the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e7a7c-dd95-4e02-af1a-51799ded4b4c",
   "metadata": {
    "id": "864e7a7c-dd95-4e02-af1a-51799ded4b4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save a string variable to specify the filepath for the PRISM_daily folder\n",
    "directory = \"./efe2b649e65e4966860ae2d935a0a1ec/data/contents/Precipitation/PRISM_daily\"\n",
    "\n",
    "# save all the filenames of the CSV files in the PRISM_daily folder\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# for each of those filenames\n",
    "for csv_file in csv_files:\n",
    "    \n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file)).drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    # rename the precipitation column to PRISM_precip_mm to prepare for merging in Step 6.1\n",
    "    df = df.rename(columns={'Precipitation (mm)' : 'PRISM_precip_mm'})\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # get the base name of the CSV file (without extension) for saving as a global variable\n",
    "    csv_base_name = os.path.splitext(csv_file)[0]\n",
    "\n",
    "    # create a variable name and store the DataFrame in it\n",
    "    variable_name = f\"PRISM_{csv_base_name}\"\n",
    "    globals()[variable_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa560b-7543-42ef-b4de-38eda57127bb",
   "metadata": {},
   "source": [
    "The daily data are now stored in DataFrames called `PRISM_daily_provo`, `PRISM_daily_weber`, and `PRISM_daily_bear`. The `globals()` function is what saves our DataFrame variable to the global environment. We're going to repeat this basic structure - set a directory, loop through the CSV files, and save their contents as global variables - for all the remaining datasets. For now, let's do a \"sanity check\" to make sure one of the global DataFrames looks the way we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828f7fe-0076-4508-92a0-deafe21adf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "PRISM_daily_bear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c3581-9654-4c95-8239-2bdcf98c0ffd",
   "metadata": {},
   "source": [
    "Looks good! We see the `Date` column and our renamed column with the precipitation data. We'll repeat this sanity check steps after importing the rest of the datasets, too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6136dc8-5c57-4312-83f1-6110140f1e99",
   "metadata": {},
   "source": [
    "### 4.3.2. Import AORC daily precipitation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c85d-2430-4dad-83f8-55644b6582a9",
   "metadata": {},
   "source": [
    "This step will be almost exactly the same as 5.1., except instead of downloading the Hydroshare resource, we're going to access data we already cleaned up in <font color='blue'>Analysis 4.1</font> and <font color='blue'> Analysis 4.2</font>. We'll also rename the precipitation column to make the data easier to understand after we merge all the datasets, and coerce the `Date` column to be a datetime object. We'll make sure the `Date` columns for the other datasets are all named this way and datetime objects as well, so we don't get unexpected behavior when we merge all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2375140-3d8d-4e68-8906-f65c0c114596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the directory and get a list of CSV files in that directory\n",
    "directory = \"./AORC_outputs\"\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('daily_precip.csv')]\n",
    "\n",
    "# lkoop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    \n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "    \n",
    "    # subset by only the columns of interest (date, precipitation, and water year)\n",
    "    df1 = df.loc[:, ['Date','avg', 'Water_Year']]\n",
    "    \n",
    "    # rename the precipitation column to AORC_precip_mm to prepare for merging in step 6.1\n",
    "    df1 = df1.rename(columns={'avg':'AORC_precip_mm'})\n",
    "    \n",
    "    # coerce Date to be datetime object using pandas\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "    \n",
    "    # get the base name of the CSV file (without extension) for saving as a global variable\n",
    "    csv_base_name = os.path.splitext(csv_file)[0]\n",
    "    \n",
    "    # create a variable name and store the DataFrame in it\n",
    "    variable_name = f\"AORC_{csv_base_name}\"\n",
    "    globals()[variable_name] = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6045-eeb8-4bc5-b812-f0b636698e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "AORC_bear_daily_precip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d15676-782c-4b23-9bdf-71930d3d257d",
   "metadata": {},
   "source": [
    "This looks good too! This data has a `Water_Year` column. Water years span from October-October and are a construct water managers use for water prediction, since precipitation usually begins in the Fall, builds through the Winter and Spring, and drops to its low point in the Summer. We want to plot our cumulative values according to each water year (see <font color='red'>step 6.3</font>), so we'll keep this column. We don't need to calculate water year for the other datasets since this column will carry over when we merge everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eef63b-43e8-41d2-aaa1-53c8328dd685",
   "metadata": {},
   "source": [
    "### 4.3.3. Import USGS daily streamflow data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff22248-bdf6-4534-85de-d0eedd7f518f",
   "metadata": {},
   "source": [
    "Now, we can move on to the runoff datasets. We'll start with USGS, then import the NWM predicted values. Again, the process is similar to 5.1. However, for the runoff datasets, we'll need to convert the units from cubic meters per second (cms) to milimeters (mm) to match the precipitation datasets. To do that, we need to know the area of each watershed. We've hard-coded this in the `areas` list below, and use that in our conversion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c59165-8460-4932-b710-6c92e957821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of areas for the watershed (in the order [bear, jordan, weber])\n",
    "cf_km_to_m = 1000000  # conversion factor to convert km2 to m2\n",
    "areas = [19462.56*cf_km_to_m, 6436.28*cf_km_to_m, 9862.35*cf_km_to_m]\n",
    "\n",
    "# set the directory and get a list of CSV files in that directory\n",
    "directory = \"./usgs_outputs\"\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    \n",
    "    # get the base name of the CSV file (without extension) for saving as a global variable\n",
    "    csv_base_name = os.path.splitext(csv_file)[0]\n",
    "    \n",
    "    # find the area of the watershed from the areas list and set a temporary string to the name of the watershed\n",
    "    \n",
    "    # if the name of the file preceding the .csv extension is 'obs_q_bear'...\n",
    "    if csv_base_name == 'obs_q_bear':\n",
    "        # select the appropriate index from the areas list\n",
    "        area = areas[0]\n",
    "        # set a shorter string variable to name the global DataFrame\n",
    "        tmp_str = 'bear'\n",
    "    elif csv_base_name == 'obs_q_jordan':\n",
    "        area = areas[1]\n",
    "        tmp_str = 'jordan'\n",
    "    else:\n",
    "        area = areas[2]\n",
    "        tmp_str = 'weber'\n",
    "    \n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "    \n",
    "    # subset by only the columns of interest (date, streamflow)\n",
    "    df1 = df.loc[:, ['datetime','00060_Mean cms']]\n",
    "    \n",
    "    # convert the 'datetime' column to datetime object (should already be one, this is just in case)\n",
    "    df1['datetime'] = pd.to_datetime(df1['datetime'])\n",
    "    \n",
    "    # remove the timestamp from the 'datetime' column and name date-only column 'Date' for merging in Step 6.1\n",
    "    df1['Date'] = df1['datetime'].dt.date\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "    \n",
    "    # drop the 'datetime' column\n",
    "    df1.drop('datetime', axis=1, inplace=True)\n",
    "    \n",
    "    # rename the streamflow column to to reflect the dataset and units\n",
    "    df1 = df1.rename(columns={'00060_Mean cms':'USGS_q_cms'})\n",
    "    \n",
    "    # convert cms to mm to prepare for merging/comparison\n",
    "    df1['USGS_q_mm'] = (df1['USGS_q_cms']*3600*24*1000)/area\n",
    "    \n",
    "    # drop the 'USGS_q_cms' column\n",
    "    df1.drop('USGS_q_cms', axis=1, inplace=True)\n",
    "    \n",
    "    # create a variable name and store the DataFrame in it\n",
    "    variable_name = f\"USGS_{tmp_str}_daily_streamflow\"\n",
    "    globals()[variable_name] = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea0bfe-f839-4c82-8ad8-6c204383deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "USGS_bear_daily_streamflow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7dd4f-e310-46b5-9fc2-1ecb0e3f8f16",
   "metadata": {},
   "source": [
    "### 4.3.4. Import NWM daily runoff data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea969ae-76e0-4b4c-a43d-c1ff02b506ec",
   "metadata": {},
   "source": [
    "Lastly, we'll import the NWM data. We set our directory to our `nwm_outputs` folder and extract the data in its constituent CSV files, converting the units to mm, renaming columns, and coercing the `Date` column to datetime as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a826f1-cab1-49b2-b377-01edc3a8885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of areas for the watershed (in the order [bear, jordan, weber])\n",
    "cf_km_to_m = 1000000  # conversion factor to convert km2 to m2\n",
    "areas = [19462.56*cf_km_to_m, 6436.28*cf_km_to_m, 9862.35*cf_km_to_m]\n",
    "\n",
    "# set the directory and get a list of CSV files in that directory\n",
    "directory = \"./nwm_outputs\"\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    \n",
    "    # get the base name of the CSV file (without extension) for naming the global variable\n",
    "    csv_base_name = os.path.splitext(csv_file)[0]\n",
    "    \n",
    "    # find the area of the watershed from the areas list and set a temporary string to the name of the watershed\n",
    "    if csv_base_name == 'nwm_q_bear':\n",
    "        area = areas[0]\n",
    "        tmp_str = 'bear'\n",
    "    elif csv_base_name == 'nwm_q_jordan':\n",
    "        area = areas[1]\n",
    "        tmp_str = 'jordan'\n",
    "    else:\n",
    "        area = areas[2]\n",
    "        tmp_str = 'weber'\n",
    "    \n",
    "    # read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file))\n",
    "    \n",
    "    # rename datetime column to 'Date' for merging in Step 6.1\n",
    "    df = df.rename(columns={'time':'Date'})\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # convert cms to mm to prepare for merging/comparison\n",
    "    df['NWM_q_mm'] = (df['Discharge m3 s-1']*3600*24*1000)/area\n",
    "    \n",
    "    # drop the 'USGS_q_cms' column\n",
    "    df.drop('Discharge m3 s-1', axis=1, inplace=True)\n",
    "    \n",
    "    # create a variable name and store the DataFrame in it\n",
    "    variable_name = f\"NWM_{tmp_str}_daily_streamflow\"\n",
    "    globals()[variable_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bda6b-eb49-4cce-8077-8d5f6ed8b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "NWM_bear_daily_streamflow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c70a99-7f8d-4492-8c61-53f12c30470d",
   "metadata": {},
   "source": [
    "Great! We're ready to merge all the datasets and calculate their cumulative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c54bea-90ab-4465-85c1-aafaf2ae8cba",
   "metadata": {},
   "source": [
    "### 4.3.5. Merge the different datasets for each river basin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410ff92-49a8-40f3-9080-68b46abe2f86",
   "metadata": {},
   "source": [
    "The code block below merges the 4 datasets (PRISM, AORC, USGS, and NWM) for each watershed and results in 3 final dataframes called `merged_bear` for the Bear River, `merged_jordan` for the Jordan-Provo River, and `merged_weber` for the Weber River.\n",
    "\n",
    "We could do this step in a `for` loop instead of repeating the steps for each DataFrame, but for simplicity's sake we did this brute force. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3aa0e9-7c3d-414f-b06e-33270d72e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force merge NWM/USGS (streamflow) and AORC/PRISM (precipitation) datasets\n",
    "\n",
    "# for the Bear River, start with the NWM data and merge the USGS data on the `Date` column\n",
    "merged_bear = NWM_bear_daily_streamflow.merge(USGS_bear_daily_streamflow, on='Date')\n",
    "# iteratively merge the merged_bear dataframe with the other datasets\n",
    "merged_bear = merged_bear.merge(AORC_bear_daily_precip, on='Date')\n",
    "merged_bear = merged_bear.merge(PRISM_daily_bear, on='Date')\n",
    "\n",
    "# repeat for the Jordan-Provo River\n",
    "merged_jordan = NWM_jordan_daily_streamflow.merge(USGS_jordan_daily_streamflow, on='Date')\n",
    "merged_jordan = merged_jordan.merge(AORC_provo_daily_precip, on='Date')\n",
    "merged_jordan = merged_jordan.merge(PRISM_daily_provo, on='Date')\n",
    "\n",
    "# repeat for the Weber River\n",
    "merged_weber = NWM_weber_daily_streamflow.merge(USGS_weber_daily_streamflow, on='Date')\n",
    "merged_weber = merged_weber.merge(AORC_weber_daily_precip, on='Date')\n",
    "merged_weber = merged_weber.merge(PRISM_daily_weber, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897198fc-1379-43b5-ad77-ad20defdcba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "merged_weber.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8be5f6-3bcc-4e21-97a2-76127387b1a5",
   "metadata": {},
   "source": [
    "We can see using the `merged_weber` DataFrame as an example that we now have the 4 daily-value datasets in one dataframe, along with the date column and the water year. Next, we want to add up all the precipitation and runoff across the water year and plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b043c91-547d-4741-8e90-b360272b563f",
   "metadata": {},
   "source": [
    "### 4.3.6. Calculate the cumulative values across the water year for each watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb7c55-909a-4f7e-9821-08583cba59fb",
   "metadata": {},
   "source": [
    "The code block below uses the pandas function `groupby()` to group the data in each column by water year and calculate the cumulative sum, storing it in a new column. We again avoid doing this in a `for` loop for time and clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6707c8b-0e3f-4f5b-90bb-f7d4d62c5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each data column (AORC/PRISM/NWM/USGS) group the data by water year and calculate the cumulative sum\n",
    "merged_bear['cum_precip_mm_AORC'] = merged_bear.groupby(['Water_Year'])['AORC_precip_mm'].cumsum()\n",
    "merged_bear['cum_precip_mm_PRISM'] = merged_bear.groupby(['Water_Year'])['PRISM_precip_mm'].cumsum()\n",
    "merged_bear['cum_runoff_mm_NWM'] = merged_bear.groupby(['Water_Year'])['NWM_q_mm'].cumsum()\n",
    "merged_bear['cum_runoff_mm_USGS'] = merged_bear.groupby(['Water_Year'])['USGS_q_mm'].cumsum()\n",
    "\n",
    "# repeat for Jorda-Provo River\n",
    "merged_jordan['cum_precip_mm_AORC'] = merged_jordan.groupby(['Water_Year'])['AORC_precip_mm'].cumsum()\n",
    "merged_jordan['cum_precip_mm_PRISM'] = merged_jordan.groupby(['Water_Year'])['PRISM_precip_mm'].cumsum()\n",
    "merged_jordan['cum_runoff_mm_NWM'] = merged_jordan.groupby(['Water_Year'])['NWM_q_mm'].cumsum()\n",
    "merged_jordan['cum_runoff_mm_USGS'] = merged_jordan.groupby(['Water_Year'])['USGS_q_mm'].cumsum()\n",
    "\n",
    "# repeat for Weber River\n",
    "merged_weber['cum_precip_mm_AORC'] = merged_weber.groupby(['Water_Year'])['AORC_precip_mm'].cumsum()\n",
    "merged_weber['cum_precip_mm_PRISM'] = merged_weber.groupby(['Water_Year'])['PRISM_precip_mm'].cumsum()\n",
    "merged_weber['cum_runoff_mm_NWM'] = merged_weber.groupby(['Water_Year'])['NWM_q_mm'].cumsum()\n",
    "merged_weber['cum_runoff_mm_USGS'] = merged_weber.groupby(['Water_Year'])['USGS_q_mm'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a06177-79c4-47d5-b2f7-59a4116a244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "merged_bear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805b5a4-51ef-4ce7-9023-2fe2850994d2",
   "metadata": {},
   "source": [
    "You should now see the cumulative-value columns to the right of our original columns. We're ready to plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7fc44-3926-417e-afc4-38b75f4e5bc2",
   "metadata": {},
   "source": [
    "### 4.3.7. Plot the cumulative precipitation and runoff for each watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03422ff6-8a93-4d38-8180-0ef731754102",
   "metadata": {},
   "source": [
    "Our last step is to plot the cumulative precipitation and runoff we just calculated across all 4 water years for which we have data. To do that, we use `Matplotlib`, which we imported as `plt` in one of the very first code blocks. Matplotlib allows you to plot multiple subpanels in a single figure. We first set the number and specifications of the subplots, then loop through the subplots and display the data for a single river basin in them. This way, we can compare trends across all the river basins and all the datasets in a single plot. \n",
    "\n",
    "We'll first plot AORC and the two runoff datasets, USGS and NWM, without PRISM. The values for PRISM are much higher and so the y-axis gets condensed, making the interactions between the other three datasets harder to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f71bff-eb5d-415a-94e1-e29886ed9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure and three subplots in one row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "# store a list of the dataframes we want to loop through\n",
    "merged_dataframes = [merged_jordan, merged_bear, merged_weber]\n",
    "\n",
    "# store a list of the river names for labeling the plots\n",
    "locations = ['Jordan-Provo River Basin', 'Bear River Basin', 'Weber River Basin']\n",
    "\n",
    "# specify the line colors for each dataset\n",
    "line_colors = ['#1f77b4', '#a6cee3', '#8c510a', '#e6550d']  \n",
    "\n",
    "# for each dataframe/subplot pair\n",
    "for idx, ax in enumerate(axs):\n",
    "    \n",
    "    # select the dataframe from the dataframe list\n",
    "    df = merged_dataframes[idx]\n",
    "    # sel\n",
    "    location = locations[idx]\n",
    "    \n",
    "    ax.plot(df['Date'], df['cum_precip_mm_AORC'], label='Precipitation (AORC)', color=line_colors[0], linestyle='--')\n",
    "    ax.plot(df['Date'], df['cum_runoff_mm_NWM'], label='Runoff (NWM)', color=line_colors[2], linestyle='--')\n",
    "    ax.plot(df['Date'], df['cum_runoff_mm_USGS'], label='Runoff (USGS)', color=line_colors[3], linewidth=3)\n",
    "    \n",
    "    ax.set_title(location, fontsize=14)\n",
    "    ax.set_xlabel('Water Year', fontsize=12)\n",
    "    ax.set_ylabel('Accumulation (mm)', fontsize=14)\n",
    "    ax.set_ylim(0, 600)\n",
    "    ax.legend(fontsize=12, facecolor='#FFEFDB', edgecolor='white')\n",
    "    ax.grid(alpha=0.5)\n",
    "    \n",
    "    # Format x-axis ticks to show only years\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_tick_params(rotation=45)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    \n",
    "    # Set custom x-axis tick labels \n",
    "    ax.set_xticklabels(['2015', '2016', '2017', '2018', '2019', '2020'])\n",
    "\n",
    "    # Change font size of y-axis ticks\n",
    "    ax.yaxis.set_tick_params(size=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "                             \n",
    "    # annotate the plot with the river basin area\n",
    "    ax.text(0.02, 0.75, f'Basin Area: \\n{areas[idx]} km²', transform=ax.transAxes, horizontalalignment='left', verticalalignment='top', fontsize=14, color='black')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf18e7b-ceb6-4e77-af9a-f152e53149df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> Note that the represented input (AORC) and output (NWM) are depicted using dashed lines, whereas the observation (USGS) is indicated by a thick solid line in the plots above. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5175097-ec95-4960-be0b-e9cc85eb6020",
   "metadata": {},
   "source": [
    "We can see a few trends from the data so far. First, AORC precipitation does seem to be higher than runoff from USGS and the NWM. For the Bear and Weber Rivers, the NWM runoff mirrors the precipitation pretty closely. USGS runoff is lower, though. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> The USGS runoff is the observed water in the river on any given day, so incorporates human water use (e.g. through agriculture, industrial production, or municipal supply). The NWM doesn't, so its modeled runoff is higher. This matches what we saw in Analysis 4.2. </b></div>\n",
    "\n",
    "Next, we'll plot the same datasets with the **PRISM** data, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5ba47-1a81-48d9-ba17-768e6ecd794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure and three subplots in one row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "# store a list of the dataframes we want to loop through\n",
    "merged_dataframes = [merged_jordan, merged_bear, merged_weber]\n",
    "\n",
    "# store a list of the river names for labeling the plots\n",
    "locations = ['Jordan-Provo River Basin', 'Bear River Basin', 'Weber River Basin']\n",
    "\n",
    "# specify the line colors for each dataset\n",
    "line_colors = ['#1f77b4', '#a6cee3', '#8c510a', '#e6550d']  \n",
    "\n",
    "# for each dataframe/subplot pair\n",
    "for idx, ax in enumerate(axs):\n",
    "    \n",
    "    # select the dataframe from the dataframe list\n",
    "    df = merged_dataframes[idx]\n",
    "    # sel\n",
    "    location = locations[idx]\n",
    "    \n",
    "    ax.plot(df['Date'], df['cum_precip_mm_AORC'], label='Precipitation (AORC)', color=line_colors[0], linestyle='--')\n",
    "    ax.plot(df['Date'], df['cum_precip_mm_PRISM'], label='Precipitation (PRISM)', color=line_colors[1], linewidth=3)\n",
    "    ax.plot(df['Date'], df['cum_runoff_mm_NWM'], label='Runoff (NWM)', color=line_colors[2], linestyle='--')\n",
    "    ax.plot(df['Date'], df['cum_runoff_mm_USGS'], label='Runoff (USGS)', color=line_colors[3], linewidth=3)\n",
    "    \n",
    "    ax.set_title(location, fontsize=14)\n",
    "    ax.set_xlabel('Water Year', fontsize=12)\n",
    "    ax.set_ylabel('Accumulation (mm)', fontsize=14)\n",
    "    ax.set_ylim(0, 1600)\n",
    "    ax.legend(fontsize=12, facecolor='#FFEFDB', edgecolor='white')\n",
    "    ax.grid(alpha=0.5)\n",
    "    \n",
    "    # Format x-axis ticks to show only years\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_tick_params(rotation=45)\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    \n",
    "    # Set custom x-axis tick labels \n",
    "    ax.set_xticklabels(['2015', '2016', '2017', '2018', '2019', '2020'])\n",
    "\n",
    "    # Change font size of y-axis ticks\n",
    "    ax.yaxis.set_tick_params(size=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "                             \n",
    "    # annotate the plot with the river basin area\n",
    "    ax.text(0.02, 0.65, f'Basin Area: \\n{areas[idx]} km²', transform=ax.transAxes, horizontalalignment='left', verticalalignment='top', fontsize=14, color='black')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27a0e5-fee7-441a-9c27-8eb4409d1e76",
   "metadata": {},
   "source": [
    "And that's it! We've completed Analysis 4.3.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><b> Delving deeper: We assumed that the PRISM daily values and the AORC daily values would be pretty different based on our comparisons of the PRISM monthly normals and the AORC daily values in Analysis I. Analysis 4.3 confirms that suspicion. The PRISM values are much higher than the AORC values. The NWM modeled runoff matches the precipitation pretty closely because it doesn't incorporate human use; it appears the main driver of the modeled runoff is the precipitation, so if this dataset isn't robust, the NWM modeled runoff (even without human use) might be less credible. This highlights the need for further benchmarking of the NWM before it can be used for water supply prediction. </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a7d5f-22ce-44ce-8ca6-ecb69a38d9f2",
   "metadata": {
    "id": "136a7d5f-22ce-44ce-8ca6-ecb69a38d9f2",
    "tags": []
   },
   "source": [
    "## <font color='blue'> **Analysis 4.4.** </font> Evaluate Great Salt Lake Shrinkage using Landsat data\n",
    "\n",
    "<div style=\"background-color: whitesmoke; padding: 10px; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2); color: #333;\">\n",
    "    \n",
    "As the water input to the GSL declines, the surface area of the lake shrinks in turn. We can visualize this shrinkage using satellite imagery. The NASA/USGS satellite program, \n",
    "    [Landsat](https://landsat.gsfc.nasa.gov), has collected images of the study area approximately every 16 days since 1984 at a 15 to 30-meter spatial resolution, depending on the band sensor. Here, we use imagery at a 15-meter spatial resolution. Some of the sensors on Landsat satellites are designed to collect radiometric information from earth's surface across the visual and near-infrared (nIR) spectra. Each Landsat scene covers an area of approximately 82 km x 185 km (113 x 115 miles). Since water absorbs light in certain spectra, atmospheric scientists can estimate the total area of a scene that is water - this is one of the things that [Google Earth Engine](https://earthengine.google.com/) does automatically using the Normalized Difference Water Index (NDWI) and that we leverage here. The NDWI calculates a normalized ratio of pixel values in the green and near-infrared bands using the following equation:\n",
    "    \n",
    "$NDWI = \\frac{G-nIR}{G+nIR}\\$\n",
    "    \n",
    "This analysis assesses the gradual shrinkage of the Great Salt Lake since 1894, employing a satellite-based approach utilizing LANDSAT satellite data. The objective of this section is to gain insights into the lake's alterations in both its surface area and volume over the past 38 years.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc8f03-d54d-4348-a953-644fe235ee78",
   "metadata": {},
   "source": [
    "The fourth analysis includes one step:  <b/>\n",
    "\n",
    "**Step A.** Examine shrinkage of the Great Salt lake using LANDSAT data\n",
    "> 4.4.1. Download LANDSAT data using Google Earth Engine (GEE) and perform a Normalized Difference Water Index (NDWI) analysis\n",
    "> \n",
    "> 4.4.2. Load LANDSAT CSV files from GEE into Python and plot a timeseries of the annual changes of NDWI over the Great Salt Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083e69-f0c3-44ec-acf4-94fde76f8102",
   "metadata": {},
   "source": [
    "### 4.4.1. Download LANDSAT data using Google Earth Engine (GEE) and perform a Normalized Difference Water Index (NDWI) analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a60de-ba1a-4c3f-a416-e47104571548",
   "metadata": {},
   "source": [
    "Here we use **Google Earth Engine (GEE) platform** to create a GIF of the best quality LANDSAT images (minimum of clouds and radiographic distortion, etc) for each year from 1984-2022. We also wanted to create a time series chart of the lake area across the same time period. To do so, we used GEE to calculate the number of water pixels in the Landsat imagery during the maximum lake area for each year.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6338e-7d53-4d77-8e4c-74e19eda85df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE: </b> This section of the code is not currently executable in the Jupyter notebook. To perform this analysis, you will have to create a Google Earth Engine (https://earthengine.google.com/) account and use their Code Editor. The accounts are free for researchers associated with academia or a nonprofit. Once you are in the code editor, you can copy/paste the javascript code below into the editor and repeat our analysis. </b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bcb9b-c9d7-45b3-a5d4-ea8dd5f7bc0b",
   "metadata": {},
   "source": [
    "The code below calculates the number of water pixels in the Landsat imagery during the maximum lake area for each year using LANDSAT 8 and LANDSAT 5 to cover the entire time period. The intermediate Landsat launch had sensor errors that made the data unusable, so we do have a gap in our time series. Also, the javascript scripts cannot export the resulting CSV files for the Landsat 8 and Landsat 5 time series; these have to be exported using GEE's **\"Download CSV\"** button. We have already collected this data and stored them in our [HydroShare resource](https://www.hydroshare.org/resource/efe2b649e65e4966860ae2d935a0a1ec/), which we will import in the following steps for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c579928-64a7-4b0b-8d9e-8b753c6d78df",
   "metadata": {
    "id": "3c579928-64a7-4b0b-8d9e-8b753c6d78df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "////////////////////////////////////////// LANDSAT8\n",
    "////////////// LOAD DATA\n",
    "// Define the bounding box geometry of the Great Salt Lake\n",
    "\n",
    "var geometry = ee.Geometry.Polygon(\n",
    "        [[[-113.14050109901953, 41.728451296684945],\n",
    "          [-113.14050109901953, 40.62038097715779],\n",
    "          [-111.75622375526953, 40.62038097715779],\n",
    "          [-111.75622375526953, 41.728451296684945]]], null, false)\n",
    "\n",
    "// Load Landsat 8 collection and filter by date and location\n",
    "// Metadata: https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C02_T1_L2\n",
    "\n",
    "var dataset = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
    "    .filterBounds(geometry)\n",
    "    .filterDate('2013-01-01', '2023-01-01');\n",
    "\n",
    "// Visualize the region\n",
    "\n",
    "var region = geometry;\n",
    "Map.addLayer(region)\n",
    "///////////////\n",
    "\n",
    "/////////////// DEFINE FUNCTIONS\n",
    "// Define a function that calculates a constanct value.\n",
    "// This function creates values of 1 for all pixels and then add them to a new band.\n",
    "\n",
    "var constant = function(image) {\n",
    "  var fun = ee.Image(0).expression(\n",
    "    '((SR_B2) / (SR_B2))', {\n",
    "      'SR_B2': image.select('SR_B2'),\n",
    "    });\n",
    "  return image.addBands(fun.rename('cte'));\n",
    "};\n",
    "\n",
    "// Define a function that masks the images for water pixels\n",
    "// The values of 7 from the band QA_PIXEL are categorized as water.\n",
    "// The Landsat QA_PIXEL band uses individual bits within its binary values to represent\n",
    "// different qualities or conditions of a pixel. By shifting the binary value 1 seven places\n",
    "// to the left, we are essentially creating a mask that has only the seventh bit (bit seven) set to 1,\n",
    "// while all other bits are set to 0.\n",
    "// This bit mask is then used in a bitwiseAND operation with the QA_PIXEL band. When we perform a bitwiseAND\n",
    "// between the QA band and this mask, we effectively extract the information stored in the seventh bit of\n",
    "// the QA band for each pixel. This information could indicate the presence or absence of a specific condition,\n",
    "// such as the pixel being classified as water.\n",
    "\n",
    "function maskWater(image) {\n",
    "    var Water = (1 << 7)\n",
    "    var pixel_qa = image.select('QA_PIXEL');\n",
    "    var mask = pixel_qa.bitwiseAnd(Water);\n",
    "    var mask2 = image.mask().reduce(ee.Reducer.min());\n",
    "    return image.updateMask(mask).updateMask(mask2);\n",
    "}\n",
    "\n",
    "// Define a function that applies scaling factors to the optical bands of\n",
    "// the image for visualization purposes.\n",
    "\n",
    "function applyScaleFactors(image) {\n",
    "  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n",
    "  return image.addBands(opticalBands, null, true);\n",
    "}\n",
    "///////////////\n",
    "\n",
    "/////////////// APPLY DEFINED FUNCTIONS\n",
    "// Create a new band with a constant value of 1 for all pixels.\n",
    "// The new dataset is called dataset_cte.\n",
    "\n",
    "var dataset_cte = dataset.map(constant);\n",
    "print(dataset_cte)\n",
    "\n",
    "// Apply the mask function to the new dataset\n",
    "\n",
    "var waterImages = dataset_cte.map(maskWater);\n",
    "print(waterImages)\n",
    "\n",
    "// Apply the scaling faction function\n",
    "\n",
    "var scaled_waterImages = waterImages.map(applyScaleFactors);\n",
    "var visualization = {\n",
    "  bands: ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "  min: 0.0,\n",
    "  max: 0.3,\n",
    "};\n",
    "\n",
    "// Apply the maximum function to the scaled images and visualize the max values\n",
    "\n",
    "var clippedImages = scaled_waterImages.map(function(image) {\n",
    "  return image.clip(geometry);\n",
    "});\n",
    "Map.addLayer(clippedImages.max(), visualization, 'True Color (321)');\n",
    "///////////////\n",
    "\n",
    "/////////////// CREATE TIMESERIES CHARTS.\n",
    "// Create a timeseries chart of the number of inundated pixels.\n",
    "var timeSeries = ui.Chart.image.seriesByRegion({\n",
    "    imageCollection: waterImages.select('cte'),\n",
    "    regions: region,\n",
    "    reducer: ee.Reducer.sum(),\n",
    "    scale: 30, // scale\n",
    "    xProperty: 'system:time_start',\n",
    "    seriesProperty: 'GreatSaltLake'\n",
    "});\n",
    "// Print the chart to the Console\n",
    "print(timeSeries);\n",
    "///////////////\n",
    "//////////////////////////////////////////\n",
    "\n",
    "////////////////////////////////////////// LANDSAT 5\n",
    "////////////// LOAD DATA\n",
    "// Load Landsat 5 collection and filter by date and location\n",
    "// Define the bounding box geometry of the Great Salt Lake\n",
    "\n",
    "var geometry = ee.Geometry.Polygon(\n",
    "        [[[-113.14050109901953, 41.728451296684945],\n",
    "          [-113.14050109901953, 40.62038097715779],\n",
    "          [-111.75622375526953, 40.62038097715779],\n",
    "          [-111.75622375526953, 41.728451296684945]]], null, false)\n",
    "\n",
    "// Metadata: https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT05_C02_T1_L2\n",
    "\n",
    "var dataset = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\")\n",
    "    .filterBounds(geometry)\n",
    "    .filterDate('1984-01-01', '2013-01-10');\n",
    "\n",
    "// Visualize the region\n",
    "\n",
    "var region = geometry;\n",
    "Map.addLayer(region)\n",
    "///////////////\n",
    "\n",
    "/////////////// DEFINE FUNCTIONS\n",
    "// Define a function that calculates a constanct value.\n",
    "// This function creates values of 1 for all pixels and then add them to a new band.\n",
    "\n",
    "var constant = function(image) {\n",
    "  var fun = ee.Image(0).expression(\n",
    "    '((SR_B2) / (SR_B2))', {\n",
    "      'SR_B2': image.select('SR_B2'),\n",
    "    });\n",
    "  return image.addBands(fun.rename('cte'));\n",
    "};\n",
    "\n",
    "// Define a function that masks the images for water pixels\n",
    "// The values of 7 from the band QA_PIXEL are categorized as water.\n",
    "// The Landsat QA_PIXEL band uses individual bits within its binary values to represent\n",
    "// different qualities or conditions of a pixel. By shifting the binary value 1 seven places\n",
    "// to the left, we are essentially creating a mask that has only the seventh bit (bit seven) set to 1,\n",
    "// while all other bits are set to 0.\n",
    "// This bit mask is then used in a bitwiseAND operation with the QA_PIXEL band. When we perform a bitwiseAND\n",
    "// between the QA band and this mask, we effectively extract the information stored in the seventh bit of\n",
    "// the QA band for each pixel. This information could indicate the presence or absence of a specific condition,\n",
    "// such as the pixel being classified as water.\n",
    "\n",
    "function maskWater(image) {\n",
    "    var Water = (1 << 7)\n",
    "    var pixel_qa = image.select('QA_PIXEL');\n",
    "    var mask = pixel_qa.bitwiseAnd(Water);\n",
    "    var mask2 = image.mask().reduce(ee.Reducer.min());\n",
    "    return image.updateMask(mask).updateMask(mask2);\n",
    "}\n",
    "\n",
    "// Define a function that applies scaling factors to the optical bands of\n",
    "// the image for visualization purposes.\n",
    "\n",
    "function applyScaleFactors(image) {\n",
    "  var opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2);\n",
    "  return image.addBands(opticalBands, null, true);\n",
    "}\n",
    "///////////////\n",
    "\n",
    "/////////////// APPLY DEFINED FUNCTIONS\n",
    "// Create a new band with a constant value of 1 for all pixels.\n",
    "// The new dataset is called dataset_cte.\n",
    "\n",
    "var dataset_cte = dataset.map(constant);\n",
    "print(dataset_cte)\n",
    "\n",
    "// Apply the mask function to the new dataset\n",
    "\n",
    "var waterImages = dataset_cte.map(maskWater);\n",
    "print(waterImages)\n",
    "\n",
    "// Apply the scaling faction function\n",
    "\n",
    "var scaled_waterImages = waterImages.map(applyScaleFactors);\n",
    "var visualization = {\n",
    "  bands: ['SR_B3', 'SR_B2', 'SR_B1'],\n",
    "  min: 0.0,\n",
    "  max: 0.3,\n",
    "};\n",
    "\n",
    "// Apply the maximum function to the scaled images and visualize the max values\n",
    "var clippedImages = scaled_waterImages.map(function(image) {\n",
    "  return image.clip(geometry);\n",
    "});\n",
    "Map.addLayer(clippedImages.max(), visualization, 'True Color (321)');\n",
    "///////////////\n",
    "\n",
    "/////////////// CREATE TIMESERIES CHARTS.\n",
    "// Create a timeseries chart of the number of inundated pixels.\n",
    "\n",
    "var timeSeries = ui.Chart.image.seriesByRegion({\n",
    "    imageCollection: waterImages.select('cte'),\n",
    "    regions: region,\n",
    "    reducer: ee.Reducer.sum(),\n",
    "    scale: 30, // scale\n",
    "    xProperty: 'system:time_start',\n",
    "    seriesProperty: 'GreatSaltLake'\n",
    "});\n",
    "// Print the chart to the Console\n",
    "print(timeSeries);\n",
    "///////////////"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059acea-141a-4d32-9bfa-3d6abcd17fd6",
   "metadata": {},
   "source": [
    "The following cell illustrates the timelapse GIF files that were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0111f-8c28-4218-9a86-0210312c8ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# URLs of the GIF files you want to display\n",
    "gif_url1 = \"https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/GEE/timelapse_hku_nd.gif\"\n",
    "gif_url2 = \"https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/data/contents/GEE/timelapse_hku.gif\"\n",
    "\n",
    "# Create HTML code to display the GIFs side by side\n",
    "html_code = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"{gif_url1}\" width=\"300\" height=\"300\" style=\"margin-right: 10px;\">\n",
    "    <img src=\"{gif_url2}\" width=\"300\" height=\"300\">\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML code in the Jupyter Notebook\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61936ee6-2392-47ce-a314-0c56fbf15a20",
   "metadata": {},
   "source": [
    "### 4.4.2. Load LANDSAT CSV files from GEE into Python and plot a timeseries of the annual changes of NDWI over the Great Salt Lake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfa0ea-12f6-431c-9456-ba79361223b2",
   "metadata": {
    "id": "f0bfa0ea-12f6-431c-9456-ba79361223b2"
   },
   "source": [
    "Next we load the CSVs we output from GEE to construct the time series using matplotlib. Note that these CSVs were already acquired during the download of the HydroShare resource in <font color='blue'> Analysis 4.3 </font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5263e2-65ab-40e7-b56c-39450d2fdb68",
   "metadata": {
    "id": "1d5263e2-65ab-40e7-b56c-39450d2fdb68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define path to where data resides\n",
    "directory = \"./efe2b649e65e4966860ae2d935a0a1ec/data/contents/GEE_Landsat_GSL\"\n",
    "\n",
    "#Get a glob of the files we want\n",
    "glob_landsat_5 = sorted(glob.glob(f'{directory}/*5.csv'))\n",
    "glob_landsat_8 = sorted(glob.glob(f'{directory}/*8.csv'))\n",
    "\n",
    "#Display globs to make sure we have files\n",
    "display(glob_landsat_5)\n",
    "print(' ')\n",
    "display(glob_landsat_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ad9a6-65f7-4cbd-93d1-b81b286bbc93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> NOTE </b>: After loading in the GEE LANDSAT 5 and 8 water pixel CSV files, we realized that there were two LANDSAT images captured over overlapping portions of the Salt Lake domain each day for the dates that imagery is available. Therefore, these CSV files contain two water pixel values per day, which is a representation of the two LANDSAT images captured over overlapping portions of the Salt Lake in a day. In the code cell below, the reader will notice that the two daily overlapping LANDSAT water pixel values are summed and the resulting value is subtracted by a constant of 908030 to attain a sinlge water pixel value for each day LANDSAT 5 and 8 imagery was availble for the period of interest (1984-2022). This constant represents the average water pixel overlap between the two daily LANDSAT images, which computed manually in ArcGIS by the team. After this processing step was completed, two additional processing steps were peformed to attain a single yearly maximum LANDSAT water pixel count, which is plotted in the subsequent code cell. First, all daily water pixel values below a threshold a constant value of 5310 were dropped. This threshold was determined by the group. Second, after dropping daily water pixel counts below the defined minimum threshold, all daily water pixel counts were resampled into groups based on the \"Year Start (YS)\" sampling frequency and then the maximum value within each yearly group was selected. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42694947-c44e-46bf-9bdd-9fa548580c5c",
   "metadata": {
    "id": "42694947-c44e-46bf-9bdd-9fa548580c5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read both landsat csv files into pandas\n",
    "#We make sure to use the first column as the index, since it represents time information\n",
    "#The pixel values are read in as strings by default, so we specificy that the values are sperated by commas for the thousands place.\n",
    "#This results in the values being read in as floats in the proper format\n",
    "df_landsat_5   = pd.read_csv(glob_landsat_5[0], index_col=0, parse_dates=True, thousands=',')\n",
    "df_landsat_8   = pd.read_csv(glob_landsat_8[0], index_col=0, parse_dates=True, thousands=',')\n",
    "\n",
    "#Concatenate the dataframes toegther so we can work on a single file\n",
    "df_landsat_all = pd.concat([df_landsat_5, df_landsat_8], ignore_index=False)\n",
    "\n",
    "#Resample and sum the concatenated landsat DataFrame so we have a single value for each day that we have a value\n",
    "#We also subtract an average pixel value of 908030 from the daily summed data.\n",
    "#This value represents an estimated water pixel value that represents the overlap between the two landsat pixel values that are in the csv files for a single day\n",
    "df_landsat_all_day_sum = df_landsat_all.resample('D').sum() - 908030\n",
    "\n",
    "#After we have summed the data over a single day, we drop all values that are below a threshold value (5310 in this case as this was determined to be the value below the data did not make sense\n",
    "df_landsat_all_day_sum_masked = df_landsat_all_day_sum.mask(df_landsat_all_day_sum < 5310).dropna().resample('YS').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8997506-65cc-4475-9501-0caec19e90c2",
   "metadata": {},
   "source": [
    "Now we plot the LANDSAT CSV file data as a timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c29f8-2061-45cc-8540-5347b6465a06",
   "metadata": {
    "id": "218c29f8-2061-45cc-8540-5347b6465a06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "#Define figure and axis for plotting\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\n",
    "#----------------------------------------------------------------------------------------\n",
    "#Define font dictionaries for plotting\n",
    "fontdict_legend_labels   = {'size': 12, 'weight': 'bold'}\n",
    "fontdict_tick_labels     = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "fontdict_axis_labels     = {'fontsize': 14, 'fontweight': 'bold'}\n",
    "fontdict_title_labels    = {'fontsize': 14, 'fontweight': 'bold'}\n",
    "fontdict_text_color_bar  = {'fontsize': 15, 'fontweight': 'normal'}\n",
    "fontdict_text_annotation = {'fontsize': 12, 'fontweight': 'normal'}\n",
    "#----------------------------------------------------------------------------------------\n",
    "#x-axis customizations\n",
    "ax_xticks      = pd.date_range('1984', '2022', freq='YS').values\n",
    "ax_xticklabels = [str(value).zfill(2) for value in np.concatenate((np.arange(84,99+1,1), np.arange(0,22+1,1)))]\n",
    "ax.set_xlim([ax_xticks[0],ax_xticks[-1]])\n",
    "ax.set_xticks(ax_xticks)\n",
    "ax.set_xticklabels(ax_xticklabels, fontdict=fontdict_tick_labels)\n",
    "ax.set_xlabel('Calendar Year, starting from 1984 to 2022', fontdict=fontdict_axis_labels)\n",
    "\n",
    "#y-axis customizations\n",
    "ax_ymin = 3*10**6\n",
    "ax_ymax = 7*10**6\n",
    "ax_ystep = 0.5*10**6\n",
    "ax_yticks = np.arange(ax_ymin, ax_ymax+ax_ystep, ax_ystep)\n",
    "ax_yticklabels = ax_yticks\n",
    "ax.set_yticks(ax_yticks)\n",
    "ax.set_yticklabels(ax_yticklabels, fontdict=fontdict_tick_labels)\n",
    "y_formatter = ScalarFormatter(useMathText=True)\n",
    "y_formatter.set_powerlimits((0, 0))\n",
    "ax.yaxis.set_major_formatter(y_formatter)\n",
    "ax.set_ylabel('Maximum Yearly Water Pixel Count', fontdict=fontdict_axis_labels)\n",
    "\n",
    "#Set title\n",
    "ax.set_title('Estimated Maximum Yearly Water Pixel Count from Landsat Over the Great Salt Lake', fontdict=fontdict_title_labels)\n",
    "\n",
    "#Plot a horizontal line that spands the entire y-axis for each of the x_ticks we have\n",
    "for x_tick in ax_xticks:\n",
    "    ax.axvline(x=x_tick, ymin=0, ymax=1, color='black', linestyle='--', linewidth=0.5, zorder=2)\n",
    "\n",
    "#Plot a horizontal lines that spans the entire x-axis for each of the y_ticks we have\n",
    "for y_tick in ax_yticks:\n",
    "    ax.axhline(y=y_tick, xmin=0, xmax=1, color='black', linestyle='--', linewidth=0.5, zorder=2)\n",
    "\n",
    "#Plot USGS observation and corresponding NWM gridpoint for each river\n",
    "ax.plot(df_landsat_all_day_sum_masked.index.values, df_landsat_all_day_sum_masked.values, color='blue', linestyle='-', linewidth=2, zorder=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdc594-1af1-4a7f-a42a-f758085eb7dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Delving Deeper: By following the LANDSAT water pixel processing steps described above, we arrive at the following plot. Although there are limitations to the methodology outlined in dealing with overlapping daily LANDSAT images, in that the average water pixel overlap constant value may be either an over or underestimation of the actual overlap shown in all images, this plot is a first look at how the yearly maximum water levels in the Great Salt Lake have generally declined over the period of 1984-2022. Note that the gap in 2012 is due to the transition of LANDSAT 5 to LANDSAT 8 data. The intermediate LANDSAT data that was supposed to cover the 2012 year was determined by the team to not be of suitable quality for this anaylsis. </b> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82dfc0-1515-41e2-a34b-2bf569184a84",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Geoethical Considerations \n",
    "[Back to the table of content](#Table-of-Content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888de095-4ad4-4fd0-bcc1-f26b7c5dcdff",
   "metadata": {},
   "source": [
    "\n",
    "To implement **FAIR** (**F**indability, **A**ccessibility, **I**nteroperability, and **R**euse of digital assets) data practices,  data and links to data essential for the successful completion of the hydrology project analyses are available on a [Hydroshare repository](https://www.hydroshare.org/resource/8e2fa6f38ce142e09fa1eb999cd5f248/).  Also, results are shared at this [HydroShare resource](https://www.hydroshare.org/resource/efe2b649e65e4966860ae2d935a0a1ec/). HydroShare is the Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI)'s web based hydrologic information system for users to share and publish data and models in a variety of flexible formats, and to make this information available in a citable, shareable, and discoverable manner. \n",
    "\n",
    "We also want to note that the reusability of this notebook is dependent on having access to the I-GUIDE platform. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iguide",
   "language": "python",
   "name": "iguide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
