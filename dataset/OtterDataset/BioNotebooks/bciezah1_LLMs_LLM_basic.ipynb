{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready loading data\n",
      "Python version: 3.9.19 (main, May  6 2024, 20:12:36) [MSC v.1916 64 bit (AMD64)]\n",
      "Conda environment: base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset \n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig \n",
    "from transformers import pipeline\n",
    "print('ready loading data')\n",
    "\n",
    "# Check Python version\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Check Conda environment\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV')\n",
    "print(\"Conda environment:\", conda_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers numpy tensorflow\n",
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Natural Language Processing using Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Once upon a time, I thought I was going to read an article called 'How to Grow a Small Garden' by a retired, self-deprecating, professional gardener and, now, I have a few recommendations.\\n\\n1)\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "# generation pipeline\n",
    "generator = pipeline('text-generation',model='gpt2')\n",
    "\n",
    "# example of text generation\n",
    "result=generator(\"Once upon a time,\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York city is famous for its high-speed-oriented cycling projects (CBOs), but there's still something to be said for a city whose urban population is now only seven times the size of Denver, Texas, which is still much smaller.\n",
      "\n",
      "And unlike Detroit or Denver, New York has been able to generate revenue from public transportation. New York also started developing public transit as well as adding bike lanes at the city's bus stops—but the city's lack of funding has made it even less effective in keeping people moving.\n",
      "\n",
      "\"I really want to see a world in which people are able to get around, if they are at all comfortable, the city,\" said Mayor Michael Bloomberg.\n",
      "\n",
      "Bloomberg said his department aims\n"
     ]
    }
   ],
   "source": [
    "llm =  pipeline('text-generation')\n",
    "prompt = \"New York city is famous for\"\n",
    "outputs = llm(prompt, max_length=150)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9950636029243469}]\n"
     ]
    }
   ],
   "source": [
    "# loading pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# creating sentiment analysis\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "# esxample of sentiment analysis\n",
    "result =  classifier(\"I love to programm in Python and R!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\bciez\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_classifier = pipeline(\"text-classification\")\n",
    "\n",
    "outputs = sentiment_classifier(\"\"\" Dear Seller, I got very impressed with the fast devlivery and careful packaging of my order\"\"\")\n",
    "\n",
    "print(outputs[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9960016012191772, 'start': 47, 'end': 55, 'answer': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# create a q-a piepeline\n",
    "qa = pipeline('question-answering')\n",
    "\n",
    "# giving some context\n",
    "context = \"Columbia University is a university located in New York\"\n",
    "question = 'Where is located Columbia University?'\n",
    "\n",
    "# get the answer\n",
    "result = qa(question=question,context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\bciez\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computational biology and machine learning\n"
     ]
    }
   ],
   "source": [
    "llm =  pipeline(\"question-answering\")\n",
    "context = \"My name is Basilio and I like computational biology and machine learning\"\n",
    "question=\"What Basilio is talking about?\"\n",
    "outputs = llm(question=question,context=context)\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alzheimer’s disease is highly heritable and characterized by amyloid plaques and tau tangles in the brain. The aim of this study was to investigate the association between genetic predisposition, Aβ misfolding in blood plasma, a unique marker of Alzheimer\n"
     ]
    }
   ],
   "source": [
    "llm = pipeline(\"summarization\",model=\"facebook/bart-large-cnn\")\n",
    "long_text = \"Alzheimer’s disease is highly heritable and characterized by amyloid plaques and tau tangles in the brain. The aim of this study was to investigate the association between genetic predisposition, Aβ misfolding in blood plasma, a unique marker of Alzheimer associated neuropathological changes, and Alzheimer’s disease occurrence within 14 years. Witin a German community-based cohort, two polygenic risk scores (clinical Alzheimer’s disease and Aβ42 based) were calculated, APOE genotype was determined, and Aβ misfolding in blood plasma was measured by immuno\u0002infrared sensor in 59 participants diagnosed with Alzheimer’s disease during 14 years of follow-up and 581 participants without dementia diagnosis.\"\n",
    "outputs = llm(long_text,max_length=60,clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5. Language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me llamo Basilio y me gusta la biología computacional y el aprendizaje automático.\n"
     ]
    }
   ],
   "source": [
    "llm =pipeline(\"translation_en_to_es\",model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "text = \"My name is Basilio and I like computational biology and machine learning\"\n",
    "outputs = llm(text,clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6. Replies to customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had\n",
      "\n",
      "Hotel reponse to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us. My wife and I loved our stay on the Riverview, and I felt like it was the only time that we had met. The service was very professional, excellent quality of care, and the bar was extremely clean and friendly. And no matter what you order: we always get our drinks delivered. We hope that you return to Riverview for the\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for text generation using the gpt2 model\n",
    "generator = pipeline('text-generation',model='gpt2',truncation=True)\n",
    "\n",
    "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had\"\n",
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n",
    "# Build the prompt for the text generation LLM\n",
    "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
    "\n",
    "# Pass the prompt to the model pipeline\n",
    "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id)\n",
    "\n",
    "# Print the augmented sequence generated by the model\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
