{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9603a741",
   "metadata": {},
   "source": [
    "# Debugging Week 4 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d7098",
   "metadata": {},
   "source": [
    "Last week, you might recall we tried to sort your classmates into some groups, but something went wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17cf857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Half:  [('Hannah', 3), ('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Second Half:  [('Zoe', 3)]\n"
     ]
    }
   ],
   "source": [
    "#This is a list of tuples in the form (name, year)\n",
    "students = [('Hannah', 3), ('Julia', 3), ('Lily', 1), ('Karina',2), ('Diana',2),('Elise', 3), ('Zoe',3), ('Keya',2)]\n",
    "\n",
    "#Students to stand on the left side of the room\n",
    "first_half_alphabet = []\n",
    "#Students to stand on the right side of the room \n",
    "second_half_alphabet = []\n",
    "\n",
    "#Sorting the students into first and second half of alphabet\n",
    "for name, year in students:\n",
    "    first_letter = name[0]\n",
    "    if first_letter < 'M': #Earlier letters have smaller numbers assinged in Python, but caps makes a diffence!\n",
    "        first_half_alphabet.append((name, year))\n",
    "    else: \n",
    "        second_half_alphabet.append((name, year))\n",
    "\n",
    "#Print out the results\n",
    "print('First Half: ',  first_half_alphabet)\n",
    "print('Second Half: ', second_half_alphabet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12660012",
   "metadata": {},
   "source": [
    "Then we flew too close to the sun and tried to put all the **upperclassmen** in the middle of the room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a626e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Half Alphabet:  [('Hannah', 3), ('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Looking at:  Hannah 3\n",
      "Removing:  ('Hannah', 3)\n",
      "First Half Alphabet:  [('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Looking at:  Lily 1\n",
      "First Half Alphabet:  [('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Looking at:  Karina 2\n",
      "First Half Alphabet:  [('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Looking at:  Diana 2\n",
      "First Half Alphabet:  [('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Elise', 3), ('Keya', 2)]\n",
      "Looking at:  Elise 3\n",
      "Removing:  ('Elise', 3)\n",
      "Final first half alphabet:  [('Julia', 3), ('Lily', 1), ('Karina', 2), ('Diana', 2), ('Keya', 2)]\n"
     ]
    }
   ],
   "source": [
    "upperclassmen = []\n",
    "\n",
    "for name, year in first_half_alphabet: \n",
    "    print(\"First Half Alphabet: \", first_half_alphabet)\n",
    "    print(\"Looking at: \", name, year)\n",
    "    if year >= 3:\n",
    "        print(\"Removing: \", (name,year))\n",
    "        upperclassmen.append((name,year))\n",
    "        first_half_alphabet.remove((name, year))\n",
    "        \n",
    "#Check results\n",
    "print(\"Final first half alphabet: \", first_half_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28bbc3",
   "metadata": {},
   "source": [
    "But when we went to go check our results, poor Julia slipped through the cracks! Let's go back and try and debug this with some print statements. **Don't forget to rerun the code block before so that first_half_alphabet still has Hannah and Elise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445af48f",
   "metadata": {},
   "source": [
    "### Talk to the person next to you: How could you change the list of students to avoid this error? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eaf748",
   "metadata": {},
   "source": [
    "## The Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776b5dc",
   "metadata": {},
   "source": [
    "OK, so it turns out trying to remove elements from a Python list while we're iterating through it is a bad idea. Let's take a step back and try and think of another approach... **Don't forget to reset our lists again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c4fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Half:  [('Lily', 1), ('Karina', 2), ('Diana', 2), ('Keya', 2)]\n",
      "Second Half:  []\n",
      "Upperclassmen:  [('Hannah', 3), ('Julia', 3), ('Elise', 3), ('Zoe', 3)]\n"
     ]
    }
   ],
   "source": [
    "#Let's just put them in seperate lists instead\n",
    "new_first_half_alphabet = []\n",
    "new_second_half_alphabet = []\n",
    "upperclassmen = []\n",
    "\n",
    "for name, year in first_half_alphabet:\n",
    "    if year >= 3: #Add them to the upperclassmen list\n",
    "        upperclassmen.append((name,year))\n",
    "    else: #Or add them to our new first half of alphabet list\n",
    "        new_first_half_alphabet.append((name,year))\n",
    "        \n",
    "for name, year in second_half_alphabet:\n",
    "    if year >= 3:\n",
    "        upperclassmen.append((name,year))\n",
    "    else:\n",
    "        new_second_half_alphabet.append((name,year))\n",
    "        \n",
    "first_half_alphabet = new_first_half_alphabet #Now overwrite our firs half of alphabet\n",
    "second_half_alphabet = new_second_half_alphabet #And the second half\n",
    "\n",
    "#Let's see how we did\n",
    "print('First Half: ',  first_half_alphabet)\n",
    "print('Second Half: ', second_half_alphabet)\n",
    "print('Upperclassmen: ', upperclassmen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b439a8",
   "metadata": {},
   "source": [
    "Voilà! Julia is in the right place. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231a884",
   "metadata": {},
   "source": [
    "# Week 5: Finishing the TTR Experiment\n",
    "\n",
    "In this week, we put together everything we've learned so far this semester to tackle the full TTR experiment.\n",
    "\n",
    "## Part 1: Removing Punctutation with Regular Expressions\n",
    "\n",
    "Here we learn about regular expressions, to help us with the super-important task of removing all punctuation from our texts.\n",
    "\n",
    "## Part 2: Iterating through Files in a Folder\n",
    "\n",
    "Here we use the `Path()` function to help load a whole folder of texts and analyze them one-by-one.\n",
    "\n",
    "## Part 3: Automatically Determining Sample Size and Producing Standardized Results\n",
    "\n",
    "Here we learn how to determine the total length of the shortest text, and then calculate the TTR only of a sample of the full text.\n",
    "\n",
    "\n",
    "## Part 4: Writing CSV files\n",
    "\n",
    "Here with use `open()` and `.write()` to get Python to spit out spreadsheet files with our results all ready to use!\n",
    "\n",
    "\n",
    "## Links\n",
    "\n",
    "* Melanie Walsh discusses regular expressions in [her chapter on web scraping](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/03-Web-Scraping-Part2.html?highlight=regular%20expressions#regular-expressions). Our discussion is probably a bit gentler, but this is here if you're looking for more explanations.\n",
    "* Walsh also covers opening and saving files in [her chapter on files and character encoding](https://melaniewalsh.github.io/Intro-Cultural-Analytics/02-Python/07-Files-Character-Encoding.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31077dc",
   "metadata": {},
   "source": [
    "# Taking Stock on the TTR Task\n",
    "\n",
    "**Read aloud**: We now know how to do most of what we need to write code that will quickly and accurately calculate non-standardized and standardized TTRs for a folder full of text files.\n",
    "\n",
    "During Week Three, we learned how to\n",
    "* Load a file\n",
    "* Split it into words\n",
    "* Count the number of words (tokens)\n",
    "\n",
    "In Week Four, we used conditionals and iteration to:\n",
    "* Count unique words (types)\n",
    "\n",
    "Today, we will learn how to:\n",
    "* Remove punctuation for more accurate type counts\n",
    "* Iterate through a folder of text files\n",
    "* Automatically determine our sample size (the total length of the shortest text)\n",
    "* Calculate the standardized TTR for each text file in our folder\n",
    "* Output our results in the form of CSV spreadsheet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bb994",
   "metadata": {},
   "source": [
    "# 1. Removing Punctuation with Regular Expressions\n",
    "\n",
    "**Regular expressions are a means of describing a type of string based on the *characters* within that string**\n",
    "\n",
    "As a class, let's try and might come up with a few linguistic moments that come up in literature that could and couldn't be described using a regular expression:\n",
    "\n",
    "### Could be described with Regex\n",
    "* Sentences with 'Sherlock' in them\n",
    "* Words with the first letter capitalized\n",
    "* Dialogue\n",
    "* Instances of contractions n't\n",
    "\n",
    "### Couldn't be described with Regex\n",
    "* Metaphoric language\n",
    "* Verbs\n",
    "* Passive voice\n",
    "* Narrative climax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5529b2",
   "metadata": {},
   "source": [
    "Regular expressions — also known by their cooler *nom de guerre* **Regex** — are a whole language of their own. And don't let that term \"regular\" fool you — they are wild and charismatic and (for humanities people, anyway) **extremely cool**.\n",
    "\n",
    "Note that regular expressions aren't only used in Python. They can be used in all programming languages, and you can even use them in good text editors like Sublime Text. Trust me, they come in extremely handy once you get a handle on them!\n",
    "\n",
    "Imagine them as a super-sophisticated version of a find-and-replace command. We've already met one of those — the `string.replace()` method. Regular expressions go way, way further. We're only going to explore a tiny fraction of what they can do...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d702e3",
   "metadata": {},
   "source": [
    "## Um, regex sounds kinda complicated. Can't we stick with string.replace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e31ee5",
   "metadata": {},
   "source": [
    "Let's explore a scenario where `string.replace()` doesn't exactly get us what we want, and we need something more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c026f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4 = open(\"sign-of-four.txt\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d3e58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Which is it to-day?” I asked,—“morphine or cocaine?”\\n\\nHe raised his eyes languidly from the old black-letter volume which he had opened. “It is cocaine,” he said,—“a seven-per-cent. solution. Would you care to try it?”\\n\\n“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4[1475:1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da9e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Which is it to-day?” I asked,—“morphine or cocaine?”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. “It is cocaine,” he said,—“a seven-per-cent. solution. Would you care to try it?”\n",
      "\n",
      "“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03f8d6",
   "metadata": {},
   "source": [
    "**Let's say we would like to do three things to clean up this string**:\n",
    "* Replace the too-risqué word \"cocaine\" with \"strawberry soda\"\n",
    "* Remove all punctuation\n",
    "* Extract only the dialogue from this exchange, storing each piece of dialogue as an item in a list\n",
    "\n",
    "Our old friend `string.replace()` can easily do the first, do the second with a lot of effort, and not do the latter at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99258478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Which is it to-day?” I asked,—“morphine or stawberry soda?”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. “It is stawberry soda,” he said,—“a seven-per-cent. solution. Would you care to try it?”\n",
      "\n",
      "“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"cocaine\", \"stawberry soda\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf95b45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Which is it to-day” I asked,—“morphine or stawberry soda”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. “It is stawberry soda,” he said,—“a seven-per-cent. solution. Would you care to try it”\n",
      "\n",
      "“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"?\", \"\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a6c71",
   "metadata": {},
   "source": [
    "As you can see, `string.replace()` can remove punctuation... but we need to specify each piece of punctuation one-by-one, which is rather laborious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f539aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is it to-day” I asked,—morphine or stawberry soda”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. It is stawberry soda,” he said,—a seven-per-cent. solution. Would you care to try it”\n",
      "\n",
      "No, indeed,” I answered, brusquely. My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"“\", \"\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7bba7",
   "metadata": {},
   "source": [
    "### Hmm, this is getting tedious, and I still don't have the dialogue...\n",
    "\n",
    "This is where **regular expressions** come in..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259ac45",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Python has a bunch of built-in functionality that you need to explicitly call on to \"activate.\" Think of it as a resources issues. Not everyone cares about fancy find-and-replace functions (boring people, to be specific) — so they don't want their Python programs loaded with lots of unnecessary commands they won't call on. There are tons of domain-specific commands (astronomy commands, economics commands, biology commands) that are probably cool but that we don't intend to use — and we don't want them bogging down our stuff, either.\n",
    "\n",
    "Due to our exquisite taste, we need regular expressions. \n",
    "\n",
    "This requires us to **load a Python library**: a set of commands that are lying politely in wait, waiting for their number to be called, ready to slide down the fireman's pole from the realm of mere potentiality into the world of the actual. The Library we seek is **named `re`.**\n",
    "\n",
    "![Fireman's pole](firemanspole.gif)\n",
    "\n",
    "The command below calls `re` down the firman's pole. We can now use Regex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd80789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20313c",
   "metadata": {},
   "source": [
    "`re` has a bunch of different **functions** bundled into it, all of whose names begin with `re` then `.` and then the same of the command. \n",
    "\n",
    "We're going to start with `re.sub()` which does more or less what `string.replace()` does, though its syntax is a bit different.\n",
    "\n",
    "Whereas `string.replace()` takes who arguments `(\"the string to replace\", \"the string to replace it with\")`, `re.sub()` takes three:\n",
    "* `re.sub(\"the string to replace\", \"the string to replace it with\", the_variable_containing_the_text)`\n",
    "\n",
    "Here's how we would do our first two tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a156810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Which is it to-day?” I asked,—“morphine or cocaine?”\\n\\nHe raised his eyes languidly from the old black-letter volume which he had opened. “It is cocaine,” he said,—“a seven-per-cent. solution. Would you care to try it?”\\n\\n“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sot4[1475:1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f93719d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is it to-day” I asked,—morphine or stawberry soda”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. It is stawberry soda,” he said,—a seven-per-cent. solution. Would you care to try it”\n",
      "\n",
      "No, indeed,” I answered, brusquely. My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = re.sub(\"cocaine\", \"strawberry soda\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0ff9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is it to-day” I asked,—morphine or stawberry soda”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. It is stawberry soda,” he said,—a seven-per-cent. solution. Would you care to try it”\n",
      "\n",
      "No, indeed,” I answered, brusquely. My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = re.sub(\"“\", \"\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342944e3",
   "metadata": {},
   "source": [
    "### Re.sub is not a mutating function\n",
    "\n",
    "Note that **the `re.sub()` function is NOT a mutating function** — so if you want to store its output, you need to explicitly stick it into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6188f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "strawberry_soda_exchange = re.sub(\"cocaine\", \"strawberry soda\", cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94bd91ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is it to-day” I asked,—morphine or stawberry soda”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. It is stawberry soda,” he said,—a seven-per-cent. solution. Would you care to try it”\n",
      "\n",
      "No, indeed,” I answered, brusquely. My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "print(strawberry_soda_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e837a5",
   "metadata": {},
   "source": [
    "## Regex 👸🤴\n",
    "\n",
    "### We won't gove over this in lecture, but check out the website for more examples of Regex power\n",
    "\n",
    "**Regex — androgynous, post-gender QueenKing 👸🤴, Regina/Rex — is capable of doing so much more than this!!**\n",
    "\n",
    "Regex is flush with power.\n",
    "\n",
    "Let's explore some of what Regex can do [at the website Regular Expressions 101](https://regex101.com).\n",
    "\n",
    "**Don't worry: you don't need to memorize all of this. The only thing you really need to know is how to use Regex to remove punctuation for our TTR task. But we thought you would enjoy seeing a demonstration of Regex's immense power!**\n",
    "\n",
    "We'll try the following:\n",
    "\n",
    "* `a`: the character `a`\n",
    "* `[aeiou]`: any one of `a`, `e`, `i`, *or* `u` (the square brackets `[]` mean \"any one of what's between me\")\n",
    "* `[aeiouAEIOU]`: same as above, but adding capital letters\n",
    "* `[a-z]`: any character in the **range** `a-z` — so, any lowercase letter\n",
    "* `[a-zA-Z]`: any lowercase or uppercase letter — so, any letter\n",
    "* `[^a-z]`: anything that is **not** `a-z` (the `^` means \"not\")\n",
    "\n",
    "Then we'll meet these fellows:\n",
    "* `\\w`: any letter\n",
    "* `\\d`: any number\n",
    "* `\\s`: any whitespace\n",
    "* `\\W`: anything *not* a letter\n",
    "* `\\D`: anything *not* a number\n",
    "* `\\S`: anything *not* whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b42d58",
   "metadata": {},
   "source": [
    "## Removing Punctuation with Regex\n",
    "\n",
    "Our adventures at Regular Expressions 101 will have showed us how we can quite simply remove \"punctuation,\" which we will define as: \n",
    "- **any character that isn't a alphanumeric: lowercase letter a-z, an uppercase letter A-Z, or a number 0-9**\n",
    "\n",
    "In the language of Regex, you would express that same definition as follows: \n",
    "- **[^a-zA-Z0-9]**\n",
    "\n",
    "We want to grab all of those and replace them with **spaces** since replacing the `-` in a word like `seven-per-cent` would turn it into a non-word like `sevenpercent` rather than three separate words, `seven per cent`, which our `string.split()` method will be able to easily \"tokenize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4786377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Which is it to-day?” I asked,—“morphine or cocaine?”\n",
      "\n",
      "He raised his eyes languidly from the old black-letter volume which he had opened. “It is cocaine,” he said,—“a seven-per-cent. solution. Would you care to try it?”\n",
      "\n",
      "“No, indeed,” I answered, brusquely. “My constitution has not got over the Afghan campaign yet. I cannot afford to throw any extra strain upon it.”\n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6787ad10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Which is it to day   I asked   morphine or cocaine    He raised his eyes languidly from the old black letter volume which he had opened   It is cocaine   he said   a seven per cent  solution  Would you care to try it     No  indeed   I answered  brusquely   My constitution has not got over the Afghan campaign yet  I cannot afford to throw any extra strain upon it  \n"
     ]
    }
   ],
   "source": [
    "cocaine_exchange = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb61039",
   "metadata": {},
   "source": [
    "**That does the trick!** \n",
    "\n",
    "So now the question becomes: **WHEN** should we remove punctuation? When `sot4` is a string, or after we've used `.split()` to split it into words?\n",
    "\n",
    "Let's try it both ways, first removing punctuation *after* `.split()`ting, and then removing it *before*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbbd59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "ce_words = cocaine_exchange.split() # First we split the text into words\n",
    "\n",
    "ce_unique_words = []\n",
    "\n",
    "for word in ce_words:\n",
    "    word = word.lower()\n",
    "    word = re.sub(\"[^a-zA-Z0-9]\", \" \", word) # Then we remove punctuation in the for loop that counts unique words\n",
    "    if word not in ce_unique_words:\n",
    "        ce_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "498da3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "[' which', 'is', 'it', 'to day  ', 'i', 'asked   morphine', 'or', 'cocaine  ', 'he', 'raised', 'his', 'eyes', 'languidly', 'from', 'the', 'old', 'black letter', 'volume', 'which', 'had', 'opened ', ' it', 'said   a', 'seven per cent ', 'solution ', 'would', 'you', 'care', 'to', 'try', 'it  ', ' no ', 'indeed  ', 'answered ', 'brusquely ', ' my', 'constitution', 'has', 'not', 'got', 'over', 'afghan', 'campaign', 'yet ', 'cannot', 'afford', 'throw', 'any', 'extra', 'strain', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print(len(ce_unique_words))\n",
    "print(ce_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de0c334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "cocaine_exchange_nopunct = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange) # First we remove punctuation\n",
    "\n",
    "cenp_words = cocaine_exchange_nopunct.split() # Then we split the text into words\n",
    "\n",
    "cenp_unique_words = []\n",
    "\n",
    "for word in cenp_words: # By the time we enter this for loop, the punctuation is already gone\n",
    "    word = word.lower()\n",
    "    if word not in cenp_unique_words:\n",
    "        cenp_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64aad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "['which', 'is', 'it', 'to', 'day', 'i', 'asked', 'morphine', 'or', 'cocaine', 'he', 'raised', 'his', 'eyes', 'languidly', 'from', 'the', 'old', 'black', 'letter', 'volume', 'had', 'opened', 'said', 'a', 'seven', 'per', 'cent', 'solution', 'would', 'you', 'care', 'try', 'no', 'indeed', 'answered', 'brusquely', 'my', 'constitution', 'has', 'not', 'got', 'over', 'afghan', 'campaign', 'yet', 'cannot', 'afford', 'throw', 'any', 'extra', 'strain', 'upon']\n"
     ]
    }
   ],
   "source": [
    "print(len(cenp_unique_words))\n",
    "print(cenp_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae93381e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(ce_unique_words) / len(ce_words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "185c5b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.3030303030303"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(cenp_unique_words) / len(cenp_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66c594",
   "metadata": {},
   "source": [
    "So **we want to remove puncutation *before* using `.split()`,** because otherwise we'll end up with a bunch of funky \"unique words\" with spaces where their punctuation once was. It doesn't solve the problem we initially had. \n",
    "\n",
    "If we remove punctuation before tokenizing with `.split()` we get what we want, because `.split()` splits whenever it meets any number of consecutive whitespace characters. So it will easily turn `asked    morphine` — with its lengthy separating whitespace — into two tokens, `asked` and `morphine`.\n",
    "\n",
    "### Now, let's do this for real with our old friend *The Sign of the Four*.\n",
    "\n",
    "Let's start with how we did it last class, before we knew how to remove punctuation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16e3bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old method without removing punctuation\n",
    "\n",
    "sot4_words = sot4.split()\n",
    "\n",
    "sot4_unique_words = []\n",
    "\n",
    "for word in sot4_words:\n",
    "    word = word.lower()\n",
    "    if word not in sot4_unique_words:\n",
    "        sot4_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5754bfc",
   "metadata": {},
   "source": [
    "... And then do it our fancy new way, using Regex to remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77b790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New method that removes punctuation\n",
    "\n",
    "sot4np = re.sub(\"[^a-zA-Z0-9]\", \" \", sot4) # The variable names here, \"np\" signals \"no punctuation\"\n",
    "\n",
    "sot4np_words = sot4np.split()\n",
    "\n",
    "sot4np_unique_words = []\n",
    "\n",
    "for word in sot4np_words:\n",
    "    word = word.lower()\n",
    "    if word not in sot4np_unique_words:\n",
    "        sot4np_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44e14a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter I The Science of Deduction\\n\\n\\nSherlock Holmes took his bottle from the corner of the mantel-piece and his hypodermic syringe from its neat morocco case. With his long, white, nervous fingers he adjusted the delicate needle, and rolled back his left shirt-cuff. For some little time his eyes rested thoughtfully upon the sinewy forearm and wrist all dotted and scarred with innumerable puncture-marks. Finally he thrust the sharp point home, pressed down the tiny piston, and sank back into the'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "897af6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter I The Science of Deduction   Sherlock Holmes took his bottle from the corner of the mantel piece and his hypodermic syringe from its neat morocco case  With his long  white  nervous fingers he adjusted the delicate needle  and rolled back his left shirt cuff  For some little time his eyes rested thoughtfully upon the sinewy forearm and wrist all dotted and scarred with innumerable puncture marks  Finally he thrust the sharp point home  pressed down the tiny piston  and sank back into the'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4np[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b403019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter',\n",
       " 'I',\n",
       " 'The',\n",
       " 'Science',\n",
       " 'of',\n",
       " 'Deduction',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'took',\n",
       " 'his',\n",
       " 'bottle',\n",
       " 'from',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mantel-piece',\n",
       " 'and',\n",
       " 'his',\n",
       " 'hypodermic']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87d84796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter',\n",
       " 'I',\n",
       " 'The',\n",
       " 'Science',\n",
       " 'of',\n",
       " 'Deduction',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'took',\n",
       " 'his',\n",
       " 'bottle',\n",
       " 'from',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mantel',\n",
       " 'piece',\n",
       " 'and',\n",
       " 'his']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4np_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6ad4d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1857,',\n",
       " '1871,',\n",
       " '1878',\n",
       " '1878,—nearly',\n",
       " '1882',\n",
       " '1882.”',\n",
       " '1882—an',\n",
       " '221_b_',\n",
       " '28th',\n",
       " '3',\n",
       " '3,',\n",
       " '340',\n",
       " '34th',\n",
       " '3rd',\n",
       " '4th',\n",
       " '7',\n",
       " '7.',\n",
       " '_a',\n",
       " '_a_,',\n",
       " '_au']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4_unique_words.sort()\n",
    "sot4_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b43ae061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1857',\n",
       " '1871',\n",
       " '1878',\n",
       " '1882',\n",
       " '221',\n",
       " '28th',\n",
       " '3',\n",
       " '340',\n",
       " '34th',\n",
       " '37',\n",
       " '3rd',\n",
       " '4th',\n",
       " '7',\n",
       " 'a',\n",
       " 'abdullah',\n",
       " 'abel',\n",
       " 'abhor',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'aborigines']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sot4np_unique_words.sort()\n",
    "sot4np_unique_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e26cf",
   "metadata": {},
   "source": [
    "The tokenization without punctuation works a lot better. \n",
    "\n",
    "### How much do you think this will affect the TTR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eced452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.240965536486677"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(sot4_unique_words) / len(sot4_words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c04a397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.279700483974066"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(sot4np_unique_words) / len(sot4np_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e966abe",
   "metadata": {},
   "source": [
    "## 👸🤴 Digression, Part I: Extracting Quotations with Regex\n",
    "\n",
    "We probably won't have time to cover this in lecture, but it's potentially fun for those of you who are interested.\n",
    "\n",
    "### Note: None of this Digression (Part 1 or Part 2) will be on the midterm or the exam. It is not something we require you to know. It's just for fun.\n",
    "\n",
    "For this task, we will need some cool new Regex characters:\n",
    "* `.`: \"any character except a newline\"\n",
    "\n",
    "And this \"quantifier\" which you add to a regular expression to signal\n",
    "* `*`: \"zero or more occurences of the thing immediately to my left\"\n",
    "\n",
    "So that the expression\n",
    "* `.*` means \"zero or more characters other than a newline\"\n",
    "\n",
    "To catch one-line quotations, one could try...\n",
    "* `\".*\"`: a `\"` character, followed by zero or more occurences of any character except a newline, followed by a `\"` character \n",
    "\n",
    "This actually won't work on *The Sign of the Four* — becuase it's from Project Gutenberg, and PG files use “curly quotes.” **Yes: `\"` and `“` and `”` are all actually different characters**!\n",
    "\n",
    "Note that we need to use “curly quotes” for Project Gutenberg files. And we got *The Sign of the Four* from PG. So we need:\n",
    "* `“.*”`: a `“` character, followed by zero or more occurences of any character except a newline, followed by a `”` character \n",
    "\n",
    "We need one added complexity: the searches are \"greedy\" (they grab as much as they can), so they sometimes add narration in between the opening and closing quotes. We can fix this by specifying that if you see a close-quote character, immediately stop. The regular expression we need is:\n",
    "\n",
    "* `“[^”]*”` — where the `[^”]` means \"any character except a close quote\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f673ba",
   "metadata": {},
   "source": [
    "The Regex command we want to use to grab all the quotations in `sot4` is `re.findall`, which takes two arguments:\n",
    "* The Regex pattern you want to find (expressed as a string, so surround it with `\"`s)\n",
    "* The string variable in which you want to look for this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"“[^”]*”\", sot4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd385601",
   "metadata": {},
   "source": [
    "As you can see, `re.findall` returns a `list` in which every match is provided as an item.\n",
    "\n",
    "That looks good, but it isn't stored anywhere, so let's grab that output and put it into a variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4_quotations = re.findall(\"“[^”]*”\", sot4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4_quotations[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7bcad",
   "metadata": {},
   "source": [
    "## 👸🤴 Digression, Part II: Creating a Literary Mashup\n",
    "\n",
    "### Note: Like the Part I of this Digression, this will not be on the midterm or the exam.\n",
    "\n",
    "Now, let's say you want to do something really fun with these quotations you just extracted... like, say, stick them into *Pride and Prejudice* so that all of Austen's dialogue is replaced with Conan Doyle's!\n",
    "\n",
    "Here's how I'm going to do it:\n",
    "* Load up P&P\n",
    "* Replace all the dialogue in P&P with the phrase \"QUOTE_HERE\" so that I know where to stick my replacement quotations.\n",
    "* Then iterate through my list of SOT4 quotations, popping them into P&P one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads P&P, a copy of which I have conveniently placed in the same folder as this notebook.\n",
    "\n",
    "pandp = open(\"pride_prejudice.txt\", encoding=\"utf-8\").read()\n",
    "\n",
    "print(pandp[470:773])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aedeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This replaces all dialogue in P&P with the phrase \"QUOTE_HERE\", \n",
    "# creating targets I can then replace one-by-one with the SOT4 quotations\n",
    "\n",
    "pride_of_the_four = re.sub(\"“[^”]*”\", \"QUOTE_HERE\", pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pride_of_the_four[470:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now I will iterate through the list of SOT4 quotations, using each one to replace one \"QUOTE_HERE\" in pride_of_the_four\n",
    "\n",
    "for quotation in sot4_quotations:\n",
    "    pride_of_the_four = re.sub(\"QUOTE.HERE\", quotation, pride_of_the_four, 1) # the \"1\" at the end of this line specifies to only make one replacement for each item in sot4_quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa35ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the amusing result...\n",
    "\n",
    "print(pride_of_the_four[470:778])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we will learn how to write files. But here's a little preview! This line saves our amazing new mashup novel as \"pride-of-the-four.txt\"\n",
    "\n",
    "open(\"pride-of-the-four.txt\", mode=\"w\", encoding=\"utf-8\").write(pride_of_the_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f908a",
   "metadata": {},
   "source": [
    "# 2. Iterating through Files in a Folder\n",
    "\n",
    "Okay, we now know how to remove punctuation, which is essential to our TTR task: we are now getting accurate token and type counts.\n",
    "\n",
    "We only need to be able to do three more things:\n",
    "* Learn how to automatically create a standardized sample size\n",
    "* Automate the loading of files so that we can can give Python a folder full of text files and let it do its thing, with no additional help from us\n",
    "* Have Python spit out some nice spreadsheet files for us: one with non-standardized values and one with standardized values.\n",
    "\n",
    "First, let's handle the folder-loading task.\n",
    "\n",
    "## The `Path` function\n",
    "\n",
    "### Sometimes in life, we need to read and write multiple text files...\n",
    "\n",
    "For this, we need to pull in another Python Library: `pathlib`, from which we are going to extract the function `Path`. We can coax it down the fireman's pole with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d2eefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5dc35",
   "metadata": {},
   "source": [
    "`Path` will help us look through a folder and find the... **pathways** that Python needs to find files we want it to look and calculate TTR values for. \n",
    "\n",
    "First, `Path` needs to know where we've stored our plain text files. We will let it know by passing it a string variable that contains the name of the folder we want it to look in. In this case (check your JupyterHubs!) we've put all the individual chapters of *The Sign of the Four* in a folder called `sot4chaps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79ae855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"sot4chaps\" # This variable can be named anything as long as is matches the variable name in the Path() command below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98f08e",
   "metadata": {},
   "source": [
    "The below command asks `Path` to look in a folder called `sot4chaps` (which it expects to find **in the same folder as the Jupyter Notebook we are currently using — which it is!**), and to print out the paths of **absolutely everything in that folder** (the `\"*\"` as the argument to the `.glob()` method is what instructs it to look for everything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8e40343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sot4chaps/firemanspole.gif\n",
      "sot4chaps/01 The Science of Deduction.txt\n",
      "sot4chaps/09 A Break in the Chain.txt\n",
      "sot4chaps/07 The Episode of the Barrel.txt\n",
      "sot4chaps/04 The Story of the Bald-Headed Man.txt\n",
      "sot4chaps/02 The Statement of the Case.txt\n",
      "sot4chaps/05 The Tragedy of Pondicherry Lodge.txt\n",
      "sot4chaps/06 Sherlock Holmes Gives a Demonstration.txt\n",
      "sot4chaps/08 The Baker Street Irregulars.txt\n",
      "sot4chaps/03 In Quest of a Solution.txt\n",
      "sot4chaps/11 The Great Agra Treasure.txt\n",
      "sot4chaps/10 The End of the Islander.txt\n",
      "sot4chaps/12 The Strange Story of Jonathan Small.txt\n"
     ]
    }
   ],
   "source": [
    "for file_path in Path(folder_path).glob(\"*\"):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ddc46a",
   "metadata": {},
   "source": [
    "### Hmm, there's a slight problem with this list\n",
    "\n",
    "Each of those things is a **file path**: a path or route that Python will need to follow — relative to the place from which it's receiving its commands; namely, this Jupyter Notebook — to get to the files we want it to analyze.\n",
    "\n",
    "As you can see, I have deviously inserted the dreaded `firemanspole.gif` into that folder. We do not want to calculate the TTR of `firemanspole.gif`!! Luckily we can tell `Path` to only look for particular kinds of files. In this case, we only want it to look at plain text files, which all end `.txt`. So we can replace the `*` in the above command (\"I want paths of everything in that folder\") with `*.txt` (\"I only want paths to the plain text files\").\n",
    "\n",
    "(NOTE: The `*` in the `Path().glob()` method means something different from the `*` in Regex. Such is life in the world of computer programming, where one must learn to speak multiple dialects to communicate...)\n",
    "\n",
    "### Let's try and just get our text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fc1a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sot4chaps/01 The Science of Deduction.txt\n",
      "sot4chaps/09 A Break in the Chain.txt\n",
      "sot4chaps/07 The Episode of the Barrel.txt\n",
      "sot4chaps/04 The Story of the Bald-Headed Man.txt\n",
      "sot4chaps/02 The Statement of the Case.txt\n",
      "sot4chaps/05 The Tragedy of Pondicherry Lodge.txt\n",
      "sot4chaps/06 Sherlock Holmes Gives a Demonstration.txt\n",
      "sot4chaps/08 The Baker Street Irregulars.txt\n",
      "sot4chaps/03 In Quest of a Solution.txt\n",
      "sot4chaps/11 The Great Agra Treasure.txt\n",
      "sot4chaps/10 The End of the Islander.txt\n",
      "sot4chaps/12 The Strange Story of Jonathan Small.txt\n"
     ]
    }
   ],
   "source": [
    "for file_path in Path(folder_path).glob('*.txt'):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78644b73",
   "metadata": {},
   "source": [
    "As you'll notice, the above list of files is not sorted, which is both aesthetically annoying and will also make it more difficult to interpret our results. (We can always sort things later using Microsoft Excel, etc., but we may as well do as much as we can right here in Python.)\n",
    "\n",
    "Thankfully, we can wrap the Python function `sorted()` around the `Path()` function, and it will open the files in an alphabetically sorted manner.\n",
    "\n",
    "### OK, but isn't this out of order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "491fe90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sot4chaps/01 The Science of Deduction.txt\n",
      "sot4chaps/02 The Statement of the Case.txt\n",
      "sot4chaps/03 In Quest of a Solution.txt\n",
      "sot4chaps/04 The Story of the Bald-Headed Man.txt\n",
      "sot4chaps/05 The Tragedy of Pondicherry Lodge.txt\n",
      "sot4chaps/06 Sherlock Holmes Gives a Demonstration.txt\n",
      "sot4chaps/07 The Episode of the Barrel.txt\n",
      "sot4chaps/08 The Baker Street Irregulars.txt\n",
      "sot4chaps/09 A Break in the Chain.txt\n",
      "sot4chaps/10 The End of the Islander.txt\n",
      "sot4chaps/11 The Great Agra Treasure.txt\n",
      "sot4chaps/12 The Strange Story of Jonathan Small.txt\n"
     ]
    }
   ],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fce68",
   "metadata": {},
   "source": [
    "Now, rather than just spewing out the file paths with the `print()` function... Let's actually **load** each of these files, and print out the first 100 characters of each, shall we?\n",
    "\n",
    "### Ah, there we go. Just to be sure if these are the right files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3464cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I The Science of Deduction\n",
      "\n",
      "\n",
      "Sherlock Holmes took his bottle from the corner of the mantel-p\n",
      "Chapter II The Statement of the Case\n",
      "\n",
      "\n",
      "Miss Morstan entered the room with a firm step and an outward\n",
      "Chapter III In Quest of a Solution\n",
      "\n",
      "\n",
      "It was half-past five before Holmes returned. He was bright, ea\n",
      "Chapter IV The Story of the Bald-Headed Man\n",
      "\n",
      "\n",
      "We followed the Indian down a sordid and common passag\n",
      "Chapter V The Tragedy of Pondicherry Lodge\n",
      "\n",
      "\n",
      "It was nearly eleven o’clock when we reached this final\n",
      "Chapter VI Sherlock Holmes Gives a Demonstration\n",
      "\n",
      "\n",
      "“Now, Watson,” said Holmes, rubbing his hands, “w\n",
      "Chapter VII The Episode of the Barrel\n",
      "\n",
      "\n",
      "The police had brought a cab with them, and in this I escort\n",
      "Chapter VIII The Baker Street Irregulars\n",
      "\n",
      "\n",
      "“What now?” I asked. “Toby has lost his character for inf\n",
      "Chapter IX A Break in the Chain\n",
      "\n",
      "\n",
      "It was late in the afternoon before I woke, strengthened and refre\n",
      "Chapter X The End of the Islander\n",
      "\n",
      "\n",
      "Our meal was a merry one. Holmes could talk exceedingly well whe\n",
      "Chapter XI The Great Agra Treasure\n",
      "\n",
      "\n",
      "Our captive sat in the cabin opposite to the iron box which he \n",
      "\n",
      "Chapter XII The Strange Story of Jonathan Small\n",
      "\n",
      "\n",
      "A very patient man was that inspector in the cab,\n"
     ]
    }
   ],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696ce37",
   "metadata": {},
   "source": [
    "### Alright, now let's put it all together and calculate the TTRs for each chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbaa054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01 The Science of Deduction' has 1042 types, 2986 tokens, and a TTR of 34.89618218352311\n",
      "'02 The Statement of the Case' has 714 types, 1867 tokens, and a TTR of 38.24317086234601\n",
      "'03 In Quest of a Solution' has 730 types, 1769 tokens, and a TTR of 41.26625211984172\n",
      "'04 The Story of the Bald-Headed Man' has 1153 types, 3884 tokens, and a TTR of 29.685890834191554\n",
      "'05 The Tragedy of Pondicherry Lodge' has 849 types, 2669 tokens, and a TTR of 31.809666541775943\n",
      "'06 Sherlock Holmes Gives a Demonstration' has 964 types, 3191 tokens, and a TTR of 30.209965528047633\n",
      "'07 The Episode of the Barrel' has 1325 types, 4277 tokens, and a TTR of 30.979658639233108\n",
      "'08 The Baker Street Irregulars' has 1085 types, 3412 tokens, and a TTR of 31.79953106682298\n",
      "'09 A Break in the Chain' has 1009 types, 3686 tokens, and a TTR of 27.373846988605532\n",
      "'10 The End of the Islander' has 1034 types, 3399 tokens, and a TTR of 30.42071197411003\n",
      "'11 The Great Agra Treasure' has 722 types, 2219 tokens, and a TTR of 32.53717890941866\n",
      "'12 The Strange Story of Jonathan Small' has 1965 types, 10445 tokens, and a TTR of 18.81282910483485\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"sot4chaps\"\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd8971",
   "metadata": {},
   "source": [
    "# 3. Automatically Determining Sample Size and Producing Standardized Results\n",
    "\n",
    "So we're done, right??\n",
    "\n",
    "### **NO!**\n",
    "\n",
    "Why not? What do we still need to be able to do?\n",
    "\n",
    "That's right: calculate *standardized* TTRs!\n",
    "\n",
    "To be specific, we still need to figure out\n",
    "* How to automatically determine our sample size (i.e., the total length of the shortest text)\n",
    "* And then how to calculate TTRs for that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113b2cf",
   "metadata": {},
   "source": [
    "## Calculating the Total Length of the Shortest Item in a List\n",
    "\n",
    "Before we do this with full-length texts, let's try with a small-scale experiment.\n",
    "\n",
    "Let's say create a list with a bunch of strings in it. How could we automatically determine the total length of the shortest of these strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2147b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of smallest string:  4\n"
     ]
    }
   ],
   "source": [
    "bunch_o_strings = [\"Adam\", \"Marta\", \"Rosie\", \"Jazz\", \"Adamillo\", \"Anna\", \"Stephen\", \"Richard\", \"Ernest\"]\n",
    "length_of_shortest_string = 0\n",
    "\n",
    "for string in bunch_o_strings:\n",
    "    string_length = len(string)\n",
    "    if string_length < length_of_shortest_string or length_of_shortest_string == 0:\n",
    "        length_of_shortest_string = string_length\n",
    "    \n",
    "print(\"Length of smallest string: \", length_of_shortest_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d2e6b",
   "metadata": {},
   "source": [
    "## Creating Standardized Sample Sizes\n",
    "\n",
    "### First, let's try \"standardizing\" our toy example. What does the comment line do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2c7e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n",
      "Mart\n",
      "Rosi\n",
      "Jazz\n",
      "Adam\n",
      "Anna\n",
      "Step\n",
      "Rich\n",
      "Erne\n"
     ]
    }
   ],
   "source": [
    "for string in bunch_o_strings:\n",
    "    string_standardized = string[:length_of_shortest_string] # What's happening here?\n",
    "    print(string_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8afffe",
   "metadata": {},
   "source": [
    "That's right: our old friend **slicing** comes to our rescure here. Remember that the syntax for slicing is `[start:stop:step]`. If we only want the first four character of a string, we write `string[:4]`. Since we won't know our sample size until we load all the files in a folder, we don't want to hard-code a number in there. So we can use a variable name instead.\n",
    "\n",
    "Now, we're going to be standardizing a sample of `lists` (texts broken up into words) rather than `strings`... but as we know, slicing lists and strings works exactly the same way. If we want the first for items of a list, we would also use `list[:4]`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2d1d4",
   "metadata": {},
   "source": [
    "## We now have ALMOST all the skills we need to quickly and accurately calculate TTRs for any number of plain text files stored together in a folder\n",
    "\n",
    "We already know how to:\n",
    "* Automatically load files in a folder, one-by-one, as strings\n",
    "* Remove punctuation, split them into words, and calculate their total number of words (tokens)\n",
    "* Record the number of unique words (types) with a for loop that also lowercases all words\n",
    "* Automatically standardize the sample size, looking only at the first x words in each text, where x is the total length of the shortest text\n",
    "\n",
    "The only things we still don't know how to do are:\n",
    "* Output the results of our analysis of the total texts (non-standardized results)\n",
    "* Output the standardized results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20a3ff",
   "metadata": {},
   "source": [
    "Below I've written out the code that does all the things we've learned so far. It doesn't store the results anywhere yet, but it does calculate them and print them out.\n",
    "\n",
    "This first cell records information for overall, non-standardized values and also determines our sample size (the total length of the shortest text). \n",
    "\n",
    "### Let's try and figure out how we can use what we learned above to calculate the TTR\n",
    "### Who can tell me what these new lines are doing? Talk to the person next to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7474fb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01 The Science of Deduction' has 1042 types, 2986 tokens, and a TTR of 34.89618218352311\n",
      "So far, the shortest text is 2986 words in length.\n",
      "\n",
      "'02 The Statement of the Case' has 714 types, 1867 tokens, and a TTR of 38.24317086234601\n",
      "So far, the shortest text is 1867 words in length.\n",
      "\n",
      "'03 In Quest of a Solution' has 730 types, 1769 tokens, and a TTR of 41.26625211984172\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'04 The Story of the Bald-Headed Man' has 1153 types, 3884 tokens, and a TTR of 29.685890834191554\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'05 The Tragedy of Pondicherry Lodge' has 849 types, 2669 tokens, and a TTR of 31.809666541775943\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'06 Sherlock Holmes Gives a Demonstration' has 964 types, 3191 tokens, and a TTR of 30.209965528047633\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'07 The Episode of the Barrel' has 1325 types, 4277 tokens, and a TTR of 30.979658639233108\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'08 The Baker Street Irregulars' has 1085 types, 3412 tokens, and a TTR of 31.79953106682298\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'09 A Break in the Chain' has 1009 types, 3686 tokens, and a TTR of 27.373846988605532\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'10 The End of the Islander' has 1034 types, 3399 tokens, and a TTR of 30.42071197411003\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'11 The Great Agra Treasure' has 722 types, 2219 tokens, and a TTR of 32.53717890941866\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n",
      "'12 The Strange Story of Jonathan Small' has 1965 types, 10445 tokens, and a TTR of 18.81282910483485\n",
      "So far, the shortest text is 1769 words in length.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"sot4chaps\"\n",
    "\n",
    "sample_size = 0  # Note this line and figure out what it's doing!\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size: # The line I noted above connects to this one and the next\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr}\")\n",
    "    print(f\"So far, the shortest text is {sample_size} words in length.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f873eb",
   "metadata": {},
   "source": [
    "The cell below uses the sample size determined in the previous step (1769 words; stored in the `sample_size` variable) to calculate the standardized TTR for the first 1769 words of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fccb85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01 The Science of Deduction' has 713 types in the standardized sample of 1769 tokens, and a TTR of 40.30525720746184.\n",
      "\n",
      "'02 The Statement of the Case' has 686 types in the standardized sample of 1769 tokens, and a TTR of 38.77897117015262.\n",
      "\n",
      "'03 In Quest of a Solution' has 730 types in the standardized sample of 1769 tokens, and a TTR of 41.26625211984172.\n",
      "\n",
      "'04 The Story of the Bald-Headed Man' has 682 types in the standardized sample of 1769 tokens, and a TTR of 38.552854720180896.\n",
      "\n",
      "'05 The Tragedy of Pondicherry Lodge' has 639 types in the standardized sample of 1769 tokens, and a TTR of 36.122102882984734.\n",
      "\n",
      "'06 Sherlock Holmes Gives a Demonstration' has 635 types in the standardized sample of 1769 tokens, and a TTR of 35.895986433013.\n",
      "\n",
      "'07 The Episode of the Barrel' has 702 types in the standardized sample of 1769 tokens, and a TTR of 39.68343697003957.\n",
      "\n",
      "'08 The Baker Street Irregulars' has 630 types in the standardized sample of 1769 tokens, and a TTR of 35.613340870548335.\n",
      "\n",
      "'09 A Break in the Chain' has 624 types in the standardized sample of 1769 tokens, and a TTR of 35.27416619559073.\n",
      "\n",
      "'10 The End of the Islander' has 624 types in the standardized sample of 1769 tokens, and a TTR of 35.27416619559073.\n",
      "\n",
      "'11 The Great Agra Treasure' has 616 types in the standardized sample of 1769 tokens, and a TTR of 34.821933295647256.\n",
      "\n",
      "'12 The Strange Story of Jonathan Small' has 618 types in the standardized sample of 1769 tokens, and a TTR of 34.934991520633126.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size] # This is the key new line in this block of code. Figure out what it does! Where did we define the variable sample_size?\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types_standardized} types in the standardized sample of {tokens_standardized} tokens, and a TTR of {ttr_standardized}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16015122",
   "metadata": {},
   "source": [
    "# 4. Writing CSV files\n",
    "\n",
    "Okay — that's a lot to read through and think about. But let's just get this beast of a TTR task finished by taking the final step: doing all this but also storing the results in a file.\n",
    "\n",
    "We'll do this with the `open()` function, the `open.write()` method, and the CSV file format.\n",
    "\n",
    "## What is a CSV?\n",
    "\n",
    "A CSV is a very simple file format for spreadsheets. The name stands for \"Comma Separated Values\" — and that's really all it is: a plain text file in which values (whatever would go into a cell in a spreadsheet) are separated by commas! The end of a row in a CSV file is signalled by the newline character `\\n`. Text cells should be wrapped in quotation marks (`\"\"`) just like strings in Python. Other than that, no tricks!\n",
    "\n",
    "A pretty table like the following:\n",
    "\n",
    "| Types | Tokens                                                                                  |\n",
    "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
    "| 12         | 24                                                                             |\n",
    "| 33         | 100                                              |\n",
    "| 75 | 500\n",
    "\n",
    "Would be expressed in a CSV file as:\n",
    "\n",
    "```\n",
    "\"Types\",\"Tokens\"\n",
    "12,24\n",
    "33,100\n",
    "75,500\n",
    "```\n",
    "\n",
    "... and a spreadhseet program like Excel would see that and know exactly what to do with it.\n",
    "\n",
    "(By the way, when Quercus gives your final grades at the end of the year to be uploaded to the eMarks system... it will be in the form of a CSV file!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f9edc",
   "metadata": {},
   "source": [
    "## Writing files with `open.write()`\n",
    "\n",
    "We already know how to **load** a file into Python and stick it into a variable. We do it like this, with the `open()` function and the `.read()` method.\n",
    "\n",
    "### First, let's save the sign of four to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b2b9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4 = open(\"sign-of-four.txt\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71652732",
   "metadata": {},
   "source": [
    "Now, this might not be the most intutive thing in the world... but actually **writing** or **creating** a file in Python happens pretty much the same way. \n",
    "* First, you **`open()`** a file. Because it's not a file that already exists — it's one you're creating out of nothing — you need to set the *argument* `mode` to `\"w\"` (write) rather than the default `\"r\"` (read).\n",
    "* Then you use the `.write()` method to write something into that file.\n",
    "\n",
    "### Next, let's write to a new text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da292500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"my-new-file.txt\", mode=\"w\", encoding=\"utf-8\").write(\"Hey, look at this, I made a file in Python!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b090610",
   "metadata": {},
   "source": [
    "We can read this back in to make sure it actually worked..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a509a54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey, look at this, I made a file in Python!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"my-new-file.txt\", mode=\"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c73d0",
   "metadata": {},
   "source": [
    "That's all you need if you want to create your new file in one shot. But we want to build ours up slowly, text by text, as we iterate through our folder of text files. \n",
    "\n",
    "For that, we'll use the following format.\n",
    "\n",
    "1. First, we `open()` a file and assign that \"file object\" to a variable called `file`.\n",
    "2. We then write whatever we want into that `file` varable, applying the `.write()` method. We can do this as many times as we like.\n",
    "3. Then we \"close\" our file using the `.close()` method.\n",
    "\n",
    "### All together now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee1fc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"another-new-file.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write(\"Here's some text\\n\")\n",
    "file.write(\"Here's some more!\\n\")\n",
    "file.write(\"And that's all I want to write for now!\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e19d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's some text\n",
      "Here's some more!\n",
      "And that's all I want to write for now!\n"
     ]
    }
   ],
   "source": [
    "print(open(\"another-new-file.txt\", mode=\"r\", encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc67e8",
   "metadata": {},
   "source": [
    "Thankfully, this all works exactly the same way for a CSV file. Except... we put in commas between values and wrap text in `\"\"`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19669346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"babys-first-spreadsheet.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Types\",\"Tokens\"\\n') # Note that if you want to write double quotes, you have to wrap them in single quotes, or Python will get confused.\n",
    "file.write(\"12,24\\n\")\n",
    "file.write(\"33,100\\n\")\n",
    "file.write(\"75,500\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd76fa",
   "metadata": {},
   "source": [
    "Now that we know how to write CSV files, we are well and truly finished with the TTR task (well... as much as anything is every finished! I can imagine a few ways to improve it, still. Can you??)\n",
    "\n",
    "Here is the whole process, in a single cell. The major part of your Lab this week is to comment this big block of code, line by line.\n",
    "\n",
    "### And here's the final TTR code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c9b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"sot4chaps\"\n",
    "\n",
    "sample_size = 0\n",
    "\n",
    "file = open(\"ttr-overall.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size:\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types},{tokens},{ttr}\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"ttr-standardized.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size]\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types_standardized},{tokens_standardized},{ttr_standardized}\\n')\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
