{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import wikipediaapi as wk\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# TODO update paths\n",
    "indo_mmlu_df = pd.read_csv('datasets/IndoMMLU main.csv')\n",
    "\n",
    "tydi_df = pd.read_json('datasets/Tydiqa v1.0 dev.jsonl', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks from CMMLU for reference\n",
    "# all_chinese_tasks = ['agronomy', 'anatomy', 'ancient_chinese', 'arts', 'astronomy', 'business_ethics', 'chinese_civil_service_exam', 'chinese_driving_rule', 'chinese_food_culture', 'chinese_foreign_policy', 'chinese_history', 'chinese_literature', \n",
    "# 'chinese_teacher_qualification', 'clinical_knowledge', 'college_actuarial_science', 'college_education', 'college_engineering_hydrology', 'college_law', 'college_mathematics', 'college_medical_statistics', 'college_medicine', 'computer_science',\n",
    "# 'computer_security', 'conceptual_physics', 'construction_project_management', 'economics', 'education', 'electrical_engineering', 'elementary_chinese', 'elementary_commonsense', 'elementary_information_and_technology', 'elementary_mathematics', \n",
    "# 'ethnology', 'food_science', 'genetics', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_geography', 'high_school_mathematics', 'high_school_physics', 'high_school_politics', 'human_sexuality',\n",
    "# 'international_law', 'journalism', 'jurisprudence', 'legal_and_moral_basis', 'logical', 'machine_learning', 'management', 'marketing', 'marxist_theory', 'modern_chinese', 'nutrition', 'philosophy', 'professional_accounting', 'professional_law', \n",
    "# 'professional_medicine', 'professional_psychology', 'public_relations', 'security_study', 'sociology', 'sports_science', 'traditional_chinese_medicine', 'virology', 'world_history', 'world_religions']\n",
    "\n",
    "chinese_tasks = ['ancient_chinese', 'chinese_civil_service_exam', 'chinese_driving_rule', 'chinese_food_culture', 'chinese_foreign_policy', 'chinese_history', 'chinese_literature', 'chinese_teacher_qualification', 'elementary_chinese', 'ethnology', 'marxist_theory', 'modern_chinese', 'security_study', 'traditional_chinese_medicine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_tasks = ['Arabic Language (General)', 'Arabic Language (Grammar)', 'Arabic Language (High School)', 'Arabic Language (Middle School)', 'Arabic Language (Primary School)', 'Civics (High School)', 'Civics (Middle School)' 'Driving Test', 'History (High School)', 'History (Middle School)', 'History (Primary School)' 'Islamic Studies', 'Islamic Studies (High School)', 'Islamic Studies (Middle School)', 'Islamic Studies (Primary School)',]\n",
    "# arabic_splits = {'test': 'All/test.csv', 'dev': 'All/dev.csv'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pseudocode\n",
    "# will need to go through each language one by one since user agent only searches one language's wikipedia\n",
    "class Language(Enum):\n",
    "    ar = arabic\n",
    "    nb = bengali\n",
    "    fi = finnish\n",
    "    id = indonesian\n",
    "    ja = japanese\n",
    "    sw = swahili\n",
    "    ko = korean\n",
    "    ru = russian\n",
    "    te = telugu\n",
    "    th = thai\n",
    "\n",
    "\n",
    "def has_no_english_wikipedia_page(df:pd.Dataframe, lang:Language):\n",
    "    user_agent = f\"PageLanguagesCheckBot-{lang.name}/0.1 (cam.t.bishop@gmail.com)\"\n",
    "    wiki = wk.Wikipedia(user_agent=user_agent, language=lang.name) # create wikipedia bot for specified language\n",
    "    lang_filtered_df = df[df['language'] == lang.value]\n",
    "    \n",
    "    # anyway to vectorize this/ not iterate?\n",
    "    title = document_title\n",
    "    \n",
    "    page = wiki.page(title)\n",
    "    lang_links = page.lang_links\n",
    "    \n",
    "# how do I get a boolean array of some form to select which rows I should use based off if there is an english page or not?\n",
    "# a[true, true, false, false]\n",
    "\n",
    "for lang in (Language):\n",
    "    has_no_english_wikipedia_page(tydi_df, lang)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
