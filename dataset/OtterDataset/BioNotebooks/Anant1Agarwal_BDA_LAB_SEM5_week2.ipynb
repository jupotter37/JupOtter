{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3ee43e-84e2-47fa-8a7d-e4fbc21e8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923fbcbc-014f-4e57-bfac-c0ab26ffbae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 21:57:01 WARN Utils: Your hostname, Anants-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.68.3.53 instead (on interface en0)\n",
      "24/11/02 21:57:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 21:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName(\"week2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9020b7b3-6149-4734-9969-79b441faf7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ef232-4926-4850-8224-3711cb1e44d6",
   "metadata": {},
   "source": [
    "1) Implement a PySpark script that applies transformations like filter and withColumn on a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcf3f6a-2349-4d6e-a8a0-0086c0576dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "|first name|last name|             subject|marks|      location|\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "|     anant|  agarwal|               maths|   99|San Franscisco|\n",
      "|   rishabh|    singh|                chem|   98|       Lucknow|\n",
      "|    kanika|  agarwal|            commerce|   95|         dubai|\n",
      "|   yashika|   mittal|           computers|   96|       seattle|\n",
      "|   prasoon|   mittal|            commerce|   94|     bangalore|\n",
      "| prashali |   mittal|Information techn...|   98|      Gurugram|\n",
      "|  shivansh|bhatnagar|             biology|   94|       Lucknow|\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "\n",
      "Filtered DataFrame:\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "|first name|last name|             subject|marks|      location|\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "|     anant|  agarwal|               maths|   99|San Franscisco|\n",
      "|   rishabh|    singh|                chem|   98|       Lucknow|\n",
      "|   yashika|   mittal|           computers|   96|       seattle|\n",
      "| prashali |   mittal|Information techn...|   98|      Gurugram|\n",
      "+----------+---------+--------------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Sample data as a dictionary\n",
    "# data_dict = {\n",
    "#     \"id\": [1, 2, 3, 4],\n",
    "#     \"name\": [\"Alice\", \"Bob\", \"Cathy\", \"David\"],\n",
    "#     \"age\": [29, 32, 25, 35]\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame from the dictionary\n",
    "# df = spark.createDataFrame(data_dict)\n",
    "csv_file_path = \"week1.ipynb.csv\"  # Replace with your CSV file path\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_file_path)\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df=df.filter(col(\"marks\")>95)\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebea23d9-f24b-4de2-bbe8-c871f2f8f6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-----+--------------+-------+\n",
      "|first name|last name|             subject|marks|      location| Remark|\n",
      "+----------+---------+--------------------+-----+--------------+-------+\n",
      "|     anant|  agarwal|               maths|   99|San Franscisco|   Good|\n",
      "|   rishabh|    singh|                chem|   98|       Lucknow|   Good|\n",
      "|    kanika|  agarwal|            commerce|   95|         dubai|Improve|\n",
      "|   yashika|   mittal|           computers|   96|       seattle|   Good|\n",
      "|   prasoon|   mittal|            commerce|   94|     bangalore|Improve|\n",
      "| prashali |   mittal|Information techn...|   98|      Gurugram|   Good|\n",
      "|  shivansh|bhatnagar|             biology|   94|       Lucknow|Improve|\n",
      "+----------+---------+--------------------+-----+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf,avg, sum ,explode, split, col, lower\n",
    "def get_remark(marks):\n",
    "    return \"Good\" if marks > 95 else \"Improve\"\n",
    "\n",
    "get_remark_udf = udf(get_remark, StringType())\n",
    "\n",
    "new_df=df.withColumn(\"Remark\",get_remark_udf(df.marks))\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88804ede-b3be-4c87-a576-401a4f7f3ab2",
   "metadata": {},
   "source": [
    "2) Write a PySpark script that performs actions like count and show on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62c4912-d3be-423f-b2f3-042f3b13e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows are : 7\n"
     ]
    }
   ],
   "source": [
    "row_count=df.count()\n",
    "print(f\"Number of Rows are : {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2815fec6-b515-4950-9d15-523381f6ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows: [Row(first name='anant', last name='agarwal', subject='maths', marks=99, location='San Franscisco'), Row(first name='rishabh', last name='singh', subject='chem', marks=98, location='Lucknow'), Row(first name='kanika', last name='agarwal', subject='commerce', marks=95, location='dubai')]\n"
     ]
    }
   ],
   "source": [
    "first_three_rows = df.take(3)\n",
    "print(\"First 3 rows:\", first_three_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8613ed9-4a71-468f-951d-2cb4b2f4094c",
   "metadata": {},
   "source": [
    "3) Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00ef73c-25c2-45c6-819d-df2d26a4e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total marks :674\n"
     ]
    }
   ],
   "source": [
    "total_marks=df.agg(sum(\"marks\").alias(\"total marks\")).collect()[0][\"total marks\"]\n",
    "print(f\"total marks :{total_marks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a667a78-bad3-480c-b4c6-19fde79a6183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg marks: 96.28571428571429\n"
     ]
    }
   ],
   "source": [
    "avg_marks=df.agg(avg(\"marks\").alias(\"avg marks\")).collect()[0][\"avg marks\"]\n",
    "print(f\"avg marks: {avg_marks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75475321-cc21-4720-ad06-31ec85d39bbb",
   "metadata": {},
   "source": [
    "4) Show how to write a PySpark DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4595a9-65a5-431c-84c3-30bcbb9f9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=\"marks.csv\"\n",
    "df.write.csv(output_path,header=True,mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b7962-9afb-49ce-bda7-508c7e44e1fe",
   "metadata": {},
   "source": [
    "5) Implement wordcount program in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62ea202-ace9-4f09-8756-561bcf607f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of: 1\n",
      "pyspark: 2\n",
      "the: 1\n",
      "world: 3\n",
      "to: 1\n",
      "welcome: 1\n",
      "hello: 3\n",
      "from: 1\n",
      "again: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"Hello world\",\n",
    "    \"Hello from PySpark\",\n",
    "    \"Welcome to the world of PySpark\",\n",
    "    \"Hello again world\"\n",
    "]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "word_counts=(rdd\n",
    "            .flatMap(lambda line: line.split())\n",
    "            .map(lambda word:(word.lower(),1))\n",
    "            .reduceByKey(lambda a,b:a+b))\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe67d96-8215-4551-a8f8-f4174414863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Counts:\n",
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|  hello|    3|\n",
      "|  world|    3|\n",
      "|pyspark|    2|\n",
      "|   from|    1|\n",
      "|welcome|    1|\n",
      "|    the|    1|\n",
      "|     of|    1|\n",
      "|     to|    1|\n",
      "|  again|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Hello world\"),\n",
    "    (2, \"Hello from PySpark\"),\n",
    "    (3, \"Welcome to the world of PySpark\"),\n",
    "    (4, \"Hello again world\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"sentence\"])\n",
    "word_counts_df = (df\n",
    "                  .select(explode(split(col(\"sentence\"), \" \")).alias(\"word\"))  \n",
    "                  .withColumn(\"word\", lower(col(\"word\")))\n",
    "                  .groupBy(\"word\")  \n",
    "                  .count()\n",
    "                  .orderBy(\"count\", ascending=False))  \n",
    "\n",
    "print(\"Word Counts:\")\n",
    "word_counts_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1911d1d-9e6e-44c0-8a61-0388dd99dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b695b845-5a90-4094-b5ab-c263cad02bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d32afe-f134-4238-ab1d-fdba7818ee9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2c1c0-b6fe-4a64-9bef-a2fd508e8ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf159b-ecc0-4425-a837-9bbdfb8c234c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2925cf-4ae1-4990-ac3f-15e20af1ad56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dca412-5b57-4a9f-9b05-c2f8119f1a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c25158-f74a-4dfa-b7d8-d735477c161a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
