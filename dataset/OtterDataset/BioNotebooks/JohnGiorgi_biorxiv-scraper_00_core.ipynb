{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Union\n",
    "\n",
    "import requests\n",
    "from tqdm import trange\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class bioRxivScraper():\n",
    "    \"\"\"A simple class for scraping articles and their metadata from bioRxiv.\"\"\"\n",
    "    def __init__(self):\n",
    "        url_advanced_search_base = \"https://www.biorxiv.org/search/%20jcode%3Abiorxiv\"\n",
    "        # F-formatted string for advanced search querys\n",
    "        url_advanced_search_params = (\"%20subject_collection_code%3A{0}%20\"\n",
    "                                      \"limit_from%3A{1}-{2}-01%20limit_to%3A{1}-{3}-01%20\"\n",
    "                                      # Choose an extremely large num to fit all results on on page\n",
    "                                      \"numresults%3A100000\")\n",
    "\n",
    "        self.url_advanced_search = url_advanced_search_base + url_advanced_search_params\n",
    "        # F-formatted string for direct (i.e. via the doi) search querys\n",
    "        self.url_direct_link_base = 'https://www.biorxiv.org/{0}'\n",
    "        # TODO (John): It may be better to scrape this from the bioRxiv homepage, that way it remains up-to-date\n",
    "        # For now, hardcoding is an OK alternative\n",
    "        self.subject_areas = [\n",
    "            \"Animal Behavior and Cognition\", \"Biochemistry\", \"Bioengineering\", \"Bioinformatics\", \"Biophysics\",\n",
    "            \"Cancer Biology\", \"Cell Biology\", \"Developmental Biology\", \"Ecology\", \"Evolutionary Biology\",\n",
    "            \"Genetics\", \"Genomics\", \"Immunology\", \"Microbiology\", \"Molecular Biology\", \"Neuroscience\",\n",
    "            \"Paleontology\", \"Pathology\", \"Pharmacology and Toxicology\", \"Physiology\", \"Plant Biology\",\n",
    "            \"Scientific Communication and Education\", \"Synthetic Biology\", \"Systems Biology\", \"Zoology\"\n",
    "        ]\n",
    "\n",
    "    def by_year(self, start_year: int, end_year: int = None, subject_areas: Union[str, List] = None) -> Dict:\n",
    "        \"\"\"Returns a dictionary keyed by doi, that contains the data/metadata of all articles uploaded\n",
    "        between `start_year` and `end_year` for a given `subject_area`.\n",
    "\n",
    "        Args:\n",
    "            subject_area (str):\n",
    "        \"\"\"\n",
    "        end_year = start_year if end_year is None else end_year\n",
    "\n",
    "        if subject_areas is not None:\n",
    "            if isinstance(subject_areas, str):\n",
    "                subject_areas = [subject_areas]\n",
    "            for sa in subject_areas:\n",
    "                if sa not in self.subject_areas:\n",
    "                    raise ValueError(f'subject_area must be one of {self.subject_areas}. Got {subject_areas}.')\n",
    "        else:\n",
    "            subject_areas = self.subject_areas\n",
    "\n",
    "        scraped_content = {}\n",
    "\n",
    "        for subject_area in subject_areas:\n",
    "            url_encoded_subject_area = self._url_encode_subject_area(subject_area)\n",
    "            for year in range(start_year, end_year + 1):\n",
    "                for month in trange(1, 13, \n",
    "                    unit='month', \n",
    "                    desc=f\"Scraping subject area: {subject_area}\", \n",
    "                    dynamic_ncols=True\n",
    "                ):\n",
    "                    resp = self._advanced_search(subject_area, year, month)\n",
    "                    html = bs(resp.text, features=\"html.parser\")\n",
    "\n",
    "                    articles = html.find_all('li', attrs={'class': 'search-result'})\n",
    "                    for article in articles:\n",
    "                        article_content = self._extract_content_from_article(article)\n",
    "\n",
    "                        if any(content is None for content in article_content.values()):\n",
    "                            continue\n",
    "\n",
    "                        resp = self._direct_search(article_content[\"content_id\"])\n",
    "                        html = bs(resp.text, features=\"html.parser\")\n",
    "\n",
    "                        abstract = html.find(\"p\", attrs={\"id\": re.compile(r\"p-\\d+\")})\n",
    "\n",
    "                        if abstract is None:\n",
    "                            continue\n",
    "                        abstract = abstract.text.strip()\n",
    "\n",
    "                        # Collect year / month / title information\n",
    "                        # TODO (John): Can we scrape the published date, and add only\n",
    "                        # one field here: \"data_published\"\n",
    "                        scraped_content[article_content[\"doi\"]] = {\n",
    "                            'month': month,\n",
    "                            'year': year,\n",
    "                            'title': article_content[\"title\"],\n",
    "                            'abstract': abstract,\n",
    "                            'subject_area': subject_area,\n",
    "                            'authors': article_content[\"authors\"],\n",
    "                        }\n",
    "\n",
    "        return scraped_content\n",
    "\n",
    "    def _url_encode_subject_area(self, subject_area: str) -> str:\n",
    "        return '%20'.join(subject_area.split(' '))\n",
    "\n",
    "    def _advanced_search(self, subject_area: str, year: int, month: int):\n",
    "        subject_area = self._url_encode_subject_area(subject_area)\n",
    "        resp = requests.post(self.url_advanced_search.format(subject_area, year, month, month + 1))\n",
    "        resp.raise_for_status()\n",
    "        return resp\n",
    "\n",
    "    def _direct_search(self, content_id: str):\n",
    "        resp = requests.post(self.url_direct_link_base.format(content_id))\n",
    "        resp.raise_for_status()\n",
    "        return resp\n",
    "\n",
    "    def _extract_content_from_article(self, article):\n",
    "        \"\"\"Given an `article` (the html representation of an article scraped from bioRxiv),\n",
    "        returns a dictionary containing important data and meta data (e.g. the title, authors and doi).\"\"\"\n",
    "        linked_title =  article.find('a', attrs={'class': 'highwire-cite-linked-title'})\n",
    "        if linked_title is not None:\n",
    "            content_id = linked_title['href'].strip()\n",
    "            doi = '/'.join(content_id.split('/')[-2:])[:-2]\n",
    "            title = linked_title.find('span', attrs={'class': 'highwire-cite-title'})\n",
    "            if title is not None:\n",
    "                title = title.text.strip()\n",
    "        else:\n",
    "            content_id, doi, title = None, None, None\n",
    "        authors = article.find_all('span', attrs={'class': 'highwire-citation-author'})\n",
    "        authors = [author.text.strip() for author in authors]\n",
    "\n",
    "        return {\"content_id\": content_id, \"doi\": doi, \"title\": title, \"authors\": authors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted make_biorxiv_dataset.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biorxiv] *",
   "language": "python",
   "name": "conda-env-biorxiv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
