{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiPDA Categories\n",
    "This notebook processes Turkish Wikipedia text data locally with WikiPDA and generates datasets relating articles to their corresponding ORES Topic categories.\n",
    "First we process the Wikitext and apply WikiPDA two get the following data:\n",
    "* ```wikipda_top_topics.csv```: Contains the page_id of all retrieved articles and the ORES category that has the highest probability.\n",
    "* ```wikipda_topics.csv``` : Contains the page_id of all retrieved articles and probability for each ORES category that WikiPDA predicts for that article.\n",
    "\n",
    "Then we exploit these data to obtain the number of edits per day and revert rates for each predicted category.\n",
    "Three more DataFrames are then created\n",
    "* ```thresholded_topics.csv```: Contains page_id, page_title and the associated topic for an article. An article may have more than one topic. An article is said to belong to a topic if WikiPDA predits that topic with a probability bigger than 0.7.\n",
    "* ```daily_edits_by_topic.csv``` :  Daily number of edits (non-bot) to all topics (topics assigned with the threshold method)\n",
    "* ```revert_rate_by_topic.csv```: Daily revert rate of all topics (topics assigned with the threshold method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "import argparse\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "from pyspark.ml.clustering import LDA\n",
    "from wikipda.article import Preprocessor, Bagger, fetch_article_data\n",
    "from wikipda.model import WikiPDAModel, ORESClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Process data and get WikiPDA topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\").setAll([\n",
    "                                   ('spark.driver.memory','32g'),\n",
    "                                   ('spark.jars.packages', 'com.databricks:spark-xml_2.11:0.8.0'),\n",
    "                                   ('spark.driver.maxResultSize', '32G'),\n",
    "                                   ('spark.local.dir', '/scratch/tmp/'),\n",
    "                                   ('spark.yarn.stagingDir', '/scratch/tmp/'),\n",
    "                                   ('spark.sql.warehouse.dir', '/scratch/tmp/')\n",
    "                                  ])\n",
    "# create the session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# create the context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikitext data \n",
    "WIKIPEDIA_DUMP = '/dlabdata1/turkish_wiki/trwiki-20210201-pages-articles-multistream.xml.bz2'\n",
    "\n",
    "# Read Articles that are in namespace 0\n",
    "wikipedia_all = spark.read.format('com.databricks.spark.xml') \\\n",
    "    .options(rowTag='page').load(WIKIPEDIA_DUMP) \\\n",
    "    .filter(\"ns = '0'\") \\\n",
    "    .filter(\"revision.text._VALUE is not null\") \\\n",
    "    .filter(\"length(revision.text._VALUE) > 0\")\n",
    "\n",
    "# Get articles\n",
    "wikipedia_articles = wikipedia_all.where(\"redirect is NULL\")\\\n",
    "                    .selectExpr(\"id\", \"revision.id revision_id\", \"revision.text._VALUE wikicode\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocessor('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the wikitext of articles in memory for processing\n",
    "wikitext = wikipedia_articles.select(\"wikicode\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "error_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all articles and add to articles and error_idx lists\n",
    "for idx, text in enumerate(wikitext):\n",
    "    try:\n",
    "        article = p.load([text], enrich=True)\n",
    "        articles.extend(article)\n",
    "    except:\n",
    "        error_idx.append(idx)\n",
    "        articles.extend([None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump articles and error indexes\n",
    "pickle.dump(error_idx, open(\"/dlabdata1/turkish_wiki/error_idx.p\", \"wb\" ))\n",
    "pickle.dump(articles, open(\"/dlabdata1/turkish_wiki/articles.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ids = wikipedia_articles.select(\"id\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_articles = list(zip(wiki_ids, articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_articles = [elem for elem in zipped_articles if elem[1] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:loading LdaMulticore object from /home/ira/wikipda_data/LDA_models/300/lda.model\n",
      "INFO:loading expElogbeta from /home/ira/wikipda_data/LDA_models/300/lda.model.expElogbeta.npy with mmap=None\n",
      "INFO:setting ignored attribute state to None\n",
      "INFO:setting ignored attribute id2word to None\n",
      "INFO:setting ignored attribute dispatcher to None\n",
      "INFO:LdaMulticore lifecycle event {'fname': '/home/ira/wikipda_data/LDA_models/300/lda.model', 'datetime': '2021-05-12T00:16:51.399950', 'gensim': '4.0.1', 'python': '3.7.7 (default, Mar 23 2020, 22:36:06) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-91-generic-x86_64-with-debian-buster-sid', 'event': 'loaded'}\n",
      "INFO:loading LdaState object from /home/ira/wikipda_data/LDA_models/300/lda.model.state\n",
      "INFO:loading sstats from /home/ira/wikipda_data/LDA_models/300/lda.model.state.sstats.npy with mmap=None\n",
      "INFO:LdaState lifecycle event {'fname': '/home/ira/wikipda_data/LDA_models/300/lda.model.state', 'datetime': '2021-05-12T00:16:54.679446', 'gensim': '4.0.1', 'python': '3.7.7 (default, Mar 23 2020, 22:36:06) \\n[GCC 7.3.0]', 'platform': 'Linux-4.15.0-91-generic-x86_64-with-debian-buster-sid', 'event': 'loaded'}\n",
      "/home/ira/.local/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    }
   ],
   "source": [
    "# Get the bag-of-links representations\n",
    "bols = Bagger().bag([elem[1] for elem in zipped_articles])\n",
    "# Get topics distribution\n",
    "model = WikiPDAModel(k=300)\n",
    "topics_distribution = model.get_distribution(bols)\n",
    "classifier = ORESClassifier()\n",
    "\n",
    "# Predict categories of articles\n",
    "text_categories = classifier.predict_category(topics_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389054,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_articles = np.array(zipped_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cats = np.c_[zipped_articles[:, 0], text_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(article_cats, open(\"/dlabdata1/turkish_wiki/article_cats.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ira/.local/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    }
   ],
   "source": [
    "# Gets probability of each topic for the articles\n",
    "category_probas = classifier.predict_proba_labeled(topics_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame containing the probability of the article belonging a certain topic\n",
    "topic_df = pd.DataFrame(category_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df['page_id'] = zipped_articles[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_df[['page_id', 'Culture.Media.Entertainment', 'STEM.Space', 'STEM.Mathematics',\n",
    "       'Geography.Regions.Africa.Central Africa',\n",
    "       'Geography.Regions.Americas.North America',\n",
    "       'History and Society.Society', 'Geography.Regions.Oceania',\n",
    "       'STEM.Engineering', 'STEM.Libraries _ Information',\n",
    "       'History and Society.Politics and government', 'STEM.Biology',\n",
    "       'Culture.Media.Music', 'Geography.Regions.Asia.West Asia',\n",
    "       'Geography.Regions.Asia.Asia_',\n",
    "       'Geography.Regions.Americas.Central America',\n",
    "       'Geography.Regions.Europe.Southern Europe',\n",
    "       'Geography.Regions.Africa.Africa_',\n",
    "       'Geography.Regions.Asia.Central Asia',\n",
    "       'History and Society.Business and economics', 'STEM.STEM_',\n",
    "       'Culture.Media.Video games', 'Culture.Media.Software',\n",
    "       'Geography.Regions.Americas.South America',\n",
    "       'Culture.Biography.Biography_', 'Culture.Visual arts.Comics and Anime',\n",
    "       'Geography.Regions.Africa.Western Africa',\n",
    "       'Geography.Regions.Africa.Southern Africa', 'Culture.Performing arts',\n",
    "       'STEM.Physics', 'Culture.Linguistics', 'Culture.Internet culture',\n",
    "       'Culture.Biography.Women', 'STEM.Technology', 'STEM.Medicine _ Health',\n",
    "       'Culture.Media.Television', 'Culture.Philosophy and religion',\n",
    "       'Culture.Visual arts.Fashion',\n",
    "       'Geography.Regions.Europe.Western Europe',\n",
    "       'Geography.Regions.Asia.Southeast Asia', 'Culture.Media.Radio',\n",
    "       'Culture.Media.Books', 'Culture.Literature',\n",
    "       'Geography.Regions.Asia.South Asia', 'STEM.Computing',\n",
    "       'Culture.Food and drink', 'Geography.Geographical',\n",
    "       'Culture.Visual arts.Architecture',\n",
    "       'Geography.Regions.Africa.Eastern Africa',\n",
    "       'Geography.Regions.Asia.East Asia', 'STEM.Earth and environment',\n",
    "       'History and Society.Transportation', 'STEM.Chemistry',\n",
    "       'Culture.Media.Films', 'History and Society.History',\n",
    "       'History and Society.Military and warfare', 'Culture.Sports',\n",
    "       'Geography.Regions.Europe.Eastern Europe',\n",
    "       'Culture.Visual arts.Visual arts_', 'Geography.Regions.Asia.North Asia',\n",
    "       'Culture.Media.Media_', 'History and Society.Education',\n",
    "       'Geography.Regions.Africa.Northern Africa',\n",
    "       'Geography.Regions.Europe.Northern Europe',\n",
    "       'Geography.Regions.Europe.Europe_']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.to_csv('/dlabdata1/turkish_wiki/processed_data/wikipda_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats = pd.DataFrame(article_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats.columns = ['page_id', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats.to_csv('/dlabdata1/turkish_wiki/processed_data/wikipda_top_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Get edits and revert rates by category.\n",
    "\n",
    "### 1) Get thresholded topics\n",
    "It's hard to qualify any Wikipedia page into only one topic. When looking at the distributions I found out that subjects such as West Asia are heavily biased and will appear as the top topic of an article since it encodes information about Turkey but the article may have another topic which is way more intuitive. Thus I decided to threshold the probabilities of the articles, and having multiple topics per article is possible. After some inspection, I decided to settle the threshold at 0.7 where I considered an article belonging to a certain topic if the probabililty estimated by WikiPDA was bigger than 0.7. The median number of topics per article is 5 with this method and we can captuee valuable information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read relevant DataFrames\n",
    "topic_df = pd.read_csv('/dlabdata1/turkish_wiki/processed_data/wikipda_topics.csv')\n",
    "pages = pd.read_csv('/dlabdata1/turkish_wiki/processed_data/page.csv')\n",
    "daily_edits = pd.read_csv('/dlabdata1/turkish_wiki/processed_data/edits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages[['page_id', 'page_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.page_id = topic_df.page_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames to get the page_title\n",
    "topic_df = pd.merge(pages, topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily number of edits by non-bots by page_id\n",
    "daily_edits = daily_edits[daily_edits['user_kind'] != 'bot'].groupby(['date', 'page_id'])[['event_user_id']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits = daily_edits.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Culture.Media.Entertainment', 'STEM.Space',\n",
    "       'STEM.Mathematics', 'Geography.Regions.Africa.Central Africa',\n",
    "       'Geography.Regions.Americas.North America',\n",
    "       'History and Society.Society', 'Geography.Regions.Oceania',\n",
    "       'STEM.Engineering', 'STEM.Libraries _ Infor mation',\n",
    "       'History and Society.Politics and government', 'STEM.Biology',\n",
    "       'Culture.Media.Music', 'Geography.Regions.Asia.West Asia',\n",
    "       'Geography.Regions.Asia.Asia_',\n",
    "       'Geography.Regions.Americas.Central America',\n",
    "       'Geography.Regions.Europe.Southern Europe',\n",
    "       'Geography.Regions.Africa.Africa_',\n",
    "       'Geography.Regions.Asia.Central Asia',\n",
    "       'History and Society.Business and economics', 'STEM.STEM_',\n",
    "       'Culture.Media.Video games', 'Culture.Media.Software',\n",
    "       'Geography.Regions.Americas.South America',\n",
    "       'Culture.Biography.Biography_', 'Culture.Visual arts.Comics and Anime',\n",
    "       'Geography.Regions.Africa.Western Africa',\n",
    "       'Geography.Regions.Africa.Southern Africa', 'Culture.Performing arts',\n",
    "       'STEM.Physics', 'Culture.Linguistics', 'Culture.Internet culture',\n",
    "       'Culture.Biography.Women', 'STEM.Technology', 'STEM.Medicine _ Health',\n",
    "       'Culture.Media.Television', 'Culture.Philosophy and religion',\n",
    "       'Culture.Visual arts.Fashion',\n",
    "       'Geography.Regions.Europe.Western Europe',\n",
    "       'Geography.Regions.Asia.Southeast Asia', 'Culture.Media.Radio',\n",
    "       'Culture.Media.Books', 'Culture.Literature',\n",
    "       'Geography.Regions.Asia.South Asia', 'STEM.Computing',\n",
    "       'Culture.Food and drink', 'Geography.Geographical',\n",
    "       'Culture.Visual arts.Architecture',\n",
    "       'Geography.Regions.Africa.Eastern Africa',\n",
    "       'Geography.Regions.Asia.East Asia', 'STEM.Earth and environment',\n",
    "       'History and Society.Transportation', 'STEM.Chemistry',\n",
    "       'Culture.Media.Films', 'History and Society.History',\n",
    "       'History and Society.Military and warfare', 'Culture.Sports',\n",
    "       'Geography.Regions.Europe.Eastern Europe',\n",
    "       'Culture.Visual arts.Visual arts_', 'Geography.Regions.Asia.North Asia',\n",
    "       'Culture.Media.Media_', 'History and Society.Education',\n",
    "       'Geography.Regions.Africa.Northern Africa',\n",
    "       'Geography.Regions.Europe.Northern Europe',\n",
    "       'Geography.Regions.Europe.Europe_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip columns prefix to make it easier to read\n",
    "topic_df.columns  = [x.split(\".\")[-1] for x in topic_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Entertainment', 'Space', 'Mathematics',\n",
    "       'Central Africa', 'North America', 'Society', 'Oceania', 'Engineering',\n",
    "       'Libraries _ Information', 'Politics and government', 'Biology',\n",
    "       'Music', 'West Asia', 'Asia_', 'Central America', 'Southern Europe',\n",
    "       'Africa_', 'Central Asia', 'Business and economics', 'STEM_',\n",
    "       'Video games', 'Software', 'South America', 'Biography_',\n",
    "       'Comics and Anime', 'Western Africa', 'Southern Africa',\n",
    "       'Performing arts', 'Physics', 'Linguistics', 'Internet culture',\n",
    "       'Women', 'Technology', 'Medicine _ Health', 'Television',\n",
    "       'Philosophy and religion', 'Fashion', 'Western Europe',\n",
    "       'Southeast Asia', 'Radio', 'Books', 'Literature', 'South Asia',\n",
    "       'Computing', 'Food and drink', 'Geographical', 'Architecture',\n",
    "       'Eastern Africa', 'East Asia', 'Earth and environment',\n",
    "       'Transportation', 'Chemistry', 'Films', 'History',\n",
    "       'Military and warfare', 'Sports', 'Eastern Europe', 'Visual arts_',\n",
    "       'North Asia', 'Media_', 'Education', 'Northern Africa',\n",
    "       'Northern Europe', 'Europe_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics corresponding to probabilities bigger than 0.7\n",
    "topic_df[topics] = topic_df[topics].where(topic_df[topics] >= 0.7, np.nan)\n",
    "topic_df[topics] = topic_df[topics].where(topic_df[topics].isna(), topics)\n",
    "topic_df = topic_df.set_index(['page_id', 'page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack to DataFrame to have each page appearing in multiple rows with all associated topics\n",
    "topic_df = topic_df.stack().reset_index()\n",
    "\n",
    "topic_df = topic_df[['page_id', 'page_title', 'level_2']]\n",
    "topic_df.columns = ['page_id', 'page_title', 'topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the topics related to Genghis Khan below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>West Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>Asia_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>Central Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>East Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>Military and warfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Cengiz_Han</td>\n",
       "      <td>North Asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  page_id  page_title                 topic\n",
       "0      10  Cengiz_Han               Society\n",
       "1      10  Cengiz_Han             West Asia\n",
       "2      10  Cengiz_Han                 Asia_\n",
       "3      10  Cengiz_Han          Central Asia\n",
       "4      10  Cengiz_Han             East Asia\n",
       "5      10  Cengiz_Han               History\n",
       "6      10  Cengiz_Han  Military and warfare\n",
       "7      10  Cengiz_Han            North Asia"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame\n",
    "topic_df.to_csv('/dlabdata1/turkish_wiki/processed_data/thresholded_topics.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.page_id = topic_df.page_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Get edits by topic\n",
    "Get daily number of edits by topic. \n",
    "\n",
    "DataFrame saved at ```../processed_data/daily_edits_by_topic.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic = pd.merge(daily_edits, topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic = daily_edits_by_topic.groupby(['date', 'topic'])['event_user_id'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic = daily_edits_by_topic.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic.columns = ['date', 'topic', 'number_of_edits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic['date'] = pd.to_datetime(daily_edits_by_topic['date'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic = daily_edits_by_topic.set_index(['date', 'topic'])\n",
    "idx = pd.date_range(daily_edits_by_topic.index.levels[0].min(), daily_edits_by_topic.index.levels[0].max())\n",
    "\n",
    "daily_edits_by_topic = daily_edits_by_topic.reindex(\n",
    "        pd.MultiIndex.from_product([idx, daily_edits_by_topic.index.levels[1]], \n",
    "                                   names=['date', 'topic']), fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits_by_topic.to_csv('/dlabdata1/turkish_wiki/processed_data/daily_edits_by_topic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Get revert rate by topic\n",
    "Get the daily revert rate to all articles of all topics. \n",
    "DataFrame saved at ```../processed_data/revert_rate_by_topic.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits = pd.read_csv('/dlabdata1/turkish_wiki/processed_data/edits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reverts = pd.read_csv('/dlabdata1/turkish_wiki/processed_data/df_reverts_by_pageid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_edits = daily_edits[daily_edits['user_kind'] != 'bot'].groupby(['date', 'page_id'])[['event_user_id']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reverts = df_reverts.groupby(['date', 'page_id'])['revision_is_identity_revert'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = pd.merge(df_reverts, daily_edits, on=['date', 'page_id'], how= 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = revert_rate.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = pd.merge(revert_rate, topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = revert_rate.groupby(['date', 'topic'])[['revision_is_identity_revert', 'event_user_id']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate['revert_rate'] = revert_rate['revision_is_identity_revert']/revert_rate['event_user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = revert_rate.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = revert_rate[['date', 'topic', 'revert_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate['date'] = pd.to_datetime(revert_rate['date'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate = revert_rate.set_index(['date', 'topic'])\n",
    "idx = pd.date_range(revert_rate.index.levels[0].min(), revert_rate.index.levels[0].max())\n",
    "\n",
    "revert_rate = revert_rate.reindex(\n",
    "        pd.MultiIndex.from_product([idx, revert_rate.index.levels[1]], \n",
    "                                   names=['date', 'topic']), fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "revert_rate.to_csv('/dlabdata1/turkish_wiki/processed_data/revert_rate_by_topic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
