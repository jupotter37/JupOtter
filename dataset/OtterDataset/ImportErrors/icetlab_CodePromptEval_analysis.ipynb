{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read jsonl file with delimiter\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# function to read csv file with delimiter\n",
    "# def read_csv(file_path, delimiter='$'):\n",
    "#     data = []\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         reader = csv.reader(file, delimiter=delimiter)\n",
    "#         for row in reader:\n",
    "#             data.append(row)\n",
    "#     return data\n",
    "\n",
    "\n",
    "def read_csv(file_path, delimiter=','):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found: {}\".format(file_path))\n",
    "        return None\n",
    "    return pd.read_csv(file_path, delimiter=delimiter)\n",
    "\n",
    "def write_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "def plot_dict_old(dictionary, title, xlabel, ylabel, width=15, height=7, same_color=True, color='gray'):\n",
    "\n",
    "    # 32 pastel colors\n",
    "    colors = ['purple', 'brown', 'pink', 'teal', 'olive', 'cyan', 'blue', 'orange', 'green', 'red', 'yellow', 'gray', 'maroon', 'lime', 'aqua', 'fuchsia', 'silver', 'skyblue']\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.xticks(rotation=90)\n",
    "    dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    if not same_color:\n",
    "        color_dict = {}\n",
    "        for key, value in dictionary.items():\n",
    "            if value not in color_dict:\n",
    "                color_dict[value] = colors.pop(0)\n",
    "            plt.bar(key, value, color=color_dict[value], width=0.5)\n",
    "            plt.bar(key, value, width=0.5)\n",
    "\n",
    "    else: \n",
    "        \n",
    "        plt.bar(dictionary.keys(), dictionary.values(), width=0.5, color=color)\n",
    "\n",
    "    # display value on the bar\n",
    "    for key, value in dictionary.items():\n",
    "        # value = round(value, 2)        \n",
    "        plt.text(key, value, str(value), ha='center')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    file_name = title.replace(' ', '_').replace('/', '_') + '.png'\n",
    "    path = '../../results/gpt4/plots/' + file_name\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# function to plot a dictionary as a bar chart\n",
    "def plot_dict(dictionary, title, xlabel, ylabel, width=15, height=7, same_color=True, color='gray'):\n",
    "    colors = ['purple', 'brown', 'pink', 'teal', 'olive', 'cyan', 'blue', 'orange', 'green', 'red', 'yellow', 'gray', 'maroon', 'lime', 'aqua', 'fuchsia', 'silver', 'skyblue']\n",
    "    \n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Extract means and standard deviations\n",
    "    keys = list(dictionary.keys())\n",
    "    means = [dictionary[key]['mean'] for key in keys]\n",
    "    std_devs = [dictionary[key]['std_dev'] for key in keys]\n",
    "    \n",
    "    if same_color:\n",
    "        color = colors.pop(0)\n",
    "        bar_colors = [color] * len(keys)\n",
    "    else:\n",
    "        bar_colors = colors[:len(keys)]  # Assign different colors\n",
    "\n",
    "    # Calculate asymmetric error bars\n",
    "    lower_errors = [min(mean, std_dev) for mean, std_dev in zip(means, std_devs)]  # Prevent negative values\n",
    "    upper_errors = std_devs  # No adjustment needed for upper errors\n",
    "    error_bars = [lower_errors, upper_errors]\n",
    "\n",
    "    # Create bar plot with asymmetric error bars\n",
    "    plt.bar(keys, means, yerr=error_bars, color=bar_colors, capsize=5)\n",
    "    \n",
    "    # Display mean values on the bar plot\n",
    "    for i, (key, mean) in enumerate(zip(keys, means)):\n",
    "        plt.text(i, mean, f'{mean:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    file_name = title.replace(' ', '_').replace('/', '_') + '.png'\n",
    "    path = '../../results/gpt4/plots/' + file_name\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def extract_function_name(groundtruth_code):\n",
    "    groundtruth_code = textwrap.dedent(groundtruth_code)\n",
    "    tree = ast.parse(groundtruth_code)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            return node.name\n",
    "    return None\n",
    "\n",
    "def add_list_to_dict(dictionary, key, value):\n",
    "    if key in dictionary:\n",
    "        dictionary[key].append(value)\n",
    "    else:\n",
    "        dictionary[key] = [value]\n",
    "\n",
    "\n",
    "def average_results(version):\n",
    "\n",
    "    directory = '../../results/gpt4/evaluation_results/'\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if '_v'+ str(version) in filename and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path, delimiter='$')\n",
    "            dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        concatenated_df = pd.concat(dataframes, ignore_index=True)  # row-wise concatenation\n",
    "        \n",
    "        # Identify numeric columns\n",
    "        numeric_columns = concatenated_df.select_dtypes(include='number').columns\n",
    "        # Identify non-numeric columns\n",
    "        non_numeric_columns = concatenated_df.select_dtypes(exclude='number').columns\n",
    "        \n",
    "        # Apply mean to numeric columns\n",
    "        averaged_numeric_df = concatenated_df.groupby('comb_id')[numeric_columns].mean().reset_index()\n",
    "        \n",
    "        # Take the first non-null value for each non-numeric column\n",
    "        first_non_null = lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "        averaged_non_numeric_df = concatenated_df.groupby('comb_id')[non_numeric_columns].agg(first_non_null).reset_index()\n",
    "        \n",
    "        # Merge the numeric and non-numeric dataframes\n",
    "        averaged_df = pd.merge(averaged_numeric_df, averaged_non_numeric_df, on='comb_id')\n",
    "        \n",
    "        # Save the result to a CSV file\n",
    "\n",
    "        averaged_df.to_csv('../../results/gpt4/evaluation_results/test_results_codereval_AVERAGED_v' + str(version) + '.csv', index=False, delimiter='$')\n",
    "\n",
    "    print(\"Averaged CSV file created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to detect outliers (points more than 'threshold' standard deviations from the mean)\n",
    "def detect_outliers(data, mean, std_dev, threshold=2):\n",
    "    outliers = [x for x in data if abs(x - mean) > threshold * std_dev]\n",
    "    return outliers\n",
    "\n",
    "# Function to plot a dictionary as a bar chart with outliers\n",
    "def plot_dict_with_outliers(dictionary, title, xlabel, ylabel, width=15, height=7, same_color=True, color='gray', threshold=2):\n",
    "    colors = ['purple', 'brown', 'pink', 'teal', 'olive', 'cyan', 'blue', 'orange', 'green', 'red', 'yellow', 'gray', 'maroon', 'lime', 'aqua', 'fuchsia', 'silver', 'white']\n",
    "    \n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Extract means, standard deviations, and data values\n",
    "    keys = list(dictionary.keys())\n",
    "    means = [dictionary[key]['mean'] for key in keys]\n",
    "    std_devs = [dictionary[key]['std_dev'] for key in keys]\n",
    "    values = [dictionary[key]['data'] for key in keys]  # Assuming raw data is stored in 'data'\n",
    "\n",
    "    if same_color:\n",
    "        color = colors.pop(0)\n",
    "        bar_colors = [color] * len(keys)\n",
    "    else:\n",
    "        bar_colors = colors[:len(keys)]  # Assign different colors\n",
    "\n",
    "    lower_errors = [min(mean, std_dev) for mean, std_dev in zip(means, std_devs)]  # Prevent negative values\n",
    "    upper_errors = std_devs  # No adjustment needed for upper errors\n",
    "    error_bars = [lower_errors, upper_errors]\n",
    "\n",
    "    plt.bar(keys, means, yerr=error_bars, color=bar_colors, capsize=5)\n",
    "\n",
    "    for i, (key, mean, std_dev, value_list) in enumerate(zip(keys, means, std_devs, values)):\n",
    "        outliers = detect_outliers(value_list, mean, std_dev, threshold)\n",
    "        \n",
    "        if outliers:  # If there are any outliers\n",
    "            plt.scatter([keys[i]] * len(outliers), outliers, color='red', zorder=5, label='Outliers' if i == 0 else \"\")  # Add label only once for the legend\n",
    "\n",
    "    for i, (key, mean) in enumerate(zip(keys, means)):\n",
    "        plt.text(i, mean, f'{mean:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    # Save and show the plot\n",
    "    file_name = title.replace(' ', '_').replace('/', '_') + '.png'\n",
    "    path = '../../results/gpt4/plots/' + file_name\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_results(version):\n",
    "    directory = '../../results/gpt4/evaluation_results/'\n",
    "    dataframes = []\n",
    "    column_order = None\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if '_v'+ str(version) in filename and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path, delimiter='$')\n",
    "            dataframes.append(df)\n",
    "            if column_order is None:\n",
    "                column_order = df.columns.tolist()  # Save the column order from the first DataFrame\n",
    "\n",
    "    if dataframes:\n",
    "        concatenated_df = pd.concat(dataframes, ignore_index=True)  # row-wise concatenation\n",
    "        \n",
    "        numeric_columns = concatenated_df.select_dtypes(include='number').columns.difference(['comb_id'])\n",
    "        non_numeric_columns = concatenated_df.select_dtypes(exclude='number').columns\n",
    "        \n",
    "        averaged_numeric_df = concatenated_df.groupby('comb_id')[numeric_columns].mean().reset_index()\n",
    "        first_non_null = lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "        averaged_non_numeric_df = concatenated_df.groupby('comb_id')[non_numeric_columns].agg(first_non_null).reset_index()\n",
    "        \n",
    "        averaged_df = pd.merge(averaged_numeric_df, averaged_non_numeric_df, on='comb_id')\n",
    "        averaged_df = averaged_df[column_order]\n",
    "        \n",
    "        averaged_df.to_csv(f'../../results/gpt4/evaluation_results/test_results_codereval_AVERAGED_v{version}.csv', index=False, sep='$')\n",
    "        print(\"Averaged CSV file created successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def average_dict(dict_to_average):\n",
    "    for key, value in dict_to_average.items():\n",
    "        mean = sum(value) / len(value)\n",
    "        variance = sum((x - mean) ** 2 for x in value) / len(value)\n",
    "        std_dev = math.sqrt(variance)\n",
    "        dict_to_average[key] = {\n",
    "            'median': np.median(value),\n",
    "            'mean': mean,\n",
    "            'std_dev': std_dev,\n",
    "            'data': value\n",
    "        }\n",
    "    return dict_to_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def check_lints(code_string, code_type=\"Original\"):\n",
    "    \"\"\"\n",
    "    Function to check linting for the given Python code string using pylint.\n",
    "    :param code_string: The Python code as a string.\n",
    "    :param code_type: Type of code (\"Original\" or \"Generated\") for display purposes.\n",
    "    :return: None\n",
    "    ignores #C0114, C0116, C0415 lints\n",
    "    \"\"\"\n",
    "    print(f\"\\nLinting {code_type} code:\")\n",
    "    # dedent the code string\n",
    "    code_string = textwrap.dedent(code_string)\n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as temp_file:\n",
    "        temp_file.write(code_string.encode('utf-8'))\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        # Run pylint on the temporary file and capture both stdout and stderr\n",
    "        result = subprocess.run(\n",
    "            [\"pylint\", temp_file_path, \"--output-format=text\"],  # Force plain text output\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        # Print the linting output from stdout and stderr\n",
    "        if result.stdout:\n",
    "            \n",
    "            \n",
    "            # remove lints that have the codes: C0114, C0116, C0415\n",
    "            list_of_lints = result.stdout.split('\\n')[1:] # this to skip the first line\n",
    "            print(\"STDOUT:\")\n",
    "            print(list_of_lints)\n",
    "\n",
    "            lint_res = result.stdout\n",
    "            if len(list_of_lints) > 0:\n",
    "                for lint in list_of_lints:\n",
    "                    if 'C0114' in lint or 'C0116' in lint or 'C0415' in lint:\n",
    "                        list_of_lints.remove(lint)\n",
    "                    lint = ':'.join(lint.split(':')[1:])\n",
    "                lint_res = '\\n'.join(list_of_lints)\n",
    "            \n",
    "            else:\n",
    "                lint_res = \"No linting issues found.\"\n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\")\n",
    "            print(result.stderr)\n",
    "\n",
    "        # Determine if linting issues were found\n",
    "        if result.returncode != 0:\n",
    "            print(f\"\\nLinting issues found in {code_type} code.\")\n",
    "        else:\n",
    "            print(f\"\\nNo linting issues found in {code_type} code.\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        lint_res = \"Linting took too long and was terminated.\"\n",
    "    \n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "    return lint_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclomatic_complexity(code_string):\n",
    "    # dedent the code string\n",
    "    code_string = textwrap.dedent(code_string)\n",
    "    # Write the code string to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as temp_file:\n",
    "        temp_file.write(code_string.encode('utf-8'))\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "    try:\n",
    "        # Run radon via subprocess and capture the output\n",
    "        result = subprocess.run(\n",
    "            [\"radon\", \"cc\", temp_file_path, \"-s\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Print the radon output\n",
    "        complexity = None\n",
    "        if result.stdout:\n",
    "            complexity = result.stdout.split('(')[1].split(')')[0]\n",
    "            print(\"Cyclomatic complexity:\", result.stdout)\n",
    "        if result.stderr:\n",
    "            print(\"Error:\", result.stderr)\n",
    "    \n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.remove(temp_file_path)\n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cognitive_complexity\n",
    "from cognitive_complexity.api import get_cognitive_complexity\n",
    "import ast\n",
    "# %pip install complexipy\n",
    "from complexipy import code_complexity\n",
    "\n",
    "def cognitive_complexity(code_string):\n",
    "    code_dedented = textwrap.dedent(code_string)\n",
    "    try:\n",
    "        funcdef = ast.parse(code_dedented).body[0]\n",
    "        return get_cognitive_complexity(funcdef)\n",
    "    except Exception as e:\n",
    "        print(\"Could not parse, trying with complexipy\")\n",
    "        try:\n",
    "            return code_complexity(code_dedented).complexity\n",
    "        except Exception as e:\n",
    "            print(\"Could not be parsed:\", e)\n",
    "            return None\n",
    "        \n",
    "\n",
    "code =\"\"\"\n",
    "    def f(self, a):\n",
    "        return a * f(a - x)  # +1 for recursion\n",
    "    \"\"\"\n",
    "\n",
    "cognitive_complexity(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# function to plot a dictionary as a violin plot\n",
    "def plot_dict_violin(dictionary, title, xlabel, ylabel, width=15, height=7, model='gpt4'):\n",
    "    # Extract keys and data (list of values) from the dictionary\n",
    "    keys = list(dictionary.keys())\n",
    "    data = [dictionary[key]['values'] for key in keys]  # Assume original data is stored in 'values'\n",
    "    \n",
    "    # Create a figure and set the size\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Convert the data into a format suitable for seaborn's violinplot\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for key, values in zip(keys, data):\n",
    "        all_data.extend(values)\n",
    "        labels.extend([key] * len(values))\n",
    "    \n",
    "    # Plot violin plot using seaborn\n",
    "    sns.violinplot(x=labels, y=all_data)\n",
    "    \n",
    "    # Customize the plot with titles and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    # Save the plot\n",
    "    file_name = title.replace(' ', '_').replace('/', '_') + '.png'\n",
    "    path = '../../results/' + model + '/plots/' + file_name\n",
    "    plt.savefig(path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_type(error_message):\n",
    "    if \"isT is not True\" in error_message or \"Result is not True\" in error_message or \"Result not True\" in error_message:\n",
    "        error_type = \"AssertionError\"\n",
    "    elif \"Module not found\" in error_message:\n",
    "        error_type = \"ModuleNotFoundError\"\n",
    "        #  everything after ModuleNotFoundError:\n",
    "    elif \"KeyError\" in error_message:\n",
    "        error_type = \"KeyError\"\n",
    "    elif \"NameError\" in error_message:\n",
    "        error_type = \"NameError\"\n",
    "    elif \"SyntaxError\" in error_message:\n",
    "        error_type = \"SyntaxError\"\n",
    "    elif \"TypeError\" in error_message:\n",
    "        error_type = \"TypeError\"\n",
    "    elif \"ValueError\" in error_message:\n",
    "        error_type = \"ValueError\"\n",
    "    elif \"IndexError\" in error_message:\n",
    "        error_type = \"IndexError\"\n",
    "    elif \"AttributeError\" in error_message:\n",
    "        error_type = \"AttributeError\"\n",
    "    elif \"FileNotFoundError\" in error_message:\n",
    "        error_type = \"FileNotFoundError\"\n",
    "    elif \"ImportError\" in error_message:\n",
    "        error_type = \"ImportError\"\n",
    "    elif \"AssertionError\" in error_message:\n",
    "        error_type = \"AssertionError\"\n",
    "    elif \"has no attribute\" in error_message:\n",
    "        error_type = \"AttributeError\"\n",
    "    elif \"cannot import name\" or \"cannot import module\" in error_message:\n",
    "        error_type = \"ImportError\"\n",
    "    else:\n",
    "        error_type = \"Error\"\n",
    "\n",
    "    return error_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "# %pip install scikit-learn\n",
    "\n",
    "def levenshtein_distance(str1, str2):\n",
    "    return Levenshtein.distance(str1, str2)\n",
    "\n",
    "def get_jaccard_similarity_1gram(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams for a given text.\n",
    "    \"\"\"\n",
    "    return {text[i:i+n] for i in range(len(text) - n + 1)}\n",
    "\n",
    "def get_jaccard_similarity(string1, string2, n=3):\n",
    "    \"\"\"\n",
    "    Calculate the Jaccard Index using n-grams.\n",
    "    \"\"\"\n",
    "    # # remove the signature\n",
    "    # string1 = string1.split('\\n', 1)[1]\n",
    "    # string2 = string2.split('\\n', 1)[1]\n",
    "\n",
    "    ngrams1 = generate_ngrams(string1, n)\n",
    "    ngrams2 = generate_ngrams(string2, n)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = ngrams1.intersection(ngrams2)\n",
    "    union = ngrams1.union(ngrams2)\n",
    "    \n",
    "    # Compute Jaccard Index\n",
    "    return len(intersection) / len(union) if union else 0\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(str1, str2):\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    return len(set1.intersection(set2)) / (len(set1) * len(set2))**0.5\n",
    "\n",
    "def compute_distance(str1, str2, method=\"Levenshtein\"):\n",
    "    if method == \"levenshtein\":\n",
    "        return levenshtein_distance(str1, str2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid similarity method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class ASTNodeCounter(ast.NodeVisitor):\n",
    "    \"\"\"Class to count the number of nodes in an AST.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "\n",
    "    def generic_visit(self, node):\n",
    "        \"\"\"Visit a node in the AST and store its type.\"\"\"\n",
    "        self.nodes.append(type(node).__name__)\n",
    "        super().generic_visit(node)\n",
    "\n",
    "def get_ast_from_code(code_string):\n",
    "    \"\"\"Parse code string and return the AST.\"\"\"\n",
    "    return ast.parse(code_string)\n",
    "\n",
    "def get_ast_node_list(tree):\n",
    "    \"\"\"Get a list of all node types in the AST.\"\"\"\n",
    "    node_counter = ASTNodeCounter()\n",
    "    node_counter.visit(tree)\n",
    "    return node_counter.nodes\n",
    "\n",
    "def compute_similarity(nodes1, nodes2):\n",
    "    \"\"\"Compute similarity as a percentage of matching nodes.\"\"\"\n",
    "    set1 = set(nodes1)\n",
    "    set2 = set(nodes2)\n",
    "\n",
    "    # Calculate common nodes\n",
    "    common_nodes = set1.intersection(set2)\n",
    "    \n",
    "    # Similarity score is the ratio of common nodes to the total unique nodes\n",
    "    total_unique_nodes = set1.union(set2)\n",
    "    similarity = len(common_nodes) / len(total_unique_nodes) if total_unique_nodes else 1\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def detect_ast_similarity(code1, code2):\n",
    "    \"\"\"Detect similarity between two code snippets based on their AST structure.\"\"\"\n",
    "    # Parse both code snippets into ASTs\n",
    "    tree1 = get_ast_from_code(code1)\n",
    "    tree2 = get_ast_from_code(code2)\n",
    "\n",
    "    # Get list of node types from each AST\n",
    "    nodes1 = get_ast_node_list(tree1)\n",
    "    nodes2 = get_ast_node_list(tree2)\n",
    "\n",
    "    # Compute similarity score\n",
    "    similarity_score = compute_similarity(nodes1, nodes2)\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "from codebleu.codebleu import calc_codebleu\n",
    "\n",
    "\n",
    "def compute_code_bleu(groundtruth_code, generated_code):\n",
    "    \"\"\"\n",
    "        {\n",
    "        'codebleu': 0.5537, \n",
    "        'ngram_match_score': 0.1041, \n",
    "        'weighted_ngram_match_score': 0.1109, \n",
    "        'syntax_match_score': 1.0, \n",
    "        'dataflow_match_score': 1.0\n",
    "        }\n",
    "    \"\"\"\n",
    "    # remove the signature\n",
    "    groundtruth_code = groundtruth_code.split('\\n', 1)[1]\n",
    "    generated_code = generated_code.split('\\n', 1)[1]\n",
    "    \n",
    "    result = calc_codebleu([groundtruth_code], [generated_code], lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=None)\n",
    "    codebleu = result['codebleu']\n",
    "    syntax_match_score = result['syntax_match_score']\n",
    "    dataflow_match_score = result['dataflow_match_score']\n",
    "\n",
    "    return codebleu, syntax_match_score, dataflow_match_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_similarities_old(dict_of_similarities, title=\"\", xlabel=\"\", ylabel=\"\", n_cols = 4 , width=25, height=5, model='gpt4', color_shades='Oranges'):\n",
    "    n_combinations = len(dict_of_similarities)\n",
    "    n_rows = (n_combinations + 1) // n_cols \n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(width, height * n_rows))  \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    cmap = plt.get_cmap(color_shades)\n",
    "\n",
    "    for idx, (combination, code_bleus) in enumerate(dict_of_similarities.items()):\n",
    "        n, bins, patches = axes[idx].hist(code_bleus, bins=40, histtype='bar', rwidth=0.8)\n",
    "        bin_positions = np.linspace(0, 1, len(patches))\n",
    "        \n",
    "        for patch, color_value in zip(patches, bin_positions):\n",
    "            patch.set_facecolor(cmap(color_value))  # Assign color based on bin position\n",
    "\n",
    "        axes[idx].set_title(title + combination)\n",
    "        axes[idx].set_xlabel(xlabel)\n",
    "        axes[idx].set_ylabel(ylabel)\n",
    "\n",
    "    for i in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_grouped_similarities(dict_ranges_similarities, title=\"\", xlabel=\"\", ylabel=\"\", n_cols = 4 , width=25, height=5, model='gpt4', color_shades='Oranges'):\n",
    "    # dict_ranges_similarities = { \"combination1\" : {\"0-10%\" : [], \"10-20%\" : [] .... }, \"combination2\" : {\"0-10%\" : [], \"10-20%\" : [] .... } }\n",
    "    n_combinations = len(dict_ranges_similarities)\n",
    "    n_rows = (n_combinations + 1) // n_cols \n",
    "\n",
    "    # to know the max y value to make the y axis the same for all plots\n",
    "    max_y_value = 0\n",
    "    for combination, dict_ranges in dict_ranges_similarities.items():\n",
    "        y = [len(dict_ranges[key]) for key in dict_ranges]\n",
    "        max_y_value = max(max_y_value, max(y))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(width, height * n_rows))  \n",
    "    axes = axes.flatten()\n",
    "\n",
    "    cmap = plt.get_cmap(color_shades)\n",
    "\n",
    "    for idx, (combination, dict_ranges) in enumerate(dict_ranges_similarities.items()):\n",
    "        data = dict_ranges\n",
    "        x = list(data.keys())\n",
    "        y = [len(data[key]) for key in x]\n",
    "\n",
    "        bars = axes[idx].bar(x, y, color=cmap(np.linspace(0.3, 1, len(x))))\n",
    "\n",
    "        axes[idx].set_title(title + combination)\n",
    "        axes[idx].set_xlabel(xlabel)\n",
    "        axes[idx].set_ylabel(ylabel)\n",
    "        axes[idx].set_ylim(0, max_y_value)\n",
    "        axes[idx].bar_label(bars, labels=[f'{val}' for val in y], label_type='edge')\n",
    "        \n",
    "    for i in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_to_per_tech_dict(per_tech_dict, score, prompt_technique):\n",
    "    score_ranges = {\n",
    "        \"0 - 10%\": 0.1,\n",
    "        \"10 - 25%\": 0.25,\n",
    "        \"25 - 50%\": 0.5,\n",
    "        \"50 - 100%\": 1.0\n",
    "    }\n",
    "\n",
    "    if prompt_technique not in per_tech_dict:\n",
    "        per_tech_dict[prompt_technique] = {key: [] for key in score_ranges}\n",
    "\n",
    "    if score is not None:\n",
    "        for range_label, threshold in score_ranges.items():\n",
    "            if score <= threshold:\n",
    "                per_tech_dict[prompt_technique][range_label].append(score)\n",
    "                break \n",
    "\n",
    "    return per_tech_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_patterns():\n",
    "    error_patterns = {\n",
    "        \"isT is not True\": \"AssertionError\",\n",
    "        \"Result is not True\": \"AssertionError\",\n",
    "        \"Module not found\": \"ModuleNotFoundError\",\n",
    "        \"KeyError\": \"KeyError\",\n",
    "        \"NameError\": \"NameError\",\n",
    "        \"SyntaxError\": \"SyntaxError\",\n",
    "        \"TypeError\": \"TypeError\",\n",
    "        \"ValueError\": \"ValueError\",\n",
    "        \"IndexError\": \"IndexError\",\n",
    "        \"AttributeError\": \"AttributeError\",\n",
    "        \"FileNotFoundError\": \"FileNotFoundError\",\n",
    "        \"ImportError\": \"ImportError\",\n",
    "        \"has no attribute\": \"AttributeError\",\n",
    "        \"Result not True!!!\": \"AssertionError\",\n",
    "        \"cannot import name\": \"ImportError\",\n",
    "        \"cannot import module\": \"ImportError\"\n",
    "    }\n",
    "    return error_patterns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_value_in_dict(dictionary, key):\n",
    "    if key in dictionary:\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 14\n",
    "model = \"mistral\"\n",
    "can_average = False\n",
    "\n",
    "if can_average:\n",
    "    data_path = \"../../results/\" + model + \"/evaluation_results/test_results_codereval_AVERAGED_v\" + str(version) + \".csv\"\n",
    "    average_results(version)\n",
    "    data = read_csv(data_path, delimiter=',')\n",
    "else:\n",
    "    data_path = \"../../results/\" + model + \"/evaluation_results/evaluation_codereval_v\" + str(version) + \".csv\"\n",
    "    data = read_csv(data_path, delimiter=',')\n",
    "\n",
    "# get data with error message is nan and test result is Failed\n",
    "# make nan as Failed test)results with \"SyntaxError: Invalid generated code\" error_message \n",
    "data['error_message'] = data['error_message'].fillna(\"SyntaxError: Invalid generated code\")\n",
    "data['test_result'] = data['test_result'].fillna(\"Failed\")\n",
    "\n",
    "# x = data[data['test_result'].isna()]\n",
    "x = data[data['test_result'] == 'Failed']\n",
    "# x = x['groundtruth_code']\n",
    "# len(data) - len(x)\n",
    "len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_file = data_path.replace('.csv', '_analysis.csv')\n",
    "\n",
    "# read in dataframe\n",
    "df = pd.read_csv(analysis_file, delimiter=',')\n",
    "# for each row, calculate the similarity between the groundtruth and generated code\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row[\"test_result\"]):\n",
    "        print(\"Skipping row with empty test result for id: \", row['task_id'])\n",
    "        continue\n",
    "        \n",
    "    groundtruth_code = row['groundtruth_code']\n",
    "    generated_code = row['generated_code']\n",
    "    # remove the signature from both codes\n",
    "    groundtruth_code = groundtruth_code.split(':', 1)[1]\n",
    "    generated_code = generated_code.split(':', 1)[1]\n",
    "    \n",
    "\n",
    "    jaccard_similarity = get_jaccard_similarity(groundtruth_code, generated_code)\n",
    "    df.at[i, 'jaccard_similarity'] = jaccard_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a new csv file\n",
    "new_analysis_file = analysis_file.replace('.csv', '_trial.csv')\n",
    "df.to_csv(new_analysis_file, index=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"\"\"\n",
    "def xy (one,\n",
    "       two,\n",
    "       three):\n",
    "    return one + two + three\n",
    "\"\"\".split(':', 1)[1]\n",
    "\n",
    "y = \"\"\"\n",
    "def x (one,\n",
    "       two,\n",
    "       three):\n",
    "    return one + two + three\n",
    "\"\"\".split(':', 1)[1]\n",
    "\n",
    "print(y)\n",
    "print(x)\n",
    "print(get_jaccard_similarity(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "errors_types = {}\n",
    "stats_per_task = {}\n",
    "stats_per_tech = {}\n",
    "error_messages = []\n",
    "coverage_per_tech = {}\n",
    "stmts_coverage_per_tech = {}\n",
    "lexical_distance_per_tech = {}\n",
    "assert_failed_per_tech = {}\n",
    "ast_similarity_per_tech = {}\n",
    "semantic_similarity_per_tech = {}\n",
    "code_bleu_per_tech = {}\n",
    "test_output_per_comb = {}\n",
    "overall_similarity_histo = {}\n",
    "flow_similarity_histo = {}\n",
    "syntax_similarity_histo = {}\n",
    "\n",
    "\n",
    "data[\"error_type\"] = \"\"\n",
    "data[\"codebleu\"] = \"\"\n",
    "data[\"syntax_similarity\"] = \"\"\n",
    "data[\"flow_similarity\"] = \"\"\n",
    "data[\"jaccard_similarity\"] = \"\"\n",
    "data[\"lint_generated\"] = \"\"\n",
    "data[\"lint_groundtruth\"] = \"\"\n",
    "data[\"cyclo_complexity_generated\"] = \"\"\n",
    "data[\"cyclo_complexity_groundtruth\"] = \"\"\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    task_id, comb_text, prompt, project, file_path, groundtruth_code, tests, packages, function_name, test_name, level, test_file_path, original_prompt_length, is_zero_shot, is_few_shot, is_chain_of_thought, is_persona, is_packages, is_signature, zero_shot_prompt, test_result, error_message, prompt_technique, generated_code, complete_response = row[\"task_id\"], row[\"combination_id\"], row[\"prompt\"], row[\"project\"], row[\"file_path\"], row[\"groundtruth_code\"], row[\"tests\"], row[\"packages\"], row[\"function_name\"], row[\"test_name\"], row[\"level\"], row[\"test_file_path\"], row[\"original_prompt_length\"], row[\"is_zero_shot\"], row[\"is_few_shot\"], row[\"is_chain_of_thought\"], row[\"is_persona\"], row[\"is_packages\"], row[\"is_signature\"], row[\"zero_shot_prompt\"], row[\"test_result\"], row[\"error_message\"], row[\"prompt_techniques_applied\"], row[\"generated_code\"], row[\"complete_response\"]\n",
    "    if test_result == \"\":\n",
    "        print(f\"Skipping task {task_id} with combination {comb_text}: Generated code could not be extracted or compiled.\")\n",
    "        continue\n",
    "\n",
    "    if generated_code == \"\":\n",
    "        print(\"Empty generated code for task: \", task_id)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    error_type = \"\"\n",
    "    normalized_error_message = \"\"\n",
    "    codebleu = None\n",
    "    syntax_similarity = None\n",
    "    flow_similarity = None\n",
    "    jaccard_similarity = None\n",
    "    cyclo_complexity_generated = None\n",
    "    cyclo_complexity_groundtruth = None\n",
    "    cognitive_complexity_generated = None\n",
    "    cognitive_complexity_groundtruth = None\n",
    "    lint_generated = None\n",
    "    lint_groundtruth = None\n",
    "\n",
    "    if test_result == \"Failed\":\n",
    "        error_messages.append(error_message)\n",
    "        print(\"Error message: \", error_message)\n",
    "        error_type = get_error_type(error_message)\n",
    "        normalized_error_message = error_message.split(error_type + \":\")\n",
    "\n",
    "        if len(normalized_error_message) > 1:\n",
    "            normalized_error_message = normalized_error_message[-1].strip()\n",
    "            normalized_error_message = error_type + \": \" + normalized_error_message\n",
    "        else:\n",
    "            normalized_error_message = error_type + \": \" + error_message\n",
    "            if error_type == \"AssertionError\":\n",
    "                normalized_error_message = \"AssertionError: Result is not True\"\n",
    "        \n",
    "\n",
    "        # most common error type\n",
    "        increment_value_in_dict(errors_types, error_type)\n",
    "        \n",
    "        # the task that failed the most\n",
    "        function_name = extract_function_name(groundtruth_code)        \n",
    "        increment_value_in_dict(stats_per_task, function_name)\n",
    "\n",
    "        # the combination with the most failed tests\n",
    "        # if error_type == \"AssertionError\":\n",
    "        increment_value_in_dict(stats_per_tech, prompt_technique)\n",
    "\n",
    "\n",
    "        if comb_text in test_output_per_comb:\n",
    "            increment_value_in_dict(test_output_per_comb[comb_text], error_type)\n",
    "        else:\n",
    "            test_output_per_comb[comb_text] = {error_type: 1}\n",
    "        \n",
    "    if test_result == \"Passed\":\n",
    "        if comb_text in test_output_per_comb:\n",
    "            increment_value_in_dict(test_output_per_comb[comb_text], test_result)\n",
    "        else:\n",
    "            test_output_per_comb[comb_text] = {test_result: 1}\n",
    "\n",
    "        # check linting and complexity\n",
    "        lint_generated = check_lints(generated_code, code_type=\"Generated\")\n",
    "        lint_groundtruth = check_lints(groundtruth_code, code_type=\"Groundtruth\")\n",
    "\n",
    "        cyclo_complexity_generated = cyclomatic_complexity(generated_code)\n",
    "        cyclo_complexity_groundtruth = cyclomatic_complexity(groundtruth_code)\n",
    "\n",
    "        cognitive_complexity_generated = cognitive_complexity(generated_code)\n",
    "        cognitive_complexity_groundtruth = cognitive_complexity(groundtruth_code)\n",
    "\n",
    "\n",
    "\n",
    "    # similarity measures\n",
    "    try:\n",
    "        jaccard_similarity = get_jaccard_similarity(groundtruth_code, generated_code)\n",
    "    except:\n",
    "        print(\"Jaccard failed for the codes: \", groundtruth_code, generated_code)\n",
    "        jaccard_similarity = None\n",
    "\n",
    "    try:\n",
    "        code_bleu, syntax_similarity, flow_similarity = compute_code_bleu(groundtruth_code, generated_code)\n",
    "    except:\n",
    "        print(\"CodeBLEU failed for the codes: \", groundtruth_code, generated_code)\n",
    "        code_bleu = None\n",
    "        syntax_similarity = None\n",
    "        flow_similarity = None\n",
    "\n",
    "    if generated_code is not None and generated_code != \"null\" and code_bleu is not None:\n",
    "\n",
    "        overall_similarity_histo = add_score_to_per_tech_dict(overall_similarity_histo, code_bleu, prompt_technique)\n",
    "        syntax_similarity_histo = add_score_to_per_tech_dict(syntax_similarity_histo, syntax_similarity, prompt_technique)\n",
    "        flow_similarity_histo = add_score_to_per_tech_dict(flow_similarity_histo, flow_similarity, prompt_technique)\n",
    "\n",
    "        if prompt_technique in code_bleu_per_tech:\n",
    "            code_bleu_per_tech[prompt_technique].append(float(code_bleu))\n",
    "            lexical_distance_per_tech[prompt_technique].append(float(syntax_similarity))\n",
    "            ast_similarity_per_tech[prompt_technique].append(float(flow_similarity))\n",
    "        else:\n",
    "            code_bleu_per_tech[prompt_technique] = [float(code_bleu)]\n",
    "            lexical_distance_per_tech[prompt_technique] = [float(syntax_similarity)]\n",
    "            ast_similarity_per_tech[prompt_technique] = [float(flow_similarity)]\n",
    "\n",
    "    data.at[index, 'error_type'] = error_type\n",
    "    data.at[index, 'normalized_error_message'] = normalized_error_message\n",
    "    data.at[index, 'codebleu'] = code_bleu\n",
    "    data.at[index, 'syntax_similarity'] = syntax_similarity\n",
    "    data.at[index, 'flow_similarity'] = flow_similarity\n",
    "    data.at[index, 'jaccard_similarity'] = jaccard_similarity\n",
    "    data.at[index, 'lint_generated'] = lint_generated\n",
    "    data.at[index, 'lint_groundtruth'] = lint_groundtruth\n",
    "    data.at[index, 'cyclo_complexity_generated'] = cyclo_complexity_generated\n",
    "    data.at[index, 'cyclo_complexity_groundtruth'] = cyclo_complexity_groundtruth\n",
    "    data.at[index, 'cognitive_complexity_generated'] = cognitive_complexity_generated\n",
    "    data.at[index, 'cognitive_complexity_groundtruth'] = cognitive_complexity_groundtruth\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'eval_output_{version}.txt', 'w') as file:\n",
    "    file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# uncomment when you want to add the similarity values to evaluation_results.csv\n",
    "output_analysis_file = data_path.replace('.csv', '_analysis.csv')\n",
    "write_csv(data, output_analysis_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_grouped_similarities(overall_similarity_histo, title=\"Code BLEU for \", xlabel=\"Code BLEU\", ylabel=\"Frequency\", model=model)\n",
    "plot_grouped_similarities(syntax_similarity_histo, title=\"Syntax similarity for \", xlabel=\"Syntax similarity\", ylabel=\"Frequency\", model=model, color_shades='Blues')\n",
    "plot_grouped_similarities(flow_similarity_histo, title=\"Flow similarity for \", xlabel=\"Flow similarity\", ylabel=\"Frequency\", model=model, color_shades='Purples')\n",
    "\n",
    "\n",
    "# plot_grouped_similarities_old(code_bleu_per_tech, title=\"Code BLEU for \" , xlabel=\"Code BLEU\", ylabel=\"Frequency\", model=model)\n",
    "# plot_grouped_similarities_old(lexical_distance_per_tech, title=\"Syntax similarity for \" , xlabel=\"Syntax similarity\", ylabel=\"Frequency\", model=model, color_shades='Blues')\n",
    "# plot_grouped_similarities_old(ast_similarity_per_tech, title=\"Flow similarity for \" , xlabel=\"Flow similarity\", ylabel=\"Frequency\", model=model, color_shades='Purples')\n",
    "\n",
    "\n",
    "# plot lexical_distance_per_tech, ast_similarity_per_tech, semantic_similarity_per_tech in one grouped bar chart side to side\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.xticks(rotation=90)\n",
    "width = 0.25\n",
    "keys = list(lexical_distance_per_tech.keys())\n",
    "\n",
    "means_lexical = [np.mean(lexical_distance_per_tech[key]) for key in keys]\n",
    "std_devs_lexical = [np.std(lexical_distance_per_tech[key]) for key in keys]\n",
    "means_ast = [np.mean(ast_similarity_per_tech[key]) for key in keys]\n",
    "std_devs_ast = [np.std(ast_similarity_per_tech[key]) for key in keys]\n",
    "means_semantic = [np.mean(code_bleu_per_tech[key]) for key in keys]\n",
    "std_devs_semantic = [np.std(code_bleu_per_tech[key]) for key in keys]\n",
    "\n",
    "lower_errors_lexical = [min(mean, std_dev) for mean, std_dev in zip(means_lexical, std_devs_lexical)]  # Prevent negative values\n",
    "upper_errors_lexical = std_devs_lexical  \n",
    "error_bars_lexical = [lower_errors_lexical, upper_errors_lexical]\n",
    "\n",
    "lower_errors_ast = [min(mean, std_dev) for mean, std_dev in zip(means_ast, std_devs_ast)] \n",
    "upper_errors_ast = std_devs_ast  \n",
    "error_bars_ast = [lower_errors_ast, upper_errors_ast]\n",
    "\n",
    "lower_errors_semantic = [min(mean, std_dev) for mean, std_dev in zip(means_semantic, std_devs_semantic)]  \n",
    "upper_errors_semantic = std_devs_semantic  \n",
    "error_bars_semantic = [lower_errors_semantic, upper_errors_semantic]\n",
    "\n",
    "# Create bar plot with error bars\n",
    "plt.bar(np.arange(len(keys)) - width, means_lexical, yerr=error_bars_lexical, width=width, label='Lexical Distance', color='blue')\n",
    "plt.bar(np.arange(len(keys)), means_ast, yerr=error_bars_ast, width=width, label='AST Similarity', color='orange')\n",
    "plt.bar(np.arange(len(keys)) + width, means_semantic, yerr=error_bars_semantic, width=width, label='Code Bleu', color='green')\n",
    "\n",
    "# Display mean values on the bar plot\n",
    "for i, (key, mean_lexical, mean_ast, mean_semantic) in enumerate(zip(keys, means_lexical, means_ast, means_semantic)):\n",
    "    plt.text(i - width, mean_lexical, f'{mean_lexical:.2f}', ha='center', va='bottom', color='black')\n",
    "    plt.text(i, mean_ast, f'{mean_ast:.2f}', ha='center', va='bottom', color='black')\n",
    "    plt.text(i + width, mean_semantic, f'{mean_semantic:.2f}', ha='center', va='bottom', color='black')\n",
    "\n",
    "# Set up legend, title, and labels\n",
    "plt.legend()\n",
    "plt.title(\"Similarity Measures per Prompt Technique\")\n",
    "plt.xlabel(\"Prompt Technique\")\n",
    "plt.ylabel(\"Similarity\")\n",
    "plt.xticks(range(len(keys)), keys)\n",
    "\n",
    "# Save and show the plot\n",
    "file_name = \"Similarity_Measures_per_Prompt_Technique.png\"\n",
    "path = \"../../results/\" + model + \"/plots/\" + file_name\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "lexical_distance_per_tech = average_dict(lexical_distance_per_tech)\n",
    "plot_dict(lexical_distance_per_tech, \"Avg. Lexical per technique v\" + str(version), \"Prompt technique\", \"Avg. Jaccard distance\", width=20, height=7, same_color=False, color='maroon')\n",
    "\n",
    "ast_similarity_per_tech = average_dict(ast_similarity_per_tech)\n",
    "plot_dict(ast_similarity_per_tech, \"AST similarity per technique v\" + str(version), \"Prompt technique\", \"AST similarity\", width=20, height=7, same_color=False, color='maroon')\n",
    "\n",
    "code_bleu_per_tech = average_dict(code_bleu_per_tech)\n",
    "plot_dict(code_bleu_per_tech, \"Code BLEU per technique v\" + str(version), \"Prompt technique\", \"Code BLEU\", width=20, height=7, same_color=False, color='maroon')\n",
    "\n",
    "# plot the error types\n",
    "plot_dict_old(errors_types, \"Error types v\" + str(version), \"Error type\", \"Number of errors\")\n",
    "\n",
    "plot_dict_old(stats_per_task, \"Tasks that failed the most v\" + str(version), \"Task ID\", \"Number of errors\")\n",
    "plot_dict_old(stats_per_tech, \"Prompt techniques that failed the most v\" + str(version), \"Prompt techniques\", \"Number of errors\", width=20, height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_per_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data: Example structure, replace with your test_output_per_comb dictionary\n",
    "data = test_output_per_comb\n",
    "num_plots = len(data)\n",
    "\n",
    "# Define consistent category order starting with \"Passed\"\n",
    "consistent_categories = ['Passed', 'AssertionError', 'ImportError', 'AttributeError', 'KeyError', 'ValueError', 'NameError', 'FileNotFoundError', 'IndexError', 'SyntaxError']\n",
    "\n",
    "# Plot dimensions\n",
    "cols = 4\n",
    "rows = (num_plots + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(25, 5 * rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "y_max = 130  # Fixed y-axis limit for all plots\n",
    "\n",
    "# Generate different shades of red for the non-passed categories\n",
    "reds = plt.cm.Reds(np.linspace(0.4, 0.8, len(consistent_categories) - 1))\n",
    "\n",
    "# Iterate through each combination and plot\n",
    "for i, (combination, results) in enumerate(data.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Ensure all categories are present in the plot (even if they don't appear in 'results')\n",
    "    values = [results.get(category, 0) for category in consistent_categories]\n",
    "    \n",
    "    # Assign colors: green for 'Passed' and shades of red for others\n",
    "    colors = ['green' if cat == 'Passed' else reds[j-1] for j, cat in enumerate(consistent_categories)]\n",
    "\n",
    "    # Plot the bar chart\n",
    "    bars = ax.bar(consistent_categories, values, color=colors)\n",
    "    ax.set_title(combination, fontsize=10)\n",
    "    ax.set_xlabel('Error Type')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    ax.set_ylim(0, y_max)  # Set fixed y-axis limit for all plots\n",
    "                    \n",
    "    \n",
    "# Delete any unused axes if necessary\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "file_name = \"Error_types_per_combination.png\"\n",
    "path = \"../../results/\" + model + \"/plots/\" + file_name\n",
    "plt.savefig(path)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
