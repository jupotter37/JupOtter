{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61416313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e9aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total envs available: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = {}\n",
    "for e in list(gym.envs.registry.all()):\n",
    "    envs[e.id] = e\n",
    "\n",
    "print('Total envs available:', len(envs))\n",
    "\n",
    "list(envs.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600ce2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -5. -10.], [ 5. 10.], (2,), float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = 5\n",
    "action_space = gym.spaces.Box(\n",
    "    low=np.array([-action_space, -10]),\n",
    "    high=np.array([+action_space, 10]), dtype=np.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d9c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dca42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65986091",
   "metadata": {},
   "source": [
    "## Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4f7970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DDPG, A2C\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a01260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 41.1     |\n",
      "|    ep_rew_mean        | 41.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 931      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.637   |\n",
      "|    explained_variance | -0.0324  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.53     |\n",
      "|    value_loss         | 7.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43.4     |\n",
      "|    ep_rew_mean        | 43.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1080     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.62    |\n",
      "|    explained_variance | 0.243    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.17     |\n",
      "|    value_loss         | 4.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 44.2     |\n",
      "|    ep_rew_mean        | 44.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1147     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.519   |\n",
      "|    explained_variance | -0.0058  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.844    |\n",
      "|    value_loss         | 6.24     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 50.4     |\n",
      "|    ep_rew_mean        | 50.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | -0.0787  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.82     |\n",
      "|    value_loss         | 5.77     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 49.8     |\n",
      "|    ep_rew_mean        | 49.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1190     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | -0.00333 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.685    |\n",
      "|    value_loss         | 5.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 51.4     |\n",
      "|    ep_rew_mean        | 51.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.479   |\n",
      "|    explained_variance | 0.0157   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.728    |\n",
      "|    value_loss         | 4.41     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 54.2     |\n",
      "|    ep_rew_mean        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1159     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0.0134   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.89     |\n",
      "|    value_loss         | 3.93     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 55.1      |\n",
      "|    ep_rew_mean        | 55.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1154      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.573    |\n",
      "|    explained_variance | -5.78e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 0.554     |\n",
      "|    value_loss         | 3.4       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.1     |\n",
      "|    ep_rew_mean        | 57.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1135     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0.00476  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.04     |\n",
      "|    value_loss         | 2.93     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.4     |\n",
      "|    ep_rew_mean        | 57.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1109     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.555   |\n",
      "|    explained_variance | -0.00303 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.865    |\n",
      "|    value_loss         | 2.54     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 58.7      |\n",
      "|    ep_rew_mean        | 58.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1103      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.494    |\n",
      "|    explained_variance | -0.000368 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.617     |\n",
      "|    value_loss         | 2.13      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61.7     |\n",
      "|    ep_rew_mean        | 61.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1105     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | -0.0001  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.383    |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 63.9     |\n",
      "|    ep_rew_mean        | 63.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1109     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.536   |\n",
      "|    explained_variance | 3.82e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.568    |\n",
      "|    value_loss         | 1.42     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 67.5      |\n",
      "|    ep_rew_mean        | 67.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1119      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.517    |\n",
      "|    explained_variance | -6.83e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.337     |\n",
      "|    value_loss         | 1.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 71.5      |\n",
      "|    ep_rew_mean        | 71.5      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1129      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.571    |\n",
      "|    explained_variance | -2.81e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.359     |\n",
      "|    value_loss         | 0.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 75.3      |\n",
      "|    ep_rew_mean        | 75.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1139      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.516    |\n",
      "|    explained_variance | -1.32e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.161     |\n",
      "|    value_loss         | 0.59      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.2     |\n",
      "|    ep_rew_mean        | 77.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1146     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.41    |\n",
      "|    explained_variance | 0.00041  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.405    |\n",
      "|    value_loss         | 0.391    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 84.3      |\n",
      "|    ep_rew_mean        | 84.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1148      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.502    |\n",
      "|    explained_variance | -1.07e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.195     |\n",
      "|    value_loss         | 0.23      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 86.9     |\n",
      "|    ep_rew_mean        | 86.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1154     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.431   |\n",
      "|    explained_variance | 1.5e-05  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91       |\n",
      "|    ep_rew_mean        | 91       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1159     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.464   |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.0449   |\n",
      "|    value_loss         | 0.0378   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyglet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m action, _state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:87\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:179\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m cartheight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30.0\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassic_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rendering\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mViewer(screen_width, screen_height)\n\u001b[1;32m    182\u001b[0m     l, r, t, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartwidth \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, cartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39mcartheight \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLforEE/lib/python3.10/site-packages/gym/envs/classic_control/rendering.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Cannot import pyglet.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    HINT: you can install pyglet directly via 'pip install pyglet'.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    But if you really just want to install all Gym dependencies and not have to think about it,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    'pip install -e .[all]' or 'pip install gym[all]' will do it.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda052e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e914fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe1379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639d7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a59e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34bf28de",
   "metadata": {},
   "source": [
    "# Learning: Cart Pole\n",
    "\n",
    "$\\text{Action Space: } \\{0: \\text{ left, } 1: \\text{ right } \\} $\n",
    "\n",
    "$\\text{Observation Space: } [\\text{Cart Position, Cart Velocity, Pole Angle, Angular Velocity}], \\text{ in range } [(-4.8:4.8), (-\\infty : \\infty), (-0.418:0.418), (-\\infty : \\infty)]$\n",
    "\n",
    "$\\text{All environments are randomly initialized to values between -0.05 to 0.05.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c36d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd6f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4e504b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00682458, -0.02802546, -0.04673157, -0.0345641 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "env.reset() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229d8dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space (state):  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "\n",
      "Action space Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation space (state): \", env.observation_space)\n",
    "print()\n",
    "print(\"Action space\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292b526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002866983413696289"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = time.time()\n",
    "env.step(env.action_space.sample())\n",
    "time.time() - s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce3083",
   "metadata": {},
   "source": [
    "#### Take Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79ec8d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samart/opt/anaconda3/lib/python3.9/site-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samart/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for i in range(40):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d536e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45570993, -0.75292265,  1.522458  ,  5.461001  ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7f24b",
   "metadata": {},
   "source": [
    "### Train Agent using Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a445fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = 4 # number of states\n",
    "action_space = 2 # number of possible actions\n",
    "\n",
    "def Qtable(state_space,action_space,bin_size = 30):\n",
    "    \n",
    "    bins = [np.linspace(-4.8,4.8,bin_size),\n",
    "            np.linspace(-4,4,bin_size),\n",
    "            np.linspace(-0.418,0.418,bin_size),\n",
    "            np.linspace(-4,4,bin_size)]\n",
    "    \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([bin_size] * state_space + [action_space])) # size = [30, 30, 30, 30, 2]\n",
    "    return q_table, bins\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cf8205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00144776, 0.54035932],\n",
       "       [0.4512143 , 0.35349524]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size=[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b05febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table, bins = Qtable(state_space, action_space, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ce9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 100, epsilon = 0.2):\n",
    "    \n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    for episode in range(1,episodes+1):\n",
    "        steps += 1 \n",
    "        # env.reset() => initial observation\n",
    "        current_state = Discrete(env.reset(),bins)\n",
    "      \n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done: \n",
    "            if episode%timestep==0: \n",
    "                env.render()\n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "            next_state = Discrete(observation,bins)\n",
    "            score += reward\n",
    "            if not done:\n",
    "                max_future_q = np.max(q_table[next_state])\n",
    "                current_q = q_table[current_state+(action,)]\n",
    "                new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "                q_table[current_state+(action,)] = new_q\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            rewards += score\n",
    "            if score > 195 and steps >= 100: print('Solved')\n",
    "        if episode % timestep == 0: print(reward / timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3941e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samart/opt/anaconda3/lib/python3.9/site-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "Q_learning(q_table, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5444bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLforEE",
   "language": "python",
   "name": "mlforee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
