{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c667153f33f793",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Text embedding models\n",
    "\n",
    "---\n",
    "\n",
    "Alejandro Ricciardi (Omegapy)  \n",
    "created date: 01/23/2024   \n",
    "[GitHub](https://github.com/Omegapy)  \n",
    "\n",
    "Credit: [LangChain](https://python.langchain.com/docs/expression_language/)\n",
    "\n",
    "<br>\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d747506242aa53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " \n",
    "Projects Description:  \n",
    "**LangChain** is a framework for developing applications powered by language models.  \n",
    "**In this project:** This project is a series of LangChain text embedding models for LLMs tutorials on Jupyter Notebook.  \n",
    "The tutorials are a series LangChain Python code examples from the https://python.langchain.com/ website.\n",
    "\n",
    "Specifically from the section [Text embedding models](https://python.langchain.com/docs/modules/data_connection/text_embedding/)\n",
    "\n",
    "⚠️ **Info**: Head to [Integrations](https://python.langchain.com/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.\n",
    "\n",
    "The ```Embeddings class``` is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\n",
    "\n",
    "<p></p>\n",
    "<b style=\"font-size:15;\">\n",
    "⚠️ This project requires an OpenAI key.\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d812a5237c2ffca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Project Map  \n",
    "- [API Keys](#api-keys)  \n",
    "- [Get started](#get-started)\n",
    "- [CacheBackedEmbeddings](#cachebackedembeddings)\n",
    "    - [Using with a Vector Store](#using-with-a-vector-store)\n",
    "    - [Swapping the ByteStore](#swapping-the-bytestore)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadd2d6d518060f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0bea1dbc415b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:33:46.366866200Z",
     "start_time": "2024-02-04T16:33:46.351931800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ.get(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc6fd5480d3b4d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Get started\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbfee95c86dd178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:33:49.865364100Z",
     "start_time": "2024-02-04T16:33:49.855349300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee868ef2da08c394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:33:50.494677500Z",
     "start_time": "2024-02-04T16:33:50.205767500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71e113ac3b014ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:34:10.269557200Z",
     "start_time": "2024-02-04T16:34:08.885038600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b0c1206a8ea86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### embed_query\n",
    "Embed single query\n",
    "Embed a single piece of text for the purpose of comparing to other embedded pieces of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab3e50b87a4302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:35:41.668006900Z",
     "start_time": "2024-02-04T16:35:40.377332100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005354681365594308,\n",
       " -0.0005715346531097276,\n",
       " 0.038875909934336914,\n",
       " -0.0029596003572924627,\n",
       " -0.008966285328704282]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f050905d2169f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ead9653458042f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## CacheBackedEmbeddings\n",
    "\n",
    "Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n",
    "\n",
    "Caching embeddings can be done using a ```CacheBackedEmbeddings```. The cache backed embedder is a wrapper around an embedder that caches embeddings in a key-value store. The text is hashed and the hash is used as the key in the cache.\n",
    "\n",
    "The main supported way to initialize a ```CacheBackedEmbeddings``` is ```from_bytes_store```. This takes in the following parameters:\n",
    "\n",
    "- underlying_embedder: The embedder to use for embedding.\n",
    "- document_embedding_cache: Any [ByteStore](https://python.langchain.com/docs/integrations/stores/) for caching document embeddings.\n",
    "- namespace: (optional, defaults to ```\"\"```) The namespace to use for document cache. This namespace is used to avoid collisions with other caches. \n",
    "For example, set it to the name of the embedding model used.\n",
    "Attention: Be sure to set the ```namespace``` parameter to avoid collisions of the same text embedded using different embeddings models.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ceb5bb05f0726d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:43:10.518855100Z",
     "start_time": "2024-02-04T16:43:10.329224300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632cff450306f24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Using with a Vector Store\n",
    "\n",
    "First, let’s see an example that uses the local file system for storing embeddings and uses FAISS vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8331dc3e5a55220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:49:12.397626300Z",
     "start_time": "2024-02-04T16:46:05.381373500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - langchain-openai\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - defaults\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install langchain-openai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17602afcbd107142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:44:19.676669700Z",
     "start_time": "2024-02-04T16:44:19.208966Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "underlying_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762c005e3e040978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:44:30.426769900Z",
     "start_time": "2024-02-04T16:44:30.409593700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a92e0718e281fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:44:55.110648300Z",
     "start_time": "2024-02-04T16:44:55.098838300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"data/state_of_the_union.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbec90c9a653ff4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T16:45:03.943315600Z",
     "start_time": "2024-02-04T16:45:03.403248800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mp:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:55\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[1;32mp:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    507\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mp:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:960\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    942\u001b[0m \n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    959\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mp:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:914\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    913\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 914\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    916\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32mp:\\Projects\\LLM-Frameworks-Tutorials\\LangChain Tutorials\\Tutorials from Langchain\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:57\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = FAISS.from_documents(documents, cached_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c787870945236",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If we try to create the vector store again, it’ll be much faster since it does not need to re-compute any embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50d41cbbabd397",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "db2 = FAISS.from_documents(documents, cached_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a940d4e359d851",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "And here are some of the embeddings that got created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2c2b25797da3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list(store.yield_keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb874b19edc2c9aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0357a5e1ba9828",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Swapping the ByteStore\n",
    "\n",
    "In order to use a different ```ByteStore```, just use it when creating your ```CacheBackedEmbeddings```. Below, we create an equivalent cached embeddings object, except using the non-persistent ```InMemoryByteStore``` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe824dc57d4e068e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import InMemoryByteStore\n",
    "\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, store, namespace=underlying_embeddings.model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719893baf5660aa4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[Project Map](#project-map)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
