{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "core.py\n",
    "環境とエージェントの抽象クラス\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class coreEnv:\n",
    "    \"\"\" 環境の抽象クラス \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" 初期処理 \"\"\"\n",
    "        # 引数の設定は適時編集\n",
    "        self.n_act = 2  # <--- 行動数を設定\n",
    "        self.done = False\n",
    "        # ------------------------- 編集ここから\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" 状態を初期化 \"\"\"\n",
    "        self.done = False\n",
    "        # ------------------------- 編集ここから\n",
    "        obs = np.array([0, 0, 0, 0])  # ndarray\n",
    "        # ------------------------- ここまで\n",
    "        return obs\n",
    "\n",
    "    def step(self, act):\n",
    "        \"\"\" 状態を更新 \"\"\"\n",
    "        # 最終状態の次の状態はリセット\n",
    "        if self.done is True:\n",
    "            obs = self.reset()\n",
    "            return None, None, obs\n",
    "        # ------------------------- 編集ここから\n",
    "        rwd = 1.0                   # float\n",
    "        done = True                 # bool\n",
    "        # ------------------------- ここまで\n",
    "        self.done = done\n",
    "        # ------------------------- 編集ここから\n",
    "        # self.done を使った処理\n",
    "        obs = np.array([0, 0, 0, 0])  # ndarray\n",
    "        # ------------------------- ここまで\n",
    "        return rwd, done, obs\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\" 状態に対応した画像を作成 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # img: 3d ndarray\n",
    "        img = np.zeros((100, 200, 3), dtype=np.uint8)\n",
    "        # ------------------------- ここまで\n",
    "        return img\n",
    "\n",
    "\n",
    "class coreAgt:\n",
    "    \"\"\" エージェントの抽象クラス \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" 初期処理 \"\"\"\n",
    "        # 引数の設定は適時編集\n",
    "        self.epsilon = 0.4\n",
    "        # ------------------------- 編集ここから\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        \"\"\" 観測に対して行動を出力 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        act = 0  # int\n",
    "        # ------------------------- ここまで\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, rwd, done, next_obs):\n",
    "        \"\"\" 学習 \"\"\"\n",
    "        if rwd is None:\n",
    "            return\n",
    "        # ------------------------- 編集ここから\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def get_Q(self, obs):\n",
    "        \"\"\" 観測に対するQ値を出力 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        Q = np.ndarray([0, 0])  # ndarray\n",
    "        # ------------------------- ここまで\n",
    "        return Q\n",
    "\n",
    "    def save_weights(self, filepath):\n",
    "        \"\"\" 方策のパラメータの保存 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        \"\"\" 方策のパラメータの読み込み \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # ------------------------- ここまで\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env_myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import cv2\n",
    "# import numpy as np\n",
    "# # import core\n",
    "# import mujoco\n",
    "# # from tqdm import trange\n",
    "\n",
    "# xml_path = './xml_models/micromouse.xml'\n",
    "# times = []\n",
    "# left_motor_kakusokudo = []\n",
    "# right_motor_kakusokudo = []\n",
    "# sensor_LF = []\n",
    "# sensor_LS = []\n",
    "# sensor_RF = []\n",
    "# sensor_RS = []\n",
    "\n",
    "\n",
    "\n",
    "# class MyEnv(coreEnv):\n",
    "\n",
    "#     def __init__(self): #引数で受け取るとかは特にないからこんな感じ　各インスタンス変数に値を入れていく\n",
    "#         self.n_act = 5\n",
    "#         self.done = False #最初だからFalse\n",
    "#         self.state = None #状態表示用変数\n",
    "#         self.m = mujoco.MjModel.from_xml_path(xml_path)#modelを読み込んで用意\n",
    "#         self.d = mujoco.MjData(self.m) #モデルのデータを用意\n",
    "\n",
    "#     def reset(self):\n",
    "#         \"\"\" 状態を初期化 \"\"\"\n",
    "#         self.done = False\n",
    "#         # ------------------------- 編集ここから\n",
    "#         self.state = 'start'\n",
    "#         mujoco.mj_resetData(self.m, self.d)\n",
    "#         obs = np.array([0, 0, 0, 0])  # ndarray\n",
    "#         # ------------------------- ここまで\n",
    "#         return obs\n",
    "    \n",
    "#     def step(self, act):\n",
    "#         \"\"\" 状態を更新 \"\"\"\n",
    "#         # 最終状態の次の状態はリセット\n",
    "#         if self.done is True:\n",
    "#             obs = self.reset()\n",
    "#             return None, None, obs\n",
    "#         # ------------------------- 編集ここから\n",
    "#         self.d.ctrl = act\n",
    "#         mujoco.mj_step(self.m, self.d)\n",
    "#         mujoco.mj_kinematics(self.m, self.d)\n",
    "#         goal_pos = self.d.geom('goal').xpos\n",
    "#         mm_pos = self.d.geom('mein_body').xpos\n",
    "#         self.goal_range = np.sqrt((goal_pos[0] - mm_pos[0])**2+(goal_pos[1] - mm_pos[1])**2)\n",
    "\n",
    "#         rwd = -self.goal_range      # float  goalとの距離がそのまま罰になる\n",
    "#         done = False                 # bool\n",
    "\n",
    "#         if self.goal_range < 0.06: #ゴールに十分近かったら報酬を与えて，終了判定\n",
    "#             rwd = 300\n",
    "#             done = True\n",
    "\n",
    "#         # ------------------------- ここまで\n",
    "#         self.done = done\n",
    "#         # ------------------------- 編集ここから\n",
    "#         # self.done を使った処理\n",
    "#         ob1 = self.d.sensordata[0].copy() #sensor_LF\n",
    "#         ob2 = self.d.sensordata[1].copy() #sensor_LS\n",
    "#         ob3 = self.d.sensordata[2].copy() #sensor_RF\n",
    "#         ob4 = self.d.sensordata[3].copy() #sensor_RS\n",
    "#         obs = np.array([ob1, ob2, ob3, ob4])  # ndarray\n",
    "#         # ------------------------- ここまで\n",
    "#         return rwd, done, obs\n",
    "    \n",
    "#     def render(self):\n",
    "#         \"\"\" 状態に対応した画像を作成 \"\"\"\n",
    "#         # ------------------------- 編集ここから\n",
    "#         # img: 3d ndarray\n",
    "#         self.viewer.sync() # mj_stepで進めたシミュレーションを，描画に反映する　　物理状態の変更を反映する\n",
    "#         # ------------------------- ここまで\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agt_TableQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "agt_tableQ.py\n",
    "Qテーブルを使ったQ学習アルゴリズム\n",
    "\"\"\"\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 自作モジュール\n",
    "# import core\n",
    "\n",
    "\n",
    "class TableQAgt(coreAgt):\n",
    "    \"\"\" Qテーブルを使ったQ学習エージェントクラス \"\"\"\n",
    "    def __init__(           # 引数とデフォルト値の設定 (A)\n",
    "            self,\n",
    "            n_act=4,            # int: 行動の種類数\n",
    "            init_val_Q=0,       # float: Q値の初期値\n",
    "            epsilon=0.1,        # float: 乱雑度\n",
    "            alpha=0.1,          # float: 学習率\n",
    "            gamma=0.9,          # float: 割引率\n",
    "            max_memory=10000,     # int: 記憶する最大の観測数\n",
    "            filepath=None,      # str: 保存用ファイル名\n",
    "            ):\n",
    "        \"\"\" 初期処理 \"\"\"\n",
    "        # 引数の設定は適時編集\n",
    "        self.epsilon = epsilon\n",
    "        # ------------------------- 編集ここから\n",
    "        self.n_act = n_act\n",
    "        # エージェントのハイパーパラメータ (B)\n",
    "        self.init_val_Q = init_val_Q\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # 保存ファイル名 (C)\n",
    "        self.filepath = filepath\n",
    "\n",
    "        # Qテーブル関連 (D)\n",
    "        self.Q = {}     # Qテーブルの辞書を用意\n",
    "        self.len_Q = 0  # Qテーブルに登録した観測の数記録変数　最初だから０\n",
    "        self.max_memory = max_memory #記録する最大の観測数\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        \"\"\" 観測に対して行動を出力 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # obsを文字列に変換 (A)　辞書でやってるから，ndarray型を変える必要があるので文字列に変換しといてる\n",
    "        obs = str(obs)\n",
    "\n",
    "        # obs が登録されていなかったら初期値を与えて登録 (B) #登録されていたら何もしない\n",
    "        self._check_and_add_observation(obs)\n",
    "\n",
    "        # 確率的に処理を分岐 (C)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # epsilon の確率(D)\n",
    "            act_select = np.random.randint(0, self.n_act)  # ランダム行動選択\n",
    "        else:\n",
    "            # 1-epsilon の確率(E)\n",
    "            act_select = np.argmax(self.Q[obs])  # Qを最大にする行動\n",
    "\n",
    "        # ------------------------- ここまで\n",
    "        return act_select\n",
    "\n",
    "    def _check_and_add_observation(self, obs):\n",
    "        \"\"\" obs が登録されていなかったら初期値を与えて登録 \"\"\"\n",
    "        if obs not in self.Q:  # (A) セルフQにその辞書のキーがなかったら，\n",
    "            self.Q[obs] = [self.init_val_Q] * self.n_act  # (B)　obsをキーとして，Qの初期値をact個用意して追加\n",
    "            self.len_Q += 1  # (C)　追加したので，Qの大きさの変数を１増やす\n",
    "            if self.len_Q > self.max_memory:  # (D)　Qの大きさがマックスを超えたら\n",
    "                print(f'観測の登録数が上限 ' +\n",
    "                      f'{self.max_memory:d} に達しました。')\n",
    "                sys.exit() #プログラムは即座に終了\n",
    "            if (self.len_Q < 100 and self.len_Q % 10 == 0) or \\\n",
    "                    (self.len_Q % 100 == 0):  # (E) 100以下で１０で割り切れる場合　または　１００で割り切れる場合\n",
    "                print(f'the number of obs in Q-table' +\n",
    "                      f' --- {self.len_Q:d}') #表示する\n",
    "\n",
    "    def learn(self, obs, act, rwd, done, next_obs):\n",
    "        \"\"\" 学習 \"\"\"\n",
    "        if rwd is None:  # rwdがNoneだったら戻る(A)\n",
    "            return\n",
    "        # ------------------------- 編集ここから\n",
    "        # obs, next_obs を文字列に変換 (B)　今とその次のobsを使って学習を行うために必要\n",
    "        obs = str(obs)\n",
    "        next_obs = str(next_obs)\n",
    "\n",
    "        # next_obs が登録されていなかったら初期値を与えて登録 (C)\n",
    "        self._check_and_add_observation(next_obs)\n",
    "\n",
    "        # 学習のターゲットを作成 (D)\n",
    "        if done is True: #動的計画法の一番後ろだったら期待値は報酬そのものであるため\n",
    "            target = rwd\n",
    "        else: #動的計画法的に，まだエピソードが終わらない場合の期待値はその報酬とその次の状態の最大のＱ値をとる行動のＱ値であるため\n",
    "            target = rwd + self.gamma * max(self.Q[next_obs])\n",
    "\n",
    "        # Qをターゲットに近づける (E)\n",
    "        self.Q[obs][act] = (1 - self.alpha) * self.Q[obs][act] + self.alpha * target #平均の更新足　ここではQ値の更新則  すべての経験の平均の期待値がここに追加されるので，ちゃんと行動価値（その行動によって得た値の平均つまり期待値）の値になっている\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def get_Q(self, obs):\n",
    "        \"\"\" 観測に対するQ値を出力 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        obs = str(obs) #受け取ったobsを文字列に変換して，\n",
    "        if obs in self.Q:   # obsがQにある (A)場合はその値を取り出す（その状態での各行動のQ値がそれぞれ出る）\n",
    "            val = self.Q[obs]\n",
    "            Q = np.array(val) #nparray型に変換してから返す\n",
    "        else:               # obsがQにない (B)\n",
    "            Q = None #なければないで終わり\n",
    "        # ------------------------- ここまで\n",
    "        return Q\n",
    "\n",
    "    def save_weights(self, filepath=None):\n",
    "        \"\"\" 方策のパラメータの保存 \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # Qテーブルの保存\n",
    "        if filepath is None:\n",
    "            filepath = self.filepath + '.pkl'\n",
    "        with open(filepath, mode='wb') as f:\n",
    "            pickle.dump(self.Q, f) #Q値を保存する　\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def load_weights(self, filepath=None):\n",
    "        \"\"\" 方策のパラメータの読み込み \"\"\"\n",
    "        # ------------------------- 編集ここから\n",
    "        # Qテーブルの読み込み\n",
    "        if filepath is None:\n",
    "            filepath = self.filepath + '.pkl'\n",
    "        with open(filepath, mode='rb') as f:\n",
    "            self.Q = pickle.load(f) #ここでQ値を復元してる\n",
    "        # ------------------------- ここまで\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sibuyaatusiタスク\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # ベクトル・行列演算ライブラリ (A)\n",
    "import mujoco\n",
    "from tqdm import trange\n",
    "\n",
    "NUM_DIZITIZED = 4\n",
    "MAX_STEP = 50000\n",
    "\n",
    "\n",
    "class MyEnv(coreEnv):\n",
    "    \"\"\" コリドータスクの環境クラス \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(                   # (B)\n",
    "            self,\n",
    "            ):\n",
    "        \"\"\" 初期処理 \"\"\"\n",
    "        \n",
    "        # 引数の設定は適時編集\n",
    "        self.n_act = 3  # <--- 行動数を設定 (C)\n",
    "        self.done = False\n",
    "        # ------------------------- 編集ここから\n",
    "        \"\"\" インスタンス生成時の処理 \"\"\"\n",
    "        # タスクパラメータ (D)\n",
    "        # self.model = mujoco.MjModel.from_xml_path('.\\micromouse_straight_new.xml')#modelを読み込んで用意 インスタンスに用意しておく\n",
    "        self.model = mujoco.MjModel.from_xml_path('.\\mouse_in_maze.xml')#modelを読み込んで用意 インスタンスに用意しておく\n",
    "        self.data = mujoco.MjData(self.model) #モデルのデータを用意\n",
    "        self.viewer = mujoco.viewer.launch_passive(self.model,self.data) #mujoco描画起動\n",
    "        self.init_pos = self.data.qpos #ロボットの初期位置座標取得\n",
    "        self.init_vel = self.data.qvel #ロボットの初期速度取得\n",
    "        self.stepcount = 0\n",
    "        self.wheel_left_id = mujoco.mj_name2id(self.model, 3,'left wheel joint')\n",
    "        self.wheel_right_id = mujoco.mj_name2id(self.model, 3,'right wheel joint')\n",
    "        self.wheel_ang_left = self.data.qpos[7]\n",
    "        self.wheel_ang_right = self.data.qpos[8]\n",
    "        print(\"ID!:\",self.wheel_left_id)\n",
    "\n",
    "        # ------------------------- ここまで\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" 状態を初期化 \"\"\"\n",
    "        self.done = False  # (A)\n",
    "        # ------------------------- 編集ここから\n",
    "        # ロボットの位置を開始位置へ戻す (C)####################################################################################################env.model.geom_pos\n",
    "        # self.data.qpos = self.init_pos\n",
    "        # self.data.qvel = self.init_vel #的な感じにして，初期の位置を与えることで初期位置に戻せるようにする　初期値を記述する　pauseしてからprint dataで取れそう？\n",
    "        self.stepcount = 0\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        # 初期の観測値を用意 (E)\n",
    "        obs = self._make_obs()\n",
    "        # ------------------------- ここまで\n",
    "        return obs\n",
    "    \n",
    "    # def discretize(value):\n",
    "    # # 0から0.18の範囲を10段階に分割\n",
    "    #     if value == -1:\n",
    "    #         value = 0.18\n",
    "    #     discrete_value = int(value / 0.018)\n",
    "    #     return min(discrete_value, 9)  # 最大値が10段階目になるように調整 9以上にならないようにしている\n",
    "\n",
    "    # def discretize_values(self, ob1, ob2, ob3, ob4):\n",
    "    #     # 各値を離散化\n",
    "    #     discrete_ob1 = self.discretize(ob1)\n",
    "    #     discrete_ob2 = self.discretize(ob2)\n",
    "    #     discrete_ob3 = self.discretize(ob3)\n",
    "    #     discrete_ob4 = self.discretize(ob4)\n",
    "    #     sum = discrete_ob1*1000 + discrete_ob2*100 + discrete_ob3*10 + discrete_ob4*1\n",
    "\n",
    "    #     return sum\n",
    "\n",
    "    # 離散化\n",
    "    def bins(self, clip_min, clip_max, num):\n",
    "        return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "\n",
    "    def _make_obs(self): #クラスの内部でしか使わない，内部用メソッドであるため_をつけている\n",
    "        \"\"\" 状態から観測を作成(状態（観測）の離散化を行って観測とする) \"\"\"\n",
    "        # 最終状態判定がTrueだったら 9999 を出力 (A)\n",
    "        if self.done is True:\n",
    "            obs = np.array([9] * 4)\n",
    "            return obs #どうせこの後はリセットするし，あまり意味はないかも\n",
    "        \n",
    "        # 4つのセンサの値を受け取り，それを離散化して0~9で表したのち，4桁の整数１つとして観測を表す\n",
    "        ob1 = self.data.sensordata[0].copy() #sensor_LF\n",
    "        ob2 = self.data.sensordata[1].copy() #sensor_LS\n",
    "        ob3 = self.data.sensordata[2].copy() #sensor_RF\n",
    "        ob4 = self.data.sensordata[3].copy() #sensor_RS\n",
    "        if ob1 == -1:\n",
    "            ob1 = 0.15\n",
    "        if ob2 == -1:\n",
    "            ob2 = 0.15\n",
    "        if ob3 == -1:\n",
    "            ob3 = 0.15\n",
    "        if ob4 == -1:\n",
    "            ob4 = 0.15\n",
    "        # quat = self.data.xquat[1]\n",
    "        # rounded_quat = round(quat, 2)\n",
    "        # self.rounded_quat_array = np.array(rounded_quat)\n",
    "\n",
    "        # goal_quat = [0, 0.7, 0, 0]\n",
    "        # self.goal_quat_array = np.array(goal_quat)\n",
    "\n",
    "        # obs = self.rounded_quat_array - self.goal_quat_array\n",
    "\n",
    "\n",
    "        obs=np.digitize(ob1, bins=self.bins(0.0, 0.1, NUM_DIZITIZED+1))*1000 + \\\n",
    "            np.digitize(ob2, bins=self.bins(0.0, 0.1, NUM_DIZITIZED))*100 + \\\n",
    "            np.digitize(ob3, bins=self.bins(0.0, 0.1, NUM_DIZITIZED+1))*10 + \\\n",
    "            np.digitize(ob4, bins=self.bins(0.0, 0.1, NUM_DIZITIZED))*1\n",
    "        \n",
    "        # print(obs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, act_select):\n",
    "        whatdone = False\n",
    "        \"\"\" 状態を更新 \"\"\"\n",
    "        # 最終状態の次の状態はリセット(A)\n",
    "        if self.done is True:\n",
    "            obs = self.reset()\n",
    "            return 0, None, obs\n",
    "        \n",
    "        # step数が基準を超えたら\n",
    "        if self.stepcount == MAX_STEP:\n",
    "            obs = self.reset()\n",
    "            rwd = 1\n",
    "            return rwd, None, obs\n",
    "        \n",
    "        \n",
    "        if act_select == 0:\n",
    "            act = [0.08, 0.08]\n",
    "        elif act_select == 1:\n",
    "            act = [-0.08, -0.08]\n",
    "        elif act_select == 2:\n",
    "            act = [-0.08, 0.08]\n",
    "        elif act_select == 3:\n",
    "            act = [0.08, -0.08] \n",
    "        # elif act_select == 4:\n",
    "        #     act = [0, 0] #行動選択を具体的なモータ出力に変換している(モータを動かす組み合わせを選ぶ)\n",
    "        else:\n",
    "            print(\"ありえない行動選択をしているこれはバグに違いない\")\n",
    "        \n",
    "        self.data.ctrl = act.copy() #行動選択のactをctrlにコピーすることで，モータを回す\n",
    "        # before_goal_range = np.sqrt((before_goal_pos[0] - before_mm_pos[0])**2+(before_goal_pos[1] - before_mm_pos[1])**2) #三平方の定理\n",
    "        self.prev_wheel_ang_left = self.data.qpos[7]\n",
    "        self.prev_wheel_ang_right = self.data.qpos[8]\n",
    "        for mjst in range(3000):\n",
    "            mujoco.mj_step(self.model, self.data) #3000step進める####決めた行動を行った後3000mjstep後の，環境の様子を強化学習の1stepと判断したいため#######################################################################################################\n",
    "            self.hit_wall_f = self.data.sensordata[4].copy() #フォースセンサの値取得\n",
    "            self.hit_wall_b = self.data.sensordata[5].copy() #フォースセンサの値取得\n",
    "            if self.hit_wall_f > 0 or self.hit_wall_b > 0:\n",
    "                rwd = (self.stepcount/MAX_STEP) - 1\n",
    "                whatdone = True\n",
    "                self.done = whatdone  # (E) #確定したdone情報を，インスタンス変数のdoneに保存\n",
    "                obs = self._make_obs()  # 行動後のobsを作成(F) ここではセンサ値を取得する　next_obsに対応している値で，行動後の観測値である．ぶつかったときのobs\n",
    "                self.viewer.sync() #mujoco描画起動\n",
    "                return rwd, whatdone, obs #ぶつかったらアウト\n",
    "\n",
    "            self.viewer.sync() #mujoco描画起動viewer\n",
    "                 # mj_stepで進めたシミュレーションを，描画に反映する　　物理状態の変更を反映する\n",
    "        self.wheel_ang_left = self.data.qpos[7]\n",
    "        self.wheel_ang_right = self.data.qpos[8]\n",
    "        self.stepcount += 1 #step数を測るためのインスタンス変数\n",
    "        # mujoco.mj_kinematics(self.model, self.data) #ネームアクセスするために必要\n",
    "        # goal_pos = self.data.geom('goal').xpos #ゴールの位置座標取得\n",
    "        robot_pos = self.data.geom('mein_body1').xpos #ロボットの位置座標取得\n",
    "        self.hit_wall_f = self.data.sensordata[4].copy() #フォースセンサの値取得\n",
    "        self.hit_wall_b = self.data.sensordata[5].copy() #フォースセンサの値取得\n",
    "        \n",
    "        \n",
    "        # if hit_wall_b != 0:\n",
    "        #     print(hit_wall_b)\n",
    "        # if hit_wall_f != 0:\n",
    "        #     print(hit_wall_f)\n",
    "\n",
    "        # goal_range = np.sqrt((goal_pos - robot_pos)**2)\n",
    "        # goal_range = np.sqrt((goal_pos[0] - robot_pos[0])**2+(goal_pos[1] - robot_pos[1])**2) #三平方の定理\n",
    "        # if goal_range < 0.16: #ゴールしたら\n",
    "        #     rwd = 2\n",
    "        #     print(\"goal!!\")\n",
    "        #     whatdone = True\n",
    "        \n",
    "        # elif\n",
    "        if whatdone != True:\n",
    "            leftwheel_ang = self.wheel_ang_left - self.prev_wheel_ang_left\n",
    "            rightwheel_ang = self.wheel_ang_right - self.prev_wheel_ang_right #前ー後 つまり進んでたら正になるし下がってたら負になる\n",
    "            rwd = (leftwheel_ang + rightwheel_ang)/50 # rwd = (1/(goal_range + 0.2))*0.005 #- np.sqrt(np.sum((self.rounded_quat_array - self.goal_quat_array)**2)) #とにかくずれがバツとなる\n",
    "            print(rwd)\n",
    "            # ob1 = self.data.sensordata[0].copy() #sensor_LF\n",
    "            # LS = self.data.sensordata[1].copy() #sensor_LS\n",
    "            # # ob3 = self.data.sensordata[2].copy() #sensor_RF\n",
    "            # RS = self.data.sensordata[3].copy() #sensor_RS\n",
    "            # # if np.isnan(LS):\n",
    "            # #     LS = 0.15\n",
    "            # # if np.isnan(RS):\n",
    "            # #     RS = 0.15\n",
    "            # LS = np.floor(LS * 1000) / 1000\n",
    "            # RS = np.floor(RS * 1000) / 1000\n",
    "            # rwd = 0.1 - abs(LS-RS)*3\n",
    "            # print(rwd)\n",
    "            \n",
    "            # print(\"abs:\",rwd)\n",
    "            whatdone = False\n",
    "\n",
    "        # ------------------------- ここまで\n",
    "        self.done = whatdone  # (E) #確定したdone情報を，インスタンス変数のdoneに保存\n",
    "        # ------------------------- 編集ここから\n",
    "        obs = self._make_obs()  # 行動後のobsを作成(F) ここではセンサ値を取得する　next_obsに対応している値で，行動後の観測値である．\n",
    "        # ------------------------- ここまで\n",
    "        return rwd, whatdone, obs\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\" 状態に対応した画像を作成 \"\"\" #工事中################＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃\n",
    "        mujoco.viewer.launch_passive(self.model, self.data).sync() # mj_stepで進めたシミュレーションを，描画に反映する　　物理状態の変更を反映する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # 操作方法の表示 (A)\n",
    "#     msg = (\n",
    "#         '\\n' +\n",
    "#         '---- 操作方法 -------------------------------------\\n'\n",
    "#         '[f] 右に進む\\n' +\n",
    "#         '[d] 拾う\\n' +\n",
    "#         '[q] 終了\\n' +\n",
    "#         'クリスタルを拾うと成功\\n' +\n",
    "#         '---------------------------------------------------'\n",
    "#     )\n",
    "#     print(msg)\n",
    "\n",
    "#     # 環境の準備 (B)\n",
    "#     env = MyEnv()\n",
    "\n",
    "#     # 環境のパラメータの与え方例\n",
    "#     \"\"\"\n",
    "#     env = CorridorEnv(\n",
    "#         field_length=6,\n",
    "#         crystal_candidate=(2, 3, 4, 5),\n",
    "#         rwd_fail=-1,\n",
    "#         rwd_move=0,\n",
    "#         rwd_crystal=10,\n",
    "#     )\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 強化学習情報の初期化 (C)\n",
    "#     t = 0\n",
    "#     obs = env.reset()\n",
    "#     act = None\n",
    "#     rwd = None\n",
    "#     done = None\n",
    "\n",
    "#     # 開始の表示 (D)\n",
    "#     print('')\n",
    "#     print('あなたのプレイ開始')\n",
    "\n",
    "#     # 強化学習情報表示の関数定義 (E)\n",
    "#     def show_info(t, act, rwd, done, obs, isFirst=False):\n",
    "#         \"\"\" 強化学習情報の表示 \"\"\"\n",
    "#         if rwd is None:  # (F)\n",
    "#             if isFirst:\n",
    "#                 tt = t\n",
    "#             else:\n",
    "#                 tt = t + 1\n",
    "#             print('')\n",
    "#             print(f'x({tt:d})={str(obs):s}')\n",
    "#         else:  # (G)\n",
    "#             msg = (\n",
    "#                 f'a({t:d})={act:d}, ' +\n",
    "#                 f'r({t:d})={rwd: .2f}, ' +\n",
    "#                 f'done({t:d})={done:}, ' +\n",
    "#                 f'x({t + 1:d})={str(obs):s}'\n",
    "#             )\n",
    "#             print(msg)\n",
    "\n",
    "#     # 強化学習情報表示 (H)\n",
    "#     show_info(t, act, rwd, done, obs, isFirst=True)\n",
    "\n",
    "#     # シミュレーション (I)\n",
    "#     while True:\n",
    "#         # 画面表示 (J)\n",
    "#         image = env.render()\n",
    "#         cv2.imshow('you', image)\n",
    "\n",
    "#         # キーの受付と終了処理 (K)\n",
    "#         key = cv2.waitKey(0)\n",
    "#         if key == ord('q'):\n",
    "#             break\n",
    "\n",
    "#         # あなたの行動選択 (L)\n",
    "#         if key in [ord('d'), ord(' ')]:\n",
    "#             act = 0  # 拾う\n",
    "#         elif key == ord('f'):\n",
    "#             act = 1  # 進む\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         # 環境の更新 (M)\n",
    "#         rwd, done, obs = env.step(act)\n",
    "\n",
    "#         # 強化学習情報表示 (N)\n",
    "#         show_info(t, act, rwd, done, obs)\n",
    "#         t += 1\n",
    "# 回転の運動方程式から考えて，力学が何が起きているか考える　摩擦適当じゃんそれはまずい．ご法度　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行部分（main?）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID!: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2500 [00:20<1:58:25,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2500 [01:06<4:36:23,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of obs in Q-table --- 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2500 [01:12<4:33:01,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/2500 [01:41<1:47:55,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 40/2500 [02:26<2:21:26,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 50/2500 [03:24<3:35:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/2500 [03:49<1:48:11,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 70/2500 [04:41<3:53:02,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/2500 [05:16<2:13:59,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 90/2500 [05:45<2:16:28,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 100/2500 [06:10<1:23:51,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 110/2500 [06:36<1:39:51,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 120/2500 [07:04<2:06:27,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 130/2500 [07:45<3:18:10,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 140/2500 [08:19<2:16:29,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 150/2500 [09:00<2:13:37,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 160/2500 [09:40<2:41:57,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 170/2500 [10:28<3:05:50,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 180/2500 [11:02<2:06:55,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 190/2500 [11:29<1:33:17,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 200/2500 [12:38<4:13:43,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 210/2500 [13:32<3:13:43,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 220/2500 [14:24<3:09:55,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 230/2500 [15:02<1:44:55,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 234/2500 [15:19<2:40:29,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of obs in Q-table --- 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 240/2500 [15:39<2:28:26,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtable保存しました\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 242/2500 [15:51<2:28:00,  3.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m act \u001b[38;5;241m=\u001b[39m agt\u001b[38;5;241m.\u001b[39mselect_action(obs) \u001b[38;5;66;03m#obsにより，行動を決める　Qテーブルのそのマス目を用意して，たまにイプシロングリーディでランダム動作が起きる\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"Qtableは：\",str(obs),agt.Q[str(obs)])\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# print(env.wheel_ang_left)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m rwd, done, next_obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m agt\u001b[38;5;241m.\u001b[39mlearn(obs, act, rwd, done, next_obs)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# print(obs)####################\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 150\u001b[0m, in \u001b[0;36mMyEnv.step\u001b[1;34m(self, act_select)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_wheel_ang_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mqpos[\u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mjst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m):\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#3000step進める####決めた行動を行った後3000mjstep後の，環境の様子を強化学習の1stepと判断したいため#######################################################################################################\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhit_wall_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msensordata[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;66;03m#フォースセンサの値取得\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhit_wall_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msensordata[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;66;03m#フォースセンサの値取得\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import mujoco.viewer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# MAX_STEP = 50000\n",
    "NUM_EPISODE = 2500\n",
    "QTABLE_SAVE_MODE = True\n",
    "INCREMENTAL_LEARNING_MODE = False\n",
    "epi_reward_graph = []\n",
    "# timestep_count = []\n",
    "sumreward = 0\n",
    "\n",
    "\n",
    "env = MyEnv()\n",
    "agt = TableQAgt()\n",
    "\n",
    "if INCREMENTAL_LEARNING_MODE:\n",
    "        # 保存されたQテーブルを読み込む\n",
    "        with open('12/10.pkl', 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "\n",
    "        # 読み込んだデータから必要な変数やリストを取り出す\n",
    "        agt.Q = loaded_data['agt.Q']\n",
    "        type(agt.Q)\n",
    "        agt.len_Q = loaded_data['agt.len_Q']\n",
    "        print(\"agt.len_Q(Qテーブルの数は)\",type(agt.len_Q))\n",
    "        print(agt.len_Q)\n",
    "\n",
    "        print(\"Qtableを読み込みました．追加学習します\")\n",
    "\n",
    "# with mujoco.viewer.launch_passive(env.model,env.data) as viewer: #mujoco描画起動\n",
    "\n",
    "obs = env.reset() #エピソード初期の観測を取得\n",
    "for episode_num in trange(NUM_EPISODE):\n",
    "    for step_num in range (MAX_STEP):\n",
    "        # time.sleep(1)  # 1秒間停止\n",
    "        act = agt.select_action(obs) #obsにより，行動を決める　Qテーブルのそのマス目を用意して，たまにイプシロングリーディでランダム動作が起きる\n",
    "        # print(\"Qtableは：\",str(obs),agt.Q[str(obs)])\n",
    "        # print(env.wheel_ang_left)\n",
    "        rwd, done, next_obs = env.step(act)\n",
    "        \n",
    "        agt.learn(obs, act, rwd, done, next_obs)\n",
    "        # print(obs)####################\n",
    "        obs = next_obs\n",
    "        sumreward += rwd\n",
    "        if done == True:\n",
    "            obs = env.reset() #エピソード初期の観測を取得\n",
    "            break\n",
    "    epi_reward_graph.append(sumreward)\n",
    "    sumreward = 0\n",
    "\n",
    "    # print(episode_num % 10)\n",
    "\n",
    "    \n",
    "    if episode_num % 10 == 9: #10エピソードごとにQテーブル保存\n",
    "        if QTABLE_SAVE_MODE:\n",
    "            # Qテーブルを保存する\n",
    "            # 保存したい変数やリストを辞書にまとめる\n",
    "            data_to_save = { #辞書でまとめてます\n",
    "                'agt.Q': agt.Q,\n",
    "                'agt.len_Q': agt.len_Q,\n",
    "            }\n",
    "            with open('12/10.pkl', 'wb') as f:\n",
    "                pickle.dump(data_to_save, f)\n",
    "            print(\"Qtable保存しました\")\n",
    "\n",
    "def generate_numbered_array(length):\n",
    "    return list(range(1, length+1))\n",
    "\n",
    "# 例: 長さ指定の配列を生成\n",
    "timestep_count = generate_numbered_array((episode_num+1))\n",
    "plt.xlabel(\"episode数\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"reward\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.title(\"エピソードごとの報酬\", fontname=\"MS Gothic\")\n",
    "plt.plot(timestep_count, epi_reward_graph, linestyle='solid', label=\"reward\")\n",
    "plt.legend(prop={'family':'MS Gothic'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "途中で学習止めた時用のグラフ出すコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, length))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 例: 長さ指定の配列を生成\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m timestep_count \u001b[38;5;241m=\u001b[39m generate_numbered_array((\u001b[43mepisode_num\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode数\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMS Gothic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMS Gothic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'episode_num' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_numbered_array(length):\n",
    "    return list(range(1, length))\n",
    "\n",
    "# 例: 長さ指定の配列を生成\n",
    "timestep_count = generate_numbered_array((episode_num+1))\n",
    "plt.xlabel(\"episode数\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"reward\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.title(\"エピソードごとの報酬\", fontname=\"MS Gothic\")\n",
    "plt.plot(timestep_count, epi_reward_graph, linestyle='solid', label=\"reward\")\n",
    "plt.legend(prop={'family':'MS Gothic'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qtableは： 3232 [-0.0018317902756860383, -0.004220287971822212, -0.002526959236582997, -2.3073350184116115e-07]\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Qtableは：\",str(obs),agt.Q[str(obs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.99999060e-01  9.92104958e-24 -1.37118702e-03  4.89481227e-20]\n"
     ]
    }
   ],
   "source": [
    "print(env.data.xquat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (99,) and (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\atusi\\OneDrive\\ドキュメント\\研究関連（修士）\\Github\\sibuya\\original_RL_mouse.ipynb セル 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atusi/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/%E7%A0%94%E7%A9%B6%E9%96%A2%E9%80%A3%EF%BC%88%E4%BF%AE%E5%A3%AB%EF%BC%89/Github/sibuya/original_RL_mouse.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mgrid()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/atusi/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/%E7%A0%94%E7%A9%B6%E9%96%A2%E9%80%A3%EF%BC%88%E4%BF%AE%E5%A3%AB%EF%BC%89/Github/sibuya/original_RL_mouse.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39m合計報酬推移\u001b[39m\u001b[39m\"\u001b[39m, fontname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMS Gothic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/atusi/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/%E7%A0%94%E7%A9%B6%E9%96%A2%E9%80%A3%EF%BC%88%E4%BF%AE%E5%A3%AB%EF%BC%89/Github/sibuya/original_RL_mouse.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39;49mplot(timestep_count, epi_reward_graph, linestyle\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msolid\u001b[39;49m\u001b[39m'\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreward\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atusi/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/%E7%A0%94%E7%A9%B6%E9%96%A2%E9%80%A3%EF%BC%88%E4%BF%AE%E5%A3%AB%EF%BC%89/Github/sibuya/original_RL_mouse.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend(prop\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mfamily\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mMS Gothic\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atusi/OneDrive/%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88/%E7%A0%94%E7%A9%B6%E9%96%A2%E9%80%A3%EF%BC%88%E4%BF%AE%E5%A3%AB%EF%BC%89/Github/sibuya/original_RL_mouse.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\colab_env\\lib\\site-packages\\matplotlib\\pyplot.py:2869\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2867\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2868\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2869\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2870\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2871\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\colab_env\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\colab_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\colab_env\\lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (99,) and (100,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 例: 長さ指定の配列を生成\n",
    "def generate_numbered_array(length):\n",
    "    return list(range(1, length+0))\n",
    "timestep_count = generate_numbered_array((episode_num+1))\n",
    "plt.xlabel(\"時間t\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"報酬\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.title(\"合計報酬推移\", fontname=\"MS Gothic\")\n",
    "plt.plot(timestep_count, epi_reward_graph, linestyle='solid', label=\"reward\")\n",
    "plt.legend(prop={'family':'MS Gothic'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.001]\n",
      " [ 0.04   0.03   0.001]\n",
      " [ 0.04  -0.03   0.001]\n",
      " [-0.04   0.03   0.001]\n",
      " [-0.04  -0.03   0.001]\n",
      " [-0.027  0.     0.001]\n",
      " [ 0.027  0.     0.001]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(env.model.geom_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "digitize() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ob1 \u001b[39m=\u001b[39m \u001b[39m0.7\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m obs\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mdigitize(ob1, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.18\u001b[39;49m, \u001b[39m10\u001b[39;49m)\u001b[39m*\u001b[39m\u001b[39m1000\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:198\u001b[0m, in \u001b[0;36mdigitize\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: digitize() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ob1 = 0.7\n",
    "obs=np.digitize(ob1, 0.0, 0.18, 10)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agt_netQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\requests\\compat.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# import torch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# from torch import nn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# import torch.optim as optim\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# from torch.utils.data import DataLoader\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# from torchvision import datasets, transforms\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Flatten\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmujoco\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\__init__.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m autograph\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:37\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[39mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v1\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m autograph\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m debugging\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mimport\u001b[39;00m errors\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\__init__.py:182\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m cluster_resolver\n\u001b[0;32m    181\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m coordinator\n\u001b[1;32m--> 182\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m    183\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollective_all_reduce_strategy\u001b[39;00m \u001b[39mimport\u001b[39;00m CollectiveAllReduceStrategy \u001b[39mas\u001b[39;00m MultiWorkerMirroredStrategy\n\u001b[0;32m    184\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcross_device_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossDeviceOps\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m coordinator\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m partitioners\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m rpc\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcentral_storage_strategy\u001b[39;00m \u001b[39mimport\u001b[39;00m CentralStorageStrategy\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollective_all_reduce_strategy\u001b[39;00m \u001b[39mimport\u001b[39;00m _CollectiveAllReduceStrategyExperimental \u001b[39mas\u001b[39;00m MultiWorkerMirroredStrategy\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\rpc\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m\"\"\"Public API for tf.distribute.experimental.rpc namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrpc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrpc_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrpc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrpc_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Server\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\python\\distribute\\experimental\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m parameter_server_strategy\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m tpu_strategy\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfailure_handling\u001b[39;00m \u001b[39mimport\u001b[39;00m failure_handling\n\u001b[0;32m     23\u001b[0m \u001b[39m# pylint: enable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m checkpoint_management\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m multi_worker_util\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfailure_handling\u001b[39;00m \u001b[39mimport\u001b[39;00m gce_util\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m constant_op\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\gce_util.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39murllib\u001b[39;00m \u001b[39mimport\u001b[39;00m request\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\requests\\__init__.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib3\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\requests\\exceptions.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mrequests.exceptions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[39mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m BaseHTTPError\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m CompatJSONDecodeError\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRequestException\u001b[39;00m(\u001b[39mIOError\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m    request.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\requests\\compat.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# -------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Pythons\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# -------\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\env_iRL\\lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[39m=\u001b[39m Union[\u001b[39mstr\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mos.PathLike[str]\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# agt_netQ.py\n",
    "# ニューラルネット（Qネットワーク）を使ったQ学習アルゴリズム\n",
    "# \"\"\"\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import datasets, transforms\n",
    "# # from tensorflow.keras.models import Sequential\n",
    "# # from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import mujoco\n",
    "\n",
    "# # 自作モジュール\n",
    "# # import core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # ネットワークの定義\n",
    "# class SimpleNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 32)  # 入力層から中間層への結合\n",
    "#         self.fc2 = nn.Linear(32, 2)  # 中間層から出力層への結合\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))  # 中間層でReLUを使用\n",
    "#         x = self.fc2(x)  # 出力層では活性化関数を使わない（恒等関数）\n",
    "#         return x\n",
    "\n",
    "# # ネットワークのインスタンス化\n",
    "# net = SimpleNet()\n",
    "\n",
    "# # オプティマイザの定義\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# # ネットワークの構造を表示\n",
    "# print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NetQAgt(coreAgt):\n",
    "#     \"\"\" Qネットワークを使ったQ学習エージェントクラス \"\"\"\n",
    "#     def __init__(               # 引数とデフォルト値の設定 (A)\n",
    "#             self,\n",
    "#             n_act=3,            # int: 行動の種類数（ネットワークの出力数）\n",
    "#             input_size=(4,),    # tuple of int: 入力サイズ\n",
    "#             n_dense=32,         # int: 中間層のニューロン数\n",
    "#             epsilon=0.1,        # float: 乱雑度\n",
    "#             gamma=0.9,          # float: 割引率\n",
    "#             filepath=None,      # str: 保存ファイル名\n",
    "#             ):\n",
    "#         \"\"\" 初期処理 \"\"\"\n",
    "#         # 引数の設定は適時編集\n",
    "#         self.epsilon = epsilon\n",
    "#         # ------------------------- 編集ここから\n",
    "\n",
    "#         # アトリビュートにパラメータを保存 (B)\n",
    "#         self.n_act = n_act\n",
    "#         self.input_size = input_size\n",
    "#         self.n_dense = n_dense\n",
    "#         self.gamma = gamma\n",
    "#         self.filepath = filepath\n",
    "\n",
    "#         # アトリビュートにモデルを保存 (C)\n",
    "#         self.model = self._build_Qnet()\n",
    "#         # ------------------------- ここまで\n",
    "\n",
    "#     def _build_Qnet(self):\n",
    "#         \"\"\" 指定したパラメータでQネットワークを構築 \"\"\"\n",
    "#         # Qネットワークの構築 (A)\n",
    "#         self.net = SimpleNet()\n",
    "#         # オプティマイザの定義\n",
    "#         self.optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "#         # 損失関数の定義\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#         return net\n",
    "\n",
    "#     def select_action(self, obs):\n",
    "#         \"\"\"  観測に対して行動を出力 \"\"\"\n",
    "#     # ------------------------- 編集ここから\n",
    "#     # 確率的に処理を分岐 (A)\n",
    "#         if np.random.rand() < self.epsilon:\n",
    "#             # ランダム行動 (B)\n",
    "#             act = np.random.randint(0, self.n_act)\n",
    "#         else:\n",
    "#             # obsに対するQ値のリストを取得 (C)\n",
    "#             Q = self.get_Q(obs)\n",
    "\n",
    "#             # Qを最大にする行動\n",
    "#             act = np.argmax(Q)\n",
    "#         # ------------------------- ここまで\n",
    "#         return act\n",
    "\n",
    "#     def get_Q(self, obs):\n",
    "#         \"\"\" 観測に対するQ値を出力 \"\"\"\n",
    "#         # ------------------------- 編集ここから\n",
    "#         # 観測obsを入力し出力を得る (A)\n",
    "#         Q = self.model.predict(\n",
    "#             obs.reshape((1,) + self.input_size))[0, :]\n",
    "#         # ------------------------- ここまで\n",
    "#         return Q\n",
    "\n",
    "#     def learn(self, obs, act, rwd, done, next_obs):\n",
    "#         \"\"\" 学習 \"\"\"\n",
    "#         if rwd is None:\n",
    "#             return\n",
    "#         # ------------------------- 編集ここから\n",
    "\n",
    "#         # obs に対するQネットワークの出力yを得る (A)\n",
    "#         y = self.get_Q(obs)\n",
    "\n",
    "#         # target にyの内容をコピーする (B)\n",
    "#         target = y.copy()\n",
    "\n",
    "#         if done is False:\n",
    "#             # 最終状態でなかったら next_obsに対する next_yを得る(C)\n",
    "#             next_y = self.get_Q(next_obs)\n",
    "\n",
    "#             # Q[obs][act]のtarget_actを作成 (D)\n",
    "#             target_act = rwd + self.gamma * max(next_y)\n",
    "#         else:\n",
    "#             # 最終状態の場合は報酬だけでtarget_actを作成 (E)\n",
    "#             target_act = rwd\n",
    "\n",
    "#         # targetのactの要素だけtarget_actにする (F)\n",
    "#         target[act] = target_act\n",
    "\n",
    "#         # obsと target のペアを与えて学習 (G)\n",
    "\n",
    "#         # 学習のループ\n",
    "#         for epoch in range(1):  # 1エポックの例\n",
    "#             optimizer.zero_grad()  # 勾配をリセット\n",
    "#             output = net(obs)  # フォワードパス\n",
    "#             loss = self.criterion(output, target)  # 損失の計算\n",
    "#             loss.backward()  # バックワードパス（勾配の計算）\n",
    "#             optimizer.step()  # パラメータの更新\n",
    "#         # ------------------------- ここまで\n",
    "#         return \n",
    "\n",
    "#     # def save_weights(self, filepath=None):\n",
    "#     #     \"\"\" モデルの重みデータの保存 \"\"\"\n",
    "#     #     # ------------------------- 編集ここから\n",
    "#     #     if filepath is None:\n",
    "#     #         filepath = self.filepath\n",
    "#     #     self.model.save(filepath + '.h5', overwrite=True)\n",
    "#     #     # ------------------------- ここまで\n",
    "\n",
    "#     # def load_weights(self, filepath=None):\n",
    "#     #     \"\"\" モデルの重みデータの読み込み \"\"\"\n",
    "#     #     # ------------------------- 編集ここから\n",
    "#     #     if filepath is None:\n",
    "#     #         filepath = self.filepath\n",
    "#     #     self.model = tf.keras.models.load_model(filepath + '.h5')\n",
    "#     #     # ------------------------- ここまで\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # エージェントのインスタンス生成 (A)\n",
    "#     agt = NetQAgt(n_act=3, input_size=(5,))\n",
    "\n",
    "#     # 行動選択 (B)\n",
    "#     obs = np.array([[1, 1, 1, 1, 1]])\n",
    "#     act = agt.select_action(obs)\n",
    "#     print('act', act)\n",
    "\n",
    "#     # 学習 (C)\n",
    "#     rwd = 1\n",
    "#     done = False\n",
    "#     next_obs = np.array([[1, 1, 1, 1, 2]])\n",
    "#     agt.learn(obs, act, rwd, done, next_obs)\n",
    "\n",
    "#     # モデル構造の表示 (D)\n",
    "#     print('モデルの構造')\n",
    "#     agt.model.summary()\n",
    "\n",
    "#     # 重みパラメータの保存 (E)\n",
    "#     agt.save_weights('agt_data/test')\n",
    "\n",
    "#     # 重みパラメータの読み込み (F)\n",
    "#     agt.load_weights('agt_data/test')\n",
    "\n",
    "#     # モデルへの観測の入力 (G)\n",
    "#     y = agt.model.predict(obs)\n",
    "#     print('モデルの出力 y', y.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mujocoで学習開始!!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleNet' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     act \u001b[39m=\u001b[39m agt\u001b[39m.\u001b[39;49mselect_action(obs) \u001b[39m#たまにQ値最適の行動を選ぶ　グリーディほう\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     rwd, done, next_obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(act) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     reward_for_graph\u001b[39m.\u001b[39mappend(rwd)\n",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     act \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_act)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# obsに対するQ値のリストを取得 (C)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_Q(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# Qを最大にする行動\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     act \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(Q)\n",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" 観測に対するQ値を出力 \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# ------------------------- 編集ここから\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# 観測obsを入力し出力を得る (A)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     obs\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size))[\u001b[39m0\u001b[39m, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# ------------------------- ここまで\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Q\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\takara\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleNet' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# from tqdm import trange\n",
    "# print('mujocoで学習開始!!!')\n",
    "# reward_for_graph = []\n",
    "\n",
    "# env=MyEnv()\n",
    "\n",
    "# agt= NetQAgt(2,4,32,0.1,0.9,)\n",
    "\n",
    "# obs = env.reset()\n",
    "# for t in range(100):\n",
    "#     act = agt.select_action(obs) #たまにQ値最適の行動を選ぶ　グリーディほう\n",
    "#     rwd, done, next_obs = env.step(act) \n",
    "#     reward_for_graph.append(rwd)\n",
    "\n",
    "#     agt.learn(obs, act, rwd, done, next_obs)\n",
    "\n",
    "#     obs = next_obs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\myprojects\\iRL\\main\\RL_mouse.ipynb セル 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/myprojects/iRL/main/RL_mouse.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fashion_mnist \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mfashion_mnist\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'keras'"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 離散化コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(value):\n",
    "    # 0から0.18の範囲を10段階に分割\n",
    "    if value == -1:\n",
    "        value = 0.18\n",
    "    discrete_value = int(value / 0.018)\n",
    "    return min(discrete_value, 9)  # 最大値が10段階目になるように調整 9以上にならないようにしている\n",
    "\n",
    "def discretize_values(ob1, ob2, ob3, ob4):\n",
    "    # 各値を離散化\n",
    "    discrete_ob1 = discretize(ob1)\n",
    "    discrete_ob2 = discretize(ob2)\n",
    "    discrete_ob3 = discretize(ob3)\n",
    "    discrete_ob4 = discretize(ob4)\n",
    "    sum = discrete_ob1*1000 + discrete_ob2*100 + discrete_ob3*10 + discrete_ob4*1\n",
    "\n",
    "    return discrete_ob1, discrete_ob2, discrete_ob3, discrete_ob4, sum\n",
    "\n",
    "# 例: ob1=0.05, ob2=0.10, ob3=0.15, ob4=0.18 の場合\n",
    "ob1 = 0.14\n",
    "ob2 = 0.10\n",
    "ob3 = 0.15\n",
    "ob4 = 0.18\n",
    "\n",
    "discrete_ob1, discrete_ob2, discrete_ob3, discrete_ob4, sum = discretize_values(ob1, ob2, ob3, ob4)\n",
    "\n",
    "print(f\"Discretized Values: {discrete_ob1}, {discrete_ob2}, {discrete_ob3}, {discrete_ob4}, {sum}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_iRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
