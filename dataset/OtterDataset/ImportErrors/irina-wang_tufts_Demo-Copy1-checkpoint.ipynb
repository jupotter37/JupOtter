{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "formed-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Nobody should eat worms.',\n",
    "    'Kamala loves cats, dogs, and fish.',\n",
    "    'Cats love fish, and dogs love bones.',\n",
    "    'Kamala bought a dog. Kamala pet a cat. Kamala fed a worm and a bone to the dog.',\n",
    "    'The dog chased the cat, which made Kamala cry. The dog was sorry.',\n",
    "    'The dog loved Kamala, which is why the dog was sorry. Kamala felt better.',\n",
    "    'Kamala is chasing the cat-fish.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.loadtxt('./data/x_train.csv', delimiter=',', skiprows=1)\n",
    "x_te = np.loadtxt('./data_digits_8_vs_9_noisy/x_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "y_tr = np.loadtxt('./data_digits_8_vs_9_noisy/y_train.csv', delimiter=',', skiprows=1)\n",
    "y_te = np.loadtxt('./data_digits_8_vs_9_noisy/y_test.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mathematical-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-marriage",
   "metadata": {},
   "source": [
    "### Bag of words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indian-congo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of feat:  33\n",
      "['and', 'better', 'bone', 'bones', 'bought', 'cat', 'cats', 'chased', 'chasing', 'cry', 'dog', 'dogs', 'eat', 'fed', 'felt', 'fish', 'is', 'kamala', 'love', 'loved', 'loves', 'made', 'nobody', 'pet', 'should', 'sorry', 'the', 'to', 'was', 'which', 'why', 'worm', 'worms']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0 0 0 0 2 0 0 1 0 0 0 3 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 1 0 1 2 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 3 0 1 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 2 0 0 0 1 0 1 2 0 1 0 0 0 0 0 1 2 0 1 1 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>better</th>\n",
       "      <th>bone</th>\n",
       "      <th>bones</th>\n",
       "      <th>bought</th>\n",
       "      <th>cat</th>\n",
       "      <th>cats</th>\n",
       "      <th>chased</th>\n",
       "      <th>chasing</th>\n",
       "      <th>cry</th>\n",
       "      <th>...</th>\n",
       "      <th>pet</th>\n",
       "      <th>should</th>\n",
       "      <th>sorry</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>was</th>\n",
       "      <th>which</th>\n",
       "      <th>why</th>\n",
       "      <th>worm</th>\n",
       "      <th>worms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  better  bone  bones  bought  cat  cats  chased  chasing  cry  ...  \\\n",
       "0    0       0     0      0       0    0     0       0        0    0  ...   \n",
       "1    1       0     0      0       0    0     1       0        0    0  ...   \n",
       "2    1       0     0      1       0    0     1       0        0    0  ...   \n",
       "3    1       0     1      0       1    1     0       0        0    0  ...   \n",
       "4    0       0     0      0       0    1     0       1        0    1  ...   \n",
       "5    0       1     0      0       0    0     0       0        0    0  ...   \n",
       "6    0       0     0      0       0    1     0       0        1    0  ...   \n",
       "\n",
       "   pet  should  sorry  the  to  was  which  why  worm  worms  \n",
       "0    0       1      0    0   0    0      0    0     0      1  \n",
       "1    0       0      0    0   0    0      0    0     0      0  \n",
       "2    0       0      0    0   0    0      0    0     0      0  \n",
       "3    1       0      0    1   1    0      0    0     1      0  \n",
       "4    0       0      1    3   0    1      1    0     0      0  \n",
       "5    0       0      1    2   0    1      1    1     0      0  \n",
       "6    0       0      0    1   0    0      0    0     0      0  \n",
       "\n",
       "[7 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "x = count_vectorizer.fit_transform(documents)\n",
    "print('Num of feat: ', len(x.toarray()[0]))\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(x.toarray())\n",
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-damages",
   "metadata": {},
   "source": [
    "### Remove stop words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of feat:  22\n",
      "['better', 'bone', 'bones', 'bought', 'cat', 'cats', 'chased', 'chasing', 'dog', 'dogs', 'eat', 'fed', 'felt', 'fish', 'kamala', 'love', 'loved', 'loves', 'pet', 'sorry', 'worm', 'worms']\n",
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 2 0 0 0 0 0 0]\n",
      " [0 1 0 1 1 0 0 0 2 0 0 1 0 0 3 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 1 0 2 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 2 0 0 0 1 0 2 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>better</th>\n",
       "      <th>bone</th>\n",
       "      <th>bones</th>\n",
       "      <th>bought</th>\n",
       "      <th>cat</th>\n",
       "      <th>cats</th>\n",
       "      <th>chased</th>\n",
       "      <th>chasing</th>\n",
       "      <th>dog</th>\n",
       "      <th>dogs</th>\n",
       "      <th>...</th>\n",
       "      <th>felt</th>\n",
       "      <th>fish</th>\n",
       "      <th>kamala</th>\n",
       "      <th>love</th>\n",
       "      <th>loved</th>\n",
       "      <th>loves</th>\n",
       "      <th>pet</th>\n",
       "      <th>sorry</th>\n",
       "      <th>worm</th>\n",
       "      <th>worms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   better  bone  bones  bought  cat  cats  chased  chasing  dog  dogs  ...  \\\n",
       "0       0     0      0       0    0     0       0        0    0     0  ...   \n",
       "1       0     0      0       0    0     1       0        0    0     1  ...   \n",
       "2       0     0      1       0    0     1       0        0    0     1  ...   \n",
       "3       0     1      0       1    1     0       0        0    2     0  ...   \n",
       "4       0     0      0       0    1     0       1        0    2     0  ...   \n",
       "5       1     0      0       0    0     0       0        0    2     0  ...   \n",
       "6       0     0      0       0    1     0       0        1    0     0  ...   \n",
       "\n",
       "   felt  fish  kamala  love  loved  loves  pet  sorry  worm  worms  \n",
       "0     0     0       0     0      0      0    0      0     0      1  \n",
       "1     0     1       1     0      0      1    0      0     0      0  \n",
       "2     0     1       0     2      0      0    0      0     0      0  \n",
       "3     0     0       3     0      0      0    1      0     1      0  \n",
       "4     0     0       1     0      0      0    0      1     0      0  \n",
       "5     1     0       2     0      1      0    0      1     0      0  \n",
       "6     0     1       1     0      0      0    0      0     0      0  \n",
       "\n",
       "[7 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "x = count_vectorizer.fit_transform(documents)\n",
    "print('Num of feat: ', len(x.toarray()[0]))\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(x.toarray())\n",
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-pressing",
   "metadata": {},
   "source": [
    "**Note: tense, persons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = count_vectorizer.build_analyzer()\n",
    "print([analyze(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-accommodation",
   "metadata": {},
   "source": [
    "### Build your own tokenizer ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spare-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9]\", \" \", str_input).lower().split()\n",
    "    \n",
    "    def prune_food(w):\n",
    "        if w == 'bones' or w == 'bone' or w == 'fish' or w == 'worms' or w == 'worm':\n",
    "            w = 'food'\n",
    "        return w\n",
    "    \n",
    "    words = [prune_food(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thirty-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of feat:  18\n",
      "['better', 'bought', 'cat', 'cats', 'chased', 'chasing', 'dog', 'dogs', 'eat', 'fed', 'felt', 'food', 'kamala', 'love', 'loved', 'loves', 'pet', 'sorry']\n",
      "[[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 2 0 2 0 0 0 0]\n",
      " [0 1 1 0 0 0 2 0 0 1 0 2 3 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 2 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 2 0 0 0 1 0 2 0 1 0 0 1]\n",
      " [0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>better</th>\n",
       "      <th>bought</th>\n",
       "      <th>cat</th>\n",
       "      <th>cats</th>\n",
       "      <th>chased</th>\n",
       "      <th>chasing</th>\n",
       "      <th>dog</th>\n",
       "      <th>dogs</th>\n",
       "      <th>eat</th>\n",
       "      <th>fed</th>\n",
       "      <th>felt</th>\n",
       "      <th>food</th>\n",
       "      <th>kamala</th>\n",
       "      <th>love</th>\n",
       "      <th>loved</th>\n",
       "      <th>loves</th>\n",
       "      <th>pet</th>\n",
       "      <th>sorry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   better  bought  cat  cats  chased  chasing  dog  dogs  eat  fed  felt  \\\n",
       "0       0       0    0     0       0        0    0     0    1    0     0   \n",
       "1       0       0    0     1       0        0    0     1    0    0     0   \n",
       "2       0       0    0     1       0        0    0     1    0    0     0   \n",
       "3       0       1    1     0       0        0    2     0    0    1     0   \n",
       "4       0       0    1     0       1        0    2     0    0    0     0   \n",
       "5       1       0    0     0       0        0    2     0    0    0     1   \n",
       "6       0       0    1     0       0        1    0     0    0    0     0   \n",
       "\n",
       "   food  kamala  love  loved  loves  pet  sorry  \n",
       "0     1       0     0      0      0    0      0  \n",
       "1     1       1     0      0      1    0      0  \n",
       "2     2       0     2      0      0    0      0  \n",
       "3     2       3     0      0      0    1      0  \n",
       "4     0       1     0      0      0    0      1  \n",
       "5     0       2     0      1      0    0      1  \n",
       "6     1       1     0      0      0    0      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', tokenizer=simple_tokenizer)\n",
    "x = count_vectorizer.fit_transform(documents)\n",
    "print('Num of feat: ', len(x.toarray()[0]))\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(x.toarray())\n",
    "pd.DataFrame(x.toarray(), columns=count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-geography",
   "metadata": {},
   "source": [
    "### More Complex tokenizer - nltk ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dangerous-wilderness",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1d2184025e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "consistent-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d251bc70ebe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mporterstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# same stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporterstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'worm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porterstemmer = PorterStemmer()\n",
    "\n",
    "# same stem \n",
    "print(porterstemmer.stem('worm'))\n",
    "print(porterstemmer.stem('worms'))\n",
    "print(porterstemmer.stem('worming'))\n",
    "print(porterstemmer.stem('wormed'))\n",
    "\n",
    "\n",
    "# irregular \n",
    "print(porterstemmer.stem('is'))\n",
    "print(porterstemmer.stem('are'))\n",
    "print(porterstemmer.stem('were'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stop_stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    \n",
    "    # remove stops using nltk\n",
    "    stops = stopwords.words('english')\n",
    "    words = [w for w in words if w not in stops]\n",
    "    \n",
    "    # stemming \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "express-planner",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_stemming_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8d3d76358a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_stemming_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Num of feat: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_stemming_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=stop_stemming_tokenizer)\n",
    "x = count_vectorizer.fit_transform(documents)\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print('Num of feat: ', len(features))\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-speaker",
   "metadata": {},
   "source": [
    "## TF / IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "grave-planet",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (/Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7f6ca63074ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (/Users/mac/opt/anaconda3/envs/ml135_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['x', 'xxxx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infrared-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nervous-trunk",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-22f09222157b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_stemming_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidVectorizer(tokenizer=stop_stemming_tokenizer, use_idf = False)\n",
    "x = count_vectorizer.fit_transform(sentences)\n",
    "features = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "heard-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf - weighs how common the word is in the entire document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "descending-probe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-92f8607575cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_stemming_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidVectorizer(tokenizer=stop_stemming_tokenizer, use_idf = False)\n",
    "x = count_vectorizer.fit_transform(sentences)\n",
    "features = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
