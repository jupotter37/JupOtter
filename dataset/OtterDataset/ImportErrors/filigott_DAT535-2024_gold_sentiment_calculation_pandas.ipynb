{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Calculation Pandas UDF\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"9\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.extraPythonPackages\", \"pandas,vaderSentiment\") \\\n",
    "    .config(\"spark.driver.extraPythonPackages\", \"pandas,vaderSentiment\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level for cleaner outputs\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:42.015 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "09:29:42.022 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/ubuntu/project/cluster-notebooks/spark-warehouse'.\n",
      "09:29:42.035 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "09:29:42.038 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f98dda2{/SQL,null,AVAILABLE,@Spark}\n",
      "09:29:42.038 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "09:29:42.039 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63231786{/SQL/json,null,AVAILABLE,@Spark}\n",
      "09:29:42.040 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "09:29:42.041 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4c125f51{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "09:29:42.041 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "09:29:42.042 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7c81e633{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "09:29:42.043 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "09:29:42.044 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bbbcee2{/static/sql,null,AVAILABLE,@Spark}\n",
      "09:29:43.031 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 159 ms to list leaf files for 1 paths.\n",
      "09:29:43.278 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "09:29:43.303 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "09:29:43.303 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "09:29:43.304 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "09:29:43.306 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "09:29:43.314 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "09:29:43.383 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 127.4 KiB, free 2004.5 MiB)\n",
      "09:29:43.408 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 2004.4 MiB)\n",
      "09:29:43.410 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on namenode:38767 (size: 47.0 KiB, free: 2004.6 MiB)\n",
      "09:29:43.413 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "09:29:43.428 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "09:29:43.429 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "09:29:43.458 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (datanode2, executor 3, partition 0, PROCESS_LOCAL, 9222 bytes) \n",
      "09:29:43.753 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on datanode2:35035 (size: 47.0 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:45.350 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1904 ms on datanode2 (executor 3) (1/1)\n",
      "09:29:45.352 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "09:29:45.360 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.024 s\n",
      "09:29:45.364 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "09:29:45.364 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 0: Stage finished\n",
      "09:29:45.366 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.087515 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:45.572 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on namenode:38767 in memory (size: 47.0 KiB, free: 2004.6 MiB)\n",
      "09:29:45.593 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on datanode2:35035 in memory (size: 47.0 KiB, free: 912.3 MiB)\n",
      "root\n",
      " |-- link: string (nullable = true)\n",
      " |-- date: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- sub_reddit: string (nullable = true)\n",
      " |-- post_id: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n",
      "09:29:46.376 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "09:29:46.377 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "09:29:46.733 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 182.217068 ms\n",
      "09:29:46.756 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 409.5 KiB, free 2004.2 MiB)\n",
      "09:29:46.770 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 45.1 KiB, free 2004.2 MiB)\n",
      "09:29:46.771 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on namenode:38767 (size: 45.1 KiB, free: 2004.6 MiB)\n",
      "09:29:46.771 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
      "09:29:46.782 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "09:29:46.821 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "09:29:46.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "09:29:46.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "09:29:46.823 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "09:29:46.825 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "09:29:46.826 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "09:29:46.862 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 18.1 KiB, free 2004.1 MiB)\n",
      "09:29:46.863 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 2004.1 MiB)\n",
      "09:29:46.864 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on namenode:38767 (size: 6.9 KiB, free: 2004.5 MiB)\n",
      "09:29:46.864 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "09:29:46.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "09:29:46.865 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 1.0 with 1 tasks resource profile 0\n",
      "09:29:46.872 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (datanode3, executor 2, partition 0, NODE_LOCAL, 9864 bytes) \n",
      "09:29:47.053 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode3:40289 (size: 6.9 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:47.832 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode3:40289 (size: 45.1 KiB, free: 912.2 MiB)\n",
      "09:29:49.280 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 2410 ms on datanode3 (executor 2) (1/1)\n",
      "09:29:49.280 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "09:29:49.282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.445 s\n",
      "09:29:49.282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "09:29:49.282 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 1: Stage finished\n",
      "09:29:49.283 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.461835 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:49.981 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on namenode:38767 in memory (size: 6.9 KiB, free: 2004.6 MiB)\n",
      "09:29:49.992 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode3:40289 in memory (size: 6.9 KiB, free: 912.3 MiB)\n",
      "09:29:50.091 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.863731 ms\n",
      "+----------------------------------------------------------------------------------------------------------------+----------+-----+---------------------+-------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|link                                                                                                            |date      |score|sub_reddit           |post_id|comment_id|body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------+----------+-----+---------------------+-------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|https://old.reddit.com/r/slp/comments/l7uc5x/snf_pros_and_cons/gl99kpj/                                         |1611939144|54   |slp                  |l7uc5x |gl99kpj   |as someone who worked in a snf for a year here are some questions to ask in an interview what is the productivity standard? is that productivity standard expected to change any time soon? ive been burned before for not asking this how many beds? it usually takes 100150 beds to support one fulltime slp because only a minority need slp services. what happens if productivity is low that day for reasons out of your control? how much pto will you be getting? are there systems in place for you taking pto without much trouble? prn who can cover slps in sister facilities who can cover ask for a tour of the facility. look in patient rooms. get a feel for the cleanliness of the place. is it well lit? any natural light? do the patients look well cared for? do the patients have their call light buttons handy or is it across the room? if you smell urinefeces thats a bad sign. how long does the average patient wait for a response after they hit their call light? if the facility has crappy andor perpetually understaffed cnas guess who gets to hear about it from patients day in and day out? you. when was the last time someone got a raise? when can you expect a raise if all goes well? how long have the rehab therapists been working there? if theyve all been working there less than a year or two thats a bad sign how long did the previous slps work here? what led to the opening of this position? aka why did the last slp quit what kind of supervision and guidance will you receive as a cf? what is their covid response? is fresh ppe supplied every day such that employees do not need to reuse ppe between days?|\n",
      "|https://old.reddit.com/r/europe/comments/l7meq9/covid_deaths_per_million_inhabitants_january_29th/gl99kad/      |1611939140|4    |europe               |l7meq9 |gl99kad   |give it a rest. theres other countries that have densely populated areas being hit worse than their sparsely populated areas. youre not unique in this regard. and why are you even pissed off? theres worse things to be getting upset about. the essence of your complaint is that england is dying from covid at a worse rate than scotland and making you look bad? have a word with yourself.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|https://old.reddit.com/r/alberta/comments/l74q3r/does_it_bother_anyone_else_that_they_arent/gl99k36/            |1611939138|1    |alberta              |l74q3r |gl99k36   |honestly this is one of the biggest disappointments with covid. there was an excellent opportunity for widespread meaningful economic reforms to help support us through the pandemic. it was hinted at so many times but no one was brave enough to take advantage and do something. instead the rich got richer while a huge chunk of the population will be facing a housing crisis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|https://old.reddit.com/r/Random_Acts_Of_Amazon/comments/l6lmvi/thanks/gl99jqi/                                  |1611939134|2    |Random_Acts_Of_Amazon|l6lmvi |gl99jqi   |thanks they are for my classroom. i have a few kids who use them regularly and with covid they keep them at their seats so no one else uses them.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|https://old.reddit.com/r/wallstreetbets/comments/l7ww16/please_remember_to_donate_some_of_the_spoils_of/gl99izx/|1611939127|7    |wallstreetbets       |l7ww16 |gl99izx   |damn... im glad youre still here with us today. thank you for being on the frontlines against covid and on the battlefield against these short sellers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------------------------------------------------+----------+-----+---------------------+-------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HDFS path for the dataset\n",
    "file_path = \"hdfs://namenode:9000/data/cleaned_dataset.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display schema and sample rows\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean comments\n",
    "def clean_comment_spark(df, column):\n",
    "    \"\"\"Clean comments in the specified column.\"\"\"\n",
    "    return df.withColumn(\n",
    "        f\"{column}_clean\",\n",
    "        F.trim(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(\n",
    "                    F.regexp_replace(\n",
    "                        F.lower(F.col(column)),  # Convert to lowercase\n",
    "                        r\"http\\S+|www\\S+|https\\S+\", \"\"),  # Remove URLs\n",
    "                    r\"@\\w+|#\", \"\"),  # Remove mentions and hashtags\n",
    "                r\"[^\\w\\s]\", \"\"),  # Remove special characters and punctuation\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:29:50.273 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "09:29:50.273 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "09:29:50.325 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 35.398896 ms\n",
      "09:29:50.329 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 408.3 KiB, free 2003.8 MiB)\n",
      "09:29:50.336 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.9 KiB, free 2003.7 MiB)\n",
      "09:29:50.336 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on namenode:38767 (size: 44.9 KiB, free: 2004.5 MiB)\n",
      "09:29:50.337 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "09:29:50.339 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "09:29:50.351 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "09:29:50.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "09:29:50.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "09:29:50.355 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "09:29:50.357 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "09:29:50.358 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "09:29:50.360 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.4 KiB, free 2003.7 MiB)\n",
      "09:29:50.363 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 2003.7 MiB)\n",
      "09:29:50.363 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on namenode:38767 (size: 7.9 KiB, free: 2004.5 MiB)\n",
      "09:29:50.364 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "09:29:50.365 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "09:29:50.365 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 2.0 with 1 tasks resource profile 0\n",
      "09:29:50.369 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2) (datanode3, executor 2, partition 0, NODE_LOCAL, 9864 bytes) \n",
      "09:29:50.402 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode3:40289 (size: 7.9 KiB, free: 912.2 MiB)\n",
      "09:29:50.463 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode3:40289 (size: 44.9 KiB, free: 912.2 MiB)\n",
      "09:29:50.566 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 197 ms on datanode3 (executor 2) (1/1)\n",
      "09:29:50.567 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "09:29:50.568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.208 s\n",
      "09:29:50.568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "09:29:50.568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 2: Stage finished\n",
      "09:29:50.568 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.216731 s\n",
      "09:29:50.583 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 9.733897 ms\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|comment_id|body_clean                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|gl99kpj   |as someone who worked in a snf for a year here are some questions to ask in an interview what is the productivity standard is that productivity standard expected to change any time soon ive been burned before for not asking this how many beds it usually takes 100150 beds to support one fulltime slp because only a minority need slp services what happens if productivity is low that day for reasons out of your control how much pto will you be getting are there systems in place for you taking pto without much trouble prn who can cover slps in sister facilities who can cover ask for a tour of the facility look in patient rooms get a feel for the cleanliness of the place is it well lit any natural light do the patients look well cared for do the patients have their call light buttons handy or is it across the room if you smell urinefeces thats a bad sign how long does the average patient wait for a response after they hit their call light if the facility has crappy andor perpetually understaffed cnas guess who gets to hear about it from patients day in and day out you when was the last time someone got a raise when can you expect a raise if all goes well how long have the rehab therapists been working there if theyve all been working there less than a year or two thats a bad sign how long did the previous slps work here what led to the opening of this position aka why did the last slp quit what kind of supervision and guidance will you receive as a cf what is their covid response is fresh ppe supplied every day such that employees do not need to reuse ppe between days|\n",
      "|gl99kad   |give it a rest theres other countries that have densely populated areas being hit worse than their sparsely populated areas youre not unique in this regard and why are you even pissed off theres worse things to be getting upset about the essence of your complaint is that england is dying from covid at a worse rate than scotland and making you look bad have a word with yourself                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|gl99k36   |honestly this is one of the biggest disappointments with covid there was an excellent opportunity for widespread meaningful economic reforms to help support us through the pandemic it was hinted at so many times but no one was brave enough to take advantage and do something instead the rich got richer while a huge chunk of the population will be facing a housing crisis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|gl99jqi   |thanks they are for my classroom i have a few kids who use them regularly and with covid they keep them at their seats so no one else uses them                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|gl99izx   |damn im glad youre still here with us today thank you for being on the frontlines against covid and on the battlefield against these short sellers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the comments and select relevant columns\n",
    "df = clean_comment_spark(df, \"body\").select(\"comment_id\", \"body_clean\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 1.0.5 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/utils.py:27\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     have_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: pip install pyspark-pandas!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define a Pandas UDF for sentiment analysis\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;129m@pandas_udf\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPandasUDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_sentiment_udf\u001b[39m(body_clean):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate sentiment score using VADER.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m SentimentIntensityAnalyzer()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/functions.py:337\u001b[0m, in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mCreates a pandas user defined function (a.k.a. vectorized user defined function).\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03mpyspark.sql.UDFRegistration.register\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# The following table shows most of Pandas data and SQL type conversions in Pandas UDFs that\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# are not yet visible to the user. Some of behaviors are buggy and might be changed in the near\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# future. The table might have to be eventually documented externally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Note: Timezone is KST.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Note: 'X' means it throws an exception during the conversion.\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m require_minimum_pyarrow_version()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# decorator @pandas_udf(returnType, functionType)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/utils.py:34\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_pandas:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pandas_version\n\u001b[1;32m     36\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pandas\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pandas_version, pandas\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     41\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas >= 1.0.5 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "# TODO: pip install pyspark-pandas!\n",
    "\n",
    "# Define a Pandas UDF for sentiment analysis\n",
    "@pandas_udf(\"float\", PandasUDFType.SCALAR)\n",
    "def calculate_sentiment_udf(body_clean):\n",
    "    \"\"\"Calculate sentiment score using VADER.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return body_clean.apply(lambda text: analyzer.polarity_scores(text)['compound'] if text else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to calculate sentiment\n",
    "df = df.withColumn(\"sentiment\", calculate_sentiment_udf(F.col(\"body_clean\")))\n",
    "\n",
    "# Show a sample of the results\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to HDFS in Parquet format\n",
    "output_path = \"hdfs://namenode:9000/data/results/sentiment_calculations.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Results written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back the results and display a preview\n",
    "result_df = spark.read.parquet(output_path)\n",
    "result_df.show(10, truncate=False)\n",
    "result_count = result_df.count()\n",
    "print(f\"Total records processed: {result_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
