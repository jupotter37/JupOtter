{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "mqYTC814JZF6",
        "outputId": "5ef9b6df-2250-4ef0-8a83-6dc93ea61bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text                  Labels\n",
              "0  BEFORE THE MOTOR ACCIDENTS CLAIMS TRIBUNAL, KA...                   TITLE\n",
              "1  PETITIONER/S:\\n\\n1.\\tAmbika w/o Late Chandraka...              PETITIONER\n",
              "2  RESPONDENT/S\\n\\n1 \\tKapil s/o Shamu Chavan, Ag...  RESPONDENT INFORMATION"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54b12eee-86ee-4cd0-9799-9a0cde06ca4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BEFORE THE MOTOR ACCIDENTS CLAIMS TRIBUNAL, KA...</td>\n",
              "      <td>TITLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PETITIONER/S:\\n\\n1.\\tAmbika w/o Late Chandraka...</td>\n",
              "      <td>PETITIONER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RESPONDENT/S\\n\\n1 \\tKapil s/o Shamu Chavan, Ag...</td>\n",
              "      <td>RESPONDENT INFORMATION</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54b12eee-86ee-4cd0-9799-9a0cde06ca4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-54b12eee-86ee-4cd0-9799-9a0cde06ca4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-54b12eee-86ee-4cd0-9799-9a0cde06ca4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-91edc36a-6a4c-4b50-bcf8-71e3dd07a212\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91edc36a-6a4c-4b50-bcf8-71e3dd07a212')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-91edc36a-6a4c-4b50-bcf8-71e3dd07a212 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/DATASET2.csv\", names=[\"Text\", \"Labels\"],header=None)\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xre5lFT86FwN",
        "outputId": "bcaa30a1-c7d3-4380-bffa-f6b6ec72d214"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZoHuzresQZc0",
        "outputId": "cd2e4ccc-06c1-4e61-9934-562676f71bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BEFORE THE MOTOR ACCIDENTS CLAIMS TRIBUNAL, KALABURAGI\\nIN THE COURT OF THE   DIST  JUDGE  AT KALABURAGI.\\n\\nM.V.C. No.\\t\\t/ 2023\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def Preprocess(text):\n",
        "    # Remove extra white spaces, tabs, and line breaks\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "b9pYUkB8JdUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"] = df[\"Text\"].map(Preprocess)"
      ],
      "metadata": {
        "id": "IgFQRnqQQ6Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHhwj8uYRonW",
        "outputId": "0f04a5f9-ecee-4351-87ee-97ca8d3835c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    BEFORE THE MOTOR ACCIDENTS CLAIMS TRIBUNAL KAL...\n",
              "1                                         MVC No  2023\n",
              "2    PETITIONERS 1 Ambika wo Late Chandrakant Ratho...\n",
              "3    RESPONDENTS 1 Kapil so Shamu Chavan Age major ...\n",
              "4    Under Sec 166 of the Motor Vehicles Act 1989 t...\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Labels\"] = df[\"Labels\"].map(Preprocess)"
      ],
      "metadata": {
        "id": "gvnCs2i5Rt-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Labels\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p38klrmjR1XH",
        "outputId": "dfb46778-b408-4390-879e-a9dc1b65c12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    CASE INFORMATION\n",
              "1    CASE INFORMATION\n",
              "2    CASE INFORMATION\n",
              "3    CASE INFORMATION\n",
              "4    CASE INFORMATION\n",
              "Name: Labels, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForMaskedLM\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Define your dataset class\n",
        "class LegalDocumentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "# Load and preprocess your dataset\n",
        "texts = df[\"Text\"]  # List of legal document text\n",
        "labels = df[\"Labels\"]  # List of corresponding labels\n",
        "j;\n",
        "# Split your dataset into training and validation sets\n",
        "split_ratio = 0.8  # 80% for training, 20% for validation\n",
        "total_samples = len(texts)\n",
        "train_size = int(split_ratio * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "train_texts = texts[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "val_texts = texts[train_size:]\n",
        "val_labels = labels[train_size:]\n",
        "\n",
        "# Initialize the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Create data loaders for training and validation\n",
        "train_dataset = LegalDocumentDataset(train_texts, train_labels, tokenizer, max_length=512)\n",
        "val_dataset = LegalDocumentDataset(val_texts, val_labels, tokenizer, max_length=512)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Define training parameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_legal_model')\n",
        "\n",
        "# Use the fine-tuned model to generate legal documents\n",
        "model = BertForMaskedLM.from_pretrained('fine_tuned_legal_model')\n",
        "\n",
        "# Generate a legal document\n",
        "input_text = \"In the Court of the DIST JUDGE AT KALABURAGI, M.V.C. No. /2023 ...\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']\n",
        "output = model.generate(input_ids, max_length=512, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48G4joP3R4q6",
        "outputId": "66c9b6ed-24f9-48b9-a886-2a4e14319704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define your dataset class\n",
        "class LegalDocumentDataset(Dataset):\n",
        "    def __init__(self, text_to_generate, tokenizer, max_length):\n",
        "        self.text_to_generate = text_to_generate\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_to_generate)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_to_generate[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# Initialize the GPT-2 tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "# Create data loaders for generating legal documents\n",
        "texts_to_generate = df['Text']\n",
        "\n",
        "generate_dataset = LegalDocumentDataset(texts_to_generate, tokenizer, max_length=512)\n",
        "generate_loader = DataLoader(generate_dataset, batch_size=1)\n",
        "\n",
        "# Generate legal documents\n",
        "generated_documents = []\n",
        "\n",
        "model.eval()\n",
        "for batch in generate_loader:\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    output = model.generate(input_ids, max_length=512, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated_documents.append(generated_text)\n",
        "\n",
        "# Print generated legal documents\n",
        "for i, document in enumerate(generated_documents):\n",
        "    print(f\"Generated Document {i + 1}:\\n\")\n",
        "    print(document)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Save the generated documents to files\n",
        "for i, document in enumerate(generated_documents):\n",
        "    with open(f\"generated_document_{i + 1}.txt\", \"w\") as file:\n",
        "        file.write(document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "ScCJyWQTSFw1",
        "outputId": "2339ec65-0eeb-451e-91a4-e4ebe681c72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6a930ac0cc39>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Create data loaders for generating legal documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtexts_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgenerate_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLegalDocumentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_to_generate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can use a different model based on your requirements\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define the structure and sections of your documents\n",
        "document_structure = {\n",
        "    \"Title/Header Information\": \"[TITLE]\",\n",
        "    \"Petitioner Information\": \"[PETITIONER]\",\n",
        "    \"Respondent Information\": \"[RESPONDENT]\",\n",
        "    \"Motor Accident Claim\": \"[CLAIM]\",\n",
        "    \"Accident Details\": \"[ACCIDENT]\",\n",
        "    \"Injury and Medical Information\": \"[INJURY]\",\n",
        "    \"Loss and Compensation Claim\": \"[CLAIM]\",\n",
        "    \"Jurisdiction and Prayer\": \"[JURISDICTION]\",\n",
        "    \"Declaration\": \"[DECLARATION]\",\n",
        "    \"List of Documents\": \"[DOCUMENTS]\",\n",
        "    \"Application for Permission to Engage Counsel\": \"[PERMISSION]\",\n",
        "    \"Memo\": \"[MEMO]\"\n",
        "}\n",
        "\n",
        "# Read data from a CSV file\n",
        "#df = pd.read_csv(\"/content/DATASET2.csv\")  # Replace with the path to your CSV file\n",
        "\n",
        "# Prepare data for fine-tuning\n",
        "text_data = df[\"Text\"]  # Assuming the text is in a column named \"Text\"\n",
        "# You might need to adapt the code to match your specific CSV format\n",
        "\n",
        "# Tokenize and preprocess the text data\n",
        "input_ids = []\n",
        "for text in text_data:\n",
        "    input_ids.append(tokenizer.encode(text, add_special_tokens=True, max_length=128, truncation=True))  #, padding=True\n",
        "print(\"\\n\",input_ids)\n",
        "\n",
        "\"\"\"\n",
        "new_list = [str(current_integer) for current_integer in input_ids]\n",
        "string_value = \"\".join(new_list)\n",
        "number = int(string_value)\n",
        "print(number)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"input_ids.txt\",\"w\") as f:\n",
        "    for ids in input_ids:\n",
        "        f.write(\" \".join(map(str, ids)) + \"\\n\")\n",
        "\n",
        "# Create a TextDataset\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path= \"input_ids.txt\",  # Pass None as we're using text_data directly\n",
        "    block_size=128,\n",
        "    #document_structure=document_structure\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create a Trainer instance and start fine-tuning\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model()\n",
        "\n",
        "# You can now use the fine-tuned model for generating legal accident documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "t3o0fhXFSGNy",
        "outputId": "5db73751-abe4-4205-80f2-1a3f541f6b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [[12473, 30818, 3336, 42982, 1581, 15859, 2389, 15365, 47666, 3955, 50, 37679, 33, 4944, 1847, 11, 509, 1847, 6242, 45570, 18878, 198, 1268, 3336, 46627, 3963, 3336, 220, 220, 360, 8808, 220, 449, 8322, 8264, 220, 5161, 509, 1847, 6242, 45570, 18878, 13, 198, 198, 44, 13, 53, 13, 34, 13, 1400, 13, 197, 197, 14, 1160, 1954, 198], [47731, 17941, 1137, 14, 50, 25, 198, 198, 16, 13, 197, 35649, 9232, 266, 14, 78, 18319, 46295, 74, 415, 26494, 375, 11, 7129, 546, 1058, 3439, 812, 11, 1609, 25, 37306, 11, 220, 198, 198, 17, 13, 197, 24095, 3972, 360, 14, 78, 46295, 74, 415, 26494, 375, 11, 7129, 546, 1058, 1433, 331, 3808, 1609, 1058, 3710, 198, 198, 18, 13, 197, 6719, 2611, 360, 14, 78, 46295, 74, 415, 26494, 375, 11, 7129, 546, 1058, 1415, 331, 3808, 1609, 1058, 13613, 11, 220, 220, 198, 198, 19, 13, 197, 2025, 73, 7344, 360, 14, 78, 46295, 74, 415, 26494, 375, 11, 7129, 546, 1058, 1485, 331, 3808, 1609, 1058, 3710, 198, 198, 20, 13, 197, 45, 45429, 73, 311, 14, 78, 46295, 74, 415, 26494], [19535, 47, 18672, 3525, 14, 50, 198, 198, 16, 220, 197, 42, 499, 346, 264, 14, 78, 27957, 84, 609, 12421, 11, 7129, 1688, 11, 1609, 25, 4870, 286, 4038, 14121, 1400, 13, 509, 32, 13, 2624, 13, 3838, 13, 4059, 18, 11, 371, 14, 78, 4627, 7745, 282, 536, 5282, 11, 311, 12769, 452, 4448, 6866, 324, 18013, 11, 309, 80, 13, 843, 4307, 13, 12612, 397, 333, 18013, 220, 198, 198, 17, 197, 1925, 349, 321, 7642, 321, 337, 13, 402, 13, 2276, 17541, 1766, 13, 220, 832, 663, 7458, 282, 2607, 11, 9385, 13, 14204, 45, 9256, 11, 9327, 8774, 2975, 11, 12612, 397, 333, 18013, 13, 220], [9203, 220, 1882, 13, 26753, 286, 262, 12533, 31365, 2191, 13, 11104, 262, 39741, 466, 29376, 4174, 329, 262, 7264, 286, 9836, 329, 262, 6821, 14, 22595, 11, 355, 257, 2742, 8852, 286, 3373, 20872, 764, 46295, 74, 415, 264, 14, 78, 4627, 272, 26494, 375, 220, 35656, 3373, 16150, 1783, 12, 220, 3724, 287, 262, 12533, 220, 5778, 13], [2504, 262, 39741, 532, 16, 318, 262, 3656, 286, 262, 20117, 46295, 74, 415, 220, 26494, 375, 290, 262, 8853, 364, 362, 284, 642, 389, 1751, 286, 262, 20117, 13, 383, 8853, 364, 718, 290, 767, 389, 3397, 286, 262, 20117, 13, 383, 8853, 364, 362, 284, 642, 389, 21423, 290, 484, 389, 739, 262, 1337, 290, 10804, 286, 262, 39741, 532, 16, 290, 262, 39741, 532, 16, 852, 3288, 2802, 468, 645, 12681, 1393, 1028, 262, 21423, 13, 198, 220, 197, 2504, 220, 319, 718, 13, 20, 13, 1238, 1954, 11, 262, 20117, 46295, 74, 415, 373, 18788, 319, 12533, 6772, 1400, 13, 25123, 13, 2624, 13, 412, 41, 642, 31020, 422, 465, 7404, 284, 311, 1236, 333, 3272, 13, 679, 373, 10311, 262, 220, 12533, 6772], [2504, 3161, 284, 262, 5778, 262, 20117, 373, 289, 1000, 290, 5448, 290, 373, 220, 4353, 812, 290, 373, 1804, 7573, 286, 4639, 290, 12839, 8263, 9588, 286, 12820, 13, 1679, 11, 830, 16327, 279, 13, 76, 13, 220, 290, 347, 11653, 64, 286, 12820, 13, 5867, 16327, 583, 1110, 13, 679, 373, 262, 691, 13748, 2888, 286, 262, 1641, 290, 8853, 364, 547, 3190, 6906, 319, 262, 3739, 286, 262, 20117, 13, 383, 20117, 373, 14329, 262, 2104, 3739, 329, 262, 9490, 290, 4414, 286, 262, 1641, 1866, 13, 14444, 284, 262, 1418, 524, 306, 1918, 286, 262, 20117, 11, 262, 1204, 286, 262, 8853, 364, 389, 11234, 5676, 290, 484, 389, 1234, 284, 1049, 30699, 290, 2994, 13], [464, 531, 5778, 4073, 2233, 284, 262, 28509, 290, 42837, 5059, 286, 262, 4639, 286, 509, 32, 13, 2624, 13, 3838, 13, 4059, 18, 5633, 10797, 3529, 2298, 510, 5719, 416, 262, 663, 4639, 13, 220, 383, 31282, 1400, 13, 352, 373, 262, 4870, 286, 262, 531, 4038, 220, 290, 262, 531, 4038, 373, 31977, 351, 262, 31282, 532, 17, 17541, 1664, 290, 262, 2450, 373, 287, 2700, 11, 12891, 1111, 262, 14502, 389, 26913, 290, 1750, 453, 17583, 284, 1414, 9836, 284, 262, 39741, 220], [16, 13, 197, 43, 793, 286, 20203, 290, 2003, 12042, 220, 197, 31273, 13, 197, 1120, 11, 405, 11, 830, 16327, 220, 220, 220, 198, 17, 13, 197, 43, 793, 286, 1842, 11, 17696, 1222, 19429, 1056, 220, 197, 31273, 13, 197, 220, 642, 11, 405, 11, 830, 16327, 198, 18, 13, 197, 43, 793, 286, 7964, 220, 197, 197, 197, 197, 197, 31273, 13, 220, 197, 220, 352, 11, 405, 11, 830, 16327, 198, 20, 13, 197, 44, 2470, 6380, 290, 35358, 220, 197, 197, 197, 31273, 13, 197, 220, 642, 11, 405, 11, 830, 16327, 198, 21, 13, 197, 8291, 10189, 220, 290, 14825, 9307, 13, 220, 197, 31273, 13, 197, 220, 352, 11, 405, 11, 830, 16327, 198, 22, 13, 220, 197, 33074, 278, 9307], [8447, 11, 262, 8835, 30, 903, 2184, 743, 26820, 307, 10607, 284, 1249, 262, 8853, 290, 7264, 262, 9836, 286, 12820, 13, 5598, 11, 405, 11, 830, 16327, 284, 262, 39741, 4769, 1111, 220, 262, 14502, 26913, 290, 1750, 453, 17583, 351, 3484, 290, 1393, 379, 262, 2494, 286, 1248, 4064, 279, 13, 64, 13, 422, 262, 3128, 286, 8853, 11, 10597, 23258, 286, 262, 2104, 2033, 220, 220, 287, 262, 1393, 286, 5316, 13, 198], [40, 14, 8845, 220, 4601, 284, 1624, 14, 423, 407, 4752, 597, 9836, 739, 1882, 12713, 878, 597, 584, 4934, 11420, 12457, 9232, 266, 14, 78, 18319, 46295, 74, 415, 26494, 375, 220, 290, 1854, 35656, 26322, 306, 13627, 326, 262, 45596, 1813, 389, 2081, 290, 3376, 284, 262, 1266, 286, 616, 1220, 674, 3725, 290, 326, 645, 1624, 287, 2461, 286, 262, 976, 5778, 468, 587, 5717, 393, 13310, 878, 597, 584, 10041, 13], [56, 11698, 367, 1340, 11698, 11, 628, 197, 818, 262, 2029, 1339, 262, 8853, 364, 9199, 355, 739, 1058, 628, 220, 197, 2504, 11, 262, 8853, 364, 389, 407, 880, 1646, 276, 351, 262, 3173, 290, 9021, 286, 32947, 2191, 11, 12891, 14765, 284, 8209, 1770, 13, 569, 13, 337, 13, 609, 12421, 11, 220, 40209, 11, 319, 465, 8378, 284, 3342, 290, 5120, 351, 262, 1339, 13, 198, 197], [13, 78, 15, 78, 13, 198, 198, 7120, 45129, 11, 628, 198, 220, 197, 818, 262, 2029, 1339, 262, 8853, 364, 25340, 284, 4439, 262, 3331, 1208, 1492, 379, 262, 640, 286, 8296, 286, 262, 2370, 13, 16766, 307, 4367, 290, 925, 355, 636, 286, 1700, 13]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c724ce0d487f>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memo...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"npu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1885\u001b[0m         \"\"\"\n\u001b[1;32m   1886\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.20.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   1788\u001b[0m                     \u001b[0;34m\"Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 )\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "model_name = \"/content/output_1\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/1.json\",\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_2\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "# Save the tokenizer to the output directory\n",
        "tokenizer.save_pretrained('/content/output_1')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "qhdw8Hvctxzn",
        "outputId": "91b6f893-66cd-497f-b4e9-9825fe6c8496"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [82/82 23:07, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/output_1/tokenizer_config.json',\n",
              " '/content/output_1/special_tokens_map.json',\n",
              " '/content/output_1/vocab.json',\n",
              " '/content/output_1/merges.txt',\n",
              " '/content/output_1/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, TFTrainer\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/data_nlp.txt\",\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_1\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.save_model()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "JsXZBQKjxR5S",
        "outputId": "9eb4b32b-9f15-47a5-ba3c-0d9c000f2134"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 30>\u001b[0m:\u001b[94m30\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTFTrainer.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'data_collator'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 30&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TFTrainer.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'data_collator'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall accelerate\n",
        "!pip cache purge\n",
        "!pip install accelerate==0.20.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3tgu8ljyisJ",
        "outputId": "59289bb5-4b74-4a13-904c-583ec08d2e9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 56\n",
            "Collecting accelerate==0.20.3\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.3) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.3) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r trainer.zip /content/output_2"
      ],
      "metadata": {
        "id": "RSlyN3k76WEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2aa8255-fad5-4a7a-b43c-1fb74c1de619"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output_2/ (stored 0%)\n",
            "  adding: content/output_2/training_args.bin (deflated 51%)\n",
            "  adding: content/output_2/vocab.json (deflated 68%)\n",
            "  adding: content/output_2/config.json (deflated 50%)\n",
            "  adding: content/output_2/generation_config.json (deflated 24%)\n",
            "  adding: content/output_2/merges.txt (deflated 53%)\n",
            "  adding: content/output_2/tokenizer_config.json (deflated 54%)\n",
            "  adding: content/output_2/special_tokens_map.json (deflated 74%)\n",
            "  adding: content/output_2/model.safetensors (deflated 7%)\n",
            "  adding: content/output_2/runs/ (stored 0%)\n",
            "  adding: content/output_2/runs/Nov03_22-52-42_dd74f5a69515/ (stored 0%)\n",
            "  adding: content/output_2/runs/Nov03_22-52-42_dd74f5a69515/events.out.tfevents.1699051963.dd74f5a69515.11510.5 (deflated 60%)\n",
            "  adding: content/output_2/runs/Nov03_22-50-54_dd74f5a69515/ (stored 0%)\n",
            "  adding: content/output_2/runs/Nov03_22-50-54_dd74f5a69515/events.out.tfevents.1699051854.dd74f5a69515.11510.4 (deflated 60%)\n",
            "  adding: content/output_2/runs/Nov03_23-02-32_dd74f5a69515/ (stored 0%)\n",
            "  adding: content/output_2/runs/Nov03_23-02-32_dd74f5a69515/events.out.tfevents.1699052553.dd74f5a69515.11510.6 (deflated 59%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "model_name = trainer\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "prompt = \"In the case of an accident, the injured party should\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "output = model.generate(input_ids, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50)\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "z0-oPpoN9vrF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "b2c3cfb1-717b-4b50-b887-ba5c1b89e58b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 5>\u001b[0m:\u001b[94m5\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2600\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2597 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m commit_hash \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2598 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(config, PretrainedConfig):                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2599 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# We make a call to the config file first (which may be absent) to get t\u001b[0m  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2600 \u001b[2m            \u001b[0mresolved_config_file = cached_file(                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2601 \u001b[0m\u001b[2m               \u001b[0mpretrained_model_name_or_path,                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2602 \u001b[0m\u001b[2m               \u001b[0mCONFIG_NAME,                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2603 \u001b[0m\u001b[2m               \u001b[0mcache_dir=cache_dir,                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/utils/\u001b[0m\u001b[1;33mhub.py\u001b[0m:\u001b[94m430\u001b[0m in \u001b[92mcached_file\u001b[0m             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m   \u001b[0muser_agent = http_user_agent(user_agent)                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 428 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 429 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# Load from URL or cache if already cached\u001b[0m                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 430 \u001b[2m      \u001b[0mresolved_file = hf_hub_download(                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 431 \u001b[0m\u001b[2m         \u001b[0mpath_or_repo_id,                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 432 \u001b[0m\u001b[2m         \u001b[0mfilename,                                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 433 \u001b[0m\u001b[2m         \u001b[0msubfolder=\u001b[94mNone\u001b[0m \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(subfolder) == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m subfolder,                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m110\u001b[0m in \u001b[92m_inner_fn\u001b[0m    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m         \u001b[0mkwargs.items(),  \u001b[2m# Kwargs values\u001b[0m                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m      \u001b[0m):                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m arg_name \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mrepo_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mfrom_id\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mto_id\u001b[0m\u001b[33m\"\u001b[0m]:                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m110 \u001b[2m            \u001b[0mvalidate_repo_id(arg_value)                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m         \u001b[0m\u001b[94melif\u001b[0m arg_name == \u001b[33m\"\u001b[0m\u001b[33mtoken\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mand\u001b[0m arg_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m            \u001b[0mhas_token = \u001b[94mTrue\u001b[0m                                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/\u001b[0m\u001b[1;33m_validators.py\u001b[0m:\u001b[94m164\u001b[0m in              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92mvalidate_repo_id\u001b[0m                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m      \u001b[0m)                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m REPO_ID_REGEX.match(repo_id):                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m164 \u001b[2m      \u001b[0m\u001b[94mraise\u001b[0m HFValidationError(                                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mRepo id must use alphanumeric chars or \u001b[0m\u001b[33m'\u001b[0m\u001b[33m-\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'\u001b[0m\u001b[33m_\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'\u001b[0m\u001b[33m.\u001b[0m\u001b[33m'\u001b[0m\u001b[33m, \u001b[0m\u001b[33m'\u001b[0m\u001b[33m--\u001b[0m\u001b[33m'\u001b[0m\u001b[33m and \u001b[0m\u001b[33m'\u001b[0m\u001b[33m..\u001b[0m\u001b[33m'\u001b[0m\u001b[33m are\u001b[0m\u001b[33m\"\u001b[0m      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m forbidden, \u001b[0m\u001b[33m'\u001b[0m\u001b[33m-\u001b[0m\u001b[33m'\u001b[0m\u001b[33m and \u001b[0m\u001b[33m'\u001b[0m\u001b[33m.\u001b[0m\u001b[33m'\u001b[0m\u001b[33m cannot start or end the name, max length is 96:\u001b[0m\u001b[33m\"\u001b[0m      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m         \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mrepo_id\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mHFValidationError: \u001b[0mRepo id must use alphanumeric chars or \u001b[32m'-'\u001b[0m, \u001b[32m'_'\u001b[0m, \u001b[32m'.'\u001b[0m, \u001b[32m'--'\u001b[0m and \u001b[32m'..'\u001b[0m are forbidden, \u001b[32m'-'\u001b[0m and \u001b[32m'.'\u001b[0m \n",
              "cannot start or end the name, max length is \u001b[1;36m96\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mtransformers.trainer.Trainer\u001b[0m\u001b[32m object at 0x7fda578d5e10\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 5&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2600</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2597 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> commit_hash <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2598 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(config, PretrainedConfig):                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2599 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># We make a call to the config file first (which may be absent) to get t</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2600 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>resolved_config_file = cached_file(                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2601 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>pretrained_model_name_or_path,                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2602 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>CONFIG_NAME,                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2603 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>cache_dir=cache_dir,                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hub.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">430</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">cached_file</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 427 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>user_agent = http_user_agent(user_agent)                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 428 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 429 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Load from URL or cache if already cached</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 430 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>resolved_file = hf_hub_download(                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 431 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>path_or_repo_id,                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>filename,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>subfolder=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(subfolder) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> subfolder,                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">110</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_fn</span>    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>kwargs.items(),  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Kwargs values</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>):                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> arg_name <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [<span style=\"color: #808000; text-decoration-color: #808000\">\"repo_id\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"from_id\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"to_id\"</span>]:                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>110 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>validate_repo_id(arg_value)                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> arg_name == <span style=\"color: #808000; text-decoration-color: #808000\">\"token\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> arg_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>has_token = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_validators.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">164</span> in              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">validate_repo_id</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> REPO_ID_REGEX.match(repo_id):                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>164 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> HFValidationError(                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"</span>      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\" forbidden, '-' and '.' cannot start or end the name, max length is 96:\"</span>      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">f\" '{</span>repo_id<span style=\"color: #808000; text-decoration-color: #808000\">}'.\"</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">HFValidationError: </span>Repo id must use alphanumeric chars or <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'_'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'--'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'..'</span> are forbidden, <span style=\"color: #008000; text-decoration-color: #008000\">'-'</span> and <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span> \n",
              "cannot start or end the name, max length is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">96</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;transformers.trainer.Trainer object at 0x7fda578d5e10&gt;'</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"/content/output_1\")  # Replace with your fine-tuned model directory\n",
        "\n",
        "\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "\n",
        "prompt =  \"\"\"\n",
        "* Date of the accident:\n",
        "* Time of the accident:\n",
        "* Location of the accident:\n",
        "* Parties involved in the accident:\n",
        "* Injuries sustained in the accident:\n",
        "* Damages incurred in the accident:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "generated_text = fine_tuned_model.generate(input_ids=tokenizer.encode(prompt, return_tensors='pt'), max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "\n",
        "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMy6nFW7jpHd",
        "outputId": "6fc31ad1-c390-46a0-b65a-009d40167a2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "* Date of the accident:\n",
            "* Time of the accident:\n",
            "* Location of the accident:\n",
            "* Parties involved in the accident:\n",
            "* Injuries sustained in the accident:\n",
            "* Damages incurred in the accident:\n",
            "The accident was reported to the police on the following day: October 1, 2004.\n",
            "On the same day, the driver of a car was killed in a road accident in Kolkata. The driver was a resident of Kalyanagar, Kailash, and was driving a white car. He was travelling at a speed of about 50 km/h. His vehicle was stopped at the intersection of Bhatkal Road and Kalkalagar Road. A police officer was present at his place of employment. On the morning of October 2, a police car stopped the vehicle and the car went into a ditch. It was found that the body of one of its occupants was lying on a pile of debris. There was no body on it. No one was injured. Police had taken the deceased to a hospital. After the autopsy, they found the remains of two of his legs and a large amount of blood on his body. They also found a small amount on one leg. All the injuries were minor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"/content/ambika_dataset.json\",\n",
        "    block_size=128,\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_legal\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.save_model()\n"
      ],
      "metadata": {
        "id": "Vvuo2wahnUde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bard\n",
        "\n",
        "from bardapi import Bard\n",
        "\n",
        "response = bard.generate(prompt + user_response, max_tokens=10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "GVXyE_drkVpv",
        "outputId": "fe5ba267-8a96-4c31-87d6-8aea9752901b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-db0f92684ae8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbardapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bard'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bardapi\n",
        "from bardapi import Bard\n",
        "\n",
        "# Set the Bard API key\n",
        "api_key = \"cgjVQwGYLkc60XgsiGIBGwOMadvlFnyIEh18GlgrDZb7junAWVxuOe0muyHYnyI5M9-HhA.\"\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "Please provide the following information about the motor vehicle accident:\n",
        "\n",
        "* Date of the accident:\n",
        "* Time of the accident:\n",
        "* Location of the accident:\n",
        "* Parties involved in the accident:\n",
        "* Injuries sustained in the accident:\n",
        "* Damages incurred in the accident:\n",
        "\n",
        "Once you have provided this information, I will generate a legal accident document on your behalf.\n",
        "\"\"\"\n",
        "\n",
        "# Get the user's response to the prompt\n",
        "user_response = input(prompt)\n",
        "\n",
        "# Create a Bard instance with your API key\n",
        "bard = Bard(token='cgjVQwGYLkc60XgsiGIBGwOMadvlFnyIEh18GlgrDZb7junAWVxuOe0muyHYnyI5M9-HhA.')\n",
        "\n",
        "\n",
        "# Generate the legal document using Bard\n",
        "response = bard.generate(prompt + user_response, max_tokens=10000)\n",
        "# Get the generated document\n",
        "generated_document = response[\"generated_text\"]\n",
        "\n",
        "# Print or save the generated document\n",
        "print(generated_document)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "LbdMRH3ZkZZs",
        "outputId": "e2712935-42c5-4521-ed82-1aea0a2ea129"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Please provide the following information about the motor vehicle accident:\n",
            "\n",
            "* Date of the accident:\n",
            "* Time of the accident:\n",
            "* Location of the accident:\n",
            "* Parties involved in the accident:\n",
            "* Injuries sustained in the accident:\n",
            "* Damages incurred in the accident:\n",
            "\n",
            "Once you have provided this information, I will generate a legal accident document on your behalf.\n",
            "dfghjkl\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6df840e32f64>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create a Bard instance with your API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mbard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cgjVQwGYLkc60XgsiGIBGwOMadvlFnyIEh18GlgrDZb7junAWVxuOe0muyHYnyI5M9-HhA.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bardapi/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, token, timeout, proxies, session, conversation_id, google_translator_api_key, language, run_code, token_from_browser)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSNlM0e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_snim0e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_BARD_API_LANG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bardapi/core.py\u001b[0m in \u001b[0;36m_get_snim0e\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0msnim0e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"SNlM0e\\\":\\\"(.*?)\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msnim0e\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0;34m\"SNlM0e value not found. Double-check __Secure-1PSID value or pass it as token='xxxxx'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n",
            "\u001b[0;31mException\u001b[0m: SNlM0e value not found. Double-check __Secure-1PSID value or pass it as token='xxxxx'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etX1VSFzSE45",
        "outputId": "7eb096c3-fbce-4e84-c7a2-cdd8d174099a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"tiiuae/falcon-7b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example text generation pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "text = text_generator(\"Generate text based on this prompt: \", max_length=5, num_return_sequences=1)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "cIoIcRIwSjJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Gradio to wrap a text to text interface around GPT-J-6B"
      ],
      "metadata": {
        "id": "oV3g1G64Jcfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGK9GG9uUYus",
        "outputId": "1f7061c1-6c7f-4a9f-e83b-5bae034827f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "Vo1RbtNBJRtH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GusXDAAgJvXp",
        "outputId": "6580a14e-b8dd-4d0d-d6a4-256fc40ce457"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(inp):\n",
        "  input_ids = tokenizer.encode(inp, return_tensors='tf')\n",
        "  beam_output = model.generate(input_ids,max_length=100,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n",
        "  output = tokenizer.decode(beam_output[0], skip_special_token=True,clean_up_tokenization_spaces=True)\n",
        "  return \".\".join(output.split(\".\")[:-1]) + \".\""
      ],
      "metadata": {
        "id": "2C8wmDkTLD9Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = gr.outputs.Textbox()\n",
        "gr.Interface(generate_text,\"text_box\",output_text,title=\"GPT-2\",\n",
        "             description=\"Hello GPT2.\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "y_SECA0MMtM8",
        "outputId": "6932f2ef-c1b9-4d89-d3ae-2bbb93fca1af"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b97cd01a4373>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m gr.Interface(generate_text,\"text_box\",output_text,title=\"GPT-2\",\n\u001b[1;32m      3\u001b[0m              description=\"Hello GPT2.\").launch()\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'outputs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_text(inp):\n",
        "  input_ids = tokenizer.encode(inp, return_tensors='tf')\n",
        "  beam_output = model.generate(input_ids,max_length=100,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n",
        "  output = tokenizer.decode(beam_output[0], skip_special_token=True,clean_up_tokenization_spaces=True)\n",
        "  return \".\".join(output.split(\".\")[:-1]) + \".\"\n",
        "\n",
        "# Create a Gradio interface with text input and text output\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"GPT-2\",\n",
        "    description=\"Hello GPT2.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "MSxfIvz_NUnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_text(inp):\n",
        "  input_ids = tokenizer.encode(inp, return_tensors='tf')\n",
        "  beam_output = model.generate(input_ids,max_length=100,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n",
        "  output = tokenizer.decode(beam_output[0], skip_special_token=True,clean_up_tokenization_spaces=True)\n",
        "  return \".\".join(output.split(\".\")[:-1]) + \".\"\n",
        "\n",
        "# Create a Gradio interface with text input and text output\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"GPT-2\",\n",
        "    description=\"Hello GPT2.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "e_7V0fmGQFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a list of prompts\n",
        "prompts = [\n",
        "\"Please provide the date of the accident:\",\n",
        "\"Please provide the time of the accident:\",\n",
        "\"Please provide the location of the accident:\",\n",
        "\"Please provide the parties involved in the accident:\",\n",
        "\"Please provide information about injuries sustained in the accident:\",\n",
        "\"Please provide accidental details:\"\n",
        "]\n",
        "\n",
        "# Create input components for each prompt\n",
        "input_components = [gr.Textbox(label=\"Date of Accident\"),\n",
        "                    gr.Textbox(label=\"Time of Accident\"),\n",
        "                    gr.Textbox(label=\"Location of Accident\"),\n",
        "                    gr.Textbox(label=\"Parties involved in the Accident\"),\n",
        "                    gr.Textbox(label=\"Injuries sustained in the Accident\"),\n",
        "                    gr.Textbox(label=\"Damages incurred in the Accident\")]\n",
        "\n",
        "def generate_text(*inputs):\n",
        "  # Combine user input with the predefined prompts\n",
        "  full_prompt = \"\\n\".join([f\"{prompt} {user_input}\" for prompt, user_input in zip(prompts, inputs)])\n",
        "  input_ids = tokenizer.encode(full_prompt, return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "  fn=generate_text,\n",
        "  inputs=input_components,\n",
        "  outputs=\"text\",\n",
        "  title=\"Legal Document Generator\",\n",
        "  description=\"Generate a legal accident document based on user input.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "lGCk9phEYHoF",
        "outputId": "80b70dee-104d-47f1-ebfb-39b2e30bc1ad"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2ef10db7e0ed260239.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2ef10db7e0ed260239.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "Please provide the following information about the motor vehicle accident:\n",
        "\n",
        "* Date of the accident:\n",
        "* Time of the accident:\n",
        "* Location of the accident:\n",
        "* Parties involved in the accident:\n",
        "* Injuries sustained in the accident:\n",
        "* Damages incurred in the accident:\n",
        "\"\"\"\n",
        "\n",
        "# Create input components for each prompt\n",
        "input_components = [gr.Textbox() for prompt in prompts]\n",
        "\n",
        "# Define a function to generate the legal document\n",
        "def generate_legal_document(full_prompt):\n",
        "  \"\"\"Generates a legal accident document.\n",
        "\n",
        "  Args:\n",
        "    full_prompt: The full prompt, including the predefined prompt and the user's response.\n",
        "\n",
        "  Returns:\n",
        "    The generated legal accident document.\n",
        "  \"\"\"\n",
        "\n",
        "  # Generate the legal document using the GPT-2 model.\n",
        "  input_ids = tokenizer.encode(full_prompt, return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "  generated_document = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  return generated_document\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "  fn=generate_legal_document,\n",
        "  inputs=input_components,\n",
        "  outputs=\"text\",\n",
        "  title=\"Legal Document Generator\",\n",
        "  description=\"Generate a legal accident document based on user input.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "Gy7a-APwaVfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.client import Client\n",
        "# Initialize the LangChain client\n",
        "client = langchain.Client()\n",
        "\n",
        "# Load the GPT-2 model\n",
        "model = langchain.Model(\"gpt2\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "Please provide the following information about the motor vehicle accident:\n",
        "\n",
        "* Date of the accident:\n",
        "* Time of the accident:\n",
        "* Location of the accident:\n",
        "* Parties involved in the accident:\n",
        "* Injuries sustained in the accident:\n",
        "* Damages incurred in the accident:\n",
        "\"\"\"\n",
        "\n",
        "# Get the user's response to the prompt\n",
        "user_response = input(prompt)\n",
        "\n",
        "# Generate the legal document\n",
        "generated_document = client.generate(model, prompt + user_response, max_length=200)\n",
        "\n",
        "# Print the generated document\n",
        "print(generated_document)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "sJjcaR-JmrGz",
        "outputId": "3d94b442-0a7f-4732-d212-f4b3aa93dc49"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m3553\u001b[0m in \u001b[92mrun_code\u001b[0m        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3550 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melif\u001b[0m async_ :                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3551 \u001b[0m\u001b[2m               \u001b[0m\u001b[94mawait\u001b[0m \u001b[96meval\u001b[0m(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3552 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m3553 \u001b[2m               \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3554 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3555 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3556 \u001b[0m\u001b[2m            \u001b[0msys.excepthook = old_excepthook                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'langchain.client'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3553</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3550 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> async_ :                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3551 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">await</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">eval</span>(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3552 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>3553 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3554 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3555 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Reset our crash handler in place</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3556 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>sys.excepthook = old_excepthook                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'langchain.client'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"/content/output_1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a list of prompts\n",
        "prompts = [\n",
        "\"Please provide the date of the accident:\",\n",
        "\"Please provide the time of the accident:\",\n",
        "\"Please provide the location of the accident:\",\n",
        "\"Please provide the parties involved in the accident:\",\n",
        "\"Please provide information about injuries sustained in the accident:\",\n",
        "\"Please provide accidental details:\"\n",
        "]\n",
        "\n",
        "# Create input components for each prompt\n",
        "input_components = [gr.Textbox(label=\"Date of Accident\"),\n",
        "                    gr.Textbox(label=\"Time of Accident\"),\n",
        "                    gr.Textbox(label=\"Location of Accident\"),\n",
        "                    gr.Textbox(label=\"Parties involved in the Accident\"),\n",
        "                    gr.Textbox(label=\"Injuries sustained in the Accident\"),\n",
        "                    gr.Textbox(label=\"Damages incurred in the Accident\")]\n",
        "\n",
        "def generate_text(*inputs):\n",
        "  # Combine user input with the predefined prompts\n",
        "  full_prompt = \"\\n\".join([f\"{prompt} {user_input}\" for prompt, user_input in zip(prompts, inputs)])\n",
        "  input_ids = tokenizer.encode(full_prompt, return_tensors='pt')\n",
        "  output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k68taUPnf0A",
        "outputId": "e4ed0804-b459-468e-cd83-4e018e3af865"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.329)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.56)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.4.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('./output_1')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('./output_1')\n",
        "\n",
        "# Create a state variable to store the previous data\n",
        "previous_data = []\n",
        "\n",
        "def chatbot(input_text):\n",
        "  \"\"\"Generates a response to the given input text.\"\"\"\n",
        "\n",
        "  # Update the state variable with the new input\n",
        "  previous_data.append(input_text)\n",
        "\n",
        "  # Generate the next response based on the previous data\n",
        "  input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "  response_ids = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "  response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "  return response_text\n",
        "\n",
        "# Create a Gradio interface for the chatbot\n",
        "iface = gr.Interface(\n",
        "  fn=chatbot,\n",
        "  inputs=[gr.Textbox()],\n",
        "  outputs=\"text\",\n",
        "  title=\"Law \",\n",
        "  description=\"Conversion.\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "EMuwIqcLntwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the fine-tuned GPT-2 language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Define the list of questions to ask the user\n",
        "questions = [\"What is the name of the victim?\", \"What is the vehicle number?\"]\n",
        "\n",
        "# Define the function to ask the user a question\n",
        "def ask_question(question):\n",
        "  print(question)\n",
        "  response = input()\n",
        "  return response\n",
        "\n",
        "# Define the function to generate text using the GPT-2 language model\n",
        "def generate_text(prompt):\n",
        "  generated_text = model.generate(input_ids=tokenizer.encode(prompt+Data_set, return_tensors='pt'), max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "  generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n",
        "# Initialize the conversation state\n",
        "conversation_state = {}\n",
        "\n",
        "# Iterate over the list of questions and ask the user each question\n",
        "for question in questions:\n",
        "  response = ask_question(question)\n",
        "  conversation_state[question] = response\n",
        "\n",
        "Data_set = \" \".join(conversation_state)\n",
        "\n",
        "# Generate text using the GPT-2 language model, based on the conversation state\n",
        "generated_text = generate_text(f\"Based on the information you have provided, the following is an enhanced version of the case details:\")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjCd7nKuuaLK",
        "outputId": "797a0095-90ad-4d6d-db8d-09147a9c9593"
      },
      "execution_count": 60,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the name of the victim?\n",
            "Sadanand\n",
            "What is the vehicle number?\n",
            "KA32 Q 657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information you have provided, the following is an enhanced version of the case details:What is the name of the victim? What is the vehicle number?What are the names of all the witnesses?Who is responsible for the crime? Who is in charge of investigating the incident?Where is this information?How did the police arrive at the scene?Why did they not arrive?When did police come to the spot?Was there any physical contact between the two men?Did they have any contact with the victims?Were there no witnesses to this incident or any other incident that occurred in the vicinity of this place?If so, what was the reason for this?Is there a witness to any of these incidents?Are there witnesses who have been present at this spot for some time?Do you know any witnesses that have come forward to tell the truth about the matter? If so what are they?\n",
            "\n",
            "The police have asked the public to come out and come and see the accused.\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the fine-tuned GPT-2 language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Define the list of questions to ask the user\n",
        "questions = [\"What is the petitioner's name?\", \"What is the respondent's name?\", \"What motor vehicle act was violated?\", \"Can you provide more details about the accident?\"]\n",
        "\n",
        "# Define the function to ask the user a question\n",
        "def ask_question(question):\n",
        "  print(question)\n",
        "  response = input()\n",
        "  return response\n",
        "\n",
        "# Define the function to generate text using the GPT-2 language model\n",
        "def generate_text(prompt):\n",
        "  generated_text = model.generate(input_ids=tokenizer.encode(prompt, return_tensors='pt'), max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "  generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "  return generated_text\n",
        "\n",
        "\n",
        "# Initialize the conversation state\n",
        "conversation_state = {}\n",
        "\n",
        "# Iterate over the list of questions and ask the user each question\n",
        "for question in questions:\n",
        "  response = ask_question(question)\n",
        "  conversation_state[question] = response\n",
        "\n",
        "Data_set = \" \".join(conversation_state)\n",
        "\n",
        "generated_text = generate_text(f\"\"\"*Case Details*\n",
        "*Petitioner Name:* {conversation_state['petitioner name']}\n",
        "*Respondent Name:* {conversation_state['respondent name']}\n",
        "\n",
        "*Motor Vehicle Act Violated:* {conversation_state['motor vehicle act']}\n",
        "\n",
        "*Accident Details:* {conversation_state['accident details']}\"\"\")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "H_d609sVy_aj",
        "outputId": "e111316b-fd81-4980-9228-fa1559f2c276"
      },
      "execution_count": 64,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the petitioner's name?\n",
            "Sadanand\n",
            "What is the respondent's name?\n",
            "Vaibhav\n",
            "What motor vehicle act was violated?\n",
            "Under  Sec. 166 of the Motor Vehicles Act. 1989\n",
            "Can you provide more details about the accident?\n",
            "That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 35>\u001b[0m:\u001b[94m36\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'petitioner name'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 35&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'petitioner name'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the fine-tuned GPT-2 language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Define the list of questions to ask the user\n",
        "questions = [\n",
        "    \"What is the petitioner's name?\",\n",
        "    \"What is the respondent's name?\",\n",
        "    \"What motor vehicle act was violated?\",\n",
        "    \"Can you provide more details about the accident?\"\n",
        "]\n",
        "\n",
        "# Define the function to ask the user a question\n",
        "def ask_question(question):\n",
        "    print(question)\n",
        "    response = input()\n",
        "    return response\n",
        "\n",
        "# Define the function to generate text using the GPT-2 language model\n",
        "def generate_text(prompt):\n",
        "    generated_text = model.generate(input_ids=tokenizer.encode(prompt, return_tensors='pt'), max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return generated_text\n",
        "\n",
        "# Initialize the conversation state\n",
        "conversation_state = {}\n",
        "\n",
        "# Iterate over the list of questions and ask the user each question\n",
        "for question in questions:\n",
        "    response = ask_question(question)\n",
        "    conversation_state[question] = response\n",
        "\n",
        "# Use the correct keys to access responses in the conversation_state\n",
        "generated_text = generate_text(f\"\"\"*Case Details*\n",
        "\n",
        "*Petitioner Name:* {conversation_state[questions[0]]}\n",
        "*Respondent Name:* {conversation_state[questions[1]]}\n",
        "*Motor Vehicle Act Violated:* {conversation_state[questions[2]]}\n",
        "*Accident Details:* {conversation_state[questions[3]]}\"\"\")\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C03hnBhv-V-D",
        "outputId": "101e22ed-6c9c-48e8-cfaa-fd2aadda5a7e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the petitioner's name?\n",
            "Sadanand\n",
            "What is the respondent's name?\n",
            "Vaibhav\n",
            "What motor vehicle act was violated?\n",
            "under the act 166 of motor vehicle \n",
            "Can you provide more details about the accident?\n",
            "That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*Case Details*\n",
            "\n",
            "*Petitioner Name:* Sadanand\n",
            "*Respondent Name:* Vaibhav\n",
            "*Motor Vehicle Act Violated:* under the act 166 of motor vehicle \n",
            "*Accident Details:* That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n",
            "\"The petitioner was taken to hospital where he succumbed and the cause of death was confirmed by the medical examiner. On the basis of his condition, he is now in the custody of Gimsa Hospital. His family has been informed and will be informed of any further developments. We are awaiting the results of investigation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Define the list of questions to ask the user\n",
        "questions = [\"What is the petitioner's name?\", \"What is the respondent's name?\", \"What motor vehicle act was violated?\", \"Can you provide more details about the accident?\"]\n",
        "\n",
        "# Define the function to ask the user a question\n",
        "def ask_question(question):\n",
        "  print(question)\n",
        "  response = input()\n",
        "  return response\n",
        "\n",
        "# Define the function to generate text using the GPT-2 language model\n",
        "def generate_text(prompt, conversation_state):\n",
        "  \"\"\"Generates text using the GPT-2 language model, based on the given prompt and conversation state.\n",
        "\n",
        "  Args:\n",
        "    prompt: The prompt to use for generating the text.\n",
        "    conversation_state: A dictionary containing the conversation state.\n",
        "\n",
        "  Returns:\n",
        "    A string containing the generated text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Encode the prompt and conversation state using the tokenizer.\n",
        "  encoded_prompt = tokenizer.encode(prompt, return_tensors='pt')\n",
        "  encoded_conversation_state = tokenizer.encode(conversation_state, return_tensors='pt')\n",
        "\n",
        "  # Concatenate the encoded prompt and conversation state.\n",
        "  encoded_input = torch.cat([encoded_prompt, encoded_conversation_state], dim=1)\n",
        "\n",
        "  # Generate text using the GPT-2 language model.\n",
        "  generated_text = model.generate(input_ids=encoded_input, max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "  # Decode the generated text.\n",
        "  decoded_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "\n",
        "  return decoded_text\n",
        "\n",
        "# Start the conversation\n",
        "conversation_state = {}\n",
        "\n",
        "for question in questions:\n",
        "  response = ask_question(question)\n",
        "  conversation_state[question] = response\n",
        "\n",
        "# Generate text using the GPT-2 language model, based on the conversation state\n",
        "generated_text = generate_text(prompt=\"*Case Details*\", conversation_state=conversation_state)\n",
        "\n",
        "# Enhance the generated text\n",
        "enhanced_text = \"\"\"Based on the information you have provided, the following is an enhanced version of the case details:\n",
        "\n",
        "*Motor Vehicle Act Violated:* Section {conversation_state['motor vehicle act']} of the Motor Vehicles Act, 1988\n",
        "\n",
        "*Accident Details:* On {date of accident}, at around {time of accident}, the petitioner, {conversation_state['petitioner name']}, was driving a {vehicle type} bearing registration number {vehicle number} on {road name} in {city name}. The respondent, {conversation_state['respondent name']}, was driving a {vehicle type} bearing registration number {respondent vehicle number} in the opposite direction. At the intersection of {road name} and {cross street name}, the respondent's vehicle collided with the petitioner's vehicle. As a result of the collision, the petitioner sustained serious injuries and was taken to the hospital for treatment.\n",
        "\n",
        "The petitioner has filed a petition under Section {section number} of the Motor Vehicles Act, 1988, seeking compensation for the injuries sustained in the accident. The petitioner has also sought punitive damages from the respondent for reckless driving.\n",
        "\n",
        "The case is currently pending before the {court name}.\"\"\"\n",
        "\n",
        "# Print the generated text to the user\n",
        "print(enhanced_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-6xhnurCD-Kg",
        "outputId": "29f436a3-e05a-4b13-e89d-44fe3c40023b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the petitioner's name?\n",
            "Sadanand\n",
            "What is the respondent's name?\n",
            "Vaibhav\n",
            "What motor vehicle act was violated?\n",
            "section 166\n",
            "Can you provide more details about the accident?\n",
            "That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 52>\u001b[0m:\u001b[94m52\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92mgenerate_text\u001b[0m:\u001b[94m31\u001b[0m                                                                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base.py\u001b[0m:\u001b[94m2569\u001b[0m in \u001b[92mencode\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2566 \u001b[0m\u001b[2;33m            \u001b[0m\u001b[33mthe `tokenize` method) or a list of integers (tokenized string ids using\u001b[0m  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2567 \u001b[0m\u001b[2;33m            \u001b[0m\u001b[33mmethod).\u001b[0m                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2568 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2569 \u001b[2m      \u001b[0mencoded_inputs = \u001b[96mself\u001b[0m.encode_plus(                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2570 \u001b[0m\u001b[2m         \u001b[0mtext,                                                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2571 \u001b[0m\u001b[2m         \u001b[0mtext_pair=text_pair,                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2572 \u001b[0m\u001b[2m         \u001b[0madd_special_tokens=add_special_tokens,                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_base.py\u001b[0m:\u001b[94m2977\u001b[0m in          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92mencode_plus\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2974 \u001b[0m\u001b[2m         \u001b[0m**kwargs,                                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2975 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2976 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m2977 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._encode_plus(                                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2978 \u001b[0m\u001b[2m         \u001b[0mtext=text,                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2979 \u001b[0m\u001b[2m         \u001b[0mtext_pair=text_pair,                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m2980 \u001b[0m\u001b[2m         \u001b[0madd_special_tokens=add_special_tokens,                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mtokenization_gpt2_fast.py\u001b[0m:\u001b[94m172\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m_encode_plus\u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mto use it with pretokenized inputs.\u001b[0m\u001b[33m\"\u001b[0m                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m      \u001b[0m)                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m172 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m()._encode_plus(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92msave_vocabulary\u001b[0m(\u001b[96mself\u001b[0m, save_directory: \u001b[96mstr\u001b[0m, filename_prefix: Optional[\u001b[96mstr\u001b[0m] = \u001b[94mNone\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m      \u001b[0mfiles = \u001b[96mself\u001b[0m._tokenizer.model.save(save_directory, name=filename_prefix)           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_fast.py\u001b[0m:\u001b[94m576\u001b[0m in           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92m_encode_plus\u001b[0m                                                                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m573 \u001b[0m\u001b[2m      \u001b[0m**kwargs,                                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m574 \u001b[0m\u001b[2m   \u001b[0m) -> BatchEncoding:                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m575 \u001b[0m\u001b[2m      \u001b[0mbatched_input = [(text, text_pair)] \u001b[94mif\u001b[0m text_pair \u001b[94melse\u001b[0m [text]                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m576 \u001b[2m      \u001b[0mbatched_output = \u001b[96mself\u001b[0m._batch_encode_plus(                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m577 \u001b[0m\u001b[2m         \u001b[0mbatched_input,                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m         \u001b[0mis_split_into_words=is_split_into_words,                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m579 \u001b[0m\u001b[2m         \u001b[0madd_special_tokens=add_special_tokens,                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mtokenization_gpt2_fast.py\u001b[0m:\u001b[94m162\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m_batch_encode_plus\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m         \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mto use it with pretokenized inputs.\u001b[0m\u001b[33m\"\u001b[0m                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m      \u001b[0m)                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m161 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m162 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m()._batch_encode_plus(*args, **kwargs)                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_encode_plus\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs) -> BatchEncoding:                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m165 \u001b[0m\u001b[2m      \u001b[0mis_split_into_words = kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mis_split_into_words\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m)                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtokenization_utils_fast.py\u001b[0m:\u001b[94m504\u001b[0m in           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92m_batch_encode_plus\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m501 \u001b[0m\u001b[2m         \u001b[0mpad_to_multiple_of=pad_to_multiple_of,                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m502 \u001b[0m\u001b[2m      \u001b[0m)                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m503 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m504 \u001b[2m      \u001b[0mencodings = \u001b[96mself\u001b[0m._tokenizer.encode_batch(                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m505 \u001b[0m\u001b[2m         \u001b[0mbatch_text_or_text_pairs,                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m506 \u001b[0m\u001b[2m         \u001b[0madd_special_tokens=add_special_tokens,                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m507 \u001b[0m\u001b[2m         \u001b[0mis_pretokenized=is_split_into_words,                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0mTextEncodeInput must be Union\u001b[1m[\u001b[0mTextInputSequence, Tuple\u001b[1m[\u001b[0mInputSequence, InputSequence\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 52&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">52</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_text</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">31</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2569</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">encode</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2566 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">the `tokenize` method) or a list of integers (tokenized string ids using</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2567 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">method).</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2568 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>encoded_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encode_plus(                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>text,                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>text_pair=text_pair,                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2572 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>add_special_tokens=add_special_tokens,                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2977</span> in          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">encode_plus</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2974 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>**kwargs,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2975 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2976 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>2977 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._encode_plus(                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2978 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>text=text,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2979 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>text_pair=text_pair,                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2980 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>add_special_tokens=add_special_tokens,                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_gpt2_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">172</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\"to use it with pretokenized inputs.\"</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>172 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>()._encode_plus(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save_vocabulary</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, save_directory: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, filename_prefix: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>files = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenizer.model.save(save_directory, name=filename_prefix)           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">576</span> in           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">573 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>**kwargs,                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">574 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>) -&gt; BatchEncoding:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>batched_input = [(text, text_pair)] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_pair <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> [text]                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>576 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>batched_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._batch_encode_plus(                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">577 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>batched_input,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>is_split_into_words=is_split_into_words,                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">579 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>add_special_tokens=add_special_tokens,                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_gpt2_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">162</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batch_encode_plus</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">159 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #808000; text-decoration-color: #808000\">\"to use it with pretokenized inputs.\"</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>162 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>()._batch_encode_plus(*args, **kwargs)                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_encode_plus</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs) -&gt; BatchEncoding:                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">165 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>is_split_into_words = kwargs.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"is_split_into_words\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">tokenization_utils_fast.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">504</span> in           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_batch_encode_plus</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">501 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>pad_to_multiple_of=pad_to_multiple_of,                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>504 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._tokenizer.encode_batch(                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">505 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>batch_text_or_text_pairs,                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">506 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>add_special_tokens=add_special_tokens,                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">507 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>is_pretokenized=is_split_into_words,                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>TextEncodeInput must be Union<span style=\"font-weight: bold\">[</span>TextInputSequence, Tuple<span style=\"font-weight: bold\">[</span>InputSequence, InputSequence<span style=\"font-weight: bold\">]]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the fine-tuned GPT-2 language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output_1\")\n",
        "\n",
        "# Define the list of questions to ask the user\n",
        "questions = [\n",
        "    \"What is the petitioner's name?\",\n",
        "    \"What is the respondent's name?\",\n",
        "    \"What motor vehicle act was violated?\",\n",
        "    \"Can you provide more details about the accident?\"\n",
        "]\n",
        "\n",
        "# Define the function to ask the user a question\n",
        "def ask_question(question):\n",
        "    print(question)\n",
        "    response = input()\n",
        "    return response\n",
        "\n",
        "# Define the function to generate text using the GPT-2 language model\n",
        "def generate_text(prompt):\n",
        "    generated_text = model.generate(input_ids=tokenizer.encode(prompt, return_tensors='pt'), max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return generated_text\n",
        "\n",
        "# Initialize the conversation state\n",
        "conversation_state = {}\n",
        "\n",
        "# Iterate over the list of questions and ask the user each question\n",
        "for question in questions:\n",
        "    response = ask_question(question)\n",
        "    conversation_state[question] = response\n",
        "\n",
        "# Concatenate user responses into a single string\n",
        "user_responses = \"\\n\".join(f\"{question} {response}\" for question, response in conversation_state.items())\n",
        "\n",
        "# Generate the legal document based on user responses\n",
        "generated_text = generate_text(user_responses)\n",
        "\n",
        "# Print the generated legal document\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om3Fyf6KFjpQ",
        "outputId": "d20dd533-5cb5-4ebe-f7c7-d7eb55a8e875"
      },
      "execution_count": 69,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the petitioner's name?\n",
            "Sadanand\n",
            "What is the respondent's name?\n",
            "abc\n",
            "What motor vehicle act was violated?\n",
            "section 166\n",
            "Can you provide more details about the accident?\n",
            "That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the petitioner's name? Sadanand\n",
            "What is the respondent's name? abc\n",
            "What motor vehicle act was violated? section 166\n",
            "Can you provide more details about the accident? That  on 6.5.2023, the deceased Sadanand was proceeding on Motor cycle No.KA.32. EJ 5374 from his village to Sannur cross. He was riding the  Motor cycle slowly with due care and precautions and at about 9.30 Pm, when he was near  Mugal Nagaon cross Sannur Shahabad road, Tq. Shahabad, Dist. Kalaburagi  i.e. 1 Km from Peth Siroor at that time the driver of the Mahindra Pick up van bearing No. KA.32. AA 5003  came  driving his vehicle from opposite side in high speed and in rash and negligent manner, endangering human lives and dashed to the Motor cycle  of the petitioner due to which the deceased Chandrakant fell down and sustained grievous injuries over ie.. grievous fracture over both legs, knee,  abdominal head,  back  and also grievous injuries over other parts of the body.  Immediately the deceased was brought to Subedar Hospital in ambulance and he was treated for the said injuries till 9.5.2023 and on 9.5.2023 the deceased succumbed to the said accidental injuries at 12.30 Pm. The body of the deceased was brought to GIMS Hospital, Kalaburagi  where the PM was conducted and after conducting the PM the body was handed over to the petitioners.\n",
            "The petitioner was informed that the death of his son was due due in part to his negligence and that he had been driving the vehicle with the intention of killing his father. On the basis of this, he filed a petition for a writ of habeas corpus in the Supreme Court and the apex court granted the writ on the ground that his death was not due because of negligence. In the case of death due by negligence, it is not necessary to prove that there was any negligence in his conduct. However, in view of that, a case like this is a matter of law and it should be decided by the court.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the trained GPT-2 model and tokenizer\n",
        "model_name = \"./output_1\"  # Path to the directory where your trained model is saved\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_legal_document(title_information, petitioner_information, respondent_information,\n",
        "                            motor_accident_claim, victim_information, employment_information,\n",
        "                            income_details, place_date_and_time_of_accident, travel_details,\n",
        "                            medical_information, vehical_information, applicants_information,\n",
        "                            relation_information, property_information, accident_detail,\n",
        "                            description_of_deceased_and_accident_impact, description_of_accident_and_liability,\n",
        "                            loss_and_compensation_claim, prayer, declaration, application_for_permission_to_engage_counsel, memo):\n",
        "    document = {\n",
        "        \"Title_Information\": title_information,\n",
        "        \"Petitioner_Information\": petitioner_information,\n",
        "        \"Respondent_Information\": respondent_information,\n",
        "        \"Motor_accident_claim\": motor_accident_claim,\n",
        "        \"Victim_information\": victim_information,\n",
        "        \"Employment_information\": employment_information,\n",
        "        \"Income_details\": income_details,\n",
        "        \"Place_date_and_time_of_accident\": place_date_and_time_of_accident,\n",
        "        \"Travel_details\": travel_details,\n",
        "        \"Medical_information\": medical_information,\n",
        "        \"Vehical_information\": vehical_information,\n",
        "        \"Applicants_information\": applicants_information,\n",
        "        \"Relation_information\": relation_information,\n",
        "        \"Property_information\": property_information,\n",
        "        \"Accident_detail\": accident_detail,\n",
        "        \"Description_of_deceased_and_accident_impact\": description_of_deceased_and_accident_impact,\n",
        "        \"Description_of_accident_and_liability\": description_of_accident_and_liability,\n",
        "        \"Loss_and_compensation_claim\": loss_and_compensation_claim,\n",
        "        \"Prayer\": prayer,\n",
        "        \"Declaration\": declaration,\n",
        "        \"Application_for_permission_to_engage_counsel\": application_for_permission_to_engage_counsel,\n",
        "        \"Memo\": memo\n",
        "    }\n",
        "\n",
        "    # Generate content for each section using the model\n",
        "    for section, prompt in document.items():\n",
        "        if prompt:\n",
        "            generated_text = model.generate(\n",
        "                input_ids=tokenizer.encode(prompt, return_tensors='pt'),\n",
        "                max_length=500,  # Adjust the length as needed\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "            )\n",
        "            generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            document[section] = generated_text\n",
        "\n",
        "    # Combine sections into the final document\n",
        "    final_document = \"\\n\\n\".join([f\"{section}:\\n{content}\" for section, content in document.items() if content])\n",
        "    return final_document\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_legal_document,\n",
        "    inputs=[\n",
        "        gr.inputs.Textbox(label=\"Title/Information\"),\n",
        "        gr.inputs.Textbox(label=\"Petitioner Information\"),\n",
        "        gr.inputs.Textbox(label=\"Respondent Information\"),\n",
        "        gr.inputs.Textbox(label=\"Motor Accident Claim\"),\n",
        "        gr.inputs.Textbox(label=\"Victim Information\"),\n",
        "        gr.inputs.Textbox(label=\"Employment Information\"),\n",
        "        gr.inputs.Textbox(label=\"Income Details\"),\n",
        "        gr.inputs.Textbox(label=\"Place, Date, and Time of Accident\"),\n",
        "        gr.inputs.Textbox(label=\"Travel Details\"),\n",
        "        gr.inputs.Textbox(label=\"Medical Information\"),\n",
        "        gr.inputs.Textbox(label=\"Vehicle Information\"),\n",
        "        gr.inputs.Textbox(label=\"Applicants Information\"),\n",
        "        gr.inputs.Textbox(label=\"Relation Information\"),\n",
        "        gr.inputs.Textbox(label=\"Property Information\"),\n",
        "        gr.inputs.Textbox(label=\"Accident Detail\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Deceased and Accident Impact\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Accident and Liability\"),\n",
        "        gr.inputs.Textbox(label=\"Loss and Compensation Claim\"),\n",
        "        gr.inputs.Textbox(label=\"Prayer\"),\n",
        "        gr.inputs.Textbox(label=\"Declaration\"),\n",
        "        gr.inputs.Textbox(label=\"Application for Permission to Engage Counsel\"),\n",
        "        gr.inputs.Textbox(label=\"Memo\"),\n",
        "    ],\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Legal Document\"),\n",
        "    title=\"Legal Document Generator\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "wucbCDcXGbz5",
        "outputId": "70a140cf-855b-4419-ffb2-e9f886e024d7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 61>\u001b[0m:\u001b[94m64\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mAttributeError: \u001b[0mmodule \u001b[32m'gradio'\u001b[0m has no attribute \u001b[32m'inputs'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 61&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">64</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span>module <span style=\"color: #008000; text-decoration-color: #008000\">'gradio'</span> has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the trained GPT-2 model and tokenizer\n",
        "model_name = \"./output_1\"  # Path to the directory where your trained model is saved\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_legal_document(title_information, petitioner_information, respondent_information,\n",
        "                             motor_accident_claim, victim_information, employment_information,\n",
        "                             income_details, place_date_and_time_of_accident, travel_details,\n",
        "                             medical_information, vehical_information, applicants_information,\n",
        "                             relation_information, property_information, accident_detail,\n",
        "                             description_of_deceased_and_accident_impact, description_of_accident_and_liability,\n",
        "                             loss_and_compensation_claim, prayer, declaration, application_for_permission_to_engage_counsel, memo):\n",
        "    document = {\n",
        "        \"Title_Information\": title_information,\n",
        "        \"Petitioner_Information\": petitioner_information,\n",
        "        \"Respondent_Information\": respondent_information,\n",
        "        \"Motor_accident_claim\": motor_accident_claim,\n",
        "        \"Victim_information\": victim_information,\n",
        "        \"Employment_information\": employment_information,\n",
        "        \"Income_details\": income_details,\n",
        "        \"Place_date_and_time_of_accident\": place_date_and_time_of_accident,\n",
        "        \"Travel_details\": travel_details,\n",
        "        \"Medical_information\": medical_information,\n",
        "        \"Vehical_information\": vehical_information,\n",
        "        \"Applicants_information\": applicants_information,\n",
        "        \"Relation_information\": relation_information,\n",
        "        \"Property_information\": property_information,\n",
        "        \"Accident_detail\": accident_detail,\n",
        "        \"Description_of_deceased_and_accident_impact\": description_of_deceased_and_accident_impact,\n",
        "        \"Description_of_accident_and_liability\": description_of_accident_and_liability,\n",
        "        \"Loss_and_compensation_claim\": loss_and_compensation_claim,\n",
        "        \"Prayer\": prayer,\n",
        "        \"Declaration\": declaration,\n",
        "        \"Application_for_permission_to_engage_counsel\": application_for_permission_to_engage_counsel,\n",
        "        \"Memo\": memo\n",
        "    }\n",
        "\n",
        "    # Generate content for each section using the model\n",
        "    for section, prompt in document.items():\n",
        "        if prompt:\n",
        "            generated_text = model.generate(\n",
        "                input_ids=tokenizer.encode(prompt, return_tensors='pt'),\n",
        "                max_length=500,  # Adjust the length as needed\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "            )\n",
        "            generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            document[section] = generated_text\n",
        "\n",
        "    # Combine sections into the final document\n",
        "    final_document = \"\\n\\n\".join([f\"{section}:\\n{content}\" for section, content in document.items() if content])\n",
        "    return final_document\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_legal_document,\n",
        "    inputs=[\n",
        "        gr.inputs.Dropdown(label=\"Title/Information\", choices=[\"Mr.\", \"Ms.\", \"Dr.\", \"Prof.\"]),\n",
        "        gr.inputs.Textbox(label=\"Petitioner Information\"),\n",
        "        gr.inputs.Textbox(label=\"Respondent Information\"),\n",
        "        gr.inputs.Textbox(label=\"Motor Accident Claim\"),\n",
        "        gr.inputs.Checkbox(label=\"Victim Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Employment Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Income Details\"),\n",
        "        gr.inputs.Checkbox(label=\"Place, Date, and Time of Accident\"),\n",
        "        gr.inputs.Checkbox(label=\"Travel Details\"),\n",
        "        gr.inputs.Checkbox(label=\"Medical Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Vehicle Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Applicants Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Relation Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Property Information\"),\n",
        "        gr.inputs.Checkbox(label=\"Accident Detail\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Deceased and Accident Impact\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Accident and Liability\"),\n",
        "        gr.inputs.Textbox(label=\"Loss and Compensation Claim\"),\n",
        "        gr.inputs.Checkbox(label=\"Prayer\"),\n",
        "        gr.inputs.Checkbox(label=\"Declaration\"),\n",
        "        gr.inputs.Checkbox(label=\"Application for Permission to Engage Counsel\"),\n",
        "        gr.inputs.Checkbox(label=\"Memo\"),\n",
        "    ],\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Legal Document\"),\n",
        "    title=\"Legal Document Generator\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "g7Qq1a5chp9f",
        "outputId": "d2ea00f0-92ea-4c0b-d87e-503e882004b4"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<cell line: 61>\u001b[0m:\u001b[94m64\u001b[0m                                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mAttributeError: \u001b[0mmodule \u001b[32m'gradio'\u001b[0m has no attribute \u001b[32m'inputs'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 61&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">64</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span>module <span style=\"color: #008000; text-decoration-color: #008000\">'gradio'</span> has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'inputs'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the trained GPT-2 model and tokenizer\n",
        "model_name = \"./output_1\"  # Path to the directory where your trained model is saved\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_legal_document(\n",
        "    title_information, petitioner_information, respondent_information,\n",
        "    motor_accident_claim, victim_information, employment_information,\n",
        "    income_details, place_date_and_time_of_accident, travel_details,\n",
        "    medical_information, vehical_information, applicants_information,\n",
        "    relation_information, property_information, accident_detail,\n",
        "    description_of_deceased_and_accident_impact, description_of_accident_and_liability,\n",
        "    loss_and_compensation_claim, prayer, declaration, application_for_permission_to_engage_counsel, memo\n",
        ")\n",
        "    # Your generation code remains the same\n",
        "\n",
        "# Define the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_legal_document,\n",
        "    inputs=[\n",
        "        gr.inputs.Textbox(label=\"Title/Information\"),\n",
        "        gr.inputs.Textbox(label=\"Petitioner Information\"),\n",
        "        gr.inputs.Textbox(label=\"Respondent Information\"),\n",
        "        gr.inputs.Textbox(label=\"Motor Accident Claim\"),\n",
        "        gr.inputs.Textbox(label=\"Victim Information\"),\n",
        "        gr.inputs.Textbox(label=\"Employment Information\"),\n",
        "        gr.inputs.Textbox(label=\"Income Details\"),\n",
        "        gr.inputs.Textbox(label=\"Place, Date, and Time of Accident\"),\n",
        "        gr.inputs.Textbox(label=\"Travel Details\"),\n",
        "        gr.inputs.Textbox(label=\"Medical Information\"),\n",
        "        gr.inputs.Textbox(label=\"Vehicle Information\"),\n",
        "        gr.inputs.Textbox(label=\"Applicants Information\"),\n",
        "        gr.inputs.Textbox(label=\"Relation Information\"),\n",
        "        gr.inputs.Textbox(label=\"Property Information\"),\n",
        "        gr.inputs.Textbox(label=\"Accident Detail\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Deceased and Accident Impact\"),\n",
        "        gr.inputs.Textbox(label=\"Description of Accident and Liability\"),\n",
        "        gr.inputs.Textbox(label=\"Loss and Compensation Claim\"),\n",
        "        gr.inputs.Textbox(label=\"Prayer\"),\n",
        "        gr.inputs.Textbox(label=\"Declaration\"),\n",
        "        gr.inputs.Textbox(label=\"Application for Permission to Engage Counsel\"),\n",
        "        gr.inputs.Textbox(label=\"Memo\"),\n",
        "    ],\n",
        "    outputs=gr.outputs.Textbox(label=\"Generated Legal Document\"),\n",
        "    title=\"Legal Document Generator\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "DdL0xNpCigYo",
        "outputId": "f0065e7d-85fb-4518-c180-02bc81885aa5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[91m\u001b[0m\n",
              "\u001b[91m\u001b[0m \u001b[1m)\u001b[0m                                                                                                \u001b[91m\u001b[0m\n",
              "\u001b[91m\u001b[0m  \u001b[1;91m\u001b[0m                                                                                               \u001b[91m\u001b[0m\n",
              "\u001b[91m\u001b[0m\n",
              "\u001b[1;91mSyntaxError: \u001b[0mexpected \u001b[32m':'\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span> <span style=\"font-weight: bold\">)</span>                                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\"></span>                                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span>expected <span style=\"color: #008000; text-decoration-color: #008000\">':'</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"/content/output_2\"  # Path to the directory where your trained model is saved\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_legal_document():\n",
        "    document = {}  # Initialize an empty dictionary to store document sections\n",
        "\n",
        "    # Collect information from the user for different sections\n",
        "    document['Title_Information'] = input(\"Enter title_information: \")\n",
        "    document['Petitioner_Information'] = input(\"Enter petitioner_information: \")\n",
        "    document['Respondent_Information'] = input(\"respondent_information: \")\n",
        "    document['Accident_detail'] = input(\"Enter accident_detai: \")\n",
        "    document['Description_of_deceased_and_accident_impact'] = input(\"Enter Description_of_deceased_and_accident_impact: \")\n",
        "    document['Loss_and_compensation_claim'] = input(\"Enter Loss_and_compensation_claim: \")\n",
        "\n",
        "\n",
        "    # Repeat the above line for other sections (Petitioner_Information, Respondent_Information, etc.)\n",
        "\n",
        "    # Generate content for each section using the model\n",
        "    for section, prompt in document.items():\n",
        "        if prompt:\n",
        "            generated_text = model.generate(\n",
        "                input_ids=tokenizer.encode(prompt, return_tensors='pt'),\n",
        "                max_length=500,  # Adjust the length as needed\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=2,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "            )\n",
        "            generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            document[section] = generated_text\n",
        "\n",
        "    # Combine sections into the final document\n",
        "    final_document = \"\\n\\n\".join([f\"{section}:\\n{content}\" for section, content in document.items() if content])\n",
        "    return final_document\n",
        "\n",
        "# Generate a legal document based on user input\n",
        "generated_document = generate_legal_document()\n",
        "print(generated_document)\n"
      ],
      "metadata": {
        "id": "tY2U3CPmkJ0O",
        "outputId": "77d8882c-6241-47af-fb83-c49b1d60f33b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter title_information: 2\n",
            "Enter petitioner_information: w\n",
            "respondent_information: w\n",
            "Enter accident_detai: wd\n",
            "Enter Description_of_deceased_and_accident_impact: \n",
            "Enter Loss_and_compensation_claim: dd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s\n",
        "\n",
        "wd\n",
        "ds"
      ],
      "metadata": {
        "id": "tWScYPeKkzvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}