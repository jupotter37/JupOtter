{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef147f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\loren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1528e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "step: 0, loss: 0.5797744393348694\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-04 16:03:24.878948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\apex\\amp\\_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
      "  warnings.warn(\"An input tensor was not cuda.\")\n",
      "D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Traceback (most recent call last):\n",
      "  File \"train_ditto.py\", line 89, in <module>\n",
      "    train(train_dataset,\n",
      "  File \"C:\\Users\\loren\\OneDrive\\Documenti\\agiwproject4\\ditto\\ditto_light\\ditto.py\", line 201, in train\n",
      "    train_step(train_iter, model, optimizer, scheduler, hp)\n",
      "  File \"C:\\Users\\loren\\OneDrive\\Documenti\\agiwproject4\\ditto\\ditto_light\\ditto.py\", line 131, in train_step\n",
      "    prediction = model(x1, x2)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\apex\\amp\\_initialize.py\", line 196, in new_fwd\n",
      "    output = old_fwd(*applier(args, input_caster),\n",
      "  File \"C:\\Users\\loren\\OneDrive\\Documenti\\agiwproject4\\ditto\\ditto_light\\ditto.py\", line 53, in forward\n",
      "    enc = self.bert(torch.cat((x1, x2)))[0][:, 0, :]\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 844, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 522, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 409, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 337, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"D:\\Programmi\\Python\\Python3.8.0-64\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 239, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.71 GiB already allocated; 0 bytes free; 2.78 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "!python train_ditto.py \\\n",
    "  --task Textual/Airbnb \\\n",
    "  --batch_size 32 \\\n",
    "  --max_len 128 \\\n",
    "  --lr 3e-5 \\\n",
    "  --n_epochs 20 \\\n",
    "  --finetuning \\\n",
    "  --lm roberta \\\n",
    "  --fp16 \\\n",
    "  --da drop_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985c7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit24f4d80062944293adfe957dd87b758f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
