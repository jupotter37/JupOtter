{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy to preprocess text into lemmatised tokens\n",
    "### Sklearn pipeline models:\n",
    "    1) Countvectoriser\n",
    "    2) Tfidf Vectoriser\n",
    "    3) Random Forest\n",
    "    4) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_csv('train.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "\n",
    "boot = False # resample the data to 10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the words into the same columns\n",
    "# not used here\n",
    "def concat_words(data):\n",
    "    keyword_filled = data.keyword.fillna('')\n",
    "    location_filled = data.location.fillna('')\n",
    "    data['all_words'] = data.text + ' ' + keyword_filled + ' ' + location_filled\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy\n",
    "df_train = df1\n",
    "df_test = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ablaze accident aftershock airplane accident ambulance annihilated annihilation apocalypse armageddon army arson arsonist attack attacked avalanche battle bioterror bioterrorism blaze blazing bleeding blew up blight blizzard blood bloody blown up body bag body bagging body bags bomb bombed bombing bridge collapse buildings burning buildings on fire burned burning burning buildings bush fires casualties casualty catastrophe catastrophic chemical emergency cliff fall collapse collapsed collide collided collision crash crashed crush crushed curfew cyclone damage danger dead death deaths debris deluge deluged demolish demolished demolition derail derailed derailment desolate desolation destroy destroyed destruction detonate detonation devastated devastation disaster displaced drought drown drowned drowning dust storm earthquake electrocute electrocuted emergency emergency plan emergency services engulfed epicentre evacuate evacuated evacuation explode exploded explosion eyewitness famine fatal fatalities fatality fear fire fire truck first responders flames flattened flood flooding floods forest fire forest fires hail hailstorm harm hazard hazardous heat wave hellfire hijack hijacker hijacking hostage hostages hurricane injured injuries injury inundated inundation landslide lava lightning loud bang mass murder mass murderer massacre mayhem meltdown military mudslide natural disaster nuclear disaster nuclear reactor obliterate obliterated obliteration oil spill outbreak pandemonium panic panicking police quarantine quarantined radiation emergency rainstorm razed refugees rescue rescued rescuers riot rioting rubble ruin sandstorm screamed screaming screams seismic sinkhole sinking siren sirens smoke snowstorm storm stretcher structural failure suicide bomb suicide bomber suicide bombing sunk survive survived survivors terrorism terrorist threat thunder thunderstorm tornado tragedy trapped trauma traumatised trouble tsunami twister typhoon upheaval violent storm volcano war zone weapon weapons whirlwind wild fires wildfire windstorm wounded wounds wreck wreckage wrecked '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique keywords\n",
    "keywords = df_train.keyword.fillna('')\n",
    "keywords = keywords.unique().tolist()\n",
    "result =''\n",
    "for ele in keywords:\n",
    "    result += ele\n",
    "    result += ' '\n",
    "result = result.replace('%20', ' ')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemma_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-98ac95f3b94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemma_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "doc = lemma_tokenizer(result)\n",
    "tokens = [cleanup(word) for word in doc if isNoise(word)==False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train, test_size=0.1, random_state=42)\n",
    "if boot == True: train = train.sample(n=10000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613 6851 762\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "[E048] Can't import language en_core_web_sm from spacy.lang: No module named 'spacy.lang.en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".lang.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy.lang.en_core_web_sm'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5b48423baf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"tagger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 1. Get Language instance, e.g. English()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# 2. Initialize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".lang.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spacy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE048\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mset_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: [E048] Can't import language en_core_web_sm from spacy.lang: No module named 'spacy.lang.en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "# creating NLP pipeline\n",
    "# working progress\n",
    "lang = \"en_core_web_sm\"\n",
    "pipeline = [\"tagger\"]\n",
    "cls = spacy.util.get_lang_class(lang)   # 1. Get Language instance, e.g. English()\n",
    "nlp = cls()                             # 2. Initialize it\n",
    "for name in pipeline:\n",
    "    component = nlp.create_pipe(tokenizer)   # 3. Create the pipeline components\n",
    "    nlp.add_pipe(component)             # 4. Add the component to the pipeline\n",
    "nlp.from_disk(model_data_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to clean words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# define which pos to filter out\n",
    "noisy_pos_tags = ['PROP', 'NUM', 'SYM']\n",
    "pattern1 = [{\"LEMMA\": {\"IN\": keywords}},\n",
    "            {\"POS\": \"NOUN\"}]\n",
    "disable_list = [\"ner\", \"parser\"]\n",
    "# functions to filter out noises and clean words\n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags:\n",
    "        is_noise = True \n",
    "        #print('pos')\n",
    "    elif token.is_stop == True:\n",
    "        is_noise = True\n",
    "        #print('stop')\n",
    "    elif token.is_punct == True:\n",
    "        is_noise = True\n",
    "        #print('punct')\n",
    "    elif token.lemma_ == '...':\n",
    "        is_noise = True\n",
    "        #print('...')\n",
    "    elif token.like_url == True:\n",
    "        is_noise = True\n",
    "        #print('url')\n",
    "    else:\n",
    "        is_noise = False\n",
    "    return is_noise \n",
    "    \n",
    "\n",
    "def cleanup(token, lower = True):\n",
    "    if lower:\n",
    "        token = token.lemma_.lower()\n",
    "    return token.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer with lemma words (and lowercase, rm stopword, symbols, number)\n",
    "def lemma_tokenizer(sentence):\n",
    "    mytokens = nlp(sentence, disable_list)\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    tokens = [cleanup(word) for word in mytokens if isNoise(word)==False ]\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complicated tokenizer with tagging:  POS + Entity + Special case\n",
    "\n",
    "def pos_tokenizer (sentence):\n",
    "    mytokens = nlp(sentence, disable_list)\n",
    "    lookups = Lookups()\n",
    "    lemmatizer = Lemmatizer(lookups)\n",
    "    word =[]\n",
    "    pos=[]\n",
    "    for token in sent:\n",
    "        word.append(token.text)\n",
    "        pos.append(token.pos_)\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in SYMBOLS ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These functions are for the pipeline for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dense transformer\n",
    "class ToDenseTransformer(BaseEstimator,TransformerMixin):\n",
    "    # define the transform operation\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    # no paramter to learn this case\n",
    "    # fit just returns an unchanged object\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Not Disaster Best Words: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Disaster Best words: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training and Verification Data\n",
    "X_train = train['all_words'].tolist()\n",
    "Y_train = train['target'].tolist()\n",
    "X_test = test['all_words'].tolist()\n",
    "Y_test = test['target'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=lemma_tokenizer)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "pipe = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('normal', ToDenseTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "pipe.fit(X_train, Y_train)\n",
    "# test\n",
    "preds = pipe.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for countvectorizer model is:\", accu.mean())\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf vectorizer\n",
    "pipe_Tfidf = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    (\"clf\", clf)\n",
    "    ])\n",
    "pipe_Tfidf.fit(X_train, Y_train)\n",
    "preds = pipe_Tfidf.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Tfidf model is:\", accu.mean())\n",
    "# print most informative words with highest coeff for Not Diaster and Diaster\n",
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other non-vectorised Classifiers\n",
    "1) Random Forest\n",
    "\n",
    "2) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "RF_clf = RandomForestClassifier(n_estimators=10)\n",
    "pipe_RF = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    (\"clf\", RF_clf)\n",
    "    ])\n",
    "pipe_RF.fit(X_train, Y_train)\n",
    "preds = pipe_RF.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Random Forest model is:\", accu.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "pipe_bayes = Pipeline(\n",
    "    [(\"vect\", vectorizer),\n",
    "    ('bayes', MultinomialNB())\n",
    "    ])\n",
    "pipe_bayes.fit(X_train, Y_train)\n",
    "preds = pipe_bayes.predict(X_test)\n",
    "accu = preds == Y_test\n",
    "print(\"accuracy: for Bayes model is:\", accu.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import random\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(5):\n",
    "    random.shuffle(X_train)\n",
    "    for raw_text, entity_offsets in X_train:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        gold = GoldParse(doc, entities=entity_offsets)\n",
    "        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)\n",
    "nlp.to_disk(\"/model\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## This is a good schematic pipeline which hopefully I will do another version to get more weighted features\n",
    "\n",
    "```python\n",
    "model = Pipeline([\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('entity_feature', Pipeline([\n",
    "                ('entity_extractor', EntityExtractor()),\n",
    "                ('entity_vect', CountVectorizer()),\n",
    "            ])),\n",
    "            ('keyphrase_feature', Pipeline([\n",
    "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
    "                ('keyphrase_vect', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights= {\n",
    "            'entity_feature': 0.6,\n",
    "            'keyphrase_feature': 0.2,\n",
    "        }\n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(0,100,2))\n",
    "print(type(*l) for l in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
