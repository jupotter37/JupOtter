{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# code based off of \n",
    "# https://github.com/mandubian/pytorch_math_dataset and\n",
    "# https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "from apex import amp\n",
    "import pickle\n",
    "\n",
    "\n",
    "import mandubian.math_dataset\n",
    "from mandubian.math_dataset import MathDatasetManager\n",
    "from mandubian.transformer import Constants\n",
    "\n",
    "# from transformer.Models import Transformer\n",
    "from mandubian.math_dataset import (\n",
    "    random_split_dataset,\n",
    "    question_answer_to_mask_batch_collate_fn\n",
    ")\n",
    "\n",
    "from mandubian.checkpoints import rotating_save_checkpoint, build_checkpoint\n",
    "from mandubian.math_dataset import np_encode_string, np_decode_string\n",
    "import mandubian.model_process\n",
    "import mandubian.utils\n",
    "from mandubian.tensorboard_utils import Tensorboard\n",
    "from mandubian.tensorboard_utils import tensorboard_event_accumulator\n",
    "\n",
    "import mandubian.checkpoints\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 detected CUDA devices\n",
      "Using CUDA device:  0\n",
      "GeForce RTX 2080\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "print(torch.cuda.device_count(), \"detected CUDA devices\")\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print(\"Using CUDA device: \", cuda_device)\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidrains_reformer.reformer_pytorch import ReformerLM, Autopadder, Recorder\n",
    "from lucidrains_reformer.reformer_pytorch import ReformerEncDec\n",
    "from lucidrains_reformer.reformer_pytorch.generative_tools import TrainingWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "mdsmgr structure ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_datasets_from_category', 'build_dataset_from_categories', 'build_dataset_from_category', 'build_dataset_from_module', 'build_dataset_from_modules', 'dfs', 'dirs', 'get_categories', 'get_modules_for_category', 'get_types', 'root_dir']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"/home/jonathan/Repos/final_year_at_ic/awesome_project/mathematics_dataset-v1.0/\"\n",
    ")\n",
    "# Examine dataset structure\n",
    "print(\"mdsmgr structure\", dir(mdsmgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MathDatasetManager._build_datasets_from_category of <mandubian.math_dataset.MathDatasetManager object at 0x7efb71341d30>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(MathDatasetManager.__dir__\n",
    "mdsmgr._build_datasets_from_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types, problem categories and problem subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus']\n",
      "modules of arithmetic dict_keys(['div', 'nearest_integer_root', 'mul_div_multiple', 'mul', 'add_or_sub', 'add_sub_multiple', 'mixed', 'add_or_sub_in_base', 'simplify_surd', 'add_or_sub_big', 'add_sub_multiple_longer', 'mixed_longer', 'div_big', 'mul_div_multiple_longer', 'mul_big'])\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))\n",
    "print(\"categories\", list(mdsmgr.get_categories()))\n",
    "print(\"modules of arithmetic\", mdsmgr.get_modules_for_category('arithmetic'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to manipulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# # Build Dataset from a single module in a category\n",
    "ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy')\n",
    "print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from a single module in a category with limited number of elements\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy', max_elements=1000)\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from several modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'train-easy')\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'interpolate')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in several categories\n",
    "# ds = mdsmgr.build_dataset_from_categories(['arithmetic', 'polynomials'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"add_sub_multiple_135\"\n",
    "now = datetime.now()\n",
    "unique_id = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "base_dir = \"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandubian.math_dataset import (\n",
    "    VOCAB_SZ, MAX_QUESTION_SZ, MAX_ANSWER_SZ\n",
    ")\n",
    "\n",
    "NUM_CPU_THREADS = 6\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 45\n",
    "GRADIENT_ACCUMULATE_EVERY = 3\n",
    "LEARNING_RATE = 6e-4\n",
    "VALIDATE_EVERY  = 20\n",
    "GENERATE_EVERY  = 60\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# hyperparameters need updates\n",
    "\n",
    "Q_SEQ_LEN = 256\n",
    "A_SEQ_LEN = 30 # unused due to requirements of axial_positon_shape\n",
    "NUM_TOKENS = VOCAB_SZ + 1\n",
    "D_MODEL = 512\n",
    "EMB_DIM = D_MODEL\n",
    "NUM_HEADS = 8\n",
    "QKV_DIM = D_MODEL / NUM_HEADS\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# training_data = mdsmgr.build_dataset_from_category('arithmetic','train-easy') # for now\n",
    "training_data = mdsmgr.build_dataset_from_module('arithmetic', 'add_sub_multiple', 'train-easy')\n",
    "\n",
    "# testing data\n",
    "# testing_data_interpolate = mdsmgr.build_dataset_from_category('arithmetic','interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_category('arithmetic','extrapolate')\n",
    "\n",
    "testing_data_interpolate = mdsmgr.build_dataset_from_module('arithmetic', 'add_sub_multiple', 'interpolate', max_elements = 1024)\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lucidrains_reformer.examples.enwik8_simple.train\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    # returns true when token is not PAD and false otherwise\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# get data splits\n",
    "train_ds, val_ds = mandubian.math_dataset.random_split_dataset(training_data,split_rate=0.9)\n",
    "\n",
    "# get pytorch dataloaders\n",
    "# Questions are padded in question_answer_to_position_batch_collate_fn\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "train_loader = cycle(train_loader)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "val_loader = cycle(val_loader)\n",
    "\n",
    "# for viewing output sequences\n",
    "gen_loader = data.DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "gen_loader = cycle(gen_loader)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    testing_data_interpolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "interpolate_loader = cycle(interpolate_loader)\n",
    "\n",
    "# extrapolate_loader = data.DataLoader(\n",
    "#     testing_data_extrapolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "#     collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "# extrapolate_loader = cycle(extrapolate_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncDec(\n",
       "  (enc): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x4x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x64x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x2x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): ReZero(\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): ReZero(\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): ReZero(\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Linear(in_features=512, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = D_MODEL,\n",
    "    enc_num_tokens = NUM_TOKENS,\n",
    "    enc_depth = NUM_LAYERS,\n",
    "    enc_max_seq_len = Q_SEQ_LEN,\n",
    "    dec_num_tokens = NUM_TOKENS,\n",
    "    dec_depth = NUM_LAYERS,\n",
    "    dec_max_seq_len = Q_SEQ_LEN,\n",
    "    # heads = 8 by default\n",
    "    axial_position_shape = (16, 16),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n",
    "    axial_position_dims = (256,256),   # the dims must sum up to the model dimensions (512 + 512 = 1024)\n",
    "    pad_value = Constants.PAD,\n",
    "    ignore_index = Constants.PAD, # see if this works. pad_value and ignore_index are probably different\n",
    "    enc_use_rezero = True,\n",
    "    dec_use_rezero = True\n",
    ")\n",
    "\n",
    "# enc_dec = Recorder(enc_dec)\n",
    "enc_dec.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer learning rate scheduler, mixed precision setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.995), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=30, verbose=True, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "# mixed precision\n",
    "enc_dec, optimizer = amp.initialize(enc_dec, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "i = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "best_val_loss = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  508 \t training loss: 1.1416015625 \t 23:17:06.358871\n",
      "Step  509 \t training loss: 1.203125 \t 23:17:18.038390\n",
      "Step  510 \t training loss: 1.1510416666666667 \t 23:17:29.784357\n",
      "Step  511 \t training loss: 1.2392578125 \t 23:17:41.462313\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 3007, 3008, 3009, 3010, 3011, 3012) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ec49997c0c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrain_loss_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRADIENT_ACCUMULATE_EVERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mbatch_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_qs_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_qs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_qs_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d6cdac4c7991>\u001b[0m in \u001b[0;36mcycle\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 3007, 3008, 3009, 3010, 3011, 3012) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    # exclude the 0th element as it is BOS\n",
    "    \n",
    "    if (i % GENERATE_EVERY) - 1 == 0:\n",
    "        enc_dec.eval()\n",
    "        gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "        prime = np_decode_string(gen_qs.numpy())\n",
    "        print('*' * 100, \"\\nQuestion: \", prime)\n",
    "        print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "        gen_qs = gen_qs.to(device, non_blocking=True)\n",
    "        gen_as = gen_as.to(device, non_blocking=True)\n",
    "        gen_qs_mask = gen_qs_mask.to(device, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            sample = enc_dec.generate(gen_qs, gen_as[:,0:1], GENERATE_LENGTH, enc_input_mask = gen_qs_mask, dec_eos_token=Constants.EOS)\n",
    "        sample = sample.cpu().numpy()\n",
    "        output_str = np_decode_string(sample)\n",
    "        print(\"Decoded Prediction: \", output_str)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", train_loss_list, fmt=\"%f\")\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", val_loss_list, fmt=\"%f\")\n",
    "            \n",
    "\n",
    "    enc_dec.train()\n",
    "    train_loss_record = 0\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        batch_qs, batch_qs_mask, batch_as, batch_as_mask = map(lambda x: x.to(device, non_blocking=True), next(train_loader))\n",
    "        train_loss = enc_dec(batch_qs, batch_as, return_loss = True, enc_input_mask = batch_qs_mask)\n",
    "        del batch_qs, batch_qs_mask, batch_as, batch_as_mask\n",
    "        with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        train_loss_record += float(train_loss)\n",
    "        del train_loss\n",
    "\n",
    "#     if i % GRADIENT_ACCUMULATE_EVERY == 0:\n",
    "    train_loss_record /= GRADIENT_ACCUMULATE_EVERY\n",
    "    print(\"Step \", i, \"\\t\", f'training loss: {train_loss_record}', \"\\t\", datetime.now().time() )\n",
    "    train_loss_list.append((i, train_loss_record))\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step(train_loss_record)\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        enc_dec.eval()\n",
    "        val_batch_qs, val_batch_qs_mask, val_batch_as, val_batch_as_mask = map(lambda x: x.to(device, non_blocking=True), next(val_loader))\n",
    "        with torch.no_grad():\n",
    "            val_loss = enc_dec(val_batch_qs, val_batch_as, return_loss = True, enc_input_mask = val_batch_qs_mask)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_loss_list.append((i, val_loss.item()))\n",
    "            \n",
    "            # if we have a good model, save it \n",
    "            if val_loss.item() < best_val_loss:\n",
    "                print(\"Checkpointing Validation Model...\")\n",
    "                best_val_loss = val_loss.item()\n",
    "                state = build_checkpoint(exp_name, unique_id, \"val\", enc_dec, optimizer, None, best_val_loss, i)\n",
    "                rotating_save_checkpoint(state, prefix=f\"{exp_name}_{unique_id}_val\", path=\"./checkpoints\", nb=5)    \n",
    "\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot([train_loss_list[0]])\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec.eval()\n",
    "while True:\n",
    "    count += 1\n",
    "\n",
    "    gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "#     print(gen_qs.size())\n",
    "#     prime = np_decode_string(gen_qs.numpy())\n",
    "#     print('*' * 100, \"\\nQuestion: \", prime)\n",
    "#     print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "#     gen_qs = gen_qs.to(device, non_blocking=True)\n",
    "#     gen_as = gen_as.to(device, non_blocking=True)\n",
    "#     gen_qs_mask = gen_qs_mask.to(device, non_blocking=True)\n",
    "#     with torch.no_grad():\n",
    "#         sample = enc_dec.generate(gen_qs, gen_as[:,0:1], GENERATE_LENGTH, enc_input_mask = gen_qs_mask, dec_eos_token=Constants.EOS)\n",
    "#     sample = sample.cpu().numpy()\n",
    "#     output_str = np_decode_string(sample)\n",
    "#     print(\"Decoded Prediction: \", output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "print(isinstance(gen_loader, Iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
