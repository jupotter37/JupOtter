{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indra622/tutorials/blob/master/Nemo_finetuning_English_to_ZerothKorean_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisities"
      ],
      "metadata": {
        "id": "sdlHR1ZgE2ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo_toolkit['all']"
      ],
      "metadata": {
        "id": "MoAqrJD83GuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cjMaek4rY8-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import tarfile\n",
        "import wget\n",
        "import copy\n",
        "from omegaconf import OmegaConf, open_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSTb6b5DriWG"
      },
      "outputs": [],
      "source": [
        "import nemo\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.collections.asr.metrics.wer import word_error_rate\n",
        "from nemo.utils import logging, exp_manager\n",
        "from collections import abc as container_abcs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eF6k4ipA5TE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained model"
      ],
      "metadata": {
        "id": "j5PqxLNgE6py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_quartznet15x5\", map_location='cpu')"
      ],
      "metadata": {
        "id": "6-tMBpTM4nZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"kresnik/librispeech_asr_test\", \"clean\")"
      ],
      "metadata": {
        "id": "5yqwUdbz5XbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = ds['test']\n",
        "sample = test_ds[0]\n",
        "sample"
      ],
      "metadata": {
        "id": "5EicjtZG6RwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "IPython.display.Audio(sample['file'])"
      ],
      "metadata": {
        "id": "8cf1lH4i6YKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = char_model.transcribe([sample['file']])\n",
        "results = char_model.transcribe(test_ds['file'][:10])"
      ],
      "metadata": {
        "id": "0-zHqU_L6hOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypothesis: \"+ result[0])\n",
        "print(\"Reference:  \" +sample['text'].lower())"
      ],
      "metadata": {
        "id": "NGfrtxtN7Pxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "eodDSRGA79OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(OmegaConf.to_yaml(char_model.cfg))"
      ],
      "metadata": {
        "id": "pLiGyOOSMnHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Korean datasets"
      ],
      "metadata": {
        "id": "eDTFyYEIE1ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### label set (character set)"
      ],
      "metadata": {
        "id": "UZztfGkaVHCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"kresnik/zeroth_korean\", \"clean\")"
      ],
      "metadata": {
        "id": "35z2Is_l5PYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ds['train']\n",
        "test_ds = ds['test']"
      ],
      "metadata": {
        "id": "1HzLoU2LFuAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds"
      ],
      "metadata": {
        "id": "3ZYXwGwLInY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(OmegaConf.to_yaml(char_model.cfg))"
      ],
      "metadata": {
        "id": "80S_pdw5KX4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds[0]"
      ],
      "metadata": {
        "id": "iqSd5ByUSkZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.remove_columns([\"speaker_id\", \"chapter_id\", \"id\", \"audio\"])\n",
        "test_ds = test_ds.remove_columns([\"speaker_id\", \"chapter_id\", \"id\", \"audio\"])"
      ],
      "metadata": {
        "id": "WMllSMFlaGnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "def get_duration(batch):\n",
        "  speech = sf.SoundFile(batch['file'])\n",
        "  duration = speech.frames / speech.samplerate\n",
        "  batch['duration'] = duration\n",
        "  return batch\n",
        "\n",
        "#def rename_key(batch):\n",
        "#  batch['audio_filepath'] = batch['file']\n",
        "  #batch.remove_columns(['file'])\n",
        "\n",
        "#  return batch\n"
      ],
      "metadata": {
        "id": "MHEV2Df4ZzPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(get_duration)\n",
        "test_ds = test_ds.map(get_duration)\n",
        "\n",
        "train_ds = train_ds.rename_column(original_column_name='file', new_column_name='audio_filepath')\n",
        "test_ds = test_ds.rename_column(original_column_name='file', new_column_name='audio_filepath')"
      ],
      "metadata": {
        "id": "FBoUV1xZbKMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### write manifest"
      ],
      "metadata": {
        "id": "uNBOliAde_Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def read_manifest(path):\n",
        "    manifest = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in tqdm(f, desc=\"Reading manifest data\"):\n",
        "            line = line.replace(\"\\n\", \"\")\n",
        "            data = json.loads(line)\n",
        "            manifest.append(data)\n",
        "    return manifest"
      ],
      "metadata": {
        "id": "bi6tI0elmh-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "train_json_path = os.path.abspath('train.json')\n",
        "test_json_path = os.path.abspath('test.json')\n",
        "\n",
        "train_json = train_ds.to_json(train_json_path)\n",
        "test_json = test_ds.to_json(test_json_path)\n",
        "\n",
        "train_manifest = read_manifest('train.json')\n",
        "test_manifest = read_manifest('test.json')"
      ],
      "metadata": {
        "id": "xkmBn7PRl6d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract chars"
      ],
      "metadata": {
        "id": "NBN6qXE1cqLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}"
      ],
      "metadata": {
        "id": "1mc4YPWlJZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_train = train_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_ds.column_names)\n",
        "vocab_test = test_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=test_ds.column_names)"
      ],
      "metadata": {
        "id": "VdXmhx9dJcx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))"
      ],
      "metadata": {
        "id": "l3qOgpQlJdn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_list"
      ],
      "metadata": {
        "id": "r-9sV8l4Jv_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_model.change_vocabulary(new_vocabulary=vocab_list)"
      ],
      "metadata": {
        "id": "9RCGGtdq402t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PPDTaLyejAR"
      },
      "outputs": [],
      "source": [
        "#@title Freeze Encoder { display-mode: \"form\" }\n",
        "freeze_encoder = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "freeze_encoder = bool(freeze_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qiTTgDGejC9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def enable_bn_se(m):\n",
        "    if type(m) == nn.BatchNorm1d:\n",
        "        m.train()\n",
        "        for param in m.parameters():\n",
        "            param.requires_grad_(True)\n",
        "\n",
        "    if 'SqueezeExcite' in type(m).__name__:\n",
        "        m.train()\n",
        "        for param in m.parameters():\n",
        "            param.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I5dx_GWejFm"
      },
      "outputs": [],
      "source": [
        "if freeze_encoder:\n",
        "    char_model.encoder.freeze()\n",
        "    char_model.encoder.apply(enable_bn_se)\n",
        "    logging.info(\"Model encoder has been frozen, and batch normalization has been unfrozen\")\n",
        "else:\n",
        "    char_model.encoder.unfreeze()\n",
        "    logging.info(\"Model encoder has been un-frozen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBIy8p0fV7sa"
      },
      "outputs": [],
      "source": [
        "char_model.cfg.labels = vocab_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzpByrdfejIA"
      },
      "outputs": [],
      "source": [
        "cfg = copy.deepcopy(char_model.cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlQ5iGrZejKy"
      },
      "outputs": [],
      "source": [
        "# Setup train, validation, test configs\n",
        "with open_dict(cfg):    \n",
        "  # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
        "  cfg.train_ds.manifest_filepath = f\"{train_json_path}\"\n",
        "  cfg.train_ds.labels = vocab_list\n",
        "  cfg.train_ds.normalize_transcripts = False\n",
        "  cfg.train_ds.batch_size = 16\n",
        "  cfg.train_ds.num_workers = 2\n",
        "  cfg.train_ds.pin_memory = True\n",
        "  cfg.train_ds.trim_silence = True\n",
        "\n",
        "  # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
        "  cfg.validation_ds.manifest_filepath = test_json_path\n",
        "  cfg.validation_ds.labels = vocab_list\n",
        "  cfg.validation_ds.normalize_transcripts = False\n",
        "  cfg.validation_ds.batch_size = 8\n",
        "  cfg.validation_ds.num_workers = 8\n",
        "  cfg.validation_ds.pin_memory = True\n",
        "  cfg.validation_ds.trim_silence = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx9DixV0ejMo"
      },
      "outputs": [],
      "source": [
        "# setup data loaders with new configs\n",
        "char_model.setup_training_data(cfg.train_ds)\n",
        "char_model.setup_multiple_validation_data(cfg.validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgoD5hOKYSKJ"
      },
      "outputs": [],
      "source": [
        "# Original optimizer + scheduler\n",
        "print(OmegaConf.to_yaml(char_model.cfg.optim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okytaslHejOm"
      },
      "outputs": [],
      "source": [
        "with open_dict(char_model.cfg.optim):\n",
        "    char_model.cfg.optim.lr = 0.01\n",
        "    char_model.cfg.optim.betas = [0.95, 0.5]  # from paper\n",
        "    char_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n",
        "    char_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
        "    char_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
        "    char_model.cfg.optim.sched.min_lr = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ6Md-dLejRA"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(char_model.cfg.spec_augment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ei9WsLzejTI"
      },
      "outputs": [],
      "source": [
        "# with open_dict(char_model.cfg.spec_augment):\n",
        "#   char_model.cfg.spec_augment.freq_masks = 2\n",
        "#   char_model.cfg.spec_augment.freq_width = 25\n",
        "#   char_model.cfg.spec_augment.time_masks = 2\n",
        "#   char_model.cfg.spec_augment.time_width = 0.05\n",
        "\n",
        "char_model.spec_augmentation = char_model.from_config_dict(char_model.cfg.spec_augment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cN1FC0o2ejVg"
      },
      "outputs": [],
      "source": [
        "#@title Metric\n",
        "use_cer = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "log_prediction = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HURZMpPwejXa"
      },
      "outputs": [],
      "source": [
        "char_model._wer.use_cer = use_cer\n",
        "char_model._wer.log_prediction = log_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaw1qsQIf1Zv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as ptl\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpus = 1\n",
        "else:\n",
        "    gpus = 0\n",
        "\n",
        "EPOCHS = 50  # 100 epochs would provide better results, but would take an hour to train\n",
        "\n",
        "trainer = ptl.Trainer(gpus=gpus, \n",
        "                      max_epochs=EPOCHS, \n",
        "                      accumulate_grad_batches=1,\n",
        "                      checkpoint_callback=False,\n",
        "                      logger=False,\n",
        "                      log_every_n_steps=50,\n",
        "                      check_val_every_n_epoch=10)\n",
        "\n",
        "# Setup model with the trainer\n",
        "char_model.set_trainer(trainer)\n",
        "\n",
        "# Finally, update the model's internal config\n",
        "char_model.cfg = char_model._cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENSpJJqcf1cG"
      },
      "outputs": [],
      "source": [
        "# Environment variable generally used for multi-node multi-gpu training.\n",
        "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
        "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
        "\n",
        "config = exp_manager.ExpManagerConfig(\n",
        "    exp_dir=f'experiments/lang/',\n",
        "    name=f\"ASR-Char-Model-Korean\",\n",
        "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
        "        monitor=\"val_wer\",\n",
        "        mode=\"min\",\n",
        "        always_save_nemo=True,\n",
        "        save_best_model=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "config = OmegaConf.structured(config)\n",
        "\n",
        "logdir = exp_manager.exp_manager(trainer, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATI2R0D7rylR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google import colab\n",
        "    COLAB_ENV = True\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "if COLAB_ENV:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir /content/experiments/lang/ASR-Char-Model-Korean/\n",
        "else:\n",
        "    print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvaESyJHf1eb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.fit(char_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ASR_CTC_Language_Finetuning2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}