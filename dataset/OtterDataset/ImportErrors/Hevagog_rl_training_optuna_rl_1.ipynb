{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (3.6.1)\n",
      "Requirement already satisfied: colorlog in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: numpy in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (24.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: Mako in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/tomek/pytorch_learning/venv/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install stable-baselines3\n",
    "# !pip install sb3-contrib\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN\n",
    "from sb3_contrib import QRDQN, TQC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try first with PPO and a small budget of 4000 steps (20 episodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"Pendulum-v1\"\n",
    "# Env used only for evaluation\n",
    "eval_envs = make_vec_env(env_id, n_envs=10)\n",
    "# 4000 training timesteps\n",
    "budget_pendulum = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Mean episode reward: -1176.79 +/- 241.61\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_model = A2C(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2C Mean episode reward: -1536.49 +/- 36.69\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"A2C Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Longer PPO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train longer\n",
    "new_budget = 10 * budget_pendulum\n",
    "\n",
    "ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(new_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Mean episode reward: -1159.79 +/- 226.92\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO - Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.17e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025463108 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 27.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -1.03e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032414593 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.3        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 4.18        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 9.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -654        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039780863 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 0.313       |\n",
      "|    value_loss           | 4.58        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 200        |\n",
      "|    ep_rew_mean          | -356       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 421        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03265425 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 1.73       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.235      |\n",
      "|    value_loss           | 3.67       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -256        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 537         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035707045 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 0.0932      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    std                  | 0.201       |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_params = {\n",
    "    \"gamma\": 0.9,\n",
    "    \"use_sde\": True,\n",
    "    \"sde_sample_freq\": 4,\n",
    "    \"learning_rate\": 1e-3,\n",
    "}\n",
    "\n",
    "# budget = 10 * budget_pendulum\n",
    "ppo_tuned_model = PPO(\"MlpPolicy\", env_id, seed=1, verbose=1, **tuned_params).learn(50_000, log_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned PPO Mean episode reward: -177.80 +/- 112.23\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"Tuned PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II: Grad Student Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 20_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_envs_cartpole = make_vec_env(\"CartPole-v1\", n_envs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.2     |\n",
      "|    ep_rew_mean        | 20.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.688   |\n",
      "|    explained_variance | -0.0129  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.95     |\n",
      "|    value_loss         | 8.69     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 19.9     |\n",
      "|    ep_rew_mean        | 19.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | 0.178    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -5.49    |\n",
      "|    value_loss         | 88.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 19.6     |\n",
      "|    ep_rew_mean        | 19.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | -0.0665  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.81     |\n",
      "|    value_loss         | 7.53     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 19.7      |\n",
      "|    ep_rew_mean        | 19.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 110       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.69     |\n",
      "|    explained_variance | -0.000388 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.65      |\n",
      "|    value_loss         | 6.52      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.7     |\n",
      "|    ep_rew_mean        | 20.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 112      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.68    |\n",
      "|    explained_variance | -0.178   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -12.9    |\n",
      "|    value_loss         | 476      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 21.3     |\n",
      "|    ep_rew_mean        | 21.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | 0.00671  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.46     |\n",
      "|    value_loss         | 5.61     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 22.2     |\n",
      "|    ep_rew_mean        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 112      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | 0.031    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 4.7      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 23       |\n",
      "|    ep_rew_mean        | 23       |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | 0.17     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.32     |\n",
      "|    value_loss         | 4.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24.2     |\n",
      "|    ep_rew_mean        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.688   |\n",
      "|    explained_variance | 0.006    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.33     |\n",
      "|    value_loss         | 4.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.3     |\n",
      "|    ep_rew_mean        | 25.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 103      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.625   |\n",
      "|    explained_variance | 0.0399   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.54     |\n",
      "|    value_loss         | 3.87     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.1     |\n",
      "|    ep_rew_mean        | 29.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0.00653  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.939    |\n",
      "|    value_loss         | 3.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 32.3     |\n",
      "|    ep_rew_mean        | 32.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.62    |\n",
      "|    explained_variance | 0.0192   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.995    |\n",
      "|    value_loss         | 2.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 34.6     |\n",
      "|    ep_rew_mean        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.607   |\n",
      "|    explained_variance | 0.000247 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.572    |\n",
      "|    value_loss         | 2.51     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 38.1      |\n",
      "|    ep_rew_mean        | 38.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 106       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.425    |\n",
      "|    explained_variance | -0.000446 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.1       |\n",
      "|    value_loss         | 2.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 42.1      |\n",
      "|    ep_rew_mean        | 42.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.65     |\n",
      "|    explained_variance | -0.000619 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.649     |\n",
      "|    value_loss         | 1.77      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 46       |\n",
      "|    ep_rew_mean        | 46       |\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | -0.00595 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.85     |\n",
      "|    value_loss         | 1.42     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.9     |\n",
      "|    ep_rew_mean        | 48.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | 0.000355 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.882    |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 55        |\n",
      "|    ep_rew_mean        | 55        |\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.534    |\n",
      "|    explained_variance | -0.000599 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.495     |\n",
      "|    value_loss         | 0.823     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58.8     |\n",
      "|    ep_rew_mean        | 58.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | 0.000926 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.295    |\n",
      "|    value_loss         | 0.584    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 62.8      |\n",
      "|    ep_rew_mean        | 62.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 109       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.547    |\n",
      "|    explained_variance | -0.000198 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.271     |\n",
      "|    value_loss         | 0.387     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 67.4      |\n",
      "|    ep_rew_mean        | 67.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 112       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.557    |\n",
      "|    explained_variance | -0.000205 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 0.22      |\n",
      "|    value_loss         | 0.225     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 72.6      |\n",
      "|    ep_rew_mean        | 72.6      |\n",
      "| time/                 |           |\n",
      "|    fps                | 114       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.585    |\n",
      "|    explained_variance | -5.96e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 0.104     |\n",
      "|    value_loss         | 0.109     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.2     |\n",
      "|    ep_rew_mean        | 77.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 118      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | 0.000152 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.162    |\n",
      "|    value_loss         | 0.0354   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 81.8     |\n",
      "|    ep_rew_mean        | 81.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | 0.000918 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.0178   |\n",
      "|    value_loss         | 0.00197  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 84.5     |\n",
      "|    ep_rew_mean        | 84.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.56    |\n",
      "|    explained_variance | -0.00665 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.000929 |\n",
      "|    value_loss         | 9.32e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91.1     |\n",
      "|    ep_rew_mean        | 91.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.51    |\n",
      "|    explained_variance | -0.0129  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.00124  |\n",
      "|    value_loss         | 5.93e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 94.6     |\n",
      "|    ep_rew_mean        | 94.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 120      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.368   |\n",
      "|    explained_variance | -0.0187  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 0.000706 |\n",
      "|    value_loss         | 1.18e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 98.8     |\n",
      "|    ep_rew_mean        | 98.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 120      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.544   |\n",
      "|    explained_variance | 0.294    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.000181 |\n",
      "|    value_loss         | 1.57e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 104      |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 119      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.000128 |\n",
      "|    value_loss         | 2.31e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 107      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 118      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.533   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 4.75e-06 |\n",
      "|    value_loss         | 4.31e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 1.25e-06 |\n",
      "|    value_loss         | 1.16e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 114      |\n",
      "|    ep_rew_mean        | 114      |\n",
      "| time/                 |          |\n",
      "|    fps                | 112      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | -4.48    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 9.23e-06 |\n",
      "|    value_loss         | 1.48e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 119      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 110      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 9.7e-07  |\n",
      "|    value_loss         | 5.24e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 124      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.415   |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 8.4e-06  |\n",
      "|    value_loss         | 3.96e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 129       |\n",
      "|    ep_rew_mean        | 129       |\n",
      "| time/                 |           |\n",
      "|    fps                | 106       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.377    |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -2.85e-06 |\n",
      "|    value_loss         | 2.1e-10   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.409   |\n",
      "|    explained_variance | -0.273   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 7.11e-05 |\n",
      "|    value_loss         | 3.61e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 138       |\n",
      "|    ep_rew_mean        | 138       |\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.457    |\n",
      "|    explained_variance | 0.084     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -0.000279 |\n",
      "|    value_loss         | 6.23e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 147       |\n",
      "|    ep_rew_mean        | 147       |\n",
      "| time/                 |           |\n",
      "|    fps                | 108       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.486    |\n",
      "|    explained_variance | -6.39e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 0.00135   |\n",
      "|    value_loss         | 2.12e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 151      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | 0.00633  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.00317  |\n",
      "|    value_loss         | 4.13e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 156       |\n",
      "|    ep_rew_mean        | 156       |\n",
      "| time/                 |           |\n",
      "|    fps                | 107       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 186       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.447    |\n",
      "|    explained_variance | -4.59e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 0.00013   |\n",
      "|    value_loss         | 2.41e-07  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:292.78 +/- 78.05\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[\n",
    "      dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n",
    "    ],\n",
    "    activation_fn=nn.Tanh,\n",
    ")\n",
    "\n",
    "hyperparams = dict(\n",
    "    n_steps=5, # number of steps to collect data before updating policy\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.99, # discount factor\n",
    "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
    "    ent_coef=0.0, # Entropy coefficient for the loss calculation\n",
    ")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 5\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for A2C hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    # Discount factor between 0.9 and 0.9999\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    # 8, 16, 32, ... 1024\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO:\n",
    "    # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`\n",
    "    # - define the network architecture search space [\"tiny\", \"small\"] -> `suggest_categorical`\n",
    "    # - define the activation function search space [\"tanh\", \"relu\"]\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]}\n",
    "        if net_arch == \"tiny\"\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: \n",
    "    # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`   \n",
    "    # 2. Create the evaluation envs\n",
    "    # 3. Create the `TrialEvalCallback`\n",
    "\n",
    "    other_params = sample_a2c_params(trial)\n",
    "    kwargs.update(other_params)\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, True)\n",
    "    \n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    other_params = sample_a2c_params(trial)\n",
    "    kwargs.update(other_params)\n",
    "    \n",
    "    # Create the RL model\n",
    "    model = A2C(**kwargs)\n",
    "\n",
    "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "\n",
    "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
    "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
    "    # TrialEvalCallback signature:\n",
    "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, True)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-20 22:54:48,530] A new study created in memory with name: no-name-7d9fe076-51ad-45d8-ad5c-1f3a4d1401b1\n",
      "/home/tomek/pytorch_learning/venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "[I 2024-05-20 22:58:34,250] Trial 0 finished with value: 9.4 and parameters: {'gamma': 0.00015246845661383903, 'max_grad_norm': 3.0913495580987727, 'exponent_n_steps': 9, 'learning_rate': 0.8877544269301049, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 0 with value: 9.4.\n",
      "[I 2024-05-20 23:01:33,309] Trial 1 finished with value: 9.2 and parameters: {'gamma': 0.029625692998308417, 'max_grad_norm': 2.2115369030439567, 'exponent_n_steps': 8, 'learning_rate': 0.09634235891072042, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 0 with value: 9.4.\n",
      "[I 2024-05-20 23:04:55,420] Trial 2 finished with value: 124.1 and parameters: {'gamma': 0.08082696633350452, 'max_grad_norm': 0.368241644006934, 'exponent_n_steps': 4, 'learning_rate': 0.0002852065514610369, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 2 with value: 124.1.\n",
      "[I 2024-05-20 23:09:24,828] Trial 3 finished with value: 9.7 and parameters: {'gamma': 0.0005292536675004424, 'max_grad_norm': 1.073780327041099, 'exponent_n_steps': 3, 'learning_rate': 0.018170300974818355, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 2 with value: 124.1.\n",
      "[I 2024-05-20 23:14:11,765] Trial 4 finished with value: 409.9 and parameters: {'gamma': 0.001467410343098566, 'max_grad_norm': 0.39490550798289603, 'exponent_n_steps': 3, 'learning_rate': 0.000386362476965838, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 4 with value: 409.9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  5\n",
      "Best trial:\n",
      "  Value: 409.9\n",
      "  Params: \n",
      "    gamma: 0.001467410343098566\n",
      "    max_grad_norm: 0.39490550798289603\n",
      "    exponent_n_steps: 3\n",
      "    learning_rate: 0.000386362476965838\n",
      "    net_arch: small\n",
      "    activation_fn: tanh\n",
      "  User attrs:\n",
      "    gamma_: 0.9985325896569014\n",
      "    n_steps: 8\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/pytorch_learning/venv/lib/python3.10/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Write report\u001b[39;00m\n\u001b[1;32m     35\u001b[0m study\u001b[38;5;241m.\u001b[39mtrials_dataframe()\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudy_results_a2c_cartpole.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m fig1 \u001b[38;5;241m=\u001b[39m \u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m fig2 \u001b[38;5;241m=\u001b[39m plot_param_importances(study)\n\u001b[1;32m     40\u001b[0m fig1\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/pytorch_learning/venv/lib/python3.10/site-packages/optuna/visualization/_optimization_history.py:222\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name, error_bar)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_optimization_history\u001b[39m(\n\u001b[1;32m    173\u001b[0m     study: Study \u001b[38;5;241m|\u001b[39m Sequence[Study],\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     error_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     info_list \u001b[38;5;241m=\u001b[39m _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "File \u001b[0;32m~/pytorch_learning/venv/lib/python3.10/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "th.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
