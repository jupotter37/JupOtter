{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xarray[complete] netcdf4 h5netcdf\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install scipy\n",
    "# %pip install dask\n",
    "# %pip install tensorflow --user\n",
    "# %pip install scikit-learn\n",
    "# %pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Conv2DTranspose, Concatenate, concatenate, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset load and train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "\n",
    "# dataset = np.load('dati/mist/datasets/dataset.npy')\n",
    "# date = np.load('dati/mist/datasets/date.npy')\n",
    "\n",
    "dataset_d = np.load('dati/mist/datasets/dataset_d.npy')\n",
    "dataset_n = np.load('dati/mist/datasets/dataset_n.npy')\n",
    "date_d = np.load('dati/mist/datasets/date_d.npy')\n",
    "date_n = np.load('dati/mist/datasets/date_n.npy')\n",
    "\n",
    "# baseline = np.load('dati/mist/datasets/baseline.npy')\n",
    "baseline_d = np.load('dati/mist/datasets/baseline_d.npy')\n",
    "baseline_n = np.load('dati/mist/datasets/baseline_n.npy')\n",
    "\n",
    "italy_mask = np.load('dati/mist/datasets/italy_mask.npy')\n",
    "data_min = np.load('dati/mist/data_min.npy')\n",
    "data_max = np.load('dati/mist/data_max.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the day and the night dataset into training and testing sets\n",
    "\n",
    "train_indices_d, temp_indices_d = train_test_split(np.arange(dataset_d.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_d, test_indices_d = train_test_split(temp_indices_d, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_d = dataset_d[train_indices_d]\n",
    "x_val_d = dataset_d[val_indices_d]\n",
    "x_test_d = dataset_d[test_indices_d]\n",
    "\n",
    "dates_train_d = date_d[train_indices_d]\n",
    "dates_val_d = date_d[val_indices_d]\n",
    "dates_test_d = date_d[test_indices_d]\n",
    "\n",
    "\n",
    "train_indices_n, temp_indices_n = train_test_split(np.arange(dataset_n.shape[0]), test_size=0.25, random_state=42)\n",
    "val_indices_n, test_indices_n = train_test_split(temp_indices_n, test_size=0.4, random_state=42)\n",
    "\n",
    "x_train_n = dataset_n[train_indices_n]\n",
    "x_val_n = dataset_n[val_indices_n]\n",
    "x_test_n = dataset_n[test_indices_n]\n",
    "\n",
    "dates_train_n = date_n[train_indices_n]\n",
    "dates_val_n = date_n[val_indices_n]\n",
    "dates_test_n = date_n[test_indices_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics, Loss and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    clear_mask = y_true[:,:,:,1:2]      # 0 for land/clouds, 1 for clear sea\n",
    "    y_true = y_true[:,:,:,0:1]          # The true SST values. Obfuscated areas are already converted to 0\n",
    "    \n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    clear_masked_error = squared_error * clear_mask\n",
    "\n",
    "    # Calculate the mean of the masked errors\n",
    "    clear_loss = tf.reduce_mean(clear_masked_error)     # The final loss\n",
    "\n",
    "    return clear_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClearMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    clear_mask = y_true[:,:,:,1:2]  # 0 for land/clouds, 1 for clear sea\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over clear sea\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    clear_masked_error = squared_error * clear_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    clr_metric = tf.reduce_sum(clear_masked_error) / tf.reduce_sum(clear_mask)\n",
    "\n",
    "    #loss = clear_loss\n",
    "    return clr_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArtificialMetric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)    # Was getting an error because of the different types: y_true in the metrics is float64 instead of the normal float32\n",
    "\n",
    "    artificial_mask = y_true[:,:,:,2:3]  # 1 for artificial clouds, 0 for the rest\n",
    "    y_true = y_true[:,:,:,0:1]  # The true SST values. Obfuscated areas are already converted to 0\n",
    "\n",
    "    # Calculate the squared error only over artificially clouded areas\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    artificial_masked_error = squared_error * artificial_mask\n",
    "    # Calculate the mean of the masked errors\n",
    "    art_metric = tf.reduce_sum(artificial_masked_error) / tf.reduce_sum(artificial_mask)\n",
    "\n",
    "    return art_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "epochs=100\n",
    "batch_size=32\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "loss = customLoss\n",
    "metrics = [ClearMetric, ArtificialMetric]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "steps_per_epoch = min(100, len(x_train_d) // batch_size)\n",
    "validation_steps = 20\n",
    "testing_steps = 20\n",
    "\n",
    "input_shape = (256, 256, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline generator function\n",
    "\n",
    "def baseline_generator(batch_size, data_d, data_n, dates_d, dates_n, dayChance=0.5):\n",
    "    while True:\n",
    "        batch_x = np.zeros((batch_size, 256, 256, 4))\n",
    "        batch_y = np.zeros((batch_size, 256, 256, 3))\n",
    "\n",
    "        #Randomly choose between day and night dataset\n",
    "        (dataset, date, baseline) = (data_d, dates_d, baseline_d) if np.random.rand() < dayChance else (data_n, dates_n, baseline_n)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Choose a random index as the current day, and 3 random indices\n",
    "            i, r1, r2, r3= np.random.randint(0, dataset.shape[0], 4)\n",
    "\n",
    "            # Extract the image and mask from the current day, and the masks from the other days\n",
    "            image_current = np.nan_to_num(dataset[i], nan=0)\n",
    "            mask_current = np.isnan(dataset[i])\n",
    "            mask_r1 = np.isnan(dataset[r1])\n",
    "            mask_r2 = np.isnan(dataset[r2])\n",
    "            mask_r3 = np.isnan(dataset[r3])\n",
    "\n",
    "            # Perform OR operation between masks\n",
    "            mask_or_r1 = np.logical_or(mask_current, mask_r1)\n",
    "            mask_or_r2 = np.logical_or(mask_current, mask_r2)\n",
    "            mask_or_r3 = np.logical_or(mask_current, mask_r3)\n",
    "            #choose the middle mask\n",
    "            masks = [mask_or_r1, mask_or_r2, mask_or_r3]\n",
    "            masks.sort(key=np.sum)\n",
    "            artificial_mask = masks[1] # The mask with the medium amount of coverage\n",
    "\n",
    "            # Apply the amplified mask to the current day's image\n",
    "            image_masked = np.where(artificial_mask, 0, image_current)\n",
    "            \n",
    "            # Convert the current date to a datetime object using pandas\n",
    "            date_series = pd.to_datetime(date[i], unit='D', origin='unix')\n",
    "            day_of_year = date_series.dayofyear\n",
    "\n",
    "            # Fix masks before they are used in the loss and metric functions\n",
    "            artificial_mask = np.logical_xor(artificial_mask, mask_current)  # 1 for artificially obfuscated, 0 for the rest\n",
    "            mask_current = np.logical_not(mask_current) # 1 for clear sea, 0 for land/clouds\n",
    "            \n",
    "            # Create batch_x and batch_y\n",
    "            batch_x[b, ..., 0] = image_masked               #artificially cloudy image\n",
    "            batch_x[b, ..., 1] = mask_current               #real mask\n",
    "            batch_x[b, ..., 2] = italy_mask                 #land-sea mask\n",
    "            batch_x[b, ..., 3] = baseline[day_of_year - 1]  #baseline values for the current day (day_of_year starts from 1)\n",
    "\n",
    "            batch_y[b, ..., 0] = image_current              #real image\n",
    "            batch_y[b, ..., 1] = mask_current               #real mask\n",
    "            batch_y[b, ..., 2] = artificial_mask            #artificial mask used for the input\n",
    "        \n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator that returns dates\n",
    "def gen_qual_dates(batch_size, data_d, data_n, qual_d, qual_n, dates_d, dates_n, dayChance=0.5):\n",
    "    while True:\n",
    "        batch_x = np.zeros((batch_size, 256, 256, 4))\n",
    "        batch_y = np.zeros((batch_size, 256, 256, 4))\n",
    "        batch_dates = []\n",
    "        batch_day_night = []\n",
    "\n",
    "        #Randomly choose between day and night dataset\n",
    "        is_day = np.random.rand() < dayChance\n",
    "        (dataset, qdataset, date, baseline) = (data_d, qual_d, dates_d, baseline_d) if is_day else (data_n, qual_n, dates_n, baseline_n)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Choose a random index as the current day, and 3 random indices\n",
    "            i, r1, r2, r3= np.random.randint(0, dataset.shape[0], 4)\n",
    "\n",
    "            # Extract the image  and mask from the current day, and the masks from the other days\n",
    "            image_current = np.nan_to_num(dataset[i], nan=0)\n",
    "            mask_current = np.isnan(dataset[i])\n",
    "            mask_r1 = np.isnan(dataset[r1])\n",
    "            mask_r2 = np.isnan(dataset[r2])\n",
    "            mask_r3 = np.isnan(dataset[r3])\n",
    "\n",
    "            # Perform OR operation between masks\n",
    "            mask_or_r1 = np.logical_or(mask_current, mask_r1)\n",
    "            mask_or_r2 = np.logical_or(mask_current, mask_r2)\n",
    "            mask_or_r3 = np.logical_or(mask_current, mask_r3)\n",
    "            #choose the middle mask\n",
    "            masks = [mask_or_r1, mask_or_r2, mask_or_r3]\n",
    "            masks.sort(key=np.sum)\n",
    "            artificial_mask = masks[1] # The mask with the medium amount of coverage\n",
    "            # Apply the amplified mask to the current day's image\n",
    "            image_masked = np.where(artificial_mask, 0, image_current)\n",
    "\n",
    "            # Convert the current date to a datetime object using pandas\n",
    "            date_series = pd.to_datetime(date[i], unit='D', origin='unix')\n",
    "            day_of_year = date_series.dayofyear\n",
    "\n",
    "            # Fix masks before they are used in the loss and metric functions\n",
    "            artificial_mask = np.logical_xor(artificial_mask, mask_current)  # 1 for artificially obfuscated, 0 for the rest\n",
    "            mask_current = np.logical_not(mask_current) # 1 for clear sea, 0 for land/clouds\n",
    "\n",
    "            #Prepare quality measurements\n",
    "            q_image = np.nan_to_num(qdataset[i], nan=-1)    # -1 for missing quality measurements\n",
    "            \n",
    "            # Create batch_x and batch_y\n",
    "            batch_x[b, ..., 0] = image_masked               #artificially cloudy image\n",
    "            batch_x[b, ..., 1] = mask_current               #real mask\n",
    "            batch_x[b, ..., 2] = italy_mask                 #land-sea mask\n",
    "            batch_x[b, ..., 3] = baseline[day_of_year - 1]  #baseline values for the current day (day_of_year starts from 1)\n",
    "\n",
    "            batch_y[b, ..., 0] = image_current              #real image\n",
    "            batch_y[b, ..., 1] = mask_current               #real mask\n",
    "            batch_y[b, ..., 2] = artificial_mask            #artificial mask used for the input\n",
    "            batch_y[b, ..., 3] = q_image                    #quality measurement\n",
    "\n",
    "            batch_dates.append(date_series.date())\n",
    "            batch_day_night.append('diurnal' if is_day else 'nocturnal')\n",
    "        \n",
    "        yield batch_x, batch_y, batch_dates, batch_day_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generators\n",
    "\n",
    "train_gen = baseline_generator(batch_size, x_train_d, x_train_n, dates_train_d, dates_train_n)\n",
    "val_gen = baseline_generator(batch_size, x_val_d, x_val_n, dates_val_d, dates_val_n)\n",
    "test_gen = baseline_generator(batch_size, x_test_d, x_test_n, dates_test_d, dates_test_n)\n",
    "\n",
    "# Test generator that returns dates\n",
    "#test_gen_dates = gen_qual_dates(batch_size, x_test_d, x_test_n, q_test_d, q_test_n, dates_test_d, dates_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "\n",
    "x,y = next(train_gen)\n",
    "r = np.random.randint(0, batch_size)    # Choose a random image from the batch\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the x data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x[r, :, :, 0], cmap='jet')\n",
    "plt.title(\"x_0 (model input)\")\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot the y data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(y[r, :, :, 0], cmap='jet')\n",
    "plt.title(\"y_0 (ground truth)\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Information about the data\n",
    "#print(np.isnan(x).any())\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"min of all x:\", np.min(x[..., 0]))\n",
    "print(\"max of all x:\", np.max(x[..., 0]))\n",
    "print(\"min of this x:\", np.min(x[r, :, :, 0]))\n",
    "print(\"max of this x:\", np.max(x[r, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net model with residual blocks\n",
    "\n",
    "def ResidualBlock(depth):\n",
    "    def apply(x):\n",
    "        input_depth = x.shape[3]    # Get the number of channels from the channels dimension\n",
    "        if input_depth == depth:    # It's already the desired channel number\n",
    "            residual = x\n",
    "        else:                       # Adjust the number of channels with a 1x1 convolution\n",
    "            residual = Conv2D(depth, kernel_size=1)(x)\n",
    "\n",
    "        x = BatchNormalization(center=False, scale=False)(x)    \n",
    "        x = Conv2D(depth, kernel_size=3, padding=\"same\", activation='swish')(x) \n",
    "        x = Conv2D(depth, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Add()([x, residual])\n",
    "        return x\n",
    "    \n",
    "    return apply\n",
    "\n",
    "\n",
    "def DownBlock(depth, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(depth)(x)\n",
    "            skips.append(x)\n",
    "        x = AveragePooling2D(pool_size=2)(x)    #downsampling\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(depth, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = UpSampling2D(size=2, interpolation=\"bilinear\")(x)   #upsampling\n",
    "        for _ in range(block_depth):\n",
    "            x = Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(depth)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_Unet(image_size, depths, block_depth):\n",
    "    input_images = Input(shape=image_size)  #input layer\n",
    "    \n",
    "    x = Conv2D(depths[0], kernel_size=1)(input_images)  #reduce the number of channels\n",
    "\n",
    "    skips = []  #store the skip connections\n",
    "    \n",
    "    for depth in depths[:-1]:   #downsampling layers\n",
    "        x = DownBlock(depth, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):    #middle layer\n",
    "        x = ResidualBlock(depths[-1])(x)\n",
    "\n",
    "    for depth in reversed(depths[:-1]):   #upsampling layers\n",
    "        x = UpBlock(depth, block_depth)([x, skips])\n",
    "\n",
    "    x = Conv2D(1, kernel_size=1, kernel_initializer=\"zeros\", name = \"output_noise\")(x)  #output layer\n",
    "    \n",
    "    return Model(input_images, outputs=x, name=\"UNetInpainter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, create it and print the summary\n",
    "depths = [32, 64, 128]\n",
    "block_depth = 2\n",
    "\n",
    "model = get_Unet(input_shape, depths, block_depth)\n",
    "# #model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with custom loss function\n",
    "opt = Adam(learning_rate=lr)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WEIGHTS\n",
    "# model.load_weights('weights/baseline.weights.h5')   # does not work on local machine\n",
    "model.load_weights('weights/baseline.h5')\n",
    "\n",
    "# SAVE WEIGHTS\n",
    "#model.save_weights('weights/baseline.h5')  # execute remotely after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "#history = model.fit(train_gen, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=val_gen, validation_steps=validation_steps, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment on Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop error calculation over tot batches\n",
    "\n",
    "# Initialize lists to store the average errors, maximum errors, and variances\n",
    "avg_errors_list = []\n",
    "avg_max_errors_list = []\n",
    "var_max_errors_list = []\n",
    "\n",
    "# Generate and evaluate tot batches\n",
    "tot = 100\n",
    "for _ in range(tot):\n",
    "    # Generate a batch\n",
    "    x_true, y_true = next(test_gen)\n",
    "    predictions = model.predict(x_true)\n",
    "\n",
    "    # Denormalize\n",
    "    predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "    true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "    # Calculate the errors\n",
    "    clearMask = y_true[..., 1]\n",
    "    errors = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "\n",
    "    # Calculate the average and maximum error for each image in the batch\n",
    "    avg_errors = np.nanmean(errors, axis=(1, 2))\n",
    "    max_errors = np.nanmax(errors, axis=(1, 2))\n",
    "\n",
    "    # Add the average error, average maximum error, and variance of maximum errors to the lists\n",
    "    avg_errors_list.append(np.mean(avg_errors))\n",
    "    avg_max_errors_list.append(np.mean(max_errors))\n",
    "    var_max_errors_list.append(np.var(max_errors))\n",
    "\n",
    "# Print the average, average maximum, and variance of maximums calculated over tot batches\n",
    "print(f\"Average error over {tot} batches:\", np.mean(avg_errors_list))\n",
    "print(f\"Average maximum error over {tot} batches:\", np.mean(avg_max_errors_list))\n",
    "print(f\"Variance of maximum errors over {tot} batches:\", np.mean(var_max_errors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors on degrees\n",
    "\n",
    "#Prediction\n",
    "x_true, y_true, t_dates, t_dn = next(test_gen_dates)\n",
    "predictions = model.predict(x_true)\n",
    "# Denormalization\n",
    "predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "input_images_denorm = ((x_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    " \n",
    "clearMask = y_true[..., 1]  # 1 for all clear sea, 0 for land/clouds\n",
    "artificialMask = y_true[..., 2] # 1 for artificially obfuscated sea parts, 0 for the rest\n",
    "NonArtificialMask = np.logical_xor(clearMask, artificialMask)   # 1 for unobfuscated clear sea, 0 for the rest\n",
    "\n",
    "# Calculate the errors (batch_size, 256, 256)\n",
    "errors = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "# Calculate the average and maximum error for each image in the batch\n",
    "avg_errors = np.nanmean(errors, axis=(1, 2))\n",
    "max_errors = np.nanmax(errors, axis=(1, 2))\n",
    "\n",
    "avg_error = np.mean(avg_errors)\n",
    "print(\"Average error:\", avg_error)\n",
    "# Calculate the average and variance of maximum errors\n",
    "avg_max_error = np.mean(max_errors)\n",
    "print(\"Average maximum error:\", avg_max_error)\n",
    "var_max_error = np.var(max_errors)\n",
    "print(\"Variance of maximum errors:\", var_max_error)\n",
    "\n",
    "print(\"===================================================\")\n",
    "\n",
    "def printValues(i, y, x):\n",
    "    print(\"Sample n°\", i+1, \", date:\", t_dates[i], \",\", t_dn[i], \"; y and x:\", y, x)\n",
    "    print(\"Error in specific point:\", errors[i, y, x])\n",
    "    print(\"Predicted value in specific point:\", predictions_denorm[i, y, x])\n",
    "    print(\"Real value in specific point:\", true_values_denorm[i, y, x])\n",
    "\n",
    "    # Define the offsets for the 8 surrounding points\n",
    "    offsets = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 0), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "    # Initialize the 3x3 matrices\n",
    "    real_values_matrix = np.full((3, 3), np.nan)\n",
    "    predicted_values_matrix = np.full((3, 3), np.nan)\n",
    "    quality_assertion_matrix = np.full((3, 3), np.nan)\n",
    "\n",
    "    # For each offset, set the value in the matrix if the point is not masked\n",
    "    for dy, dx in offsets:\n",
    "        ny, nx = y + dy, x + dx\n",
    "        if 0 <= ny < 256 and 0 <= nx < 256 and clearMask[i, ny, nx]:\n",
    "            real_values_matrix[dy+1, dx+1] = true_values_denorm[i, ny, nx]\n",
    "            predicted_values_matrix[dy+1, dx+1] = predictions_denorm[i, ny, nx]\n",
    "            quality_assertion_matrix[dy+1, dx+1] = y_true[i, ny, nx, 3]\n",
    "\n",
    "    print(\"3x3 matrix of real values:\")\n",
    "    print(real_values_matrix)\n",
    "    print(\"3x3 matrix of predicted values:\")\n",
    "    print(predicted_values_matrix)\n",
    "    print(\"3x3 matrix of quality assertion values:\")\n",
    "    print(quality_assertion_matrix)\n",
    "\n",
    "\n",
    "max_error_indices = np.nanargmax(errors.reshape(errors.shape[0], -1), axis=1)  # Find the index of the maximum error in each flattened image\n",
    "max_error_positions = np.unravel_index(max_error_indices, errors.shape[1:]) # Convert the index back to 2D coordinates\n",
    "\n",
    "# Plot each image with the position of the maximum error\n",
    "for i, (y, x) in enumerate(zip(*max_error_positions)):\n",
    "    # Check if the maximum error for this sample is equal to or above 5\n",
    "    if max_errors[i] >= 5:\n",
    "        #in that point, for all samples, print max_error, its predicted value and its ground_truth value\n",
    "        printValues(i,y,x)\n",
    "\n",
    "        plt.figure(figsize=(12, 12))\n",
    "\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.imshow(errors[i], cmap='jet')\n",
    "        plt.title(f\"Error - Batch {i+1}\")\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        masked_prediction = np.where(italy_mask, predictions_denorm[i], np.nan)\n",
    "        plt.imshow(masked_prediction, cmap='viridis')\n",
    "        plt.scatter(x, y, color='red')\n",
    "        plt.title(f\"Prediction (masked) with max error marked - Batch {i+1}\")\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        mask_overlay = np.where(clearMask[i], true_values_denorm[i], np.nan)\n",
    "        plt.imshow(mask_overlay, cmap='viridis')\n",
    "        plt.title(f\"Real image - Batch {i+1}\")\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        mask_overlay = np.where(NonArtificialMask[i], input_images_denorm[i], np.nan)\n",
    "        plt.imshow(mask_overlay, cmap='viridis')\n",
    "        plt.title(f\"Input image - Batch {i+1}\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for problematic spots in real images\n",
    "\n",
    "# Generate a batch\n",
    "x_true, y_true, t_dates, t_dn = next(test_gen_dates)\n",
    "predictions = model.predict(x_true)\n",
    "\n",
    "# Denormalize the true values\n",
    "predictions_denorm = ((predictions[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "true_values_denorm = ((y_true[..., 0] + 1) / 2) * (data_max - data_min) + data_min\n",
    "\n",
    "# Get the clear mask\n",
    "clearMask = y_true[..., 1]  # 1 for all clear sea, 0 for land/clouds\n",
    "\n",
    "# Get the quality assessment values\n",
    "quality_values = np.where(clearMask, y_true[..., 3], np.nan)\n",
    "\n",
    "# Initialize an array to store the differences\n",
    "errors = np.where(clearMask, np.abs(predictions_denorm - true_values_denorm), np.nan)\n",
    "\n",
    "# For each image in the batch\n",
    "for i in range(batch_size):\n",
    "    problematic_spots = []\n",
    "    problematic_spot_found = False\n",
    "\n",
    "    # For each pixel in the image\n",
    "    for y in range(true_values_denorm.shape[0]):\n",
    "        for x in range(true_values_denorm.shape[1]):\n",
    "            if clearMask[i, y, x] != 0: # Only consider the pixel if it's not masked\n",
    "                pixel = true_values_denorm[i, y, x]\n",
    "\n",
    "                # Get the values of the neighbors\n",
    "                neighbors_mask = clearMask[i, max(0, y-1):min(y+2, true_values_denorm.shape[0]), max(0, x-1):min(x+2, true_values_denorm.shape[1])]\n",
    "                neighbors_values = true_values_denorm[i, max(0, y-1):min(y+2, true_values_denorm.shape[0]), max(0, x-1):min(x+2, true_values_denorm.shape[1])]\n",
    "                neighbors_values[neighbors_mask == 0] = np.nan  # Replace masked values with np.nan\n",
    "\n",
    "                # Calculate the max difference\n",
    "                max_diff = np.nanmax(np.abs(neighbors_values - pixel)) if np.count_nonzero(~np.isnan(neighbors_values)) > 0 else 0\n",
    "\n",
    "                # If the max difference is greater than a tot amount, it's a problematic spot\n",
    "                if max_diff > 5:\n",
    "                    print(\"Sample n°\", i+1, \", date:\", t_dates[i], \",\", t_dn[i])\n",
    "                    print(f\"Problematic spot found in position ({y}, {x}), max difference: {max_diff}\")\n",
    "                    print(\"Adjacent values:\")\n",
    "                    print(neighbors_values)\n",
    "\n",
    "                    # Print the quality assertion values for the pixel and its neighbors\n",
    "                    quality_assertion_values = quality_values[i, max(0, y-1):min(y+2, true_values_denorm.shape[0]), max(0, x-1):min(x+2, true_values_denorm.shape[1])]\n",
    "                    print(\"Quality assertion values:\")\n",
    "                    print(quality_assertion_values)\n",
    "\n",
    "                    # Set the flag to True and store the coordinates of the problematic spot\n",
    "                    problematic_spot_found = True\n",
    "                    problematic_spots.append((x, y))\n",
    "\n",
    "    # If a problematic spot was found, plot the error image and the error image with the problematic spots marked\n",
    "    if problematic_spot_found:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(24, 12))\n",
    "        # Plot the error image\n",
    "        axs[0].imshow(errors[i], cmap='jet')\n",
    "        axs[0].set_title(f\"Error map - Batch {i+1}\")\n",
    "        # Plot the error image with the problematic spots marked\n",
    "        axs[1].imshow(errors[i], cmap='jet')\n",
    "        axs[1].scatter(*zip(*problematic_spots), color='magenta')  # Scatter plot of the problematic spots\n",
    "        axs[1].set_title(f\"Error map - Batch {i+1} (Problematic spots marked)\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to show loss and metrics\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "x_true, y_true = next(test_gen)\n",
    "results = model.evaluate(x_true, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Generate predictions. This generates a batch of data.\n",
    "x_true, y_true = next(test_gen)\n",
    "print(\"x_true shape:\", x_true.shape)\n",
    "print(\"y_true shape:\", y_true.shape)\n",
    "print(\"is there a Nan in x?\", np.isnan(x_true).any())\n",
    "print(\"is there a Nan in y?\", np.isnan(y_true).any())\n",
    "\n",
    "predictions = model.predict(x_true)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(\"y's min:\", np.min(y_true[:, :, :, 0]))\n",
    "print(\"y's max:\", np.max(y_true[:, :, :, 0]))\n",
    "print(\"x's min:\", np.min(predictions[:, :, :, 0]))\n",
    "print(\"x's max:\", np.max(predictions[:, :, :, 0]))\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "evalx = model.evaluate(x_true, y_true)\n",
    "print(\"evalx: \", evalx)\n",
    "xloss = customLoss(y_true, predictions)\n",
    "print(\"xloss: \", xloss)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "#get the coordinates of min and max values in a single prediction. This is to check if the model is predicting the same values as the true ones\n",
    "coordxmin = np.argmin(predictions[0, :, :, 0])\n",
    "coordxmax = np.argmax(predictions[0, :, :, 0])\n",
    "print(\"first x's min:\", coordxmin%256, coordxmin//256, np.nanmin(predictions[0, :, :, 0]))\n",
    "print(\"first x's max:\", coordxmax%256, coordxmax//256, np.nanmax(predictions[0, :, :, 0]))\n",
    "print(\"predictions in coordxmin:\", predictions[0, coordxmin//256, coordxmin%256, 0])\n",
    "print(\"predictions in coordxmax:\", predictions[0, coordxmax//256, coordxmax%256, 0])\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "# Plot the predictions and true values\n",
    "\n",
    "for i in range(10):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # Plot the true value\n",
    "    plt.subplot(1, 3, 1)\n",
    "    mask_overlay = np.where(y_true[i, :, :, 1], y_true[i, :, :, 0], np.nan)\n",
    "    plt.imshow(mask_overlay, cmap='jet', vmin=-1, vmax=1)\n",
    "    plt.title(\"y_0 (Ground Truth)\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot the prediction with the land mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    masked_prediction = np.where(italy_mask, predictions[i, :, :, 0], np.nan)\n",
    "    plt.imshow(masked_prediction, cmap='jet', vmin=-1, vmax=1)\n",
    "    plt.title(\"Prediction with Land Mask\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # # Plot the predicted 'pure' value \n",
    "    # plt.subplot(1, 3, 3)\n",
    "    # plt.imshow(predictions[i], cmap='jet', vmin=-1, vmax=1)\n",
    "    # plt.title(\"Unmasked prediction (DEBUG)\")\n",
    "    # plt.colorbar()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MSE for the predictions and the baseline\n",
    "\n",
    "batch_x, batch_y = next(test_gen)\n",
    "predictions = model.predict(batch_x)\n",
    "\n",
    "filter_mask = batch_y[..., 1].astype(bool)    # We filter out the land and cloud data\n",
    "\n",
    "\n",
    "# Calculate the MSE for the predictions and the baseline, and the average MSEs in that batch. Only consider the ocean data.\n",
    "mse_predictions = [mean_squared_error(batch_y[i, :, :, 0][filter_mask[i]], predictions[i, :, :, 0][filter_mask[i]]) for i in range(batch_size)]\n",
    "mse_baseline = [mean_squared_error(batch_y[i, :, :, 0][filter_mask[i]], batch_x[i, :, :, 3][filter_mask[i]]) for i in range(batch_size)]\n",
    "print('MSE for predictions:', mse_predictions)\n",
    "print('MSE for baseline:', mse_baseline)\n",
    "\n",
    "avg_mse_predictions = np.mean(mse_predictions)\n",
    "avg_mse_baseline = np.mean(mse_baseline)\n",
    "print('Average MSE for predictions:', avg_mse_predictions)\n",
    "print('Average MSE for baseline:', avg_mse_baseline)\n",
    "\n",
    "# Plot the MSEs\n",
    "indices = np.arange(batch_size + 1)   # Add 1 to the batch size to include the average MSEs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(indices[:-1] - 0.2, mse_predictions, width=0.4, label='Predictions')\n",
    "ax.bar(indices[:-1] + 0.2, mse_baseline, width=0.4, label='Baseline')\n",
    "ax.bar(batch_size - 0.2, avg_mse_predictions, width=0.4, color='blue', label='Avg of Predictions')\n",
    "ax.bar(batch_size + 0.2, avg_mse_baseline, width=0.4, color='red', label='Avg of Baseline')\n",
    "\n",
    "ax.set_xlabel('Batch Index')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('MSE for Predictions and Baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "#fig.savefig(\"mseGraph.png\")\n",
    "\n",
    "\n",
    "# Check if any prediction has an MSE greater than the baseline\n",
    "for i in range(batch_size):\n",
    "    if mse_predictions[i] > mse_baseline[i]:\n",
    "        # Plot the prediction and ground truth\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        mask_overlay = np.where(batch_y[i, :, :, 1], batch_y[i, :, :, 0], np.nan)\n",
    "        plt.imshow(mask_overlay, cmap='jet')\n",
    "        plt.title(f\"Ground Truth {i}\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        italy_overlay = np.where(italy_mask, predictions[i, :, :, 0], np.nan)\n",
    "        plt.imshow(italy_overlay, cmap='jet')\n",
    "        plt.title(f\"Prediction {i}\")\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
