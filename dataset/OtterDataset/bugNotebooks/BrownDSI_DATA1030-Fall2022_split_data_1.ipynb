{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudcard\n",
    "- **I find it tricky to determine the appropriate number of bins when plotting histograms.**\n",
    "    - it is tricky. Try a couple of options and see what works best. This is as much of art as science.\n",
    "- **when to split continuous variables and how to determine cut off points.**\n",
    "    - why would you want to split continuous variables?\n",
    "- **When should we convert a continuous variable to a categorical one? How are the cutoffs between the categories decided (e.g. <= 50k and > 50k in the gross income example)?**\n",
    "    - honestly, I don't know how that cutoff was determined.\n",
    "    - maybe 50k was the top 10% earner cutoff in 1990?\n",
    "- **What packages would you recommend for visualizing 3D plots? Or is it not advised to plot data at that dimensionality?**\n",
    "    - I fiercely HATE 3D plots so I dont have recommendations :)\n",
    "    - they are clanky and difficult to make sense of at a glance\n",
    "    - this is just my subjective opinion though, you won't lose points if you create 3D plots in a problem set of report\n",
    "- **If you have one categorical and one continuous variable, there are multiple suggestions to use: \"category-specific histograms, box plot, violin plot\", so which among these should be chosen?**\n",
    "    - your personal choice\n",
    "    - there is no right or wrong choice here, go with the one you like the most\n",
    "- **During the exploratory data analysis process, is it best to begin with a scatter matrix and then conduct other visualizations (histogram, boxplot, violin plot, etc.)? Or should one begin with the \"simpler\" plots first?**\n",
    "    - the scatter matrix is a good start to make sense of the numerical features but you need other plots for categorical and ordinal features\n",
    "- **for two categorical axises, isn't stack bar plot the same as the violin plot?**\n",
    "    - nope, the violin plot shows the distrubtion of continuous features\n",
    "    - categorical features have no distributions\n",
    "- **I'm unclear about the difference between violin plots and bar plots.**\n",
    "    - bar plots show summary stats of the continuous feature like the mean, +/- 1 standard deviation, 1 percentlie and 99 percentile\n",
    "    - on the other hand, the violin plot shows the distribution of the continuous feature\n",
    "- **I am not sure why we are setting bins= the squared root of n.**\n",
    "    - that's just a rule of thumb\n",
    "    - for distributions that are close to gaussian, bins = sqrt(n) gives a good histogram\n",
    "- **muddiest part of the lecture is interpreting the scatter matrix that holds 36 different visualizations. Does only compare the 2 axis per visualization or does it show a relationship among all visualizations?**\n",
    "   - it contains one scatter plot for each possible feature pair\n",
    "- **How to figure out those code in a quick way?**\n",
    "    - I don't know but if you figure out, let me know :)\n",
    "    - I don't think there is a quick way here, you need to put in the work and the hours\n",
    "- **I didn't understand what a logspace normal distribution meant**\n",
    "    - print out that part of the code and read the manual of np.logspace\n",
    "- **is it necessary to visualize every column/feature before conducting an analysis? What if there are many columns?**\n",
    "    - it is highly recommended\n",
    "    - if there are many columns, automate the plotting process\n",
    "    - loop through each column to prepare plots\n",
    "- **How did we use plt.semilogx() today? What is it helpful for in the context of the exercise that we have seen in class?**\n",
    "- **I was a bit confused on the log scale, how will we truly know when it is appropriate in practice?**\n",
    "- **How do you know when to use transformations like a log scale on your data?**\n",
    "    - semilogx is helpful if a quantity varies over several orders of magnitudes like the capital gain which can be anything from a few dollars to 100k\n",
    "- **How do you convert a column from 'object' to a specific datatype?**\n",
    "    - check on stackoverflow or in the pandas manual\n",
    "- **If I want to look at two categorical variables (similar to the stacked barplot data with 'race' and 'gross-income') but I have rows where I know the race but the gross income is unknown, should the data be normalized to exclude rows with the missing data or should the bar graph have a separate stack for unknown income?**\n",
    "    - I'd add a separate unknown income stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Split iid data\n",
    "By the end of this lecture, you will be able to\n",
    "- apply basic split to iid datasets\n",
    "- apply k-fold split to iid datasets\n",
    "- apply stratified splits to imbalanced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The supervised ML pipeline\n",
    "The goal: Use the training data (X and y) to develop a <font color='red'>model</font> which can <font color='red'>accurately</font> predict the target variable (y_new') for previously unseen data (X_new).\n",
    "\n",
    "**1. Exploratory Data Analysis (EDA)**: you need to understand your data and verify that it doesn't contain errors\n",
    "   - do as much EDA as you can!\n",
    "    \n",
    "<span style=\"background-color: #FFFF00\">**2. Split the data into different sets**: most often the sets are train, validation, and test (or holdout)</span>\n",
    "   - practitioners often make errors in this step!\n",
    "   - you can split the data randomly, based on groups, based on time, or any other non-standard way if necessary to answer your ML question\n",
    "\n",
    "**3. Preprocess the data**: ML models only work if X and Y are numbers! Some ML models additionally require each feature to have 0 mean and 1 standard deviation (standardized features)\n",
    "   - often the original features you get contain strings (for example a gender feature would contain 'male', 'female', 'non-binary', 'unknown') which needs to transformed into numbers\n",
    "   - often the features are not standardized (e.g., age is between 0 and 100) but it needs to be standardized\n",
    "    \n",
    "**4. Choose an evaluation metric**: depends on the priorities of the stakeholders\n",
    "   - often requires quite a bit of thinking and ethical considerations\n",
    "     \n",
    "**5. Choose one or more ML techniques**: it is highly recommended that you try multiple models\n",
    "   - start with simple models like linear or logistic regression\n",
    "   - try also more complex models like nearest neighbors, support vector machines, random forest, etc.\n",
    "    \n",
    "**6. Tune the hyperparameters of your ML models (aka cross-validation)**\n",
    "   - ML techniques have hyperparameters that you need to optimize to achieve best performance\n",
    "   - for each ML model, decide which parameters to tune and what values to try\n",
    "   - loop through each parameter combination\n",
    "       - train one model for each parameter combination\n",
    "       - evaluate how well the model performs on the validation set\n",
    "   - take the parameter combo that gives the best validation score\n",
    "   - evaluate that model on the test set to report how well the model is expected to perform on previously unseen data\n",
    "    \n",
    "**7. Interpret your model**: black boxes are often not useful\n",
    "   - check if your model uses features that make sense (excellent tool for debugging)\n",
    "   - often model predictions are not enough, you need to be able to explain how the model arrived to a particular prediction (e.g., in health care)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we split the data?\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit models to training data\n",
    "   - evaluate each model on validation set\n",
    "   - we find hyper-parameter values that optimize the validation score\n",
    "- we want to know how the model will perform on previously unseen data\n",
    "   - apply our final model on the test set\n",
    "   \n",
    "### We need to split the data into three parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap from the second lecture\n",
    "\n",
    "- **the learner's input**\n",
    "    - Domain set $\\mathcal{X}$ - a set of objects we wish to label.\n",
    "    - Label set $\\mathcal{Y}$ - a set of possible labels. \n",
    "    - Training data $S = ((x_1, y_1),...,(x_m,y_m))$ - a finite sequence of pairs from $\\mathcal{X}$, $\\mathcal{Y}$. This is what the learner has access to.\n",
    "        - $X = (x_1,...,x_m)$ is the feature matrix which is usually a 2D matrix, and $Y = (y_1,...,y_m)$ is the target variable which is a vector.\n",
    "        \n",
    "- let's denote the probability distribution over $\\mathcal{X}$ by $D$.\n",
    "- let's assume there is some correct labeling function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$. \n",
    "- **a training example is then generated by sampling $x_i$ from $D$, and the label $y_i$ is generated using $f$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.I.D. assumption\n",
    "\n",
    "- **the i.i.d. assumption**: the examples in the training set are independently and identically distributed according to $D$\n",
    "    - every $x_i$ is freshly sampled from $D$ and then labelled by $f$\n",
    "    - that is, $x_i$ and $y_i$ are picked independently of the other instances\n",
    "    - $S$ is a window through which the learner gets partial info about $D$ and the labeling function $f$\n",
    "    - the larger the sample gets, the more likely it is to reflect more accurately $D$ and $f$\n",
    "    \n",
    "\n",
    "- examples of not iid data:\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- **apply basic split to iid datasets**\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- 60% train, 20% validation, 20% test for small datasets\n",
    "- 98% train, 1% validation, 1% test for large datasets\n",
    "    - if you have 1 million points, you still have 10000 points in validation and test which is plenty to assess model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's work with the adult data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         <=50K.\n",
      "1         <=50K.\n",
      "2          >50K.\n",
      "3          >50K.\n",
      "4         <=50K.\n",
      "          ...   \n",
      "16276     <=50K.\n",
      "16277     <=50K.\n",
      "16278     <=50K.\n",
      "16279     <=50K.\n",
      "16280      >50K.\n",
      "Name: gross-income, Length: 16281, dtype: object\n",
      "   age   workclass  fnlwgt      education  education-num       marital-status  \\\n",
      "0   25     Private  226802           11th              7        Never-married   \n",
      "1   38     Private   89814        HS-grad              9   Married-civ-spouse   \n",
      "2   28   Local-gov  336951     Assoc-acdm             12   Married-civ-spouse   \n",
      "3   44     Private  160323   Some-college             10   Married-civ-spouse   \n",
      "4   18           ?  103497   Some-college             10        Never-married   \n",
      "\n",
      "           occupation relationship    race      sex  capital-gain  \\\n",
      "0   Machine-op-inspct    Own-child   Black     Male             0   \n",
      "1     Farming-fishing      Husband   White     Male             0   \n",
      "2     Protective-serv      Husband   White     Male             0   \n",
      "3   Machine-op-inspct      Husband   Black     Male          7688   \n",
      "4                   ?    Own-child   White   Female             0   \n",
      "\n",
      "   capital-loss  hours-per-week  native-country  \n",
      "0             0              40   United-States  \n",
      "1             0              50   United-States  \n",
      "2             0              40   United-States  \n",
      "3             0              40   United-States  \n",
      "4             0              30   United-States  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('data/adult_test.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "print(y)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (9768, 14) (9768,)\n",
      "(6513, 14) (6513,)\n",
      "validation set: (3256, 14) (3256,)\n",
      "test set: (3257, 14) (3257,)\n",
      "       age          workclass  fnlwgt      education  education-num  \\\n",
      "4050    22            Private  335950        HS-grad              9   \n",
      "11446   29            Private   78261        HS-grad              9   \n",
      "12427   74   Self-emp-not-inc  160009     Assoc-acdm             12   \n",
      "5702    39       Self-emp-inc   31709   Some-college             10   \n",
      "13058   50            Private  144084        HS-grad              9   \n",
      "\n",
      "            marital-status        occupation    relationship    race      sex  \\\n",
      "4050         Never-married     Other-service   Not-in-family   Black     Male   \n",
      "11446            Separated   Protective-serv   Not-in-family   White     Male   \n",
      "12427   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
      "5702    Married-civ-spouse      Adm-clerical            Wife   White   Female   \n",
      "13058            Separated             Sales       Unmarried   White   Female   \n",
      "\n",
      "       capital-gain  capital-loss  hours-per-week  native-country  \n",
      "4050              0             0              70   United-States  \n",
      "11446             0             0              55   United-States  \n",
      "12427             0             0              30   United-States  \n",
      "5702              0             0              20   United-States  \n",
      "13058             0             0              55   United-States  \n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,\\\n",
    "                    train_size = 0.6,random_state = random_state)\n",
    "print('training set:',X_train.shape, y_train.shape) # 60% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 40% of points are in other\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,\\\n",
    "                    train_size = 0.5,random_state = random_state)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 20% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 20% of points are in test\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness due to splitting\n",
    "- the model performance, validation and test scores will change depending on which points are in train, val, test\n",
    "    - inherent randomness or uncertainty of the ML pipeline\n",
    "- change the random state a couple of times and repeat the whole ML pipeline to assess how much the random splitting affects your test score\n",
    "    - you would expect a similar uncertainty when the model is deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1\n",
    "\n",
    "What's the second train_test_split line if you want to end up with 60-20-20 in train-val-test? Print out the sizes of X_train, X_val, X_test to verify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other, X_test, y_other, y_test = train_test_split(X,y,\\\n",
    "                    train_size = 0.8,random_state=random_state)\n",
    "# add your line below and chose the correct solution from canvas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- **apply k-fold split to iid datasets**\n",
    "- <font color='lightgray'>apply stratified splits to imbalanced data</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other splitting strategy for iid data: k-fold splitting\n",
    "\n",
    "<center><img src=\"figures/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |  \n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_fold>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |  \n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold. Otherwise, this\n",
      " |      parameter has no effect.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting `random_state`\n",
      " |  to an integer.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  StratifiedKFold : Takes group information into account to avoid building\n",
      " |      folds with imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold : Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,), default=None\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like of shape (n_samples,), default=None\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13024, 14) (13024,)\n",
      "test set: (3257, 14) (3257,)\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age          workclass      education\n",
      "9850    59            Private   Some-college\n",
      "103     58   Self-emp-not-inc            9th\n",
      "1383    45            Private        HS-grad\n",
      "11034   49   Self-emp-not-inc      Bachelors\n",
      "14876   59   Self-emp-not-inc      Bachelors\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age     workclass      education\n",
      "13384   60   Federal-gov      Bachelors\n",
      "8471    20       Private        HS-grad\n",
      "13406   21             ?   Some-college\n",
      "13394   35       Private        HS-grad\n",
      "15123   38       Private   Some-college\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age     workclass      education\n",
      "647     60             ?      Bachelors\n",
      "9314    26       Private   Some-college\n",
      "14499   52       Private        HS-grad\n",
      "7332    53   Federal-gov     Assoc-acdm\n",
      "12523   21       Private           10th\n",
      "   training set: (10419, 14) (10419,)\n",
      "   validation set: (2605, 14) (2605,)\n",
      "       age workclass      education\n",
      "5294    53   Private        HS-grad\n",
      "3481    41   Private        HS-grad\n",
      "7671    49   Private   Some-college\n",
      "11055   39   Private      Bachelors\n",
      "12751   18         ?           12th\n",
      "   training set: (10420, 14) (10420,)\n",
      "   validation set: (2604, 14) (2604,)\n",
      "       age      workclass     education\n",
      "4265    23              ?          10th\n",
      "5290    23        Private       HS-grad\n",
      "1157    56   Self-emp-inc   Prof-school\n",
      "12344   18        Private          11th\n",
      "13683   55        Private       HS-grad\n"
     ]
    }
   ],
   "source": [
    "random_state =42\n",
    "\n",
    "# first split to separate out the test set\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print(X_other.shape,y_other.shape)\n",
    "print('test set:',X_test.shape,y_test.shape)\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('   training set:',X_train.shape, y_train.shape) \n",
    "    print('   validation set:',X_val.shape, y_val.shape) \n",
    "    # the validation set contains different points in each iteration\n",
    "    print(X_val[['age','workclass','education']].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many splits should I create?\n",
    "- tough question, 3-5 is most common\n",
    "- if you do n splits, n models will be trained, so the larger the n, the most computationally intensive it will be to train the models\n",
    "- KFold is usually better suited to small datasets\n",
    "- KFold is good to estimate uncertainty due to random splitting of train and val, but it is not perfect\n",
    "    - the test set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why shuffling iid data is important?\n",
    "- by default, data is not shuffled by Kfold which can introduce errors!\n",
    "<center><img src=\"figures/kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quiz 2\n",
    "Given the labels below, what are the balances of each class?\n",
    "\n",
    "y = [0,0,0,2,2,0,0,2,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='lightgray'>Split iid data</font>\n",
    "<font color='lightgray'>By the end of this lecture, you will be able to</font>\n",
    "- <font color='lightgray'>apply basic split to iid datasets</font>\n",
    "- <font color='lightgray'>apply k-fold split to iid datasets</font>\n",
    "- **apply stratified splits to imbalanced data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data\n",
    "- imbalanced data: only a small fraction of the points are in one of the classes, usually ~5% or less but there is no hard limit here\n",
    "- examples:\n",
    "    - people visit a bank's website. do they sign up for a new credit card?\n",
    "        - most customers just browse and leave the page\n",
    "        - usually 1% or less of the customers get a credit card (class 1), the rest leaves the page without signing up (class 0).\n",
    "    - fraud detection\n",
    "        - only a tiny fraction of credit card payments are fraudulent\n",
    "    - rare disease diagnosis\n",
    "- the issue with imbalanced data:\n",
    "    - if you apply train_test_split or KFold, you might not have class 1 points in one of your sets by chance\n",
    "    - this is what we need to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: stratified splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    990\n",
      "1     10\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/imbalanced_data.csv')\n",
    "\n",
    "X = df[['feature1','feature2']]\n",
    "y = df['y']\n",
    "\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**balance without stratification:**\n",
      "(array([0, 1]), array([591,   9]))\n",
      "(array([0]), array([200]))\n",
      "(array([0, 1]), array([199,   1]))\n",
      "**balance with stratification:**\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "(array([0, 1]), array([198,   2]))\n"
     ]
    }
   ],
   "source": [
    "# 4 and 10\n",
    "\n",
    "random_state = 4\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('**balance without stratification:**')\n",
    "# a variation on the order of 1% which would be too much for imbalanced data!\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "print(np.unique(y_val,return_counts=True))\n",
    "print(np.unique(y_test,return_counts=True))\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "print('**balance with stratification:**')\n",
    "# very little variation (in the 4th decimal point only) which is important if the problem is imbalanced\n",
    "print(np.unique(y_train,return_counts=True))\n",
    "print(np.unique(y_val,return_counts=True))\n",
    "print(np.unique(y_test,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stratified folds\n",
    "<center><img src=\"figures/stratified_kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StratifiedKFold in module sklearn.model_selection._split:\n",
      "\n",
      "class StratifiedKFold(_BaseKFold)\n",
      " |  StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      " |  \n",
      " |  Stratified K-Folds cross-validator.\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets.\n",
      " |  \n",
      " |  This cross-validation object is a variation of KFold that returns\n",
      " |  stratified folds. The folds are made by preserving the percentage of\n",
      " |  samples for each class.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <stratified_k_fold>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |  \n",
      " |  shuffle : bool, default=False\n",
      " |      Whether to shuffle each class's samples before splitting into batches.\n",
      " |      Note that the samples within each split will not be shuffled.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      " |      indices, which controls the randomness of each fold for each class.\n",
      " |      Otherwise, leave `random_state` as `None`.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import StratifiedKFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([0, 0, 1, 1])\n",
      " |  >>> skf = StratifiedKFold(n_splits=2)\n",
      " |  >>> skf.get_n_splits(X, y)\n",
      " |  2\n",
      " |  >>> print(skf)\n",
      " |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in skf.split(X, y):\n",
      " |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [1 3] TEST: [0 2]\n",
      " |  TRAIN: [0 2] TEST: [1 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The implementation is designed to:\n",
      " |  \n",
      " |  * Generate test sets such that all contain the same distribution of\n",
      " |    classes, or as close as possible.\n",
      " |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      " |    ``y = [1, 0]`` should not change the indices generated.\n",
      " |  * Preserve order dependencies in the dataset ordering, when\n",
      " |    ``shuffle=False``: all samples from class k in some test set were\n",
      " |    contiguous in y, or separated in y by samples from classes other than k.\n",
      " |  * Generate test sets where the smallest and largest differ by at most one\n",
      " |    sample.\n",
      " |  \n",
      " |  .. versionchanged:: 0.22\n",
      " |      The previous implementation did not follow the last constraint.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StratifiedKFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |          Note that providing ``y`` is sufficient to generate the splits and\n",
      " |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      " |          ``X`` instead of actual training data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |          Stratification is done based on the y labels.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting `random_state`\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test balance: (array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([596,   4]))\n",
      "(array([0, 1]), array([196,   4]))\n",
      "new fold\n",
      "(array([0, 1]), array([593,   7]))\n",
      "(array([0, 1]), array([199,   1]))\n",
      "new fold\n",
      "(array([0, 1]), array([592,   8]))\n",
      "(array([0]), array([200]))\n",
      "new fold\n",
      "(array([0, 1]), array([595,   5]))\n",
      "(array([0, 1]), array([197,   3]))\n"
     ]
    }
   ],
   "source": [
    "# what we did before: variance in balance on the order of 1%\n",
    "random_state = 2\n",
    "\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print('test balance:',np.unique(y_test,return_counts=True))\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=4,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    print('new fold')\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print(np.unique(y_train,return_counts=True))\n",
    "    print(np.unique(y_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test balance: (array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n",
      "new fold\n",
      "(array([0, 1]), array([594,   6]))\n",
      "(array([0, 1]), array([198,   2]))\n"
     ]
    }
   ],
   "source": [
    "# stratified K Fold: variation in balance is very small (4th decimal point)\n",
    "random_state = 42\n",
    "\n",
    "# stratified train-test split\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "print('test balance:',np.unique(y_test,return_counts=True))\n",
    "\n",
    "# do StratifiedKFold split on other\n",
    "kf = StratifiedKFold(n_splits=4,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    print('new fold')\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print(np.unique(y_train,return_counts=True))\n",
    "    print(np.unique(y_val,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
