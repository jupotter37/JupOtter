{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aae7192-2ee4-4e00-af49-c9964275b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import string\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import bleu_score\n",
    "from rapidfuzz import fuzz\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c8242f5-1de7-4a1d-b99e-e69e99a57d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__\n",
    "def _normalize_(x: str, remove_latex_flag: bool = True) -> str:\n",
    "    REMOVE_PUNCT = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    if remove_latex_flag:\n",
    "        x = _remove_latex_(x)\n",
    "\n",
    "    x = x.translate(REMOVE_PUNCT)\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = x.lower()\n",
    "    x = x.strip()\n",
    "\n",
    "    return x\n",
    "\n",
    "def _extract_latex_(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    latex_pattern = re.compile(r'(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\)|\\\\begin\\{.*?\\}.*?\\\\end\\{.*?\\})', re.DOTALL)\n",
    "\n",
    "    latex_expressions = latex_pattern.findall(text)\n",
    "\n",
    "    stripped_text = latex_pattern.sub('', text).strip()\n",
    "\n",
    "    return latex_expressions\n",
    "\n",
    "def _remove_latex_(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    try:\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\$(.*?)\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\$\\$(.*?)\\$\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\\\\\[(.*?)\\\\\\]', r'\\1', text)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        text = re.sub(r'\\{|\\}', '', text)\n",
    "    except:\n",
    "        text = ''\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4536113-bf9f-4025-a102-1945987df607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize()\n",
    "def safe_word_tokenize(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def parallel_tokenize(series):\n",
    "    \"\"\"\n",
    "    Tokenize a Dask Series in parallel.\n",
    "    \"\"\"\n",
    "    return series.map(safe_word_tokenize, meta=('token', 'object'))\n",
    "\n",
    "def extract_parts(token_list):\n",
    "    \"\"\"\n",
    "    Extract sublist (of tokens) post tokenization of the text column.\n",
    "    \"\"\"\n",
    "    length = len(token_list)\n",
    "    first_10 = token_list[:max(1, length // 10)]\n",
    "    mid_start = length // 2 - max(1, length // 20)\n",
    "    mid_end = length // 2 + max(1, length // 20)\n",
    "    middle_10 = token_list[mid_start:mid_end]\n",
    "    last_10 = token_list[-max(1, length // 10):]\n",
    "    \n",
    "    return first_10, middle_10, last_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcedeb8a-e1d8-4b5c-b1db-fb8fecdbcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_metrics\n",
    "# - BLEU\n",
    "def compute_bleu(row, reference_col, candidate_col):\n",
    "    return bleu_score.sentence_bleu(\n",
    "        [row[reference_col]], \n",
    "        row[candidate_col], \n",
    "        smoothing_function=bleu_score.SmoothingFunction().method1\n",
    "    )\n",
    "\n",
    "def parallel_bleu(df, reference_col, candidate_col):\n",
    "    return df.apply(lambda row: compute_bleu(row, reference_col, candidate_col), axis=1, meta=('x', 'f8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9cc9ce1-b797-4d34-a46d-9c626574a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - CAR (Character Accuracy Rate)\n",
    "def compute_approx_car(row, reference_col, candidate_col):\n",
    "    '''\n",
    "    Complement of character error rate (CER); hence: character accuracy rate (CAR)\n",
    "    '''\n",
    "    similarity = fuzz.ratio(row[reference_col], row[candidate_col])\n",
    "    return similarity / 100.0\n",
    "\n",
    "# Compute CAR scores in parallel for the entire dataframe\n",
    "def parallel_car(df, reference_col, candidate_col):\n",
    "    return df.apply(lambda row: compute_approx_car(row, reference_col, candidate_col), axis=1, meta=('car_score', 'f8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c893913c-d3ed-4d45-869c-48db39810f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ROUGE-1\n",
    "def compute_rouge1(row, reference_col, candidate_col, scorer):\n",
    "    reference_text = str(row[reference_col]) if pd.notnull(row[reference_col]) else \"\"\n",
    "    candidate_text = str(row[candidate_col]) if pd.notnull(row[candidate_col]) else \"\"\n",
    "    score = scorer.score(reference_text, candidate_text)\n",
    "    return score['rougeL'].fmeasure\n",
    "\n",
    "def parallel_rouge1(df, reference_col, candidate_col, scorer):\n",
    "    return df.apply(lambda row: compute_rouge1(row, reference_col, candidate_col, scorer), axis=1, meta=('x', 'f8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3265e20d-d3b1-46da-a39b-cf086183fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaskResponseTable:\n",
    "    def __init__(self,\n",
    "                 db_src_filename: str,\n",
    "                 db_dst_filename: str,\n",
    "                 chunk_index: int = -1,\n",
    "                 chunk_size: int = -1,\n",
    "                 num_cores: int = 100,\n",
    "                 overwrite_flag: bool = False,\n",
    "                 parser_columns: list[str] = ['pymupdf', 'nougat', 'grobid'],\n",
    "                 root_dir: Path = Path('/lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/database')) -> None:\n",
    "        \"\"\"\n",
    "        Assume `html` is ground truth text\n",
    "        \"\"\"\n",
    "\n",
    "        max_int = sys.maxsize\n",
    "        while True:\n",
    "            try:\n",
    "                csv.field_size_limit(max_int)\n",
    "                break\n",
    "            except OverflowError:\n",
    "                max_int = int(max_int / 10)\n",
    "\n",
    "        # Validate root directory\n",
    "        root_dir = Path(root_dir)\n",
    "        assert root_dir.is_dir(), f\"Path `root_dir` does not exist or is not directory. Invalid directory: {root_dir}\"\n",
    "        \n",
    "        # Paths\n",
    "        self.db_src_path = Path(root_dir) / db_src_filename\n",
    "        self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "        self.overwrite_flag = overwrite_flag\n",
    "        self.parser_columns = parser_columns\n",
    "        self.chunk_index = chunk_index\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_cores = num_cores\n",
    "        \n",
    "        # Validate file paths\n",
    "        assert self.db_src_path.is_file(), f\"Source CSV path invalid. No such path: {self.db_src_path}\"\n",
    "        if not self.overwrite_flag:\n",
    "            assert not self.db_dst_path.is_file(), f\"Destination Path invalid. File already exists at path: {self.db_dst_path}\"\n",
    "        \n",
    "        # Load DataFrame with Dask\n",
    "        self.df = dd.read_csv(self.db_src_path, sep='|', sample=5000000, sample_rows=10, on_bad_lines='skip', engine='python')\n",
    "\n",
    "        # fill NaNs\n",
    "        for col in self.df.columns:\n",
    "            if self.df[col].dtype == 'object' and col not in {'html', 'path'}:\n",
    "                self.df[col] = self.df[col].fillna('')\n",
    "        \n",
    "        # Drop NA rows\n",
    "        df_proc = self.df.dropna()\n",
    "\n",
    "        # Subset DataFrame\n",
    "        if self.chunk_index != -1 and self.chunk_size != -1:\n",
    "            # Check validity \n",
    "            assert self.chunk_index >= 0, f\"`chunk_index` should be non-negative (or -1) but is {self.chunk_index}\"\n",
    "            assert self.chunk_size > 0, f\"`chunk_size` should be positive (or -1) but is {self.chunk_size}\"\n",
    "            \n",
    "            # Overwrite self.db_dst_path\n",
    "            db_dst_filename = Path(db_dst_filename).stem + f'_{self.chunk_index}-{len(self.df) // self.chunk_size}' + Path(db_dst_filename).suffix\n",
    "            self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "            \n",
    "            # Subset DataFrame\n",
    "            i_start, i_end = self.chunk_index * self.chunk_size, min((self.chunk_index + 1) * self.chunk_size, len(self.df))\n",
    "            if i_start >= len(self.df):\n",
    "                raise ValueError(f'i_start index exceeds length of Dataframe: i_start={i_start} for len(df_proc)={len(df_proc)}')\n",
    "            self.df = self.df.loc[i_start:i_end]\n",
    "\n",
    "            # DEBUG\n",
    "            print(f'len(df_proc): {len(self.df)}, i_start/i_end: ', i_start, i_end)\n",
    "\n",
    "        # Status\n",
    "        print(f'DF loaded, nrows : {len(self.df)} after removing NANs & subsetting')\n",
    "\n",
    "        # Normalized text columns\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            self.df[f'{parser_col}_norm'] = self.df.apply(lambda row: _normalize_(row[parser_col]), axis=1, meta=('norm', 'object'))\n",
    "\n",
    "        # LaTeX text columns\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            self.df[f'{parser_col}_latex'] = self.df.apply(lambda row: _extract_latex_(row[parser_col]), axis=1, meta=('latex', 'object'))\n",
    "\n",
    "        # Initialize Dask client for parallel processing\n",
    "        self.client = Client(n_workers=self.num_cores)\n",
    "\n",
    "    def tokenize(self,):\n",
    "        \"\"\"\n",
    "        Tokenize html/parser text columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        # This works great!\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            print(f'Tokenizing {parser_col} ... ')\n",
    "            self.df[f'{parser_col}_token'] = parallel_tokenize(self.df[parser_col])\n",
    "            self.df[f'{parser_col}_norm_token'] = parallel_tokenize(self.df[f'{parser_col}_norm'])\n",
    "            print('... completed!\\n')\n",
    "\n",
    "        # Extract and assign beginning, middle, and end parts\n",
    "        def extract_and_assign(series, col_prefix):\n",
    "            length = series.map(len, meta=('length', 'int'))\n",
    "            self.df[f'{col_prefix}Beg_token'] = series.map(lambda x: x[:max(1, len(x) // 10)], meta=('token', 'object'))\n",
    "            self.df[f'{col_prefix}Mid_token'] = series.map(lambda x: x[len(x) // 2 - max(1, len(x) // 20):len(x) // 2 + max(1, len(x) // 20)], meta=('token', 'object'))\n",
    "            self.df[f'{col_prefix}End_token'] = series.map(lambda x: x[-max(1, len(x) // 10):], meta=('token', 'object'))\n",
    "    \n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            extract_and_assign(self.df[f'{parser_col}_token'], parser_col)\n",
    "            extract_and_assign(self.df[f'{parser_col}_norm_token'], f'{parser_col}_norm')\n",
    "    \n",
    "        pass\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute metrics BLEU, ROUGE, METEOR, CAR etc.\n",
    "        \"\"\"\n",
    "\n",
    "        # BLEU\n",
    "        if True:\n",
    "            for parser_col in self.parser_columns:\n",
    "                print(f'Computing BLEU for {parser_col} ... ')\n",
    "                self.df[f'{parser_col}_bleu'] = parallel_bleu(self.df, f'{parser_col}_token', 'html_token')\n",
    "                self.df[f'{parser_col}_norm_bleu'] = parallel_bleu(self.df, f'{parser_col}_norm_token', 'html_norm_token')\n",
    "                print(f'... completed!\\n')\n",
    "\n",
    "        # CAR\n",
    "        if True:\n",
    "            for parser_col in self.parser_columns:\n",
    "                print(f'Computing CAR for {parser_col} ... ')\n",
    "                self.df[f'{parser_col}_car'] = parallel_car(self.df, f'{parser_col}_token', 'html_token')\n",
    "                self.df[f'{parser_col}_norm_car'] = parallel_car(self.df, f'{parser_col}_norm_token', 'html_norm_token')\n",
    "                print(f'... completed!\\n')\n",
    "\n",
    "        # ROUGE-2\n",
    "        if True:\n",
    "            scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "            for parser_col in self.parser_columns:\n",
    "                print(f'Computing ROUGE2 for {parser_col} ... ')\n",
    "                self.df[f'{parser_col}_rougeL'] = parallel_rouge1(self.df, f'{parser_col}', 'html', scorer)\n",
    "                self.df[f'{parser_col}_norm_rougeL'] = parallel_rouge1(self.df, f'{parser_col}_norm', 'html_norm', scorer)\n",
    "                print(f'... completed!\\n')\n",
    "\n",
    "        pass\n",
    "\n",
    "    def save(self, output_filename='out_metrics.csv'):\n",
    "        \"\"\"\n",
    "        Save the computed metrics DataFrame to a CSV file with '|' as the separator.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df.to_csv(output_filename, sep='|', index=False)\n",
    "        print(f\"Metrics saved to {output_filename}.\")\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087699f9-4edc-4fe7-9ce2-61255376d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DaskResponseTable(root_dir='/lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/database',\n",
    "                      chunk_index=0,\n",
    "                      chunk_size=4564,\n",
    "                      num_cores=8,\n",
    "                      db_src_filename='parser_output_raw.csv', \n",
    "                      db_dst_filename='out_df_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85cef1-4ef2-4ba4-b936-5c775846916e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5fd48-a829-462a-992a-344ed003b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024fbc1-faf0-43c4-b192-ad5602fbd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works\n",
    "#pandas_df = t.df.compute()\n",
    "#pandas_df.head()\n",
    "\n",
    "t.tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb95f29-e00e-49c3-9b6a-1444272f9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "t.compute_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f7647-2490-450c-ba1c-243cd72f8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start: ', time.time())\n",
    "t.save('./tmp/random.csv')\n",
    "print('end: ', time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f21906-8f9e-4333-b6f2-575601337332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 cores, tokenization only\n",
    "#1725230038.5646489 - 1725229906.095144\n",
    "# 8 cores, tokenization only\n",
    "#1725230700.3915436 - 1725230568.392145\n",
    "# 8 cores with CAR only\n",
    "#1725231052.6019797 - 1725230791.3853154\n",
    "# 8 cores with BLEU only\n",
    "#1725231464.9768543 - 1725231148.9887695\n",
    "# 8 cores with ROUGE2 only BUT ONLY 8 ROWS!\n",
    "#1725238226.0218575 - 1725238104.1046085\n",
    "# 8 cores with ROUGE2 (proper)\n",
    "#1725239619.280274 - 1725239053.972454\n",
    "# 8 cores with ROUGE1 (proper) -> 565\n",
    "#1725243417.534024 - 1725242852.1266394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fa4a1-11e5-4dfb-8631-bc6b802523d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METEOR\n",
    "def compute_meteor(row, reference_col, candidate_col):\n",
    "    return translate.meteor_score.meteor_score(\n",
    "        [row[reference_col]], \n",
    "        row[candidate_col]\n",
    "    )\n",
    "\n",
    "def parallel_meteor(df, reference_col, candidate_col):\n",
    "    return df.apply(lambda row: self.compute_meteor(row, reference_col, candidate_col), axis=1, meta=('x', 'f8'))\n",
    "\n",
    "# CAR (Character Accuracy Rate)\n",
    "def compute_approx_car(row, reference_col, candidate_col):\n",
    "    '''\n",
    "    Complement of character error rate (CER); hence: character accuracy rate (CAR)\n",
    "    '''\n",
    "    similarity = fuzz.ratio(row[reference_col], row[candidate_col])\n",
    "    return similarity / 100.0\n",
    "\n",
    "def parallel_car(df, reference_col, candidate_col):\n",
    "    return df.apply(lambda row: compute_approx_car(row, reference_col, candidate_col), axis=1, meta=('x', 'f8'))\n",
    "\n",
    "# ROUGE scores\n",
    "def compute_rouge(row, reference_col, candidate_col, scorer):\n",
    "    scores = scorer.score(row[reference_col], row[candidate_col])\n",
    "    return [scores['rouge1'].fmeasure, scores['rouge2'].fmeasure, scores['rougeL'].fmeasure]\n",
    "\n",
    "def parallel_rouge(df, reference_col, candidate_col, scorer):\n",
    "    return df.apply(lambda row: compute_rouge(row, reference_col, candidate_col, scorer), axis=1, meta=('x', 'f8'))\n",
    "\n",
    "\n",
    "class FastResponseTable:\n",
    "    def __init__(self,\n",
    "                 db_src_filename:str,\n",
    "                 db_dst_filename:str,\n",
    "                 chunk_index:int=-1,\n",
    "                 chunk_size:int=-1,\n",
    "                 num_cores:int=100,\n",
    "                 overwrite_flag:bool = False,\n",
    "                 parser_columns:list[str]=['pymupdf', 'nougat', 'grobid'],\n",
    "                 root_dir:Path=Path('/lus/eagle/projects/argonne_tpc/siebenschuh/aurora_gpt/database'),\n",
    "                 n:int=-1) -> None:\n",
    "        \"\"\"\n",
    "        Assume `html` is groudntruth text\n",
    "        \"\"\"\n",
    "\n",
    "        # validate\n",
    "        root_dir = Path(root_dir)\n",
    "        assert root_dir.is_dir(), f\"Path`root_dir` does not exist or is not directory. Invalid directory: {root_dir}\"\n",
    "        \n",
    "        # paths\n",
    "        self.db_src_path = Path(root_dir) / db_src_filename\n",
    "        self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "        self.overwrite_flag = overwrite_flag\n",
    "        self.n = n\n",
    "        self.parser_columns = parser_columns\n",
    "        self.chunk_index = chunk_index\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_cores = num_cores\n",
    "        \n",
    "        # raw data\n",
    "        assert self.db_src_path.is_file(), f\"Source CSV path invalid. No such path: {self.db_src_path}\"\n",
    "        if not self.overwrite_flag:\n",
    "            assert not self.db_dst_path.is_file(), f\"Destination Path invalid. File already exists at path: {self.db_dst_path}\"\n",
    "        \n",
    "        # load df\n",
    "        df_raw = pd.read_csv(self.db_src_path, sep='|')\n",
    "        \n",
    "        # drop NA rows\n",
    "        df_proc = df_raw.dropna()\n",
    "\n",
    "        # subset DataFrame\n",
    "        if self.chunk_index != -1 and self.chunk_size != -1:\n",
    "            # check validity \n",
    "            assert self.chunk_index >= 0, f\"`chunk_index` should be non-negative (or -1) but is {chunk_index}\"\n",
    "            assert self.chunk_size > 0, f\"`chunk_size` should be positive (or -1) but is {chunk_size}\"\n",
    "            \n",
    "            # overwrite self.db_dst_path\n",
    "            db_dst_filename = Path(db_dst_filename).stem + f'_{self.chunk_index}-{len(df_proc) // self.chunk_size}' + Path(db_dst_filename).suffix\n",
    "            self.db_dst_path = Path(root_dir) / db_dst_filename\n",
    "            \n",
    "            # subset dataframe\n",
    "            i_start, i_end = chunk_index * chunk_size, min((chunk_index+1) * chunk_size, len(df_proc))\n",
    "            if i_start >= len(df_proc):\n",
    "                raise ValueError(f'i_start index exceeds length of Dataframe: i_start={i_start} for len(df_proc)={len(df_proc)}')\n",
    "            df_proc = df_proc.iloc[i_start:i_end]\n",
    "\n",
    "            #  DEBUG\n",
    "            print(f'len(df_proc): {len(df_proc)}, i_start/i_end: ', i_start, i_end)\n",
    "\n",
    "        # status\n",
    "        print(f'DF loaded...\\n{len(df_raw)} rows\\n... {len(df_proc)} after removing NANs & subsetting')\n",
    "        \n",
    "        # subset\n",
    "        if n > 0:\n",
    "            df_proc = df_proc.iloc[:round(n)]\n",
    "            print(f'n={n} ... Only use first n rows.')\n",
    "\n",
    "        # normalized text columns\n",
    "        df_proc['html_norm'] = df_proc.apply(lambda row: self.normalize(row['html']), axis=1)\n",
    "        for parser_col in self.parser_columns:\n",
    "            df_proc[f'{parser_col}_norm'] = df_proc.apply(lambda row: self.normalize(row[parser_col]), axis=1)\n",
    "\n",
    "        # latex text columns\n",
    "        df_proc['html_latex'] = df_proc.apply(lambda row: self.extract_latex(row['html']), axis=1)\n",
    "        for parser_col in self.parser_columns:\n",
    "            df_proc[f'{parser_col}_latex'] = df_proc.apply(lambda row: self.extract_latex(row[parser_col]), axis=1)\n",
    "            \n",
    "        # assign\n",
    "        self.df = df_proc\n",
    "        \n",
    "        # Convert to Dask DataFrame\n",
    "        self.df = dd.from_pandas(df_proc, npartitions=self.num_cores)\n",
    "\n",
    "        # Initialize Dask client for parallel processing\n",
    "        self.client = Client(n_workers=self.num_cores)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def extract_latex(self, text):\n",
    "        \"\"\"\n",
    "        Extracts LaTeX expressions from the input text and returns both the LaTeX expressions and the text with LaTeX stripped.\n",
    "    \n",
    "        Parameters:\n",
    "            text (str): The input string containing LaTeX expressions.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing two elements:\n",
    "                - A list of extracted LaTeX expressions.\n",
    "                - A string with the LaTeX expressions removed.\n",
    "        \"\"\"\n",
    "        # Regular expression to match LaTeX expressions\n",
    "        latex_pattern = re.compile(r'(\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.*?\\\\\\)|\\\\begin\\{.*?\\}.*?\\\\end\\{.*?\\})', re.DOTALL)\n",
    "    \n",
    "        # Extract all LaTeX expressions\n",
    "        latex_expressions = latex_pattern.findall(text)\n",
    "    \n",
    "        # Strip the LaTeX expressions from the text\n",
    "        stripped_text = latex_pattern.sub('', text).strip()\n",
    "    \n",
    "        return latex_expressions\n",
    "        \n",
    "    def remove_latex(self, text:str) -> str:\n",
    "        \"\"\"\n",
    "        Remove LaTeX formatting from a string\n",
    "        \"\"\"\n",
    "        # Remove LaTeX commands (e.g., \\textbf{...}, \\emph{...}, etc.)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{(.*?)\\}', r'\\1', text)\n",
    "        \n",
    "        # Remove inline math (e.g., $...$)\n",
    "        text = re.sub(r'\\$(.*?)\\$', r'\\1', text)\n",
    "        \n",
    "        # Remove display math (e.g., \\[...\\] or $$...$$)\n",
    "        text = re.sub(r'\\$\\$(.*?)\\$\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\\\\\[(.*?)\\\\\\]', r'\\1', text)\n",
    "        \n",
    "        # Remove other LaTeX-specific characters (e.g., \\, \\%, etc.)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        \n",
    "        # Remove braces and any content between them\n",
    "        text = re.sub(r'\\{|\\}', '', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize(self, x:str, remove_latex_flag:bool=True) -> str:\n",
    "        \"\"\"\n",
    "        Normalize the text\n",
    "        \"\"\"\n",
    "    \n",
    "        # const\n",
    "        REMOVE_PUNCT = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "        # remove latex\n",
    "        if remove_latex_flag:\n",
    "            x = self.remove_latex(x)\n",
    "    \n",
    "        # remove escape characters\n",
    "        x = x.translate(REMOVE_PUNCT)\n",
    "        x = re.sub(r\"\\s+\", \" \", x)\n",
    "        x = x.lower()\n",
    "        x = x.strip()\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def tokenize_columns(self):\n",
    "        \"\"\"\n",
    "        Tokenize html/parser text columns in dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        # get current df\n",
    "        df_proc = self.df\n",
    "\n",
    "        # Tokenize columns in parallel\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            print(f'Tokenizing {parser_col} ... ')\n",
    "            df_proc[f'{parser_col}_token'] = parallel_tokenize(df_proc[parser_col])\n",
    "            df_proc[f'{parser_col}_norm_token'] = parallel_tokenize(df_proc[f'{parser_col}_norm'])\n",
    "            print('... completed!\\n')\n",
    "\n",
    "        # append beginning/middle/end part\n",
    "        for parser_col in ['html'] + self.parser_columns:\n",
    "            f_proc[f'{parser_col}Beg_token'], df_proc[f'{parser_col}Mid_token'], df_proc[f'{parser_col}End_token'] = zip(*df_proc[f'{parser_col}_token'].map(self.extract_parts).compute())\n",
    "            df_proc[f'{parser_col}Beg_norm_token'], df_proc[f'{parser_col}Mid_norm_token'], df_proc[f'{parser_col}End_norm_token'] = zip(*df_proc[f'{parser_col}_norm_token'].map(self.extract_parts).compute())\n",
    "        \n",
    "        # re-assign\n",
    "        self.df = df_proc\n",
    "\n",
    "    def compute_metrics(self, normalized: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Processes the table in parallel using Dask\n",
    "        \"\"\"\n",
    "        \n",
    "        # Copy frame of processed columns\n",
    "        df_proc = self.df\n",
    "\n",
    "        # BLEU\n",
    "        print('BLEU ...')\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['', 'Beg', 'Mid', 'End']:\n",
    "                df_proc[f'{parser_col}{part}_bleu'] = self.parallel_bleu(df_proc, f'html{part}_token', f'{parser_col}_token')\n",
    "                df_proc[f'{parser_col}{part}_bleu_norm'] = self.parallel_bleu(df_proc, f'html{part}_norm_token', f'{parser_col}_norm_token')\n",
    "        print('...done')\n",
    "\n",
    "        # ROUGE\n",
    "        print('ROUGE ...')\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['']:\n",
    "                # raw text\n",
    "                rouge_scores = self.parallel_rouge(df_proc, f'html{part}', f'{parser_col}{part}', scorer)\n",
    "                df_proc[f'{parser_col}{part}_rouge1'], df_proc[f'{parser_col}{part}_rouge2'], df_proc[f'{parser_col}{part}_rougeL'] = zip(*rouge_scores.compute())\n",
    "\n",
    "                # normalized text\n",
    "                rouge_scores_norm = self.parallel_rouge(df_proc, f'html{part}_norm', f'{parser_col}{part}_norm', scorer)\n",
    "                df_proc[f'{parser_col}{part}_rouge1_norm'], df_proc[f'{parser_col}{part}_rouge2_norm'], df_proc[f'{parser_col}{part}_rougeL_norm'] = zip(*rouge_scores_norm.compute())\n",
    "        print('...done')\n",
    "\n",
    "        # CAR (Character Accuracy Rate)\n",
    "        print('CAR ...')\n",
    "        for parser_col in self.parser_columns:\n",
    "            print(f'   {parser_col}')\n",
    "            for part in ['', 'Beg', 'Mid', 'End']:\n",
    "                df_proc[f'{parser_col}{part}_car'] = self.parallel_car(df_proc, f'html{part}_token', f'{parser_col}{part}_token')\n",
    "                df_proc[f'{parser_col}{part}_car_norm'] = self.parallel_car(df_proc, f'html{part}_norm_token', f'{parser_col}{part}_norm_token')\n",
    "        print('...done')\n",
    "\n",
    "        # Assign the final DataFrame to self.df_metrics\n",
    "        self.df_metrics = df_proc.compute()\n",
    "\n",
    "    def save_table(self) -> None:\n",
    "        \"\"\"Store processed table\"\"\"\n",
    "        self.df_metrics.to_csv(self.db_dst_path, sep='|')\n",
    "\n",
    "    def load_table(self) -> None:\n",
    "        \"\"\"Load processed table\"\"\"\n",
    "        assert self.db_dst_path.is_file(), f\"File does not exist at path: {self.db_dst_path}\"\n",
    "        self.df_metrics = pd.read_csv(self.db_dst_path, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12964ad-7846-40a1-a54e-a09647e547b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = FastResponseTable(root_dir='./tmp',\n",
    "                      chunk_index=0,\n",
    "                      chunk_size=100,\n",
    "                      num_cores=100,\n",
    "                      db_src_filename='df_500.csv', \n",
    "                      db_dst_filename='out_df_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce045ee-413d-47cb-a9fe-fc89317bed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t.tokenize_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d8b56-8bab-4ade-9d90-aeb082778f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = t.df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7965a2-9563-4be2-b19b-3e024a63e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9cd7c-4ae1-4c3a-a2dd-ab24e6ba4220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
