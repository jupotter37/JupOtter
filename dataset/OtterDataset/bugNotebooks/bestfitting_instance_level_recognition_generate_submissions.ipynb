{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import argparse\n",
    "from torch.nn import DataParallel\n",
    "from importlib import import_module\n",
    "from argparse import Namespace\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from timeit import default_timer as timer\n",
    "try:\n",
    "  import silence_tensorflow.auto\n",
    "except ImportError:\n",
    "  pass\n",
    "from albumentations import Normalize\n",
    "import copy\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import gc\n",
    "import operator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from layers.normalization import L2N\n",
    "from torch.autograd import Variable\n",
    "from config.config import *\n",
    "from config.en_config import *\n",
    "from dataset.landmark_dataset import *\n",
    "from utilities.vectors_utils import *\n",
    "import struct\n",
    "import faiss\n",
    "import pydegensac\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "from sklearn.cluster import DBSCAN as dbscan\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.transform import AffineTransform\n",
    "from skimage.measure import ransac as _ransac\n",
    "from utilities.superpointglue_util import read_image as spg_read_image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Classification')\n",
    "parser.add_argument('-f', default='', type=str)\n",
    "parser.add_argument('--en_cfgs', type=str, default='en_m4_b7_b6_b5_r152_i800', help='')\n",
    "parser.add_argument('--module', '-m', type=str, default='efficientnet_gem_fc_face', help='model ')\n",
    "parser.add_argument('--model_name', type=str, default='class_efficientnet_b7_gem_fc_arcface2_1head', help='model name')\n",
    "parser.add_argument('--gpus', default='0', type=str, help='use gpu (default: None (use cpu))')\n",
    "parser.add_argument('--num_classes', default=81313, type=int, help='number of classes (default: 203094)')\n",
    "parser.add_argument('--in_channels', default=3, type=int, help='in channels (default: 3)')\n",
    "parser.add_argument('--img_size', default=800, type=int, help='image size (default: None)')\n",
    "parser.add_argument('--scale', default=None, type=str, help='scale (default: None)')\n",
    "parser.add_argument('--loss', default='SoftmaxLoss', type=str, help='loss function SoftmaxLoss')\n",
    "parser.add_argument('--scheduler', default='Adam', type=str, help='scheduler name')\n",
    "parser.add_argument('--out_dir', default='r101', type=str, help='output dir (default: None)')\n",
    "parser.add_argument('--kaggle', default=0, type=int, help='0:local 1:kaggle')\n",
    "parser.add_argument('--debug', default=0, type=int, help='is debug')\n",
    "parser.add_argument('--overwrite', default=0, type=int, help='is overwrite feature cache')\n",
    "parser.add_argument('--predict_epoch', default=None, type=str, help='number epoch to predict')\n",
    "parser.add_argument('--batch_size', default=4, type=int)\n",
    "parser.add_argument('--preprocessing', type=int, default=1)\n",
    "parser.add_argument('--num_to_rerank', type=int, default=10)\n",
    "parser.add_argument('--top_k', type=int, default=3)\n",
    "parser.add_argument('--ransac', default=1, type=int)\n",
    "parser.add_argument('--nolandmark_num', type=int, default=5000)\n",
    "parser.add_argument('--valid_num', type=int, default=20000)\n",
    "parser.add_argument('--do_train', type=int, default=1)\n",
    "parser.add_argument('--do_valid', type=int, default=1)\n",
    "parser.add_argument('--do_test', type=int, default=0)\n",
    "parser.add_argument('--ransac_type', type=str, default='ssp')\n",
    "parser.add_argument('--ransac_weight', type=float, default=1)\n",
    "parser.add_argument('--store_keypoint', type=int, default=1)\n",
    "parser.add_argument('--ransac_parts', default=1, type=int)\n",
    "parser.add_argument('--ransac_part', default=0, type=int)\n",
    "parser.add_argument('--lgb_model_dir', type=str, default='/kaggle/input/models')\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PUBLIC_TRAIN_IMAGES = 1580470\n",
    "\n",
    "# RANSAC parameters:\n",
    "MAX_INLIER_SCORE = 70\n",
    "MAX_REPROJECTION_ERROR = 4.0\n",
    "MAX_RANSAC_ITERATIONS = 1000\n",
    "HOMOGRAPHY_CONFIDENCE = 0.99\n",
    "\n",
    "def load_labelmap(TRAIN_LABELMAP_PATH):\n",
    "  with open(TRAIN_LABELMAP_PATH, mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    labelmap = {row['id']: row['landmark_id'] for row in csv_reader}\n",
    "  return labelmap\n",
    "\n",
    "def save_submission_csv(args, DATASET_DIR, predictions=None):\n",
    "  \"\"\"Saves optional `predictions` as submission.csv.\n",
    "\n",
    "  The csv has columns {id, landmarks}. The landmarks column is a string\n",
    "  containing the label and score for the id, separated by a ws delimeter.\n",
    "\n",
    "  If `predictions` is `None` (default), submission.csv is copied from\n",
    "  sample_submission.csv in `IMAGE_DIR`.\n",
    "\n",
    "  Args:\n",
    "    predictions: Optional dict of image ids to dicts with keys {class, score}.\n",
    "  \"\"\"\n",
    "\n",
    "  if predictions is None:\n",
    "    # Dummy submission!\n",
    "    shutil.copyfile(\n",
    "        os.path.join(DATASET_DIR, 'sample_submission.csv'), 'submission.csv')\n",
    "    return\n",
    "\n",
    "  if args.kaggle:\n",
    "    submit_fname = 'submission.csv'\n",
    "  else:\n",
    "    submit_dir = f'{RESULT_DIR}/submissions/{args.out_dir}'\n",
    "    os.makedirs(submit_dir, exist_ok=True)\n",
    "    submit_fname = f'{submit_dir}/submission.csv'\n",
    "\n",
    "  with open(submit_fname, 'w') as submission_csv:\n",
    "    csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'landmarks'])\n",
    "    csv_writer.writeheader()\n",
    "    for image_id, prediction in predictions.items():\n",
    "      label = prediction['class']\n",
    "      score = prediction['score']\n",
    "      csv_writer.writerow({'id': image_id, 'landmarks': f'{label} {score}'})\n",
    "  return submit_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestDataset(Dataset):\n",
    "\n",
    "  def __init__(self, args, df, img_dir):\n",
    "    self.args = args\n",
    "    self.img_size = (args.img_size, args.img_size)\n",
    "    self.img_dir = img_dir\n",
    "    self.img_ids = df[ID].values\n",
    "    print(f'img_size: {self.img_size}')\n",
    "    self.norm = Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.img_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.img_ids[idx]\n",
    "    img_dir = self.img_dir\n",
    "    if self.args.kaggle:\n",
    "      fname = f'{img_dir}/{img_id[0]}/{img_id[1]}/{img_id[2]}/{img_id}.jpg'\n",
    "      if not os.path.exists(fname):\n",
    "        fname = f'{img_dir}/{img_id}.jpg'\n",
    "      image = cv2.imread(fname)\n",
    "    else:\n",
    "      fname = f'{img_dir}/{img_id}.jpg'\n",
    "      if not os.path.exists(fname):\n",
    "        fname = f'{DATA_DIR}/images/test/{img_id}.jpg'\n",
    "      image = cv2.imread(fname)\n",
    "    image = image[..., ::-1]\n",
    "\n",
    "    if self.args.img_size is not None:\n",
    "      if image.shape[:2] != self.img_size:\n",
    "        image = cv2.resize(image, self.img_size)\n",
    "    else:\n",
    "      raise Exception()\n",
    "\n",
    "    if self.args.preprocessing==1:\n",
    "      image = self.norm(image=image)['image']\n",
    "    else:\n",
    "      image = image / 255.0\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image = torch.from_numpy(image).float()\n",
    "    return image\n",
    "\n",
    "def create_dataset(args, df, img_dir):\n",
    "  dataset = TestDataset(args, df, img_dir)\n",
    "  data_loader = DataLoader(\n",
    "    dataset,\n",
    "    sampler=SequentialSampler(dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=default_collate,\n",
    "  )\n",
    "  return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_map(test_ids, train_ids_labels_and_scores, top_k=3):\n",
    "  \"\"\"Makes dict from test ids and ranked training ids, labels, scores.\"\"\"\n",
    "  prediction_map = dict()\n",
    "  for test_index, test_id in enumerate(test_ids):\n",
    "    aggregate_scores = {}\n",
    "    if top_k > 0:\n",
    "      sub_train_ids_labels_and_scores = train_ids_labels_and_scores[test_index][:top_k]\n",
    "    else:\n",
    "      sub_train_ids_labels_and_scores = train_ids_labels_and_scores[test_index]\n",
    "    for _, label, score in sub_train_ids_labels_and_scores:\n",
    "      if label not in aggregate_scores:\n",
    "        aggregate_scores[label] = 0\n",
    "      aggregate_scores[label] += float(score)\n",
    "    label, score = max(aggregate_scores.items(), key=operator.itemgetter(1))\n",
    "    prediction_map[test_id] = {'score': score, 'class': label}\n",
    "  return prediction_map\n",
    "\n",
    "def extract_global_features(args, model, df, image_dir, dataset, return_prob=False):\n",
    "  N = len(df)\n",
    "  if args.kaggle:\n",
    "    features_dir = f'/kaggle/input/features/{args.out_dir}'\n",
    "  else:\n",
    "    features_dir = f'{RESULT_DIR}/features/{args.out_dir}'\n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "  if args.scale is not None:\n",
    "    features_fname = f'{features_dir}/epoch{args.predict_epoch}_i{args.scale}_{dataset}_features_{N}.fvecs'\n",
    "    img_ids_fname = f'{features_dir}/epoch{args.predict_epoch}_i{args.scale}_{dataset}_img_ids_{N}.npy'\n",
    "  elif args.img_size is not None:\n",
    "    features_fname = f'{features_dir}/epoch{args.predict_epoch}_i{args.img_size}_{dataset}_features_{N}.fvecs'\n",
    "    img_ids_fname = f'{features_dir}/epoch{args.predict_epoch}_i{args.img_size}_{dataset}_img_ids_{N}.npy'\n",
    "  else:\n",
    "    raise Exception()\n",
    "  try:\n",
    "    if args.parts > 1:\n",
    "      block = len(df) // args.parts + 1\n",
    "      df = df.iloc[args.part * block:(args.part + 1) * block].reset_index(drop=True)\n",
    "      features_fname = features_fname.replace(f'_{N}.fvecs', f'_{N}_{args.parts}_{args.part}.fvecs')\n",
    "      img_ids_fname = img_ids_fname.replace(f'_{N}.npy', f'_{N}_{args.parts}_{args.part}.npy')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  if return_prob:\n",
    "    cls_idxes_fname = features_fname.replace('_features_', '_cls_idxes_top50_').\\\n",
    "      replace('.fvecs', '.npy')\n",
    "    cls_probs_fname = features_fname.replace('_features_', '_cls_probs_top50_').\\\n",
    "      replace('.fvecs', '.npy')\n",
    "    print(cls_probs_fname)\n",
    "\n",
    "  print(features_fname)\n",
    "  do_cache = not args.kaggle\n",
    "  if ope(img_ids_fname) and not args.overwrite:\n",
    "    img_ids = np.load(img_ids_fname, allow_pickle=True)\n",
    "    embeddings = fvecs_read(features_fname)\n",
    "    if return_prob:\n",
    "      cls_idxes = np.load(cls_idxes_fname, allow_pickle=True)\n",
    "      cls_probs = np.load(cls_probs_fname, allow_pickle=True)\n",
    "  else:\n",
    "    if do_cache:\n",
    "      f = open(features_fname, 'wb')\n",
    "    dataloader = create_dataset(args, df, image_dir)\n",
    "    embeddings = []\n",
    "    cls_idxes = []\n",
    "    cls_probs = []\n",
    "    for it, images in tqdm(enumerate(dataloader),total=len(dataloader), desc=f'extract {dataset}'):\n",
    "      image_tensor = Variable(images.cuda(), volatile=True)\n",
    "\n",
    "      if return_prob:\n",
    "        logit, embedding = model(image_tensor, None)\n",
    "      else:\n",
    "        if hasattr(model.module, 'extract_feature'):\n",
    "          embedding = model.module.extract_feature(image_tensor)\n",
    "        elif hasattr(model.module, 'extract_feat'):\n",
    "          embedding = model.module.extract_feat(image_tensor)\n",
    "        else:\n",
    "          raise Exception('extract_feature')\n",
    "      embedding = L2N()(embedding)\n",
    "      embedding = embedding.cpu().detach().numpy()\n",
    "      embeddings.append(embedding)\n",
    "\n",
    "      if return_prob:\n",
    "        prob = F.softmax(logit, dim=1).cpu().numpy()\n",
    "        top50_idxes = np.argsort(prob, axis=1)[:, :-51:-1].astype('int32')\n",
    "        top50_probs = prob[\n",
    "          np.concatenate([np.arange(len(prob)).reshape(-1, 1)] * top50_idxes.shape[1], axis=1), top50_idxes].astype(\n",
    "          'float32')\n",
    "        cls_idxes.append(top50_idxes)\n",
    "        cls_probs.append(top50_probs)\n",
    "\n",
    "      if do_cache:\n",
    "        for ebd in embedding:\n",
    "          D = len(ebd)\n",
    "          f.write(struct.pack('<I%df' % D, D, *list(ebd)))\n",
    "\n",
    "    img_ids = dataloader.dataset.img_ids\n",
    "    if return_prob:\n",
    "      cls_idxes = np.concatenate(cls_idxes, axis=0)\n",
    "      cls_probs = np.concatenate(cls_probs, axis=0)\n",
    "    if do_cache:\n",
    "      f.flush()\n",
    "      f.close()\n",
    "      np.save(img_ids_fname, img_ids)\n",
    "      if return_prob:\n",
    "        np.save(cls_idxes_fname, cls_idxes)\n",
    "        np.save(cls_probs_fname, cls_probs)\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "  if return_prob:\n",
    "    return img_ids, np.array(embeddings), np.array(cls_idxes), np.array(cls_probs)\n",
    "  else:\n",
    "    return img_ids, np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_tensor(image_path):\n",
    "  return tf.convert_to_tensor(\n",
    "      np.array(PIL.Image.open(image_path).convert('RGB')))\n",
    "\n",
    "def extract_local_features(local_model_tf, local_model_tf_constant, image_path):\n",
    "  \"\"\"Extracts local features for the given `image_path`.\"\"\"\n",
    "\n",
    "  image_tensor = load_image_tensor(image_path)\n",
    "\n",
    "  features = local_model_tf(\n",
    "    image_tensor,\n",
    "    local_model_tf_constant['DELG_IMAGE_SCALES_TENSOR'],\n",
    "    local_model_tf_constant['DELG_SCORE_THRESHOLD_TENSOR'],\n",
    "    local_model_tf_constant['LOCAL_FEATURE_NUM_TENSOR'],\n",
    "  )\n",
    "\n",
    "  # Shape: (N, 2)\n",
    "  keypoints = tf.divide(\n",
    "      tf.add(\n",
    "          tf.gather(features[0], [0, 1], axis=1),\n",
    "          tf.gather(features[0], [2, 3], axis=1)), 2.0).numpy()\n",
    "\n",
    "  # Shape: (N, 128)\n",
    "  descriptors = tf.nn.l2_normalize(\n",
    "      features[1], axis=1, name='l2_normalization').numpy()\n",
    "\n",
    "  return keypoints, descriptors\n",
    "\n",
    "def compute_putative_matching_keypoints(test_keypoints,\n",
    "                                        test_descriptors,\n",
    "                                        train_keypoints,\n",
    "                                        train_descriptors,\n",
    "                                        max_distance=0.9):\n",
    "  \"\"\"Finds matches from `test_descriptors` to KD-tree of `train_descriptors`.\"\"\"\n",
    "\n",
    "  train_descriptor_tree = spatial.cKDTree(train_descriptors)\n",
    "  _, matches = train_descriptor_tree.query(\n",
    "      test_descriptors, distance_upper_bound=max_distance)\n",
    "\n",
    "  test_kp_count = test_keypoints.shape[0]\n",
    "  train_kp_count = train_keypoints.shape[0]\n",
    "\n",
    "  test_matching_keypoints = np.array([\n",
    "      test_keypoints[i,]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "  ])\n",
    "  train_matching_keypoints = np.array([\n",
    "      train_keypoints[matches[i],]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "  ])\n",
    "\n",
    "  return test_matching_keypoints, train_matching_keypoints\n",
    "\n",
    "def compute_num_inliers(test_keypoints, test_descriptors, train_keypoints,\n",
    "                        train_descriptors, do_kdtree=True):\n",
    "  \"\"\"Returns the number of RANSAC inliers.\"\"\"\n",
    "\n",
    "  if do_kdtree:\n",
    "    test_match_kp, train_match_kp = compute_putative_matching_keypoints(\n",
    "        test_keypoints, test_descriptors, train_keypoints, train_descriptors)\n",
    "  else:\n",
    "    test_match_kp, train_match_kp = test_keypoints, train_keypoints\n",
    "  if test_match_kp.shape[0] <= 4:  # Min keypoints supported by `pydegensac.findHomography()`\n",
    "    return 0\n",
    "\n",
    "  try:\n",
    "    _, mask = pydegensac.findHomography(test_match_kp, train_match_kp,\n",
    "                                        MAX_REPROJECTION_ERROR,\n",
    "                                        HOMOGRAPHY_CONFIDENCE,\n",
    "                                        MAX_RANSAC_ITERATIONS)\n",
    "  except np.linalg.LinAlgError:  # When det(H)=0, can't invert matrix.\n",
    "    return 0\n",
    "\n",
    "  return int(copy.deepcopy(mask).astype(np.float32).sum())\n",
    "\n",
    "def get_inliers(loc1, desc1, loc2, desc2):\n",
    "  n_feat1, n_feat2 = loc1.shape[0], loc2.shape[0]\n",
    "\n",
    "  # from scipy.spatial import cKDTree\n",
    "  KD_THRESH = 0.8\n",
    "  d1_tree = cKDTree(desc1)\n",
    "  distances, indices = d1_tree.query(desc2, distance_upper_bound=KD_THRESH)\n",
    "\n",
    "  loc2_to_use = np.array([loc2[i, ] for i in range(n_feat2) if indices[i] != n_feat1])\n",
    "  loc1_to_use = np.array([loc1[indices[i], ] for i in range(n_feat2) if indices[i] != n_feat1])\n",
    "\n",
    "  np.random.seed(114514)\n",
    "\n",
    "  # from skimage.measure import ransac as _ransac\n",
    "  # from skimage.transform import AffineTransform\n",
    "  try:\n",
    "    model_robust, inliers = _ransac(\n",
    "      (loc1_to_use, loc2_to_use),\n",
    "      AffineTransform,\n",
    "      min_samples=3,\n",
    "      residual_threshold=20,\n",
    "      max_trials=1000)\n",
    "    return sum(inliers)\n",
    "  except:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_score(num_inliers, global_score, weight=1.0, max_inlier_score=None):\n",
    "  if max_inlier_score is None:\n",
    "    max_inlier_score = MAX_INLIER_SCORE\n",
    "  local_score = min(num_inliers, max_inlier_score) / max_inlier_score\n",
    "  return local_score*weight + global_score\n",
    "\n",
    "\n",
    "def get_cached_num_inliers(ransac_cache_dir, test_image_id, train_image_id):\n",
    "  ransac_fname = f'{ransac_cache_dir}/{test_image_id}_{train_image_id}.npy'\n",
    "  if ope(ransac_fname):\n",
    "    num_inliers = np.load(ransac_fname, allow_pickle=True)\n",
    "  else:\n",
    "    ransac_fname = f'{ransac_cache_dir}/{train_image_id}_{test_image_id}.npy'\n",
    "    if ope(ransac_fname):\n",
    "      num_inliers = np.load(ransac_fname, allow_pickle=True)\n",
    "    else:\n",
    "      ransac_fname = f'{ransac_cache_dir}/{test_image_id}_{train_image_id}.npy'\n",
    "      num_inliers = None\n",
    "  return ransac_fname, num_inliers\n",
    "\n",
    "def get_whole_cached_num_inliers(args):\n",
    "  ransac_cache_dir, keypoint_cache_dir = get_ransac_cache_dir(args)\n",
    "  whole_ransac_fname = f'{ransac_cache_dir}/whole_ransac_inliers.pkl'\n",
    "  if ope(whole_ransac_fname):\n",
    "    with open(whole_ransac_fname, 'rb') as dbfile:\n",
    "      data = pickle.load(dbfile)\n",
    "  else:\n",
    "    data = dict()\n",
    "  return data\n",
    "\n",
    "def save_whole_cached_num_inliers(args, data):\n",
    "  ransac_cache_dir, keypoint_cache_dir = get_ransac_cache_dir(args)\n",
    "  whole_ransac_fname = f'{ransac_cache_dir}/whole_ransac_inliers.pkl'\n",
    "  with open(whole_ransac_fname, 'wb') as dbfile:\n",
    "    pickle.dump(data, dbfile)\n",
    "\n",
    "def load_cached_keypoints(keypoint_cache_dir, img_id):\n",
    "  keypoint_fname = f'{keypoint_cache_dir}/keypoint_{img_id}.pkl'\n",
    "  if ope(keypoint_fname):\n",
    "    with open(keypoint_fname, 'rb') as dbfile:\n",
    "      data = pickle.load(dbfile)\n",
    "    return data\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def save_cached_keypoints(keypoint_cache_dir, img_id, keypoints, scores, descriptors, scales):\n",
    "  keypoint_fname = f'{keypoint_cache_dir}/keypoint_{img_id}.pkl'\n",
    "  if not ope(keypoint_fname):\n",
    "    data = {\n",
    "      'keypoints': keypoints[0].cpu().numpy(),\n",
    "      'scores': scores[0].data.cpu().numpy(),\n",
    "      'descriptors': descriptors[0].data.cpu().numpy(),\n",
    "      'scales': scales,\n",
    "    }\n",
    "    with open(keypoint_fname, 'wb') as dbfile:\n",
    "      pickle.dump(data, dbfile)\n",
    "\n",
    "def load_cached_matches(keypoint_cache_dir, query_image_id, index_image_id):\n",
    "  match_fname = f'{keypoint_cache_dir}/match_query_{query_image_id}_index_{index_image_id}.pkl'\n",
    "  if ope(match_fname):\n",
    "    try:\n",
    "      with open(match_fname, 'rb') as dbfile:\n",
    "        data = pickle.load(dbfile)\n",
    "    except:\n",
    "      data = None\n",
    "    return data\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def save_cached_matches(keypoint_cache_dir, query_image_id, index_image_id,\n",
    "                        matches0, matches1, matching_scores0, matching_scores1):\n",
    "  match_fname = f'{keypoint_cache_dir}/match_query_{query_image_id}_index_{index_image_id}.pkl'\n",
    "  if not ope(match_fname):\n",
    "    data = {\n",
    "      'matches0': matches0.cpu().numpy(),\n",
    "      'matches1': matches1.cpu().numpy(),\n",
    "      'matching_scores0': matching_scores0.data.cpu().numpy(),\n",
    "      'matching_scores1': matching_scores1.data.cpu().numpy(),\n",
    "    }\n",
    "    with open(match_fname, 'wb') as dbfile:\n",
    "      pickle.dump(data, dbfile)\n",
    "\n",
    "def generate_superpoint_superglue(args, test_image_id, test_image_path, train_image_id, train_image_path,\n",
    "                                  test_image_dict, superpointglue_net, do_cache, keypoint_cache_dir):\n",
    "  if test_image_id in test_image_dict:\n",
    "    test_image, test_inp, test_scales, test_keypoints, test_scores, test_descriptors = test_image_dict[test_image_id]\n",
    "  else:\n",
    "    test_image, test_inp, test_scales = spg_read_image(test_image_path, resize=[800], rotation=0, resize_float=False)\n",
    "    test_keypoints, test_scores, test_descriptors = None, None, None\n",
    "  train_image, train_inp, train_scales = spg_read_image(train_image_path, resize=[800], rotation=0, resize_float=False)\n",
    "\n",
    "  data_inp = {'image0': test_inp, 'image1': train_inp}\n",
    "  if test_keypoints is not None:\n",
    "    data_inp = {**data_inp, **{'keypoints0': test_keypoints, 'scores0': test_scores, 'descriptors0': test_descriptors}}\n",
    "  pred = superpointglue_net(data_inp)\n",
    "\n",
    "  test_keypoints, test_scores, test_descriptors = pred['keypoints0'], pred['scores0'], pred['descriptors0']\n",
    "  train_keypoints, train_scores, train_descriptors = pred['keypoints1'], pred['scores1'], pred['descriptors1']\n",
    "  test_train_matches0, test_train_matches1 = pred['matches0'], pred['matches1']\n",
    "  test_train_matching_scores0, test_train_matching_scores1 = pred['matching_scores0'], pred['matching_scores1']\n",
    "  if do_cache and args.store_keypoint:\n",
    "    save_cached_keypoints(keypoint_cache_dir, test_image_id, test_keypoints, test_scores, test_descriptors, test_scales)\n",
    "    save_cached_keypoints(keypoint_cache_dir, train_image_id, train_keypoints, train_scores, train_descriptors, train_scales)\n",
    "    save_cached_matches(keypoint_cache_dir, test_image_id, train_image_id, test_train_matches0,\n",
    "                        test_train_matches1, test_train_matching_scores0, test_train_matching_scores1)\n",
    "\n",
    "  test_image_dict[test_image_id] = (test_image, test_inp, test_scales, test_keypoints, test_scores, test_descriptors)\n",
    "\n",
    "  pred['scales0'] = test_scales\n",
    "  pred['scales1'] = train_scales\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ransac_cache_dir(args):\n",
    "  if args.kaggle:\n",
    "    cache_root = '/kaggle/working'\n",
    "  else:\n",
    "    cache_root = f'{DATA_DIR}/cache'\n",
    "  if (args.ransac_type is None) or (args.ransac_type == '') or (args.ransac_type.lower() == 'degensac'):\n",
    "    ransac_cache_dir = f'{cache_root}/ransac_1s/'\n",
    "    keypoint_cache_dir = None\n",
    "  elif args.ransac_type.lower() == 'skransac':\n",
    "    ransac_cache_dir = f'{cache_root}/ransac_20191st_1s/'\n",
    "    keypoint_cache_dir = None\n",
    "  elif args.ransac_type.lower() == 'superpointglue':\n",
    "    ransac_cache_dir = f'{cache_root}/ransac_superpointglue_l800_1s/'\n",
    "    keypoint_cache_dir = f'{cache_root}/keypoint_superpoint_l800_1s/'\n",
    "  elif args.ransac_type.lower() == 'ssp':\n",
    "    ransac_cache_dir = f'{cache_root}/ransac_ssp_l800_1s/'\n",
    "    keypoint_cache_dir = f'{cache_root}/keypoint_superpoint_l800_1s/'\n",
    "  else:\n",
    "    raise ValueError(f'{args.ransac_type} error, only available [degensac, skransac, SuperPointGlue, ssp]')\n",
    "  os.makedirs(ransac_cache_dir, exist_ok=True)\n",
    "  if keypoint_cache_dir is not None:\n",
    "    os.makedirs(keypoint_cache_dir, exist_ok=True)\n",
    "  return ransac_cache_dir, keypoint_cache_dir\n",
    "\n",
    "def rescore_and_rerank_by_num_inliers(args, test_image_dir, train_image_dir,\n",
    "                                      test_image_id, train_ids_labels_and_scores, ignore_global_score=False, do_sort=True,\n",
    "                                      superpointglue_net=None, return_num_inliers=False, cache_num_inliers_dict=None):\n",
    "  \"\"\"Returns rescored and sorted training images by local feature extraction.\"\"\"\n",
    "  do_cache = not args.kaggle\n",
    "  ransac_cache_dir, keypoint_cache_dir = get_ransac_cache_dir(args)\n",
    "  cache_num_inliers_dict = dict() if cache_num_inliers_dict is None else cache_num_inliers_dict\n",
    "\n",
    "  if args.kaggle:\n",
    "    test_image_path = f'{test_image_dir}/{test_image_id[0]}/{test_image_id[1]}/{test_image_id[2]}/{test_image_id}.jpg'\n",
    "  else:\n",
    "    test_image_path = f'{test_image_dir}/{test_image_id}.jpg'\n",
    "    if not ope(test_image_path):\n",
    "      test_image_path = f'{DATA_DIR}/images/test/{test_image_id}.jpg'\n",
    "  test_image_dict = {}\n",
    "\n",
    "  ransac_inliers = []\n",
    "  for i in range(len(train_ids_labels_and_scores)):\n",
    "    train_image_id, label, global_score = train_ids_labels_and_scores[i]\n",
    "    ransac_fname, num_inliers = None, cache_num_inliers_dict.get((test_image_id, train_image_id), None)\n",
    "    if num_inliers is None:\n",
    "      ransac_fname, num_inliers = get_cached_num_inliers(ransac_cache_dir, test_image_id, train_image_id)\n",
    "    if num_inliers is None:\n",
    "\n",
    "      if args.kaggle:\n",
    "        train_image_path = f'{train_image_dir}/{train_image_id[0]}/{train_image_id[1]}/{train_image_id[2]}/{train_image_id}.jpg'\n",
    "        if not ope(train_image_path):\n",
    "          train_image_path = f'{train_image_dir}/{train_image_id}.jpg'\n",
    "      else:\n",
    "        train_image_path = f'{train_image_dir}/{train_image_id}.jpg'\n",
    "        if not ope(train_image_path):\n",
    "          train_image_path = f'{DATA_DIR}/images/test/{train_image_id}.jpg'\n",
    "\n",
    "      if (args.ransac_type is not None) and (args.ransac_type.lower() == 'ssp'):\n",
    "        match_data = load_cached_matches(keypoint_cache_dir, test_image_id, train_image_id)\n",
    "        if match_data is None:\n",
    "          pred = generate_superpoint_superglue(args, test_image_id, test_image_path, train_image_id, train_image_path,\n",
    "                                               test_image_dict, superpointglue_net, do_cache, keypoint_cache_dir)\n",
    "          test_scales = pred['scales0']\n",
    "          test_keypoints = copy.deepcopy(pred['keypoints0'])[0].cpu().numpy()\n",
    "\n",
    "          train_scales = pred['scales1']\n",
    "          train_keypoints = copy.deepcopy(pred['keypoints1'])[0].cpu().numpy()\n",
    "\n",
    "          matches0 = pred['matches0'].cpu().numpy()[0]\n",
    "        else:\n",
    "          test_keypoint_data = load_cached_keypoints(keypoint_cache_dir, test_image_id)\n",
    "          test_keypoints, test_scales = test_keypoint_data['keypoints'], test_keypoint_data['scales']\n",
    "\n",
    "          train_keypoint_data = load_cached_keypoints(keypoint_cache_dir, train_image_id)\n",
    "          train_keypoints, train_scales = train_keypoint_data['keypoints'], train_keypoint_data['scales']\n",
    "\n",
    "          matches0 = match_data['matches0'][0]\n",
    "\n",
    "        test_keypoints = test_keypoints * np.array([list(test_scales)])\n",
    "        test_keypoints = test_keypoints[:, ::-1]\n",
    "        train_keypoints = train_keypoints * np.array([list(train_scales)])\n",
    "        train_keypoints = train_keypoints[:, ::-1]\n",
    "\n",
    "        valid0 = matches0 > -1\n",
    "        test_keypoints = test_keypoints[valid0]\n",
    "        train_keypoints = train_keypoints[matches0[valid0]]\n",
    "        num_inliers = compute_num_inliers(test_keypoints, None, train_keypoints, None, do_kdtree=False)\n",
    "      if do_cache and ransac_fname is not None:\n",
    "        np.save(ransac_fname, num_inliers)\n",
    "\n",
    "    cache_num_inliers_dict[(test_image_id, train_image_id)] = num_inliers\n",
    "    if ignore_global_score:\n",
    "      total_score = get_total_score(num_inliers, 0.)\n",
    "    else:\n",
    "      total_score = get_total_score(num_inliers, global_score, weight=args.ransac_weight, max_inlier_score=90)\n",
    "    train_ids_labels_and_scores[i] = (train_image_id, label, total_score)\n",
    "    ransac_inliers.append((train_image_id, num_inliers))\n",
    "  if do_sort:\n",
    "    train_ids_labels_and_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "  if return_num_inliers:\n",
    "    return ransac_inliers\n",
    "  else:\n",
    "    return train_ids_labels_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nolandmark_by_dbscan(test_ids, test_embeddings, nolandmark_ids, nolandmark_embeddings):\n",
    "  # dbscan\n",
    "  features = np.vstack([test_embeddings, nolandmark_embeddings])\n",
    "  clusters = dbscan(eps=0.85, n_jobs=-1, min_samples=1).fit_predict(features)\n",
    "  clusters_np = np.c_[np.r_[test_ids, nolandmark_ids], clusters]\n",
    "  clusters_df = pd.DataFrame(data=clusters_np, columns=[ID, 'clusters'])\n",
    "  clusters_df['is_nolandmark'] = [0]*len(test_ids) + [1]*len(nolandmark_ids)\n",
    "  clusters_gb = clusters_df.groupby('clusters')['is_nolandmark'].agg(['count', 'sum']).reset_index()\n",
    "  clusters_gb.columns = ['clusters', 'clusters_num', 'nolandmark_num']\n",
    "  clusters_gb['nolandmark_rate'] = clusters_gb['nolandmark_num'] / clusters_gb['clusters_num']\n",
    "\n",
    "  test_clusters = clusters_df[0: len(test_ids)]\n",
    "  test_clusters = test_clusters.merge(clusters_gb, on='clusters', how='left')\n",
    "  return test_clusters\n",
    "\n",
    "def do_retrieval(args, labelmap, train_ids, train_embeddings,\n",
    "                 test_embeddings, num_to_rerank, do_dba=False, gallery_set='index'):\n",
    "  train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n",
    "\n",
    "  if do_dba:\n",
    "    faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "    faiss_index.add(train_embeddings)\n",
    "    dba_lens = 10\n",
    "    weights = np.logspace(0, -1.5, dba_lens)\n",
    "    weights /= np.sum(weights)\n",
    "    D, I = faiss_index.search(train_embeddings, dba_lens)\n",
    "    new_xb = 0\n",
    "    for i, weight in enumerate(weights):\n",
    "      new_xb = new_xb + train_embeddings[I[:, i]] * weight\n",
    "    train_embeddings = new_xb\n",
    "\n",
    "  faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "  faiss_index.add(train_embeddings)\n",
    "  D, I = faiss_index.search(test_embeddings, num_to_rerank)  # actual search\n",
    "  if not args.kaggle:\n",
    "    save_faiss_results(args, D, I, gallery_set, num_to_rerank, test_embeddings)\n",
    "  for test_index in range(test_embeddings.shape[0]):\n",
    "    train_ids_labels_and_scores[test_index] = [\n",
    "      (train_ids[train_index], labelmap[train_ids[train_index]], distance)\n",
    "      for train_index, distance in zip(I[test_index], D[test_index])\n",
    "    ]\n",
    "  return train_ids_labels_and_scores\n",
    "\n",
    "\n",
    "def save_faiss_results(args, D, I, gallery_set, topn, df, suffix=''):\n",
    "  dataset = '%s_%s' % ('test', gallery_set)\n",
    "  faiss_dir = f'{RESULT_DIR}/faiss/{args.out_dir}'\n",
    "  os.makedirs(faiss_dir, exist_ok=True)\n",
    "  if args.scale is not None:\n",
    "    I_fname = f'{faiss_dir}/epoch{args.predict_epoch}_i{args.scale}_{dataset}_knn_top{topn}_i_{len(df)}{suffix}.npz'\n",
    "    D_fname = f'{faiss_dir}/epoch{args.predict_epoch}_i{args.scale}_{dataset}_knn_top{topn}_d_{len(df)}{suffix}.npz'\n",
    "  elif args.img_size is not None:\n",
    "    I_fname = f'{faiss_dir}/epoch{args.predict_epoch}_i{args.img_size}_{dataset}_knn_top{topn}_i_{len(df)}{suffix}.npz'\n",
    "    D_fname = f'{faiss_dir}/epoch{args.predict_epoch}_i{args.img_size}_{dataset}_knn_top{topn}_d_{len(df)}{suffix}.npz'\n",
    "  np.savez_compressed(I_fname, i=I)\n",
    "  np.savez_compressed(D_fname, d=D)\n",
    "\n",
    "def get_retrieval_type(args, labelmap, train_ids, train_embeddings, test_embeddings, num_to_rerank):\n",
    "  faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "  faiss_index.add(train_embeddings)\n",
    "  D, I = faiss_index.search(test_embeddings, num_to_rerank)  # actual search\n",
    "  test_retrieval_type = {}\n",
    "  for test_index in tqdm(range(test_embeddings.shape[0]), total=test_embeddings.shape[0]):\n",
    "    index_idx = I[test_index]\n",
    "    target_nunique = len(np.unique([labelmap[train_ids[i]] for i in index_idx]))\n",
    "    max_score = np.max(D[test_index])\n",
    "    min_score = np.min(D[test_index])\n",
    "    if target_nunique <= 2 and min_score > 0.9:\n",
    "      _type = 1\n",
    "    elif target_nunique <= 2 and max_score > 0.85:\n",
    "      _type = 2\n",
    "    elif target_nunique == num_to_rerank:\n",
    "      _type = 4\n",
    "    else:\n",
    "      _type = 3\n",
    "    test_retrieval_type[test_index] = _type\n",
    "\n",
    "  return test_retrieval_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rerank(args, local_model_tf, local_model_tf_constant, superpointglue_net, test_image_dir, predictions, test_ids, test_embeddings, rerank_topk=2000):\n",
    "  predictions_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['score', 'class'])\n",
    "  predictions_df = predictions_df.reset_index().rename(columns={'index': ID})\n",
    "  predictions_df = predictions_df.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "  predictions_df = predictions_df.head(rerank_topk)\n",
    "  labelmap = {}\n",
    "  for _id in predictions_df[ID]:\n",
    "    labelmap[_id] = -1\n",
    "  rerank_ids = []\n",
    "  for _idx, row in tqdm(predictions_df.iterrows(), desc='rerank', total=len(predictions_df)):\n",
    "    search_id = row[ID]\n",
    "    if search_id in rerank_ids:\n",
    "      continue\n",
    "    search_idx = test_ids.tolist().index(search_id)\n",
    "    search_ebd = test_embeddings[search_idx]\n",
    "    query_ids = predictions_df[ID].values[_idx+1:].tolist()\n",
    "    query_ids = list(set(query_ids) - set(rerank_ids))\n",
    "    if len(query_ids) <= 0:\n",
    "      continue\n",
    "    query_idx = pd.Series(index=test_ids, data=np.arange(len(test_ids)))[query_ids].values\n",
    "    query_ebds = test_embeddings[query_idx]\n",
    "\n",
    "    train_ids_labels_and_scores = do_retrieval(args, labelmap, query_ids, query_ebds, search_ebd.reshape(1, -1), args.rerank_retrieval_num)\n",
    "\n",
    "    ransac_inliers = rescore_and_rerank_by_num_inliers(args, local_model_tf, local_model_tf_constant,\n",
    "                                                          test_image_dir, test_image_dir, search_id, train_ids_labels_and_scores[0],\n",
    "                                                          superpointglue_net=superpointglue_net, ignore_global_score=False, do_sort=False, return_num_inliers=True)\n",
    "    ransac_inliers = pd.DataFrame(ransac_inliers, columns=[ID, 'inliers'])\n",
    "    ransac_inliers['inliers'] = ransac_inliers['inliers'].astype(int)\n",
    "    ransac_inliers = ransac_inliers[ransac_inliers['inliers'] > args.rerank_inliers_limit]\n",
    "    ransac_inliers = ransac_inliers.sort_values('inliers', ascending=False).reset_index(drop=True)\n",
    "    rerank_ids.extend(ransac_inliers[ID].values.tolist())\n",
    "    for _rank, row in ransac_inliers.iterrows():\n",
    "      _score = predictions[search_id]['score'] - (_rank+1) * 0.001\n",
    "      predictions[row[ID]]['score'] = _score\n",
    "  print(f'rerank: {len(rerank_ids)}')\n",
    "  return predictions\n",
    "\n",
    "def detect_nolandmark(args, predictions, test_ids, test_image_dir):\n",
    "  nl_ids = []\n",
    "  do_cache = not args.kaggle\n",
    "  detect_cache_dir = f'{DATA_DIR}/cache/detect/'\n",
    "  os.makedirs(detect_cache_dir, exist_ok=True)\n",
    "\n",
    "  # load model\n",
    "  if args.kaggle:\n",
    "    detector_model_dir = '/kaggle/input/pretrained/d2r_frcnn_20190411'\n",
    "  else:\n",
    "    detector_model_dir = '/data5/data/pretrained/d2r_frcnn_20190411'\n",
    "  detector_fn = detector.MakeDetector(detector_model_dir)\n",
    "\n",
    "  for _, test_image_id in tqdm(enumerate(test_ids), total=len(test_ids), desc='do detect'):\n",
    "    if args.kaggle:\n",
    "      test_image_path = f'{test_image_dir}/{test_image_id[0]}/{test_image_id[1]}/{test_image_id[2]}/{test_image_id}.jpg'\n",
    "    else:\n",
    "      test_image_path = f'{test_image_dir}/{test_image_id}.jpg'\n",
    "      if not ope(test_image_path):\n",
    "        test_image_path = f'{DATA_DIR}/images/test/{test_image_id}.jpg'\n",
    "\n",
    "    boxes_path = f'{detect_cache_dir}/{test_image_id}.boxes'\n",
    "    if ope(boxes_path):\n",
    "      (boxes_out, scores_out, class_indices_out) = box_io.ReadFromFile(boxes_path)\n",
    "    else:\n",
    "      im = np.expand_dims(np.array(utils.RgbLoader(test_image_path)), 0)\n",
    "      (boxes_out, scores_out, class_indices_out) = detector_fn(im)\n",
    "      boxes_out, scores_out, class_indices_out = boxes_out[0], scores_out[0], class_indices_out[0]\n",
    "      if do_cache:\n",
    "        box_io.WriteToFile(boxes_path, boxes_out, scores_out, class_indices_out)\n",
    "\n",
    "    (selected_boxes, selected_scores, selected_class_indices) = \\\n",
    "      _FilterBoxesByScore(boxes_out, scores_out, class_indices_out, args.detect_thresh)\n",
    "    if len(selected_boxes) > 0:\n",
    "      selected_areas = (selected_boxes[:, 3] - selected_boxes[:, 1]) * (selected_boxes[:, 2] - selected_boxes[:, 0])\n",
    "      max_area = selected_areas.max()\n",
    "    else:\n",
    "      max_area = 0\n",
    "\n",
    "    if max_area <= args.detect_area:\n",
    "      nl_ids.append(test_image_id)\n",
    "      predictions[test_image_id]['score'] = predictions[test_image_id]['score'] - 2\n",
    "  print(f'detect_nl: {len(nl_ids)}')\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(args, local_model_tf, local_model_tf_constant, superpointglue_net, test_image_dir, predictions, labelmap, test_ids, test_embeddings, train_ids, train_embeddings, nolandmark_ids, nolandmark_embeddings):\n",
    "  if args.nolandmark_cluster_type != 0:\n",
    "    nolandmark_preds = get_nolandmark_by_dbscan(test_ids, test_embeddings, nolandmark_ids, nolandmark_embeddings)\n",
    "    if args.nolandmark_cluster_type == 1:\n",
    "      nolandmark_preds = nolandmark_preds[nolandmark_preds['nolandmark_num'] > args.nolandmark_cluster_num_limit]\n",
    "      print(f'set {len(nolandmark_preds)} nolandmark')\n",
    "      for index, row in nolandmark_preds.iterrows():\n",
    "        predictions[row[ID]]['score'] = 0\n",
    "    elif args.nolandmark_cluster_type in [2,3]:\n",
    "      nolandmark_preds1 = nolandmark_preds[nolandmark_preds['nolandmark_num'] >= args.nolandmark_cluster_num_limit]\n",
    "      print(f'0: set {len(nolandmark_preds1)} nolandmark')\n",
    "      for index, row in nolandmark_preds1.iterrows():\n",
    "        if args.nolandmark_cluster_type == 2:\n",
    "          predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - 2 * row['nolandmark_rate'] - 0.1 * row['nolandmark_num']\n",
    "        else:\n",
    "          predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - row['nolandmark_rate']*math.exp(row['nolandmark_rate'])\n",
    "\n",
    "      nolandmark_preds2 = nolandmark_preds[nolandmark_preds['nolandmark_num'] < args.nolandmark_cluster_num_limit]\n",
    "      print(f'1: set {len(nolandmark_preds2)} nolandmark')\n",
    "      for index, row in nolandmark_preds2.iterrows():\n",
    "        predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - min(0.2, row['nolandmark_rate'])\n",
    "\n",
    "  if args.nolandmark_retrieval:\n",
    "    nl_labelmap = copy.deepcopy(labelmap)\n",
    "    for nl_id in nolandmark_ids:\n",
    "      nl_labelmap[nl_id] = -1\n",
    "    train_nl_ids = np.hstack((train_ids, nolandmark_ids))\n",
    "    train_nl_embeddings = np.vstack((train_embeddings, nolandmark_embeddings))\n",
    "    nl_train_ids_labels_and_scores = do_retrieval(args, nl_labelmap, train_nl_ids, train_nl_embeddings,\n",
    "                                                  test_embeddings, args.num_to_rerank, gallery_set='nolandmark')\n",
    "    nl_predictions = get_prediction_map(test_ids, nl_train_ids_labels_and_scores, args.num_to_rerank)\n",
    "    nl_predictions_df = pd.DataFrame.from_dict(nl_predictions, orient='index', columns=['score', 'class']).reset_index().rename(columns={'index': ID})\n",
    "    nl_predictions_df = nl_predictions_df[nl_predictions_df['class'] == -1]\n",
    "    print(f'nl retrieval: set {len(nl_predictions_df)} nolandmark')\n",
    "    for index, row in nl_predictions_df.iterrows():\n",
    "      predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - row['score']\n",
    "\n",
    "  retrieval2_nl_ids = None\n",
    "  if args.nolandmark_retrieval2:\n",
    "    nl_labelmap = {}\n",
    "    for nl_id in nolandmark_ids:\n",
    "      nl_labelmap[nl_id] = -1\n",
    "    nl_train_ids_labels_and_scores = do_retrieval(args, nl_labelmap, nolandmark_ids, nolandmark_embeddings,\n",
    "                                                  test_embeddings, args.num_to_rerank, gallery_set='nolandmark')\n",
    "    nl_train_ids_labels_and_scores = np.array(nl_train_ids_labels_and_scores)\n",
    "    nl_predictions_df = pd.DataFrame(nl_train_ids_labels_and_scores[:, :3, -1].astype(float), columns=['top1', 'top2', 'top3'])\n",
    "    nl_predictions_df.insert(0, ID, test_ids)\n",
    "    nl_predictions_df['top_mean'] = nl_predictions_df[['top1', 'top2', 'top3']].mean(axis=1)\n",
    "    retrieval2_nl_ids = nl_predictions_df[nl_predictions_df['top1'] >= 0.55][ID].values\n",
    "    nl_predictions_df = nl_predictions_df[nl_predictions_df['top3'] >= 0.3]\n",
    "    print(f'nl retrieval2: set {len(nl_predictions_df)} nolandmark')\n",
    "    for index, row in nl_predictions_df.iterrows():\n",
    "      if args.nolandmark_retrieval2_type == 0:\n",
    "        predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - row['top3'] * 1.5\n",
    "      elif args.nolandmark_retrieval2_type == 1:\n",
    "        predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - row['top3'] * 1.5\n",
    "        if row['top3'] > 0.5:\n",
    "          predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - 3\n",
    "      elif args.nolandmark_retrieval2_type == 2:\n",
    "        predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - (row['top3']-0.3) * 4\n",
    "      elif args.nolandmark_retrieval2_type == 3:\n",
    "        predictions[row[ID]]['score'] = predictions[row[ID]]['score'] - (row['top1']+row['top2']+row['top3'])*0.5\n",
    "\n",
    "  if args.rule:\n",
    "    predictions_df = pd.DataFrame.from_dict(predictions, orient='index', columns=['score', 'class'])\n",
    "    predictions_df = predictions_df.reset_index().rename(columns={'index':ID})\n",
    "    predictions_vc = predictions_df.groupby('class')[ID].count()\n",
    "    nl_classes = predictions_vc[predictions_vc > args.rule_limit].index.values\n",
    "    c = 0\n",
    "    for k in predictions.keys():\n",
    "      if predictions[k]['class'] in nl_classes:\n",
    "        if predictions[k]['score'] > args.protect_score:\n",
    "          continue\n",
    "        predictions[k]['score'] = predictions[k]['score'] - 2\n",
    "        c = c + 1\n",
    "    print(f'rule: set {c} nolandmark')\n",
    "\n",
    "  if args.rule2:\n",
    "    retrieval_type = get_retrieval_type(args, labelmap, train_ids, train_embeddings, test_embeddings, 5)\n",
    "    retrieval_type_df = pd.DataFrame.from_dict(retrieval_type, orient='index', columns=['type']).reset_index().rename(columns={'index':ID})\n",
    "    print(f'rule2:')\n",
    "    print(retrieval_type_df['type'].value_counts())\n",
    "    for index, row in retrieval_type_df.iterrows():\n",
    "      _id = test_ids[row[ID]]\n",
    "      if args.rule2_type == 1:\n",
    "        if retrieval2_nl_ids is not None:\n",
    "          if _id in retrieval2_nl_ids:\n",
    "            continue\n",
    "      if row['type'] == 1:\n",
    "        predictions[_id]['score'] = predictions[_id]['score'] + 3\n",
    "      elif row['type'] == 2:\n",
    "        predictions[_id]['score'] = predictions[_id]['score'] + 1\n",
    "      elif row['type'] == 3:\n",
    "        predictions[_id]['score'] = predictions[_id]['score'] + 0.5\n",
    "\n",
    "  if args.detect_nl:\n",
    "    predictions = detect_nolandmark(args, predictions, test_ids, test_image_dir)\n",
    "\n",
    "  if args.rerank:\n",
    "    predictions = do_rerank(args, local_model_tf, local_model_tf_constant, superpointglue_net, test_image_dir, predictions, test_ids, test_embeddings, rerank_topk=args.rerank_num)\n",
    "\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_filter_index(args, train_ids, train_embeddings, nolandmark_ids, nolandmark_embeddings):\n",
    "  nl_labelmap = dict([(i, -1) for i in nolandmark_ids])\n",
    "  nl_train_ids_labels_and_scores = do_retrieval(args, nl_labelmap, nolandmark_ids, nolandmark_embeddings, train_embeddings, 3)\n",
    "  nl_train_ids_labels_and_scores = np.array(nl_train_ids_labels_and_scores)\n",
    "  nl_predictions_df = pd.DataFrame(nl_train_ids_labels_and_scores[:, :, -1].astype(float), columns=['top1', 'top2', 'top3'])\n",
    "  nl_predictions_df.insert(0, ID, train_ids)\n",
    "  nl_predictions_df['top_mean'] = nl_predictions_df[['top1', 'top2', 'top3']].mean(axis=1)\n",
    "  nl_predictions_df = nl_predictions_df[nl_predictions_df['top3'] <= args.filter_index_thresh]\n",
    "  print(f'nl filter_index: set {len(nl_predictions_df)} not nolandmark')\n",
    "  nnl_indexs = nl_predictions_df.index.values\n",
    "  train_ids = train_ids[nnl_indexs]\n",
    "  train_embeddings = train_embeddings[nnl_indexs]\n",
    "  return train_ids, train_embeddings\n",
    "\n",
    "\n",
    "def GAP_vector(pred, conf, true):\n",
    "  '''\n",
    "  Compute Global Average Precision (aka micro AP), the metric for the\n",
    "  Google Landmark Recognition competition.\n",
    "  This function takes predictions, labels and confidence scores as vectors.\n",
    "  In both predictions and ground-truth, use None/np.nan for \"no label\".\n",
    "\n",
    "  Args:\n",
    "      pred: vector of integer-coded predictions\n",
    "      conf: vector of probability or confidence scores for pred\n",
    "      true: vector of integer-coded labels for ground truth\n",
    "      return_x: also return the data frame used in the calculation\n",
    "\n",
    "  Returns:\n",
    "      GAP score\n",
    "  '''\n",
    "  x = pd.DataFrame({'pred': pred, 'conf': conf, 'true': true})\n",
    "  x.sort_values('conf', ascending=False, inplace=True, na_position='last')\n",
    "  x['correct'] = (x.true == x.pred).astype(int)\n",
    "  x['prec_k'] = x.correct.cumsum() / (np.arange(len(x)) + 1)\n",
    "  x['term'] = x.prec_k * x.correct\n",
    "  gap = x.term.sum() / x.true.count()\n",
    "  return gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Xy(test_df, topk_labels, topk_scores, topk=5):\n",
    "  # generate X\n",
    "  _topk_scores = []\n",
    "  for i in range(topk_scores.shape[0]):\n",
    "    counter = Counter()\n",
    "    for j in range(topk_scores.shape[1]):\n",
    "      counter[topk_labels[i, j]] += topk_scores[i, j]\n",
    "    sub_topk_scores = counter.most_common(topk)\n",
    "    sub_topk_scores.extend([(-1, 0.)] * max(topk - len(sub_topk_scores), 0))\n",
    "    _topk_scores.append(sub_topk_scores)\n",
    "  topk_scores = np.array(_topk_scores)\n",
    "  topk_X, topk_labels = topk_scores[:, :, 1], topk_scores[:, :, 0]\n",
    "  topk_labels = topk_labels.astype('int32')\n",
    "\n",
    "  topk_df = pd.DataFrame(data=topk_X, columns=[f'top{i}_sum_score' for i in range(topk)])\n",
    "  topk_df['top0_top1'] = topk_df['top0_sum_score'] - topk_df['top1_sum_score']\n",
    "  topk_df.insert(0, ID, test_df[ID].values)\n",
    "  if CTARGET in test_df.columns:\n",
    "    topk_df.insert(1, CTARGET, test_df[CTARGET].values)\n",
    "  else:\n",
    "    topk_df.insert(1, CTARGET, None)\n",
    "\n",
    "  # generate y\n",
    "  # topk + nl + other\n",
    "  topk_df['y'] = None\n",
    "  if CTARGET in test_df.columns:\n",
    "    ys = []\n",
    "    for ctarget, topk_label in zip(test_df[CTARGET], topk_labels):\n",
    "      if 'nan' == ctarget:\n",
    "        y = topk+1\n",
    "      else:\n",
    "        if ctarget in topk_label:\n",
    "          y = np.where(topk_label == ctarget)[0][0]\n",
    "        else:\n",
    "          y = topk + 1\n",
    "      ys.append(y)\n",
    "    topk_df['y'] = ys\n",
    "  return topk_df, topk_labels\n",
    "\n",
    "def add_topn_features(feats_df, topn_labels, labels_and_scores, prefix, topk, model_idx):\n",
    "  features_labels = labels_and_scores[:, :, 1].astype('int32')\n",
    "  features_scores = labels_and_scores[:, :, 2].astype('float32')\n",
    "  _topk_scores = []\n",
    "  for i in range(features_scores.shape[0]):\n",
    "    counter = Counter()\n",
    "    for j in range(features_scores.shape[1]):\n",
    "      counter[features_labels[i, j]] += features_scores[i, j]\n",
    "    sub_topk_scores = [(tl, counter[tl]) for tl in topn_labels[i]]\n",
    "    _topk_scores.append(sub_topk_scores)\n",
    "  topk_scores = np.array(_topk_scores)[:, :, 1]\n",
    "  features_cols = [f'm{model_idx}_{prefix}_top{i}_score' for i in range(topk)]\n",
    "  for idx in range(len(features_cols)):\n",
    "    feats_df[features_cols[idx]] = topk_scores[:, idx]\n",
    "  return feats_df\n",
    "\n",
    "def add_features(args, feats_df, topn_labels,\n",
    "                 retrieval_train_ids_labels_and_scores,\n",
    "                 ransac_train_ids_labels_and_scores,\n",
    "                 nolandmark_ids_labels_and_scores,\n",
    "                 model_idx):\n",
    "  ret_topk_labels = retrieval_train_ids_labels_and_scores[:, :, 1].astype('int32')\n",
    "  ret_topk_scores = retrieval_train_ids_labels_and_scores[:, :, 2].astype('float32')\n",
    "  nolandmark_scores = nolandmark_ids_labels_and_scores[:, :, 2].astype('float32')\n",
    "  assert len(feats_df)==len(topn_labels)\n",
    "  assert len(ret_topk_labels)==len(topn_labels)\n",
    "  assert len(ret_topk_labels)==len(nolandmark_scores)\n",
    "\n",
    "  for i in range(args.top_k):\n",
    "    select_idx = np.array([topn_labels[:, i] == ret_topk_labels[:, j] for j in range(args.num_to_rerank)]).T\n",
    "    feats_df[f'm{model_idx}_retrieval_top{i}_max'] = np.max(ret_topk_scores*select_idx, axis=1)\n",
    "    feats_df[f'm{model_idx}_retrieval_top{i}_mean'] = np.mean(ret_topk_scores*select_idx, axis=1)\n",
    "\n",
    "  feats_df[f'm{model_idx}_retrieval_top0_top1_max'] = feats_df[f'm{model_idx}_retrieval_top0_max'] - \\\n",
    "                                                      feats_df[f'm{model_idx}_retrieval_top1_max']\n",
    "   # groupby\n",
    "  for i in range(args.top_k):\n",
    "    feats_df[f'top0'] = topn_labels[:, 0]\n",
    "    feats_df[f'm{model_idx}_gp_top{i}_retrieval_mean'] = feats_df.groupby([f'top0'])[f'm{model_idx}_retrieval_top{i}_max'].transform('mean')\n",
    "    feats_df[f'm{model_idx}_gp_top{i}_retrieval_max'] = feats_df.groupby([f'top0'])[f'm{model_idx}_retrieval_top{i}_max'].transform('max')\n",
    "    feats_df[f'm{model_idx}_gp_top{i}_retrieval_std'] = feats_df.groupby([f'top0'])[f'm{model_idx}_retrieval_top{i}_max'].transform('std')\n",
    "    del feats_df[f'top0']\n",
    "  return feats_df\n",
    "\n",
    "def add_multi_models_features(feats_df, train_labels_scores_list, model_num, topk):\n",
    "  for top in range(topk):\n",
    "    cols = [f'm{m}_nol_top{top}_score' for m in range(model_num)]\n",
    "    feats_df[f'nol_top{top}_score_mean'] = feats_df[cols].mean(axis=1)\n",
    "    feats_df[f'nol_top{top}_score_std'] = feats_df[cols].std(axis=1)\n",
    "\n",
    "    cols = [f'm{m}_retrieval_top{top}_max' for m in range(model_num)]\n",
    "    feats_df[f'retrieval_top{top}_max_mean'] = feats_df[cols].mean(axis=1)\n",
    "    feats_df[f'retrieval_top{top}_max_std'] = feats_df[cols].std(axis=1)\n",
    "\n",
    "    cols = [f'm{m}_retrieval_top{top}_score' for m in range(model_num)]\n",
    "    feats_df[f'retrieval_top{top}_score_mean'] = feats_df[cols].mean(axis=1)\n",
    "    feats_df[f'retrieval_top{top}_score_std'] = feats_df[cols].std(axis=1)\n",
    "  return feats_df\n",
    "\n",
    "def add_nl_features(feats_df, nolandmark_ids_labels_and_scores, model_idx, suffix=''):\n",
    "  nolandmark_scores = nolandmark_ids_labels_and_scores[:, :, 2].astype('float32')\n",
    "  nolandmark_cols = [f'm{model_idx}_nol{suffix}_top{i}_score' for i in range(nolandmark_scores.shape[1])]\n",
    "  for idx in range(len(nolandmark_cols)):\n",
    "    feats_df[nolandmark_cols[idx]] = nolandmark_scores[:, idx]\n",
    "  return feats_df\n",
    "\n",
    "def get_lgb_params(num_class=5):\n",
    "  params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 7,  # 2^max_depth - 1\n",
    "    'max_depth': 3,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'num_boost_round': 300,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'num_threads': 8,\n",
    "    'num_class': num_class,\n",
    "    'verbose': -1,\n",
    "  }\n",
    "  return params\n",
    "\n",
    "def get_split_ix(df, n_splits, fold, random_state=100):\n",
    "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "  for idx, (train_indices, valid_indices) in enumerate(kf.split(df[ID].values)):\n",
    "    if idx == fold:\n",
    "      return train_indices, valid_indices\n",
    "\n",
    "def get_train_val_data(df_trainval, n_splits, fold, random_state=100):\n",
    "  train_indices, valid_indices = get_split_ix(df_trainval, n_splits, fold, random_state=random_state)\n",
    "  df_train = df_trainval.iloc[train_indices]\n",
    "  df_val = df_trainval.iloc[valid_indices]\n",
    "  return df_train, df_val\n",
    "\n",
    "def train_model(args, params, lgbm_feats_df, feat_cols, folds_num, fold, random_state=100, topk=5):\n",
    "  df_train, df_val = get_train_val_data(lgbm_feats_df, folds_num, fold, random_state=random_state)\n",
    "  X_train = df_train[feat_cols]\n",
    "  y_train = df_train['y']\n",
    "  # print(np.unique(y_train))\n",
    "\n",
    "  X_val = df_val[feat_cols]\n",
    "  y_val = df_val['y']\n",
    "  print(X_train.shape, X_val.shape)\n",
    "\n",
    "  xgtrain = lgb.Dataset(X_train.values, y_train.values, feature_name=feat_cols)\n",
    "  xgvalid = lgb.Dataset(X_val.values, y_val.values, feature_name=feat_cols)\n",
    "\n",
    "  valid_sets = [xgtrain, xgvalid]\n",
    "  valid_names = ['train', 'valid']\n",
    "  params['metric'] = 'multi_logloss'\n",
    "  _eval_func = None\n",
    "  clf = lgb.train(params,\n",
    "                  xgtrain,\n",
    "                  valid_sets=valid_sets,\n",
    "                  valid_names=valid_names,\n",
    "                  evals_result={},\n",
    "                  num_boost_round=params['num_boost_round'],\n",
    "                  early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                  verbose_eval=10,\n",
    "                  feval=_eval_func)\n",
    "\n",
    "  return clf\n",
    "\n",
    "def get_lgbm_prediction_map(probs, img_ids, topk_labels, topk=5, show=True):\n",
    "  idxes = np.arange(len(img_ids))\n",
    "  preds = np.argmax(probs, axis=1)\n",
    "  if show:\n",
    "    print('pred result')\n",
    "    print(pd.Series(preds).value_counts().sort_index())\n",
    "\n",
    "  nl_idxes = preds == topk\n",
    "  preds[preds > 1] = 0\n",
    "  preds[topk_labels[idxes, preds] == -1] = 0  # none use top1\n",
    "  if show:\n",
    "    print('final pred result')\n",
    "    print(pd.Series(preds).value_counts().sort_index())\n",
    "\n",
    "  lgbm_preds = topk_labels[idxes, preds]\n",
    "  lgbm_scores = probs[idxes, preds]\n",
    "  lgbm_scores[nl_idxes] = (1-np.max(probs, axis=1))[nl_idxes]\n",
    "  predictions = {\n",
    "    img_ids[i]: {'score': lgbm_scores[i], 'class': lgbm_preds[i]} for i in range(len(img_ids))\n",
    "  }\n",
    "  return predictions\n",
    "\n",
    "def save_model(clf, model_fpath):\n",
    "  with open(model_fpath, 'wb') as dbfile:\n",
    "    pickle.dump(clf, dbfile)\n",
    "\n",
    "def load_model(model_fpath):\n",
    "  with open(model_fpath, 'rb') as dbfile:\n",
    "    clf = pickle.load(dbfile)\n",
    "  return clf\n",
    "\n",
    "def do_metric(args, pred_probs, pred_img_ids, pred_labels, targets):\n",
    "  predictions = get_lgbm_prediction_map(pred_probs, pred_img_ids, pred_labels, topk=args.top_k, show=False)\n",
    "  preds = []\n",
    "  scores = []\n",
    "  for image_id in pred_img_ids:\n",
    "    prediction = predictions[image_id]\n",
    "    label = prediction['class']\n",
    "    score = prediction['score']\n",
    "    preds.append(label)\n",
    "    scores.append(score)\n",
    "  score = GAP_vector(np.array(preds).astype(float), np.array(scores), targets.astype(float))\n",
    "  return score\n",
    "\n",
    "def do_lgbm(args, test_df,\n",
    "            en_train_ids_labels_and_scores,\n",
    "            train_labels_scores_list,\n",
    "            nolandmark_labels_scores_list,\n",
    "            ransac_labels_scores_list):\n",
    "  if args.kaggle:\n",
    "    lgbm_dir = f'{args.lgb_model_dir}/lgbm/{args.out_dir}'\n",
    "  else:\n",
    "    lgbm_dir = f'{RESULT_DIR}/models/lgbm/{args.out_dir}'\n",
    "  os.makedirs(lgbm_dir, exist_ok=True)\n",
    "\n",
    "  # generate features\n",
    "  en_train_ids_labels_and_scores = np.array(en_train_ids_labels_and_scores)\n",
    "  topk_labels = en_train_ids_labels_and_scores[:, :, 1].astype('int32')\n",
    "  topk_scores = en_train_ids_labels_and_scores[:, :, 2].astype('float32')\n",
    "\n",
    "  feats_df, topn_labels = generate_Xy(test_df, topk_labels, topk_scores, topk=args.top_k)\n",
    "  print(feats_df['y'].value_counts().sort_index())\n",
    "\n",
    "  M = len(train_labels_scores_list)\n",
    "  for i in range(M):\n",
    "    retrieval_train_ids_labels_and_scores = np.array(train_labels_scores_list[i])\n",
    "    nolandmark_ids_labels_and_scores = np.array(nolandmark_labels_scores_list[i])\n",
    "\n",
    "    if i == 0:\n",
    "      ransac_train_ids_labels_and_scores = np.array(ransac_labels_scores_list[i])\n",
    "      feats_df = add_topn_features(feats_df, topn_labels, ransac_train_ids_labels_and_scores, 'ransac', args.top_k, i)\n",
    "    else:\n",
    "      ransac_train_ids_labels_and_scores = None\n",
    "\n",
    "    feats_df = add_topn_features(feats_df, topn_labels, retrieval_train_ids_labels_and_scores, 'retrieval', args.top_k, i)\n",
    "    feats_df = add_nl_features(feats_df, nolandmark_ids_labels_and_scores, i)\n",
    "    feats_df = add_features(args, feats_df, topn_labels,\n",
    "                            retrieval_train_ids_labels_and_scores,\n",
    "                            ransac_train_ids_labels_and_scores,\n",
    "                            nolandmark_ids_labels_and_scores, i)\n",
    "  feats_df = add_multi_models_features(feats_df, train_labels_scores_list, model_num=M, topk=args.top_k)\n",
    "\n",
    "  # prepare data\n",
    "  folds_num = 5\n",
    "  models_num = 10\n",
    "  num_class = args.top_k + 2  # topk + nl + other\n",
    "  params = get_lgb_params(num_class=num_class)\n",
    "  base_feat_cols = [col for col in feats_df.columns if col not in [ID, CTARGET, 'y']]\n",
    "\n",
    "  np.random.seed(100)\n",
    "  model_feat_cols = []\n",
    "  model_random_states = []\n",
    "  for model_idx in range(models_num):\n",
    "    feat_cols = np.random.choice(base_feat_cols, size=int(1.0 * len(base_feat_cols)), replace=False).tolist()\n",
    "    model_feat_cols.append(feat_cols)\n",
    "    model_random_states.append(np.random.randint(0, 100000))\n",
    "\n",
    "  # train and predict\n",
    "  feat_imp_list = []\n",
    "  base_pred_probs = []\n",
    "  for model_idx in range(models_num):\n",
    "    feat_cols = model_feat_cols[model_idx]\n",
    "    random_state = model_random_states[model_idx]\n",
    "    print(f'model{model_idx} - random_state{random_state}')\n",
    "    print(len(feat_cols), feat_cols)\n",
    "    pred_probs = np.zeros((len(feats_df), num_class))\n",
    "    feat_imp = pd.Series(index=feat_cols, data=0.)\n",
    "    for fold_idx in range(folds_num):\n",
    "      print(fold_idx, '*' * 50)\n",
    "      model_fpath = f'{lgbm_dir}/m{models_num}.{model_idx}_f{folds_num}.{fold_idx}_top{args.top_k}_feats{len(feat_cols)}_{args.valid_num}.pkl'\n",
    "      if args.do_train:\n",
    "        clf = train_model(args, params, feats_df, feat_cols, folds_num, fold_idx, random_state=random_state, topk=args.top_k)\n",
    "        save_model(clf, model_fpath)\n",
    "      else:\n",
    "        clf = load_model(model_fpath)\n",
    "\n",
    "      fold_feat_imp = pd.Series(data=clf.feature_importance(), index=clf.feature_name())\n",
    "      feat_imp += fold_feat_imp / float(folds_num)\n",
    "\n",
    "      if args.do_valid:\n",
    "        _, valid_indices = get_split_ix(feats_df, folds_num, fold_idx, random_state=random_state)\n",
    "        valid_probs = clf.predict(feats_df.iloc[valid_indices][feat_cols])\n",
    "        pred_probs[valid_indices] = valid_probs\n",
    "      elif args.do_test:\n",
    "        test_probs = clf.predict(feats_df[feat_cols])\n",
    "        pred_probs += test_probs / folds_num\n",
    "    feat_imp_list.append(feat_imp)\n",
    "    base_pred_probs.append(pred_probs)\n",
    "\n",
    "  pred_img_ids = feats_df[ID].values\n",
    "  pred_labels = topn_labels\n",
    "  pred_targets = feats_df[CTARGET].values\n",
    "\n",
    "  # select models\n",
    "  selected_model_fpath = f'{lgbm_dir}/selected_indices.npy'\n",
    "  if args.do_valid:\n",
    "    scores = []\n",
    "    for pred_probs in base_pred_probs:\n",
    "      score = do_metric(args, pred_probs, pred_img_ids, pred_labels, pred_targets)\n",
    "      scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    print('selected before: [', ', '.join([f'{s:.4f}' for s in scores.tolist()]) + ']')\n",
    "    selected_idxes = np.argsort(scores)[::-1][:5] # top5\n",
    "    print('selected after: [', ', '.join([f'{s:.4f}' for s in scores[selected_idxes].tolist()]) + ']')\n",
    "    np.save(selected_model_fpath, selected_idxes)\n",
    "\n",
    "    pred_probs = np.mean(np.array(base_pred_probs)[selected_idxes], axis=0)\n",
    "  elif args.do_test:\n",
    "    selected_idxes = np.load(selected_model_fpath)\n",
    "    pred_probs = np.mean(np.array(base_pred_probs)[selected_idxes], axis=0)\n",
    "  else:\n",
    "    selected_idxes = np.arange(models_num)\n",
    "    pred_probs = None\n",
    "\n",
    "  # feature importance\n",
    "  feat_imp = pd.Series(index=base_feat_cols, data=0.)\n",
    "  for selected_idx in selected_idxes:\n",
    "    feat_imp += feat_imp_list[selected_idx].reindex(index=base_feat_cols).fillna(0) / len(selected_idxes)\n",
    "  print(feat_imp.sort_values(ascending=False)[:50])\n",
    "\n",
    "  return pred_probs, pred_img_ids, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids(test_ids, sub_test_ids):\n",
    "  if test_ids is None:\n",
    "    test_ids = sub_test_ids\n",
    "  else:\n",
    "    assert np.array_equal(test_ids, sub_test_ids)\n",
    "  return test_ids\n",
    "\n",
    "def norm(test_embeddings):\n",
    "  test_embeddings = test_embeddings / (np.linalg.norm(test_embeddings, ord=2, axis=1, keepdims=True) + EPS)\n",
    "  return test_embeddings\n",
    "\n",
    "def merge_retrieval(train_ids_labels_and_scores, sub_train_ids_labels_and_scores, weight=1.0):\n",
    "  for test_index in range(len(sub_train_ids_labels_and_scores)):\n",
    "    sub_train_ids_labels_and_scores[test_index] = [\n",
    "      (train_id, int(label), float(score) * weight) for train_id, label, score in sub_train_ids_labels_and_scores[test_index]\n",
    "    ]\n",
    "\n",
    "  if train_ids_labels_and_scores is None:\n",
    "    train_ids_labels_and_scores = sub_train_ids_labels_and_scores\n",
    "  else:\n",
    "    for test_index in range(len(sub_train_ids_labels_and_scores)):\n",
    "      train_ids_labels_and_scores_map = {\n",
    "        train_id: (train_id, int(label), float(score)) for train_id, label, score in train_ids_labels_and_scores[test_index]\n",
    "      }\n",
    "      for train_id, label, score in sub_train_ids_labels_and_scores[test_index]:\n",
    "        train_ids_labels_and_scores_map[train_id] = (train_id, int(label), train_ids_labels_and_scores_map.get(train_id, (None, None, 0.))[-1] + float(score))\n",
    "      train_ids_labels_and_scores[test_index] = sorted([v for _, v in train_ids_labels_and_scores_map.items()], key=lambda x: -x[-1])\n",
    "  return train_ids_labels_and_scores\n",
    "\n",
    "def np_save(fname, v, kaggle=False):\n",
    "  if not kaggle:\n",
    "    np.save(fname, v)\n",
    "\n",
    "def get_predictions(args, en_cfgs, superpointglue_net,\n",
    "                    labelmap, train_df, test_df, nolandmark_df,\n",
    "                    test_image_dir, train_image_dir, nolandmark_image_dir):\n",
    "  test_ids, test_embeddings_list = None, []\n",
    "  train_ids, train_embeddings_list = None, []\n",
    "  nolandmark_ids, nolandmark_embeddings_list = None, []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    \"\"\"Gets predictions using embedding similarity and local feature reranking.\"\"\"\n",
    "\n",
    "    for en_cfg in en_cfgs:\n",
    "      net = en_cfg['net']\n",
    "      _args = Namespace(**{\n",
    "        'out_dir': en_cfg['out_dir'],\n",
    "        'kaggle': args.kaggle,\n",
    "        'scale': en_cfg.get('scale', None),\n",
    "        'img_size': en_cfg.get('img_size', None),\n",
    "        'predict_epoch': en_cfg['predict_epoch'],\n",
    "        'batch_size': en_cfg['batch_size'],\n",
    "        'preprocessing': en_cfg['preprocessing'],\n",
    "        'overwrite': args.overwrite,\n",
    "      })\n",
    "\n",
    "      sub_test_ids, sub_test_embeddings = extract_global_features(_args, net, test_df, test_image_dir,\n",
    "                                                                  dataset='test')\n",
    "      sub_train_ids, sub_train_embeddings = extract_global_features(_args, net, train_df, train_image_dir,\n",
    "                                                                    dataset='train')\n",
    "      sub_nolandmark_ids, sub_nolandmark_embeddings = extract_global_features(_args, net, nolandmark_df,\n",
    "                                                                      nolandmark_image_dir, dataset='nolandmark')\n",
    "      test_ids = get_img_ids(test_ids, sub_test_ids)\n",
    "      train_ids = get_img_ids(train_ids, sub_train_ids)\n",
    "      nolandmark_ids = get_img_ids(nolandmark_ids, sub_nolandmark_ids)\n",
    "      test_embeddings_list.append(sub_test_embeddings)\n",
    "      train_embeddings_list.append(sub_train_embeddings)\n",
    "      nolandmark_embeddings_list.append(sub_nolandmark_embeddings)\n",
    "\n",
    "  en_test_embeddings = []\n",
    "  en_train_embeddings = []\n",
    "  en_nolandmark_embeddings = []\n",
    "  for i,en_cfg in enumerate(en_cfgs):\n",
    "    en_test_embeddings.append(test_embeddings_list[i] * en_cfg['weight'])\n",
    "    en_train_embeddings.append(train_embeddings_list[i] * en_cfg['weight'])\n",
    "    en_nolandmark_embeddings.append(nolandmark_embeddings_list[i] * en_cfg['weight'])\n",
    "\n",
    "  en_test_embeddings = norm(np.concatenate(en_test_embeddings, axis=1))\n",
    "  print('test_embeddings shape', en_test_embeddings.shape)\n",
    "  en_train_embeddings = norm(np.concatenate(en_train_embeddings, axis=1))\n",
    "  print('train_embeddings shape', en_train_embeddings.shape)\n",
    "  en_nolandmark_embeddings = norm(np.concatenate(en_nolandmark_embeddings, axis=1))\n",
    "  print('nolandmark_embeddings shape', en_nolandmark_embeddings.shape)\n",
    "\n",
    "  test_embeddings_list.insert(0, en_test_embeddings)\n",
    "  train_embeddings_list.insert(0, en_train_embeddings)\n",
    "  nolandmark_embeddings_list.insert(0, en_nolandmark_embeddings)\n",
    "\n",
    "  nolandmark_labelmap = dict([(i, -1) for i in nolandmark_ids])\n",
    "  train_labels_scores_list = []\n",
    "  nolandmark_labels_scores_list = []\n",
    "  ransac_labels_scores_list = []\n",
    "  en_train_ids_labels_and_scores = None\n",
    "\n",
    "  cache_dir = f'{RESULT_DIR}/cache/{args.out_dir}/'\n",
    "  if not args.kaggle:\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "  for i in tqdm(range(len(test_embeddings_list))):\n",
    "    train_embeddings = train_embeddings_list[i]\n",
    "    test_embeddings = test_embeddings_list[i]\n",
    "    nolandmark_embeddings = nolandmark_embeddings_list[i]\n",
    "\n",
    "    retrieval_fname = f'{cache_dir}/m{i}_retrieval_{args.valid_num}.npy'\n",
    "    if ope(retrieval_fname) and False:\n",
    "      print('load', retrieval_fname)\n",
    "      train_ids_labels_and_scores = np.load(retrieval_fname, allow_pickle=True).tolist()\n",
    "    else:\n",
    "      train_ids_labels_and_scores = do_retrieval(args, labelmap, train_ids,\n",
    "                                               train_embeddings, test_embeddings,\n",
    "                                               args.num_to_rerank, gallery_set='index')\n",
    "      np_save(retrieval_fname, train_ids_labels_and_scores, kaggle=args.kaggle)\n",
    "\n",
    "    nolandmark_ids_labels_and_scores = do_retrieval(args, nolandmark_labelmap, nolandmark_ids,\n",
    "                                                    nolandmark_embeddings, test_embeddings,\n",
    "                                                    args.num_to_rerank, gallery_set='nolandmark')\n",
    "\n",
    "    def do_ransac(ransac_fname, test_ids, test_image_dir, train_image_dir, train_ids_labels_and_scores):\n",
    "      if ope(ransac_fname):\n",
    "        print('load', ransac_fname)\n",
    "        ransac_train_ids_labels_and_scores = np.load(ransac_fname, allow_pickle=True)\n",
    "      else:\n",
    "        ransac_train_ids_labels_and_scores = None\n",
    "        if args.ransac:\n",
    "          ransac_train_ids_labels_and_scores = copy.deepcopy(train_ids_labels_and_scores)\n",
    "          if args.ransac_parts > 1:\n",
    "            block = len(ransac_train_ids_labels_and_scores) // args.ransac_parts + 1\n",
    "            ransac_train_ids_labels_and_scores = \\\n",
    "              ransac_train_ids_labels_and_scores[args.ransac_part * block:(args.ransac_part + 1) * block]\n",
    "            test_ids = test_ids[args.ransac_part * block:(args.ransac_part + 1) * block]\n",
    "\n",
    "          cache_num_inliers_dict = None\n",
    "          if args.kaggle:\n",
    "            cache_num_inliers_dict = get_whole_cached_num_inliers(args)\n",
    "          for test_index, test_id in tqdm(enumerate(test_ids), total=len(test_ids), desc='do ransac'):\n",
    "            ransac_train_ids_labels_and_scores[test_index] = rescore_and_rerank_by_num_inliers(\n",
    "              args, test_image_dir, train_image_dir, test_id,\n",
    "              ransac_train_ids_labels_and_scores[test_index],\n",
    "              superpointglue_net=superpointglue_net, ignore_global_score=True, do_sort=False,\n",
    "                cache_num_inliers_dict=cache_num_inliers_dict)\n",
    "          if args.kaggle:\n",
    "            save_whole_cached_num_inliers(args, cache_num_inliers_dict)\n",
    "\n",
    "          if args.ransac_parts>1:\n",
    "            return\n",
    "          np_save(ransac_fname, ransac_train_ids_labels_and_scores, kaggle=args.kaggle)\n",
    "      return ransac_train_ids_labels_and_scores\n",
    "\n",
    "    ransac_fname = f'{cache_dir}/m{i}_ransac_{args.ransac_type}_{args.valid_num}.npy'\n",
    "    if i == 0:\n",
    "      ransac_train_ids_labels_and_scores = do_ransac(ransac_fname, test_ids, test_image_dir,\n",
    "                                                       train_image_dir, train_ids_labels_and_scores)\n",
    "    else:\n",
    "      ransac_train_ids_labels_and_scores = None\n",
    "\n",
    "    nl_ransac_fname = f'{cache_dir}/m{i}_ransac_nl_{args.ransac_type}_{args.valid_num}.npy'\n",
    "    if i > 0:\n",
    "      en_train_ids_labels_and_scores = merge_retrieval(en_train_ids_labels_and_scores,\n",
    "                                                       copy.deepcopy(train_ids_labels_and_scores),\n",
    "                                                        weight=en_cfgs[i-1]['weight'])\n",
    "\n",
    "    train_labels_scores_list.append(train_ids_labels_and_scores)\n",
    "    nolandmark_labels_scores_list.append(nolandmark_ids_labels_and_scores)\n",
    "    ransac_labels_scores_list.append(ransac_train_ids_labels_and_scores)\n",
    "\n",
    "  en_train_ids_labels_and_scores = [i[:args.num_to_rerank] for i in en_train_ids_labels_and_scores]\n",
    "  en_fname = f'{cache_dir}/en_{args.valid_num}.npy'\n",
    "  if ope(en_fname) and False:\n",
    "    print('load', en_fname)\n",
    "    en_train_ids_labels_and_scores = np.load(en_fname, allow_pickle=True).tolist()\n",
    "  else:\n",
    "    cache_num_inliers_dict = None\n",
    "    if args.kaggle:\n",
    "      cache_num_inliers_dict = get_whole_cached_num_inliers(args)\n",
    "    for test_index, test_id in tqdm(enumerate(test_ids), total=len(test_ids), desc='do ransac'):\n",
    "      en_train_ids_labels_and_scores[test_index] = rescore_and_rerank_by_num_inliers(\n",
    "        args, test_image_dir, train_image_dir, test_id,\n",
    "        en_train_ids_labels_and_scores[test_index], superpointglue_net=superpointglue_net,\n",
    "        cache_num_inliers_dict=cache_num_inliers_dict)\n",
    "    np_save(en_fname, en_train_ids_labels_and_scores, kaggle=args.kaggle)\n",
    "\n",
    "  pred_probs, pred_img_ids, pred_labels = do_lgbm(args, test_df,\n",
    "                                                  en_train_ids_labels_and_scores,\n",
    "                                                  train_labels_scores_list,\n",
    "                                                  nolandmark_labels_scores_list,\n",
    "                                                  ransac_labels_scores_list\n",
    "                                                  )\n",
    "\n",
    "  predictions = get_lgbm_prediction_map(pred_probs, pred_img_ids, pred_labels, topk=args.top_k)\n",
    "\n",
    "  del test_embeddings_list\n",
    "  del train_embeddings_list\n",
    "  del nolandmark_embeddings_list\n",
    "  gc.collect()\n",
    "  return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, en_cfg):\n",
    "  _module = import_module(f'net_torch.{en_cfg[\"module\"]}')\n",
    "  net = getattr(_module, en_cfg['model_name'])(args=Namespace(**{\n",
    "    'num_classes': en_cfg['num_classes'],\n",
    "    'in_channels': en_cfg['in_channels'],\n",
    "    'can_print': True,\n",
    "  }))\n",
    "\n",
    "  if args.kaggle:\n",
    "    model_file = f'/kaggle/input/models2/{en_cfg[\"out_dir\"]}/{en_cfg[\"predict_epoch\"]}.pth'\n",
    "  else:\n",
    "    model_file = f'{RESULT_DIR}/models/{en_cfg[\"out_dir\"]}/{en_cfg[\"predict_epoch\"]}.pth'\n",
    "  print('load model file: %s' % model_file)\n",
    "  checkpoint = torch.load(model_file)\n",
    "  net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "  net = DataParallel(net)\n",
    "  net.cuda()\n",
    "  net.eval()\n",
    "  return net\n",
    "\n",
    "\n",
    "def load_superpointglue_model():\n",
    "  from net_torch.superpointglue.matching import Matching\n",
    "  if args.kaggle:\n",
    "    model_dir = '/kaggle/input/superpointglue-models/superpoint_superglue_models'\n",
    "  else:\n",
    "    model_dir = f'{DATA_DIR}/input/superpoint_superglue_models'\n",
    "  config = {\n",
    "    'superpoint': {\n",
    "      'nms_radius': 4,\n",
    "      'keypoint_threshold': 0.005,\n",
    "      'max_keypoints': 1024,\n",
    "      'model_dir': model_dir,\n",
    "    },\n",
    "    'superglue': {\n",
    "      'weights': 'outdoor',  # indoor, outdoor\n",
    "      'sinkhorn_iterations': 20,\n",
    "      'match_threshold': 0.2,\n",
    "      'model_dir': model_dir,\n",
    "    }\n",
    "  }\n",
    "  superpointglue = Matching(config).eval().cuda()\n",
    "  return superpointglue\n",
    "\n",
    "def main():\n",
    "  start_time = timer()\n",
    "  if args.kaggle and args.debug:\n",
    "    args.nolandmark_num = 20\n",
    "  print(f'nolandmark_num: {args.nolandmark_num}')\n",
    "  args.can_print = True\n",
    "\n",
    "  if args.gpus is not None:\n",
    "    print('using gpu ' + args.gpus)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n",
    "\n",
    "  en_cfgs = eval(args.en_cfgs)\n",
    "  for en_cfg in en_cfgs:\n",
    "    print(f'weight: {en_cfg[\"weight\"]}')\n",
    "    if args.gpus is None:\n",
    "      en_cfg['net'] = None\n",
    "    else:\n",
    "      en_cfg['net'] = load_model(args, en_cfg)\n",
    "\n",
    "  superpointglue_net = None\n",
    "  if (args.gpus is not None) and (args.ransac):\n",
    "    superpointglue_net = load_superpointglue_model()\n",
    "\n",
    "  if args.kaggle:\n",
    "    INPUT_DIR = os.path.join('..', 'input')\n",
    "    DATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2020')\n",
    "    TEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "    TRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "    TRAIN_LABELMAP_PATH = os.path.join(DATASET_DIR, 'train.csv')\n",
    "    test_df = pd.read_csv(os.path.join(DATASET_DIR, 'sample_submission.csv'))\n",
    "    train_df = pd.read_csv(TRAIN_LABELMAP_PATH)\n",
    "\n",
    "    if not args.debug and len(train_df) == NUM_PUBLIC_TRAIN_IMAGES:\n",
    "      print(\n",
    "        f'Found {NUM_PUBLIC_TRAIN_IMAGES} training images. Copying sample submission.'\n",
    "      )\n",
    "      save_submission_csv(args, DATASET_DIR)\n",
    "      return\n",
    "  else:\n",
    "    DATASET_DIR = DATA_DIR\n",
    "    TEST_IMAGE_DIR = f'{DATA_DIR}/images/train'\n",
    "    TRAIN_IMAGE_DIR = f'{DATA_DIR}/images/train'\n",
    "    TRAIN_LABELMAP_PATH = f'{DATA_DIR}/input/train_labelmap_lgb.csv'\n",
    "\n",
    "    c_test_df = pd.read_csv(f'{DATA_DIR}/raw/recognition_solution_v2.1.csv')\n",
    "    v2c_df = pd.read_csv(f'{DATA_DIR}/split/train2020.csv')\n",
    "    mapping_df = v2c_df[[TARGET, CTARGET]].drop_duplicates(TARGET, keep='first')\n",
    "    landmark_test2019_df = c_test_df[c_test_df[CTARGET].isin(mapping_df[CTARGET].astype(str))]\n",
    "    c_test_df[CTARGET] = c_test_df[CTARGET].astype(str)\n",
    "\n",
    "    if args.debug:\n",
    "      test_num = 10000\n",
    "    else:\n",
    "      test_num = args.valid_num\n",
    "    index_num = test_num*10\n",
    "    test_landmark_num = int(test_num * 0.2)\n",
    "    landmark_test2019_df[CTARGET] = landmark_test2019_df[CTARGET].astype(int)\n",
    "    print('test 2019 landmark num', len(landmark_test2019_df))\n",
    "    print('test 2019 landmark nunique', landmark_test2019_df[CTARGET].nunique())\n",
    "\n",
    "    num = test_landmark_num - len(landmark_test2019_df)\n",
    "    v2xc_df = pd.read_csv(f'{DATA_DIR}/split/v2xc/random_train_cv0.csv')\n",
    "    v2xc_df = v2xc_df[~v2xc_df[ID].isin(v2c_df[ID])]\n",
    "    v2xc_df = v2xc_df.merge(mapping_df[[TARGET, CTARGET]], how='left', on=TARGET)\n",
    "    v2xc_landmark_df = v2xc_df.drop_duplicates(TARGET, keep='first')\n",
    "    v2xc_landmark = v2xc_landmark_df.sample(num//2, random_state=1, replace=False)[TARGET]\n",
    "    v2xc_landmark_df = v2xc_df[v2xc_df[TARGET].isin(v2xc_landmark)]\n",
    "    v2xc_landmark_df = v2xc_landmark_df.groupby(TARGET).head(20)\n",
    "    v2xc_landmark_df = v2xc_landmark_df.sample(num, random_state=1, replace=False)\n",
    "    # print(v2xc_landmark_df[CTARGET].value_counts()[:20])\n",
    "    print('v2xc landmark num', len(v2xc_landmark_df))\n",
    "    print('v2xc landmark nunique', v2xc_landmark_df[CTARGET].nunique())\n",
    "\n",
    "    landmark_test_df = pd.concat((landmark_test2019_df, v2xc_landmark_df[[ID, CTARGET]]))\n",
    "    print('landmark num', len(landmark_test_df))\n",
    "    # print(landmark_test_df[CTARGET].value_counts()[:20])\n",
    "\n",
    "    nolandmark_test_df = c_test_df[c_test_df[CTARGET]=='nan']\n",
    "    nolandmark_test_df = nolandmark_test_df.sample(test_num - len(landmark_test_df), random_state=1, replace=False)\n",
    "    print('nolandmark num', len(nolandmark_test_df))\n",
    "    test_df = pd.concat((landmark_test_df, nolandmark_test_df[[ID, CTARGET]]))\n",
    "    test_df.to_csv(f'{DATA_DIR}/input/valid_v2_{test_num}.csv', index=False)\n",
    "\n",
    "    v2c_train_df = pd.read_csv(f'{DATA_DIR}/split/v2c/random_train_cv0.csv')\n",
    "    v2c_train_df = v2c_train_df.merge(v2c_df[[ID, CTARGET]], how='left', on=ID)\n",
    "    v2c_in_test = v2c_train_df[CTARGET].isin(landmark_test_df[CTARGET])\n",
    "    v2c_index = v2c_train_df[v2c_in_test]\n",
    "    v2c_other_index = v2c_train_df[~v2c_in_test].sample(index_num - len(v2c_index), random_state=1, replace=False)\n",
    "    print('v2c index num', len(v2c_index))\n",
    "    print('v2c other index num', len(v2c_other_index))\n",
    "    train_df = pd.concat((v2c_index, v2c_other_index))\n",
    "    train_df[TARGET] = train_df[CTARGET].values\n",
    "    train_df[[ID, TARGET]].to_csv(TRAIN_LABELMAP_PATH, index=False)\n",
    "\n",
    "  if args.kaggle:\n",
    "    nolandmark_df = pd.read_csv(os.path.join(INPUT_DIR, '2019test-5k', 'nolandmark_v1.csv'))\n",
    "    NOLANDMARK_IMAGE_DIR = os.path.join(INPUT_DIR, '2019test', 'test')\n",
    "  else:\n",
    "    nolandmark_df = pd.read_csv(f'{DATA_DIR}/split/nolandmark_v1.csv')\n",
    "    nolandmark_df = nolandmark_df[~nolandmark_df[ID].isin(test_df[ID])]\n",
    "    NOLANDMARK_IMAGE_DIR = f'{DATA_DIR}/images/test'\n",
    "\n",
    "  nolandmark_df = nolandmark_df.sample(args.nolandmark_num, random_state=1, replace=False)\n",
    "  print('sample nolandmark num', len(nolandmark_df))\n",
    "\n",
    "  if args.kaggle and args.debug:\n",
    "      test_df = test_df[:10]\n",
    "      train_df = train_df[:10]\n",
    "\n",
    "  print('test num', len(test_df))\n",
    "  print('train num', len(train_df))\n",
    "\n",
    "  labelmap = load_labelmap(TRAIN_LABELMAP_PATH)\n",
    "\n",
    "  args.out_dir = f'{args.en_cfgs}_lgb'\n",
    "  predictions = get_predictions(args, en_cfgs, superpointglue_net,\n",
    "                                labelmap, train_df, test_df, nolandmark_df,\n",
    "                                TEST_IMAGE_DIR, TRAIN_IMAGE_DIR, NOLANDMARK_IMAGE_DIR)\n",
    "  submit_fname = save_submission_csv(args, DATASET_DIR, predictions)\n",
    "\n",
    "  if not args.kaggle:\n",
    "    pred_df = pd.read_csv(submit_fname).fillna('')\n",
    "    assert np.array_equal(np.sort(test_df[ID].values), np.sort(pred_df[ID].values))\n",
    "    pred_df = pd.merge(test_df[[ID]], pred_df, on=ID, how='left')\n",
    "    pred = [i.split(' ')[0] if i != '' else np.nan for i in pred_df[CTARGET]]\n",
    "    conf = [i.split(' ')[1] if i != '' else np.nan for i in pred_df[CTARGET]]\n",
    "    gap = GAP_vector(np.array(pred).astype(float), np.array(conf).astype(float), test_df[CTARGET].astype(float))\n",
    "    print('gap: %.4f' % gap)\n",
    "    shutil.copy(submit_fname, submit_fname.replace('.csv', f'_{gap:.4f}.csv'))\n",
    "    time = (timer() - start_time) / 60\n",
    "    print('run time: %.2fmin' % time)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
