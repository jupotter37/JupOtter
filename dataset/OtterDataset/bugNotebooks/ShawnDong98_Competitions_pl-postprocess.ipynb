{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cabc9e3-85ea-4a75-88b6-44e37de4835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90960920-1c02-42d4-b1c7-5e6f5f632a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.optim import create_optimizer_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c17f17-e3a6-4baf-bb9d-78e37dbdb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path('../datasets/kaggle/happy-whale-and-dolphin/')\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "\n",
    "DATA_ROOT_DIR = INPUT_DIR / \"happy-whale-and-dolphin-backfin\"\n",
    "TRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\n",
    "TEST_DIR = DATA_ROOT_DIR / \"test_images\"\n",
    "TRAIN_CSV_PATH = DATA_ROOT_DIR / \"train.csv\"\n",
    "SAMPLE_SUBMISSION_CSV_PATH = DATA_ROOT_DIR / \"sample_submission.csv\"\n",
    "PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"720\" / \"submission.csv\"\n",
    "IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"backfin\" / \"ids_without_backfin.npy\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "ENCODER_CLASSES_PATH = OUTPUT_DIR / \"encoder_classes.npy\"\n",
    "TEST_CSV_PATH = OUTPUT_DIR / \"test.csv\"\n",
    "TRAIN_CSV_ENCODED_FOLDED_PATH = OUTPUT_DIR / \"train_encoded_folded.csv\"\n",
    "CHECKPOINTS_DIR = OUTPUT_DIR / \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46e2beb-f364-4d00-9ba0-e7a87c022be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 4\n",
    "MODEL_NAME = \"convnext_base_384_in22ft1k\"\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 12\n",
    "DEBUG = False\n",
    "SUBMISSION_CSV_PATH = OUTPUT_DIR / (\"pl_\"  + f\"ensemble_{FOLD}_\" + \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4163831d-d870-47ce-b1f7-c76674777749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(id: str, dir: Path) -> str:\n",
    "    return f\"{dir / id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eab89e1-c9da-4927-b2fd-0c8371c7178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>10938</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1453</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>5158</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4031</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>7726</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species  individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale          10938   \n",
       "1  000562241d384d.jpg      humpback_whale           1453   \n",
       "2  0007c33415ce37.jpg  false_killer_whale           5158   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin           4031   \n",
       "4  00087baf5cef7a.jpg      humpback_whale           7726   \n",
       "\n",
       "                                          image_path  kfold  \n",
       "0  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "1  ../datasets/kaggle/happy-whale-and-dolphin/hap...    1.0  \n",
       "2  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "3  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "4  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG: train_df = pd.read_csv(TRAIN_CSV_PATH)[:1000]\n",
    "else: train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train_df[\"image_path\"] = train_df[\"image\"].apply(get_image_path, dir=TRAIN_DIR)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_df[\"individual_id\"] = encoder.fit_transform(train_df[\"individual_id\"])\n",
    "np.save(ENCODER_CLASSES_PATH, encoder.classes_)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "for fold, (_, val_) in enumerate(skf.split(X=train_df, y=train_df.individual_id)):\n",
    "    train_df.loc[val_, \"kfold\"] = fold\n",
    "    \n",
    "train_df.to_csv(TRAIN_CSV_ENCODED_FOLDED_PATH, index=False)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425d4bad-fb92-4ff5-936c-13152c6405d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>image_path</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                         image_path  \\\n",
       "0  000110707af0ba.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "1  0006287ec424cb.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "2  000809ecb2ccad.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "3  00098d1376dab2.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "4  000b8d89c738bd.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "\n",
       "   individual_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sample submission csv as template\n",
    "if DEBUG: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)[:100]\n",
    "else: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)\n",
    "test_df[\"image_path\"] = test_df[\"image\"].apply(get_image_path, dir=TEST_DIR)\n",
    "\n",
    "test_df.drop(columns=[\"predictions\"], inplace=True)\n",
    "\n",
    "# Dummy id\n",
    "test_df[\"individual_id\"] = 0\n",
    "\n",
    "test_df.to_csv(TEST_CSV_PATH, index=False)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085545dd-4e73-4379-bf31-298d0045ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Optional[Callable] = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = self.df[\"image\"].values\n",
    "        self.image_paths = self.df[\"image_path\"].values\n",
    "        self.targets = self.df[\"individual_id\"].values\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_name = self.image_names[index]\n",
    "\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = self.targets[index]\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return {\"image_name\": image_name, \"image\": image, \"target\": target}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6eec81-c93d-42b6-b0bf-70225c9161bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_csv_encoded_folded: str,\n",
    "        test_csv: str,\n",
    "        val_fold: float,\n",
    "        image_size: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_df = pd.read_csv(train_csv_encoded_folded)\n",
    "        self.test_df = pd.read_csv(test_csv)\n",
    "        \n",
    "        self.transform = create_transform(\n",
    "            input_size=(self.hparams.image_size, self.hparams.image_size),\n",
    "            crop_pct=1.0,\n",
    "        )\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Split train df using fold\n",
    "            train_df = self.train_df[self.train_df.kfold != self.hparams.val_fold].reset_index(drop=True)\n",
    "            val_df = self.train_df[self.train_df.kfold == self.hparams.val_fold].reset_index(drop=True)\n",
    "\n",
    "            self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform)\n",
    "            self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.train_dataset, train=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.test_dataset)\n",
    "\n",
    "    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=train,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1eb18b-b8bd-4ce0-addd-4530553ad3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "# Added type annotations, device, and 16bit support\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta + m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float,\n",
    "        m: float,\n",
    "        easy_margin: bool,\n",
    "        ls_eps: float,\n",
    "    ):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        # Enable 16 bit precision\n",
    "        cosine = cosine.to(torch.float32)\n",
    "\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0aa92c8-ed67-42e8-88e0-1234182aae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        pretrained: bool,\n",
    "        drop_rate: float,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        arc_s: float,\n",
    "        arc_m: float,\n",
    "        arc_easy_margin: bool,\n",
    "        arc_ls_eps: float,\n",
    "        optimizer: str,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        len_train_dl: int,\n",
    "        epochs:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n",
    "        self.embedding = nn.Linear(self.model.get_classifier().in_features, embedding_size)\n",
    "        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        self.arc = ArcMarginProduct(\n",
    "            in_features=embedding_size,\n",
    "            out_features=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            easy_margin=arc_easy_margin,\n",
    "            ls_eps=arc_ls_eps,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.model(images)\n",
    "        embeddings = self.embedding(features)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=self.hparams.optimizer,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            steps_per_epoch=self.hparams.len_train_dl,\n",
    "            epochs=self.hparams.epochs,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n",
    "        images, targets = batch[\"image\"], batch[\"target\"]\n",
    "\n",
    "        embeddings = self(images)\n",
    "        outputs = self.arc(embeddings, targets, self.device)\n",
    "\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        \n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed5a5ef4-01de-4356-921b-0e6f43f3b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_module(checkpoint_path: str, device: torch.device) -> LitModule:\n",
    "    module = LitModule.load_from_checkpoint(checkpoint_path)\n",
    "    module.to(device)\n",
    "    module.eval()\n",
    "\n",
    "    return module\n",
    "\n",
    "def load_dataloaders(\n",
    "    train_csv_encoded_folded: str,\n",
    "    test_csv: str,\n",
    "    val_fold: float,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    datamodule.setup()\n",
    "\n",
    "    train_dl = datamodule.train_dataloader()\n",
    "    val_dl = datamodule.val_dataloader()\n",
    "    test_dl = datamodule.test_dataloader()\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "def load_encoder() -> LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(ENCODER_CLASSES_PATH, allow_pickle=True)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embeddings(\n",
    "    module: pl.LightningModule, dataloader: DataLoader, encoder: LabelEncoder, stage: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    all_image_names = []\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Creating {stage} embeddings\"):\n",
    "        image_names = batch[\"image_name\"]\n",
    "        images = batch[\"image\"].to(module.device)\n",
    "        targets = batch[\"target\"].to(module.device)\n",
    "\n",
    "        embeddings = module(images)\n",
    "\n",
    "        all_image_names.append(image_names)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     break\n",
    "\n",
    "    all_image_names = np.concatenate(all_image_names)\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    all_embeddings = normalize(all_embeddings, axis=1, norm=\"l2\")\n",
    "    all_targets = encoder.inverse_transform(all_targets)\n",
    "\n",
    "    return all_image_names, all_embeddings, all_targets\n",
    "\n",
    "\n",
    "def create_and_search_index(embedding_size: int, train_embeddings: np.ndarray, val_embeddings: np.ndarray, k: int):\n",
    "    index = faiss.IndexFlatIP(embedding_size)\n",
    "    index.add(train_embeddings)\n",
    "    D, I = index.search(val_embeddings, k=k)  # noqa: E741\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def create_val_targets_df(\n",
    "    train_targets: np.ndarray, val_image_names: np.ndarray, val_targets: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    allowed_targets = np.unique(train_targets)\n",
    "    val_targets_df = pd.DataFrame(np.stack([val_image_names, val_targets], axis=1), columns=[\"image\", \"target\"])\n",
    "    val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), \"target\"] = \"new_individual\"\n",
    "\n",
    "    return val_targets_df\n",
    "\n",
    "\n",
    "def create_distances_df(\n",
    "    image_names: np.ndarray, targets: np.ndarray, D: np.ndarray, I: np.ndarray, stage: str  # noqa: E741\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    distances_df = []\n",
    "    for i, image_name in tqdm(enumerate(image_names), desc=f\"Creating {stage}_df\"):\n",
    "        target = targets[I[i]]\n",
    "        distances = D[i]\n",
    "        subset_preds = pd.DataFrame(np.stack([target, distances], axis=1), columns=[\"target\", \"distances\"])\n",
    "        subset_preds[\"image\"] = image_name\n",
    "        distances_df.append(subset_preds)\n",
    "\n",
    "    distances_df = pd.concat(distances_df).reset_index(drop=True)\n",
    "    distances_df = distances_df.groupby([\"image\", \"target\"]).distances.max().reset_index()\n",
    "    distances_df = distances_df.sort_values(\"distances\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return distances_df\n",
    "\n",
    "\n",
    "def get_cv(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    all_preds = get_predictions(valid_df, threshold=0.5)\n",
    "    cv = 0\n",
    "    for i, row in val_targets_df.iterrows():\n",
    "        target = row.target\n",
    "        preds = all_preds[row.image]\n",
    "        val_targets_df.loc[i, 0.5] = map_per_image(target, preds)\n",
    "\n",
    "    cv = val_targets_df[0.5].mean()\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "\n",
    "def get_best_threshold(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    best_th = 0\n",
    "    best_cv = 0\n",
    "    for th in [0.1 * x for x in range(11)]:\n",
    "        all_preds = get_predictions(valid_df, threshold=th)\n",
    "\n",
    "        cv = 0\n",
    "        for i, row in val_targets_df.iterrows():\n",
    "            target = row.target\n",
    "            preds = all_preds[row.image]\n",
    "            val_targets_df.loc[i, th] = map_per_image(target, preds)\n",
    "\n",
    "        cv = val_targets_df[th].mean()\n",
    "\n",
    "        print(f\"th={th} cv={cv}\")\n",
    "\n",
    "        if cv > best_cv:\n",
    "            best_th = th\n",
    "            best_cv = cv\n",
    "\n",
    "    print(f\"best_th={best_th}\")\n",
    "    print(f\"best_cv={best_cv}\")\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "    print(f\"best_th_adjusted={best_th}\")\n",
    "\n",
    "    return best_th, best_cv\n",
    "\n",
    "\n",
    "def get_predictions(df: pd.DataFrame, threshold: float = 0.2):\n",
    "    sample_list = [\"938b7e931166\", \"5bf17305f073\", \"7593d2aee842\", \"7362d7a01d00\", \"956562ff2888\"]\n",
    "\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating predictions for threshold={threshold}\"):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, \"new_individual\"]\n",
    "        else:\n",
    "            predictions[row.image] = [\"new_individual\", row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# TODO: add types\n",
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def create_predictions_df(test_df: pd.DataFrame, best_th: float) -> pd.DataFrame:\n",
    "    predictions = get_predictions(test_df, best_th)\n",
    "\n",
    "    predictions = pd.Series(predictions).reset_index()\n",
    "    predictions.columns = [\"image\", \"predictions\"]\n",
    "    predictions[\"predictions\"] = predictions[\"predictions\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ad7c73-75c8-4fc9-97ae-55d7c37ad6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    convnext_checkpoint_path: str,\n",
    "    efficientnet_checkpoint_path: str,\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    k: int = 50,\n",
    "):\n",
    "    module_convnext_large_384 = load_eval_module(convnext_checkpoint_path, torch.device(\"cuda\"))\n",
    "    module_efficientnet_b7_512 = load_eval_module(efficientnet_checkpoint_path, torch.device(\"cuda\"))\n",
    "\n",
    "    convnext_train_dl, convnext_val_dl, convnext_test_dl = load_dataloaders(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=384,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    eff_train_dl, eff_val_dl, eff_test_dl = load_dataloaders(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=512,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    encoder = load_encoder()\n",
    "    \n",
    "    train_image_names, train_embeddings_convnext, train_targets = get_embeddings(module_convnext_large_384, convnext_train_dl, encoder, stage=\"train\")\n",
    "    val_image_names, val_embeddings_convnext, val_targets = get_embeddings(module_convnext_large_384, convnext_val_dl, encoder, stage=\"val\")\n",
    "    test_image_names, test_embeddings_convnext, test_targets = get_embeddings(module_convnext_large_384, convnext_test_dl, encoder, stage=\"test\")\n",
    "    save_convnext = {\n",
    "        'train':{\n",
    "                'train_image_names': train_image_names,\n",
    "                'train_embeddings': train_embeddings_convnext,\n",
    "                'train_targets': train_targets,\n",
    "            },\n",
    "        'val':{\n",
    "                'val_image_names': val_image_names,\n",
    "                'val_embeddings': val_embeddings_convnext,\n",
    "                'val_targets': val_targets,\n",
    "            },\n",
    "        'test':{\n",
    "                'test_image_names': test_image_names,\n",
    "                'test_embeddings': test_embeddings_convnext,\n",
    "                'test_targets': test_targets,\n",
    "            },\n",
    "    }\n",
    "    torch.save(save_convnext, f\"./cache/convnext/convnext_large_384_in22ft1k_{384}_{FOLD}.pth\")   \n",
    "    train_image_names, train_embeddings_eff, train_targets = get_embeddings(module_efficientnet_b7_512, eff_train_dl, encoder, stage=\"train\")\n",
    "    val_image_names, val_embeddings_eff, val_targets = get_embeddings(module_efficientnet_b7_512, eff_val_dl, encoder, stage=\"val\")\n",
    "    test_image_names, test_embeddings_eff, test_targets = get_embeddings(module_efficientnet_b7_512, eff_test_dl, encoder, stage=\"test\")\n",
    "    save_efficientnet = {\n",
    "        'train':{\n",
    "                'train_image_names': train_image_names,\n",
    "                'train_embeddings': train_embeddings_eff,\n",
    "                'train_targets': train_targets,\n",
    "            },\n",
    "        'val':{\n",
    "                'val_image_names': val_image_names,\n",
    "                'val_embeddings': val_embeddings_eff,\n",
    "                'val_targets': val_targets,\n",
    "            },\n",
    "        'test':{\n",
    "                'test_image_names': test_image_names,\n",
    "                'test_embeddings': test_embeddings_eff,\n",
    "                'test_targets': test_targets,\n",
    "            },\n",
    "    }\n",
    "    torch.save(save_efficientnet, f\"./cache/efficientnet/tf_efficientnet_b7_ns_{512}_{FOLD}.pth\")   \n",
    "\n",
    "    \n",
    "#     best_w = 0\n",
    "#     best_cv = 0\n",
    "#     for w in range(0, 105, 5): \n",
    "#         train_embeddings = w/100 * train_embeddings_convnext + (1 - w/100) * train_embeddings_eff\n",
    "#         val_embeddings = w/100 * val_embeddings_convnext + (1 - w/100) * val_embeddings_eff\n",
    "#         D, I = create_and_search_index(512, train_embeddings, val_embeddings, k)  # noqa: E741\n",
    "#         print(\"Created index with train_embeddings\")\n",
    "\n",
    "#         val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "\n",
    "#         val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "        \n",
    "#         cv = get_cv(val_targets_df, val_df)\n",
    "        \n",
    "#         print(f\"{w/100} * ConvNext + {1-w/100} * Efficient, current cv: {cv}\")\n",
    "#         if cv > best_cv:\n",
    "#             best_w = w\n",
    "#             best_cv = cv\n",
    "            \n",
    "#     print(f\"best_w: {best_w}, best_cv: {best_cv}\")\n",
    "    \n",
    "#     train_embeddings = best_w/100 * train_embeddings_convnext + (1 - best_w/100) * train_embeddings_eff\n",
    "#     val_embeddings = best_w/100 * val_embeddings_convnext + (1 - best_w/100) * val_embeddings_eff\n",
    "#     test_embeddings = best_w/100 * test_embeddings_convnext + (1 - best_w/100) * test_embeddings_eff\n",
    "\n",
    "#     train_embeddings = np.concatenate([train_embeddings, val_embeddings])\n",
    "#     train_targets = np.concatenate([train_targets, val_targets])\n",
    "#     print(\"Updated train_embeddings and train_targets with val data\")\n",
    "\n",
    "#     D, I = create_and_search_index(512, train_embeddings, test_embeddings, k)  # noqa: E741\n",
    "#     print(\"Created index with train_embeddings\")\n",
    "\n",
    "#     test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n",
    "#     print(f\"test_df=\\n{test_df.head()}\")\n",
    "\n",
    "#     predictions = create_predictions_df(test_df, 0.5)\n",
    "#     print(f\"predictions.head()={predictions.head()}\")\n",
    "    \n",
    "#     # Fix missing predictions\n",
    "#     # From https://www.kaggle.com/code/jpbremer/backfins-arcface-tpu-effnet/notebook\n",
    "#     public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n",
    "#     ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True)\n",
    "\n",
    "#     ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n",
    "\n",
    "#     predictions = pd.concat(\n",
    "#         [\n",
    "#             predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n",
    "#             public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n",
    "#             public_predictions[public_predictions[\"image\"].isin(ids2)],\n",
    "#         ]\n",
    "#     )\n",
    "#     predictions = predictions.drop_duplicates()\n",
    "\n",
    "#     predictions.to_csv(SUBMISSION_CSV_PATH, index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec72b5c-7267-49a8-a569-d29d59bc6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-19b078e174a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mval_fold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-13-4b358158f151>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(convnext_checkpoint_path, efficientnet_checkpoint_path, train_csv_encoded_folded, test_csv, val_fold, image_size, batch_size, num_workers, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ):\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodule_convnext_large_384\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_eval_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvnext_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodule_efficientnet_b7_512\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_eval_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mefficientnet_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-7cfcd4e03b68>\u001b[0m in \u001b[0;36mload_eval_module\u001b[0;34m(checkpoint_path, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_eval_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mLitModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLitModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "infer(\n",
    "    convnext_checkpoint_path=CHECKPOINTS_DIR / f\"convnext_large_384_in22ft1k_{384}_{FOLD}.ckpt\",\n",
    "    efficientnet_checkpoint_path=CHECKPOINTS_DIR / f\"tf_efficientnet_b7_ns_{512}_{FOLD}.ckpt\", \n",
    "    image_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_fold=FOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0fd0c11-1c35-4c80-af44-d7fb346de91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_(\n",
    "    checkpoint_path: str,\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    k: int = 50,\n",
    "):\n",
    "    module = load_eval_module(checkpoint_path, torch.device(\"cuda\"))\n",
    "\n",
    "    train_dl, val_dl, test_dl = load_dataloaders(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    print(len(train_dl))\n",
    "\n",
    "    encoder = load_encoder()\n",
    "    \n",
    "    train_image_names, train_embeddings, train_targets = get_embeddings(module, train_dl, encoder, stage=\"train\")\n",
    "    val_image_names, val_embeddings, val_targets = get_embeddings(module, val_dl, encoder, stage=\"val\")\n",
    "    test_image_names, test_embeddings, test_targets = get_embeddings(module, test_dl, encoder, stage=\"test\")\n",
    "    save_efficientnet = {\n",
    "        'train':{\n",
    "                'train_image_names': train_image_names,\n",
    "                'train_embeddings': train_embeddings,\n",
    "                'train_targets': train_targets,\n",
    "            },\n",
    "        'val':{\n",
    "                'val_image_names': val_image_names,\n",
    "                'val_embeddings': val_embeddings,\n",
    "                'val_targets': val_targets,\n",
    "            },\n",
    "        'test':{\n",
    "                'test_image_names': test_image_names,\n",
    "                'test_embeddings': test_embeddings,\n",
    "                'test_targets': test_targets,\n",
    "            },\n",
    "    }\n",
    "    torch.save(save_efficientnet, f\"./cache/convnext/convnext_base_384_in22ft1k_{512}_{FOLD}.pth\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c812b77-ab1f-4d9f-8a86-1462d93dcc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5110496c4323446f920b074a1728fa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating train embeddings:   0%|          | 0/2771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c233d190529e4e108fa53054e1354d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val embeddings:   0%|          | 0/693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ec7d25cbcb44e786b8d5435695966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test embeddings:   0%|          | 0/2329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "infer_(\n",
    "    checkpoint_path=CHECKPOINTS_DIR / f\"convnext_base_384_in22ft1k_{512}_{FOLD}.ckpt\", \n",
    "    image_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_fold=FOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2be1a3-3394-44e2-b2cb-90088545c979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
