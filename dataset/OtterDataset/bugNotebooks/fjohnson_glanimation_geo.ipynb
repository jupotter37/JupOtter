{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9525731a-be8c-4bf7-a975-6e7384535ffb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: turfpy in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: geopandas in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pygeos in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.14)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.22.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (1.26.8)\n",
      "Requirement already satisfied: geojson in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from turfpy) (3.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from turfpy) (1.8.0)\n",
      "Requirement already satisfied: shapely in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from turfpy) (2.0.6)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fjohnson.yikes\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib requests networkx turfpy geopandas pygeos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f27d15-3727-4ee0-8fa4-ff22848b654e",
   "metadata": {},
   "source": [
    "This notebook is used for preparing shipping paths for the Welland Lock animation.\n",
    "\n",
    "Required files\n",
    "1. '1854 1875 1882 WCR.csv'-> Historical Shipping CSV derived from spreadsheet file\n",
    "2. 'water_paths.json' -> Shipping paths converted to geojson with QGIS. Derived from this: https://www.arcgis.com/home/item.html?id=a4940deebec84fb9b6afa65afcbf891d#overview\n",
    "3. 'locations.csv' -> File holding final QA edits for the locations used in the animation. Four columns: \n",
    "- Location: The name of the location\n",
    "- Monk: Dr. Monk's chosen location\n",
    "- Final: When Monk's location is fed into the mapbox geocoding api, what the location that results - we need to feed the location into the mapbox geocoding api to get a coordinate. \n",
    "- Coord: If there is a mismatch between the final and the monk columns, I've gone in and manually put in the coordinate we want, and in this case the mapbox geocoding api is not used.\n",
    "\n",
    "Output\n",
    "1. manifest.json -> Paths that the animation will use to illustrate ship voyages\n",
    "2. debug.json -> For debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4b2612-da2f-45f7-9fa6-d8088126d965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import requests\n",
    "import networkx as nx\n",
    "import urllib.parse\n",
    "from datetime import date, datetime, timedelta\n",
    "from math import radians, sin, cos, atan2, sqrt, isnan, floor\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd31328-ed3f-4efb-9cc3-3330963c4982",
   "metadata": {},
   "source": [
    "Read in CSV data, clean it up, and prepare the first section of the json data that will be exported.\n",
    "This first section is the csv converted to json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3b6b2c-9244-4900-afee-5bd9649c6277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('1854 1875 1882 WCR.csv')\n",
    "\n",
    "def not_empty(item):\n",
    "    if type(item) == float and isnan(item):\n",
    "        return False\n",
    "    elif type(item) == type(None):\n",
    "        return False \n",
    "    elif type(item) == str and item.strip() == '':\n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "\n",
    "cargo = []\n",
    "for i,v in df[['Cargo 1', 'Cargo 2', 'Cargo 3', 'Cargo 4']].iterrows():\n",
    "    cargo_this = map(str.strip, filter(not_empty, [v['Cargo 1'], v['Cargo 2'], v['Cargo 3'],v['Cargo 4']]))\n",
    "    cargo.append(list(cargo_this))\n",
    "df['Cargo'] = cargo\n",
    "df = df.drop(columns=['Cargo 1', 'Cargo 2', 'Cargo 3', 'Cargo 4'])\n",
    "\n",
    "strip = lambda x: str(x).strip()\n",
    "for col in df:\n",
    "    if col != 'Cargo':\n",
    "        df[col] = df[col].apply(strip)\n",
    "    \n",
    "#Check values are clean\n",
    "assert (df['Year'].apply(lambda x: x in ['1854','1875','1882']).all())\n",
    "assert df['Day'].apply(lambda x: int(x)>=1 and int(x)<=31).all()\n",
    "\n",
    "for col in ['Nationality', 'Vessel Type', 'Name of Vessel']:\n",
    "    try: df[col] = df[col].apply(str.strip)\n",
    "    except TypeError as e:\n",
    "        print(col)\n",
    "\n",
    "month_to_int = {\"January\":1, \"Febuary\":2, \"March\": 3, \"April\": 4, \"May\": 5, \n",
    " \"June\":6, \"July\":7, \"August\":8, \"September\":9,\"October\":10,\"November\":11, \"December\":12}\n",
    "df['Date'] = df[['Year','Month','Day']].apply(lambda x: str(date(int(x.Year), month_to_int[x.Month], int(x.Day))), axis=1)    \n",
    "df.drop(columns=['Year','Month','Day'], inplace=True)\n",
    "\n",
    "#Drop voyages for these locations. We don't know where they are.\n",
    "unresolved_locations = [\n",
    "    'Albro',\n",
    "    'Black Bear',\n",
    "    'Boynat Inlet',\n",
    "    'Can Vill',\n",
    "    'Cana',\n",
    "    'Cape Ronence',\n",
    "    'Cida Dal',\n",
    "    'Clairmont',\n",
    "    'Cork Ireland',\n",
    "    'Creek',\n",
    "    'Dinestown',\n",
    "    'Elevator',\n",
    "    'Forrester',\n",
    "    'Government',\n",
    "    'Mevey',\n",
    "    'Monsoon',\n",
    "    'Pass',\n",
    "    'Point Plag',\n",
    "    'Port Davis',\n",
    "    'Port Flamber',\n",
    "    'St. Duram',\n",
    "    'Survey',\n",
    "    'Venice',\n",
    "    'White Stall'\n",
    "]\n",
    "df_unresolved = df['Where From'].map(lambda v: v in unresolved_locations)\n",
    "df_unresolved = df_unresolved | df['Where Bound'].map(lambda v: v in unresolved_locations)\n",
    "df = df[~df_unresolved]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c2820-5182-490a-a989-78a2f0d3413e",
   "metadata": {},
   "source": [
    "Show some information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d40c91-fe4f-4f5a-bb47-645f907ad424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'American', 'British'}\n",
      "{'Scow', 'Steamer', 'Propeller', 'Brigantine', 'Boat', 'Tug', 'Barge', 'Sailboat', 'Yacht', 'Dredge', 'Barkentine', 'Raft', 'Steam Barge', 'Steam Yacht', 'Schooner'}\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "print(set(df['Nationality']))\n",
    "print(set(df['Vessel Type']))\n",
    "\n",
    "#Total number of days\n",
    "print(len(df['Date'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ed3dc-ad4e-46f2-857c-7290aecf3488",
   "metadata": {},
   "source": [
    "Read in the shipping path data that was converted to json using QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6884aa87-0a82-4f52-81a1-ca1286f2e0f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(Path('data')/'water_paths.json') as wp_geojson:\n",
    "    gj = json.loads(wp_geojson.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0aed0-256d-464a-a5dc-1afd5b0b7918",
   "metadata": {},
   "source": [
    "Define functions for transforming the path information into a graph and working with that graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122df4cb-bb69-4a45-9939-e7caf441178f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_filter(gj, gid_list=None, exclude=True):\n",
    "    '''filter out features from geojson. optionally exclude or include only features with gid in gid_list.'''\n",
    "    filtered = []\n",
    "    for f in gj['features']:\n",
    "        if not gid_list:\n",
    "            filtered.append(f)\n",
    "            continue\n",
    "            \n",
    "        if exclude and f['properties']['gid'] not in gid_list: \n",
    "            filtered.append(f)\n",
    "        elif not exclude and f['properties']['gid'] in gid_list: \n",
    "            filtered.append(f)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def distance(p1, p2):\n",
    "    '''returns distance in KM between two points'''\n",
    "    earth_radius = 6371\n",
    "    lat1, lat2 = radians(p1[1]), radians(p2[1])\n",
    "    lon1, lon2 = radians(p1[0]), radians(p2[0])\n",
    "    \n",
    "    #https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    base = earth_radius * c\n",
    "    return base\n",
    "\n",
    "def draw_graph(G):\n",
    "    pos = nx.planar_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, font_weight='bold')\n",
    "    el=nx.get_edge_attributes(G,'weight')\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=el)\n",
    "\n",
    "def path_distance(g, path):\n",
    "    prev = path[0]\n",
    "    distances = []\n",
    "    for p in path[1:-1]:\n",
    "        distances.append(g.edges[(prev,p)]['weight'])\n",
    "        prev = p\n",
    "    p = path[-1]\n",
    "    distances.append(g.edges[(prev,p)]['weight'])\n",
    "    return sum(distances)\n",
    "\n",
    "def geojson_to_graph(features):\n",
    "    G = nx.Graph()\n",
    "    for feat in features:\n",
    "        assert len(feat['geometry']['coordinates']) == 1\n",
    "        feat_geo = feat['geometry']['coordinates'][0]\n",
    "        anode = str(feat['properties']['anode'])\n",
    "        bnode = str(feat['properties']['bnode'])\n",
    "        #length = feat['properties']['length']\n",
    "        id = feat['properties']['OBJECTID']\n",
    "        id_postfix = 1\n",
    "        G.add_node(anode, coordinates=tuple(feat_geo[0]))\n",
    "        G.add_node(bnode, coordinates=tuple(feat_geo[-1]))\n",
    "\n",
    "        prev = anode\n",
    "        prev_coord = feat_geo[0]\n",
    "        for point in feat_geo[1:-1]:\n",
    "            name = f\"{id}_{id_postfix}\"\n",
    "            id_postfix += 1\n",
    "            G.add_node(name, coordinates=tuple(point))\n",
    "            G.add_edge(prev,name, weight=distance(prev_coord, point))\n",
    "            prev = name\n",
    "            prev_coord = point\n",
    "        G.add_edge(prev, bnode, weight=distance(prev_coord, feat_geo[-1]))\n",
    "    return G\n",
    "\n",
    "def coordinates_set(g):\n",
    "    return {g.nodes[node]['coordinates'] for node in g.nodes}\n",
    "\n",
    "def closest_coordinate(all_cords, candidate_points):\n",
    "    point_distances = []\n",
    "    for point in candidate_points:\n",
    "        #find the closest node coordinate in the path graph to this point\n",
    "        cc = sorted(all_cords, key=lambda coord: distance(coord, point))[0]\n",
    "        #note down the point and the distance to its closest coordinate\n",
    "        point_distances.append((cc, point, distance(cc, point)))\n",
    "    \n",
    "    #return the pair that has the smallest distance\n",
    "    return sorted(point_distances, key=lambda record: record[2])[0]\n",
    "\n",
    "def get_features_coords(features):\n",
    "    return [feat['center'] for feat in features]\n",
    "\n",
    "def location_name_to_coord(g, location_csv_pdf):\n",
    "    '''Using the geocoding api, map locations to their most likely lat/lon'''\n",
    "    #only return results within the specific bounding box\n",
    "    bounding_box = '-95.13478080553124%2C39.119757307389875%2C-70.72830423014456%2C52.52603829818719'\n",
    "    access_token = 'pk.eyJ1IjoiZmpvaG5zODg4IiwiYSI6ImNsaGh6eXo1dDAzMDMzbW1td3BqOXFoaDMifQ.RBC_0mpQ25-GRAZDA0E0oA'\n",
    "    mapping = {}\n",
    "    all_coords = coordinates_set(g)\n",
    "    \n",
    "    for row in location_csv_pdf.iterrows():\n",
    "        row_data = row[1]\n",
    "        \n",
    "        lat_lon = row_data.Coord\n",
    "        place_name = row_data.Location\n",
    "        location = row_data.Monk\n",
    "        resolved_location = row_data.Final\n",
    "        \n",
    "        if type(lat_lon) == str:\n",
    "            lat,lon = lat_lon.split(',')\n",
    "            lat = float(lat)\n",
    "            lon = float(lon)\n",
    "            cc = closest_coordinate(all_coords, [[lon,lat]]) #notice lon,lat order is reversed. this is what we want.\n",
    "            mapping[place_name] = {'graphcoord':cc[0], 'geocoded_coord':cc[1], 'geocode_name': resolved_location}\n",
    "        else:\n",
    "            location = urllib.parse.quote(location)\n",
    "            req_url = f'https://api.mapbox.com/geocoding/v5/mapbox.places/{location}.json?bbox={bounding_box}&access_token={access_token}'\n",
    "            features = requests.get(req_url).json()\n",
    "            if not len(features['features']):\n",
    "                print(req_url)\n",
    "                continue\n",
    "            #assert len(features['features']), features\n",
    "\n",
    "            #Use this instead to compare all returned search results against graph coordinates\n",
    "            #use the pair that has the minimum distance from one another\n",
    "            #candidate_points = get_features_coords(features['features'])\n",
    "            #cc = closest_coordinate(all_coords, candidate_points)\n",
    "            #mapping[loc] = {'graphcoord':cc[0], 'geocoded_coord':cc[1]}\n",
    "\n",
    "            geocode = features['features'][0] #get the first and most relevant result\n",
    "            cc = closest_coordinate(all_coords, [geocode['center']])\n",
    "            mapping[place_name] = {'graphcoord':cc[0], 'geocoded_coord':cc[1], 'geocode_name': geocode['place_name']}\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def coord_to_node(coordinate):\n",
    "    for node in g.nodes:\n",
    "        if g.nodes[node]['coordinates'] == coordinate:\n",
    "            return node\n",
    "    raise ValueError(\"Could not find node matching given coordinate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbac965-9f1f-49fa-8966-447e6b634c59",
   "metadata": {},
   "source": [
    "Create the graph representation of the geojson data. Create mappings using the Mapbox geocoding api that translate place names to lon/lat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc774921-2c13-4fc4-8ef5-e4da82492688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Exclude the lone multiline path in the geojson data\n",
    "#this is okay since it its a series of disconnectd strings anyways\n",
    "features = feature_filter(gj, [837], exclude=True)\n",
    "g = geojson_to_graph(features)\n",
    "location_csv = pd.read_csv('locations.csv')\n",
    "location_csv.Location = location_csv.Location.map(lambda s: s.strip())\n",
    "loc_cord_map = location_name_to_coord(g, location_csv)\n",
    "loc_node_map = {loc:coord_to_node(loc_cord_map[loc]['graphcoord']) for loc in location_csv.Location}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b25517-24e0-4c16-889e-401366856e3f",
   "metadata": {},
   "source": [
    "Some functions that output feature collections (geojson) of the locations that were previously just strings. Basically just used to verify locations that were looked up using the geocoding api make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bfcd74-bd5b-4711-ac25-b144b0cba978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mapping_to_geojson(mapping):\n",
    "    '''Dump location lon/dat data'''\n",
    "    \n",
    "    features = {\n",
    "      \"type\": \"FeatureCollection\",\n",
    "      \"features\": []\n",
    "    }\n",
    "    for loc in mapping:\n",
    "        loc_gj = {\"type\": \"Feature\",\n",
    "                  \"id\": loc,\n",
    "                  \"geometry\": {\n",
    "                      \"type\": \"Point\", \n",
    "                      \"coordinates\": mapping[loc]['graphcoord']},\n",
    "                  \"properties\": {\"name\": loc}}\n",
    "        features['features'].append(loc_gj)\n",
    "    return json.dumps(features)\n",
    "\n",
    "def mapping_to_geojson_debug(mapping):\n",
    "    '''Dump queried/looked up lon/lat locations with their most likely graph locations.\n",
    "    Provided as a LineString so that the discrepancy can be observed'''\n",
    "    \n",
    "    linefeatures = {\n",
    "      \"type\": \"FeatureCollection\",\n",
    "      \"features\": []\n",
    "    }\n",
    "    estfeatures = {\n",
    "      \"type\": \"FeatureCollection\",\n",
    "      \"features\": []\n",
    "    }\n",
    "    realfeatures = {\n",
    "      \"type\": \"FeatureCollection\",\n",
    "      \"features\": []\n",
    "    }\n",
    "    features = {}\n",
    "    \n",
    "    for loc in mapping:\n",
    "        loc_gj = {\"type\": \"Feature\",\n",
    "                  \"id\": loc,\n",
    "                  \"geometry\": {\n",
    "                      \"type\": \"LineString\", \n",
    "                      \"coordinates\": [mapping[loc]['graphcoord'], mapping[loc]['geocoded_coord']]},\n",
    "                  \"properties\": {\"name\": loc}}\n",
    "        est = {\"type\": \"Feature\",\n",
    "               \"id\": loc,\n",
    "               \"geometry\":{\"type\":\"Point\", \"coordinates\":mapping[loc]['graphcoord']},\n",
    "               \"properties\":{\"name\":loc}}\n",
    "        real = {\"type\": \"Feature\",\n",
    "               \"id\": loc,\n",
    "               \"geometry\":{\"type\":\"Point\", \"coordinates\":mapping[loc]['geocoded_coord']},\n",
    "               \"properties\":{\"name\":mapping[loc]['geocode_name']}}\n",
    "        linefeatures['features'].append(loc_gj) \n",
    "        estfeatures['features'].append(est)\n",
    "        realfeatures['features'].append(real)\n",
    "    \n",
    "    features['lines'] = linefeatures\n",
    "    features['estimation'] = estfeatures\n",
    "    features['real'] = realfeatures\n",
    "    return json.dumps(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ee9ab1-4d3e-4cfa-9675-dc816db84f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = mapping_to_geojson_debug(loc_cord_map)\n",
    "with open(Path('data')/'debug.json','w') as debug_file:\n",
    "    debug_file.write(debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5cc5e-9189-460d-9722-f07a3159bef2",
   "metadata": {},
   "source": [
    "Code for navigating the created graph representation and deriving the shortest paths for the 'Where From' and 'Where Bound' columns of the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b60afb5-79cd-4cc0-80d0-6c9d952a7b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_to_coordinates(g, path):\n",
    "    return [g.nodes[node]['coordinates'] for node in path]\n",
    "\n",
    "def coordinates_to_geojson(coordinates):\n",
    "    geo_json = {\"type\": \"Feature\",\n",
    "                \"properties\":{},\n",
    "                \"geometry\":{\n",
    "                    \"type\":\"LineString\",\n",
    "                    \"coordinates\":[]\n",
    "                }}\n",
    "                \n",
    "    \n",
    "    for coordinate in coordinates:\n",
    "        geo_json['geometry']['coordinates'].append(list(coordinate))\n",
    "    \n",
    "    return geo_json\n",
    "\n",
    "def shortest_path_to_geojson(g, start, end):\n",
    "    if isinstance(start, int): \n",
    "        start = str(start)\n",
    "    if isinstance(end, int):\n",
    "        end = str(end)\n",
    "        \n",
    "    sp = nx.shortest_path(g,start,end, weight='weight')\n",
    "    return coordinates_to_geojson(path_to_coordinates(g, sp))\n",
    "\n",
    "def shortest_paths_to_geojson(g, start_end_pairs):\n",
    "    '''Given a list of star/end pairs, find the shortest path and return it as a series of linestrings in geojson'''\n",
    "    geo_json = {\"type\":\"FeatureCollection\",\n",
    "                \"features\":[]\n",
    "               }\n",
    "    geo_json['features'] = [shortest_path_to_geojson(g, start, end) for start, end in start_end_pairs]\n",
    "    return geo_json\n",
    "\n",
    "def gen_all_paths(g, df, loc_node_map):\n",
    "    paths = []\n",
    "    bad_paths = set()\n",
    "    for _,locs in df[['Where From','Where Bound']].drop_duplicates().iterrows():\n",
    "        start_name = locs['Where From']\n",
    "        dst_name = locs['Where Bound']\n",
    "        node_start = loc_node_map[start_name]\n",
    "        node_end = loc_node_map[dst_name]\n",
    "        path_name = f\"{start_name}+{dst_name}\"\n",
    "        try:\n",
    "            feature = shortest_path_to_geojson(g, node_start, node_end)\n",
    "            feature['properties']['path'] = path_name\n",
    "            paths.append(feature)\n",
    "        except nx.NetworkXNoPath:\n",
    "            bad_paths.add(path_name)\n",
    "    \n",
    "    feature_collection = {\"type\":\"FeatureCollection\", \"features\":paths}\n",
    "    return json.dumps(feature_collection), bad_paths\n",
    "\n",
    "good,bad = gen_all_paths(g, df, loc_node_map)\n",
    "assert not bad, bad\n",
    "    \n",
    "# # Save cleaned dataframe as JSON    \n",
    "# manifest = df.to_json(orient=\"records\")\n",
    "\n",
    "# with open(Path('data')/'manifest.json','w') as manifest_file:\n",
    "#     manifest_file.write(f'{{\"manifest\":{manifest},\\n \"routes\":{good}}}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab8f54d-a05d-4e2f-add4-f5598722ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns out I misinterpreted the date in the data; it's not when a vessel leaves, but rather when a vessel arrives at the \n",
    "#canal. Fix up the start times here by determining the distance from a vessel's origin to the canal and then determining \n",
    "#how long it would take it to travel this distance. That time is then substracted from the date in the data to get the \n",
    "#starting date\n",
    "\n",
    "from geojson import  Polygon, Feature, LineString\n",
    "from turfpy.measurement import length\n",
    "from turfpy.transformation import intersect\n",
    "from turfpy.misc import line_intersect\n",
    "\n",
    "north = LineString([[-79.219,43.221],[-79.1601,43.215]])\n",
    "south = LineString([[-79.2597,42.8823],[-79.2260,42.8762]])\n",
    "entrance = None\n",
    "\n",
    "def distance_to_canal(coordinates):\n",
    "    distance=0\n",
    "    start=coordinates[0]\n",
    "    for point in coordinates[1:]:\n",
    "        cur_line = LineString([start,point])\n",
    "        features = line_intersect(cur_line,north)['features']\n",
    "        entrance = \"north\"\n",
    "        if not features:\n",
    "            features = line_intersect(cur_line,south)['features']\n",
    "            entrance = \"south\"\n",
    "        if features:\n",
    "            end = features[0]['geometry']['coordinates']\n",
    "            distance += length(LineString([start,end]), units='km')\n",
    "            return distance, entrance\n",
    "        distance += length(cur_line, units='km')\n",
    "        start=point\n",
    "        \n",
    "    return None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68b59c17-93fe-4386-9aaa-37fda62c82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "routes = json.loads(good)\n",
    "route_map = {feat['properties']['path']:feat['geometry']['coordinates'] for feat in routes['features']}\n",
    "cache = {}\n",
    "updated_dates = {}\n",
    "skipped = 0\n",
    "entrance = None\n",
    "\n",
    "#distance in km from south/north mouth of canal to lock3\n",
    "south_lock3_dist = 31.0  \n",
    "north_lock3_dist = 7.6\n",
    "canal_speed = 19.0/24 #canal speed is 19KM per day\n",
    "south_entrance_offset = south_lock3_dist/canal_speed #offset in hours\n",
    "north_entrance_offset = north_lock3_dist/canal_speed\n",
    "\n",
    "vessel_speeds = [\n",
    "    ['Schooner',5],['Brigantine',4], ['Barkentine',4],\n",
    "    ['Propeller',8],['Steamer',8],['Tug',10],\n",
    "    ['Scow',4], ['Other',3]]\n",
    "\n",
    "for vessel in vessel_speeds:\n",
    "    speed = vessel[1]\n",
    "    vessel[1] = speed * 1.609 #convert mph to kmh\n",
    "\n",
    "vessel_speeds = {v[0]:v[1] for v in vessel_speeds}\n",
    "\n",
    "for row in data.iterrows():\n",
    "    i=row[0]\n",
    "    row=row[1]\n",
    "    vtype=row['Vessel Type']\n",
    "    src=row['Where From']\n",
    "    dst=row['Where Bound']\n",
    "    route=f\"{src}+{dst}\"\n",
    "    if route in cache:\n",
    "        distance,entrance = cache[route]\n",
    "    else: \n",
    "        distance,entrance = distance_to_canal(route_map[route])\n",
    "        cache[route] = distance,entrance\n",
    "    if not distance: \n",
    "        #This means that the path of this vessel is contained within the canal itself\n",
    "        #and the date does not need to be changed\n",
    "        skipped += 1\n",
    "    else:\n",
    "        try:\n",
    "            vessel_speed = vessel_speeds[vtype]\n",
    "        except KeyError:\n",
    "            vessel_speed = vessel_speeds['Other']\n",
    "        vdate = datetime.strptime(row['Date'],'%Y-%m-%d') \n",
    "        canal_offset = south_entrance_offset if entrance==\"south\" else north_entrance_offset\n",
    "        offset = distance/vessel_speed + canal_offset #hrs back\n",
    "        vdate = vdate - timedelta(hours=offset)\n",
    "        updated_dates[i] = vdate\n",
    "\n",
    "rounded_dates = {i:updated_dates[i].strftime('%Y-%m-%d %H:00:00') for i in updated_dates}\n",
    "\n",
    "df2 = data.copy()\n",
    "series_ud = pd.Series(data=rounded_dates.values(), index=rounded_dates.keys(), name='Date')\n",
    "df2.update(series_ud)\n",
    "manifest = df2.sort_values(by=['Date']).to_json(orient=\"records\")\n",
    "\n",
    "with open(Path('data')/'manifest.json','w') as manifest_file:\n",
    "    manifest_file.write(f'{{\"manifest\":{manifest},\\n \"routes\":{good}}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac6280f0-f801-4d18-b08d-c43d2430c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_precise = {i:updated_dates[i].strftime('%Y-%m-%d %H:%M:%S') for i in updated_dates}\n",
    "df2 = data.copy()\n",
    "df2.drop(['LAC Image #','Cargo','Direction'],axis=1,inplace=True)\n",
    "series_ud = pd.Series(data=dates_precise.values(), index=dates_precise.keys(), name='Leaving')\n",
    "df2.join(series_ud).sort_values(by=['Date']).to_csv('dataout.csv',index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
