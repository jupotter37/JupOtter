{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key\n",
    "api_key = 'api_key'\n",
    "\n",
    "# Global Model variables\n",
    "global_model_path = 'global_model/block_diagram_global_information'\n",
    "empty_folder = 'global_model/block_diagram_global_information/dataset/c2t_data/'    # create an empty folder\n",
    "global_limit_token = 300\n",
    "\n",
    "# OCR Variable\n",
    "pororo_path = 'ocr_pororo'\n",
    "\n",
    "# Local Model Variables variables\n",
    "local_model_path = 'local_model/block_diagram_symbol_detection/symbol_detection'\n",
    "object_detection_output_path = 'local_model/block_diagram_symbol_detection/symbol_detection/runs/detect/exp/labels'\n",
    "yolo_weights_path = 'local_model/block_diagram_symbol_detection/symbol_detection/runs/train/best_all/weights/best.pt'\n",
    "yolo_yaml_file = 'local_model/block_diagram_symbol_detection/symbol_detection/data/mydata.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Infromation Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\.conda\\envs\\app_block\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shrey\\.conda\\envs\\app_block\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from donut import DonutModel\n",
    "\n",
    "# Load the pre-trained model\n",
    "global_model = DonutModel.from_pretrained(global_model_path) \n",
    "\n",
    "# Move the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    global_model.half()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    global_model.to(device)\n",
    "    \n",
    "# Function to process a single image\n",
    "def global_model_process(image):\n",
    "\n",
    "    try:\n",
    "        # Load and process the image\n",
    "        image1 = Image.fromarray(image)\n",
    "        task_name = os.path.basename(empty_folder)  \n",
    "        result = global_model.inference(image=image1, prompt=f\"<s_{task_name}>\")[\"predictions\"][0]\n",
    "\n",
    "        # Extract the relevant information from the result\n",
    "        if 'c2t' in result:\n",
    "            text_result = result['c2t']\n",
    "        else:\n",
    "            text_result = result['text_sequence']\n",
    "\n",
    "        # Limit the result to 500 tokens\n",
    "        limited_result = ' '.join(text_result.split()[:global_limit_token])\n",
    "\n",
    "        return limited_result\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return an empty string in case of an error\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Information Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(local_model_path)\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\n",
    "from utils.general import LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2, increment_path, non_max_suppression, print_args, scale_boxes, strip_optimizer, xyxy2xywh\n",
    "from utils.plots import Annotator, colors, save_one_box, save_block_box\n",
    "from utils.torch_utils import select_device, smart_inference_mode\n",
    "\n",
    "def load_model(weights, device, dnn, data, fp16):\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=fp16)\n",
    "    return model\n",
    "\n",
    "def run_single_image_inference(model, img_path, stride, names, pt, conf_thres=0.35, iou_thres=0.7, max_det=100, augment=True, visualize=False, line_thickness=1, hide_labels=False, hide_conf=False, save_conf=False, save_crop=False, save_block=True, imgsz=(640, 640), vid_stride=1, bs=1, classes=None, agnostic_nms=False, save_txt=True, save_img=True):\n",
    "    dataset = LoadImages(img_path, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)  # Load image from file\n",
    "    imgsz = check_img_size(imgsz, s=stride) \n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        with dt[0]:\n",
    "            im = torch.from_numpy(im).to(model.device)\n",
    "            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "\n",
    "        # Inference\n",
    "        with dt[1]:\n",
    "            visualize = False\n",
    "            pred = model(im, augment=augment, visualize=visualize)\n",
    "\n",
    "        # NMS\n",
    "        with dt[2]:\n",
    "            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Process predictions\n",
    "        sorted_data_list = []\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            seen += 1\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            s += '%gx%g ' % im.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop or save_block else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, 5].unique():\n",
    "                    n = (det[:, 5] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "                \n",
    "                data_for_image=[]\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    data_for_image.append((int(cls), xywh))\n",
    "                    c = int(cls)  # integer class\n",
    "                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "\n",
    "            # Sort the data based on the top-left coordinates (Y first, then X)\n",
    "            sorted_data_for_image = sorted(data_for_image, key=lambda x: (x[1][1], x[1][0]))\n",
    "            sorted_data_list.extend(sorted_data_for_image)\n",
    "    \n",
    "    # Return the combined sorted data as a tuple\n",
    "    return tuple(sorted_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-72-g064365d Python-3.9.18 torch-1.12.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20873139 parameters, 0 gradients, 47.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "yolo_model = load_model(yolo_weights_path, device='cpu', dnn=False, data=yolo_yaml_file, fp16=False)\n",
    "stride, names, pt = yolo_model.stride, yolo_model.names, yolo_model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlockSplit: Break image into smaller units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def read_list(annotation_list, image):\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    edge = []\n",
    "    node = []\n",
    "    for annotation in annotation_list:\n",
    "        category, bbox_norm = annotation\n",
    "\n",
    "        x_norm, y_norm, w_norm, h_norm = bbox_norm\n",
    "\n",
    "        x = x_norm * image_width\n",
    "        y = y_norm * image_height\n",
    "        w = w_norm * image_width\n",
    "        h = h_norm * image_height\n",
    "\n",
    "        if category == 0:\n",
    "            if w < h:\n",
    "                edge1 = (x, y - h/2)\n",
    "                edge2 = (x, y + h/2)\n",
    "                edge.append([(x, y, w, h), edge1, edge2])\n",
    "            elif w >= h:\n",
    "                edge1 = (x - w/2, y)\n",
    "                edge2 = (x + w/2, y)\n",
    "                edge.append([(x, y, w, h), edge1, edge2])\n",
    "        elif category in [1, 2, 3, 5]:\n",
    "            t = (x, y + h/2)\n",
    "            b = (x, y - h/2)\n",
    "            l = (x - w/2, y)\n",
    "            r = (x + w/2, y)\n",
    "            node.append([(x, y, w, h), t, b, l, r])\n",
    "\n",
    "    return edge, node\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "def calculate_relative_position(edge_point, node_point):\n",
    "    # Calculate the relative position of the node with respect to the edge point\n",
    "    dx = node_point[0] - edge_point[0]\n",
    "    dy = node_point[1] - edge_point[1]\n",
    "\n",
    "    # Return the relative position as a tuple (dx, dy)\n",
    "    return dx, dy\n",
    "\n",
    "def find_closest_node(edge, node):\n",
    "    results = []\n",
    "    for edge_box in edge:\n",
    "        edge1, edge2 = edge_box[1], edge_box[2]\n",
    "        min_distance1 = float('inf')\n",
    "        min_distance2 = float('inf')\n",
    "        closest_node1 = None\n",
    "        closest_node2 = None\n",
    "\n",
    "        for node_box in node:\n",
    "            for i in range(4):\n",
    "                node_point = node_box[1 + i]\n",
    "\n",
    "                distance1 = calculate_distance(edge1, node_point)\n",
    "                distance2 = calculate_distance(edge2, node_point)\n",
    "\n",
    "                if distance1 < min_distance1:\n",
    "                    min_distance1 = distance1\n",
    "                    closest_node1 = node_box\n",
    "\n",
    "                if distance2 < min_distance2:\n",
    "                    min_distance2 = distance2\n",
    "                    closest_node2 = node_box\n",
    "\n",
    "        # Calculate relative positions of closest nodes with respect to edges\n",
    "        rel_pos1 = calculate_relative_position(edge1, closest_node1[0])\n",
    "        rel_pos2 = calculate_relative_position(edge1, closest_node2[0])\n",
    "\n",
    "        # Choose the closest node based on relative positions\n",
    "        if rel_pos1[0] < 0 or rel_pos1[1] < 0:\n",
    "            # If closest_node1 is to the left or above edge1, prefer it\n",
    "            results.append(('edge_box', edge_box, 'closest_node1', closest_node1, 'closest_node2', closest_node2))\n",
    "        else:\n",
    "            # Otherwise, prefer closest_node2\n",
    "            results.append(('edge_box', edge_box, 'closest_node1', closest_node2, 'closest_node2', closest_node1))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(pororo_path)\n",
    "\n",
    "from main import PororoOcr\n",
    "\n",
    "ocr = PororoOcr()\n",
    "ocr.get_available_langs()\n",
    "ocr.get_available_models()\n",
    "\n",
    "def pororo_ocr(img_path):\n",
    "    ocr.run_ocr(img_path, debug=False) \n",
    "    res = ocr.get_ocr_result()\n",
    "    word_coordinates = []\n",
    "\n",
    "    for i in range(len(res['description'])):\n",
    "        word = res['description'][i]\n",
    "        vertices = res['bounding_poly'][i]['vertices']\n",
    "        x_min = min(vertex['x'] for vertex in vertices)\n",
    "        y_min = min(vertex['y'] for vertex in vertices)\n",
    "        x_max = max(vertex['x'] for vertex in vertices)\n",
    "        y_max = max(vertex['y'] for vertex in vertices)\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        word_coordinates.append((word, (x_min, y_min, width, height)))   ## (word, (x,y,w,h))\n",
    "\n",
    "    return word_coordinates\n",
    "\n",
    "def process_edge_box(image, edge_box):\n",
    "    # Extract the coordinates and dimensions from the edge box\n",
    "    x_mid, y_mid, w, h = map(int, edge_box)\n",
    "    x1, y1 = x_mid - w // 2, y_mid - h // 2\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "\n",
    "    # Crop the image to the specified region\n",
    "    roi_edge = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray_roi = cv2.cvtColor(roi_edge, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding to the cropped region\n",
    "    _, thresholded_edge = cv2.threshold(gray_roi, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Invert the colors (make the background black and the object white)\n",
    "    inverted_edge = cv2.bitwise_not(thresholded_edge)\n",
    "\n",
    "    return inverted_edge\n",
    "\n",
    "def find_head_tail(thresholded_edge, closest_node1_text, closest_node2_text):\n",
    "    h, w = thresholded_edge.shape\n",
    "\n",
    "    if h > w:\n",
    "        half1 = thresholded_edge[:h // 2, :]\n",
    "        half2 = thresholded_edge[h // 2:, :]\n",
    "    else:\n",
    "        half1 = thresholded_edge[:, :w // 2]\n",
    "        half2 = thresholded_edge[:, w // 2:]\n",
    "\n",
    "    # Ensure single-channel\n",
    "    half1 = cv2.cvtColor(half1, cv2.COLOR_BGR2GRAY) if len(half1.shape) == 3 else half1\n",
    "    half2 = cv2.cvtColor(half2, cv2.COLOR_BGR2GRAY) if len(half2.shape) == 3 else half2\n",
    "\n",
    "    white_pixels_half1 = cv2.countNonZero(half1)\n",
    "    white_pixels_half2 = cv2.countNonZero(half2)\n",
    "\n",
    "    if white_pixels_half1 > white_pixels_half2:\n",
    "        # Perform swap \n",
    "        return closest_node2_text, closest_node1_text\n",
    "    else:\n",
    "        # Return original head-tail if not swapping\n",
    "        return closest_node1_text, closest_node2_text\n",
    "\n",
    "def calculate_distance2(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2_mid, y2_mid, w2, h2 = box2\n",
    "    \n",
    "    # Calculate the coordinates of the center of box2\n",
    "    x2 = x2_mid - w2 / 2\n",
    "    y2 = y2_mid - h2 / 2\n",
    "\n",
    "    center1 = np.array([x1 + w1 / 2, y1 + h1 / 2])\n",
    "    center2 = np.array([x2 + w2 / 2, y2 + h2 / 2])\n",
    "\n",
    "    distance = np.linalg.norm(center1 - center2)\n",
    "    return distance\n",
    "\n",
    "# Function to extract all text from specific coordinates using pororoocr results\n",
    "def extract_text_from_coordinates(coordinates, ocr_result):\n",
    "    x_mid, y_mid, w, h = coordinates\n",
    "\n",
    "    # Calculate the top-left and bottom-right corners of the bounding box\n",
    "    x = x_mid - w // 2\n",
    "    y = y_mid - h // 2\n",
    "\n",
    "    # Collect all words that lie inside the specified coordinates\n",
    "    matching_words = [word for word, (word_x, word_y, word_w, word_h) in ocr_result\n",
    "                      if x <= word_x <= x + w and y <= word_y <= y + h]\n",
    "\n",
    "    # Combine the matching words into a single string separated by spaces\n",
    "    combined_text = ' '.join(matching_words)\n",
    "\n",
    "    return combined_text, matching_words\n",
    "\n",
    "def process_json_file(image, json_result):\n",
    "\n",
    "    try:\n",
    "        data = json_result\n",
    "\n",
    "        ocr_result = pororo_ocr(image)\n",
    "\n",
    "        # Initialize empty list for triplets\n",
    "        triplets = []\n",
    "\n",
    "        # Iterate through each result in the list\n",
    "        for result in data['results']:\n",
    "            # Find the index of 'edge_box', 'closest_node1', and 'closest_node2'\n",
    "            edge_box_index = result.index('edge_box')\n",
    "            closest_node1_index = result.index('closest_node1')\n",
    "            closest_node2_index = result.index('closest_node2')\n",
    "\n",
    "            # Extract the coordinates of edge_box, closest_node1, and closest_node2\n",
    "            edge_box = result[edge_box_index + 1][0]\n",
    "            closest_node1 = result[closest_node1_index + 1][0]\n",
    "            closest_node2 = result[closest_node2_index + 1][0]\n",
    "\n",
    "            # Append the triplet to the list\n",
    "            triplets.append({'edge_box': edge_box, 'closest_node1': closest_node1, 'closest_node2': closest_node2})\n",
    "\n",
    "        formatted_triplets = []\n",
    "        used_words = set() \n",
    "\n",
    "        # find head, tail\n",
    "        for triplet in triplets:\n",
    "            closest_node1_coords = triplet['closest_node1']\n",
    "            closest_node2_coords = triplet['closest_node2']\n",
    "\n",
    "            edge_box = triplet['edge_box']\n",
    "\n",
    "            # Extract text using pororoocr results\n",
    "            closest_node1_text, closest_node1_words = extract_text_from_coordinates(closest_node1_coords, ocr_result)\n",
    "            closest_node2_text, closest_node2_words = extract_text_from_coordinates(closest_node2_coords, ocr_result)\n",
    "\n",
    "            # Add used words to the set\n",
    "            used_words.update(closest_node1_words)\n",
    "            used_words.update(closest_node2_words)\n",
    "\n",
    "            # Process the edge box using the separate function\n",
    "            thresholded_edge = process_edge_box(image, edge_box)\n",
    "            head, tail = find_head_tail(thresholded_edge, closest_node1_text, closest_node2_text)\n",
    "\n",
    "            # Add used words to the set\n",
    "            used_words.update(head.split())\n",
    "            used_words.update(tail.split())\n",
    "\n",
    "            # Check if any Korean characters are present using regular expression\n",
    "            if re.search('[\\u3131-\\u3163\\uac00-\\ud7a3]+', head + tail):\n",
    "                relation = \"와 연계된\"\n",
    "            else:\n",
    "                relation = \"connected with\"  # Fallback to English if no Korean characters are found\n",
    "\n",
    "            # Format the output\n",
    "            output_triplet = f\"<H> {head} <R> {relation} <T> {tail}\"\n",
    "\n",
    "            # Append the formatted triplet to the list\n",
    "            formatted_triplets.append(output_triplet)\n",
    "\n",
    "        # Find relations\n",
    "        relation_word_info = [word_info for word_info in ocr_result if word_info[0] not in used_words]\n",
    "        nearest_edge_box_indexes = []\n",
    "\n",
    "        for relation_info in relation_word_info:\n",
    "            word, word_box = relation_info\n",
    "            distances = []\n",
    "\n",
    "            for triplet in triplets:\n",
    "                edge_box = triplet['edge_box']\n",
    "                distance = calculate_distance2(word_box, edge_box)\n",
    "                distances.append(distance)\n",
    "                \n",
    "            nearest_edge_box_index = np.argmin(distances)\n",
    "            nearest_edge_box_indexes.append(nearest_edge_box_index)\n",
    "\n",
    "        # Iterate through the nearest_edge_box_indexes and relation_word_info simultaneously\n",
    "        for nearest_edge_box_index, (relation_word, _) in zip(nearest_edge_box_indexes, relation_word_info):\n",
    "            # Check the language of the relation word\n",
    "            if re.search('[\\u3131-\\u3163\\uac00-\\ud7a3]+', head + tail):\n",
    "                # English text, update using 'connected with'\n",
    "                formatted_triplets[nearest_edge_box_index] = formatted_triplets[nearest_edge_box_index].replace('<R> 와 연계된', f'<R> {relation_word}')\n",
    "            else:\n",
    "                # Korean text, update using '와 연계된'\n",
    "                formatted_triplets[nearest_edge_box_index] = formatted_triplets[nearest_edge_box_index].replace('<R> connected with', f'<R> {relation_word}')\n",
    "                \n",
    "        return formatted_triplets, ocr_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        ocr_output = pororo_ocr(image)\n",
    "        formatted_triplets = ''\n",
    "        return formatted_triplets, ocr_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration (GPT4v) Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt4(base64_image, local_model_output, global_model_output, ocr, lang, todo):\n",
    "\n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "      \"model\": \"gpt-4-vision-preview\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"Your task is to generate {todo} of the given block diagram image with the help of Reference summary, Reference triplets and the OCR outputs (word, [x,y,w,h]) only in {lang} Language without mentioning about these helps in the Output. \\nReference summary: {global_model_output} \\nReference triplets: {local_model_output} \\nOCR Output: {ocr}\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"temperature\": 0, \n",
    "      \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    response_json = response.json()\n",
    "\n",
    "    # Extract 'content' from the JSON\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration (GPT4v) QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def gpt4qa(base64_image, local_model_output, global_model_output, ocr, lang, question, todo):\n",
    "\n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "      \"model\": \"gpt-4-vision-preview\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"Your task is to answer the given Question {todo} based on the given block diagram image with the help of Reference summary, Reference triplets and the OCR outputs (word, [x,y,w,h]) only in {lang} Language without mentioning about these helps in the Output. \\nQuestion: {question} \\nReference summary: {global_model_output} \\nReference triplets: {local_model_output} \\nOCR Output: {ocr}\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"temperature\": 0, \n",
    "      \"max_tokens\": 2000,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    response_json = response.json()\n",
    "\n",
    "    # Extract 'content' from the JSON\n",
    "    content = response_json['choices'][0]['message']['content']\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def process_image(input_image, task=\"Short Description\", lang=\"Korean\", question=None):\n",
    "\n",
    "    img = np.array(input_image)\n",
    "    # OCR\n",
    "    ocr_output = pororo_ocr(img)\n",
    "\n",
    "    ## Global Information Extractor\n",
    "    global_model_output = global_model_process(input_image)\n",
    "\n",
    "    ## Local Information Extractor\n",
    "    # Object Detection\n",
    "    # Save Gradio input image to a temporary file\n",
    "    temp_image_path = tempfile.mktemp(suffix=\".jpg\")\n",
    "\n",
    "    # Convert the image to RGB mode before saving\n",
    "    input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(input_image_rgb)\n",
    "    pil_image.save(temp_image_path, format=\"JPEG\")\n",
    "\n",
    "    # sort output generated from Object Detection\n",
    "    txt_data_labels = run_single_image_inference(yolo_model, temp_image_path, stride, names, pt)\n",
    "\n",
    "    # Cleanup: Remove the temporary image file\n",
    "    os.remove(temp_image_path)\n",
    "\n",
    "    # Extract triplets\n",
    "    img = np.array(input_image)\n",
    "    edge, node = read_list(txt_data_labels, img)\n",
    "    crop_image_result = find_closest_node(edge, node)\n",
    "    json_result = {\n",
    "        'results': crop_image_result\n",
    "    }\n",
    "\n",
    "    # Find head, relation, tail\n",
    "    local_model_output, ocr_output = process_json_file(img, json_result)\n",
    "\n",
    "    # Assuming 'input_image' is your NumPy array representing the image\n",
    "    img_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the NumPy array to a PIL image\n",
    "    pil_image2 = Image.fromarray(np.uint8(img_rgb))\n",
    "\n",
    "    # Save the PIL image to a BytesIO object in JPEG format\n",
    "    image_bytes = io.BytesIO()\n",
    "    pil_image2.save(image_bytes, format='JPEG')\n",
    "\n",
    "    # Get the bytes from the BytesIO object\n",
    "    image_data = image_bytes.getvalue()\n",
    "\n",
    "    # Encode bytes to base64\n",
    "    encode_image = base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "    if task == \"Short QA\":\n",
    "        final_result = gpt4qa(encode_image, local_model_output, global_model_output, ocr_output, lang, question, todo='very shortly')\n",
    "    elif task == \"Long QA\":\n",
    "        final_result = gpt4qa(encode_image, local_model_output, global_model_output, ocr_output, lang, question, todo='in detail')\n",
    "    elif task == \"Short Description\":\n",
    "        final_result = gpt4(encode_image, local_model_output, global_model_output, ocr_output, lang, todo='a very short Description in one paragraph only')\n",
    "    else:\n",
    "        final_result = gpt4(encode_image, local_model_output, global_model_output, ocr_output, lang, todo='the Description in detail')\n",
    "        \n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://bc6186a7bda0d640c2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bc6186a7bda0d640c2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Provide sample images as examples\n",
    "sample_images = [\n",
    "    \"test_sample/155502.png\",\n",
    "    \"test_sample/155958.png\",\n",
    "    \"test_sample/160132.png\",\n",
    "    \"test_sample/kor_real_world_292.jpg\",\n",
    "    \"test_sample/kor_flowchart_6458.jpg\",\n",
    "    \"test_sample/kor_graphlr_30.jpg\",\n",
    "    \"test_sample/Connect (19).png\",\n",
    "    \"test_sample/eng_flowchart_1369.jpg\",\n",
    "    \"test_sample/eng_flowchart_2726.jpg\",\n",
    "    # Add more sample image paths as needed\n",
    "]\n",
    "\n",
    "# Create a Gradio interface with custom image display settings and two dropdowns for task and language selection\n",
    "iface = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=[\n",
    "        \"image\",\n",
    "        gr.Dropdown([\"Short Description\", \"Long Description\", \"Short QA\", \"Long QA\"], label=\"Select Task\"),\n",
    "        gr.Dropdown([\"Korean\", \"English\"], label=\"Select Language\"),\n",
    "        gr.Textbox(label=\"Enter Question (QA)\", placeholder=\"Type your question here only for QA Task\", visible=True)\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    examples=[[sample_images[0]], [sample_images[1]], [sample_images[2]], [sample_images[3]], [sample_images[4]], [sample_images[5]], [sample_images[6]], [sample_images[7]], [sample_images[8]]],\n",
    "    examples_per_page=len(sample_images),\n",
    "    title=\"Block Diagram Assistant\",\n",
    "    description=\"Block Diagram Image\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blosum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
