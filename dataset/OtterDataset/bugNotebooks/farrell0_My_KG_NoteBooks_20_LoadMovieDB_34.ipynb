{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7405996c-7c99-4e47-8b5b-69f72355a319",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 00: Notebook overview .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f14aa-01d9-4d3c-9986-a42fa4b15d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  This Notebook is part of a set that demonstrate GNN using a movie dataset.\n",
    "#  About this Notebook,\n",
    "#\n",
    "#  .  There was a Kaggle GNN challenge circa 2019 detailed here,\n",
    "#        https://www.kaggle.com/c/movie-genre-classification/data\n",
    "#\n",
    "#     That data is locked down, but a similarly themed dataset also on \n",
    "#     Kaggle is here,\n",
    "#        https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\n",
    "#\n",
    "#     The above is the data set in use here.\n",
    "#\n",
    "#     In this first NoteBook, basically we mung input data, and create\n",
    "#     graphs.\n",
    "#\n",
    "#\n",
    "#  .  1 GB, plus or minus. Most of that volume comes from reviews. The\n",
    "#     movies run 30-40 MB, and the cast and crew about 190 MB.\n",
    "#\n",
    "#     The data is CSV, with embedded arrays of JSON.\n",
    "#     To remove dependencies on GS/S3 hosted data, this program expects this\n",
    "#     data to be local to the container hosting this Jupyter Notebook.\n",
    "#\n",
    "#     Since we host all of our assets on GitHub, and GitHub has a 25 MB file\n",
    "#     size limit, the total data set is now split across multiple files.\n",
    "#     You do not need to host data on GitHub. You will need the CSV data\n",
    "#     files that we read to be local, and under a folder titled,\n",
    "#        ./02_Files\n",
    "#\n",
    "#  .  The existing schema for just Movies is listed here,\n",
    "#  \n",
    "#        10_movies_metadata.csv\n",
    "#        -----------------------------------------\n",
    "#           adult                      ..   False\n",
    "#           belongs_to_collection      ..\n",
    "#           budget                     ..   2700000\n",
    "#           genres                     ..   \"[{'id': 35, 'name': 'Comedy'}]\"\n",
    "#           homepage                   ..   http://www.animalhouse.com/\n",
    "#           id                         ..   8469\n",
    "#           imdb_id                    ..   tt0077975\n",
    "#           original_language          ..   en\n",
    "#           original_title             ..   Animal House\n",
    "#           overview                   ..   \"At a 1962 College, Dean Vernon Wormer is determined to expel\n",
    "#                                            the entire Delta Tau Chi Fraternity, but those troublemakers\n",
    "#                                            have other plans for him.\"\n",
    "#           popularity                 ..   7.525382\n",
    "#           poster_path                ..   /AuJkgAh7zAGsm7Oo3CGyDtYvzg0.jpg\n",
    "#           production_companies       ..   \"[{'name': 'Universal Pictures', 'id': 33}, {'name': 'Oregon Film Factory',\n",
    "#                                               'id': 13298}, {'name': 'Stage III Productions', 'id': 13300}]\"\n",
    "#           production_countries       ..   \"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\"\n",
    "#           release_date               ..   1978-07-27\n",
    "#           revenue                    ..   141000000\n",
    "#           runtime                    ..   109.0\n",
    "#           spoken_languages           ..   \"[{'iso_639_1': 'en', 'name': 'English'}]\"\n",
    "#           status                     ..   Released\n",
    "#           tagline                    ..   It was the Deltas against the rules... the rules lost!\n",
    "#           title                      ..   Animal House\n",
    "#           video                      ..   False\n",
    "#           vote_average               ..   7.0\n",
    "#           vote_count                 ..   420\n",
    "#\n",
    "#     From the above, we load the following into a DataFrame of Movie nodes,\n",
    "#\n",
    "#           id                         ..   8469\n",
    "#           id_asint                   ..   8469\n",
    "#           title                      ..   Animal House\n",
    "#           genres                     ..   \"[{'id': 35, 'name': 'Comedy'}]\"\n",
    "#           overview                   ..   \"At a 1962 College, Dean Vernon Wormer is determined to expel\n",
    "#                                            the entire Delta Tau Chi Fraternity, but those troublemakers\n",
    "#                                            have other plans for him.\"\n",
    "#           tagline                    ..   It was the Deltas against the rules... the rules lost!\n",
    "#\n",
    "#           popularity                 ..   7.525382\n",
    "#           production_companies       ..   \"[{'name': 'Universal Pictures', 'id': 33}, {'name': 'Oregon Film Factory',\n",
    "#                                               'id': 13298}, {'name': 'Stage III Productions', 'id': 13300}]\"\n",
    "#           release_date               ..   1978-07-27\n",
    "#           revenue                    ..   141000000\n",
    "#           runtime                    ..   109.0\n",
    "#           spoken_languages           ..   \"[{'iso_639_1': 'en', 'name': 'English'}]\"\n",
    "#           vote_average               ..   7.0\n",
    "#           vote_count                 ..   420\n",
    "#\n",
    "#      Notice the following from above,\n",
    "#\n",
    "#         ..  genres is an array of JSON, with each genre being unique identified via a numeric.\n",
    "#             We will take the first genre and put it into a property on each node titled, primary_genre.\n",
    "#         ..  We will leave all remaining JSON untouched, stored as strings.\n",
    "\n",
    "\n",
    "#  .  The existing schema is for Keywords is listed here,\n",
    "#\n",
    "#        11_keywords.csv\n",
    "#        -----------------------------------------\n",
    "#           id                         ..   8469\n",
    "#           keywords                   ..   \"[{'id': 572, 'name': 'sex'}, {'id': 2483, 'name': 'nudity'},\n",
    "#                                             {'id': 3616, 'name': 'college'}, {'id': 157632, 'name': 'fraternity'},\n",
    "#                                             {'id': 158507, 'name': 'gross out comedy'}, {'id': 160450, 'name': 'dean'},\n",
    "#                                             {'id': 171400, 'name': 'fraternity house'}, {'id': 208983, 'name': 'probation'},\n",
    "#                                             {'id': 208992, 'name': '1960s'}, {'id': 209506, 'name': 'college freshman'},\n",
    "#                                             {'id': 236316, 'name': 'anarchic comedy'}]\"\n",
    "#\n",
    "#      From the above, the following is offered,\n",
    "#   \n",
    "#         ..  id  joins with  movie.id\n",
    "#         ..  keywords.id  already enumerates keywords associated with the movies for us.\n",
    "#             Super handy.\n",
    "\n",
    "\n",
    "#  .  We also have data for,\n",
    "# \n",
    "#        ..  12_Credits  (split into; Cast, Crew)\n",
    "#        ..  14|15_Ratings\n",
    "#        ..  16|17_(External) Links\n",
    "#\n",
    "#     And will likely add these at a later date.\n",
    "\n",
    "\n",
    "#  Below we continue by loading the raw data, and performing some validations on\n",
    "#  statements made, assumptions, and similar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a2149-2554-45a3-be06-8e2ebea43932",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Part 01: Load just Movies into a DataFrame, first look, first data corrections, drop unwanted columns\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - (Nothing)\n",
    "   - (Data files local on disk)\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - Boolean :: MY_DEBUG\n",
    "   - DF :: df_movies\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86887d-60ed-489b-89f6-3591d7c0cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Setting display options, and a flag for outputting more information\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "   #\n",
    "pd.set_option(\"display.width\", 480)\n",
    "\n",
    "#  Sets horizontal scroll for wide outputs\n",
    "#\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "#  MY_DEBUG = True\n",
    "MY_DEBUG = False\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a874df7-993f-404d-aa53-c8ca266f40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "   #\n",
    "#  import pandas as pd                                        #  Already above\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c62ab-efc7-44de-ac83-4a0cd23f27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Load DataFrame with raw input data associated with Movies\n",
    "#\n",
    "\n",
    "l_InputFiles  = [\n",
    "   \"./02_Files/40_Movies_01.txt\",\n",
    "   \"./02_Files/41_Movies_02.txt\",\n",
    "]\n",
    "\n",
    "#  Import as type string, and handle errors later.\n",
    "#\n",
    "df_data1 = dd.read_csv(\n",
    "   l_InputFiles,\n",
    "   delimiter  = \",\",\n",
    "   skiprows   = 1,                                            #  Skip the first line of each file, since it's the column headers\n",
    "   dtype      = {\n",
    "      \"adult\"                     : np.dtype(str),\n",
    "      \"belongs_to_collection\"     : np.dtype(str),\n",
    "      \"budget\"                    : np.dtype(str),\n",
    "      \"genres\"                    : np.dtype(str),\n",
    "      \"homepage\"                  : np.dtype(str),\n",
    "      \"id\"                        : np.dtype(str),\n",
    "      \"imdb_id\"                   : np.dtype(str),\n",
    "      \"original_language\"         : np.dtype(str),\n",
    "      \"original_title\"            : np.dtype(str),\n",
    "      \"overview\"                  : np.dtype(str),\n",
    "      \"popularity\"                : np.dtype(str),\n",
    "      \"poster_path\"               : np.dtype(str),\n",
    "      \"production_companies\"      : np.dtype(str),\n",
    "      \"production_countries\"      : np.dtype(str),\n",
    "      \"release_date\"              : np.dtype(str),\n",
    "      \"revenue\"                   : np.dtype(str),\n",
    "      \"runtime\"                   : np.dtype(str),\n",
    "      \"spoken_languages\"          : np.dtype(str),\n",
    "      \"status\"                    : np.dtype(str),\n",
    "      \"tagline\"                   : np.dtype(str),\n",
    "      \"title\"                     : np.dtype(str),\n",
    "      \"video\"                     : np.dtype(str),\n",
    "      \"vote_average\"              : np.dtype(str),\n",
    "      \"vote_count\"                : np.dtype(str),\n",
    "      },\n",
    "   names      = [\n",
    "      \"adult\", \"belongs_to_collection\", \"budget\", \"genres\", \"homepage\", \"id\", \"imdb_id\",\n",
    "      \"original_language\", \"original_title\", \"overview\", \"popularity\", \"poster_path\",\n",
    "      \"production_companies\", \"production_countries\", \"release_date\", \"revenue\", \"runtime\",\n",
    "      \"spoken_languages\", \"status\", \"tagline\", \"title\", \"video\", \"vote_average\", \"vote_count\",\n",
    "      ]\n",
    "   )   \n",
    "\n",
    "df_data1.compute()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dab9a4-76a1-400c-83d3-2ead7cfe57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY: Initial look at the data, sanity check-\n",
    "#\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   print(\"Count of Movies: %d\" % (len(df_data1.index)))\n",
    "      #\n",
    "   print(tabulate(df_data1.head(2), headers='keys', tablefmt='psql'))\n",
    "\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   print(\"\")\n",
    "   print(\"ID         Title                              Tagline\")\n",
    "   print(\"========   ================================   ===========================================\")\n",
    "      #\n",
    "   for l_each in df_data1.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 10):\n",
    "         print(\"%8s   %-32s   %s\" % (l_each.id, l_each.title, l_each.tagline))\n",
    "   \n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Count of Movies: 45466\n",
    "#     +----+---------+----------------------------------------------   -------------------------------------------------------+----------+---------------------------------------------\n",
    "#     |    | adult   | belongs_to_collection                                                                                  |   budget | genres                          \n",
    "#     |----+---------+-------------------------------------------   ----------------------------------------------------------+----------+-------------------------------------------------\n",
    "#     |  0 | False   | {'id': 10194, 'name': 'Toy Story Collectij   pg', 'backdrop_path': '/9FBwqcd9IRruEDUrTdcaafOMKUq.jpg'} | 30000000 | [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id'\n",
    "#     |  1 | False   | nan                                                                                                    | 65000000 | [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'na\n",
    "#     +----+---------+----------------------------------------   -------------------------------------------------------------+----------+---------------------------------------\n",
    "#     \n",
    "#     ID         Title                              Tagline\n",
    "#     ========   ================================   ===========================================\n",
    "#          862   Toy Story                          nan\n",
    "#         8844   Jumanji                            Roll the dice and unleash the excitement!\n",
    "#        15602   Grumpier Old Men                   Still Yelling. Still Fighting. Still Ready for Love.\n",
    "#        31357   Waiting to Exhale                  Friends are the people who let you be yourself... and never let you forget it.\n",
    "#        11862   Father of the Bride Part II        Just When His World Is Back To Normal... He's In For The Surprise Of His Life!\n",
    "#          949   Heat                               A Los Angeles Crime Saga\n",
    "#        11860   Sabrina                            You are cordially invited to the most surprising merger of the year.\n",
    "#        45325   Tom and Huck                       The Original Bad Boys.\n",
    "#         9091   Sudden Death                       Terror goes into overtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba212a-147d-4f1d-815e-4c0e870497c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY: Check df_data.id, see if it is a valid integer\n",
    "#\n",
    "\n",
    "#  A couple of ways to do this. Because we're making this function more\n",
    "#  re-usable, it adds some code below.\n",
    "#\n",
    "    \n",
    "def f_check_int(i_arg1, i_arg2):\n",
    "    \n",
    "   l_cntr = 0\n",
    "   l_fail = 0\n",
    " \n",
    "   #  This approach uses named, then ordinal reference (ordinal: by position/number).\n",
    "   #\n",
    "   # for l_each in i_arg1[[i_arg2]].itertuples():\n",
    "   #    l_cntr += 1\n",
    "   #       #\n",
    "   #    try:\n",
    "   #       int(l_each[1])\n",
    "   #    except:\n",
    "   #       print(\"Column not an integer value: %s\" % (l_each[1]) )\n",
    "   #       l_fail+= 1\n",
    "        \n",
    "   #  This approach uses a dictionary, and is the one we prefer.\n",
    "   #\n",
    "   for l_each in i_arg1.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      try:\n",
    "        \n",
    "         # l_each_dict = l_each._asdict()    \n",
    "         # l_each_id   = l_each_dict.get(i_arg2)\n",
    "        \n",
    "         l_each_id  = l_each._asdict().get(i_arg2)\n",
    "            #\n",
    "         int(l_each_id)\n",
    "        \n",
    "      except:\n",
    "         print(\"Column not an integer value: %s\" % (l_each_id) )\n",
    "         l_fail+= 1\n",
    "        \n",
    "   return(l_cntr, l_fail)\n",
    "           \n",
    "       \n",
    "if (MY_DEBUG):\n",
    "   l_cntr, l_fail = f_check_int(df_data1, \"id\")\n",
    "      #\n",
    "   print(\"\")\n",
    "   print(\"Number of rows: %d   Number of bad rows: %d\" % (l_cntr, l_fail) )  #\n",
    "\n",
    "print(\"--\")\n",
    "    \n",
    "    \n",
    "#  Sample output,\n",
    "#\n",
    "#     Column not an integer value: 1997-08-20\n",
    "#     Column not an integer value: 2012-09-29\n",
    "#     Column not an integer value: 2014-01-01\n",
    "#\n",
    "#     Number of rows: 45466   Number of bad rows: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380ce71-3fba-4908-afaf-99416a1ef760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Create a new column with just valid integer movie ids\n",
    "#\n",
    "\n",
    "#  This one line construct works, is safe, other. But .. .. it overwrites\n",
    "#  the existing id column. There is no means currently, using this construct,\n",
    "#  to save the output to a newly created column.\n",
    "#\n",
    "# df_data = df_data[df_data.id.str.isnumeric()]\n",
    "\n",
    "\n",
    "def f_cast_int(i_arg1):\n",
    "   try:\n",
    "      l_return = int(i_arg1) \n",
    "   except:\n",
    "      l_return = -1\n",
    "   return l_return\n",
    "\n",
    "\n",
    "df_data1[\"id_asint\"] = df_data1.id.map(lambda x: f_cast_int(x) )\n",
    "\n",
    "\n",
    "if(MY_DEBUG):\n",
    "   print(tabulate(df_data1[[\"id\", \"id_asint\", \"title\", \"tagline\"]].head(4), headers='keys', tablefmt='psql'))\n",
    "   print(\"\")\n",
    "      #\n",
    "   l_cntr = 0\n",
    "   l_fail = 0\n",
    "      #\n",
    "   for l_each in df_data1.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(type(l_each.id_asint))\n",
    "      if (l_each.id_asint == -1):\n",
    "         l_fail += 1\n",
    "            #\n",
    "   print(\"\")\n",
    "   print(\"Number of rows with a -1 value for id_asint: %d\" % (l_fail))\n",
    "            \n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     +----+-------+------------+-------------------+--------------------------------------------------------------------------------+\n",
    "#     |    |    id |   id_asint | title             | tagline                                                                        |\n",
    "#     |----+-------+------------+-------------------+--------------------------------------------------------------------------------|\n",
    "#     |  0 |   862 |        862 | Toy Story         | nan                                                                            |\n",
    "#     |  1 |  8844 |       8844 | Jumanji           | Roll the dice and unleash the excitement!                                      |\n",
    "#     |  2 | 15602 |      15602 | Grumpier Old Men  | Still Yelling. Still Fighting. Still Ready for Love.                           |\n",
    "#     |  3 | 31357 |      31357 | Waiting to Exhale | Friends are the people who let you be yourself... and never let you forget it. |\n",
    "#     +----+-------+------------+-------------------+--------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63125ad3-c679-420c-b8e6-1048226be084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Above, we potentially created a number of movies with the\n",
    "#  same id: -1\n",
    "#  These were movies with a non-integer \"id\". This count was reported\n",
    "#  above.\n",
    "#\n",
    "#  Filter these rows out.\n",
    "#\n",
    "\n",
    "df_data2 = df_data1[df_data1.id_asint != -1]\n",
    "\n",
    "print(\"Number of rows pre-processing: %d   Number post: %d\" % (len(df_data1), len(df_data2)))\n",
    "   #\n",
    "del df_data1\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of rows pre-processing: 45466   Number post: 45463\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22d946-f053-40b5-b699-c58b39b31152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Later we will (classify, node label predict) on; title, tagline,\n",
    "#  overview.\n",
    "#  As such, we want to be certain these are present (effectively, not null).\n",
    "#\n",
    "\n",
    "def f_check_title_tagline_overview(i_arg1):\n",
    "   l_cntr1 = 0\n",
    "   l_cntr2 = 0\n",
    "   l_cntr3 = 0\n",
    "      #\n",
    "   for l_each in i_arg1.itertuples():\n",
    "      #\n",
    "      #  This construct should work for missing columns also\n",
    "      #\n",
    "      if not (isinstance(l_each.title   , str)):\n",
    "         l_cntr1 += 1\n",
    "      if not (isinstance(l_each.tagline , str)):\n",
    "         l_cntr2 += 1\n",
    "      if not (isinstance(l_each.overview, str)):\n",
    "         l_cntr3 += 1\n",
    "            #\n",
    "   return (l_cntr1, l_cntr2, l_cntr3)\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr1, l_cntr2, l_cntr3 = f_check_title_tagline_overview(df_data2)\n",
    "      # \n",
    "   print(\"Num of bad titles: %-6d   Bad taglines: %-6d   Bad overviews: %-6d\" % (l_cntr1, l_cntr2, l_cntr3))\n",
    "\n",
    "\n",
    "#  Fixing missing or incorrect data\n",
    "#\n",
    "df_data3 = df_data2.assign(\n",
    "   title    = lambda x: x.title   .fillna(\"Unknown\").astype(str),\n",
    "   tagline  = lambda x: x.tagline .fillna(\"Unknown\").astype(str),\n",
    "   overview = lambda x: x.overview.fillna(\"Unknown\").astype(str),\n",
    "   )\n",
    "      #\n",
    "del df_data2\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr1, l_cntr2, l_cntr3 = f_check_title_tagline_overview(df_data3)\n",
    "      # \n",
    "   print(\"Num of bad titles: %-6d   Bad taglines: %-6d   Bad overviews: %-6d\" % (l_cntr1, l_cntr2, l_cntr3))\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Num of bad titles: 3        Bad taglines: 25051    Bad overviews: 954   \n",
    "#     Num of bad titles: 0        Bad taglines: 0        Bad overviews: 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a827014-0911-4eb4-ab37-4fa6bf8453b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Drop unwanted columns (keep wanted columns)\n",
    "#\n",
    "\n",
    "try:\n",
    "   df_movies = df_data3[[\"id\", \"id_asint\", \"title\", \"overview\", \"tagline\", \"budget\",\n",
    "      \"genres\", \"popularity\", \"production_companies\", \"release_date\", \"revenue\",\n",
    "      \"runtime\", \"vote_average\", \"vote_count\", ]]\n",
    "   df_movies.compute()\n",
    "   del df_data3\n",
    "except:\n",
    "   pass\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   print(tabulate(df_movies.head(2), headers='keys', tablefmt='psql'))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample data,\n",
    "#\n",
    "#     +----+------+------------+-----------+---------------------------------------------------------------------------------------------------------\n",
    "#     |    |   id |   id_asint | title     | overview                                                                                       \n",
    "#     |----+------+------------+-----------+---------------------------------------------------------------------------------------------------------------\n",
    "#     |  0 |  862 |        862 | Toy Story | Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto th\n",
    "#     |  1 | 8844 |       8844 | Jumanji   | When siblings Judy and Peter discover an enchanted board game that opens the door to a magic\n",
    "#     +----+------+------------+-----------+-----------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33b3bd-a47c-4261-8319-f8da65ba271e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Part 02: Check just Genres, a column in Movies, save primary genres with id\n",
    "\n",
    "We are actually done with Movies proper at this point, and have only genres data, which is embedded in Movies, to process.\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed1a2a-6272-4951-8774-353ef2b2e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY: Check column type of genres; Eg., how should we process this data\n",
    "#\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_movies.itertuples():\n",
    "      l_cntr += 1\n",
    "      if (l_cntr < 3):\n",
    "         print(type(l_each.genres))\n",
    "         print(     l_each.genres )\n",
    "        \n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     <class 'str'>\n",
    "#     [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     <class 'str'>\n",
    "#     [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'name': 'Fantasy'}, {'id': 10751, 'name': 'Family'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86d12b-b0e7-452b-a5a5-bf105bda91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE:\n",
    "#\n",
    "#  . Save the original 'genres' string as JSON, 'genres_json'.\n",
    "#  . Save the first 'genres.name' as 'genres_primary'.\n",
    "#  . Save the first 'genres.id'   as 'genres_primary_id'.\n",
    "#\n",
    "#  .  Why 'primary' ?\n",
    "#        Many movies are listed as having many genres. Since we act\n",
    "#        to demonstrate node prediction, we want fewer/easier node\n",
    "#        (types).\n",
    "#\n",
    "#        Given more data, it might be better to just combine these\n",
    "#        multiple genres listing into one (hybrid) genre per movie.\n",
    "\n",
    "\n",
    "#  Effectively, here, we are checking if the string is valid JSON.\n",
    "#  The proper quotes get munged each time we save, and you will see\n",
    "#  we effectively run this mung/cast each time when decoding this\n",
    "#  string.\n",
    "#\n",
    "def f_genres_json(i_arg1):\n",
    "   try:\n",
    "      l_str1 = str(i_arg1)                                                  #  Needed this, was getting odd  json.loads()  errors otherwise\n",
    "      l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3 = json.loads(l_str2)\n",
    "      l_return = l_str3\n",
    "   except:\n",
    "      l_return = json.loads('[{\"id\": -1, \"name\": \"Unknown\"}]')              #  This is a genres_json.id, and genres_json.name\n",
    "   return l_return\n",
    "      #\n",
    "df_movies[\"genres_json\"      ] = df_movies.genres.map     (lambda x: f_genres_json(x)       )\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_movies.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(str(type(l_each.genres_json)) + \"   \" + str(type(l_each.genres_json[0])) + \"   \" + str(l_each.genres_json))\n",
    "        \n",
    "        \n",
    "#  This block can get deleted-\n",
    "#     Effectively this block is replaced by a better block below.\n",
    "#     Really just keeping this block for teaching.\n",
    "#\n",
    "#  def f_genres_primary(i_arg1):\n",
    "#     try:\n",
    "#        l_return = i_arg1[0][\"name\"]\n",
    "#     except:\n",
    "#        l_return = \"Unknown\"\n",
    "#     return l_return\n",
    "#        #\n",
    "#  def f_genres_primary_id(i_arg1):\n",
    "#     try:\n",
    "#        l_return = i_arg1[0][\"id\"]\n",
    "#     except:\n",
    "#        l_return = -1\n",
    "#     return l_return\n",
    "#     #\n",
    "#  df_movies[\"genres_primary\"   ] = df_movies.genres_json.map(lambda x: f_genres_primary   (x) )\n",
    "#  df_movies[\"genres_primary_id\"] = df_movies.genres_json.map(lambda x: f_genres_primary_id(x) )\n",
    "\n",
    "\n",
    "def f_primary(i_arg1, i_col):\n",
    "   try:\n",
    "      l_return = i_arg1[0][i_col]\n",
    "   except:\n",
    "      if (i_col == \"id\"):\n",
    "         l_return = -1\n",
    "      else:\n",
    "         l_return = \"Unknown\"\n",
    "   return l_return\n",
    "      #\n",
    "df_movies[\"genres_primary\"   ] = df_movies.genres_json.map(lambda x: f_primary(x, \"name\") )\n",
    "df_movies[\"genres_primary_id\"] = df_movies.genres_json.map(lambda x: f_primary(x, \"id\"  ) )\n",
    "   #\n",
    "df_movies = df_movies.assign(LABEL=lambda x: \"Movies\")\n",
    "        \n",
    "        \n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "   print(\"\")\n",
    "      #\n",
    "   for l_each in df_movies.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(str(type(l_each.genres_primary)) + \"   \" + l_each.genres_primary + \"   \" + str(type(l_each.genres_primary_id)) + \"   \" + str(l_each.genres_primary_id))\n",
    "        \n",
    "      \n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     <class 'list'>   <class 'dict'>   [{'id': 16, 'name': 'Animation'}, {'id': 35, 'name': 'Comedy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     <class 'list'>   <class 'dict'>   [{'id': 12, 'name': 'Adventure'}, {'id': 14, 'name': 'Fantasy'}, {'id': 10751, 'name': 'Family'}]\n",
    "#     \n",
    "#     <class 'str'>   Animation   <class 'int'>   16\n",
    "#     <class 'str'>   Adventure   <class 'int'>   12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e1f19-e1cd-4668-8e5b-57c72cc3e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY:  (we do create stuff here, but only to report, and discard in the same cell)\n",
    "#   \n",
    "#  Analysis on just genres- How many unique values do we have ?\n",
    "#\n",
    "#     .  Copy just  Movies.genres_json  into a new DataFrame. \n",
    "#     .  Extract all  'Movies.genres_json.name'  from the JSON string into list.\n",
    "#     .  Pivot this list of genres names into separate rows.\n",
    "#     .  Count the unique genres names.\n",
    "\n",
    "\n",
    "df_genres1 = df_movies[[\"genres_json\"]]\n",
    "\n",
    "\n",
    "#  Convert the genres_json array of dictionaries into an array of just genres.names\n",
    "#\n",
    "def f_genres_arr(i_arg1):\n",
    "   l_arr  = []\n",
    "      #\n",
    "   try:\n",
    "      for l_each in i_arg1:\n",
    "         l_name = l_each[\"name\"]\n",
    "         l_arr += [l_name]\n",
    "      l_return = l_arr\n",
    "   except:\n",
    "      l_return = [ \"Unknown\" ]\n",
    "   return l_return\n",
    "      #\n",
    "df_genres1[\"genres_names\"] = df_genres1.genres_json.map(lambda x: f_genres_arr(x), meta=(\"genres_json\", \"object\"))\n",
    "\n",
    "\n",
    "#  Count the above with a group by, and sort\n",
    "#\n",
    "df_genres2 = df_genres1.explode(\"genres_names\")\n",
    "   #\n",
    "df_genres3 = df_genres2.groupby(\"genres_names\")[\"genres_names\"].count().compute().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False)\n",
    "    \n",
    "\n",
    "      ########################################\n",
    "        \n",
    "        \n",
    "#  Output for review\n",
    "#\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "   print(\"All   Genre entries for all movies ..\")\n",
    "   print(\"-------------------------------\")\n",
    "      #\n",
    "   for l_each in df_genres3.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 50):\n",
    "         print(\"Genre name: %-48s  %d\" % (l_each.genres_names, l_each.count))\n",
    "           \n",
    "   print(\"Total: %d\" % (len(df_genres3.index)))\n",
    "   print(\"\")\n",
    "\n",
    "\n",
    "      ########################################\n",
    "\n",
    "\n",
    "#  See how the above differs from df_data2.genres_primary\n",
    "#\n",
    "df_genres4 = df_movies.groupby(\"genres_primary\")[\"genres_primary\"].count().compute().reset_index(name=\"count\").sort_values(by=\"count\", ascending=False)\n",
    "    \n",
    "\n",
    "#  Output for review\n",
    "#\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "   print(\"First Genre entry   for all movies ..\")\n",
    "   print(\"-------------------------------\")\n",
    "      #\n",
    "   for l_each in df_genres4.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 50):\n",
    "         print(\"Genre name: %-48s  %d\" % (l_each.genres_primary, l_each.count))\n",
    "    \n",
    "   print(\"Total: %d\" % (len(df_genres4.index)))\n",
    "   print(\"\")\n",
    "    \n",
    "    \n",
    "      ########################################\n",
    "        \n",
    "        \n",
    "del df_genres1\n",
    "del df_genres2\n",
    "del df_genres3\n",
    "# del df_genres4                                                    #  Save df_genres4, we may use it in the next cell\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     All   Genre entries for all movies ..\n",
    "#     -------------------------------\n",
    "#     Genre name: Drama                                             20265\n",
    "#     Genre name: Comedy                                            13182\n",
    "#     Genre name: Thriller                                          7624\n",
    "#     Genre name: Romance                                           6735\n",
    "#     Genre name: Action                                            6596\n",
    "#     Genre name: Horror                                            4673\n",
    "#     Genre name: Crime                                             4307\n",
    "#     Genre name: Documentary                                       3932\n",
    "#     Genre name: Adventure                                         3496\n",
    "#     Genre name: Science Fiction                                   3049\n",
    "#     Genre name: Family                                            2770\n",
    "#     Genre name: Mystery                                           2467\n",
    "#     Genre name: Fantasy                                           2313\n",
    "#     Genre name: Animation                                         1935\n",
    "#     Genre name: Foreign                                           1622\n",
    "#     Genre name: Music                                             1598\n",
    "#     Genre name: History                                           1398\n",
    "#     Genre name: War                                               1323\n",
    "#     Genre name: Western                                           1042\n",
    "#     Genre name: TV Movie                                          767\n",
    "#     Total: 20\n",
    "#     \n",
    "#     First Genre entry   for all movies ..\n",
    "#     -------------------------------\n",
    "#     Genre name: Drama                                             11966\n",
    "#     Genre name: Comedy                                            8820\n",
    "#     Genre name: Action                                            4489\n",
    "#     Genre name: Documentary                                       3415\n",
    "#     Genre name: Horror                                            2619\n",
    "#     Genre name: Unknown                                           2442                #  We created this value from missing data.\n",
    "#     Genre name: Crime                                             1685\n",
    "#     Genre name: Thriller                                          1665\n",
    "#     Genre name: Adventure                                         1514\n",
    "#     Genre name: Romance                                           1191\n",
    "#     Genre name: Animation                                         1124\n",
    "#     Genre name: Fantasy                                           704\n",
    "#     Genre name: Science Fiction                                   647\n",
    "#     Genre name: Mystery                                           554\n",
    "#     Genre name: Family                                            524\n",
    "#     Genre name: Music                                             487\n",
    "#     Genre name: Western                                           451\n",
    "#     Genre name: TV Movie                                          390\n",
    "#     Genre name: War                                               379\n",
    "#     Genre name: History                                           279\n",
    "#     Genre name: Foreign                                           118\n",
    "#     Total: 21\n",
    "\n",
    "\n",
    "\n",
    "#  Before some of the data corrections/validations we made above, there \n",
    "#  were more Genres Names.\n",
    "#\n",
    "#  Samples of those,\n",
    "#\n",
    "#     Genre name: Odyssey Media                                     1\n",
    "#     Genre name: Pulser Productions                                1\n",
    "#     Genre name: Rogue State                                       1\n",
    "#     Genre name: Vision View Entertainment                         1\n",
    "#     Genre name: Mardock Scramble Production Committee             1\n",
    "#     Genre name: Telescene Film Group Productions                  1\n",
    "#     Genre name: Sentai Filmworks                                  1\n",
    "#     Genre name: GoHands                                           1\n",
    "#     Genre name: Carousel Productions                              1\n",
    "#     Genre name: BROSTA TV                                         1\n",
    "#     Genre name: Aniplex                                           1\n",
    "#     Genre name: The Cartel                                        1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f7366-5615-44e3-aab1-1509e0333d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Graph visually, use one of the already included/installed libraries-\n",
    "#\n",
    "#     See, https://seaborn.pydata.org/generated/seaborn.barplot.html\n",
    "#          https://seaborn.pydata.org/examples/index.html\n",
    "#\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   sns.set(rc = {'figure.figsize':(5,4)})\n",
    "\n",
    "   sns.barplot(x = \"count\", y = \"genres_primary\", data=df_genres4)\n",
    "   \n",
    "   plt.title(\"Genres by Count, Descending\", fontsize = 20)\n",
    "   plt.xlabel(\"Count\",  size = 16)\n",
    "   plt.ylabel(\"Genres\", size = 16)\n",
    "   \n",
    "   plt.show()\n",
    "    \n",
    "    \n",
    "del df_genres4\n",
    "   #\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a9530-6320-436f-86c6-f04c8e63805c",
   "metadata": {},
   "source": [
    "\n",
    "<div> \n",
    "<img src=\"./01_Images/20-BarChart-1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0622f8f-36bf-497d-9797-04e1f5c7ee79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Part 00: Checkpoint our current state\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e39c-5cba-444f-8cb9-969308215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We have a DataFrame titled,  df_movies  with the following features,\n",
    "#\n",
    "#     id                         ..   8469\n",
    "#     id_asint                   ..   8469,              an integer\n",
    "#     title                      ..   Animal House\n",
    "#     overview                   ..   \"At a 1962 College, Dean Vernon Wormer is determined to expel\n",
    "#                                      the entire Delta Tau Chi Fraternity, but those troublemakers\n",
    "#                                      have other plans for him.\"\n",
    "#     tagline                    ..   It was the Deltas against the rules... the rules lost!\n",
    "#     budget                     ..   2700000\n",
    "#     genres                     ..   \"[{'id': 35, 'name': 'Comedy'}]\"\n",
    "#     popularity                 ..   7.525382\n",
    "#     production_companies       ..   \"[{'name': 'Universal Pictures', 'id': 33}, {'name': 'Oregon Film Factory',\n",
    "#                                         'id': 13298}, {'name': 'Stage III Productions', 'id': 13300}]\"\n",
    "#     release_date               ..   1978-07-27\n",
    "#     revenue                    ..   141000000\n",
    "#     runtime                    ..   109.0\n",
    "#     vote_average               ..   7.0\n",
    "#     vote_count                 ..   420\n",
    "#\n",
    "#     genres_json                ..   (same as above, case as JSON/dictionary)\n",
    "#     genres_primary             ..   Just the first genres.name, a string\n",
    "#     genres_primary_id          ..   Just the first genres.id, an integer\n",
    "    \n",
    "    \n",
    "#  Currently, one use case for GNN requires a bi-partitite graph. We have additional data\n",
    "#  sets for,\n",
    "#\n",
    "#        11_keywords.csv\n",
    "#        -----------------------------------------\n",
    "#           id                         ..   8469\n",
    "#           keywords                   ..   \"[{'id': 572, 'name': 'sex'}, {'id': 2483, 'name': 'nudity'},\n",
    "#                                             {'id': 3616, 'name': 'college'}, {'id': 157632, 'name': 'fraternity'},\n",
    "#                                             {'id': 158507, 'name': 'gross out comedy'}, {'id': 160450, 'name': 'dean'},\n",
    "#                                             {'id': 171400, 'name': 'fraternity house'}, {'id': 208983, 'name': 'probation'},\n",
    "#                                             {'id': 208992, 'name': '1960s'}, {'id': 209506, 'name': 'college freshman'},\n",
    "#                                             {'id': 236316, 'name': 'anarchic comedy'}]\"\n",
    "#\n",
    "#      From the above, the following is offered,\n",
    "#   \n",
    "#         ..  id  joins with movie.id\n",
    "#         ..  keywords.id  already enumerates keywords associated with the movies for us.\n",
    "#             Super handy.\n",
    "#\n",
    "#  .  We also have data for,\n",
    "# \n",
    "#        ..  12_Credits  (split into; Cast, Crew)\n",
    "#        ..  14|15_Ratings\n",
    "#        ..  16|17_(External) Links\n",
    "\n",
    "\n",
    "#  From here, we proceed with just  keywords\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bbf87-4a79-4f91-a585-97dbf50c2f65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 03: Work on Keywords, which also gives us the Edge between Movies and Keywords\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_described_by\n",
    "   - Boolean :: MY_DEBUG\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c206abe-b7c9-4f24-9c38-bd59fde31526",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Load DataFrame with raw input data. This time we are looking\n",
    "#  at keywords.\n",
    "#\n",
    "\n",
    "l_InputFiles  = [\n",
    "   \"./02_Files/50_Keywords_00.txt\",\n",
    "]\n",
    "\n",
    "df_data1 = dd.read_csv(\n",
    "   l_InputFiles,\n",
    "   delimiter  = \",\",\n",
    "   skiprows   = 1,                                            #  Skip the first line of each file, since it's the column headers\n",
    "   dtype      = {\n",
    "      \"id\"                        : np.dtype(str),            #  This relates to movie.id\n",
    "      \"keywords\"                  : np.dtype(str),            #  In the source CSV, this column was titled 'values', a bad idea.\n",
    "      },                                                      #     (We see below, this is actually a JSON array; each entry itself with an id and a keyword name.)\n",
    "   names      = [\n",
    "      \"id\", \"keywords\"\n",
    "      ]\n",
    "   )   \n",
    "\n",
    "df_data1.compute()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadf6da-3e86-4d23-89a2-8a7b65b2a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY: Initial look at the data, sanity check-\n",
    "#\n",
    "\n",
    "if(MY_DEBUG):\n",
    "   print(\"Number of keyword data lines: %d\" % len(df_data1.index))\n",
    "      #\n",
    "   print(tabulate(df_data1.head(2), headers='keys', tablefmt='psql'))\n",
    "\n",
    "   l_cntr = 0\n",
    "   l_fail = 0\n",
    "      #\n",
    "   for l_each in df_data1.itertuples():\n",
    "      l_cntr += 1\n",
    "      try:\n",
    "         int(l_each.id)\n",
    "      except:\n",
    "         print(\"Keyword with a non-integer keyword.id value: %s\" % (l_each.id))\n",
    "         l_fail += 1\n",
    "    \n",
    "   print(\"\")\n",
    "      #\n",
    "   print(\"Number of total keyword data lines: %d  Number with numeric id: %d   Number with a non-numeric id: %d\" % (l_cntr, (l_cntr - l_fail), l_fail ))\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of keyword data lines: 46419\n",
    "#     +----+------+----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#     |    |   id | keywords                                                                                                                      \n",
    "#     |----+------+------------------------------------------------------------------------------------------------------------------------------------\n",
    "#     |  0 |  862 | [{'id': 931, 'name': 'jealousy'}, {'id': 4290, 'name': 'toy'}, {'id': 5202, 'name': 'boy'}, {'id': 6054, 'name': 'friendship'}, {'id': 97\n",
    "#     |  1 | 8844 | [{'id': 10090, 'name': 'board game'}, {'id': 10941, 'name': 'disappearance'}, {'id': 15101, 'name': \"based on children's book\"}, {'id': 33467, 'name': 'n\n",
    "#     +----+------+--------------------------------------------------------------------------------------------------------------------------------\n",
    "#     \n",
    "#     Number of total keyword data lines: 46419  Number with numeric id: 46419   Number with a non-numeric id: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b956878-e622-4a3e-ac53-90133ccd412f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: We always import data as type string, as we prefer to do data\n",
    "#  correction at our own control, (versus whatever casting failures might\n",
    "#  occur when importing as integer, or other more restrictive data types).\n",
    "#\n",
    "#     (It looked like the keywords.id field was entirely made of integers,\n",
    "#     but, lets also count when we convert this field.)\n",
    "#\n",
    "#  keywords.id was brought in as type string. Create keywords.id_asint\n",
    "#\n",
    "\n",
    "df_data2 = df_data1.assign(\n",
    "   id_asint = lambda x: x.id.fillna(-1).astype(int),\n",
    "   )\n",
    "      #\n",
    "del df_data1\n",
    "\n",
    "\n",
    "if(MY_DEBUG):\n",
    "   print(tabulate(df_data2[[\"id\", \"id_asint\", \"keywords\"]].head(4), headers='keys', tablefmt='psql'))\n",
    "   print(\"\")\n",
    "      #\n",
    "   l_cntr = 0\n",
    "   l_fail = 0\n",
    "      #\n",
    "   for l_each in df_data2.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(type(l_each.id_asint))\n",
    "      if (l_each.id_asint == -1):\n",
    "         l_fail += 1\n",
    "            #\n",
    "   print(\"\")\n",
    "   print(\"Number of rows with a -1 value for id_asint: %d\" % (l_fail))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     +----+-------+------------+----------------------------------------------------------------------------------------------\n",
    "#     |    |    id |   id_asint | keywords                                                                             \n",
    "#     |----+-------+------------+------------------------------------------------------------------------------------------------------\n",
    "#     |  0 |   862 |        862 | [{'id': 931, 'name': 'jealousy'}, {'id': 4290, 'name': 'toy'}, {'id': 5202, 'name': 'boy\n",
    "#     |  1 |  8844 |       8844 | [{'id': 10090, 'name': 'board game'}, {'id': 10941, 'name': 'disappearance'}, {'id': 15101, 'na\n",
    "#     |  2 | 15602 |      15602 | [{'id': 1495, 'name': 'fishing'}, {'id': 12392, 'name': 'best friend'}, {'id': 179431, \n",
    "#     |  3 | 31357 |      31357 | [{'id': 818, 'name': 'based on novel'}, {'id': 10131, 'name': 'interracial relationship'}, {'id':\n",
    "#     +----+-------+------------+--------------------------------------------------------------------------------------\n",
    "#     \n",
    "#     <class 'int'>\n",
    "#     <class 'int'>\n",
    "#     \n",
    "#     Number of rows with a -1 value for id_asint: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8860b61-83f5-4e0e-a92a-3ffa19a73e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  READ-ONLY: Now we have lists of keys for movie.id in two dataframes;\n",
    "#  df_movies from above, and this, df_data\n",
    "#  \n",
    "#  How well do the two lists match up ?  (Do the join pairs align well ?)\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "    \n",
    "   #  Think of this a a standard SQL left outer join-\n",
    "   #\n",
    "   df_data3 = df_data2.copy()\n",
    "      #\n",
    "   df_intersect1 = df_movies.merge(df_data3.drop_duplicates(), on=[\"id_asint\",\"id_asint\"], how=\"left\", indicator=True)\n",
    "      #\n",
    "   l_in_movies   = 0\n",
    "   l_in_both_m   = 0\n",
    "      #\n",
    "   for l_each in df_intersect1.itertuples():\n",
    "      if (l_each._21 == \"both\"):\n",
    "         l_in_both_m   += 1\n",
    "      else:\n",
    "         print(\"Id of movie not found in keywords list: %-10s   %-10s   Title: %s\" % (l_each.id_x, l_each._21, l_each.title))\n",
    "         l_in_movies   += 1\n",
    "            #\n",
    "   del df_data3\n",
    "\n",
    "\n",
    "   print(\"\")\n",
    "            \n",
    "\n",
    "   df_movies2 = df_movies[[\"id_asint\"]]\n",
    "      #\n",
    "   df_intersect2 = df_data2.merge(df_movies2.drop_duplicates(), on=[\"id_asint\",\"id_asint\"], how=\"left\", indicator=True)\n",
    "      #\n",
    "   l_in_keywords = 0\n",
    "   l_in_both_k   = 0\n",
    "      #\n",
    "   for l_each in df_intersect2.itertuples():\n",
    "      if (l_each._4 == \"both\"):\n",
    "         l_in_both_k   += 1\n",
    "      else:\n",
    "         print(\"Id of keyword not found in movies list: %-10s   %-10s   %s\" % (l_each.id_asint, l_each._4, str(l_each.keywords)))\n",
    "         l_in_keywords += 1\n",
    "            #\n",
    "   del df_movies2\n",
    "            \n",
    "            \n",
    "   print(\"\")\n",
    "      #\n",
    "   print(\"Number of movies:                                     %10d\" % (len(df_movies)))\n",
    "   print(\"Number of keyword records:                            %10d\" % (len(df_data2 )))\n",
    "   print(\"-------------------------------------------------------------------------\")\n",
    "   print(\"Count where same id in both lists (parent: movie):    %10d\" % (l_in_both_m))\n",
    "   print(\"Count where same id in both lists (parent: keywords): %10d\" % (l_in_both_k))\n",
    "   print(\"Count where movie.id not in keywords.id:              %10d\" % (l_in_movies))\n",
    "   print(\"Count where keyword.id not in movie.id:               %10d\" % (l_in_keywords))\n",
    "\n",
    "   del df_intersect1\n",
    "   del df_intersect2\n",
    "      #\n",
    "   print(\"\")\n",
    "\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Id of movie not found in keywords list: 401840       left_only    Title: School's out\n",
    "#     \n",
    "#     Number of movies:                                          45463\n",
    "#     Number of keyword records:                                 46419\n",
    "#     -------------------------------------------------------------------------\n",
    "#     Count where same id in both lists (parent: movie):         45462\n",
    "#     Count where same id in both lists (parent: keywords):      46419\n",
    "#     Count where movie.id not in keywords.id:                       1\n",
    "#     Count where keyword.id not in movie.id: \n",
    "\n",
    "#  Above,\n",
    "#\n",
    "#     .  There is one movie id with no corresponding keyword record.\n",
    "#        We are okay with that.\n",
    "#        In graph parlance, we will have two sets of nodes (Movies and \n",
    "#        Keywords), and one of the Movies will have no relationship to\n",
    "#        Keywords. That does happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e505f-e1a1-4508-b6b4-6025b17273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: A number of the keywords string (array)'s are empty. \n",
    "#  Let's get sense of those, and add them to, \n",
    "#     (df_data.keywords_length)\n",
    "#\n",
    "\n",
    "def f_length(i_arg1):\n",
    "   try:\n",
    "      l_str1 = str(i_arg1) \n",
    "      l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3 = json.loads(l_str2)\n",
    "      l_return = len(l_str3)\n",
    "   except:\n",
    "      l_return = -1\n",
    "   return l_return\n",
    "\n",
    "df_data2[\"keywords_length\"] = df_data2.keywords.map(lambda x: f_length(x))\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   df_data3 = df_data2.groupby(\"keywords_length\")[\"keywords_length\"].count().compute().reset_index(name=\"count\").sort_values(by=\"keywords_length\", ascending=True)\n",
    "      #\n",
    "   print(\"Number of keywords == -1,   Means we couldn't parse the JSON string.\")\n",
    "   print(\"Number of keywords ==  0,   Means there really were zero values in valid JSON.\")\n",
    "   print(\"--------------------------------------------\")\n",
    "      #\n",
    "   l_cntr = 0\n",
    "   l_nbad = 0\n",
    "      #\n",
    "   for l_each in df_data3.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 10):\n",
    "         print(\"Number of keywords:   %-6d   How many records of this count: %d\" % (l_each.keywords_length, l_each.count))\n",
    "      if (l_each.keywords_length < 1):\n",
    "         l_nbad += l_each.count\n",
    "\n",
    "   print(\"\")\n",
    "   print(\"Total records: %d   Number of bad records: %d   Percentage: %d %%\" % ( len(df_data2), l_nbad, (l_nbad / len(df_data2) * 100 )) )\n",
    "      #\n",
    "   del df_data3\n",
    "   \n",
    "    \n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     Number of keywords == -1,   Means we couldn't parse the JSON string.\n",
    "#     Number of keywords ==  0,   Means there really were zero values in valid JSON.\n",
    "#     --------------------------------------------\n",
    "#     Number of keywords:   -1       How many records of this count: 631\n",
    "#     Number of keywords:   0        How many records of this count: 14795\n",
    "#     Number of keywords:   1        How many records of this count: 6610\n",
    "#     Number of keywords:   2        How many records of this count: 4779\n",
    "#     Number of keywords:   3        How many records of this count: 4244\n",
    "#     Number of keywords:   4        How many records of this count: 3255\n",
    "#     Number of keywords:   5        How many records of this count: 2802\n",
    "#     Number of keywords:   6        How many records of this count: 1914\n",
    "#     Number of keywords:   7        How many records of this count: 1456\n",
    "#\n",
    "#     Total records: 46419   Number of bad records: 15426   Percentage: 33 %\n",
    "\n",
    "#  Comment; With 33% of our records not having keywords, that will limit\n",
    "#  the effeciveness of adding keywords to any ML/predicive-analytics we perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd23a8-de02-4def-a758-bedde700c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Change/repair those missing/other keyword records above-\n",
    "#\n",
    "#     Strangely, map() works with an \"if\", map_partitions() does not.\n",
    "\n",
    "\n",
    "def f_repair(i_arg1):\n",
    "\n",
    "   #  This form of \"if\" block,\n",
    "   #\n",
    "   #     if (i_arg1.keywords_length < 1):\n",
    "   #        return \"[{'id': -1, 'name': 'Unknown'}]\"\n",
    "   #     else:\n",
    "   #        return i_arg1.keywords\n",
    "   #     return l_return\n",
    "   #\n",
    "   #  Throws this error,\n",
    "   #     >> ValueError('The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().')\n",
    "   #\n",
    "   #  So we use this form of \"if\" below,\n",
    "    \n",
    "   l_return = np.where(i_arg1.keywords_length < 1 , \"[{'id': -1, 'name': 'Unknown'}]\", i_arg1.keywords)\n",
    "                       \n",
    "   return l_return\n",
    "    \n",
    "\n",
    "df_data2[\"keywords_str\"] = df_data2.map_partitions(f_repair)\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_data2.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 10):\n",
    "         print(l_each.keywords_str)\n",
    "    \n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     [{'id': 931, 'name': 'jealousy'}, {'id': 4290, 'name': 'toy'}, {'id': 5202, 'name': 'boy'}, {'id': 6054, 'name': 'friends\n",
    "#     [{'id': -1, 'name': 'Unknown'}]\n",
    "#     [{'id': 1495, 'name': 'fishing'}, {'id': 12392, 'name': 'best friend'}, {'id': 179431, 'name': 'duringcreditsstin\n",
    "#     [{'id': 818, 'name': 'based on novel'}, {'id': 10131, 'name': 'interracial relationship'}, {'id': 14768, 'name': 'single \n",
    "#     [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'name': 'midlife crisis'}, {'id': 2246, 'name': 'confidence'}, {'id': 4995, 'name': '\n",
    "#     [{'id': 642, 'name': 'robbery'}, {'id': 703, 'name': 'detective'}, {'id': 974, 'name': 'bank'}, {'id': 1523, 'name': 'obses\n",
    "#     [{'id': 90, 'name': 'paris'}, {'id': 380, 'name': 'brother brother relationship'}, {'id': 2072, 'name': 'chauff\n",
    "#     [{'id': -1, 'name': 'Unknown'}]\n",
    "#     [{'id': 949, 'name': 'terrorist'}, {'id': 1562, 'name': 'hostage'}, {'id': 1653, 'name': 'explosive'}, {'id': 193533, 'na\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e96dc-29d0-4146-804d-070552ed6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: This cell is a direct copy of a cell two above. Here though, we run the\n",
    "#  routines on our corrected keywords_str, where before it raw keywords.\n",
    "#\n",
    "\n",
    "def f_length(i_arg1):\n",
    "   try:\n",
    "      l_str1 = str(i_arg1) \n",
    "      l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3 = json.loads(l_str2)\n",
    "      l_return = len(l_str3)\n",
    "   except:\n",
    "      l_return = -1\n",
    "   return l_return\n",
    "\n",
    "df_data2[\"keywords_length\"] = df_data2.keywords_str.map(lambda x: f_length(x))\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   df_data3 = df_data2.groupby(\"keywords_length\")[\"keywords_length\"].count().compute().reset_index(name=\"count\").sort_values(by=\"keywords_length\", ascending=True)\n",
    "      #\n",
    "   print(\"Number of keywords == -1,   Means we couldn't parse the JSON string.\")\n",
    "   print(\"Number of keywords ==  0,   Means there really were zero values in valid JSON.\")\n",
    "   print(\"--------------------------------------------\")\n",
    "      #\n",
    "   l_cntr = 0\n",
    "   l_nbad = 0\n",
    "      #\n",
    "   for l_each in df_data3.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 10):\n",
    "         print(\"Number of keywords:   %-6d   How many records of this count: %d\" % (l_each.keywords_length, l_each.count))\n",
    "      if (l_each.keywords_length < 1):\n",
    "         l_nbad += l_each.count\n",
    "\n",
    "   print(\"\")\n",
    "   print(\"Total records: %d   Number of bad records: %d   Percentage: %d %%\" % ( len(df_data2), l_nbad, (l_nbad / len(df_data2) * 100 )) )\n",
    "      #\n",
    "   del df_data3\n",
    "   \n",
    "    \n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output from before,\n",
    "#\n",
    "#     Number of keywords:   -1       How many records of this count: 631\n",
    "#     Number of keywords:   0        How many records of this count: 14795\n",
    "#     Number of keywords:   1        How many records of this count: 6610\n",
    "#     Number of keywords:   2        How many records of this count: 4779\n",
    "#     Number of keywords:   3        How many records of this count: 4244\n",
    "#     Number of keywords:   4        How many records of this count: 3255\n",
    "#     Number of keywords:   5        How many records of this count: 2802\n",
    "#     Number of keywords:   6        How many records of this count: 1914\n",
    "#     Number of keywords:   7        How many records of this count: 1456\n",
    "#\n",
    "#     Total records: 46419   Number of bad records: 15426   Percentage: 33 %\n",
    "\n",
    "#  Sample output now,\n",
    "#\n",
    "#     Number of keywords == -1,   Means we couldn't parse the JSON string.\n",
    "#     Number of keywords ==  0,   Means there really were zero values in valid JSON.\n",
    "#     --------------------------------------------\n",
    "#     Number of keywords:   1        How many records of this count: 22036\n",
    "#     Number of keywords:   2        How many records of this count: 4779\n",
    "#     Number of keywords:   3        How many records of this count: 4244\n",
    "#     Number of keywords:   4        How many records of this count: 3255\n",
    "#     Number of keywords:   5        How many records of this count: 2802\n",
    "#     Number of keywords:   6        How many records of this count: 1914\n",
    "#     Number of keywords:   7        How many records of this count: 1456\n",
    "#     Number of keywords:   8        How many records of this count: 1016\n",
    "#     Number of keywords:   9        How many records of this count: 832\n",
    "#     \n",
    "#     Total records: 46419   Number of bad records: 0   Percentage: 0 %\n",
    "\n",
    "#  Comment;  the data is cleaner, not more informative. Effectively we\n",
    "#  updated all of the (bad) data to equal, [{\"id\": -1, \"name\": \"Unknown\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6543028-a64a-49db-a036-0ea62a03884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE:  keywords_str is currently a string, covert to a dictionary,\n",
    "#  so that we may pivot (explode) later\n",
    "\n",
    "def f_keywords_json(i_arg1):\n",
    "   try:\n",
    "      l_str1 = str(i_arg1)                                                  #  Needed this, was getting odd  json.loads()  errors otherwise\n",
    "      l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3 = json.loads(l_str2)\n",
    "      l_return = l_str3\n",
    "   except:\n",
    "      l_return = json.loads('[{\"id\": -1, \"name\": \"Unknown\"}]')\n",
    "   return l_return\n",
    "\n",
    "df_data2[\"keywords_json\"] = df_data2.keywords_str.map(lambda x: f_keywords_json(x))\n",
    "    \n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_data2.itertuples():\n",
    "      l_cntr +=1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(l_each.id, l_each.keywords_json)\n",
    "         print(type(l_each.keywords_json))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     862 [{'id': 931, 'name': 'jealousy'}, {'id': 4290, 'name': 'toy'}, {'id': 5202, 'name': 'boy'}, {'id': 6054, 'name': 'friendship'}, {'id': 9713, 'name': 'friends'}, {'id': 9823, 'name': 'rivalry'}, {'id': 165503, 'name': 'boy next door'}, {'id': 170722, 'name': 'new toy'}, {'id': 187065, 'name': 'toy comes to life'}]\n",
    "#     <class 'list'>\n",
    "#     8844 [{'id': -1, 'name': 'Unknown'}]\n",
    "#     <class 'list'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555dc2d-5ddf-4d30-84fd-5399bc6e82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  WRITE: Build a new DataFrame with the unique Keywords. \n",
    "#\n",
    "#  We'll use this to count, but also this can be our list of nodes for the graph\n",
    "#  with Label, Keywords.\n",
    "\n",
    "\n",
    "#  Here is our pivot, and drop duplicates\n",
    "#\n",
    "df_data3 = df_data2[[\"id\", \"keywords_json\"]]\n",
    "df_data4 = df_data3.explode(\"keywords_json\")\n",
    "\n",
    "\n",
    "   #\n",
    "df_data4[\"keywords_flat\"] = df_data4.keywords_json.map(lambda x: str(x) )\n",
    "   #\n",
    "df_data5 =  df_data4[[\"keywords_flat\"]].drop_duplicates()\n",
    "    \n",
    "    \n",
    "print(\"Number of input records: %-8d   When pivoted out: %-8d   When de-duplicated: %-8d\" % ( len(df_data3.index), len(df_data4.index), len(df_data5.index) ))\n",
    "    \n",
    "    \n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "   print(\"\")\n",
    "      #\n",
    "   for l_each in df_data5.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 10):\n",
    "         print(str(type(l_each.keywords_flat)) + \"   \" + l_each.keywords_flat)   \n",
    "            \n",
    "    \n",
    "def f_get(i_arg1, i_col):\n",
    "   try:\n",
    "      l_str1   = i_arg1\n",
    "      l_str2   = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3   = json.loads(l_str2)\n",
    "      l_return = l_str3[i_col]\n",
    "   except:\n",
    "      if (i_col == \"id\"):\n",
    "         l_return = -1\n",
    "      else:\n",
    "         l_return = \"Unknown\"\n",
    "   return l_return\n",
    "\n",
    "\n",
    "#  These columns are currently embedded in a dictionary. Pull them up a\n",
    "#  level; ease of use later\n",
    "#\n",
    "\n",
    "df_data5[\"id\"  ] = df_data5.keywords_flat.map(lambda x: f_get(x, \"id\"  ) )\n",
    "df_data5[\"name\"] = df_data5.keywords_flat.map(lambda x: f_get(x, \"name\") )\n",
    "   #\n",
    "df_keywords = df_data5[[\"id\", \"name\"]]\n",
    "df_keywords = df_keywords.assign(LABEL=lambda x: \"Keyword\")\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "   print(\"\")\n",
    "      #\n",
    "   for l_each in df_keywords.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 20):\n",
    "         print(l_each) \n",
    "\n",
    "\n",
    "#  Don't delete df_data2 yet, we use it below-\n",
    "#\n",
    "del  df_data3\n",
    "del  df_data4\n",
    "del  df_data5\n",
    "   #\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     <class 'str'>   {'id': 931, 'name': 'jealousy'}\n",
    "#     <class 'str'>   {'id': 4290, 'name': 'toy'}\n",
    "#     <class 'str'>   {'id': 5202, 'name': 'boy'}\n",
    "#     <class 'str'>   {'id': 6054, 'name': 'friendship'}\n",
    "#     <class 'str'>   {'id': 9713, 'name': 'friends'}\n",
    "#     <class 'str'>   {'id': 9823, 'name': 'rivalry'}\n",
    "#     <class 'str'>   {'id': 165503, 'name': 'boy next door'}\n",
    "#     <class 'str'>   {'id': 170722, 'name': 'new toy'}\n",
    "#     <class 'str'>   {'id': 187065, 'name': 'toy comes to life'}\n",
    "#     \n",
    "#     Pandas(Index=0, id=931, name='jealousy', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=4290, name='toy', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=5202, name='boy', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=6054, name='friendship', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=9713, name='friends', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=9823, name='rivalry', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=165503, name='boy next door', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=170722, name='new toy', LABEL='Keyword')\n",
    "#     Pandas(Index=0, id=187065, name='toy comes to life', LABEL='Keyword')\n",
    "#     Pandas(Index=1, id=-1, name='Unknown', LABEL='Keyword')\n",
    "#     Pandas(Index=2, id=1495, name='fishing', LABEL='Keyword')\n",
    "#     Pandas(Index=2, id=12392, name='best friend', LABEL='Keyword')\n",
    "#     Pandas(Index=2, id=179431, name='duringcreditsstinger', LABEL='Keyword')\n",
    "#     Pandas(Index=2, id=208510, name='old men', LABEL='Keyword')\n",
    "#     Pandas(Index=3, id=818, name='based on novel', LABEL='Keyword')\n",
    "#     Pandas(Index=3, id=10131, name='interracial relationship', LABEL='Keyword')\n",
    "#     Pandas(Index=3, id=14768, name='single mother', LABEL='Keyword')\n",
    "#     Pandas(Index=3, id=15160, name='divorce', LABEL='Keyword')\n",
    "#     Pandas(Index=3, id=33455, name='chick flick', LABEL='Keyword')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cac69b-a821-4e3f-b0df-b0a035bcdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  We need a different pivot to build the edge between Movies and Keywords.\n",
    "#\n",
    "#     (Basically, we use the outer key/id, which points to movies. The id\n",
    "#      we were processing above was the key/id for keyword.)\n",
    "#\n",
    "\n",
    "df_data3 = df_data2[[\"id\", \"keywords_json\"]]\n",
    "   #\n",
    "df_data4 = df_data3.explode(\"keywords_json\")\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_data4.itertuples():\n",
    "      l_cntr += 1\n",
    "      if (l_cntr < 3):\n",
    "         print(l_each)\n",
    "    \n",
    "\n",
    "def f_keyword_id(i_arg1):\n",
    "   try:\n",
    "      l_str1 = str(i_arg1) \n",
    "      l_str2 = l_str1.replace(\"'\", \"\\\"\")\n",
    "      l_str3 = json.loads(l_str2)\n",
    "         #\n",
    "      l_return = l_str3[\"id\"]\n",
    "   except:\n",
    "      l_return = -1\n",
    "          \n",
    "   return l_return\n",
    "\n",
    "\n",
    "df_data4[\"movie_id\"  ] = df_data4.id.map(lambda x: x)\n",
    "   #\n",
    "df_data4[\"keyword_id\"] = df_data4.keywords_json.map(lambda x: f_keyword_id(x) )\n",
    "   #\n",
    "df_data5 = df_data4[[\"movie_id\", \"keyword_id\"]]\n",
    "   #\n",
    "df_edges = df_data5.drop_duplicates()\n",
    "df_edges = df_edges.assign(TYPE=lambda x: \"RELATES_TO\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Number of input records: %-8d   When pivoted out: %-8d   When de-duplicated: %-8d\" % ( len(df_data3.index), len(df_data4.index), len(df_edges.index) ))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   l_cntr = 0\n",
    "      #\n",
    "   for l_each in df_edges.itertuples():\n",
    "      l_cntr += 1\n",
    "         #\n",
    "      if (l_cntr < 3):\n",
    "         print(l_each)\n",
    "            \n",
    "del df_data2\n",
    "del df_data3\n",
    "del df_data4\n",
    "del df_data5\n",
    "        \n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "# Sample output,\n",
    "#\n",
    "#     Pandas(Index=0, id='862', keywords_json={'id': 931, 'name': 'jealousy'})\n",
    "#     Pandas(Index=0, id='862', keywords_json={'id': 4290, 'name': 'toy'})\n",
    "#     \n",
    "#     Number of input records: 46419      When pivoted out: 168018     When de-duplicated: 165494  \n",
    "#     \n",
    "#     Pandas(Index=0, movie_id='862', keyword_id=931, TYPE='RELATES_TO')\n",
    "#     Pandas(Index=0, movie_id='862', keyword_id=4290, TYPE='RELATES_TO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2724f0c5-1669-4ede-9648-0f497c45cfc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 00: Checkpoint our current state\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - DF :: df_movies\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8074a0-0eec-49b9-9d6a-23130f0804f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  df_movies  is our Movie DataFrame\n",
    "#\n",
    "print(\"Number of Movies: %d\" % (len(df_movies.index) ))\n",
    "   #\n",
    "if (MY_DEBUG):\n",
    "   print(tabulate(df_movies.head(2), headers='keys', tablefmt='psql'))\n",
    "   print(\"\")\n",
    "\n",
    "#  df_keywords  is our Keywords DataFrame\n",
    "#\n",
    "print(\"Number of Keywords: %d\" % (len(df_keywords.index) ))\n",
    "   #\n",
    "if (MY_DEBUG):\n",
    "   print(tabulate(df_keywords.head(2), headers='keys', tablefmt='psql'))\n",
    "   print(\"\")\n",
    "\n",
    "#  df_edges  is our Edges DataFrame\n",
    "#\n",
    "df_edges = df_edges.assign(TYPE=lambda x: \"RELATES_TO\")\n",
    "print(\"Number of Edges: %d\" % (len(df_edges.index) ))\n",
    "   #\n",
    "if (MY_DEBUG):\n",
    "   print(tabulate(df_edges.head(2), headers='keys', tablefmt='psql'))\n",
    "   print(\"\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"--\")\n",
    "\n",
    "#  Sample output\n",
    "#\n",
    "#     Number of Movies: 45463\n",
    "#     +----+------+-----------+-------------------------    \n",
    "#     |    |   id | title     | overview                       ...                                           | genres_primary   |   genres_primary_id | LABEL   |\n",
    "#     |----+------+-----------+--------------------            ...  \n",
    "#     |  0 |  862 | Toy Story | Led by Woody, Andy's toys      ...            d': 10751, 'name': 'Family'}]  | Animation        |                  16 | Movie   |\n",
    "#     |  1 | 8844 | Jumanji   | When siblings Judy and Pe      ...    sy'}, {'id': 10751, 'name': 'Family'}] | Adventure        |                  12 | Movie   |\n",
    "#     +----+------+-----------+------------------\n",
    "#     \n",
    "#     Number of Keywords: 19424\n",
    "#     +----+---------------------------------+---------+\n",
    "#     |    | keywords_str                    | LABEL   |\n",
    "#     |----+---------------------------------+---------|\n",
    "#     |  0 | {'id': 931, 'name': 'jealousy'} | Keyword |\n",
    "#     |  0 | {'id': 4290, 'name': 'toy'}     | Keyword |\n",
    "#     +----+---------------------------------+---------+\n",
    "#\n",
    "#     Number of Keywords: 165494\n",
    "#     +----+------------+--------------+------------+\n",
    "#     |    |   movie_id |   keyword_id | TYPE       |\n",
    "#     |----+------------+--------------+------------|\n",
    "#     |  0 |        862 |          931 | RELATES_TO |\n",
    "#     |  0 |        862 |         4290 | RELATES_TO |\n",
    "#     +----+------------+--------------+------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9417163-054c-4b1b-be69-04af4b48d147",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  Part 04: Setting up (n) graphs, plus some useful checking\n",
    "\n",
    "Enter:  \n",
    "\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - **my_graph1**            A similarity graph, used for our first GNN routine\n",
    "   - **my_graph2**            A homogeneous, bi-partite graph, used for our second GNN routine\n",
    "   -    ...\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a009883-fadd-4dfe-9f83-efd5e4ef6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  The KatanaGraph remote API is expected to run from a node external to\n",
    "#  the Katana Graph cluster itself.\n",
    "#\n",
    "#  This differs from the distributed API, which is meant to run primitives\n",
    "#  on the Katana Graph worker nodes.\n",
    "#\n",
    "\n",
    "from katana import remote\n",
    "from katana.remote import import_data\n",
    "\n",
    "my_client = remote.Client()\n",
    "\n",
    "print(my_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc3b51-a38f-406d-b4cb-4bcd13c5f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PARTITIONS  = 3\n",
    "   #\n",
    "DB_NAME         = \"my_db\"\n",
    "GRAPH_NAME1     = \"my_graph1\"                         #  Much later we may use 3 graphs. We could\n",
    "GRAPH_NAME2     = \"my_graph2\"                         #  also just use 'projections'.\n",
    "GRAPH_NAME3     = \"my_graph3\"\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3aad8-ae4a-4849-8419-d343026999d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This section; basic graph and database setup, reset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfb83b-fd1f-46c3-96ad-95ceed14058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DELETE ALL GRAPHS\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   for l_graph in my_client.get_database(name=l_database.name).graphs_in_database():\n",
    "      l_handle=my_client.get_database(name=l_database.name).get_graph_by_id(id=l_graph.graph_id)\n",
    "      l_handle.delete()\n",
    "\n",
    "for l_graph in my_client.graphs():\n",
    "   print(\"GRAPH ID: \", l_graph.graph_id, \"      GRAPH Version: \", l_graph.version)\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04909ce0-17bc-4214-84a7-3016cdbf52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DELETE ALL DATABASES\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   if (l_database.name != \"default\"):\n",
    "      my_client.get_database(name=l_database.name).delete_database()\n",
    "      print(\"--\")\n",
    "\n",
    "for l_database in my_client.databases():\n",
    "   print(\"DB ID: \", l_database.database_id, \"     DB Name: \", l_database.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb24521-9a8d-4fad-bdec-52876bc71e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CREATE DATABASE\n",
    "\n",
    "my_database = my_client.create_database(name=DB_NAME)\n",
    "\n",
    "print(my_database.database_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6371d-6d53-4052-b895-f61c76b0792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CREATE GRAPHS\n",
    "\n",
    "my_graph1=my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME1, num_partitions=NUM_PARTITIONS)\n",
    "my_graph2=my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME2, num_partitions=NUM_PARTITIONS)\n",
    "my_graph3=my_client.get_database(name=DB_NAME).create_graph(name=GRAPH_NAME3, num_partitions=NUM_PARTITIONS)\n",
    "\n",
    "print(my_graph1)\n",
    "print(my_graph2)\n",
    "print(my_graph3)\n",
    "\n",
    "my_graph = my_graph1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f26f43-4698-4a9f-bdf8-cec03a9ed226",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Part 05: Actually import into the graphs\n",
    "\n",
    "Enter:  \n",
    "    \n",
    "   - **my_graph1**            A similarity graph, used for our first GNN routine\n",
    "   - **my_graph2**            A homogeneous, bi-partite graph, used for our second GNN routine\n",
    "   -    ...\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   - **my_graph1**            A similarity graph, used for our first GNN routine\n",
    "   - **my_graph2**            A homogeneous, bi-partite graph, used for our second GNN routine\n",
    "   -    ...\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f5092-fcf9-4cdb-861f-8d22a0a2fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Need to drop movies.genres_json because of this issue,\n",
    "#\n",
    "#     ValueError: Failed to convert partition to expected pyarrow schema:\n",
    "#         `ArrowTypeError(\"Expected bytes, got a 'list' object\", 'Conversion failed for column genres_json with type object')`\n",
    "#     \n",
    "#     Expected partition schema:\n",
    "#         id: string\n",
    "#         title: string\n",
    "#              ...\n",
    "#         vote_count: string\n",
    "#         genres_json: string                                              <-------\n",
    "#         genres_primary: string\n",
    "#         genres_primary_id: string\n",
    "#         LABEL: string\n",
    "#     \n",
    "#     Received partition schema:\n",
    "#         id: string\n",
    "#         title: string\n",
    "#              ...\n",
    "#         vote_count: string\n",
    "#         genres_json: list<item: struct<id: int64, name: string>>         <-------\n",
    "#           child 0, item: struct<id: int64, name: string>\n",
    "#               child 0, id: int64\n",
    "#               child 1, name: string\n",
    "#         genres_primary: string\n",
    "#         genres_primary_id: int64\n",
    "#         LABEL: string\n",
    "\n",
    "\n",
    "#  df_movies.drop(\"genres_json\", axis=1)\n",
    "#  df_movies.compute()\n",
    "#  \n",
    "#  print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba15117-b555-4953-8580-38e4222f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "#\n",
    "#  Finally now, load the vertices/nodes into a graph\n",
    "#\n",
    "#  Some hinkiness we need to work around ..\n",
    "#\n",
    "#     .  The Dask DataFrames here were loaded from CSV, and those CSV\n",
    "#        files were found, in scope.\n",
    "#        The KG DataFrame importer will reference that same file\n",
    "#        pathname, and the file will not be in scope. Basically,\n",
    "#        it was expected that these files be on S3/GS all along.\n",
    "#        I hate to have that dependency because, just one more thing\n",
    "#        to have to manage.\n",
    "\n",
    "pd_movies      = df_movies.compute()\n",
    "pd_keywords    = df_keywords.compute()\n",
    "pd_edges       = df_edges.compute()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ce45-e17f-49e4-ac98-af1dc3bf8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Import into graph1, out full graph\n",
    "#\n",
    "\n",
    "with import_data.DataFrameImporter(my_graph1) as df_importer:\n",
    "    \n",
    "   #  Movies\n",
    "   #\n",
    "   df_importer.nodes_dataframe(\n",
    "      pd_movies[[\n",
    "         \"id\", \"title\", \"overview\", \"tagline\", \"budget\", \"genres\", \"popularity\", \"production_companies\",\n",
    "         \"release_date\", \"revenue\", \"runtime\", \"vote_average\", \"vote_count\", \"genres_primary\",\n",
    "         \"genres_primary_id\", \"LABEL\"\n",
    "      ]],\n",
    "      id_column  = \"id\",\n",
    "      id_space   = \"Movies\"\n",
    "      )\n",
    "\n",
    "   #  Keywords\n",
    "   #\n",
    "   df_importer.nodes_dataframe(\n",
    "      pd_keywords[[\"id\", \"name\", \"LABEL\"]],\n",
    "      id_column  = \"id\",\n",
    "      id_space   = \"Keywords\"\n",
    "      )  \n",
    "    \n",
    "   #  Edge, RELATES_TO\n",
    "   #\n",
    "   #  We do this twice, to provide for reverse edges\n",
    "   #\n",
    "   df_importer.edges_dataframe(\n",
    "      pd_edges[[\"movie_id\", \"keyword_id\", \"TYPE\"]],\n",
    "      source_id_space      = \"Movies\",\n",
    "      destination_id_space = \"Keywords\",\n",
    "      source_column        = \"movie_id\",\n",
    "      destination_column   = \"keyword_id\",\n",
    "      type                 = \"RELATES_TO\"\n",
    "      )\n",
    "   df_importer.edges_dataframe(\n",
    "      pd_edges[[\"movie_id\", \"keyword_id\", \"TYPE\"]],\n",
    "      source_id_space      = \"Keywords\",\n",
    "      destination_id_space = \"Movies\",\n",
    "      source_column        = \"keyword_id\",\n",
    "      destination_column   = \"movie_id\",\n",
    "      type                 = \"RELATES_TO\"\n",
    "      )\n",
    "\n",
    "   df_importer.insert()\n",
    "\n",
    "del pd_movies\n",
    "del pd_keywords\n",
    "del pd_edges\n",
    "\n",
    "print(\"--\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd8587-3ed9-4659-b550-95811d2e8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  UI for choosing which of 3 graphs to point to\n",
    "#\n",
    "#     (Much like HTML, this visual control has an ID. So, you can only have\n",
    "#      one of these. IF you need two, you can, but you have more variable\n",
    "#      work to do.)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#  Support function for a radio button UI used below\n",
    "#\n",
    "\n",
    "l_arr = [\n",
    "   \"Graph 01 - Full graph\",\n",
    "   \"Graph 02 - Similarity graph, just nodes of one type\",\n",
    "   \"Graph 03 - Bi-paritite graph\",\n",
    "]\n",
    "\n",
    "\n",
    "def f_set_graph():\n",
    "   global l_whichgraph\n",
    "\n",
    "#  if   (l_whichgraph.value == \"Graph 02 - Similarity graph, just nodes of one type\"):\n",
    "   if   (l_whichgraph.value == l_arr[1]):\n",
    "      l_str = GRAPH_NAME2\n",
    "   elif (l_whichgraph.value == l_arr[2]):\n",
    "      l_str = GRAPH_NAME3\n",
    "   else:\n",
    "      l_str = GRAPH_NAME1\n",
    "   \n",
    "   try:\n",
    "      l_my_graph, *_ = my_client.get_database(name=DB_NAME).find_graphs_by_name(l_str)\n",
    "      return l_my_graph\n",
    "   except:\n",
    "      return None\n",
    "\n",
    "\n",
    "l_whichgraph = widgets.RadioButtons(\n",
    "   options = [\n",
    "      l_arr[0],\n",
    "      l_arr[1],\n",
    "      l_arr[2],\n",
    "   ],\n",
    "   default     = l_arr[0],\n",
    "   description = \"Use this: \",\n",
    "   disabled    = False,\n",
    "   layout      = {'width': 'max-content'}\n",
    ")\n",
    "\n",
    "l_whichgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedb2fa-cec7-474a-acc2-5cefbbb3660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Between the DataFrame and the graph we lose 30 nodes-\n",
    "#     (We should investigate that at some point. It's a little)\n",
    "#      crazy, because the edges are okay.)\n",
    "\n",
    "if (MY_DEBUG):\n",
    "   display(\"Number of DataFrame Nodes: %d\" % ( len(df_movies.index) + len(df_keywords.index) ))\n",
    "   display(\"Number of DataFrame Edges: %d\" % ( len(df_edges.index ) * 2                      ))\n",
    "      #                                       \n",
    "   print(\"\")\n",
    "   \n",
    "   my_graph = f_set_graph()\n",
    "   print(my_graph)\n",
    "                                              \n",
    "   display(\"Number of Graph Nodes: %d\" % (my_graph.num_nodes()))\n",
    "   display(\"Number of Graph Edges: %s\" % (my_graph.num_edges()))\n",
    "\n",
    "print(\"--\")\n",
    "\n",
    "\n",
    "#  Sample output,\n",
    "#\n",
    "#     'Number of DataFrame Nodes: 64886'\n",
    "#     'Number of DataFrame Edges: 330988'\n",
    "#     'Number of Graph Nodes: 64856'\n",
    "#     'Number of Graph Edges: 330988'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de0948-d6b5-4095-844b-e5cad36a5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (MY_DEBUG):\n",
    "   my_graph = f_set_graph()\n",
    "\n",
    "   l_result = my_graph.query(\"\"\"\n",
    "   \n",
    "      MATCH (n) -[r]-> (m)\n",
    "      RETURN n, r, m\n",
    "      LIMIT 1000                        //  Limit is 25,000 for visualization, smaller is better\n",
    "      \n",
    "      \"\"\",\n",
    "      contextualize=True)\n",
    "   \n",
    "   l_result.view()\n",
    "    \n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c909d-a6ec-4abc-9a5c-09bf4dab6894",
   "metadata": {},
   "source": [
    "\n",
    "<div> \n",
    "<img src=\"./01_Images/10-Movie-Query-1.png\" alt=\"Drawing\" style=\"width: 1600px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e6093-0b9a-46e9-b427-175aa90a0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (MY_DEBUG):\n",
    "   my_graph = f_set_graph()\n",
    "\n",
    "   l_result = my_graph.query(\"\"\"\n",
    "\n",
    "      MATCH (n: Movies) -[r]-> (m)\n",
    "      WHERE n.id = \"8469\"                    //  Animal House\n",
    "      RETURN n, r, m\n",
    "      \n",
    "      \"\"\",\n",
    "      contextualize=True)\n",
    "   \n",
    "   l_result.view()\n",
    "\n",
    "print(\"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da066a7-1cd0-495b-885e-3529e4c68586",
   "metadata": {},
   "source": [
    "\n",
    "<div> \n",
    "<img src=\"./01_Images/11-Movie-Query-2.png\" alt=\"Drawing\" style=\"width: 1600px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c242fae-0e14-428d-842e-ef41db8a9312",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Part 06: Also write data to local file\n",
    "\n",
    "Enter:  \n",
    "    \n",
    "   - **my_graph1**            A similarity graph, used for our first GNN routine\n",
    "   - **my_graph2**            A homogeneous, bi-partite graph, used for our second GNN routine\n",
    "   -    ...\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG\n",
    "   \n",
    "Exit:\n",
    "\n",
    "   -\n",
    "   -\n",
    "   -    ...\n",
    "   - **my_graph1**            A similarity graph, used for our first GNN routine\n",
    "   - **my_graph2**            A homogeneous, bi-partite graph, used for our second GNN routine\n",
    "   -    ...\n",
    "   - DF :: df_movies\n",
    "   - DF :: df_keywords\n",
    "   - DF :: df_edges\n",
    "   - Boolean :: MY_DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded4162-031e-4cc1-b01c-03dfb28cd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Save our DataFrames as local files for any possible use later on.\n",
    "#  More,\n",
    "#\n",
    "#     .  As a local file, there are no limits on file size, other.\n",
    "#        Because we backup to GitHub, which does have a 25MB file size limit,\n",
    "#        we need to keep these files smaller than 25MB.\n",
    "#\n",
    "#     .  So, we will write as Parquet with Gip thrown in.\n",
    "#\n",
    "\n",
    "#  Not required; convert Dask DataFrame to Pandas to eliminate partitioning.\n",
    "#  Larger files, we wouldn't do this.\n",
    "#\n",
    "if (MY_DEBUG):\n",
    "   pd_movies = df_movies.compute()\n",
    "      #\n",
    "   pd_movies.to_parquet(\"02_Files/44_movies.parquet.gzip\", compression=\"gzip\")          #  Right now, about 9MB\n",
    "   \n",
    "   \n",
    "   pd_keywords = df_keywords.compute()\n",
    "      #\n",
    "   pd_keywords.to_parquet(\"02_Files/54_keywords.parquet.gzip\", compression=\"gzip\")      #  Right now, less than 1MB\n",
    "   \n",
    "   \n",
    "   pd_edges = df_edges.compute()\n",
    "      #\n",
    "   pd_edges.to_parquet(\"02_Files/64_edges.parquet.gzip\", compression=\"gzip\")            #  Right now, less than 1MB\n",
    "\n",
    "\n",
    "print(\"--\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
