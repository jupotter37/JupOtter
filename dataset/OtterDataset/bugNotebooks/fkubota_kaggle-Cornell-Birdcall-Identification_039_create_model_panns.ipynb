{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- アライさんのsedの[入門ノートブック](https://www.kaggle.com/hidehisaarai1213/introduction-to-sound-event-detection)を参考にしている。\n",
    "- nb025の改良\n",
    "    - f_scoreを追加"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "cmd = \"git rev-parse --short HEAD\"\n",
    "hash = subprocess.check_output(cmd.split()).strip().decode('utf-8')\n",
    "print(hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '038'\n",
    "DEBUG = False\n",
    "N_SPLITS = 5\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 35\n",
    "DIR_MODEL = './../data_ignore/model'\n",
    "# PATH_EVENT = './../data_ignore/event/nb017_event_rms/nb017_event_rms.csv'\n",
    "\n",
    "if DEBUG:\n",
    "    print('debug mode')\n",
    "    N_SPLITS = 2\n",
    "    N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:31.262304Z",
     "iopub.status.busy": "2020-08-14T10:26:31.261641Z",
     "iopub.status.idle": "2020-08-14T10:26:43.679533Z",
     "shell.execute_reply": "2020-08-14T10:26:43.678430Z"
    },
    "papermill": {
     "duration": 12.43994,
     "end_time": "2020-08-14T10:26:43.679682",
     "exception": false,
     "start_time": "2020-08-14T10:26:31.239742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import audioread\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display as display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "# from catalyst.dl import SupervisedRunner, State, CallbackOrder, Callback, CheckpointCallback\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:43.721167Z",
     "iopub.status.busy": "2020-08-14T10:26:43.720157Z",
     "iopub.status.idle": "2020-08-14T10:26:43.726429Z",
     "shell.execute_reply": "2020-08-14T10:26:43.725812Z"
    },
    "papermill": {
     "duration": 0.033698,
     "end_time": "2020-08-14T10:26:43.726552",
     "exception": false,
     "start_time": "2020-08-14T10:26:43.692854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "    \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger\n",
    "    \n",
    "    \n",
    "@contextmanager\n",
    "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    \n",
    "    \n",
    "set_seed(1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "#         self.best_state_dict = {}\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "#         self.best_state_dict = model.state_dict()\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:43.757897Z",
     "iopub.status.busy": "2020-08-14T10:26:43.757128Z",
     "iopub.status.idle": "2020-08-14T10:26:43.759607Z",
     "shell.execute_reply": "2020-08-14T10:26:43.760169Z"
    },
    "papermill": {
     "duration": 0.021548,
     "end_time": "2020-08-14T10:26:43.760287",
     "exception": false,
     "start_time": "2020-08-14T10:26:43.738739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ROOT = Path.cwd().parent\n",
    "INPUT_ROOT = ROOT / \"data_ignore/external_dataset/32khz\"\n",
    "# RAW_DATA = INPUT_ROOT / \"birdsong-recognition\"\n",
    "TRAIN_AUDIO_DIR = RAW_DATA / \"train_audio\"\n",
    "TRAIN_RESAMPLED_AUDIO_DIRS = [\"../data_ignore/external_dataset/32khz/birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)]\n",
    "# TEST_AUDIO_DIR = RAW_DATA / \"test_audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent\n",
    "INPUT_EX_ROOT = ROOT / 'data_ignore/external_dataset/32khz'\n",
    "TRAIN_RESAMPLED_AUDIO_DIRS = [INPUT_EX_ROOT / \"birdsong-resampled-train-audio-{:0>2}\"\n",
    "                              .format(i)  for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:43.794375Z",
     "iopub.status.busy": "2020-08-14T10:26:43.793466Z",
     "iopub.status.idle": "2020-08-14T10:26:44.076854Z",
     "shell.execute_reply": "2020-08-14T10:26:44.075869Z"
    },
    "papermill": {
     "duration": 0.30389,
     "end_time": "2020-08-14T10:26:44.076977",
     "exception": false,
     "start_time": "2020-08-14T10:26:43.773087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_RESAMPLED_AUDIO_DIRS[0] / \"train_mod.csv\")\n",
    "\n",
    "test = pd.read_csv('./../data/external_dataset/birdcall-check/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:44.186427Z",
     "iopub.status.busy": "2020-08-14T10:26:44.160452Z",
     "iopub.status.idle": "2020-08-14T10:26:44.202387Z",
     "shell.execute_reply": "2020-08-14T10:26:44.201812Z"
    },
    "papermill": {
     "duration": 0.062799,
     "end_time": "2020-08-14T10:26:44.202497",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.139698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        \"\"\"Implementation of STFT with Conv1d. The function has the same output \n",
    "        of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame\n",
    "        if win_length is None:\n",
    "            win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified\n",
    "        if hop_length is None:\n",
    "            hop_length = int(win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size\n",
    "        fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n",
    "            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, data_length)\n",
    "        Returns:\n",
    "          real: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "          imag: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "    \n",
    "    \n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0, \n",
    "        freeze_parameters=True):\n",
    "        \"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.core.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n",
    "            win_length=win_length, window=window, center=center, \n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        Returns:\n",
    "          spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    \n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n",
    "        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        \"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps)\n",
    "        \n",
    "        Output: (batch_size, time_steps, mel_bins)\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        \"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.core.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:44.246248Z",
     "iopub.status.busy": "2020-08-14T10:26:44.240679Z",
     "iopub.status.idle": "2020-08-14T10:26:44.249107Z",
     "shell.execute_reply": "2020-08-14T10:26:44.248561Z"
    },
    "papermill": {
     "duration": 0.03404,
     "end_time": "2020-08-14T10:26:44.249211",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.215171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012344,
     "end_time": "2020-08-14T10:26:44.325825",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.313481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:44.384942Z",
     "iopub.status.busy": "2020-08-14T10:26:44.363459Z",
     "iopub.status.idle": "2020-08-14T10:26:44.388830Z",
     "shell.execute_reply": "2020-08-14T10:26:44.388352Z"
    },
    "papermill": {
     "duration": 0.051099,
     "end_time": "2020-08-14T10:26:44.388925",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.337826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    pad = framewise_output[:, -1:, :].repeat(\n",
    "        1, frames_num - framewise_output.shape[1], 1)\n",
    "    \"\"\"tensor for padding\"\"\"\n",
    "\n",
    "    output = torch.cat((framewise_output, pad), dim=1)\n",
    "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "            bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "\n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\",\n",
    "                 temperature=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.bn_att = nn.BatchNorm1d(out_features)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:44.451102Z",
     "iopub.status.busy": "2020-08-14T10:26:44.432124Z",
     "iopub.status.idle": "2020-08-14T10:26:44.453850Z",
     "shell.execute_reply": "2020-08-14T10:26:44.453343Z"
    },
    "papermill": {
     "duration": 0.052706,
     "end_time": "2020-08-14T10:26:44.453941",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.401235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PANNsCNN14Att(nn.Module):\n",
    "    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n",
    "                 mel_bins: int, fmin: int, fmax: int, classes_num: int):\n",
    "        super().__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "        self.interpolate_ratio = 32  # Downsampled ratio\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(\n",
    "            sr=sample_rate,\n",
    "            n_fft=window_size,\n",
    "            n_mels=mel_bins,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            ref=ref,\n",
    "            amin=amin,\n",
    "            top_db=top_db,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(\n",
    "            time_drop_width=64,\n",
    "            time_stripes_num=2,\n",
    "            freq_drop_width=8,\n",
    "            freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(mel_bins)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.att_block = AttBlock(2048, classes_num, activation='sigmoid')\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "    def cnn_feature_extractor(self, x):\n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, input, mixup_lambda=None):\n",
    "        # t1 = time.time()\n",
    "        x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "        return x, frames_num\n",
    "        \n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x, frames_num = self.preprocess(input, mixup_lambda=mixup_lambda)\n",
    "\n",
    "        # Output shape (batch size, channels, time, frequency)\n",
    "        x = self.cnn_feature_extractor(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       self.interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output,\n",
    "            'clipwise_output': clipwise_output\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031231,
     "end_time": "2020-08-14T10:26:48.347603",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.316372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:48.444830Z",
     "iopub.status.busy": "2020-08-14T10:26:48.429645Z",
     "iopub.status.idle": "2020-08-14T10:26:48.461345Z",
     "shell.execute_reply": "2020-08-14T10:26:48.460812Z"
    },
    "papermill": {
     "duration": 0.082421,
     "end_time": "2020-08-14T10:26:48.461444",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.379023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIRD_CODE = {\n",
    "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
    "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
    "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
    "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
    "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
    "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
    "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
    "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
    "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
    "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
    "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n",
    "    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n",
    "    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n",
    "    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n",
    "    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n",
    "    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n",
    "    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n",
    "    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n",
    "    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n",
    "    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n",
    "    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n",
    "    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n",
    "    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n",
    "    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n",
    "    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n",
    "    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n",
    "    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n",
    "    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n",
    "    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n",
    "    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n",
    "    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n",
    "    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n",
    "    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n",
    "    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n",
    "    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n",
    "    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n",
    "    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n",
    "    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n",
    "    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n",
    "    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n",
    "    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n",
    "    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n",
    "    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n",
    "    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n",
    "    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n",
    "    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n",
    "    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n",
    "    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n",
    "    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n",
    "    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n",
    "    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n",
    "    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n",
    "    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n",
    "}\n",
    "\n",
    "INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:48.540845Z",
     "iopub.status.busy": "2020-08-14T10:26:48.540238Z",
     "iopub.status.idle": "2020-08-14T10:26:48.544594Z",
     "shell.execute_reply": "2020-08-14T10:26:48.544102Z"
    },
    "papermill": {
     "duration": 0.050901,
     "end_time": "2020-08-14T10:26:48.544697",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.493796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PERIOD = 5\n",
    "\n",
    "class PANNsDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_list: List[List[str]],\n",
    "            waveform_transforms=None):\n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "                y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return {\"waveform\": y, \"targets\": labels}\n",
    "    \n",
    "class PANNsDatasetMod(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_list: List[List[str]],\n",
    "            waveform_transforms=None):\n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * PERIOD\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                start = np.random.randint(len_y - effective_length)\n",
    "                y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "\n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return y, labels\n",
    "    \n",
    "class PANNsEventRmsDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            file_list: List[List[str]],\n",
    "            waveform_transforms=None):\n",
    "        self.file_list = file_list  # list of list: [file_path, ebird_code]\n",
    "        self.waveform_transforms = waveform_transforms\n",
    "        self.df_event = pd.read_csv(PATH_EVENT)\n",
    "        self.sr_feat = 4\n",
    "        self.period = 5  # n sec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        wav_path, ebird_code = self.file_list[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "        else:\n",
    "            len_y = len(y)\n",
    "            effective_length = sr * self.period\n",
    "            if len_y < effective_length:\n",
    "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
    "                start = np.random.randint(effective_length - len_y)\n",
    "                new_y[start:start + len_y] = y\n",
    "                y = new_y.astype(np.float32)\n",
    "            elif len_y > effective_length:\n",
    "                basename = os.path.basename(wav_path)\n",
    "                event_sec_list = self.df_event.query('filename == @basename').event_sec_list.to_list()[0]\n",
    "                event_sec_list = np.array(self.string_to_list(event_sec_list))\n",
    "                ed_sec = len_y / sr\n",
    "                st_range_sec = self.period/2 + 0.0001\n",
    "                ed_range_sec = ed_sec - st_range_sec\n",
    "                mask_range = (st_range_sec <= event_sec_list) & (event_sec_list <= ed_range_sec)\n",
    "                event_sec_list = event_sec_list[mask_range]\n",
    "                \n",
    "                # on event\n",
    "                if len(event_sec_list) != 0:\n",
    "                    choice = random.choice(event_sec_list)\n",
    "                    start = int((choice - self.period/2) * sr)\n",
    "                    y = y[start:start + effective_length].astype(np.float32)\n",
    "#                     IPython.display.display(IPython.display.Audio(y, rate=sr))\n",
    "                # off event\n",
    "                else:\n",
    "                    # event を検出できなかったらランダムにクロップ\n",
    "                    start = np.random.randint(len_y - effective_length)\n",
    "                    y = y[start:start + effective_length].astype(np.float32)\n",
    "            else:\n",
    "                y = y.astype(np.float32)\n",
    "        \n",
    "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
    "        labels[BIRD_CODE[ebird_code]] = 1\n",
    "\n",
    "        return y, labels\n",
    "    \n",
    "    def string_to_list(self, list_str):\n",
    "        for str_replace in ['\\n', '[', ']']:\n",
    "            list_str = list_str.replace(str_replace, '')\n",
    "\n",
    "        split = list_str.split(' ')\n",
    "        events_num = []\n",
    "        for text in split:\n",
    "            try:\n",
    "                num = np.float32(text)\n",
    "                events_num.append(num)\n",
    "            except:\n",
    "                pass\n",
    "        return events_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031604,
     "end_time": "2020-08-14T10:26:48.609308",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.577704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:48.692139Z",
     "iopub.status.busy": "2020-08-14T10:26:48.691461Z",
     "iopub.status.idle": "2020-08-14T10:26:48.695848Z",
     "shell.execute_reply": "2020-08-14T10:26:48.695373Z"
    },
    "papermill": {
     "duration": 0.054385,
     "end_time": "2020-08-14T10:26:48.695948",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.641563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PANNsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bce = nn.BCELoss()\n",
    "#         self.bce = nn.BCEWithLogitsLoss()\n",
    "#         self.bce = StableBCELoss()\n",
    "\n",
    "    def forward(self, _input, target):\n",
    "        input_ = _input[\"clipwise_output\"]\n",
    "#         input_ = _input + torch.finfo(torch.float32).eps\n",
    "        input_ = torch.where(torch.isnan(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "        input_ = torch.where(torch.isinf(input_),\n",
    "                             torch.zeros_like(input_),\n",
    "                             input_)\n",
    "\n",
    "        target = target.float()\n",
    "\n",
    "        return self.bce(input_, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032181,
     "end_time": "2020-08-14T10:26:48.760978",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.728797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:48.854976Z",
     "iopub.status.busy": "2020-08-14T10:26:48.849889Z",
     "iopub.status.idle": "2020-08-14T10:26:48.858082Z",
     "shell.execute_reply": "2020-08-14T10:26:48.857544Z"
    },
    "papermill": {
     "duration": 0.065321,
     "end_time": "2020-08-14T10:26:48.858189",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.792868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class F1Callback(Callback):\n",
    "    def __init__(self,\n",
    "                 input_key: str = \"targets\",\n",
    "                 output_key: str = \"logits\",\n",
    "                 model_output_key: str = \"clipwise_output\",\n",
    "                 prefix: str = \"f1\"):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "\n",
    "        self.input_key = input_key\n",
    "        self.output_key = output_key\n",
    "        self.model_output_key = model_output_key\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on_loader_start(self, state: State):\n",
    "        self.prediction: List[np.ndarray] = []\n",
    "        self.target: List[np.ndarray] = []\n",
    "\n",
    "    def on_batch_end(self, state: State):\n",
    "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
    "        out = state.output[self.output_key]\n",
    "\n",
    "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
    "\n",
    "        self.prediction.append(clipwise_output)\n",
    "        self.target.append(targ)\n",
    "\n",
    "        y_pred = clipwise_output.argmax(axis=1)\n",
    "        y_true = targ.argmax(axis=1)\n",
    "\n",
    "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        state.batch_metrics[self.prefix] = score\n",
    "\n",
    "    def on_loader_end(self, state: State):\n",
    "        y_pred = np.concatenate(self.prediction, axis=0).argmax(axis=1)\n",
    "        y_true = np.concatenate(self.target, axis=0).argmax(axis=1)\n",
    "        score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        state.loader_metrics[self.prefix] = score\n",
    "        if state.is_valid_loader:\n",
    "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
    "                                self.prefix] = score\n",
    "        else:\n",
    "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score\n",
    "\n",
    "\n",
    "class mAPCallback(Callback):\n",
    "    def __init__(self,\n",
    "                 input_key: str = \"targets\",\n",
    "                 output_key: str = \"logits\",\n",
    "                 model_output_key: str = \"clipwise_output\",\n",
    "                 prefix: str = \"mAP\"):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "        self.input_key = input_key\n",
    "        self.output_key = output_key\n",
    "        self.model_output_key = model_output_key\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def on_loader_start(self, state: State):\n",
    "        self.prediction: List[np.ndarray] = []\n",
    "        self.target: List[np.ndarray] = []\n",
    "\n",
    "    def on_batch_end(self, state: State):\n",
    "        targ = state.input[self.input_key].detach().cpu().numpy()\n",
    "        out = state.output[self.output_key]\n",
    "\n",
    "        clipwise_output = out[self.model_output_key].detach().cpu().numpy()\n",
    "\n",
    "        self.prediction.append(clipwise_output)\n",
    "        self.target.append(targ)\n",
    "\n",
    "        score = average_precision_score(targ, clipwise_output, average=None)\n",
    "        score = np.nan_to_num(score).mean()\n",
    "        state.batch_metrics[self.prefix] = score\n",
    "\n",
    "    def on_loader_end(self, state: State):\n",
    "        y_pred = np.concatenate(self.prediction, axis=0)\n",
    "        y_true = np.concatenate(self.target, axis=0)\n",
    "        score = average_precision_score(y_true, y_pred, average=None)\n",
    "        score = np.nan_to_num(score).mean()\n",
    "        state.loader_metrics[self.prefix] = score\n",
    "        if state.is_valid_loader:\n",
    "            state.epoch_metrics[state.valid_loader + \"_epoch_\" +\n",
    "                                self.prefix] = score\n",
    "        else:\n",
    "            state.epoch_metrics[\"train_epoch_\" + self.prefix] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033622,
     "end_time": "2020-08-14T10:26:48.924696",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.891074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train\n",
    "\n",
    "Some code are taken from https://www.kaggle.com/ttahara/training-birdsong-baseline-resnest50-fast .\n",
    "Thanks @ttahara!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:44.961633Z",
     "iopub.status.busy": "2020-08-14T10:26:44.960605Z",
     "iopub.status.idle": "2020-08-14T10:26:46.429944Z",
     "shell.execute_reply": "2020-08-14T10:26:46.431123Z"
    },
    "papermill": {
     "duration": 1.508482,
     "end_time": "2020-08-14T10:26:46.431338",
     "exception": false,
     "start_time": "2020-08-14T10:26:44.922856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"sample_rate\": 32000,\n",
    "    \"window_size\": 1024,\n",
    "    \"hop_size\": 320,\n",
    "    \"mel_bins\": 64,\n",
    "    \"fmin\": 50,\n",
    "    \"fmax\": 14000,\n",
    "    \"classes_num\": 264\n",
    "}\n",
    "\n",
    "model = PANNsCNN14Att(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:49.001106Z",
     "iopub.status.busy": "2020-08-14T10:26:49.000144Z",
     "iopub.status.idle": "2020-08-14T10:26:49.708541Z",
     "shell.execute_reply": "2020-08-14T10:26:49.707629Z"
    },
    "papermill": {
     "duration": 0.749533,
     "end_time": "2020-08-14T10:26:49.708664",
     "exception": false,
     "start_time": "2020-08-14T10:26:48.959131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21375, 38)\n",
      "(21375, 3)\n",
      "(21375, 39)\n"
     ]
    }
   ],
   "source": [
    "tmp_list = []\n",
    "for audio_d in TRAIN_RESAMPLED_AUDIO_DIRS:\n",
    "    if not audio_d.exists():\n",
    "        continue\n",
    "    for ebird_d in audio_d.iterdir():\n",
    "        if ebird_d.is_file():\n",
    "            continue\n",
    "        for i, wav_f in enumerate(ebird_d.iterdir()):\n",
    "            bool_n_splits = i==N_SPLITS\n",
    "            if bool_n_splits and DEBUG: break  # if DEBUG=True: 1bird/n_splits file\n",
    "            tmp_list.append([ebird_d.name, wav_f.name, wav_f.as_posix()])\n",
    "            \n",
    "train_wav_path_exist = pd.DataFrame(\n",
    "    tmp_list, columns=[\"ebird_code\", \"resampled_filename\", \"file_path\"])\n",
    "\n",
    "del tmp_list\n",
    "\n",
    "train_all = pd.merge(\n",
    "    train, train_wav_path_exist, on=[\"ebird_code\", \"resampled_filename\"], how=\"inner\")\n",
    "\n",
    "print(train.shape)\n",
    "print(train_wav_path_exist.shape)\n",
    "print(train_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:49.806182Z",
     "iopub.status.busy": "2020-08-14T10:26:49.804681Z",
     "iopub.status.idle": "2020-08-14T10:26:49.875178Z",
     "shell.execute_reply": "2020-08-14T10:26:49.875766Z"
    },
    "papermill": {
     "duration": 0.133113,
     "end_time": "2020-08-14T10:26:49.875942",
     "exception": false,
     "start_time": "2020-08-14T10:26:49.742829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 5)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "train_all[\"fold\"] = -1\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(train_all, train_all[\"ebird_code\"])):\n",
    "    train_all.iloc[val_index, -1] = fold_id\n",
    "    \n",
    "# # check the propotion\n",
    "fold_proportion = pd.pivot_table(train_all, index=\"ebird_code\", columns=\"fold\", values=\"xc_id\", aggfunc=len)\n",
    "print(fold_proportion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:49.960106Z",
     "iopub.status.busy": "2020-08-14T10:26:49.959257Z",
     "iopub.status.idle": "2020-08-14T10:26:50.006856Z",
     "shell.execute_reply": "2020-08-14T10:26:50.007827Z"
    },
    "papermill": {
     "duration": 0.094264,
     "end_time": "2020-08-14T10:26:50.007988",
     "exception": false,
     "start_time": "2020-08-14T10:26:49.913724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] train: 17100, val: 4275\n"
     ]
    }
   ],
   "source": [
    "use_fold = 0\n",
    "train_file_list = train_all.query(\"fold != @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\n",
    "val_file_list = train_all.query(\"fold == @use_fold\")[[\"file_path\", \"ebird_code\"]].values.tolist()\n",
    "\n",
    "print(\"[fold {}] train: {}, val: {}\".format(use_fold, len(train_file_list), len(val_file_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T10:26:50.093491Z",
     "iopub.status.busy": "2020-08-14T10:26:50.092046Z",
     "iopub.status.idle": "2020-08-14T10:26:59.767467Z",
     "shell.execute_reply": "2020-08-14T10:26:59.766328Z"
    },
    "papermill": {
     "duration": 9.724143,
     "end_time": "2020-08-14T10:26:59.767606",
     "exception": false,
     "start_time": "2020-08-14T10:26:50.043463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# loaders\n",
    "loaders = {\n",
    "    \"train\": data.DataLoader(PANNsDatasetMod(train_file_list, None), \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, \n",
    "                             num_workers=2, \n",
    "                             pin_memory=True, \n",
    "                             drop_last=True),\n",
    "    \"valid\": data.DataLoader(PANNsDatasetMod(val_file_list, None), \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=False,\n",
    "                             num_workers=2,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=False)\n",
    "}\n",
    "\n",
    "# model\n",
    "model_config[\"classes_num\"] = 527\n",
    "model = PANNsCNN14Att(**model_config)\n",
    "weights = torch.load(\"./../data_ignore/model/PANNs/Cnn14_DecisionLevelAtt_mAP=0.425.pth\")\n",
    "# Fixed in V3\n",
    "model.load_state_dict(weights[\"model\"])\n",
    "model.att_block = AttBlock(2048, 264, activation='sigmoid')\n",
    "model.att_block.init_weights()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Loss\n",
    "criterion = PANNsLoss().to(device)\n",
    "\n",
    "# callbacks\n",
    "# callbacks = [\n",
    "#     F1Callback(input_key=\"targets\", output_key=\"logits\", prefix=\"f1\"),\n",
    "#     mAPCallback(input_key=\"targets\", output_key=\"logits\", prefix=\"mAP\"),\n",
    "#     CheckpointCallback(save_n_best=0)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, loss_func):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()*data.size(0)\n",
    "    scheduler.step()\n",
    "    loss = epoch_train_loss / len(train_loader.dataset)\n",
    "    del data\n",
    "    return loss\n",
    "\n",
    "# def get_epoch_loss(model, device, valid_loder, loss_func):\n",
    "#     model.eval()\n",
    "#     epoch_valid_loss = 0\n",
    "#     for batch_idx, (data, target) in enumerate(progress_bar(valid_loader)):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         output = model(data)\n",
    "#         loss = loss_func(output, target)\n",
    "#         epoch_valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "#     loss = epoch_valid_loss / len(valid_loader.dataset)\n",
    "    \n",
    "# #     if np.isnan(loss):\n",
    "# #         raise Exception\n",
    "        \n",
    "#     del data\n",
    "#     return loss\n",
    "\n",
    "def get_epoch_loss_score_panns(model, device, valid_loder, loss_func):\n",
    "    model.eval()\n",
    "    epoch_valid_loss = 0\n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    for batch_idx, (data, target) in enumerate(progress_bar(valid_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        epoch_valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "#         out_numpy = output.detach().cpu().numpy()\n",
    "        _y_pred = output['clipwise_output'].detach().cpu().numpy().argmax(axis=1)\n",
    "        y_pred_list.append(_y_pred)\n",
    "        _y_true = target.detach().cpu().numpy().argmax(axis=1)\n",
    "        y_true_list.append(_y_true)\n",
    "    \n",
    "    loss = epoch_valid_loss / len(valid_loader.dataset)\n",
    "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "    y_true = np.concatenate(y_true_list, axis=0)\n",
    "    f_score = f1_score(y_true, y_pred, average='macro')\n",
    "    del data\n",
    "    return loss, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "losses_train = []\n",
    "losses_valid = []\n",
    "epochs = []\n",
    "\n",
    "train_loader = loaders['train']\n",
    "valid_loader = loaders['valid']\n",
    "early_stopping = EarlyStopping(patience=15, verbose=True)\n",
    "n_epochs = N_EPOCHS\n",
    "# n_epoch = 50\n",
    "for epoch in progress_bar(range(1, n_epochs+1)):\n",
    "    print(f'\\n epoch: {epoch}/{n_epochs} {time.ctime()}')\n",
    "    loss_train = train(model, device, train_loader, optimizer, scheduler, criterion)\n",
    "    loss_valid, f_score_valid = get_epoch_loss_score_panns(model, device, valid_loader, criterion)\n",
    "    print(f'loss_train: {loss_train:.6f}, loss_valid: {loss_valid:.6f}, f1(macro): {f_score_valid:.6f}')\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    losses_train.append(loss_train)\n",
    "    losses_valid.append(loss_valid)\n",
    "    \n",
    "\n",
    "    if np.isnan(loss_valid):\n",
    "        pass\n",
    "    else:\n",
    "        early_stopping(loss_valid, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "# model.load_state_dict(early_stopping.best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa5b18b97f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, losses_train, '-x', label='train')\n",
    "plt.plot(epochs, losses_valid, '-x', label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    pass\n",
    "else:\n",
    "    load_weights = torch.load(save_path)\n",
    "    model.load_state_dict(load_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valid, f_score_valid = get_epoch_loss_score(model, device, valid_loader, loss_func)\n",
    "loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n",
    "                 img_size=224, melspectrogram_parameters={}):\n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.img_size = img_size\n",
    "        self.melspectrogram_parameters = melspectrogram_parameters\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = 32000\n",
    "        sample = self.df.loc[idx, :]\n",
    "        site = sample.site\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        if site == \"site_3\":\n",
    "            y = self.clip.astype(np.float32)\n",
    "            len_y = len(y)\n",
    "            start = 0\n",
    "            end = SR * 5\n",
    "            images = []\n",
    "            while len_y > start:\n",
    "                y_batch = y[start:end].astype(np.float32)\n",
    "                if len(y_batch) != (SR * 5):\n",
    "                    break\n",
    "                start = end\n",
    "                end = end + SR * 5\n",
    "                \n",
    "                melspec = librosa.feature.melspectrogram(y_batch,\n",
    "                                                         sr=SR,\n",
    "                                                         **self.melspectrogram_parameters)\n",
    "                melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "                image = mono_to_color(melspec)\n",
    "                height, width, _ = image.shape\n",
    "                image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "                image = np.moveaxis(image, 2, 0)\n",
    "                image = (image / 255.0).astype(np.float32)\n",
    "                images.append(image)\n",
    "            images = np.asarray(images)\n",
    "            return images, row_id, site\n",
    "        else:\n",
    "            end_seconds = int(sample.seconds)\n",
    "            start_seconds = int(end_seconds - 5)\n",
    "            \n",
    "            start_index = SR * start_seconds\n",
    "            end_index = SR * end_seconds\n",
    "            \n",
    "            y = self.clip[start_index:end_index].astype(np.float32)\n",
    "\n",
    "            melspec = librosa.feature.melspectrogram(y, sr=SR, **self.melspectrogram_parameters)\n",
    "            melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "            image = mono_to_color(melspec)\n",
    "            height, width, _ = image.shape\n",
    "            image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "            image = np.moveaxis(image, 2, 0)\n",
    "            image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "            return image, row_id, site\n",
    "        \n",
    "def prediction_for_clip(test_df: pd.DataFrame, \n",
    "                        clip: np.ndarray, \n",
    "                        model, \n",
    "                        mel_params: dict, \n",
    "                        threshold=0.5):\n",
    "\n",
    "    dataset = TestDataset(df=test_df, \n",
    "                          clip=clip,\n",
    "                          img_size=224,\n",
    "                          melspectrogram_parameters=mel_params)\n",
    "    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model.eval()\n",
    "    prediction_dict = {}\n",
    "    for image, row_id, site in progress_bar(loader):\n",
    "        site = site[0]\n",
    "        row_id = row_id[0]\n",
    "        if site in {\"site_1\", \"site_2\"}:\n",
    "            image = image.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "                proba = F.sigmoid(prediction).detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "            events = proba >= threshold\n",
    "            labels = np.argwhere(events).reshape(-1).tolist()\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # to avoid prediction on large batch\n",
    "            image = image.squeeze(0)\n",
    "            batch_size = 16\n",
    "            whole_size = image.size(0)\n",
    "            if whole_size % batch_size == 0:\n",
    "                n_iter = whole_size // batch_size\n",
    "            else:\n",
    "                n_iter = whole_size // batch_size + 1\n",
    "                \n",
    "            all_events = set()\n",
    "            for batch_i in range(n_iter):\n",
    "                batch = image[batch_i * batch_size:(batch_i + 1) * batch_size]\n",
    "#                 if batch.ndim == 3:\n",
    "#                     batch = batch.unsqueeze(0)\n",
    "\n",
    "                batch = batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(batch)\n",
    "                    proba = F.sigmoid(prediction).detach().cpu().numpy()              \n",
    "                events = proba >= threshold\n",
    "                for i in range(len(events)):\n",
    "                    event = events[i, :]\n",
    "                    labels = np.argwhere(event).reshape(-1).tolist()\n",
    "                    for label in labels:\n",
    "                        all_events.add(label)\n",
    "                        \n",
    "            labels = list(all_events)\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[row_id] = \"nocall\"\n",
    "        else:\n",
    "            labels_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            prediction_dict[row_id] = label_string\n",
    "    return prediction_dict\n",
    "\n",
    "def prediction(test_df: pd.DataFrame,\n",
    "               test_audio: Path,\n",
    "               mel_params: dict,\n",
    "               model,\n",
    "               threshold=0.5):\n",
    "    \n",
    "    unique_audio_id = test_df.audio_id.unique()\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dfs = []\n",
    "    for audio_id in unique_audio_id:\n",
    "        with timer(f\"Loading {audio_id}\", logger):\n",
    "            clip, _ = librosa.load(f'{test_audio}/{audio_id}.mp3',\n",
    "                                   sr=TARGET_SR,\n",
    "                                   mono=True,\n",
    "                                   res_type=\"kaiser_fast\")\n",
    "        \n",
    "        test_df_for_audio_id = test_df.query(\n",
    "            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n",
    "        with timer(f\"Prediction on {audio_id}\", logger):\n",
    "            prediction_dict = prediction_for_clip(test_df_for_audio_id,\n",
    "                                                  clip=clip,\n",
    "                                                  model=model,\n",
    "                                                  mel_params=mel_params,\n",
    "                                                  threshold=threshold)\n",
    "        row_id = list(prediction_dict.keys())\n",
    "        birds = list(prediction_dict.values())\n",
    "        prediction_df = pd.DataFrame({\n",
    "            \"row_id\": row_id,\n",
    "            \"birds\": birds\n",
    "        })\n",
    "        prediction_dfs.append(prediction_df)\n",
    "    \n",
    "    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "    return prediction_df\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str, logger):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "        \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "set_seed(1213)\n",
    "\n",
    "dir_test_audio = '../data/external_dataset/birdcall-check/test_audio/'\n",
    "test = pd.read_csv('./../data/external_dataset/birdcall-check/test.csv')\n",
    "sub = pd.read_csv('./../data_ignore/official/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspectrogram_parameters = {\n",
    "#     \"n_mels\": 128,\n",
    "#     \"fmin\": 20,\n",
    "#     \"fmax\": 16000\n",
    "# }\n",
    "\n",
    "THRESHOLD = 0.6\n",
    "TARGET_SR = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = prediction(test_df=test,\n",
    "                        test_audio=dir_test_audio,\n",
    "                        model=model,\n",
    "                        mel_params=settings['dataset']['params']['melspectrogram_parameters'],\n",
    "                        threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "submission.head(80)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
