{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Process documents training set with pyConTextNLP\n",
    "- Measure system performance\n",
    "- Review false positives and false negatives\n",
    "- Edit knowledge base to reduce errors\n",
    "- Iteratively improve system\n",
    "- Run system on final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some packages\n",
    "import os\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP import itemData\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets\n",
    "# And also our utilities for this class\n",
    "\n",
    "from nlp_pneumonia_utils import read_doc_annotations, list_errors, clearPyConTextRegularExpressions\n",
    "from nlp_pneumonia_utils import mark_document_with_html, pneumonia_annotation_html_markup\n",
    "from nlp_pneumonia_utils import classify_pneumonia_document, markup_context_document\n",
    "# from nlp_pneumonia_utils import DocumentClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Evaluation and Error Analysis\n",
    "\n",
    "In the previous notebook, we developed an NLP system to extract mentions of pneumonia, check whether they are modified by certain semantic modifiers, and use that information to classify a document as being either **positive** or **negative** for pneumonia.\n",
    "\n",
    "In this notebook, we will use an annotated dataset to compare our system with a human gold standard. By comparing with expert annotations, we can see instances where our system is making mistakes and we can modify it to have better performance. The two types of errors to look for are:\n",
    "\n",
    "1. **False positives** - documents which we say have pneumonia but the human annotators did not\n",
    "2. **False negatives** - documents which the annotators said have pneumonia but we missed\n",
    "\n",
    "To measure our system's performance, we'll calculate:\n",
    "- **Precision** - how likely our positive classifications are to actually have pneumonia. A system with many *false positives* will have *low precision*\n",
    "- **Recall** - how likely it is that our system will identify a document which has pneumonia. A system with many *false negatives* will have *low recall*\n",
    "- **F1** - the balanced average of the two\n",
    "\n",
    "## Workflow\n",
    "1. Process our training set with pyConText and calculate metrics\n",
    "2. Look at documents which our system classified incorrectly\n",
    "3. Make changes to our knowledge base or code\n",
    "4. Re-run on the training set\n",
    "5. Once we have a score we're happy with, run our system on the test set to get a final score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Processing training set and calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearPyConTextRegularExpressions()\n",
    "full_targets_path = 'file:///' + os.path.join(os.getcwd(), \"KB/pneumonia_targets.tsv\")\n",
    "modifier_file_path = 'file:///' + os.path.join(os.getcwd(), \"KB/pneumonia_modifiers.tsv\")\n",
    "\n",
    "targets = pyConTextNLP.itemData.instantiateFromCSVtoitemData(full_targets_path)\n",
    "modifiers = pyConTextNLP.itemData.instantiateFromCSVtoitemData(modifier_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, these are the classes which tell us a mention of pneumonia\n",
    "# doesn't mean the patient actually has pneumonia\n",
    "non_positive_categories = [\"definite_negated_existence\",\n",
    "                 \"probable_negated_existence\",\n",
    "                 \"probable_existence\",\n",
    "                \"indication\",\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our expert annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the training documents and annotations\n",
    "annotated_doc_map = read_doc_annotations('pneumonia_data/training_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our system performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process the corpus using docClassifier to return errors\n",
    "fns, fps, context_docs = list_errors(gold_docs=annotated_doc_map, \n",
    "                      modifiers=modifiers, \n",
    "                      targets=targets, \n",
    "                      non_positive_categories=non_positive_categories,\n",
    "                      print_prediction_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we didn't use any modifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the corpus using docClassifier to return errors\n",
    "_ = list_errors(gold_docs=annotated_doc_map, \n",
    "                      modifiers=[], \n",
    "                      targets=targets, \n",
    "                      non_positive_categories=non_positive_categories,\n",
    "                      print_prediction_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Error Analysis\n",
    "Let's now take a closer look at the documents which we classified incorrectly. We identified the error documents and saved them in the lists `fps` and `fns`. We'll look at both the human annotations and the results of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"evidence_of_pneumonia\": \"orange\",\n",
    "    \"definite_negated_existence\": \"red\",\n",
    "    \"probable_negated_existence\": \"indianred\",\n",
    "    \"ambivalent_existence\": \"forestgreen\",\n",
    "    \"probable_existence\": \"forestgreen\",\n",
    "    \"definite_existence\": \"green\",\n",
    "    \"historical\": \"goldenrod\",\n",
    "    \"indication\": \"pink\",\n",
    "    \"acute\": \"golden\"\n",
    "}        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display false negatives\n",
    "Now we can display the **false negatives** with expert annotations.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function let's us iterate through all documents and view the markup\n",
    "def view_annotation_markup(anno_docs):\n",
    "    @interact(i=ipywidgets.IntSlider(min=0, max=len(anno_docs)-1))\n",
    "    def _view_markup(i):\n",
    "        report_html = pneumonia_annotation_html_markup(anno_docs[i])\n",
    "        report_html = report_html.replace('\\n', '<br>')\n",
    "        display(HTML(report_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_names, fn_docs = zip(*[(k, v) for k, v in annotated_doc_map.items() if k in fns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of the false negative documents to review\n",
    "idx = 0\n",
    "fn_name = fn_names[idx]\n",
    "fn_gold_doc = fn_docs[idx]\n",
    "fn_nlp_doc = context_docs[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_annotation_markup([fn_gold_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP System Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Document name: {}\".format(fn_name))\n",
    "display(HTML(mark_document_with_html(\n",
    "    fn_nlp_doc, \n",
    "    colors=colors, default_color=\"black\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with our classification function\n",
    "classify_pneumonia_document(fn_nlp_doc, non_positive_categories, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Display false positives\n",
    "Let's do the same thing with false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_names, fp_docs = zip(*list((k, v) for k,v in annotated_doc_map.items() if k in fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of the false negative documents to review\n",
    "idx = 0\n",
    "fp_name = fp_names[idx]\n",
    "fp_gold_doc = fp_docs[idx]\n",
    "fp_nlp_doc = context_docs[fp_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_annotation_markup([fp_gold_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp_name)\n",
    "display(HTML(mark_document_with_html(fp_nlp_doc, colors = colors, default_color=\"black\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the document markups\n",
    "fp_nlp_doc.getDocumentGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with our classification function\n",
    "classify_pneumonia_document(fp_nlp_doc, non_positive_categories, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "What are the causes of false positives? What are the causes of false negatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve system\n",
    "As you identify errors in the output, you can start to make changes to the system. Some changes you can make are:\n",
    "\n",
    "- Add terms to targets.tsv\n",
    "- Add terms to modifiers.tsv\n",
    "- Add semantic classes to `non_positive_categories`\n",
    "\n",
    "### Discussion\n",
    "\n",
    "For each of the options above, will a change reduce **false positives** or **false negatives**? Will that affect **precision** or **recall**?\n",
    "\n",
    "### Discussion\n",
    "\n",
    "As you make changes to improve either precision or recall, what happens to the other metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set\n",
    "Once you feel like you have a good system, you will evaluate your NLP system on a new set of documents which you haven't seen before. This is important to show how **generalizable** your system is - can it work well on documents which you hadn't reviewed when developing the system?\n",
    "\n",
    "**Once you run your system on your test set, you can't make any more changes!** It's important that you only evaluate using the test documents *one time*. This is the score you would report in a publication. You can review the errors after, but you should consider your system to be frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the training documents and annotations\n",
    "test_annotated_doc_map = read_doc_annotations('pneumonia_data/test_v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT WHEN YOU ARE READY TO RUN\n",
    "test_fns, test_fps, test_context_docs = list_errors(gold_docs=test_annotated_doc_map, \n",
    "                      modifiers=modifiers, \n",
    "                      targets=targets, \n",
    "                      non_positive_categories=non_positive_categories,\n",
    "                      print_prediction_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
