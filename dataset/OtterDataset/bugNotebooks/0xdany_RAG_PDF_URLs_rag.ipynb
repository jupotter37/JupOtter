{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-augmented Generation using PDF URLs\n",
    "\n",
    "**Author**: Dany Raihan\n",
    "\n",
    "This project, for demonstartive purpose, uses `langchain` to chain outputs together into a flow for radibility, `chromadb` as Vector database, `text-embedding-3-small` by OPENAI to vectorize text, and `gpt-4o-mini` by OPENAI to query contexts and questions. However, you can replace any of the componet using any model or alternatives.\n",
    "\n",
    "\n",
    "## Workflow:\n",
    "1. **Fetch PDF Content**: Download and extract content from a list of PDF URLs.\n",
    "2. **Trim Metadata**: Prepare the necessary metadata for vectorization.\n",
    "3. **Split Text into Chunks**: Convert the PDF content with metadata into overlapping text chunks to prepare the data for vectorization.\n",
    "4. **Vectorize the Text Chunks**: Vectorize the text chunks using the specified embedding model and store the vectors in a Chroma vector database.\n",
    "5. **Query the Vector Store**: Perform a similarity search in the vector store based on a given query to retrieve the most relevant documents.\n",
    "6. **Parse the Search Results**: Parse and join the text from the search results to create a context for the model.\n",
    "7. **Construct Instruction's Model**: Construct the instruction that includes the context and the query to be passed to the model.\n",
    "8. **Perform Retrieval-augmented Generation**: Generate the final answer using the RAG pipeline by querying the model with the constructed instruction.\n",
    "\n",
    "Each step is crucial in constructing a reliable RAG pipeline, especially when working with unstructured data like PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Configuration\n",
    "\n",
    "Define the initial configurations necessary to set up our RAG pipeline. This includes specifying the PDF URLs, setting environment variables like API keys, and defining the models we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# list the pdf urls\n",
    "urls = ['https://bitcoin.org/bitcoin.pdf']\n",
    "\n",
    "# Configurations\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"enter_your_openai_key_here\"\n",
    "\n",
    "# Models\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "\n",
    "# Vector database configuration [optional]\n",
    "collection_name = \"example_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Fetch PDF Contents\n",
    "\n",
    "This step covers the process of downloading and extracting text content from the provided PDF URLs. We use `langchain`'s `PyMuPDFLoader` for loading the PDF documents. This step is essential as it converts unstructured PDF data into a format suitable for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import re\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All URLs are valid\n"
     ]
    }
   ],
   "source": [
    "# Validate the URLs\n",
    "def ensure_url(string: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensures the given string is a URL by adding 'http://' if it doesn't start with 'http://' or 'https://'.\n",
    "    Raises an error if the string is not a valid URL.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The string to be checked and possibly modified.\n",
    "\n",
    "    Returns:\n",
    "        str: The modified string that is ensured to be a URL.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the string is not a valid URL.\n",
    "    \"\"\"\n",
    "    if not string.startswith((\"http://\", \"https://\")):\n",
    "        string = \"http://\" + string\n",
    "\n",
    "    # Basic URL validation regex from https://stackoverflow.com/a/7160778\n",
    "    url_regex = re.compile(\n",
    "        r\"^(https?:\\/\\/)?\"  # optional protocol\n",
    "        r\"(www\\.)?\"  # optional www\n",
    "        r\"([a-zA-Z0-9.-]+)\"  # domain\n",
    "        r\"(\\.[a-zA-Z]{2,})?\"  # top-level domain\n",
    "        r\"(:\\d+)?\"  # optional port\n",
    "        r\"(\\/[^\\s]*)?$\",  # optional path\n",
    "        re.IGNORECASE,\n",
    "    )\n",
    "\n",
    "    if not url_regex.match(string):\n",
    "        raise ValueError(f\"Invalid URL: {string}\")\n",
    "\n",
    "    return string\n",
    "\n",
    "for url in urls:\n",
    "    try:\n",
    "        ensure_url(url)\n",
    "        print(\"✅ All URLs are valid\")\n",
    "    except ValueError as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully fetched PDF content with its metadata\n",
      "Content: -> \n",
      "Bitcoin: A Peer-to-Peer Electronic Cash System\n",
      "Satoshi Nakamoto\n",
      "satoshin@gmx.com\n",
      "www.bitcoin.org\n",
      "Abs...\n",
      "Metadata: -> \n",
      "{'source': '/var/folders/q9/6pq5y1l504j2_bdkt5hykm7h0000gn/T/tmpw3wbv20w.pdf', 'file_path': '/var/folders/q9/6pq5y1l504j2_bdkt5hykm7h0000gn/T/tmpw3wbv20w.pdf', 'page': 0, 'total_pages': 9, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Writer', 'producer': 'OpenOffice.org 2.4', 'creationDate': \"D:20090324113315-06'00'\", 'modDate': '', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# Fetch the PDF content\n",
    "def fetch_pdf_content(urls: list[str]) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Fetches and parses text from one or more PDF URLs.\n",
    "\n",
    "    Parameters:\n",
    "        urls (list[str]): A list of PDF URLs.\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: A list of Document objects containing the text and metadata from each PDF.\n",
    "    \"\"\"\n",
    "    urls = [ensure_url(url.strip()) for url in urls if url.strip()]\n",
    "    data = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n",
    "                temp_file.write(response.content)\n",
    "                temp_file_path = temp_file.name\n",
    "\n",
    "            pdf_loader = PyMuPDFLoader(file_path=temp_file_path)\n",
    "            pdf_docs = pdf_loader.load()\n",
    "\n",
    "            for pdf_doc in pdf_docs:\n",
    "                data.append(Document(page_content=pdf_doc.page_content, metadata=pdf_doc.metadata))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data\n",
    "\n",
    "pdf_data = []\n",
    "\n",
    "try:\n",
    "    pdf_data = fetch_pdf_content(urls)\n",
    "    print(\"✅ Successfully fetched PDF content with its metadata\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch PDF content: {e}\")\n",
    "    \n",
    "print(f\"Content: -> \\n{pdf_data[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: -> \\n{pdf_data[0].metadata}\")\n",
    "    \n",
    "# for doc in pdf_data:\n",
    "#     print(f\"Content: {doc.page_content[:100]}...\")\n",
    "#     print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Trim Metadata\n",
    "Trim the necessary metadata fields that will be used for vectorization. This step involves filtering out only the important metadata that will be passed along in the vectorization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully trimmed metadata for vectorization\n"
     ]
    }
   ],
   "source": [
    "# Trim the necessary metadata to be used for vectorization\n",
    "important_metadata = ['title', 'page', 'creator', 'author'] # fields to be used for vectorization are costumizable\n",
    "\n",
    "trimmed_pdf_data = []\n",
    "for data in pdf_data:\n",
    "      trimmed_metadata = {key: data.metadata.get(key) for key in important_metadata if key in data.metadata}\n",
    "      trimmed_data_object = Document(page_content=data.page_content, metadata=trimmed_metadata)\n",
    "      trimmed_pdf_data.append(trimmed_data_object)\n",
    "      \n",
    "# print(f\"Content: -> \\n{trimmed_pdf_data[0].page_content[:100]}...\")\n",
    "# print(f\"Metadata: -> \\n{trimmed_pdf_data[0].metadata}\")\n",
    "print(\"✅ Successfully trimmed metadata for vectorization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Split Text into Chunks\n",
    "Convert PDFs with metadata into overlapping text chunks. This step prepares the documents for vectorization by breaking them into manageable chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Content: Bitcoin: A Peer-to-Peer Electronic Cash System\n",
      "Sat...\n",
      "Metadata: {'title': '', 'page': 0, 'creator': 'Writer', 'author': ''}\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "Content: it came from the largest pool of CPU power. As \n",
      "lo...\n",
      "Metadata: {'title': '', 'page': 0, 'creator': 'Writer', 'author': ''}\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "Content: really possible, since financial institutions cann...\n",
      "Metadata: {'title': '', 'page': 0, 'creator': 'Writer', 'author': ''}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to split text into chunks based on specified criteria\n",
    "def split_text(documents: List[Document], chunk_overlap: int = 200, chunk_size: int = 1000, separator: str = \"\\n\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on specified criteria.\n",
    "\n",
    "    Parameters:\n",
    "        documents (List[Document]): A list of Document objects to split.\n",
    "        chunk_overlap (int): Number of characters to overlap between chunks.\n",
    "        chunk_size (int): The maximum number of characters in each chunk.\n",
    "        separator (str): The character to split on. Defaults to newline.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of Document objects with the split content.\n",
    "    \"\"\"\n",
    "    # Initialize the text splitter\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        chunk_size=chunk_size,\n",
    "        separator=separator,\n",
    "    )\n",
    "    # Split the documents\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "    # Convert the split documents back to the desired format\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "# Parameters for splitting\n",
    "chunk_overlap = 200\n",
    "chunk_size = 1000\n",
    "separator = \" \"  # Split by space for demonstration\n",
    "\n",
    "# Split the text\n",
    "splitted_data = split_text(trimmed_pdf_data, chunk_overlap, chunk_size, separator)\n",
    "\n",
    "# Display the split chunks (displaying only the first 3 chunks for demonstration)\n",
    "for idx, doc in enumerate(splitted_data[0:3]):\n",
    "    print(f\"Chunk {idx + 1}:\")\n",
    "    print(f\"Content: {doc.page_content[:50]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Vectorize the Text Chunks\n",
    "Vectorize the text chunks using the specified embedding model. This step involves creating a vector representation of the text data, which is crucial for the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import List, Optional\n",
    "from copy import deepcopy\n",
    "from langchain.schema import Document\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "from loguru import logger\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-13 03:03:00.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36madd_documents_to_vector_store\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mAdding 29 documents to the Vector Store.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully built the Chroma vector store collection named 'example_collection'\n"
     ]
    }
   ],
   "source": [
    "# Helper function to add documents to the vector store\n",
    "def add_documents_to_vector_store(vector_store: Chroma, ingest_data: List[Document], allow_duplicates: bool, limit: Optional[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Adds documents to the Vector Store.\n",
    "\n",
    "    Parameters:\n",
    "        vector_store (Chroma): The Chroma vector store instance.\n",
    "        ingest_data (List[Document]): The data to ingest into the vector store.\n",
    "        allow_duplicates (bool): Whether to allow duplicates in the vector store.\n",
    "        limit (Optional[int]): The limit on the number of records to compare when `allow_duplicates` is False.\n",
    "    \"\"\"\n",
    "    if not ingest_data:\n",
    "        return\n",
    "\n",
    "    _stored_documents_without_id = []\n",
    "    if not allow_duplicates:\n",
    "        stored_data = vector_store.similarity_search(\"\", k=limit)  # Fetch existing data\n",
    "        for doc in deepcopy(stored_data):\n",
    "            del doc.metadata['id']\n",
    "            _stored_documents_without_id.append(doc)\n",
    "\n",
    "    documents_to_add = []\n",
    "    for doc in ingest_data:\n",
    "        if doc not in _stored_documents_without_id:\n",
    "            documents_to_add.append(doc)\n",
    "\n",
    "    if documents_to_add:\n",
    "        logger.debug(f\"Adding {len(documents_to_add)} documents to the Vector Store.\")\n",
    "        vector_store.add_documents(documents_to_add)\n",
    "    else:\n",
    "        logger.debug(\"No documents to add to the Vector Store.\")\n",
    "\n",
    "# Function to build the Chroma vector store\n",
    "def build_vector_store(\n",
    "    collection_name: str = \"vector_collection\",\n",
    "    persist_directory: Optional[str] = None,\n",
    "    chroma_server_host: Optional[str] = None,\n",
    "    chroma_server_http_port: Optional[int] = None,\n",
    "    chroma_server_grpc_port: Optional[int] = None,\n",
    "    chroma_server_ssl_enabled: Optional[bool] = False,\n",
    "    chroma_server_cors_allow_origins: Optional[list] = None,\n",
    "    embedding_model: str = \"text-embedding-3-small\",\n",
    "    ingest_data: Optional[List[Document]] = None,\n",
    "    allow_duplicates: bool = True,\n",
    "    limit: Optional[int] = None,\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Builds the Chroma vector store.\n",
    "\n",
    "    Parameters:\n",
    "        collection_name (str): The name of the collection in Chroma.\n",
    "        persist_directory (Optional[str]): Directory to persist data.\n",
    "        chroma_server_host (Optional[str]): Chroma server host.\n",
    "        chroma_server_http_port (Optional[int]): Chroma server HTTP port.\n",
    "        chroma_server_grpc_port (Optional[int]): Chroma server gRPC port.\n",
    "        chroma_server_ssl_enabled (Optional[bool]): Whether SSL is enabled on the server.\n",
    "        chroma_server_cors_allow_origins (Optional[list]): CORS origins allowed by the server.\n",
    "        embedding_model (str): The embedding model to use.\n",
    "        ingest_data (Optional[List[Document]]): Data to ingest into the vector store.\n",
    "        allow_duplicates (bool): Whether to allow duplicates in the vector store.\n",
    "        limit (Optional[int]): Limit on the number of records to compare when `allow_duplicates` is False.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: The initialized Chroma vector store instance.\n",
    "    \"\"\"\n",
    "    # Initialize Chroma settings and client\n",
    "    chroma_settings = None\n",
    "    client = None\n",
    "    if chroma_server_host:\n",
    "        chroma_settings = Settings(\n",
    "            chroma_server_cors_allow_origins=chroma_server_cors_allow_origins or [],\n",
    "            chroma_server_host=chroma_server_host,\n",
    "            chroma_server_http_port=chroma_server_http_port or None,\n",
    "            chroma_server_grpc_port=chroma_server_grpc_port or None,\n",
    "            chroma_server_ssl_enabled=chroma_server_ssl_enabled,\n",
    "        )\n",
    "        client = Client(settings=chroma_settings)\n",
    "\n",
    "    # Resolve persist directory\n",
    "    persist_directory = persist_directory or None\n",
    "\n",
    "    # Initialize embedding function\n",
    "    embedding_function = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "    # Initialize the Chroma vector store\n",
    "    chroma = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=persist_directory,\n",
    "        client=client,\n",
    "        embedding_function=embedding_function,\n",
    "    )\n",
    "\n",
    "    # Add documents to the vector store\n",
    "    add_documents_to_vector_store(chroma, ingest_data, allow_duplicates, limit)\n",
    "\n",
    "    return chroma\n",
    "\n",
    "\n",
    "\n",
    "# Build the vector store\n",
    "chroma_store = build_vector_store(\n",
    "    collection_name=collection_name,\n",
    "    embedding_model=embedding_model,\n",
    "    ingest_data=splitted_data,\n",
    "    allow_duplicates=False,\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "print(f\"✅ Successfully built the Chroma vector store collection named '{collection_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Query the vector store\n",
    "Perform a similarity search based on a given query. This step retrieves the most relevant documents from the vector store based on the input query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully performed a search with the query 'what probability density used to estimate attackers potential progress in bitcon network?'\n",
      "Result 1:\n",
      "Content: spent.\n",
      "The race between the honest chain and an attacker chain can be characterized as a Binomial \n",
      "Random Walk. The success event is the honest chain being extended by one block, increasing its \n",
      "lead by +1, and the failure event is the attacker's chain being extended by one block, reducing the \n",
      "gap by -1.\n",
      "The probability of an attacker catching up from a given deficit is analogous to a Gambler's \n",
      "Ruin problem. Suppose a gambler with unlimited credit starts at a deficit and plays potentially an \n",
      "infinite number of trials to try to reach breakeven. We can calculate the probability he ever \n",
      "reaches breakeven, or that an attacker ever catches up with the honest chain, as follows [8]:\n",
      "p = probability an honest node finds the next block\n",
      "q = probability the attacker finds the next block\n",
      "qz = probability the attacker will ever catch up from z blocks behind\n",
      "q z={\n",
      "1\n",
      "if p≤q\n",
      "q/ p\n",
      "z\n",
      "if pq}\n",
      "6\n",
      "Identities\n",
      "Transactions\n",
      "Trusted\n",
      "Third Party\n",
      "Counterparty\n",
      "Public\n",
      "Identities\n",
      "Transactions\n",
      "Public\n",
      "New...\n",
      "Metadata: {'author': '', 'creator': 'Writer', 'page': 5, 'title': ''}\n",
      "--------------------------------------------------\n",
      "Result 2:\n",
      "Content: prevents the sender from preparing a chain of blocks ahead of time by working on \n",
      "it continuously until he is lucky enough to get far enough ahead, then executing the transaction at \n",
      "that moment. Once the transaction is sent, the dishonest sender starts working in secret on a \n",
      "parallel chain containing an alternate version of his transaction.\n",
      "The recipient waits until the transaction has been added to a block and z blocks have been \n",
      "linked after it. He doesn't know the exact amount of progress the attacker has made, but \n",
      "assuming the honest blocks took the average expected time per block, the attacker's potential \n",
      "progress will be a Poisson distribution with expected value:\n",
      "=z q\n",
      "p\n",
      "To get the probability the attacker could still catch up now, we multiply the Poisson density for \n",
      "each amount of progress he could have made by the probability he could catch up from that point:\n",
      "∑\n",
      "k=0\n",
      "∞\n",
      "ke\n",
      "−\n",
      "k! ⋅{\n",
      "q/ p\n",
      "z−k\n",
      "if k≤z\n",
      "1\n",
      "if kz}\n",
      "Rearranging to avoid summing the infinite tail of the...\n",
      "Metadata: {'author': '', 'creator': 'Writer', 'page': 6, 'title': ''}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Write the query\n",
    "query = 'what probability density used to estimate attackers potential progress in bitcon network?'\n",
    "\n",
    "# Perform a search\n",
    "search_results = chroma_store.similarity_search(query, k=5)\n",
    "print(f\"✅ Successfully performed a search with the query '{query}'\")\n",
    "\n",
    "# Display the top 3 search results\n",
    "for idx, result in enumerate(search_results[:2]):\n",
    "    print(f\"Result {idx + 1}:\")\n",
    "    print(f\"Content: {result.page_content}...\")\n",
    "    print(f\"Metadata: {result.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Parse the Query-Similarity Search Results\n",
    "Parse the text from the search results and join them to create a context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text from the search results, and join them\n",
    "search_result_text = [result.page_content for result in search_results]\n",
    "search_result_text = \"\\n\".join(search_result_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Construct Instruction Format for the Model\n",
    "Construct the instruction that will be passed to the model, including both the context and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"Context: {search_result_text} \\nQuestion: {query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Perform Retrieval-augmented Generation (RAG)\n",
    "Generate the final answer using the RAG pipeline, leveraging the context and the query to produce a coherent response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully generated the answer using Retrieval-augmented Generation pipeline\n",
      "Answer: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The probability density used to estimate the attacker's potential progress in the Bitcoin network is a Poisson distribution. The expected value \\( \\lambda \\) for this distribution is given by the formula:\n",
       "\n",
       "\\[\n",
       "\\lambda = z \\cdot \\frac{q}{p}\n",
       "\\]\n",
       "\n",
       "where:\n",
       "- \\( z \\) is the number of blocks the attacker is behind,\n",
       "- \\( q \\) is the probability that the attacker finds the next block,\n",
       "- \\( p \\) is the probability that the honest node finds the next block.\n",
       "\n",
       "The Poisson density function evaluates the probability of the attacker making \\( k \\) blocks of progress given the expected value \\( \\lambda \\), which is used to derive the probability that the attacker could catch up from that point.\n",
       "\n",
       "In the code example you provided, the Poisson probability mass function is computed for \\( k \\) blocks of progress using the formula:\n",
       "\n",
       "\\[\n",
       "\\text{Poisson}(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n",
       "\\]\n",
       "\n",
       "This is integrated into a summation to evaluate the attacker's overall chances of catching up when accounting for all potential progress \\( k \\)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "result = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": instructions }],\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully generated the answer using Retrieval-augmented Generation pipeline\")\n",
    "print(\"Answer: \\n\")\n",
    "\n",
    "# Display the answer in markdown format\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(result.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
