{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Start from a Completed Hyper-Parameter Tuning Job\n",
    "Once the previous hyper-parameter tuning job completes, we can analyze the results and perform another round of optimization using `Warm Start`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-04-25-19-24-12-990\n"
     ]
    }
   ],
   "source": [
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: sagemaker-scikit-learn-2020-04-25-19-24-12-990\n"
     ]
    }
   ],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train'.format(scikit_processing_job_name)\n",
    "prefix_validation = '{}/output/bert-validation'.format(scikit_processing_job_name)\n",
    "prefix_test = '{}/output/bert-test'.format(scikit_processing_job_name)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-train\n",
      "2020-04-25 19:30:15   50965015 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-04-25 19:31:00   71723377 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(train_s3_uri)\n",
    "!aws s3 ls $train_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\r\n",
      "import random\r\n",
      "import pandas as pd\r\n",
      "from glob import glob\r\n",
      "import pprint\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.7.2'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from transformers import TFDistilBertForSequenceClassification\r\n",
      "from transformers import TextClassificationPipeline\r\n",
      "from transformers.configuration_distilbert import DistilBertConfig\r\n",
      "\r\n",
      "CLASSES = [1, 2, 3, 4, 5]\r\n",
      "\r\n",
      "def select_data_and_label_from_record(record):\r\n",
      "    x = {\r\n",
      "        'input_ids': record['input_ids'],\r\n",
      "        'input_mask': record['input_mask'],\r\n",
      "        'segment_ids': record['segment_ids']\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record['label_ids']\r\n",
      "\r\n",
      "    return (x, y)\r\n",
      "\r\n",
      "\r\n",
      "def file_based_input_dataset_builder(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    # For training, we want a lot of parallel reading and shuffling.\r\n",
      "    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n",
      "\r\n",
      "    if pipe_mode:\r\n",
      "        print('***** Using pipe_mode with channel {}'.format(channel))\r\n",
      "        from sagemaker_tensorflow import PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format='TFRecord')\r\n",
      "    else:\r\n",
      "        print('***** Using input_filenames {}'.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    def _decode_record(record, name_to_features):\r\n",
      "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        # TODO:  wip/bert/bert_attention_head_view/train.py\r\n",
      "        # Convert input_ids into input_tokens with DistilBert vocabulary \r\n",
      "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\r\n",
      "#              hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\r\n",
      "        return record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          lambda record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "    dataset.cache()\r\n",
      "\r\n",
      "    if is_training:\r\n",
      "        dataset = dataset.shuffle(seed=42,\r\n",
      "                                  buffer_size=1000,\r\n",
      "                                  reshuffle_each_iteration=True)\r\n",
      "\r\n",
      "    return dataset\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--train_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--test_data',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_CHANNEL_TEST'])\r\n",
      "    # This points to the S3 location - this should not be used by our code\r\n",
      "    # We should use /opt/ml/model/ instead\r\n",
      "#     parser.add_argument('--model_dir', \r\n",
      "#                         type=str, \r\n",
      "#                         default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--output_dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DIR'])\r\n",
      "    # This is unused\r\n",
      "    parser.add_argument('--output_data_dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\r\n",
      "    parser.add_argument('--hosts', \r\n",
      "                        type=list, \r\n",
      "                        default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current_host', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CURRENT_HOST'])    \r\n",
      "    parser.add_argument('--num_gpus', \r\n",
      "                        type=int, \r\n",
      "                        default=os.environ['SM_NUM_GPUS'])\r\n",
      "    parser.add_argument('--use_xla',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--use_amp',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--max_seq_length',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--train_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--validation_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--test_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--epochs',\r\n",
      "                        type=int,\r\n",
      "                        default=2)\r\n",
      "    parser.add_argument('--learning_rate',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00003)\r\n",
      "    parser.add_argument('--epsilon',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00000001)\r\n",
      "    parser.add_argument('--train_steps_per_epoch',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--validation_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--test_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--freeze_bert_layer',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--enable_sagemaker_debugger',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--run_validation',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_test',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_sample_predictions',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    print(\"Args:\") \r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    print(\"Environment Variables:\") \r\n",
      "    pprint.pprint(dict(env_var), width = 1) \r\n",
      "\r\n",
      "    train_data = args.train_data\r\n",
      "    print('train_data {}'.format(train_data))\r\n",
      "    validation_data = args.validation_data\r\n",
      "    print('validation_data {}'.format(validation_data))\r\n",
      "    test_data = args.test_data\r\n",
      "    print('test_data {}'.format(test_data))    \r\n",
      "\r\n",
      "#    model_dir = args.model_dir\r\n",
      "#    print('model_dir {}'.format(model_dir))    \r\n",
      "    local_model_dir = os.environ['SM_MODEL_DIR']\r\n",
      "\r\n",
      "    output_dir = args.output_dir\r\n",
      "    print('output_dir {}'.format(output_dir))    \r\n",
      "\r\n",
      "    # This is unused\r\n",
      "#    output_data_dir = args.output_data_dir\r\n",
      "#    print('output_data_dir {}'.format(output_data_dir))    \r\n",
      "    hosts = args.hosts\r\n",
      "    print('hosts {}'.format(hosts))    \r\n",
      "    current_host = args.current_host\r\n",
      "    print('current_host {}'.format(current_host))    \r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    print('num_gpus {}'.format(num_gpus))\r\n",
      "    use_xla = args.use_xla\r\n",
      "    print('use_xla {}'.format(use_xla))    \r\n",
      "    use_amp = args.use_amp\r\n",
      "    print('use_amp {}'.format(use_amp))    \r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    print('max_seq_length {}'.format(max_seq_length))    \r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    print('train_batch_size {}'.format(train_batch_size))    \r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    print('validation_batch_size {}'.format(validation_batch_size))    \r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    print('test_batch_size {}'.format(test_batch_size))    \r\n",
      "    epochs = args.epochs\r\n",
      "    print('epochs {}'.format(epochs))    \r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    print('learning_rate {}'.format(learning_rate))    \r\n",
      "    epsilon = args.epsilon\r\n",
      "    print('epsilon {}'.format(epsilon))    \r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    print('train_steps_per_epoch {}'.format(train_steps_per_epoch))    \r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    print('validation_steps {}'.format(validation_steps))    \r\n",
      "    test_steps = args.test_steps\r\n",
      "    print('test_steps {}'.format(test_steps))    \r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    print('freeze_bert_layer {}'.format(freeze_bert_layer))    \r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))    \r\n",
      "    run_validation = args.run_validation\r\n",
      "    print('run_validation {}'.format(run_validation))    \r\n",
      "    run_test = args.run_test\r\n",
      "    print('run_test {}'.format(run_test))    \r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "    print('run_sample_predictions {}'.format(run_sample_predictions))    \r\n",
      "\r\n",
      "    # Determine if PipeMode is enabled \r\n",
      "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\r\n",
      "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\r\n",
      "    print('Using pipe_mode: {}'.format(pipe_mode))\r\n",
      " \r\n",
      "    # Model Output \r\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, 'transformers/fine-tuned/')\r\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # SavedModel Output\r\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # Tensorboard Logs \r\n",
      "    tensorboard_logs_path = os.path.join(output_dir, 'tensorboard') \r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=True)\r\n",
      "\r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    # smdebug currently (0.7.2) does not support MultiWorkerMirroredStrategy()\r\n",
      "    # distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
      "    with distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\r\n",
      "        print('train_data_filenames {}'.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel='train',\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=True,\r\n",
      "            drop_remainder=False,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = None\r\n",
      "        config = None\r\n",
      "        model = None\r\n",
      "\r\n",
      "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\r\n",
      "        successful_download = False\r\n",
      "        retries = 0\r\n",
      "        while (retries < 5 and not successful_download):\r\n",
      "            try:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                          num_labels=len(CLASSES))\r\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                                              config=config)\r\n",
      "                successful_download = True\r\n",
      "                print('Sucessfully downloaded after {} retries.'.format(retries))\r\n",
      "            except:\r\n",
      "                retries = retries + 1\r\n",
      "                random_sleep = random.randint(1, 30)\r\n",
      "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        if not tokenizer or not model or not config:\r\n",
      "            print('Not properly initialized...')\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        print('** use_amp {}'.format(use_amp))        \r\n",
      "        if use_amp:\r\n",
      "            # loss scaling is currently required when using mixed precision\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "        print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))\r\n",
      "        if enable_sagemaker_debugger:\r\n",
      "            print('*** DEBUGGING ***')\r\n",
      "            import smdebug.tensorflow as smd\r\n",
      "            # This assumes that we specified debugger_hook_config\r\n",
      "            callback = smd.KerasHook.create_from_json_file()\r\n",
      "            print(callback)\r\n",
      "            print('*** CALLBACK {} ***'.format(callback))\r\n",
      "            callbacks.append(callback)\r\n",
      "            optimizer = callback.wrap_optimizer(optimizer)\r\n",
      "\r\n",
      "        callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\r\n",
      "        callbacks.append(callback)\r\n",
      "            \r\n",
      "        print('*** OPTIMIZER {} ***'.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        print('Trained model {}'.format(model))\r\n",
      "        print(model.summary())\r\n",
      "\r\n",
      "        if run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\r\n",
      "            print('validation_data_filenames {}'.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='validation',\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "            \r\n",
      "            # HACK:  trim the Validation dataset down to equal the number of validation steps to workaround PipeMode issue\r\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\r\n",
      "            \r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=True,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)\r\n",
      "            print(train_and_validation_history)\r\n",
      "        else: # Not running validation\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=True,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)\r\n",
      "            print(train_history)\r\n",
      "\r\n",
      "        if run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\r\n",
      "            print('test_data_filenames {}'.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='test',\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "            print(test_history)\r\n",
      "\r\n",
      "            \r\n",
      "        # Save the fine-tuned Transformers Model\r\n",
      "        print('transformer_fine_tuned_model_path {}'.format(transformer_fine_tuned_model_path))   \r\n",
      "\r\n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\r\n",
      "\r\n",
      "        # Save the TensorFlow SavedModel\r\n",
      "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \r\n",
      "        model.save(tensorflow_saved_model_path, save_format='tf')\r\n",
      "\r\n",
      "    if run_sample_predictions:\r\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\r\n",
      "                                                                       id2label={\r\n",
      "                                                                        0: 1,\r\n",
      "                                                                        1: 2,\r\n",
      "                                                                        2: 3,\r\n",
      "                                                                        3: 4,\r\n",
      "                                                                        4: 5\r\n",
      "                                                                       },\r\n",
      "                                                                       label2id={\r\n",
      "                                                                        1: 0,\r\n",
      "                                                                        2: 1,\r\n",
      "                                                                        3: 2,\r\n",
      "                                                                        4: 3,\r\n",
      "                                                                        5: 4\r\n",
      "                                                                       })\r\n",
      "\r\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "        if num_gpus >= 1:\r\n",
      "            inference_device = 0 # GPU 0\r\n",
      "        else:\r\n",
      "            inference_device = -1 # CPU\r\n",
      "        print('inference_device {}'.format(inference_device))\r\n",
      "\r\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                        tokenizer=tokenizer,\r\n",
      "                                                        framework='tf',\r\n",
      "                                                        device=inference_device)  \r\n",
      "\r\n",
      "        print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\r\n",
      "        print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\r\n",
      "        print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.00000001\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=1000\n",
    "validation_steps=1000\n",
    "test_steps=1000\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.8xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "max_seq_length=128\n",
    "freeze_bert_layer=True\n",
    "input_mode='Pipe'\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={\n",
    "                               'epsilon': epsilon,\n",
    "                               'validation_batch_size': validation_batch_size,\n",
    "                               'test_batch_size': test_batch_size,                                             \n",
    "                               'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                               'validation_steps': validation_steps,\n",
    "                               'test_steps': test_steps,\n",
    "                               'use_xla': use_xla,\n",
    "                               'use_amp': use_amp,                                             \n",
    "                               'max_seq_length': max_seq_length,\n",
    "                               'run_validation': run_validation,\n",
    "                               'run_test': run_test,\n",
    "                               'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       train_max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Warm Start Config\n",
    "We configure `WarmStartConfig` using 1 or more  of the previous hyper-parameter tuning job runs called the `parent` jobs - as well as a `WarmStartType`.  The parents must have finished either with one of the following success or failure states: `Completed`, `Stopped`, or `Failed`.\n",
    "\n",
    "`WarmStartType` is one of the following strategies:\n",
    "\n",
    "* `IDENTICAL_DATA_AND_ALGORITHM` uses the same input data and algorithm as the parent tuning jobs, but allows a practitioner to explore more hyper-parameter range values.  Upon completion, a tuning job with this strategy will return an additional field, `OverallBestTrainingJob` containing the best model candidate including this tuning job as well as the completed parent tuning jobs.\n",
    "* `TRANSFER_LEARNING` allows you to transfer the knowledge from previous tuning jobs.  You can use different input dataset and algorithm - as well as everything from the `IDENTICAL_DATA_AND_ALGORITHM` strategy.\n",
    "\n",
    "_Note:  Recursive parent-child relationships are not supported._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-200425-2332\n"
     ]
    }
   ],
   "source": [
    "print(tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Tuning Job Name: tensorflow-training-200425-2332\n"
     ]
    }
   ],
   "source": [
    "print('Previous Tuning Job Name: {}'.format(tuning_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import WarmStartConfig\n",
    "from sagemaker.tuner import WarmStartTypes\n",
    "\n",
    "warm_start_config = WarmStartConfig(warm_start_type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM, \n",
    "                                    parents={tuning_job_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Hyper-Parameter Ranges to Explore for the Warm Start Tuning Job\n",
    "While not necessary, we can choose to statically define any hyper-parameters that we are not choosing to explore in this WarmStart optimization run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.tuner import CategoricalParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "                                                \n",
    "hyperparameter_ranges = {\n",
    "    'epochs': IntegerParameter(8, 64, scaling_type='Logarithmic'),\n",
    "    'learning_rate': ContinuousParameter(0.00015, 0.00075, scaling_type='Linear'),\n",
    "    'train_batch_size': CategoricalParameter([128, 512, 1024]),\n",
    "    'freeze_bert_layer': CategoricalParameter([True, False])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameter Tuning Job with Warm Start Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metrics_definitions,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=1,\n",
    "    strategy='Bayesian',\n",
    "    early_stopping_type='Auto',\n",
    "    warm_start_config=warm_start_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train_data, \n",
    "           'validation': s3_input_validation_data,\n",
    "           'test': s3_input_test_data\n",
    "          }, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If You See an Error, Please Wait for the Hyper-Parameter Tuning Job to Complete from the Previous Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check Tuning Job Status\n",
    "\n",
    "Re-run this cell to track the status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'CreationTime': datetime.datetime(2020, 4, 26, 0, 23, 53, 721000, tzinfo=tzlocal()),\n",
      " 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-west-2:903253828154:hyper-parameter-tuning-job/tensorflow-training-200426-0023',\n",
      " 'HyperParameterTuningJobConfig': {'HyperParameterTuningJobObjective': {'MetricName': 'validation:accuracy',\n",
      "                                                                        'Type': 'Maximize'},\n",
      "                                   'ParameterRanges': {'CategoricalParameterRanges': [{'Name': 'train_batch_size',\n",
      "                                                                                       'Values': ['\"128\"',\n",
      "                                                                                                  '\"512\"',\n",
      "                                                                                                  '\"1024\"']},\n",
      "                                                                                      {'Name': 'freeze_bert_layer',\n",
      "                                                                                       'Values': ['\"True\"',\n",
      "                                                                                                  '\"False\"']}],\n",
      "                                                       'ContinuousParameterRanges': [{'MaxValue': '0.00075',\n",
      "                                                                                      'MinValue': '0.00015',\n",
      "                                                                                      'Name': 'learning_rate',\n",
      "                                                                                      'ScalingType': 'Linear'}],\n",
      "                                                       'IntegerParameterRanges': [{'MaxValue': '64',\n",
      "                                                                                   'MinValue': '8',\n",
      "                                                                                   'Name': 'epochs',\n",
      "                                                                                   'ScalingType': 'Logarithmic'}]},\n",
      "                                   'ResourceLimits': {'MaxNumberOfTrainingJobs': 2,\n",
      "                                                      'MaxParallelTrainingJobs': 1},\n",
      "                                   'Strategy': 'Bayesian',\n",
      "                                   'TrainingJobEarlyStoppingType': 'Auto'},\n",
      " 'HyperParameterTuningJobName': 'tensorflow-training-200426-0023',\n",
      " 'HyperParameterTuningJobStatus': 'InProgress',\n",
      " 'LastModifiedTime': datetime.datetime(2020, 4, 26, 0, 24, 5, 771000, tzinfo=tzlocal()),\n",
      " 'ObjectiveStatusCounters': {'Failed': 0, 'Pending': 1, 'Succeeded': 0},\n",
      " 'OverallBestTrainingJob': {'CreationTime': datetime.datetime(2020, 4, 25, 23, 32, 8, tzinfo=tzlocal()),\n",
      "                            'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:accuracy',\n",
      "                                                                            'Value': 0.7354999780654907},\n",
      "                            'ObjectiveStatus': 'Succeeded',\n",
      "                            'TrainingEndTime': datetime.datetime(2020, 4, 25, 23, 44, 20, tzinfo=tzlocal()),\n",
      "                            'TrainingJobArn': 'arn:aws:sagemaker:us-west-2:903253828154:training-job/tensorflow-training-200425-2332-001-7d12b860',\n",
      "                            'TrainingJobName': 'tensorflow-training-200425-2332-001-7d12b860',\n",
      "                            'TrainingJobStatus': 'Completed',\n",
      "                            'TrainingStartTime': datetime.datetime(2020, 4, 25, 23, 34, 16, tzinfo=tzlocal()),\n",
      "                            'TunedHyperParameters': {'epochs': '2',\n",
      "                                                     'freeze_bert_layer': '\"True\"',\n",
      "                                                     'learning_rate': '3.443581060524813e-05',\n",
      "                                                     'train_batch_size': '\"128\"'},\n",
      "                            'TuningJobName': 'tensorflow-training-200425-2332'},\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '4580',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 26 Apr 2020 00:24:06 GMT',\n",
      "                                      'x-amzn-requestid': '7adc163b-6cc4-4421-84f4-5885fa64c8f4'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '7adc163b-6cc4-4421-84f4-5885fa64c8f4',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TrainingJobDefinition': {'AlgorithmSpecification': {'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                             'Regex': 'loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'train:accuracy',\n",
      "                                                                             'Regex': 'accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:loss',\n",
      "                                                                             'Regex': 'val_loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:accuracy',\n",
      "                                                                             'Regex': 'val_accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'ObjectiveMetric',\n",
      "                                                                             'Regex': 'val_accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'}],\n",
      "                                                      'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-gpu-py3',\n",
      "                                                      'TrainingInputMode': 'Pipe'},\n",
      "                           'EnableInterContainerTrafficEncryption': False,\n",
      "                           'EnableManagedSpotTraining': False,\n",
      "                           'EnableNetworkIsolation': False,\n",
      "                           'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-train'}}},\n",
      "                                               {'ChannelName': 'validation',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-validation'}}},\n",
      "                                               {'ChannelName': 'test',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-903253828154/sagemaker-scikit-learn-2020-04-25-19-24-12-990/output/bert-test'}}}],\n",
      "                           'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-west-2-903253828154/'},\n",
      "                           'ResourceConfig': {'InstanceCount': 1,\n",
      "                                              'InstanceType': 'ml.p3.8xlarge',\n",
      "                                              'VolumeSizeInGB': 1024},\n",
      "                           'RoleArn': 'arn:aws:iam::903253828154:role/TeamRole',\n",
      "                           'StaticHyperParameters': {'_tuning_objective_metric': 'validation:accuracy',\n",
      "                                                     'epsilon': '1e-08',\n",
      "                                                     'max_seq_length': '128',\n",
      "                                                     'model_dir': '\"s3://sagemaker-us-west-2-903253828154/tensorflow-training-2020-04-26-00-22-22-563/model\"',\n",
      "                                                     'run_sample_predictions': 'true',\n",
      "                                                     'run_test': 'true',\n",
      "                                                     'run_validation': 'true',\n",
      "                                                     'sagemaker_container_log_level': '20',\n",
      "                                                     'sagemaker_enable_cloudwatch_metrics': 'false',\n",
      "                                                     'sagemaker_estimator_class_name': '\"TensorFlow\"',\n",
      "                                                     'sagemaker_estimator_module': '\"sagemaker.tensorflow.estimator\"',\n",
      "                                                     'sagemaker_job_name': '\"tensorflow-training-2020-04-26-00-23-53-299\"',\n",
      "                                                     'sagemaker_program': '\"tf_bert_reviews.py\"',\n",
      "                                                     'sagemaker_region': '\"us-west-2\"',\n",
      "                                                     'sagemaker_submit_directory': '\"s3://sagemaker-us-west-2-903253828154/tensorflow-training-2020-04-26-00-23-53-299/source/sourcedir.tar.gz\"',\n",
      "                                                     'test_batch_size': '128',\n",
      "                                                     'test_steps': '1000',\n",
      "                                                     'train_steps_per_epoch': '1000',\n",
      "                                                     'use_amp': 'true',\n",
      "                                                     'use_xla': 'true',\n",
      "                                                     'validation_batch_size': '128',\n",
      "                                                     'validation_steps': '1000'},\n",
      "                           'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      " 'TrainingJobStatusCounters': {'Completed': 0,\n",
      "                               'InProgress': 1,\n",
      "                               'NonRetryableError': 0,\n",
      "                               'RetryableError': 0,\n",
      "                               'Stopped': 0},\n",
      " 'WarmStartConfig': {'ParentHyperParameterTuningJobs': [{'HyperParameterTuningJobName': 'tensorflow-training-200425-2332'}],\n",
      "                     'WarmStartType': 'IdenticalDataAndAlgorithm'}}\n",
      "Not yet complete, but {} jobs have completed.\n",
      "No training jobs have reported results yet.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "job_description = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = job_description['HyperParameterTuningJobStatus']\n",
    "\n",
    "print('\\n')\n",
    "print(status)\n",
    "print('\\n')\n",
    "pprint(job_description)\n",
    "\n",
    "if status != 'Completed':\n",
    "    job_count = job_description['TrainingJobStatusCounters']['Completed']\n",
    "    print('Not yet complete, but {} jobs have completed.')\n",
    "    \n",
    "    if job_description.get('BestTrainingJob', None):\n",
    "        print(\"Best candidate:\")\n",
    "        pprint(job_description['BestTrainingJob']['TrainingJobName'])\n",
    "        pprint(job_description['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric'])\n",
    "    else:\n",
    "        print(\"No training jobs have reported results yet.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/hyper-tuning-jobs/tensorflow-training-200426-0023\">Hyper-Parameter Tuning Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/hyper-tuning-jobs/{}\">Hyper-Parameter Tuning Job</a></b>'.format(region, tuning_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Tuning Job\n",
    "### _Note:  This will fail at first.  Please wait about 15-30 seconds and re-run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_bert_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tensorflow-training-200426-0023-001-374665c5</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>13.0</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>\"1024\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FinalObjectiveValue TrainingEndTime  \\\n",
       "0                None            None   \n",
       "\n",
       "                                TrainingJobName TrainingJobStatus  \\\n",
       "0  tensorflow-training-200426-0023-001-374665c5        InProgress   \n",
       "\n",
       "  TrainingStartTime  epochs freeze_bert_layer  learning_rate train_batch_size  \n",
       "0              None    13.0            \"True\"       0.000693           \"1024\"  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "hp_results = HyperparameterTuningJobAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    hyperparameter_tuning_job_name=tuning_job_name\n",
    ")\n",
    "\n",
    "df_results = hp_results.dataframe()\n",
    "\n",
    "df_results.sort_values('FinalObjectiveValue', ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Overall Best Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_bert_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tensorflow-training-200426-0023-001-374665c5</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>13.0</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>\"1024\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FinalObjectiveValue TrainingEndTime  \\\n",
       "0                None            None   \n",
       "\n",
       "                                TrainingJobName TrainingJobStatus  \\\n",
       "0  tensorflow-training-200426-0023-001-374665c5        InProgress   \n",
       "\n",
       "  TrainingStartTime  epochs freeze_bert_layer  learning_rate train_batch_size  \n",
       "0              None    13.0            \"True\"       0.000693           \"1024\"  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_candidate_tuning_job_name = df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tensorflow-training-200426-0023-001-374665c5\n",
      "Name: TrainingJobName, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(best_candidate_tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'best_candidate_tuning_job_name' (Series)\n"
     ]
    }
   ],
   "source": [
    "%store best_candidate_tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
