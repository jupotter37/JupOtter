{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Representation Design Pattern\n",
    "\n",
    "This chapter looks at the different types of machine learning problems and analyses how the model architectures vary depending on the problem.\n",
    "\n",
    "The input and output types are tow key factors impacting the model architecture. For example the output required from a model could impact if we choose a regression or a classification model. Special nerual network layers exist for specific types of input data: convolutional layers for images, speech text and other data with spatiotemporal correlation, recurrent networks for sequential data. Special classes of solutions exist for commonly occuring problems like recommendations (matrix factorization), or time-series (ARIMA). A group of simpler model stogher with common isioms can be used to solve more compex problems e.g. text generation often involves a classification model whose outputs are postprocessed using a beam search algorithm.\n",
    "\n",
    "## Design Pattern 5: Reframing\n",
    "\n",
    "This pattern refers to chaning the ouput of ML problem. For example, we could take something that is intuitively a regression problem and instead pose it as a classification prblem.\n",
    "\n",
    "### Problem\n",
    "\n",
    "The first step of building any ML solution is framing the problem. Is this a supervised learning problem? Or unsupervised? What are the features? If it is a supervised problem what are the labels? What amount of error is acceptable? Of course, the answers to these questions must be considered in context with the training, the task at hand, and the metrics for success.\n",
    "\n",
    "For example, if we wanted to predict the amount of rainfall in a given area we could make this a regression problem. We could also treat this as a time series model. There are lost of adjustments we can make to improve our model. Is regression the onle wat we can pose this task? Perhaps we can re-frame our machine learning objective in a way that improves our task performance.\n",
    "\n",
    "### Solution\n",
    "\n",
    "If we used a regression model to predict the the amount of rail fall we're limiting to a prediciton of a single number. We can reframe this as a classification problem where one approach would be to model a discrete probability distribution e.g. we have binned amount of rain-fall as a class e.g. `0-0.05mm`, `0.5 - 1.0mm` etc and each class will have an associated probability. We can also have a regerssion model to predict a real-value number.\n",
    "\n",
    "Both the regression approach and the re-framed classification approach give a prediction of the rainfall. However, the classification approach allows the model to capture teh probability distribution of rainfall of different quantities.\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "By reframing we lose a little precision due to bucketing, but gain the expressivess of a full probability density function. The discretised predictions provided by the classificatoin model are more adept at learning a complex target then a more rigid regression model.\n",
    "\n",
    "Added advantage of this classification framing is that we obtain posterior probability distribution of our predicited values which provides more nuanced information. Suppose the learned distribution is bimodal. By modelling a classificaton as a discrete probability distribution, the model is able to caputre the biomodal structure of the predictions. Where as if we used a regression model we would lose this information.\n",
    "\n",
    "#### Capturing Uncertainty\n",
    "\n",
    "Looking at the dataset of babies born to 25 year old mothers at 38 weeks shows a normal distribution with a mean at 7.5 pounds. There is a nontrivial likelihood (33%) that a given baby is less than 6.5 pounds or more than 8.5 pounds (this is 1 STD either side of the mean, page 83 in book). The width of this distribuiton indicates the irreducible error inherent to the problem of predicting baby weight. If we framed it as a regression problem the best RMSE we can obtain is the standard deviation of the distribution.\n",
    "\n",
    "If we frame it as a regression problem we would have to state the prediction result as 7.5 +/- 1.0 (or whatever the STD is). Yet the width if the distribution will vary for different combinations of inputs, and so learning the width is another machine learning problem in itself. For example, at the 36th week, for mothers of the same age, the standard deviation is 1.16 pounds.\n",
    "\n",
    "Has the distribution been multimodal (with multiple peaks), the case for reframing the problem as a classification problem would have been even stronger. However, it is helpful to realise that because of the law of large numbers, as long as we capture all of the relevant inputs, many of the distributions we will encounter on large datasets will be bell-shaped, although other distributions are possible. The wider the bell curve the more the width varies at different values of inputs, the more important it is to capture uncertainty and the stronger the case for reframing the regression problem as a classification one.\n",
    "\n",
    "By reframing the problem, we train the model as a multiclass classification that learns a discrete probability distribution for the given trainin examples. These discretised predictions are more flexible in terms of capturing uncertainty and better able to approximate the complex target than a regression model. At inference time, the model then predicts a collection of probabilities corresponding to these ptential outputs. That is, we obtain a discrete PDF giving the relative likelihood of any specific weight.\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "There is rarely just one way to frame a problem. For example, bucketizing the output values of a regression is an approach to reframing the problem as a classification task. Another apporach is multitask learning that combines both tasks (classification and regression) into a single model using multiple prediction heads. With any reframing technique, being aware of data limitations or the risk of introducing label bias is important.\n",
    "\n",
    "#### Bucketised outputs\n",
    "\n",
    "The typical approach to reframing a regression task as a classification task is to bucketise the output values. For example, if out model is to be used to inidicate how much rain we will get on a given day it we may bucket the values into say 5 groups.\n",
    "\n",
    "The regression problem now becomes a classification problem. Intuitively, it is easier to predict 5 categories than to predict a single continuous number. By using categorical outputs, the model is incentivised less for getting arbitrarily close to the actual output value since we've essentially changed the output label to a range of values instead of a single number.\n",
    "\n",
    "#### Restricting the Prediction Range\n",
    "\n",
    "For a given problem the prediction range may be `[3,20]`. If we train a regression model there is always a chance than the model may make predictions outside of this range. One way to limit the range is to reframe the problem. For example, we could build a DNN where the last layer is a sigmoid later and we then map the `[0,1]` to the range of `[3,20]`.\n",
    "\n",
    "#### Label Bias\n",
    "\n",
    "It is important to consider the nature of the target label when reframing the problem. For example, suppose we regramed our recommendation model to a classification task that predicts the likelihood a user will click on a certain video thumbnail. This seems like a resonable reframing since our goal is to provide content a user will select and watch. But be careful. This chance of albel is not actually in line with our prediction task. By optimising for user clicks, our model will inadvertently promote click bait and not actually recommend content to use to the user.\n",
    "\n",
    "Instead, a more advantageous label would be video watch time, reframing our recommendation as a regression instead. Or predict the likelihood that the user will watch at least half the video clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 6: Multilabel\n",
    "\n",
    "The multilabel design pattern refers to a problem where we assign more than one label to a given training example.\n",
    "\n",
    "### Problem\n",
    "\n",
    "Often prediction tasks involve applying a single classification to a given training example. This prediction is determined from N possible classes where N is greater than 1. In this case, it's commin to use softmax as the activation function for the output layer. Using softmax, the output of out model is an N-element array, where the sum of all the values adds up to 1. Each value indicates the probability that a particular example is associated with the class at the index.\n",
    "\n",
    "For example, if out model is classifying images as cats, dogs or rabbits, the softmax output might loos like this for a given image `[0.89, 0.02, 0.09]`. This means out model is predicting an 89% chanbe the image is a  cat. Because each image can only have one possible label in this scenario, we can take the `argmax` (index of the highest probability) to determine our model's predicted class. The less-common scenario is when each training exmaple can be assigned more than one label, which is what this pattern addresses.\n",
    "\n",
    "The multilabel design pattern exists for odels trained on all data modalities. For image classificiation, in the earlier example we could instead used images which depicted multiple animals, and could therefore have multiple labels. The same can be applied to text models e.g. a news article could belong to many different categories.\n",
    "\n",
    "The design pattern can also apply to tabular datasets for example healthecare data could be used to predicit multiple conditions.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The solution is to use a sigmoid activation function in our final layer instead of a softmax. Each individual in a sigmoid array is a float between 0 and 1. That is to say, when implementing the Multilabel design pattern, our label needs to be multi-hot encoded. The length of the multi-hot array corresponds with the number of classes in our model, and each output in this label array will be a sigmoid value.\n",
    "\n",
    "The main differenc between the sigmoid and softmax is that the softmax array is guaranteed to contain three values that sum to 1, where as the sigmoid out put will contain three values each between 0 and 1.\n",
    "\n",
    "The sigmoid is a nonlinear, continuous and differentiable activation function that takes the outputs of each neuron in the previous layer in the modle and squashes the value of thos outputs between 0 and 1.\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "- **Multiclass classification**: Each example can have only 1 label\n",
    "- **Binary classification**: The number of classes is 2\n",
    "- **Multilabel classification**: Each example can have many labels\n",
    "\n",
    "If a multiclass scenario use softmax and in a binary classification scenario use sigmoid. In a multilabel scenario we use a sigmoid for each label.\n",
    "\n",
    "For the multilabel scenario we can use the binary cross entropy loss because a multilabel problem is essentially `n` smaller binary classification problems.\n",
    "\n",
    "#### Parsing Sigmoid Results\n",
    "\n",
    "By applying a sigmoid per class we obtain a probability per class. To assign labels to a given prediction we can say if the probability of a label is above 50% it should be assigned to the data point. Additionally we can also apply `n_specific_tag` / `n_total_examples` as a threshold for each class. Here, `n_specific_tag` is the number of examples with one tag in the dataset and `n_total_examples` is the total number of examples in the training set across all tags. This ensures that the model is doing better than guessing a certain label based on its occurrence in the training dataset.\n",
    "\n",
    "For a more precise approach read this [paper](https://pralab.diee.unica.it/sites/default/files/pillai_PR2013_Thresholding_0.pdf). Uses S-Cut for optimizing your models F-measure.\n",
    "\n",
    "#### Dataset Considerations\n",
    "\n",
    "Dataset balancing is important for ML models and is more nuanced for the Multlabel design pattern. \n",
    "\n",
    "For model to learn what each unique label is we'll want to ensure the training dataset consists of varied combinations of each tag. If two labels occur frequently together the model may not learn to classify the label if it appears on its own. To account fo this think about the relationships between the labels and count the number of training exmaples that belong to each overlapping combinations of labels.\n",
    "\n",
    "We can consider hierarchical labels if the dataset allows. e.g.\n",
    "```\n",
    "animal -> invertebrate -> arthropod -> spider\n",
    "```\n",
    "There are two common approaches to for handling heirarchical labels:\n",
    "- Use a flat approach and put every label in the same output array. Make sure there are enough samples at each leaf node\n",
    "- Use cascade design pattern. Build one model to identify higher-level labels. Based on the higher-level classification, send the example to a separate model for a more specific classification task. E.g. higher level model predicits a datapoint to be an \"animal\" we then send the datapoint to differnent model(s) to apply more granular labels.\n",
    "\n",
    "Flat approach more straighforward. However, this might cause the model to lose information about more detailed label classes since ther will naturally be more training examples with higher-level labels in the dataset.\n",
    "\n",
    "#### Inputs with Overlapping Labels\n",
    "\n",
    "The multilabel approach to predicitions is usefull in overlapping labels. For example, if an image contains multiple fashion items and two people we're to label the items in it such as:\n",
    "- Long sleeved blazer\n",
    "- Double breasted blazer\n",
    "Both labels are correct but the issue arises in the situation where depending on who labelled the image the model may predict things differently. There is where multilabel is usefull because it allows us to associate both overlapping labels with an image.\n",
    "\n",
    "#### One Versus Rest\n",
    "\n",
    "Another technique for handling multilabel classification is to trian multiple binary classifiers instead of one multilabel model. This apporach is called _one versus rest_. We would train a binary classifier for each label.\n",
    "\n",
    "This can help with tate categories since the modell will be performing only one classification taks at a time on each input. The disadvantage of this approach is the added complexity of training many different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 7: Ensembles\n",
    "\n",
    "Pattern combines multiple ML models and aggregates their results to make predictions. Ensembles can be an effective means to improve performance and produce predictions that are better than any single model\n",
    "\n",
    "### Problem\n",
    "\n",
    "Imagine we have a model where it was trained such that the error on the training set it almost zero. However in production or on the holdout set a lot of our predictions are wrong. What went wrong? and how can we fix it?\n",
    "\n",
    "Error in an ML model can be broken down into three parts:\n",
    "- **Irreducible error**: Error due to bias and error due to variance. This is an inherent error resulting from noise in the dataset, the framing of the problem or bad training examples e.g. measurement errors. We can't do much about this error type.\n",
    "- **Bias and Variance** This is a reducible error and here we can influence our model's performance.\n",
    "    - Bias is the model's inability to learn enough about the relationship between the datapoints\n",
    "    - Variance caputres the models inability to generalise on new unseen examples\n",
    "    \n",
    "High bias oversimplifies the relationship between the features and is said to underfit. High variance has learned too much about the training data and is said to overfit. The ideal model will have low bias and low variance, in practice this is difficult, known as the bias-variance trade-off. \n",
    "\n",
    "### Solution\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning models as a technique to decrease the bias and/or variance and imporve model performance. By building several models with different inductive biases and aggregating their outputs, we hope to get a model with better performance.\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "Bagging is short for bootstrap aggregating and is a type of parallel ensembling method and is used to address high variance in ML models. The bootstrap part of bagging referes to the datasets used for training the ensemble members. Specifically, if there are $k$ submodels, then there are $k$ separate datasets used for training each submodel of the ensemble. Each dataset is constructed by randomly sampling (with replacement) from the original training dataset. This means there is a high probability that any of the $k$ datasets will be missing some training examples, but also any dataset will likely have repeated training examples.\n",
    "\n",
    "A good example of baggin is the random forest model. Each tree is trained on randomly sampled subsets of the entire training data, then the tree predicitions are aggregated to produce a prediction.\n",
    "\n",
    "Model averaging as seen in bagging is a powerful and reliable method for reducing model variance. As we'll see, different ensemble methods combine multiple submodels in different ways, sometimes using different models, different algorithms, or even different objective functions. With bagging, the model and algorithms are the same. For example, with random forest, the submodels are all short decision tree.\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "Another ensemble technique, different to bagging. Boosting ultimately constructs an ensemble model with more capacity than the individual member models. For this reason, boosting provides a more effective means of reducing bias than variance. The idea behind boosting is to iteratively build an ensemble of models where each successive model focuses on learning the examples the previous model got wrong. In short, boosting iteratively improves upon a sequence of weak learners taking a weighted average to ultimatley yield a strong learner.\n",
    "\n",
    "At start of the boosting procedure, a simple base model `f_0` is selected. For a regression task, the base model could just be the average target value: `f_0 = np.mean(Y_train`. For the first iteration step, the residuals `delta_1` are measured and approximated via a separate model. This residual model can be anything, but typically isn't complicated e.g. a weak learner such as a decision tree. The approximation provided by the residual model is then added to the current prediction, and the process continues.\n",
    "\n",
    "After many iterations, the residuals tend towards zero and the prediction gets better and better at modelling the original training dataset.\n",
    "\n",
    "Some wekk-know boosting algorithms are: AdaBoost, Gradient Boosting Machines and XGBoost.\n",
    "\n",
    "#### Stacking\n",
    "\n",
    "Stacking is an ensemble method that combines the outputs of a collection of models to make a prediction. The initial models, which are typically of a different model types, are trained to completion on the full training dataset. Then, a secondary meta-model is trained using the inital model output featues. This second meta-model learns how to best combine the outcomes of the inital models to decrease the training error and can be any type of the machine learning model.\n",
    "\n",
    "To do this we train all models in the ensemble on the full training dataset. These submodels are incorporated into a larger stacking ensemble model as individual inputs. We then train a model on the outputs of these sub-models.\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "Model averaging methods like bagging work because typically the individual models that make up the ensemble model will not all make the same erros on the test set. In an ideal situation, each individual model is off by a random amount, so when theor results are averaged, the random errors cancel out, and the prediction os closer to the correct answer. There is wisdom in the crowd.\n",
    "\n",
    "Boosting works because the model is punished more and more according to the residuals at each iteration step. With each iteration, the ensemble model is encouraged to get better and better at predicting those hard-to-predict examples. Stacking works because it combines the best of both badding and boosting. The secondary model can be thought of as a more sophisticated version of model averaging.\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "If the errors in each model are correlated, model averaging doesn't help at all. On the other hand, if the errors are perfectly uncorrelated the variance should decrease with the number of models (k): `var/k`. So the expected square error decreases linearly with the number of `k` models in the ensemble. Overall, on average, the ensemble will perform at least as well as any of the individual models in the ensemble. Futhermore, if the models make independent errors i.e. their errors are not correlated, then the ensemble will perform significantly better. The key to success with bagging is model diversity.\n",
    "\n",
    "Model averaging can even benefit neural networks trained on the same dataset. In fact, one recommended solution to fix the high variance of neural networks is to train multiple models and aggregate their predictions.\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "Boosting algorithm works by iteratively imporving the model to reduce the prediciton error. Each new weak learner corrects for the mistakes of the previous prection by modeling the residuals at `delta_i` of each step. The final prediction is the sum of the outputs from the base learner and each of the successive weak learners. Boosting iteratively builds a strong learner from a sequence of weak learners that model the residual error of the previous iteration.\n",
    "\n",
    "Thus, the resulting ensemble model becomes successively more and more complex, having more capactity than any one of its members. This also explains why bootstraping is particularly good for combating high bias. By iteratively focusing on the hard-to-predict exampels boosting effectively decreases the bias of the resulting model.\n",
    "\n",
    "#### Stacking\n",
    "\n",
    "Stacking can be thought of as an extension of simple model averaging where we train `k` models to completion on the training dataset, then average the results to deterimine a prediciton. Simple model averaging is similar to bagging, but the models in the ensemble could be of different types, while for bagging, the modesl are of the same type. More generally, we could modify the averaging step to take a weighted average, for example, to give more weight to one model en our ensemble over the others. The weighting could be based of the relative accuracy of the models.\n",
    "\n",
    "Stacking is a more advanced version of model averaging, where instead of taking the average or weighted average, we train a second model ML model on the outputs to learn how to best combine the results to the models in our ensemble to product a prediction. This provides all the benefits of decreasing variance as with badding techniques but also controls for high bias.\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Increased Training and Design Time\n",
    "\n",
    "By using an ensemble model we've introduced an additional amount of overhead in our model development, not to mention maintenance, inference compelxity and resource usage if the ensemble model goes into production. This can become impactical if the number of models in the ensemble increases.\n",
    "\n",
    "We should carefully consider if the increased overhead is worth the complexity.\n",
    "\n",
    "#### Dropout as Bagging\n",
    "\n",
    "Techniques like dropout provide a powerful and effective alternative. Dropout is know as a regularisation technique in deep learning but can also understood as an approximation to bagging. Dropout in a neural network randomly turns off neurons for each mini-batch of training, essentially evaluating a bagged ensemble of exponentially many neural networks. Dropout is not the same as bagging though. In the case of bagging, the models are independent, while when training with dropout, the models share parameters. In bagging also, the models are trained to convergence on their respective training dataset. However, when training with dropout, the ensemble member models would onle be trained for a single training step because different nodes are dropped out each iteration in the training loop.\n",
    "\n",
    "#### Decreased Model Interpretability\n",
    "\n",
    "In deep learning understanding why our model makes predictions is already difficult. This problem is compounded with ensemble models.\n",
    "\n",
    "#### Choosing the Right Tool for the Problem\n",
    "\n",
    "Some ensemble techniques are better at addressing bias or variance than others:\n",
    "\n",
    "| Problem                     | Ensemble Solution |\n",
    "|-----------------------------|------------------:|\n",
    "| High Bias (underfitting)    |          Boosting |\n",
    "| High Variance (overfitting) |           Bagging |\n",
    "\n",
    "Using the wrong ensemble method for our problem won't necessarily improve performance, it will add uneeded overhead.\n",
    "\n",
    "#### Other Ensembles\n",
    "\n",
    "There are an array of ensembles to choose from: bayesian, neural nets, RL etc... The ensemble design pattern encompasses techniques that combine multiple ML models to improve overall model performance and can be particularly useful when addressing common training issues like high bias or high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 8: Cascade\n",
    "\n",
    "Addresses situations where a ML problem can be profitably broken into a series of ML problems\n",
    "\n",
    "### Problem\n",
    "\n",
    "What happens if we need to predict a value during usual and unusual activity? The model will ignore the unusual activity because its rare. If the unusual activity is also associated with abnormal values then trainability suffers.\n",
    "\n",
    "Example, how to identify resellers? A store makes millions of tranactions and only a few thousand are reseller transactions. Don't really know at the time of purchase if the item being bought is from a retail buyer or reseller.\n",
    "\n",
    "If we have labelled instances of re-seller tranactions we can overweight these instances when training the model. This is suboptimal because we need to get the more common retail buyer use case as correct as possible. We don't want trade of accuracy between the two types of customers. However, these types of customers behave differently a retail buyer may return an item within a week whereas a re-seller will only return the item if they cannot sell it which can be several months later.\n",
    "\n",
    "A way to address this problem is with the cascade design pattern. We break the problem into four parts:\n",
    "1. Prediciting whether a specific transaction is by a reseller\n",
    "2. Train one model on sales to retail buyers\n",
    "3. Train the second model on sales to resellers\n",
    "4. In production, combining the output of the three separate models to predict return likelihood for every item purchased and the probability that the transactions is by a reseller.\n",
    "\n",
    "This allows different decisions on items likely to be returned depending on the type of buyer and ensures that the models in steps 2 and 3 are as accurate as possible on their segment of the training data. Each of these models is relatively easy to train. The first is simply a classifier, and if the unusual activity is extremely rare, we can use the rebalancing patter to address this. The next two models are essentially classification models trained on different segments of the training data. The combination is deterministic since we choose which model to run based on whether the activity belonged to a reseller.\n",
    "\n",
    "The problem comes during prediction. At prediction time, we don't have true labels just the output of the first classification model. Based on the output of the first model we will have to determine which of the two sales models to invoke. The problem is that we are training on labels, but at inference time, we will have to make decisions based on predictions. And predictions have errors. So, the second and third models will be required to make predictions on data that they might have never seen during training.\n",
    "\n",
    "How do we train a cascade of models where the output of one model is an input to the following model or determines the selection of subsequent models.\n",
    "\n",
    "### Solution\n",
    "\n",
    "A ML problem where the output of one model is and input to the following model or determines the selection of subequent models is called a `cascade`. Special care is needed when training these models.\n",
    "\n",
    "For example, a model which has unusual circumstances can be solved by treating is as a cascade of four ML models:\n",
    "1. A classification model to identify the circumstance\n",
    "2. One model trained on unusual circumstances\n",
    "3. A separate model trained on typical circumstances\n",
    "4. A model to combine the output of the two separate models, because the output is a probablisitc combination of two outputs\n",
    "\n",
    "Looks similar to an ensemble of models but is considered not because of the special experiment design pattern required when doing a cascade.\n",
    "\n",
    "Image we want to know where to stock bicycles at stations, we wish to predict the distance between rental and return stations. The goal of the model is to predicti the distance we need to transport the bicycle back to the rental location given features such as time of day, location of rental etc... Rentals longer than four hours are very different to shorter rentals in terms of behaviour and stocking alogrithm will require both outputs (prob rental longer than four hours and distance bicycle needs to be transported). However, only a small fraction of rentals involve such abnormal trips.\n",
    "\n",
    "One solution is to train a model to classify trips into long or typical trips. It can be tempting to split the training set into two parts based on the actual duration of the rental and train the next two models, one on long rentals and the other on typical rentals. The problem is that the classification model just discussed will have errors, these errors will be passed to the models downstream.\n",
    "\n",
    "Instead, after training the classification model, we need to use the predictions of this model to create the training dataset for the next set of models. For example, we could create the training dataset for the model to predict the distance of typical rentals. Take the predictions where `predicicted_trip_type = 'typical'` and use this as training data to make predictions on the distance. We do the same for where `predicicted_trip_type = 'long'`.\n",
    "\n",
    "Finally, our evaluation prediction should take into account that we need to use three trained models not just one. This is what the term the cascade desing pattern.\n",
    "\n",
    "When ever upstream models are re-trained the downstream models should also be re-trained.\n",
    "\n",
    "### Trade-Off and Alternatives\n",
    "\n",
    "- Don't go overboard with this pattern, this is not necessarily a best practise\n",
    "- Adds a lot of complexity\n",
    "\n",
    "#### Deterministic Inputs\n",
    "\n",
    "Splitting an ML problem is usually a bad idea, since an ML model can/should learn combinations of multiple factors:\n",
    "- If a condition can be known deterministically from the input (holiday shopping vs weekday shopping), we should just add the condition as one more input to the model\n",
    "- If the condition involves an extrema in just on input (e.g. customers living nearby vs far away, with the meaning of near/far needing to be learned from the data), we can use `Mixed Input Representation` to handle it\n",
    "\n",
    "The cascade pattern hadles an unusual scenario for which we do not have a categorical input, and for which extreme values need to be learned from multiple inputs\n",
    "\n",
    "#### Single Model\n",
    "\n",
    "Don't use the cascade pattern in a scenario where a single model would suffice.\n",
    "\n",
    "#### Internal Consistency\n",
    "\n",
    "Cascade is needed when we need to maintain internal consistency amongst the precitions of multiple models. We are trying to do more than just to predict the unusal activity. The reason we would use cascade is that the imbalanced label output is needed as an input to subsequent models and is useful in and of itself.\n",
    "\n",
    "Similarly, suppose that the reason we are training the model to predict a customers propensity to buy is to make a discounted offer. Whether or not we make the discounted offer, and the amount of discount, will very often depend on whether this customer is comparison shopping or not. Given this, we need internal consistency between two models (the model for comparison shoppers and the model for propensity to buy). In this case the cascade design pattern is needed.\n",
    "\n",
    "#### Pre-trained models\n",
    "\n",
    "The cascade pattern is needed when we wish to reuse the output of a pre-trained model as an input to out model. We should train our model on the output of the pre-trained model.\n",
    "\n",
    "#### Regression in rare situations\n",
    "\n",
    "The cascade pattern can be helpful when carrying out regression when some values are much more common than others. For example, you want to predict rainfall from satellite images. It might be the case that 99% of the pixels, it doesn't rain. In such cases, it can be helpful to create a stacked classification model followed by a regression model:\n",
    "\n",
    "1. First predict whether or not it is going to rain\n",
    "2. For pixels where the model predicts rain is not likely, predict a rainfall amount of zero.\n",
    "3. Train a regression model to predict the rainfall amount on pixels where the model predicts that rain is likely \n",
    "\n",
    "Important to realise that the classification model is not perfect, and so the regression model has to be trained on pixels that the classification model predicts as likely to be raining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 9: Netural Class\n",
    "\n",
    "Instead of training a a binary classifier that outputs the probability of an event, train a three-class classifier that outputs disjoint probabilies for `yes`, `no` and `maybe`. Disjoint here means that the classes do not overlap.\n",
    "\n",
    "### Problem\n",
    " \n",
    "Imagine we have painkiller A used for stomache pains and painkiller B used for liver problems. Beyond the stated use cases doctors default to their preferred painkiller. Training a binary classifier on such a dataset will lead to poor accuracy because the model will need to get the essentially artitrary classes correct.\n",
    " \n",
    "### Solution\n",
    "\n",
    "This happens at the data collection level. If a doctor prescribes painkiller A we should also ask in painkiller B would be acceptable. Based in this we have a neutal class where both painkiller A and B would be acceptable.\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "#### Synthetic Data\n",
    "- 10% of data for stomace pains uses painkiller A\n",
    "- 10% of data for liver problems uses painkiller B\n",
    "- 80% of the data will have arbitrarily assigned a painkiller\n",
    "\n",
    "If we trained a model with two classes painkiller A or painkiller B the best accuracy we can get is 60%. If we create three classes and put the randomly assigned into that class we can get a much higher accuracy. The neurtal class pattern helps us avoid losing model accuracy because of arbitrarily labelled data.\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### When Human Experts Disagree\n",
    "\n",
    "Neutral class is helpful in dealing with disagreements among human experts. Like in the medicine example above.\n",
    "\n",
    "When it comes to human labelling, every pattern is labelled by multiple experts. Therefore, we know a priori which cases humans disagree about. It might seem simpler to discard such cases, and simply train a binary classifier as it doesn't matter what the model does on the neutral class. This has two problems:\n",
    "\n",
    "1. False confidence tends to affect the acceptance of the model by human experts. A model that outputs a neutral determination if often more acceptable to experts than a model that is wrongly confident in cases where the human expert would have chosen an alternative\n",
    "2. If we are trainin a cascade of models, then downstream models will be extremely sensitive to neutral classes. If we continue to improve this model, downstream models could change dramatically from version to version\n",
    "\n",
    "Another alternative is to use the agreement among human labellers as the weight of a patter during training. Thus, if 5 experts agree on a diagnosis, the training pattern gets a weight of 1, while if its a 3 to 2 split, the weight of the pattern might be only 0.6. This allows us to train a binary classifier, but overweight the classifier towards the \"sure\" class. If the model outputs 0.5, it is unclear whether it is because this reflects a situation where there was insufficient training data, or whether it is a situation where human experts disagree. Using a neutral class to capture areas of disagreement allows us to disambiguate the two situations.\n",
    "\n",
    "#### Customer Satisfaction\n",
    "\n",
    "Good to use in situations where we attempt to predict customer satisfaction. For example, if we has survey responses from customer who rated their experience on a scale of 1 - 10, it might be helpful to bucket the ratings inot three categories: 1 to 4 as bad, 5 to 7 as neutral and 8 - 10 as good. If instead we trained a binary classifier by thresholding 6, the model will spend too much effort trying to get essentially neutral responses correct.\n",
    "\n",
    "#### Reframing with Neutral Class\n",
    "\n",
    "On the stock market we want trade a security based on whether the model thinks it will go up or down. Because the stock market is volatile and the speed with which new information is reflected in stock prices, trying to trade on small predicted ups and downs is likely to lead to high trading costs and poo profits over time.\n",
    "\n",
    "The solution to this create a training dataset consisting of three classes:\n",
    "- Stocks that went up more than 5% - buy calls\n",
    "- Stocks that went down more than 5% - buy puts\n",
    "- The remaining stocks are in the neutral category\n",
    "\n",
    "Rather than training a regression model on how much stocks will go up, we can now train a classification model with these three classes and pick the most confident predictions for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 10: Rebalancing\n",
    "\n",
    "- Provides approaches for handling imbalanced datasets\n",
    "    - e.g. where a single label makes up the majority of the dataset\n",
    "- The pattern addresses where how to build models with datasets where few examples exist for a specific class or classes\n",
    "\n",
    "### Problem \n",
    "- Models learn best when there are balanced examples for each class\n",
    "- In cases like fraud detection, fraudulent transactions are very rare compared to regular transacitons. There is less data of fraud cases for the model to learn off.\n",
    "- Imbalances dataset apply to many models, binary classification, multclass classification, multilabel classification and even regression. In the regression setting, imbalanced datasets refer to data with outlier values that are either much higher or lower than the median of the dataset\n",
    "- The accuracy metric when training on skewed datasets is misleading\n",
    "    - If the dataset is 99% a single class and the accuracy is shown to be 99% the chances are the model only predicted the majority class and so hadn't learnt anything about distinguishing between the majority and minority classees\n",
    "    - To debug this it's always good to look at the confusion matrix\n",
    "    \n",
    "### Solution\n",
    "\n",
    "- Since accuracy can be misleading we need to choose an appropriate evaluation metric.\n",
    "- There are techniques to handle imbalanced datasets such as:\n",
    "    - Downsampling: changes the balance of the underlying dataset\n",
    "    - Weighting: Changes how our model handles certain classes\n",
    "    - Upsampling: duplicates examples from the minority class, and often involves augmentations to generate additional samples\n",
    "\n",
    "#### Choosing an evaluation metric\n",
    "\n",
    "- For imbalanced datasets its best to use metrics like precision, recall or F-measure to get a complete picture of how the model is performing\n",
    "    - Precision: measures the percentage of positive classifications that were correct our of all positive predictions\n",
    "    - Recall: Measures the proportion of actual positive examples that were identified correctly by the model\n",
    "    - Biggest difference between the two metrics above is the denominator used to calculate them. For precision its the total number of positive class predictions mae by our model. For recall, its the number of actual positive class examples present in the dataset.\n",
    "- Recall and Precision are Inversely correlated. The F-measure is a metric that ranges from 0 - 1 and takes both precision and recall into account and is calculated like so:\n",
    "\n",
    "```\n",
    "2 * (precision * recall / (precision * recall))\n",
    "```\n",
    "\n",
    "- Example\n",
    "    - 1000 samples 50 of which are fraud\n",
    "    - Model predicts 930 / 950 nonfraudulent examples correctly\n",
    "    - Model predicits 15 / 50 fraudulent examples correctly\n",
    "    - Precision is 15 / 35 = 42%\n",
    "    - Recall is 15 / 50 = 30%\n",
    "    - F-measure is 35%\n",
    "    - Accuracy here would be ~94%. These metrics do a better job at capturing the models inability to identidy fraudulent transacitons  \n",
    "- When calculating metrics on imbalanced datasets make sure the metrics are calculated on the \"unsampled\" dataset e.g. the original dataset with the original class balance and no upsampling or downsampling\n",
    "- Average Precision-Recall captures the model performance across all thresholds and has shown to be better than AUC. This is because average precision-recall places more emphasis on how many predictions the model got right out of the total number it assigned to the psoitive class. The gives more weight to the positive class. AUC on the other hand treats both classes equally.\n",
    "\n",
    "#### Downsampling\n",
    "\n",
    "- Solution that changes the underlying dataset rather than the model\n",
    "- Decrease the number of samples from the majority class used during training\n",
    "- Downsampling is usually combine with the ensemble pattern, following these steps:\n",
    "    1. Downsample the majority class and use all the instances of the minority class\n",
    "    2. Train a model and add it to the ensemble\n",
    "    3. Repeat\n",
    "- During inference take the median output of the ensemble models\n",
    "- Downsampling can also be applied to regression models but is more nuanced since the majority \"class\" in out data includes a range of values rather than a single label.\n",
    "\n",
    "#### Weighted Classes\n",
    "\n",
    "- Changes the weight the model gives to examples from each class\n",
    "- Tells the model to treat specific labels with more importance during training\n",
    "- We want the model to assign more weight to the minority class\n",
    "- Exactly how much weight should be assigned is up to the user\n",
    "- How is it typically done?\n",
    "    - e.g. minority class is 0.1% of dataset, reasonable to conclude model should treat this class with 1000x more weight than the majority class\n",
    "    - It's common to divide the weight value by 2 for each class so that the average weight of an example is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n",
      "0.5005005005005005\n"
     ]
    }
   ],
   "source": [
    "num_minority_samples = 1\n",
    "num_majority_samples = 999\n",
    "total_examples = num_minority_samples + num_majority_samples\n",
    "\n",
    "minority_class_weight = 1 / (num_minority_samples / total_examples) / 2\n",
    "majority_class_weight = 1 / (num_majority_samples / total_examples) / 2\n",
    "\n",
    "print(minority_class_weight)\n",
    "print(majority_class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would then pass these weight to model during training\n",
    "\n",
    "#### Upsampling\n",
    "\n",
    "- Overrepresents the minority class by replicating examples and/or generating synthetic examples\n",
    "- Often done in combination with downsampling of the majority class\n",
    "- The approach of combining upsampling and downsampling is referred to as \"synthetic minority over-sampling technique (SMOTE)\n",
    "    - SMOTE provides an algorithm that constructs synthetic examples by analysing the feature space of minority class examples to generate similar examples within the same space using a nearest neighor approach\n",
    "    \n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Reframing and Cascade\n",
    "\n",
    "- Can turn a classification into a regression problem or vice versa\n",
    "    e.g. we have a regression problem where the majority of our training data falls withing a certain range, with a few outliers. Assuming we care about predicting the outliers, we could convert this into a classificatio problem by bucketing the majority of the data in one bucket and the outliers in another.\n",
    "- Another approach is the cascade pattern, training three separate regression models for each class. Then, we can use out multidesign pattern solution by passing out inital classification model an example and using the result of that classification to decide which regression model to send the examples to for numeric purposes.\n",
    "\n",
    "#### Anomaly Detection\n",
    "\n",
    "- There are two approaches to handling regression models for imbalanced datasets:\n",
    "    - Use the model's error on a prediction as a signal\n",
    "    - Cluster incoming data and compare the distance of each new data point to existing clusters\n",
    "    \n",
    "- Example, let's say we're training a model on temperature sesnor data to predict future temps\n",
    "    - Using error as a signal, after training the model, we could then compare the model's predicted value with the actual value for the current point in time. If there is a significant difference between the predicted and actual current value, we could flag the incoming data point as an anomaly. This requires, a model trained with good accuracy on enough historical data to rely on its quality for future predictions.\n",
    "    - In the clustering scenario, we first build a model with a clustering algorithm. Once we have our clusters, we can then generate predictions on new data and look at that predictions distance from existing clusters. If the distance is high, we can flag the data point as an anomaly. We could then use high-distance values to conclude that this data point might be an anomaly. This approach is especially useful if we dont' know the labels for out data in advance. Once we generate cluster predictions on enough examples we, could then build a supervised learning model using the predicted cluster labels.\n",
    "    \n",
    "#### Number of Minority Class Examples Available\n",
    "\n",
    "- If you have only a few hundred examples of the minority class you may want to avoid downsampling the majority class to address the imbalance\n",
    "- A natural effect of downsampling to a subset of the majority class is losing some information stored in those examples\n",
    "\n",
    "#### Combining Different Techniques\n",
    "\n",
    "- Downsampling and class weights can be combined for optimal results\n",
    "    - First downsample to out desired balance\n",
    "    - For the re-balanced dataset calculat the class weights\n",
    "    - Pass weights to the model\n",
    "- Downsampling can be combined with the ensemble pattern\n",
    "    - Use different subsets of the majority class to train multiple models and then ensemble these models\n",
    "    - Example: 100 minority class examples and 1,000 majority class examples\n",
    "        - Split the majority class into groups of 10 with 100 in each group\n",
    "        - Train 10 classifiers, each with the same 100 minority class and 100 different majority examples.\n",
    "        - Could apply bagging here too, on the majority class\n",
    "- Can adjust the threshold for our classifier to optimise for precision or recall\n",
    "    - If we care more that our model is correct when it makes a positive prediction optimise for precision - reduces false positives\n",
    "    - If you want to avoid missing a potential positive class you should optimise for recall - reduces false negatives\n",
    "    \n",
    "#### Choosing a Model Architecture\n",
    "- Tabular data, [decision trees](https://ieeexplore.ieee.org/document/7424479) have been shown to work well\n",
    "- Tree based methods also work well small and imbalanced datasets \n",
    "- Could use LSTMs for detecting anomalies in time-series data\n",
    "- Deep learning with image data:\n",
    "    - Downsampling, weighted classes, upsampling, or a combination of techniques\n",
    "- For text:\n",
    "    - More difficult to generate synthetic data\n",
    "    - Rely on downsampling and weighted classes\n",
    "\n",
    "#### Importance of Explainability\n",
    "\n",
    "- Importance to understand how model is making predictions\n",
    "- Can help verify that the model is picking up on the correct signals to makes it predictions and help explain the model's behaviour to end users\n",
    "- Some tools:\n",
    "    - SHAP\n",
    "    - [What-If](https://oreil.ly/Vf3D-)\n",
    "- Model explanations can take many forms, one of which is called `attribution values`\n",
    "- The attribution values tell us how much ech feature in our model influenced the model's prediciton\n",
    "    - +ve attribution for a feature pushed the models prediction up\n",
    "    - -ve attribution for a feature pushed the models prediction down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
