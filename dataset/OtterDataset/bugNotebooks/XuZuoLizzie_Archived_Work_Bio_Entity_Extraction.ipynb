{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XuZuoLizzie/Archived_Work/blob/main/Bio_Entity_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVFxVxa091vI"
      },
      "source": [
        "# Extract Biomedical Entites from Literaure"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a demo on how we can use data from Europe PMC Annotations to train a pipeline that extracts Cell entities from full-text articles.\n",
        "\n",
        "Please note that this notebook only demonstrates the process from data preparation to model training. To further improve the NER model trained in the notebook, we need to process more data and allocate more computational resources."
      ],
      "metadata": {
        "id": "rXfpy7p54Ecm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUaeY-iN99RV"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdnONInq0GJL",
        "outputId": "d7e9aa45-b713-4981-e947-6896b025134d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pubmed_parser\n",
            "  Downloading pubmed_parser-0.3.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pubmed_parser) (4.9.4)\n",
            "Collecting unidecode (from pubmed_parser)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pubmed_parser) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pubmed_parser) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pubmed_parser) (1.25.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pubmed_parser) (7.4.4)\n",
            "Collecting pytest-cov (from pubmed_parser)\n",
            "  Downloading pytest_cov-4.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pubmed_parser) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pubmed_parser) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pubmed_parser) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pubmed_parser) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pubmed_parser) (2.0.1)\n",
            "Collecting coverage[toml]>=5.2.1 (from pytest-cov->pubmed_parser)\n",
            "  Downloading coverage-7.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.1/234.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pubmed_parser) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pubmed_parser) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pubmed_parser) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pubmed_parser) (2024.2.2)\n",
            "Building wheels for collected packages: pubmed_parser\n",
            "  Building wheel for pubmed_parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pubmed_parser: filename=pubmed_parser-0.3.1-py3-none-any.whl size=18495 sha256=63f9d9bae3e73b8811caeb4ba890f67ec58f86d7edd7bb3aaaf651f37d794a21\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/d4/ed/dae73ff36a1adbc2d306265ff1b262692e0a84b78f56597d8c\n",
            "Successfully built pubmed_parser\n",
            "Installing collected packages: unidecode, coverage, pytest-cov, pubmed_parser\n",
            "Successfully installed coverage-7.4.3 pubmed_parser-0.3.1 pytest-cov-4.1.0 unidecode-1.3.8\n",
            "Collecting scispacy\n",
            "  Downloading scispacy-0.5.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy<3.7.0,>=3.6.0 (from scispacy)\n",
            "  Downloading spacy-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.11 (from scispacy)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scispacy) (2.31.0)\n",
            "Collecting conllu (from scispacy)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.25.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.3.2)\n",
            "Collecting nmslib>=1.7.3.6 (from scispacy)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.2.2)\n",
            "Collecting pysbd (from scispacy)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2 (from nmslib>=1.7.3.6->scispacy)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.8 (from spacy<3.7.0,>=3.6.0->scispacy)\n",
            "  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (0.9.0)\n",
            "Collecting pathy>=0.10.0 (from spacy<3.7.0,>=3.6.0->scispacy)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (4.66.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->scispacy) (3.3.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->scispacy)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->scispacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->scispacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->scispacy) (4.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->scispacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->scispacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->scispacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->scispacy) (2.1.5)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13578645 sha256=88e4f32b8516ad9661e49d7fb8e48e09f4e7fc0535c22ce86b8150d52f08fb53\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: scipy, pysbd, pybind11, pathlib-abc, conllu, pathy, nmslib, thinc, spacy, scispacy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.3\n",
            "    Uninstalling thinc-8.2.3:\n",
            "      Successfully uninstalled thinc-8.2.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.4\n",
            "    Uninstalling spacy-3.7.4:\n",
            "      Successfully uninstalled spacy-3.7.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed conllu-4.5.3 nmslib-2.1.1 pathlib-abc-0.1.1 pathy-0.11.0 pybind11-2.6.1 pysbd-0.3.4 scipy-1.10.1 scispacy-0.5.3 spacy-3.6.1 thinc-8.1.12\n"
          ]
        }
      ],
      "source": [
        "! pip install pubmed_parser\n",
        "! pip install scispacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czfIIuBn0YuA"
      },
      "source": [
        "### Load entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1esZ2WqD-RHa"
      },
      "source": [
        " Select and extract the biomedical entities from Annotations API. I selected 'Cell' entity in this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qECg4mcs9jMe"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fYxBsyplbjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7958f357-d0b0-4c6d-aad8-10ab73e8d8fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data folder created.\n"
          ]
        }
      ],
      "source": [
        "directory_path = \"/content/data\"\n",
        "if not os.path.exists(directory_path):\n",
        "   os.makedirs(directory_path)\n",
        "   print(\"Data folder created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzk8Rbte-dBN"
      },
      "outputs": [],
      "source": [
        "def make_api_calls_and_save(num_calls):\n",
        "    cursor_mark = 0\n",
        "    for i in range(num_calls):\n",
        "        url = f\"https://www.ebi.ac.uk/europepmc/annotations_api/annotationsBySectionAndOrType?type=Cell&filter=1&format=JSON&pageSize=8&cursorMark={cursor_mark}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the response is successful\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            # Update the cursor_mark for the next call, assuming the new cursor_mark is part of the response\n",
        "            cursor_mark = data.get('nextCursorMark', cursor_mark)\n",
        "\n",
        "            # Save the response data into a JSON file\n",
        "            file_name = f'cell_ann_{i}.json'\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            with open(file_path, 'w') as file:\n",
        "                json.dump(data, file, indent=4)\n",
        "            print(f\"Data saved to {file_name}\")\n",
        "        else:\n",
        "            print(f\"Failed to get data for call {i}: Status code {response.status_code}\")\n",
        "            break  # Stop making further calls if there's a failure\n",
        "\n",
        "        # Wait for 0.5 seconds before making the next call\n",
        "        time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXddCXUJl2yI",
        "outputId": "497b8ed8-3e16-4234-eee6-b0dcd628292f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to cell_ann_0.json\n",
            "Data saved to cell_ann_1.json\n"
          ]
        }
      ],
      "source": [
        "num_calls = 2 # Set number of calls, here I set to 2 and download 2 * 8 = 16 articles in total\n",
        "make_api_calls_and_save(num_calls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exA3MmYhBDXW"
      },
      "source": [
        "Load saved JSON response into a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p6QEGbnA0Aq"
      },
      "outputs": [],
      "source": [
        "# Function to process a single JSON file\n",
        "def process_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        articles_data = []\n",
        "        for article in data['articles']:\n",
        "            for annotation in article['annotations']:\n",
        "                prefix = annotation.get('prefix', '')\n",
        "                exact = annotation.get('exact', '')\n",
        "                postfix = annotation.get('postfix', '')\n",
        "                prefix_exact_postfix = f\"{prefix}{exact}{postfix}\"\n",
        "                articles_data.append({\n",
        "                    'pmcid': article.get('pmcid', None),\n",
        "                    'exact': exact,\n",
        "                    'type': annotation.get('type', None),\n",
        "                    'prefix_exact_postfix': prefix_exact_postfix\n",
        "                })\n",
        "        return articles_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "l4fjiMb82jqJ",
        "outputId": "6cc03011-5850-4bfd-85c9-4059ce325f97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            pmcid               exact  type  \\\n",
              "0      PMC6833180          macrophage  Cell   \n",
              "1      PMC6833180         macrophages  Cell   \n",
              "2      PMC6833180         macrophages  Cell   \n",
              "3      PMC6833180                   T  Cell   \n",
              "4      PMC6833180          lymphocyte  Cell   \n",
              "...           ...                 ...   ...   \n",
              "20573  PMC6919427  hematopoietic stem  Cell   \n",
              "20574  PMC6919427              T cell  Cell   \n",
              "20575  PMC6919427              T cell  Cell   \n",
              "20576  PMC6919427              T cell  Cell   \n",
              "20577  PMC6919427             T cells  Cell   \n",
              "\n",
              "                                    prefix_exact_postfix  \n",
              "0      s a minimally toxic macrophage repolarizing ag...  \n",
              "1      bitory impact of M2 macrophages on the activit...  \n",
              "2      e repolarization of macrophages by RRx-001 may...  \n",
              "3              A-4 (anti cytotoxic T-lymphocyte-associat  \n",
              "4      4 (anti cytotoxic T-lymphocyte-associated prot...  \n",
              "...                                                  ...  \n",
              "20573  kground:\\nAutologous hematopoietic stem and pr...  \n",
              "20574     CCR5-CD4+, and CD8+ T cell counts and SHIV pla  \n",
              "20575     odel simulations of T cell and SHIV dynamics a  \n",
              "20576     driver of CD4+CCR5− T cell growth, and rapid l  \n",
              "20577   letion of CCR5+CD4+ T cells.\\nFurther, model pro  \n",
              "\n",
              "[20578 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dec6920b-e7c3-40f1-8390-7879be734291\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pmcid</th>\n",
              "      <th>exact</th>\n",
              "      <th>type</th>\n",
              "      <th>prefix_exact_postfix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PMC6833180</td>\n",
              "      <td>macrophage</td>\n",
              "      <td>Cell</td>\n",
              "      <td>s a minimally toxic macrophage repolarizing ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PMC6833180</td>\n",
              "      <td>macrophages</td>\n",
              "      <td>Cell</td>\n",
              "      <td>bitory impact of M2 macrophages on the activit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PMC6833180</td>\n",
              "      <td>macrophages</td>\n",
              "      <td>Cell</td>\n",
              "      <td>e repolarization of macrophages by RRx-001 may...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PMC6833180</td>\n",
              "      <td>T</td>\n",
              "      <td>Cell</td>\n",
              "      <td>A-4 (anti cytotoxic T-lymphocyte-associat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PMC6833180</td>\n",
              "      <td>lymphocyte</td>\n",
              "      <td>Cell</td>\n",
              "      <td>4 (anti cytotoxic T-lymphocyte-associated prot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20573</th>\n",
              "      <td>PMC6919427</td>\n",
              "      <td>hematopoietic stem</td>\n",
              "      <td>Cell</td>\n",
              "      <td>kground:\\nAutologous hematopoietic stem and pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20574</th>\n",
              "      <td>PMC6919427</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>CCR5-CD4+, and CD8+ T cell counts and SHIV pla</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20575</th>\n",
              "      <td>PMC6919427</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>odel simulations of T cell and SHIV dynamics a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20576</th>\n",
              "      <td>PMC6919427</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>driver of CD4+CCR5− T cell growth, and rapid l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20577</th>\n",
              "      <td>PMC6919427</td>\n",
              "      <td>T cells</td>\n",
              "      <td>Cell</td>\n",
              "      <td>letion of CCR5+CD4+ T cells.\\nFurther, model pro</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20578 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dec6920b-e7c3-40f1-8390-7879be734291')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dec6920b-e7c3-40f1-8390-7879be734291 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dec6920b-e7c3-40f1-8390-7879be734291');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d049a149-7fa2-4155-accd-497941626d4a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d049a149-7fa2-4155-accd-497941626d4a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d049a149-7fa2-4155-accd-497941626d4a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e2643425-bbca-4731-b609-64b3685f9fc7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e2643425-bbca-4731-b609-64b3685f9fc7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20578,\n  \"fields\": [\n    {\n      \"column\": \"pmcid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"PMC6833180\",\n          \"PMC6833189\",\n          \"PMC6821132\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1240,\n        \"samples\": [\n          \"Midbrain\",\n          \"Leukocytes\",\n          \"neuronal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Cell\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prefix_exact_postfix\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19152,\n        \"samples\": [\n          \", body weight loss, inflammatory cell extravasation, neur\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "extracted_data = []\n",
        "\n",
        "# Process each JSON file in the directory\n",
        "for filename in os.listdir(directory_path):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        extracted_data.extend(process_json_file(file_path))\n",
        "\n",
        "df = pd.DataFrame(extracted_data)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN1bs2Gu0nAV"
      },
      "source": [
        "### Load articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGsMCiYz5cEJ"
      },
      "source": [
        "Download full-text articles from Articles RESTful API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCZ7khJf1rpT"
      },
      "outputs": [],
      "source": [
        "from lxml import etree\n",
        "import pubmed_parser as pp\n",
        "import scispacy\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JrsPz7T7lMg",
        "outputId": "8a1651c5-4263-44be-ee25-7b61481b4963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PMC6833180', 'PMC6833189', 'PMC6854655', 'PMC6763540', 'PMC6501469', 'PMC6821132', 'PMC6802965', 'PMC6937151', 'PMC6584520', 'PMC6504235']\n",
            "16 articles in total.\n"
          ]
        }
      ],
      "source": [
        "pmcid_list = df['pmcid'].unique().tolist()\n",
        "print(pmcid_list[:10])\n",
        "print(\"%d articles in total.\" % len(pmcid_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZSM6Y9p5a2F"
      },
      "outputs": [],
      "source": [
        "def download_pmc_article(pmcid):\n",
        "    url = f'https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML'\n",
        "\n",
        "    # Check if the XML file already exists\n",
        "    file_path = os.path.join(directory_path, f'{pmcid}.xml')\n",
        "    if os.path.exists(file_path):\n",
        "        print(f'{pmcid} already exists, skipping download.')\n",
        "        return\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Save the article as XML\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f'{pmcid} downloaded successfully.')\n",
        "    else:\n",
        "        print(f'Error downloading {pmcid}. Status code: {response.status_code}')\n",
        "\n",
        "    time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FUb7Svj9Bt7",
        "outputId": "6c0d6009-b7b4-4010-ae8f-263d6d54c5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PMC6833180 downloaded successfully.\n",
            "PMC6833189 downloaded successfully.\n",
            "PMC6854655 downloaded successfully.\n",
            "PMC6763540 downloaded successfully.\n",
            "PMC6501469 downloaded successfully.\n",
            "PMC6821132 downloaded successfully.\n",
            "PMC6802965 downloaded successfully.\n",
            "PMC6937151 downloaded successfully.\n",
            "PMC6584520 downloaded successfully.\n",
            "PMC6504235 downloaded successfully.\n",
            "PMC6636997 downloaded successfully.\n",
            "PMC6726422 downloaded successfully.\n",
            "PMC6851788 downloaded successfully.\n",
            "PMC6636905 downloaded successfully.\n",
            "PMC6592685 downloaded successfully.\n",
            "PMC6919427 downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "for pmcid in pmcid_list:\n",
        "    download_pmc_article(pmcid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5nsw-XP3ndh"
      },
      "source": [
        "Parse the title, abstract and main text from XML files using Pubmed Parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S10TkGpN-6Z1"
      },
      "outputs": [],
      "source": [
        "def parse_full_text(pmc_file_path):\n",
        "    para_dict = pp.parse_pubmed_paragraph(pmc_file_path, all_paragraph=False)\n",
        "    main_text_list = []\n",
        "\n",
        "    for paragraph in para_dict:\n",
        "        cleaned_paragraph = paragraph['text'].strip()\n",
        "        main_text_list.append(cleaned_paragraph)\n",
        "\n",
        "    main_text = '\\n'.join(main_text_list)\n",
        "    info_dict = pp.parse_pubmed_xml(pmc_file_path)\n",
        "    title = info_dict['full_title'].strip()\n",
        "    abstract = info_dict['abstract'].strip()\n",
        "    full_text = title + '\\n\\n' + abstract + '\\n\\n' + main_text\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PILipU31Uv"
      },
      "source": [
        "Segment full text into sentences using Scispacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WloisD720ar",
        "outputId": "efa9c121-e0f3-4e0d-ccd3-711964e47878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.7.0,>=3.6.1 in /usr/local/lib/python3.10/dist-packages (from en-core-sci-sm==0.5.3) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.1->en-core-sci-sm==0.5.3) (2.1.5)\n",
            "Building wheels for collected packages: en-core-sci-sm\n",
            "  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.3-py3-none-any.whl size=14776165 sha256=e8328228b769ba7a07a7580b262041eeb5e4b6112917573b034122e52ac260ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/27/08/5863b9fc5a65254f943eff433dd1e0fafc7ac4595be28d789d\n",
            "Successfully built en-core-sci-sm\n",
            "Installing collected packages: en-core-sci-sm\n",
            "Successfully installed en-core-sci-sm-0.5.3\n"
          ]
        }
      ],
      "source": [
        "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAZlwZ7eC3Xz",
        "outputId": "337c3520-37ef-4455-ec94-aa392666d284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
            "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed PMC6833189.xml\n",
            "Processed PMC6726422.xml\n",
            "Processed PMC6501469.xml\n",
            "Processed PMC6802965.xml\n",
            "Processed PMC6636905.xml\n",
            "Processed PMC6584520.xml\n",
            "Processed PMC6851788.xml\n",
            "Processed PMC6854655.xml\n",
            "Processed PMC6636997.xml\n",
            "Processed PMC6919427.xml\n",
            "Processed PMC6937151.xml\n",
            "Processed PMC6592685.xml\n",
            "Processed PMC6821132.xml\n",
            "Processed PMC6833180.xml\n",
            "Processed PMC6504235.xml\n",
            "Processed PMC6763540.xml\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_sci_sm\")\n",
        "\n",
        "full_text_dict = {}\n",
        "for file_name in os.listdir(directory_path):\n",
        "    if file_name.endswith('.xml'):\n",
        "        pmcid = file_name.replace('.xml', '')\n",
        "        full_text = parse_full_text(os.path.join(directory_path, file_name))\n",
        "        doc = nlp(full_text)\n",
        "        full_text_dict[pmcid] = [sentence.text.strip() for sentence in doc.sents]\n",
        "        print(f'Processed {file_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LAwSJkvB7NKc",
        "outputId": "4107256e-9c50-4fc1-a7dd-986e745122f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           pmcid                                          sentences\n",
              "0     PMC6833189  34th Annual Meeting & Pre-Conference Programs ...\n",
              "1     PMC6726422  Astrocyte morphogenesis is dependent on BDNF s...\n",
              "2     PMC6726422  Herein, we demonstrate astrocytes express high...\n",
              "3     PMC6726422  Using a novel culture paradigm, we show that a...\n",
              "4     PMC6726422  Deletion of TrkB.T1, globally and astrocyte-sp...\n",
              "...          ...                                                ...\n",
              "5188  PMC6504235  Paired-end (75 × 75 bp) sequencing was perform...\n",
              "5189  PMC6504235  The number of biological replicates used were:...\n",
              "5190  PMC6504235  Each sample was sequenced to a depth of approx...\n",
              "5191  PMC6504235  Sequencing data have been deposited in GEO und...\n",
              "5192  PMC6763540                                 ESP Abstracts 2013\n",
              "\n",
              "[5193 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c8a074a-9d01-472a-bf76-5e815d521977\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pmcid</th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PMC6833189</td>\n",
              "      <td>34th Annual Meeting &amp; Pre-Conference Programs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PMC6726422</td>\n",
              "      <td>Astrocyte morphogenesis is dependent on BDNF s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PMC6726422</td>\n",
              "      <td>Herein, we demonstrate astrocytes express high...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PMC6726422</td>\n",
              "      <td>Using a novel culture paradigm, we show that a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PMC6726422</td>\n",
              "      <td>Deletion of TrkB.T1, globally and astrocyte-sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5188</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>Paired-end (75 × 75 bp) sequencing was perform...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5189</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>The number of biological replicates used were:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5190</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>Each sample was sequenced to a depth of approx...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>Sequencing data have been deposited in GEO und...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5192</th>\n",
              "      <td>PMC6763540</td>\n",
              "      <td>ESP Abstracts 2013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5193 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c8a074a-9d01-472a-bf76-5e815d521977')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c8a074a-9d01-472a-bf76-5e815d521977 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c8a074a-9d01-472a-bf76-5e815d521977');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cc90dfee-01d9-4ad6-934e-fb79423ee46c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc90dfee-01d9-4ad6-934e-fb79423ee46c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cc90dfee-01d9-4ad6-934e-fb79423ee46c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_570adedb-e83d-46b7-a448-5afea174d7fa\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('article_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_570adedb-e83d-46b7-a448-5afea174d7fa button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('article_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "article_df",
              "summary": "{\n  \"name\": \"article_df\",\n  \"rows\": 5193,\n  \"fields\": [\n    {\n      \"column\": \"pmcid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"PMC6833189\",\n          \"PMC6726422\",\n          \"PMC6584520\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5139,\n        \"samples\": [\n          \"iGB-hi cells, rather than iGB-lo cells, tended\\u00a0to\\u00a0dominate the generation of CD80hi iMB cells,\\u00a0even when we used day 1 iGB cells (Figure 3\\u2014figure supplement 1g).\",\n          \"Dead cells, detected by using propidium iodide or Fixable Viability Dye (eBioscience), were gated out in all FCM experiments.\",\n          \"Plots represent individual data points, median and interquartile range across all subjects.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Convert the segmented full-text to a DataFrame\n",
        "article_df = pd.DataFrame(list(full_text_dict.items()), columns=['pmcid', 'sentences'])\n",
        "\n",
        "# Expand the 'sentences' column vertically\n",
        "article_df = article_df.explode('sentences').reset_index(drop=True)\n",
        "display(article_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBzUA1nQ7cvO"
      },
      "source": [
        "### Extract the whole sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "D5ONlUbK8MFC",
        "outputId": "b65ed79c-4211-427b-dd05-e6841f91cbfa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           pmcid                   exact  type  \\\n",
              "0     PMC6854655                neuronal  Cell   \n",
              "1     PMC6854655                  neuron  Cell   \n",
              "2     PMC6854655                      ON  Cell   \n",
              "3     PMC6854655  retinal ganglion cells  Cell   \n",
              "4     PMC6854655                    RGCs  Cell   \n",
              "...          ...                     ...   ...   \n",
              "3554  PMC6592685                  T cell  Cell   \n",
              "3555  PMC6592685                  T cell  Cell   \n",
              "3556  PMC6592685                monocyte  Cell   \n",
              "3557  PMC6592685                  T cell  Cell   \n",
              "3558  PMC6592685                monocyte  Cell   \n",
              "\n",
              "                                   prefix_exact_postfix  \\\n",
              "0      s absorbed into the neuronal mass model that is    \n",
              "1        tion tuning of a V1 neuron is restricted by th   \n",
              "2            ocal arrangement of ON and OFF retinal gan   \n",
              "3     ement of ON and OFF retinal ganglion cells (RG...   \n",
              "4          nal ganglion cells (RGCs) [2, 3], we suggest   \n",
              "...                                                 ...   \n",
              "3554     y, both CD4 and CD8 T cell subsets could be fo   \n",
              "3555     here was no obvious T cell or monocyte marker    \n",
              "3556   o obvious T cell or monocyte marker that could d   \n",
              "3557     at the frequency of T cell:monocyte complexes    \n",
              "3558   frequency of T cell:monocyte complexes was not a   \n",
              "\n",
              "                                               sentence  \n",
              "0     The homogeneous local connectivity is absorbed...  \n",
              "1     By expanding the statistical wiring model prop...  \n",
              "2     By expanding the statistical wiring model prop...  \n",
              "3     By expanding the statistical wiring model prop...  \n",
              "4     By expanding the statistical wiring model prop...  \n",
              "...                                                 ...  \n",
              "3554  Additionally, both CD4 and CD8 T cell subsets ...  \n",
              "3555  Thus, there was no obvious T cell or monocyte ...  \n",
              "3556  Thus, there was no obvious T cell or monocyte ...  \n",
              "3557  We found that the frequency of T cell:monocyte...  \n",
              "3558  We found that the frequency of T cell:monocyte...  \n",
              "\n",
              "[3559 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95c4faac-4585-47e5-b652-81b85f33f87a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pmcid</th>\n",
              "      <th>exact</th>\n",
              "      <th>type</th>\n",
              "      <th>prefix_exact_postfix</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PMC6854655</td>\n",
              "      <td>neuronal</td>\n",
              "      <td>Cell</td>\n",
              "      <td>s absorbed into the neuronal mass model that is</td>\n",
              "      <td>The homogeneous local connectivity is absorbed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PMC6854655</td>\n",
              "      <td>neuron</td>\n",
              "      <td>Cell</td>\n",
              "      <td>tion tuning of a V1 neuron is restricted by th</td>\n",
              "      <td>By expanding the statistical wiring model prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PMC6854655</td>\n",
              "      <td>ON</td>\n",
              "      <td>Cell</td>\n",
              "      <td>ocal arrangement of ON and OFF retinal gan</td>\n",
              "      <td>By expanding the statistical wiring model prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PMC6854655</td>\n",
              "      <td>retinal ganglion cells</td>\n",
              "      <td>Cell</td>\n",
              "      <td>ement of ON and OFF retinal ganglion cells (RG...</td>\n",
              "      <td>By expanding the statistical wiring model prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PMC6854655</td>\n",
              "      <td>RGCs</td>\n",
              "      <td>Cell</td>\n",
              "      <td>nal ganglion cells (RGCs) [2, 3], we suggest</td>\n",
              "      <td>By expanding the statistical wiring model prop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3554</th>\n",
              "      <td>PMC6592685</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>y, both CD4 and CD8 T cell subsets could be fo</td>\n",
              "      <td>Additionally, both CD4 and CD8 T cell subsets ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3555</th>\n",
              "      <td>PMC6592685</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>here was no obvious T cell or monocyte marker</td>\n",
              "      <td>Thus, there was no obvious T cell or monocyte ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3556</th>\n",
              "      <td>PMC6592685</td>\n",
              "      <td>monocyte</td>\n",
              "      <td>Cell</td>\n",
              "      <td>o obvious T cell or monocyte marker that could d</td>\n",
              "      <td>Thus, there was no obvious T cell or monocyte ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3557</th>\n",
              "      <td>PMC6592685</td>\n",
              "      <td>T cell</td>\n",
              "      <td>Cell</td>\n",
              "      <td>at the frequency of T cell:monocyte complexes</td>\n",
              "      <td>We found that the frequency of T cell:monocyte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3558</th>\n",
              "      <td>PMC6592685</td>\n",
              "      <td>monocyte</td>\n",
              "      <td>Cell</td>\n",
              "      <td>frequency of T cell:monocyte complexes was not a</td>\n",
              "      <td>We found that the frequency of T cell:monocyte...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3559 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95c4faac-4585-47e5-b652-81b85f33f87a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95c4faac-4585-47e5-b652-81b85f33f87a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95c4faac-4585-47e5-b652-81b85f33f87a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c9931902-9933-4539-88ad-3a11bd1d0462\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9931902-9933-4539-88ad-3a11bd1d0462')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c9931902-9933-4539-88ad-3a11bd1d0462 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3f50f7f2-836f-47db-b2a6-4997f09a65d4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ner_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3f50f7f2-836f-47db-b2a6-4997f09a65d4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ner_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ner_df",
              "summary": "{\n  \"name\": \"ner_df\",\n  \"rows\": 3559,\n  \"fields\": [\n    {\n      \"column\": \"pmcid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"PMC6636905\",\n          \"PMC6821132\",\n          \"PMC6636997\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"grid\",\n          \"cholinergic neurons\",\n          \"erythrocyte\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Cell\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prefix_exact_postfix\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3219,\n        \"samples\": [\n          \"learn whether other astrocyte populations might a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1958,\n        \"samples\": [\n          \"A recent study in which human utricle tissue was transduced with an Atoh1 adenovirus also reported phenotypes consistent with incomplete hair cell differentiation, including an absence of stereocilia and actin bundling proteins such as espin, and an absence of a cuticular plate and ribbon synapses (Taylor et al., 2018).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Merge the DataFrames on 'pmcid'\n",
        "merged_df = pd.merge(df, article_df, on='pmcid', how='inner')\n",
        "\n",
        "# Check if 'prefix_exact_postfix' is part of 'sentence' for each row\n",
        "merged_df['is_included'] = merged_df.apply(lambda row: row['prefix_exact_postfix'] in row['sentences'], axis=1)\n",
        "included_df = merged_df[merged_df['is_included'] == True]\n",
        "\n",
        "ner_df = included_df[['pmcid', 'exact', 'type', 'prefix_exact_postfix', 'sentences']].copy().reset_index(drop=True)\n",
        "ner_df = ner_df.rename(columns={'sentences': 'sentence'})\n",
        "display(ner_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9PrdL18MFpE"
      },
      "source": [
        "## Train an NER Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzcbx8bcMLRi"
      },
      "source": [
        "### Preprocess Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7qIZnRajsKq"
      },
      "source": [
        "Tokenize sentences and convert the dataset into BIO (Begin, Inside, Outside) format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbya-yT_Hrtc"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "vDiL9a7Jjrbh",
        "outputId": "b0bb76af-a748-4cf5-8145-3caf57b714bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           pmcid                                           sentence  \\\n",
              "0     PMC6504235  (C) Heat map showing the relative enrichment o...   \n",
              "1     PMC6504235  112 of these genes appear to be generic marker...   \n",
              "2     PMC6504235  24 hr of exposure to gentamicin led to signifi...   \n",
              "3     PMC6504235  70 of the top 100 enriched utricle hair cell g...   \n",
              "4     PMC6504235  A number of transcription factors have been pr...   \n",
              "...          ...                                                ...   \n",
              "1953  PMC6937151  When we examined the proximal zebrafish intest...   \n",
              "1954  PMC6937151  Whereas HF feeding normally reduces the EEC mo...   \n",
              "1955  PMC6937151  Whether EECs adopt the same mechanisms as neur...   \n",
              "1956  PMC6937151  Wild-type adult EKW zebrafish were bred and cl...   \n",
              "1957  PMC6937151  Within the intestinal epithelium, EECs are sur...   \n",
              "\n",
              "                                                  exact  \\\n",
              "0     [hair, cells, supporting cells, cochlear hair ...   \n",
              "1           [hair cells, neonatal, cochlear hair cells]   \n",
              "2                                           [hair cell]   \n",
              "3                               [hair cell, hair cells]   \n",
              "4                                           [hair cell]   \n",
              "...                                                 ...   \n",
              "1953                                             [EECs]   \n",
              "1954                                              [EEC]   \n",
              "1955                                          [neurons]   \n",
              "1956                                             [eggs]   \n",
              "1957                                [EECs, enterocytes]   \n",
              "\n",
              "                                                   type  \\\n",
              "0     [Cell, Cell, Cell, Cell, Cell, Cell, Cell, Cel...   \n",
              "1                                    [Cell, Cell, Cell]   \n",
              "2                                                [Cell]   \n",
              "3                                          [Cell, Cell]   \n",
              "4                                                [Cell]   \n",
              "...                                                 ...   \n",
              "1953                                             [Cell]   \n",
              "1954                                             [Cell]   \n",
              "1955                                             [Cell]   \n",
              "1956                                             [Cell]   \n",
              "1957                                       [Cell, Cell]   \n",
              "\n",
              "                                   prefix_exact_postfix  \n",
              "0     [eq peaks in utricle hair cells, utricle supp,...  \n",
              "1     [ generic markers of hair cells, as they are a...  \n",
              "2     [ led to significant hair cell loss in the utr...  \n",
              "3     [00 enriched utricle hair cell genes have been...  \n",
              "4     [e with Atoh1 during hair cell induction, such...  \n",
              "...                                                 ...  \n",
              "1953     [iscovered that most EECs had adopted a close]  \n",
              "1954      [ormally reduces the EEC morphology score, t]  \n",
              "1955  [ same mechanisms as neurons to prune their cell]  \n",
              "1956     [red and clutches of eggs from three distinct]  \n",
              "1957  [estinal epithelium, EECs are surrounded by a,...  \n",
              "\n",
              "[1958 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8f3e8c1-88aa-40f7-ac62-9c8fc2f57f6b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pmcid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>exact</th>\n",
              "      <th>type</th>\n",
              "      <th>prefix_exact_postfix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>(C) Heat map showing the relative enrichment o...</td>\n",
              "      <td>[hair, cells, supporting cells, cochlear hair ...</td>\n",
              "      <td>[Cell, Cell, Cell, Cell, Cell, Cell, Cell, Cel...</td>\n",
              "      <td>[eq peaks in utricle hair cells, utricle supp,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>112 of these genes appear to be generic marker...</td>\n",
              "      <td>[hair cells, neonatal, cochlear hair cells]</td>\n",
              "      <td>[Cell, Cell, Cell]</td>\n",
              "      <td>[ generic markers of hair cells, as they are a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>24 hr of exposure to gentamicin led to signifi...</td>\n",
              "      <td>[hair cell]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ led to significant hair cell loss in the utr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>70 of the top 100 enriched utricle hair cell g...</td>\n",
              "      <td>[hair cell, hair cells]</td>\n",
              "      <td>[Cell, Cell]</td>\n",
              "      <td>[00 enriched utricle hair cell genes have been...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>A number of transcription factors have been pr...</td>\n",
              "      <td>[hair cell]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[e with Atoh1 during hair cell induction, such...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1953</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>When we examined the proximal zebrafish intest...</td>\n",
              "      <td>[EECs]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[iscovered that most EECs had adopted a close]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1954</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Whereas HF feeding normally reduces the EEC mo...</td>\n",
              "      <td>[EEC]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ormally reduces the EEC morphology score, t]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1955</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Whether EECs adopt the same mechanisms as neur...</td>\n",
              "      <td>[neurons]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ same mechanisms as neurons to prune their cell]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1956</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Wild-type adult EKW zebrafish were bred and cl...</td>\n",
              "      <td>[eggs]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[red and clutches of eggs from three distinct]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1957</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Within the intestinal epithelium, EECs are sur...</td>\n",
              "      <td>[EECs, enterocytes]</td>\n",
              "      <td>[Cell, Cell]</td>\n",
              "      <td>[estinal epithelium, EECs are surrounded by a,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1958 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8f3e8c1-88aa-40f7-ac62-9c8fc2f57f6b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8f3e8c1-88aa-40f7-ac62-9c8fc2f57f6b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8f3e8c1-88aa-40f7-ac62-9c8fc2f57f6b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9c15b14e-3389-4cd0-8e20-2d8353599371\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9c15b14e-3389-4cd0-8e20-2d8353599371')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9c15b14e-3389-4cd0-8e20-2d8353599371 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c27b62b8-48ff-4a90-bf4e-7cc8008dad29\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('grouped_ner_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c27b62b8-48ff-4a90-bf4e-7cc8008dad29 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('grouped_ner_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "grouped_ner_df",
              "summary": "{\n  \"name\": \"grouped_ner_df\",\n  \"rows\": 1958,\n  \"fields\": [\n    {\n      \"column\": \"pmcid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"PMC6854655\",\n          \"PMC6592685\",\n          \"PMC6802965\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1958,\n        \"samples\": [\n          \"However, because we have yet to identify a phagocytosis pathway required for death, our data do not rule out a scenario in which microglia first kill astrocytes via a separate (non-apoptotic) pathway before immediately engulfing the doomed cells.\",\n          \"We found strong positive correlations between the capillary RBC flux and both Mean-PO2 and SO2 in the downstream (V1-V3) capillaries, but not in the upstream (A1-A3) capillaries (Figure 7f,g), suggesting that a positive correlation between the RBC flux and oxygenation may be gradually building up along the capillary paths.\",\n          \"By averaging over all the RBCs identified in each capillary and then across the 58 capillaries, we obtained the mean RBC longitudinal size (6.9\\u00a0\\u00b1\\u00a03.0 \\u00b5m; Mean\\u00a0\\u00b1STD).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prefix_exact_postfix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "grouped_ner_df = ner_df.groupby(['pmcid', 'sentence']).agg({\n",
        "    'exact': lambda x: list(x),\n",
        "    'type': lambda x: list(x),\n",
        "    'prefix_exact_postfix': lambda x: list(x)\n",
        "}).reset_index()\n",
        "\n",
        "display(grouped_ner_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1fWIsTmtUaS"
      },
      "outputs": [],
      "source": [
        "# Function to find start and end token index for exact entity matches\n",
        "def find_exact_entity_span(doc, entities):\n",
        "    exact_entity_spans = []\n",
        "    for ent in entities:\n",
        "        ent = ent\n",
        "        start_char = doc.text.find(ent)\n",
        "        if start_char != -1:\n",
        "            end_char = start_char + len(ent)\n",
        "            start_token = None\n",
        "            end_token = None\n",
        "            for token in doc:\n",
        "                # Identify the start token\n",
        "                if start_token is None and start_char <= token.idx:\n",
        "                    start_token = token.i\n",
        "                # Identify the end token\n",
        "                if end_token is None and end_char <= token.idx + len(token):\n",
        "                    end_token = token.i\n",
        "                    break\n",
        "            if start_token is not None and end_token is not None:\n",
        "                exact_entity_spans.append((start_token, end_token, ent))\n",
        "    return exact_entity_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJmK0ZbPtKAu"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_tag(row):\n",
        "    doc = nlp(row['sentence'])\n",
        "    exact_entity_spans = find_exact_entity_span(doc, row['exact'])\n",
        "    bio_tags = ['O'] * len(doc)\n",
        "\n",
        "    for start, end, ent in exact_entity_spans:\n",
        "        entity_type = row['type'][row['exact'].index(ent)]\n",
        "        if start is not None:\n",
        "            bio_tags[start] = f'B-{entity_type}'\n",
        "            for i in range(start + 1, end + 1):\n",
        "                bio_tags[i] = f'I-{entity_type}'\n",
        "\n",
        "    return [(token.text, tag) for token, tag in zip(doc, bio_tags)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "n32kXFH9pOKq",
        "outputId": "f7830c64-97f8-4de3-fa4b-f69ffc4a68d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           pmcid                                           sentence  \\\n",
              "0     PMC6504235  (C) Heat map showing the relative enrichment o...   \n",
              "1     PMC6504235  112 of these genes appear to be generic marker...   \n",
              "2     PMC6504235  24 hr of exposure to gentamicin led to signifi...   \n",
              "3     PMC6504235  70 of the top 100 enriched utricle hair cell g...   \n",
              "4     PMC6504235  A number of transcription factors have been pr...   \n",
              "...          ...                                                ...   \n",
              "1953  PMC6937151  When we examined the proximal zebrafish intest...   \n",
              "1954  PMC6937151  Whereas HF feeding normally reduces the EEC mo...   \n",
              "1955  PMC6937151  Whether EECs adopt the same mechanisms as neur...   \n",
              "1956  PMC6937151  Wild-type adult EKW zebrafish were bred and cl...   \n",
              "1957  PMC6937151  Within the intestinal epithelium, EECs are sur...   \n",
              "\n",
              "                                                  exact  \\\n",
              "0     [hair, cells, supporting cells, cochlear hair ...   \n",
              "1           [hair cells, neonatal, cochlear hair cells]   \n",
              "2                                           [hair cell]   \n",
              "3                               [hair cell, hair cells]   \n",
              "4                                           [hair cell]   \n",
              "...                                                 ...   \n",
              "1953                                             [EECs]   \n",
              "1954                                              [EEC]   \n",
              "1955                                          [neurons]   \n",
              "1956                                             [eggs]   \n",
              "1957                                [EECs, enterocytes]   \n",
              "\n",
              "                                                   type  \\\n",
              "0     [Cell, Cell, Cell, Cell, Cell, Cell, Cell, Cel...   \n",
              "1                                    [Cell, Cell, Cell]   \n",
              "2                                                [Cell]   \n",
              "3                                          [Cell, Cell]   \n",
              "4                                                [Cell]   \n",
              "...                                                 ...   \n",
              "1953                                             [Cell]   \n",
              "1954                                             [Cell]   \n",
              "1955                                             [Cell]   \n",
              "1956                                             [Cell]   \n",
              "1957                                       [Cell, Cell]   \n",
              "\n",
              "                                   prefix_exact_postfix  \\\n",
              "0     [eq peaks in utricle hair cells, utricle supp,...   \n",
              "1     [ generic markers of hair cells, as they are a...   \n",
              "2     [ led to significant hair cell loss in the utr...   \n",
              "3     [00 enriched utricle hair cell genes have been...   \n",
              "4     [e with Atoh1 during hair cell induction, such...   \n",
              "...                                                 ...   \n",
              "1953     [iscovered that most EECs had adopted a close]   \n",
              "1954      [ormally reduces the EEC morphology score, t]   \n",
              "1955  [ same mechanisms as neurons to prune their cell]   \n",
              "1956     [red and clutches of eggs from three distinct]   \n",
              "1957  [estinal epithelium, EECs are surrounded by a,...   \n",
              "\n",
              "                                               bio_tags  \n",
              "0     [((, O), (C, O), (), O), (Heat, O), (map, O), ...  \n",
              "1     [(112, O), (of, O), (these, O), (genes, O), (a...  \n",
              "2     [(24, O), (hr, O), (of, O), (exposure, O), (to...  \n",
              "3     [(70, O), (of, O), (the, O), (top, O), (100, O...  \n",
              "4     [(A, O), (number, O), (of, O), (transcription,...  \n",
              "...                                                 ...  \n",
              "1953  [(When, O), (we, O), (examined, O), (the, O), ...  \n",
              "1954  [(Whereas, O), (HF, O), (feeding, O), (normall...  \n",
              "1955  [(Whether, O), (EECs, O), (adopt, O), (the, O)...  \n",
              "1956  [(Wild-type, O), (adult, O), (EKW, O), (zebraf...  \n",
              "1957  [(Within, O), (the, O), (intestinal, O), (epit...  \n",
              "\n",
              "[1958 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-860d9aae-cde8-44de-a75e-14d453659814\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pmcid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>exact</th>\n",
              "      <th>type</th>\n",
              "      <th>prefix_exact_postfix</th>\n",
              "      <th>bio_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>(C) Heat map showing the relative enrichment o...</td>\n",
              "      <td>[hair, cells, supporting cells, cochlear hair ...</td>\n",
              "      <td>[Cell, Cell, Cell, Cell, Cell, Cell, Cell, Cel...</td>\n",
              "      <td>[eq peaks in utricle hair cells, utricle supp,...</td>\n",
              "      <td>[((, O), (C, O), (), O), (Heat, O), (map, O), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>112 of these genes appear to be generic marker...</td>\n",
              "      <td>[hair cells, neonatal, cochlear hair cells]</td>\n",
              "      <td>[Cell, Cell, Cell]</td>\n",
              "      <td>[ generic markers of hair cells, as they are a...</td>\n",
              "      <td>[(112, O), (of, O), (these, O), (genes, O), (a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>24 hr of exposure to gentamicin led to signifi...</td>\n",
              "      <td>[hair cell]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ led to significant hair cell loss in the utr...</td>\n",
              "      <td>[(24, O), (hr, O), (of, O), (exposure, O), (to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>70 of the top 100 enriched utricle hair cell g...</td>\n",
              "      <td>[hair cell, hair cells]</td>\n",
              "      <td>[Cell, Cell]</td>\n",
              "      <td>[00 enriched utricle hair cell genes have been...</td>\n",
              "      <td>[(70, O), (of, O), (the, O), (top, O), (100, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PMC6504235</td>\n",
              "      <td>A number of transcription factors have been pr...</td>\n",
              "      <td>[hair cell]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[e with Atoh1 during hair cell induction, such...</td>\n",
              "      <td>[(A, O), (number, O), (of, O), (transcription,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1953</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>When we examined the proximal zebrafish intest...</td>\n",
              "      <td>[EECs]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[iscovered that most EECs had adopted a close]</td>\n",
              "      <td>[(When, O), (we, O), (examined, O), (the, O), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1954</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Whereas HF feeding normally reduces the EEC mo...</td>\n",
              "      <td>[EEC]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ormally reduces the EEC morphology score, t]</td>\n",
              "      <td>[(Whereas, O), (HF, O), (feeding, O), (normall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1955</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Whether EECs adopt the same mechanisms as neur...</td>\n",
              "      <td>[neurons]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[ same mechanisms as neurons to prune their cell]</td>\n",
              "      <td>[(Whether, O), (EECs, O), (adopt, O), (the, O)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1956</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Wild-type adult EKW zebrafish were bred and cl...</td>\n",
              "      <td>[eggs]</td>\n",
              "      <td>[Cell]</td>\n",
              "      <td>[red and clutches of eggs from three distinct]</td>\n",
              "      <td>[(Wild-type, O), (adult, O), (EKW, O), (zebraf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1957</th>\n",
              "      <td>PMC6937151</td>\n",
              "      <td>Within the intestinal epithelium, EECs are sur...</td>\n",
              "      <td>[EECs, enterocytes]</td>\n",
              "      <td>[Cell, Cell]</td>\n",
              "      <td>[estinal epithelium, EECs are surrounded by a,...</td>\n",
              "      <td>[(Within, O), (the, O), (intestinal, O), (epit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1958 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-860d9aae-cde8-44de-a75e-14d453659814')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-860d9aae-cde8-44de-a75e-14d453659814 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-860d9aae-cde8-44de-a75e-14d453659814');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-148e18ac-4b6b-43cf-aea3-7e282568f075\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-148e18ac-4b6b-43cf-aea3-7e282568f075')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-148e18ac-4b6b-43cf-aea3-7e282568f075 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8961b857-60d4-4b91-84d6-d3d91194506a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('grouped_ner_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8961b857-60d4-4b91-84d6-d3d91194506a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('grouped_ner_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "grouped_ner_df",
              "summary": "{\n  \"name\": \"grouped_ner_df\",\n  \"rows\": 1958,\n  \"fields\": [\n    {\n      \"column\": \"pmcid\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"PMC6854655\",\n          \"PMC6592685\",\n          \"PMC6802965\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1958,\n        \"samples\": [\n          \"However, because we have yet to identify a phagocytosis pathway required for death, our data do not rule out a scenario in which microglia first kill astrocytes via a separate (non-apoptotic) pathway before immediately engulfing the doomed cells.\",\n          \"We found strong positive correlations between the capillary RBC flux and both Mean-PO2 and SO2 in the downstream (V1-V3) capillaries, but not in the upstream (A1-A3) capillaries (Figure 7f,g), suggesting that a positive correlation between the RBC flux and oxygenation may be gradually building up along the capillary paths.\",\n          \"By averaging over all the RBCs identified in each capillary and then across the 58 capillaries, we obtained the mean RBC longitudinal size (6.9\\u00a0\\u00b1\\u00a03.0 \\u00b5m; Mean\\u00a0\\u00b1STD).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"exact\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prefix_exact_postfix\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bio_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Apply the function to each row\n",
        "grouped_ner_df['bio_tags'] = grouped_ner_df.apply(tokenize_and_tag, axis=1)\n",
        "display(grouped_ner_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TviG2hMmERG8"
      },
      "source": [
        "Shuffle and Split the dataset into Train:Test = 4:1. Then save the sets to TSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0NlpRbbAUcj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IBYhsk6_Ywh"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame\n",
        "df_shuffled = grouped_ner_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split the DataFrame into train and temp sets first (80% train, 20% test)\n",
        "df_train, df_test = train_test_split(df_shuffled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyxY5b8oCqGv"
      },
      "outputs": [],
      "source": [
        "def save_to_tsv(df, output_file_path):\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        # Iterate over each row in the DataFrame\n",
        "        for _, row in df.iterrows():\n",
        "            # Retrieve the list of (token, tag) tuples from the specified column\n",
        "            bio_tags = row['bio_tags']\n",
        "            # Write each tuple to the file, token and tag separated by a tab\n",
        "            for token, tag in bio_tags:\n",
        "                f.write(f\"{token}\\t{tag}\\n\")\n",
        "            # Write a blank line after each sentence's tags to separate the data\n",
        "            f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXEYSsS_Cq6D"
      },
      "outputs": [],
      "source": [
        "# Save each set to TSV files\n",
        "save_to_tsv(df_train, '/content/data/train_data.txt')\n",
        "save_to_tsv(df_test, '/content/data/test_data.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsx5G3bzMQg4"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I used the Stanza Python NLP Library to train the NER model."
      ],
      "metadata": {
        "id": "E-YynKQ4qdQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stanfordnlp/stanza.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh-9U3GkhHVT",
        "outputId": "6b59af88-a556-4450-e786-087b198b1d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stanza'...\n",
            "remote: Enumerating objects: 40118, done.\u001b[K\n",
            "remote: Counting objects: 100% (2205/2205), done.\u001b[K\n",
            "remote: Compressing objects: 100% (683/683), done.\u001b[K\n",
            "remote: Total 40118 (delta 1680), reused 1961 (delta 1520), pack-reused 37913\u001b[K\n",
            "Receiving objects: 100% (40118/40118), 83.21 MiB | 17.44 MiB/s, done.\n",
            "Resolving deltas: 100% (30760/30760), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TqKRD2Ylf6f",
        "outputId": "c10e0bda-8a2e-4698-8d1c-85e7455644d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stanza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the environment setting."
      ],
      "metadata": {
        "id": "N-Q7EDA600rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git checkout dev\n",
        "! git checkout -b cell_ner\n",
        "! echo $PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFwHTaulhJvM",
        "outputId": "ab42b2d6-41f3-44b8-d53a-9337eacc69ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
            "Switched to a new branch 'dev'\n",
            "Switched to a new branch 'cell_ner'\n",
            "/env/python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D0vIg-9leM3",
        "outputId": "aa82c0be-7e27-4a35-9f2f-0688731108f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/stanza\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji (from stanza==1.8.0)\n",
            "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza==1.8.0) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza==1.8.0) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.8.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza==1.8.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza==1.8.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza==1.8.0) (1.3.0)\n",
            "Installing collected packages: emoji, stanza\n",
            "  Running setup.py develop for stanza\n",
            "Successfully installed emoji-2.10.1 stanza-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the input data to the application directory."
      ],
      "metadata": {
        "id": "wY6D9eiHz7Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/stanza/data/Input')\n",
        "os.makedirs('/content/stanza/data/ner')"
      ],
      "metadata": {
        "id": "KDALzX6aS36x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/data/train_data.txt /content/stanza/data/Input\n",
        "! cp /content/data/test_data.txt /content/stanza/data/Input"
      ],
      "metadata": {
        "id": "1AGuXhaUymF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I modifed a few sources files to so that we can quickly go through training process. More changes are needed if we are implementing a proper training component."
      ],
      "metadata": {
        "id": "iKG4pGAes35q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile stanza/utils/datasets/ner/convert_bn_daffodil.py\n",
        "\"\"\"\n",
        "Convert a Bengali NER dataset to our internal .json format\n",
        "\n",
        "The dataset is here:\n",
        "\n",
        "https://github.com/Rifat1493/Bengali-NER/tree/master/Input\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import tempfile\n",
        "\n",
        "from stanza.utils.datasets.ner.utils import read_tsv, write_dataset\n",
        "\n",
        "def redo_time_tags(sentences):\n",
        "    \"\"\"\n",
        "    Replace all TIM, TIM with B-TIM, I-TIM\n",
        "\n",
        "    A brief use of Google Translate suggests the time phrases are\n",
        "    generally one phrase, so we don't want to turn this into B-TIM, B-TIM\n",
        "    \"\"\"\n",
        "    new_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        new_sentence = []\n",
        "        prev_time = False\n",
        "        for word, tag in sentence:\n",
        "            if tag == 'TIM':\n",
        "                if prev_time:\n",
        "                    new_sentence.append((word, \"I-TIM\"))\n",
        "                else:\n",
        "                    prev_time = True\n",
        "                    new_sentence.append((word, \"B-TIM\"))\n",
        "            else:\n",
        "                prev_time = False\n",
        "                new_sentence.append((word, tag))\n",
        "        new_sentences.append(new_sentence)\n",
        "\n",
        "    return new_sentences\n",
        "\n",
        "def strip_words(dataset):\n",
        "    return [[(x[0].strip().replace('\\ufeff', ''), x[1]) for x in sentence] for sentence in dataset]\n",
        "\n",
        "def filter_blank_words(train_file, train_filtered_file):\n",
        "    \"\"\"\n",
        "    As of July 2022, this dataset has blank words with O labels, which is not ideal\n",
        "\n",
        "    This method removes those lines\n",
        "    \"\"\"\n",
        "    with open(train_file, encoding=\"utf-8\") as fin:\n",
        "        with open(train_filtered_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for line in fin:\n",
        "                if line.strip() == 'O':\n",
        "                    continue\n",
        "                fout.write(line)\n",
        "\n",
        "def filter_broken_tags(train_sentences):\n",
        "    \"\"\"\n",
        "    Eliminate any sentences where any of the tags were empty\n",
        "    \"\"\"\n",
        "    return [x for x in train_sentences if not any(y[1] is None for y in x)]\n",
        "\n",
        "def filter_bad_words(train_sentences):\n",
        "    \"\"\"\n",
        "    Not bad words like poop, but characters that don't exist\n",
        "\n",
        "    These characters look like n and l in emacs, but they are really\n",
        "    0xF06C and 0xF06E\n",
        "    \"\"\"\n",
        "    return [[x for x in sentence if not x[0] in (\"\", \"\")] for sentence in train_sentences]\n",
        "\n",
        "def read_datasets(in_directory):\n",
        "    \"\"\"\n",
        "    Reads & splits the train data, reads the test data\n",
        "\n",
        "    There is no validation data, so we split the training data into\n",
        "    two pieces and use the smaller piece as the dev set\n",
        "\n",
        "    Also performeed is a conversion of TIM -> B-TIM, I-TIM\n",
        "    \"\"\"\n",
        "    # make sure we always get the same shuffle & split\n",
        "    random.seed(1234)\n",
        "\n",
        "    train_file = os.path.join(in_directory, \"Input\", \"train_data.txt\")\n",
        "    with tempfile.TemporaryDirectory() as tempdir:\n",
        "        train_filtered_file = os.path.join(tempdir, \"train.txt\")\n",
        "        filter_blank_words(train_file, train_filtered_file)\n",
        "        train_sentences = read_tsv(train_filtered_file, text_column=0, annotation_column=1, keep_broken_tags=True)\n",
        "    train_sentences = filter_broken_tags(train_sentences)\n",
        "    train_sentences = filter_bad_words(train_sentences)\n",
        "    train_sentences = redo_time_tags(train_sentences)\n",
        "    train_sentences = strip_words(train_sentences)\n",
        "\n",
        "    test_file = os.path.join(in_directory, \"Input\", \"test_data.txt\")\n",
        "    test_sentences = read_tsv(test_file, text_column=0, annotation_column=1, keep_broken_tags=True)\n",
        "    test_sentences = filter_broken_tags(test_sentences)\n",
        "    test_sentences = filter_bad_words(test_sentences)\n",
        "    test_sentences = redo_time_tags(test_sentences)\n",
        "    test_sentences = strip_words(test_sentences)\n",
        "\n",
        "    random.shuffle(train_sentences)\n",
        "    split_len = len(train_sentences) * 9 // 10\n",
        "    dev_sentences = train_sentences[split_len:]\n",
        "    train_sentences = train_sentences[:split_len]\n",
        "\n",
        "    datasets = (train_sentences, dev_sentences, test_sentences)\n",
        "    return datasets\n",
        "\n",
        "def convert_dataset(in_directory, out_directory):\n",
        "    \"\"\"\n",
        "    Reads the datasets using read_datasets, then write them back out\n",
        "    \"\"\"\n",
        "    datasets = read_datasets(in_directory)\n",
        "    # write_dataset(datasets, out_directory, \"bn_daffodil\")\n",
        "    write_dataset(datasets, out_directory, \"en_cell\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_path', type=str, default=\"/home/john/extern_data/ner/bangla/Bengali-NER\", help=\"Where to find the files\")\n",
        "    parser.add_argument('--output_path', type=str, default=\"/home/john/stanza/data/ner\", help=\"Where to output the results\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    convert_dataset(args.input_path, args.output_path)\n"
      ],
      "metadata": {
        "id": "1_RD6U-Du3GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1aa39bb-3ad7-4a2b-842f-43c28a24c72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting stanza/utils/datasets/ner/convert_bn_daffodil.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile stanza/utils/datasets/ner/prepare_ner_dataset.py\n",
        "\n",
        "\"\"\"Converts raw data files into json files usable by the training script.\n",
        "\n",
        "Currently it supports converting wikiner datasets, available here:\n",
        "  https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500\n",
        "  - download the language of interest to {Language}-WikiNER\n",
        "  - then run\n",
        "    prepare_ner_dataset.py French-WikiNER\n",
        "\n",
        "Also, Finnish Turku dataset, available here:\n",
        "  - https://turkunlp.org/fin-ner.html\n",
        "  - https://github.com/TurkuNLP/turku-ner-corpus\n",
        "    git clone the repo into $NERBASE/finnish\n",
        "    you will now have a directory\n",
        "    $NERBASE/finnish/turku-ner-corpus\n",
        "  - prepare_ner_dataset.py fi_turku\n",
        "\n",
        "FBK in Italy produced an Italian dataset.\n",
        "  - KIND: an Italian Multi-Domain Dataset for Named Entity Recognition\n",
        "    Paccosi T. and Palmero Aprosio A.\n",
        "    LREC 2022\n",
        "  - https://arxiv.org/abs/2112.15099\n",
        "  The processing here is for a combined .tsv file they sent us.\n",
        "  - prepare_ner_dataset.py it_fbk\n",
        "  There is a newer version of the data available here:\n",
        "    https://github.com/dhfbk/KIND\n",
        "  TODO: update to the newer version of the data\n",
        "\n",
        "IJCNLP 2008 produced a few Indian language NER datasets.\n",
        "  description:\n",
        "    http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3\n",
        "  download:\n",
        "    http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5\n",
        "  The models produced from these datasets have extremely low recall, unfortunately.\n",
        "  - prepare_ner_dataset.py hi_ijc\n",
        "\n",
        "FIRE 2013 also produced NER datasets for Indian languages.\n",
        "  http://au-kbc.org/nlp/NER-FIRE2013/index.html\n",
        "  The datasets are password locked.\n",
        "  For Stanford users, contact Chris Manning for license details.\n",
        "  For external users, please contact the organizers for more information.\n",
        "  - prepare_ner_dataset.py hi-fire2013\n",
        "\n",
        "HiNER is another Hindi dataset option\n",
        "  https://github.com/cfiltnlp/HiNER\n",
        "  - HiNER: A Large Hindi Named Entity Recognition Dataset\n",
        "    Murthy, Rudra and Bhattacharjee, Pallab and Sharnagat, Rahul and\n",
        "    Khatri, Jyotsana and Kanojia, Diptesh and Bhattacharyya, Pushpak\n",
        "  There are two versions:\n",
        "    hi_hinercollapsed and hi_hiner\n",
        "  The collapsed version has just PER, LOC, ORG\n",
        "  - convert data as follows:\n",
        "    cd $NERBASE\n",
        "    mkdir hindi\n",
        "    cd hindi\n",
        "    git clone git@github.com:cfiltnlp/HiNER.git\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset hi_hiner\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset hi_hinercollapsed\n",
        "\n",
        "Ukranian NER is provided by lang-uk, available here:\n",
        "  https://github.com/lang-uk/ner-uk\n",
        "  git clone the repo to $NERBASE/lang-uk\n",
        "  There should be a subdirectory $NERBASE/lang-uk/ner-uk/data at that point\n",
        "  Conversion script graciously provided by Andrii Garkavyi @gawy\n",
        "  - prepare_ner_dataset.py uk_languk\n",
        "\n",
        "There are two Hungarian datasets are available here:\n",
        "  https://rgai.inf.u-szeged.hu/node/130\n",
        "  http://www.lrec-conf.org/proceedings/lrec2006/pdf/365_pdf.pdf\n",
        "  We combined them and give them the label hu_rgai\n",
        "  You can also build individual pieces with hu_rgai_business or hu_rgai_criminal\n",
        "  Create a subdirectory of $NERBASE, $NERBASE/hu_rgai, and download both of\n",
        "    the pieces and unzip them in that directory.\n",
        "  - prepare_ner_dataset.py hu_rgai\n",
        "\n",
        "Another Hungarian dataset is here:\n",
        "  - https://github.com/nytud/NYTK-NerKor\n",
        "  - git clone the entire thing in your $NERBASE directory to operate on it\n",
        "  - prepare_ner_dataset.py hu_nytk\n",
        "\n",
        "The two Hungarian datasets can be combined with hu_combined\n",
        "  TODO: verify that there is no overlap in text\n",
        "  - prepare_ner_dataset.py hu_combined\n",
        "\n",
        "BSNLP publishes NER datasets for Eastern European languages.\n",
        "  - In 2019 they published BG, CS, PL, RU.\n",
        "  - http://bsnlp.cs.helsinki.fi/bsnlp-2019/shared_task.html\n",
        "  - In 2021 they added some more data, but the test sets\n",
        "    were not publicly available as of April 2021.\n",
        "    Therefore, currently the model is made from 2019.\n",
        "    In 2021, the link to the 2021 task is here:\n",
        "    http://bsnlp.cs.helsinki.fi/shared-task.html\n",
        "  - The below method processes the 2019 version of the corpus.\n",
        "    It has specific adjustments for the BG section, which has\n",
        "    quite a few typos or mis-annotations in it.  Other languages\n",
        "    probably need similar work in order to function optimally.\n",
        "  - make a directory $NERBASE/bsnlp2019\n",
        "  - download the \"training data are available HERE\" and\n",
        "    \"test data are available HERE\" to this subdirectory\n",
        "  - unzip those files in that directory\n",
        "  - we use the code name \"bg_bsnlp19\".  Other languages from\n",
        "    bsnlp 2019 can be supported by adding the appropriate\n",
        "    functionality in convert_bsnlp.py.\n",
        "  - prepare_ner_dataset.py bg_bsnlp19\n",
        "\n",
        "NCHLT produced NER datasets for many African languages.\n",
        "  Unfortunately, it is difficult to make use of many of these,\n",
        "  as there is no corresponding UD data from which to build a\n",
        "  tokenizer or other tools.\n",
        "  - Afrikaans:  https://repo.sadilar.org/handle/20.500.12185/299\n",
        "  - isiNdebele: https://repo.sadilar.org/handle/20.500.12185/306\n",
        "  - isiXhosa:   https://repo.sadilar.org/handle/20.500.12185/312\n",
        "  - isiZulu:    https://repo.sadilar.org/handle/20.500.12185/319\n",
        "  - Sepedi:     https://repo.sadilar.org/handle/20.500.12185/328\n",
        "  - Sesotho:    https://repo.sadilar.org/handle/20.500.12185/334\n",
        "  - Setswana:   https://repo.sadilar.org/handle/20.500.12185/341\n",
        "  - Siswati:    https://repo.sadilar.org/handle/20.500.12185/346\n",
        "  - Tsivenda:   https://repo.sadilar.org/handle/20.500.12185/355\n",
        "  - Xitsonga:   https://repo.sadilar.org/handle/20.500.12185/362\n",
        "  Agree to the license, download the zip, and unzip it in\n",
        "  $NERBASE/NCHLT\n",
        "\n",
        "UCSY built a Myanmar dataset.  They have not made it publicly\n",
        "  available, but they did make it available to Stanford for research\n",
        "  purposes.  Contact Chris Manning or John Bauer for the data files if\n",
        "  you are Stanford affiliated.\n",
        "  - https://arxiv.org/abs/1903.04739\n",
        "  - Syllable-based Neural Named Entity Recognition for Myanmar Language\n",
        "    by Hsu Myat Mo and Khin Mar Soe\n",
        "\n",
        "Hanieh Poostchi et al produced a Persian NER dataset:\n",
        "  - git@github.com:HaniehP/PersianNER.git\n",
        "  - https://github.com/HaniehP/PersianNER\n",
        "  - Hanieh Poostchi, Ehsan Zare Borzeshi, Mohammad Abdous, and Massimo Piccardi,\n",
        "    \"PersoNER: Persian Named-Entity Recognition\"\n",
        "  - Hanieh Poostchi, Ehsan Zare Borzeshi, and Massimo Piccardi,\n",
        "    \"BiLSTM-CRF for Persian Named-Entity Recognition; ArmanPersoNERCorpus: the First Entity-Annotated Persian Dataset\"\n",
        "  - Conveniently, this dataset is already in BIO format.  It does not have a dev split, though.\n",
        "    git clone the above repo, unzip ArmanPersoNERCorpus.zip, and this script will split the\n",
        "    first train fold into a dev section.\n",
        "\n",
        "SUC3 is a Swedish NER dataset provided by Språkbanken\n",
        "  - https://spraakbanken.gu.se/en/resources/suc3\n",
        "  - The splitting tool is generously provided by\n",
        "    Emil Stenstrom\n",
        "    https://github.com/EmilStenstrom/suc_to_iob\n",
        "  - Download the .bz2 file at this URL and put it in $NERBASE/sv_suc3shuffle\n",
        "    It is not necessary to unzip it.\n",
        "  - Gustafson-Capková, Sophia and Britt Hartmann, 2006,\n",
        "    Manual of the Stockholm Umeå Corpus version 2.0.\n",
        "    Stockholm University.\n",
        "  - Östling, Robert, 2013, Stagger\n",
        "    an Open-Source Part of Speech Tagger for Swedish\n",
        "    Northern European Journal of Language Technology 3: 1–18\n",
        "    DOI 10.3384/nejlt.2000-1533.1331\n",
        "  - The shuffled dataset can be converted with dataset code\n",
        "    prepare_ner_dataset.py sv_suc3shuffle\n",
        "  - If you fill out the license form and get the official data,\n",
        "    you can get the official splits by putting the provided zip file\n",
        "    in $NERBASE/sv_suc3licensed.  Again, not necessary to unzip it\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset sv_suc3licensed\n",
        "\n",
        "DDT is a reformulation of the Danish Dependency Treebank as an NER dataset\n",
        "  - https://danlp-alexandra.readthedocs.io/en/latest/docs/datasets.html#dane\n",
        "  - direct download link as of late 2021: https://danlp.alexandra.dk/304bd159d5de/datasets/ddt.zip\n",
        "  - https://aclanthology.org/2020.lrec-1.565.pdf\n",
        "    DaNE: A Named Entity Resource for Danish\n",
        "    Rasmus Hvingelby, Amalie Brogaard Pauli, Maria Barrett,\n",
        "    Christina Rosted, Lasse Malm Lidegaard, Anders Søgaard\n",
        "  - place ddt.zip in $NERBASE/da_ddt/ddt.zip\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset da_ddt\n",
        "\n",
        "NorNE is the Norwegian Dependency Treebank with NER labels\n",
        "  - LREC 2020\n",
        "    NorNE: Annotating Named Entities for Norwegian\n",
        "    Fredrik Jørgensen, Tobias Aasmoe, Anne-Stine Ruud Husevåg,\n",
        "    Lilja Øvrelid, and Erik Velldal\n",
        "  - both Bokmål and Nynorsk\n",
        "  - This dataset is in a git repo:\n",
        "    https://github.com/ltgoslo/norne\n",
        "    Clone it into $NERBASE\n",
        "    git clone git@github.com:ltgoslo/norne.git\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset nb_norne\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset nn_norne\n",
        "\n",
        "tr_starlang is a set of constituency trees for Turkish\n",
        "  The words in this dataset (usually) have NER labels as well\n",
        "\n",
        "  A dataset in three parts from the Starlang group in Turkey:\n",
        "  Neslihan Kara, Büşra Marşan, et al\n",
        "    Creating A Syntactically Felicitous Constituency Treebank For Turkish\n",
        "    https://ieeexplore.ieee.org/document/9259873\n",
        "  git clone the following three repos\n",
        "    https://github.com/olcaytaner/TurkishAnnotatedTreeBank-15\n",
        "    https://github.com/olcaytaner/TurkishAnnotatedTreeBank2-15\n",
        "    https://github.com/olcaytaner/TurkishAnnotatedTreeBank2-20\n",
        "  Put them in\n",
        "    $CONSTITUENCY_HOME/turkish    (yes, the constituency home)\n",
        "  python3 -m stanza.utils.datasets.ner.prepare_ner_dataset tr_starlang\n",
        "\n",
        "GermEval2014 is a German NER dataset\n",
        "  https://sites.google.com/site/germeval2014ner/data\n",
        "  https://drive.google.com/drive/folders/1kC0I2UGl2ltrluI9NqDjaQJGw5iliw_J\n",
        "  Download the files in that directory\n",
        "    NER-de-train.tsv NER-de-dev.tsv NER-de-test.tsv\n",
        "  put them in\n",
        "    $NERBASE/germeval2014\n",
        "  then run\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset de_germeval2014\n",
        "\n",
        "The UD Japanese GSD dataset has a conversion by Megagon Labs\n",
        "  https://github.com/megagonlabs/UD_Japanese-GSD\n",
        "  https://github.com/megagonlabs/UD_Japanese-GSD/tags\n",
        "  - r2.9-NE has the NE tagged files inside a \"spacy\"\n",
        "    folder in the download\n",
        "  - expected directory for this data:\n",
        "    unzip the .zip of the release into\n",
        "      $NERBASE/ja_gsd\n",
        "    so it should wind up in\n",
        "      $NERBASE/ja_gsd/UD_Japanese-GSD-r2.9-NE\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset ja_gsd\n",
        "\n",
        "L3Cube is a Marathi dataset\n",
        "  - https://arxiv.org/abs/2204.06029\n",
        "    https://arxiv.org/pdf/2204.06029.pdf\n",
        "    https://github.com/l3cube-pune/MarathiNLP\n",
        "  - L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT models\n",
        "    Parth Patil, Aparna Ranade, Maithili Sabane, Onkar Litake, Raviraj Joshi\n",
        "\n",
        "  Clone the repo into $NERBASE/marathi\n",
        "    git clone git@github.com:l3cube-pune/MarathiNLP.git\n",
        "  Then run\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset mr_l3cube\n",
        "\n",
        "Daffodil University produced a Bangla NER dataset\n",
        "  - https://github.com/Rifat1493/Bengali-NER\n",
        "  - https://ieeexplore.ieee.org/document/8944804\n",
        "  - Bengali Named Entity Recognition:\n",
        "    A survey with deep learning benchmark\n",
        "    Md Jamiur Rahman Rifat, Sheikh Abujar, Sheak Rashed Haider Noori,\n",
        "    Syed Akhter Hossain\n",
        "\n",
        "  Clone the repo into a \"bangla\" subdirectory of $NERBASE\n",
        "    cd $NERBASE/bangla\n",
        "    git clone git@github.com:Rifat1493/Bengali-NER.git\n",
        "  Then run\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset bn_daffodil\n",
        "\n",
        "LST20 is a Thai NER dataset from 2020\n",
        "  - https://arxiv.org/abs/2008.05055\n",
        "    The Annotation Guideline of LST20 Corpus\n",
        "    Prachya Boonkwan, Vorapon Luantangsrisuk, Sitthaa Phaholphinyo,\n",
        "    Kanyanat Kriengket, Dhanon Leenoi, Charun Phrombut,\n",
        "    Monthika Boriboon, Krit Kosawat, Thepchai Supnithi\n",
        "  - This script processes a version which can be downloaded here after registration:\n",
        "    https://aiforthai.in.th/index.php\n",
        "  - There is another version downloadable from HuggingFace\n",
        "    The script will likely need some modification to be compatible\n",
        "    with the HuggingFace version\n",
        "  - Download the data in $NERBASE/thai/LST20_Corpus\n",
        "    There should be \"train\", \"eval\", \"test\" directories after downloading\n",
        "  - Then run\n",
        "    pytohn3 -m stanza.utils.datasets.ner.prepare_ner_dataset th_lst20\n",
        "\n",
        "Thai-NNER is another Thai NER dataset, from 2022\n",
        "  - https://github.com/vistec-AI/Thai-NNER\n",
        "  - https://aclanthology.org/2022.findings-acl.116/\n",
        "    Thai Nested Named Entity Recognition Corpus\n",
        "    Weerayut Buaphet, Can Udomcharoenchaikit, Peerat Limkonchotiwat,\n",
        "    Attapol Rutherford, and Sarana Nutanong\n",
        "  - git clone the data to $NERBASE/thai\n",
        "  - On the git repo, there should be a link to a more complete version\n",
        "    of the dataset.  For example, in Sep. 2023 it is here:\n",
        "    https://github.com/vistec-AI/Thai-NNER#dataset\n",
        "    The Google drive it goes to has \"postproc\".\n",
        "    Put the train.json, dev.json, and test.json in\n",
        "    $NERBASE/thai/Thai-NNER/data/scb-nner-th-2022/postproc/\n",
        "  - Then run\n",
        "    pytohn3 -m stanza.utils.datasets.ner.prepare_ner_dataset th_nner22\n",
        "\n",
        "\n",
        "NKJP is a Polish NER dataset\n",
        "  - http://nkjp.pl/index.php?page=0&lang=1\n",
        "    About the Project\n",
        "  - http://zil.ipipan.waw.pl/DistrNKJP\n",
        "    Wikipedia subcorpus used to train charlm model\n",
        "  - http://clip.ipipan.waw.pl/NationalCorpusOfPolish?action=AttachFile&do=view&target=NKJP-PodkorpusMilionowy-1.2.tar.gz\n",
        "    Annotated subcorpus to train NER model.\n",
        "    Download and extract to $NERBASE/Polish-NKJP or leave the gzip in $NERBASE/polish/...\n",
        "\n",
        "kk_kazNERD is a Kazakh dataset published in 2021\n",
        "  - https://github.com/IS2AI/KazNERD\n",
        "  - https://arxiv.org/abs/2111.13419\n",
        "    KazNERD: Kazakh Named Entity Recognition Dataset\n",
        "    Rustem Yeshpanov, Yerbolat Khassanov, Huseyin Atakan Varol\n",
        "  - in $NERBASE, make a \"kazakh\" directory, then git clone the repo there\n",
        "    mkdir -p $NERBASE/kazakh\n",
        "    cd $NERBASE/kazakh\n",
        "    git clone git@github.com:IS2AI/KazNERD.git\n",
        "  - Then run\n",
        "    pytohn3 -m stanza.utils.datasets.ner.prepare_ner_dataset kk_kazNERD\n",
        "\n",
        "Masakhane NER is a set of NER datasets for African languages\n",
        "  - MasakhaNER: Named Entity Recognition for African Languages\n",
        "    Adelani, David Ifeoluwa; Abbott, Jade; Neubig, Graham;\n",
        "    D’souza, Daniel; Kreutzer, Julia; Lignos, Constantine;\n",
        "    Palen-Michel, Chester; Buzaaba, Happy; Rijhwani, Shruti;\n",
        "    Ruder, Sebastian; Mayhew, Stephen; Azime, Israel Abebe;\n",
        "    Muhammad, Shamsuddeen H.; Emezue, Chris Chinenye;\n",
        "    Nakatumba-Nabende, Joyce; Ogayo, Perez; Anuoluwapo, Aremu;\n",
        "    Gitau, Catherine; Mbaye, Derguene; Alabi, Jesujoba;\n",
        "    Yimam, Seid Muhie; Gwadabe, Tajuddeen Rabiu; Ezeani, Ignatius;\n",
        "    Niyongabo, Rubungo Andre; Mukiibi, Jonathan; Otiende, Verrah;\n",
        "    Orife, Iroro; David, Davis; Ngom, Samba; Adewumi, Tosin;\n",
        "    Rayson, Paul; Adeyemi, Mofetoluwa; Muriuki, Gerald;\n",
        "    Anebi, Emmanuel; Chukwuneke, Chiamaka; Odu, Nkiruka;\n",
        "    Wairagala, Eric Peter; Oyerinde, Samuel; Siro, Clemencia;\n",
        "    Bateesa, Tobius Saul; Oloyede, Temilola; Wambui, Yvonne;\n",
        "    Akinode, Victor; Nabagereka, Deborah; Katusiime, Maurice;\n",
        "    Awokoya, Ayodele; MBOUP, Mouhamadane; Gebreyohannes, Dibora;\n",
        "    Tilaye, Henok; Nwaike, Kelechi; Wolde, Degaga; Faye, Abdoulaye;\n",
        "    Sibanda, Blessing; Ahia, Orevaoghene; Dossou, Bonaventure F. P.;\n",
        "    Ogueji, Kelechi; DIOP, Thierno Ibrahima; Diallo, Abdoulaye;\n",
        "    Akinfaderin, Adewale; Marengereke, Tendai; Osei, Salomey\n",
        "  - https://github.com/masakhane-io/masakhane-ner\n",
        "  - git clone the repo to $NERBASE\n",
        "  - Then run\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset lcode_masakhane\n",
        "  - You can use the full language name, the 3 letter language code,\n",
        "    or in the case of languages with a 2 letter language code,\n",
        "    the 2 letter code for lcode.  The tool will throw an error\n",
        "    if the language is not supported in Masakhane.\n",
        "\n",
        "SiNER is a Sindhi NER dataset\n",
        "  - https://aclanthology.org/2020.lrec-1.361/\n",
        "    SiNER: A Large Dataset for Sindhi Named Entity Recognition\n",
        "    Wazir Ali, Junyu Lu, Zenglin Xu\n",
        "  - It is available via git repository\n",
        "    https://github.com/AliWazir/SiNER-dataset\n",
        "    As of Nov. 2022, there were a few changes to the dataset\n",
        "    to update a couple instances of broken tags & tokenization\n",
        "  - Clone the repo to $NERBASE/sindhi\n",
        "    mkdir $NERBASE/sindhi\n",
        "    cd $NERBASE/sindhi\n",
        "    git clone git@github.com:AliWazir/SiNER-dataset.git\n",
        "  - Then, prepare the dataset with this script:\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset sd_siner\n",
        "\n",
        "en_sample is the toy dataset included with stanza-train\n",
        "  https://github.com/stanfordnlp/stanza-train\n",
        "  this is not meant for any kind of actual NER use\n",
        "\n",
        "ArmTDP-NER is an Armenian NER dataset\n",
        "  - https://github.com/myavrum/ArmTDP-NER.git\n",
        "    ArmTDP-NER: The corpus was developed by the ArmTDP team led by Marat M. Yavrumyan\n",
        "    at the Yerevan State University by the collaboration of \"Armenia National SDG Innovation Lab\"\n",
        "    and \"UC Berkley's Armenian Linguists' network\".\n",
        "  - in $NERBASE, make a \"armenian\" directory, then git clone the repo there\n",
        "    mkdir -p $NERBASE/armenian\n",
        "    cd $NERBASE/armenian\n",
        "    git clone https://github.com/myavrum/ArmTDP-NER.git\n",
        "  - Then run\n",
        "    python3 -m stanza.utils.datasets.ner.prepare_ner_dataset hy_armtdp\n",
        "\n",
        "en_conll03 is the classic 2003 4 class CoNLL dataset\n",
        "  - The version we use is posted on HuggingFace\n",
        "  - https://huggingface.co/datasets/conll2003\n",
        "  - The prepare script will download from HF\n",
        "    using the datasets package, then convert to json\n",
        "  - Introduction to the CoNLL-2003 Shared Task:\n",
        "    Language-Independent Named Entity Recognition\n",
        "    Tjong Kim Sang, Erik F. and De Meulder, Fien\n",
        "  - python3 stanza/utils/datasets/ner/prepare_ner_dataset.py en_conll03\n",
        "\n",
        "en_conll03ww is CoNLL 03 with Worldwide added to the training data.\n",
        "  - python3 stanza/utils/datasets/ner/prepare_ner_dataset.py en_conll03ww\n",
        "\n",
        "en_conllpp is a test set from 2020 newswire\n",
        "  - https://arxiv.org/abs/2212.09747\n",
        "  - https://github.com/ShuhengL/acl2023_conllpp\n",
        "  - Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?\n",
        "    Shuheng Liu, Alan Ritter\n",
        "  - git clone the repo in $NERBASE\n",
        "  - then run\n",
        "    python3 stanza/utils/datasets/ner/prepare_ner_dataset.py en_conllpp\n",
        "\n",
        "en_ontonotes is the OntoNotes 5 on HuggingFace\n",
        "  - https://huggingface.co/datasets/conll2012_ontonotesv5\n",
        "  - python3 stanza/utils/datasets/ner/prepare_ner_dataset.py en_ontonotes\n",
        "  - this downloads the \"v12\" version of the data\n",
        "\n",
        "en_worldwide-4class is an English non-US newswire dataset\n",
        "  - annotated by MLTwist and Aya Data, with help from Datasaur,\n",
        "    collected at Stanford\n",
        "  - work to be published at EMNLP Findings\n",
        "  - the 4 class version is converted to the 4 classes in conll,\n",
        "    then split into train/dev/test\n",
        "  - clone https://github.com/stanfordnlp/en-worldwide-newswire\n",
        "    into $NERBASE/en_worldwide\n",
        "\n",
        "en_worldwide-9class is an English non-US newswire dataset\n",
        "  - annotated by MLTwist and Aya Data, with help from Datasaur,\n",
        "    collected at Stanford\n",
        "  - work to be published at EMNLP Findings\n",
        "  - the 9 class version is not edited\n",
        "  - clone https://github.com/stanfordnlp/en-worldwide-newswire\n",
        "    into $NERBASE/en_worldwide\n",
        "\n",
        "zh-hans_ontonotes is the ZH split of the OntoNotes dataset\n",
        "  - https://catalog.ldc.upenn.edu/LDC2013T19\n",
        "  - https://huggingface.co/datasets/conll2012_ontonotesv5\n",
        "  - python3 stanza/utils/datasets/ner/prepare_ner_dataset.py zh-hans_ontonotes\n",
        "  - this downloads the \"v4\" version of the data\n",
        "\n",
        "\n",
        "AQMAR is a small dataset of Arabic Wikipedia articles\n",
        "  - http://www.cs.cmu.edu/~ark/ArabicNER/\n",
        "  - Recall-Oriented Learning of Named Entities in Arabic Wikipedia\n",
        "    Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, and Noah A. Smith.\n",
        "    In Proceedings of the 13th Conference of the European Chapter of\n",
        "    the Association for Computational Linguistics, Avignon, France,\n",
        "    April 2012.\n",
        "  - download the .zip file there and put it in\n",
        "    $NERBASE/arabic/AQMAR\n",
        "  - there is a challenge for it here:\n",
        "    https://www.topcoder.com/challenges/f3cf483e-a95c-4a7e-83e8-6bdd83174d38\n",
        "  - alternatively, we just randomly split it ourselves\n",
        "  - currently, running the following reproduces the random split:\n",
        "    python3 stanza/utils/datasets/ner/prepare_ner_dataset.py ar_aqmar\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "\n",
        "from stanza.models.common.constant import treebank_to_short_name, lcode2lang, lang_to_langcode, two_to_three_letters\n",
        "import stanza.utils.default_paths as default_paths\n",
        "\n",
        "from stanza.utils.datasets.ner.preprocess_wikiner import preprocess_wikiner\n",
        "from stanza.utils.datasets.ner.split_wikiner import split_wikiner\n",
        "import stanza.utils.datasets.ner.build_en_combined as build_en_combined\n",
        "import stanza.utils.datasets.ner.conll_to_iob as conll_to_iob\n",
        "import stanza.utils.datasets.ner.convert_ar_aqmar as convert_ar_aqmar\n",
        "import stanza.utils.datasets.ner.convert_bn_daffodil as convert_bn_daffodil\n",
        "import stanza.utils.datasets.ner.convert_bsf_to_beios as convert_bsf_to_beios\n",
        "import stanza.utils.datasets.ner.convert_bsnlp as convert_bsnlp\n",
        "import stanza.utils.datasets.ner.convert_en_conll03 as convert_en_conll03\n",
        "import stanza.utils.datasets.ner.convert_fire_2013 as convert_fire_2013\n",
        "import stanza.utils.datasets.ner.convert_ijc as convert_ijc\n",
        "import stanza.utils.datasets.ner.convert_kk_kazNERD as convert_kk_kazNERD\n",
        "import stanza.utils.datasets.ner.convert_lst20 as convert_lst20\n",
        "import stanza.utils.datasets.ner.convert_nner22 as convert_nner22\n",
        "import stanza.utils.datasets.ner.convert_mr_l3cube as convert_mr_l3cube\n",
        "import stanza.utils.datasets.ner.convert_my_ucsy as convert_my_ucsy\n",
        "import stanza.utils.datasets.ner.convert_ontonotes as convert_ontonotes\n",
        "import stanza.utils.datasets.ner.convert_rgai as convert_rgai\n",
        "import stanza.utils.datasets.ner.convert_nytk as convert_nytk\n",
        "import stanza.utils.datasets.ner.convert_starlang_ner as convert_starlang_ner\n",
        "import stanza.utils.datasets.ner.convert_nkjp as convert_nkjp\n",
        "import stanza.utils.datasets.ner.prepare_ner_file as prepare_ner_file\n",
        "import stanza.utils.datasets.ner.convert_sindhi_siner as convert_sindhi_siner\n",
        "import stanza.utils.datasets.ner.ontonotes_multitag as ontonotes_multitag\n",
        "import stanza.utils.datasets.ner.simplify_en_worldwide as simplify_en_worldwide\n",
        "import stanza.utils.datasets.ner.suc_to_iob as suc_to_iob\n",
        "import stanza.utils.datasets.ner.suc_conll_to_iob as suc_conll_to_iob\n",
        "import stanza.utils.datasets.ner.convert_hy_armtdp as convert_hy_armtdp\n",
        "from stanza.utils.datasets.ner.utils import convert_bio_to_json, get_tags, read_tsv, write_dataset, random_shuffle_by_prefixes, read_prefix_file, combine_files\n",
        "\n",
        "SHARDS = ('train', 'dev', 'test')\n",
        "\n",
        "class UnknownDatasetError(ValueError):\n",
        "    def __init__(self, dataset, text):\n",
        "        super().__init__(text)\n",
        "        self.dataset = dataset\n",
        "\n",
        "def process_turku(paths, short_name):\n",
        "    assert short_name == 'fi_turku'\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"finnish\", \"turku-ner-corpus\", \"data\", \"conll\")\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    for shard in SHARDS:\n",
        "        input_filename = os.path.join(base_input_path, '%s.tsv' % shard)\n",
        "        if not os.path.exists(input_filename):\n",
        "            raise FileNotFoundError('Cannot find %s component of %s in %s' % (shard, short_name, input_filename))\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(input_filename, output_filename)\n",
        "\n",
        "def process_it_fbk(paths, short_name):\n",
        "    assert short_name == \"it_fbk\"\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], short_name)\n",
        "    csv_file = os.path.join(base_input_path, \"all-wiki-split.tsv\")\n",
        "    if not os.path.exists(csv_file):\n",
        "        raise FileNotFoundError(\"Cannot find the FBK dataset in its expected location: {}\".format(csv_file))\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    split_wikiner(base_output_path, csv_file, prefix=short_name, suffix=\"io\", shuffle=False, train_fraction=0.8, dev_fraction=0.1)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name, suffix=\"io\")\n",
        "\n",
        "\n",
        "def process_languk(paths, short_name):\n",
        "    assert short_name == 'uk_languk'\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], 'lang-uk', 'ner-uk', 'data')\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    train_test_split_fname = os.path.join(paths[\"NERBASE\"], 'lang-uk', 'ner-uk', 'doc', 'dev-test-split.txt')\n",
        "    convert_bsf_to_beios.convert_bsf_in_folder(base_input_path, base_output_path, train_test_split_file=train_test_split_fname)\n",
        "    for shard in SHARDS:\n",
        "        input_filename = os.path.join(base_output_path, convert_bsf_to_beios.CORPUS_NAME, \"%s.bio\" % shard)\n",
        "        if not os.path.exists(input_filename):\n",
        "            raise FileNotFoundError('Cannot find %s component of %s in %s' % (shard, short_name, input_filename))\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(input_filename, output_filename)\n",
        "\n",
        "\n",
        "def process_ijc(paths, short_name):\n",
        "    \"\"\"\n",
        "    Splits the ijc Hindi dataset in train, dev, test\n",
        "\n",
        "    The original data had train & test splits, so we randomly divide\n",
        "    the files in train to make a dev set.\n",
        "\n",
        "    The expected location of the IJC data is hi_ijc.  This method\n",
        "    should be possible to use for other languages, but we have very\n",
        "    little support for the other languages of IJC at the moment.\n",
        "    \"\"\"\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], short_name)\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    test_files = [os.path.join(base_input_path, \"test-data-hindi.txt\")]\n",
        "    test_csv_file = os.path.join(base_output_path, short_name + \".test.csv\")\n",
        "    print(\"Converting test input %s to space separated file in %s\" % (test_files[0], test_csv_file))\n",
        "    convert_ijc.convert_ijc(test_files, test_csv_file)\n",
        "\n",
        "    train_input_path = os.path.join(base_input_path, \"training-hindi\", \"*utf8\")\n",
        "    train_files = glob.glob(train_input_path)\n",
        "    train_csv_file = os.path.join(base_output_path, short_name + \".train.csv\")\n",
        "    dev_csv_file = os.path.join(base_output_path, short_name + \".dev.csv\")\n",
        "    print(\"Converting training input from %s to space separated files in %s and %s\" % (train_input_path, train_csv_file, dev_csv_file))\n",
        "    convert_ijc.convert_split_ijc(train_files, train_csv_file, dev_csv_file)\n",
        "\n",
        "    for csv_file, shard in zip((train_csv_file, dev_csv_file, test_csv_file), SHARDS):\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(csv_file, output_filename)\n",
        "\n",
        "\n",
        "def process_fire_2013(paths, dataset):\n",
        "    \"\"\"\n",
        "    Splits the FIRE 2013 dataset into train, dev, test\n",
        "\n",
        "    The provided datasets are all mixed together at this point, so it\n",
        "    is not possible to recreate the original test conditions used in\n",
        "    the bakeoff\n",
        "    \"\"\"\n",
        "    short_name = treebank_to_short_name(dataset)\n",
        "    langcode, _ = short_name.split(\"_\")\n",
        "    short_name = \"%s_fire2013\" % langcode\n",
        "    if not langcode in (\"hi\", \"en\", \"ta\", \"bn\", \"mal\"):\n",
        "        raise UnkonwnDatasetError(dataset, \"Language %s not one of the FIRE 2013 languages\" % langcode)\n",
        "    language = lcode2lang[langcode].lower()\n",
        "\n",
        "    # for example, FIRE2013/hindi_train\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"FIRE2013\", \"%s_train\" % language)\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    train_csv_file = os.path.join(base_output_path, \"%s.train.csv\" % short_name)\n",
        "    dev_csv_file   = os.path.join(base_output_path, \"%s.dev.csv\" % short_name)\n",
        "    test_csv_file  = os.path.join(base_output_path, \"%s.test.csv\" % short_name)\n",
        "\n",
        "    convert_fire_2013.convert_fire_2013(base_input_path, train_csv_file, dev_csv_file, test_csv_file)\n",
        "\n",
        "    for csv_file, shard in zip((train_csv_file, dev_csv_file, test_csv_file), SHARDS):\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(csv_file, output_filename)\n",
        "\n",
        "def process_wikiner(paths, dataset):\n",
        "    short_name = treebank_to_short_name(dataset)\n",
        "\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], dataset)\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    expected_filename = \"aij*wikiner*\"\n",
        "    input_files = [x for x in glob.glob(os.path.join(base_input_path, expected_filename)) if not x.endswith(\"bz2\")]\n",
        "    if len(input_files) == 0:\n",
        "        raw_input_path = os.path.join(base_input_path, \"raw\")\n",
        "        input_files = [x for x in glob.glob(os.path.join(raw_input_path, expected_filename)) if not x.endswith(\"bz2\")]\n",
        "        if len(input_files) > 1:\n",
        "            raise FileNotFoundError(\"Found too many raw wikiner files in %s: %s\" % (raw_input_path, \", \".join(input_files)))\n",
        "    elif len(input_files) > 1:\n",
        "        raise FileNotFoundError(\"Found too many raw wikiner files in %s: %s\" % (base_input_path, \", \".join(input_files)))\n",
        "\n",
        "    if len(input_files) == 0:\n",
        "        raise FileNotFoundError(\"Could not find any raw wikiner files in %s or %s\" % (base_input_path, raw_input_path))\n",
        "\n",
        "    csv_file = os.path.join(base_output_path, short_name + \"_csv\")\n",
        "    print(\"Converting raw input %s to space separated file in %s\" % (input_files[0], csv_file))\n",
        "    try:\n",
        "        preprocess_wikiner(input_files[0], csv_file)\n",
        "    except UnicodeDecodeError:\n",
        "        preprocess_wikiner(input_files[0], csv_file, encoding=\"iso8859-1\")\n",
        "\n",
        "    # this should create train.bio, dev.bio, and test.bio\n",
        "    print(\"Splitting %s to %s\" % (csv_file, base_output_path))\n",
        "    split_wikiner(base_output_path, csv_file, prefix=short_name)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def get_rgai_input_path(paths):\n",
        "    return os.path.join(paths[\"NERBASE\"], \"hu_rgai\")\n",
        "\n",
        "def process_rgai(paths, short_name):\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    base_input_path = get_rgai_input_path(paths)\n",
        "\n",
        "    if short_name == 'hu_rgai':\n",
        "        use_business = True\n",
        "        use_criminal = True\n",
        "    elif short_name == 'hu_rgai_business':\n",
        "        use_business = True\n",
        "        use_criminal = False\n",
        "    elif short_name == 'hu_rgai_criminal':\n",
        "        use_business = False\n",
        "        use_criminal = True\n",
        "    else:\n",
        "        raise UnknownDatasetError(short_name, \"Unknown subset of hu_rgai data: %s\" % short_name)\n",
        "\n",
        "    convert_rgai.convert_rgai(base_input_path, base_output_path, short_name, use_business, use_criminal)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def get_nytk_input_path(paths):\n",
        "    return os.path.join(paths[\"NERBASE\"], \"NYTK-NerKor\")\n",
        "\n",
        "def process_nytk(paths, short_name):\n",
        "    \"\"\"\n",
        "    Process the NYTK dataset\n",
        "    \"\"\"\n",
        "    assert short_name == \"hu_nytk\"\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    base_input_path = get_nytk_input_path(paths)\n",
        "\n",
        "    convert_nytk.convert_nytk(base_input_path, base_output_path, short_name)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def concat_files(output_file, *input_files):\n",
        "    input_lines = []\n",
        "    for input_file in input_files:\n",
        "        with open(input_file) as fin:\n",
        "            lines = fin.readlines()\n",
        "        if not len(lines):\n",
        "            raise ValueError(\"Empty input file: %s\" % input_file)\n",
        "        if not lines[-1]:\n",
        "            lines[-1] = \"\\n\"\n",
        "        elif lines[-1].strip():\n",
        "            lines.append(\"\\n\")\n",
        "        input_lines.append(lines)\n",
        "    with open(output_file, \"w\") as fout:\n",
        "        for lines in input_lines:\n",
        "            for line in lines:\n",
        "                fout.write(line)\n",
        "\n",
        "\n",
        "def process_hu_combined(paths, short_name):\n",
        "    assert short_name == \"hu_combined\"\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    rgai_input_path = get_rgai_input_path(paths)\n",
        "    nytk_input_path = get_nytk_input_path(paths)\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmp_output_path:\n",
        "        convert_rgai.convert_rgai(rgai_input_path, tmp_output_path, \"hu_rgai\", True, True)\n",
        "        convert_nytk.convert_nytk(nytk_input_path, tmp_output_path, \"hu_nytk\")\n",
        "\n",
        "        for shard in SHARDS:\n",
        "            rgai_input = os.path.join(tmp_output_path, \"hu_rgai.%s.bio\" % shard)\n",
        "            nytk_input = os.path.join(tmp_output_path, \"hu_nytk.%s.bio\" % shard)\n",
        "            output_file = os.path.join(base_output_path, \"hu_combined.%s.bio\" % shard)\n",
        "            concat_files(output_file, rgai_input, nytk_input)\n",
        "\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_bsnlp(paths, short_name):\n",
        "    \"\"\"\n",
        "    Process files downloaded from http://bsnlp.cs.helsinki.fi/bsnlp-2019/shared_task.html\n",
        "\n",
        "    If you download the training and test data zip files and unzip\n",
        "    them without rearranging in any way, the layout is somewhat weird.\n",
        "    Training data goes into a specific subdirectory, but the test data\n",
        "    goes into the top level directory.\n",
        "    \"\"\"\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"bsnlp2019\")\n",
        "    base_train_path = os.path.join(base_input_path, \"training_pl_cs_ru_bg_rc1\")\n",
        "    base_test_path = base_input_path\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    output_train_filename = os.path.join(base_output_path, \"%s.train.csv\" % short_name)\n",
        "    output_dev_filename   = os.path.join(base_output_path, \"%s.dev.csv\" % short_name)\n",
        "    output_test_filename  = os.path.join(base_output_path, \"%s.test.csv\" % short_name)\n",
        "\n",
        "    language = short_name.split(\"_\")[0]\n",
        "\n",
        "    convert_bsnlp.convert_bsnlp(language, base_test_path, output_test_filename)\n",
        "    convert_bsnlp.convert_bsnlp(language, base_train_path, output_train_filename, output_dev_filename)\n",
        "\n",
        "    for shard, csv_file in zip(SHARDS, (output_train_filename, output_dev_filename, output_test_filename)):\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(csv_file, output_filename)\n",
        "\n",
        "NCHLT_LANGUAGE_MAP = {\n",
        "    \"af\":  \"NCHLT Afrikaans Named Entity Annotated Corpus\",\n",
        "    # none of the following have UD datasets as of 2.8.  Until they\n",
        "    # exist, we assume the language codes NCHTL are sufficient\n",
        "    \"nr\":  \"NCHLT isiNdebele Named Entity Annotated Corpus\",\n",
        "    \"nso\": \"NCHLT Sepedi Named Entity Annotated Corpus\",\n",
        "    \"ss\":  \"NCHLT Siswati Named Entity Annotated Corpus\",\n",
        "    \"st\":  \"NCHLT Sesotho Named Entity Annotated Corpus\",\n",
        "    \"tn\":  \"NCHLT Setswana Named Entity Annotated Corpus\",\n",
        "    \"ts\":  \"NCHLT Xitsonga Named Entity Annotated Corpus\",\n",
        "    \"ve\":  \"NCHLT Tshivenda Named Entity Annotated Corpus\",\n",
        "    \"xh\":  \"NCHLT isiXhosa Named Entity Annotated Corpus\",\n",
        "    \"zu\":  \"NCHLT isiZulu Named Entity Annotated Corpus\",\n",
        "}\n",
        "\n",
        "def process_nchlt(paths, short_name):\n",
        "    language = short_name.split(\"_\")[0]\n",
        "    if not language in NCHLT_LANGUAGE_MAP:\n",
        "        raise UnknownDatasetError(short_name, \"Language %s not part of NCHLT\" % language)\n",
        "    short_name = \"%s_nchlt\" % language\n",
        "\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"NCHLT\", NCHLT_LANGUAGE_MAP[language], \"*Full.txt\")\n",
        "    input_files = glob.glob(base_input_path)\n",
        "    if len(input_files) == 0:\n",
        "        raise FileNotFoundError(\"Cannot find NCHLT dataset in '%s'  Did you remember to download the file?\" % base_input_path)\n",
        "\n",
        "    if len(input_files) > 1:\n",
        "        raise ValueError(\"Unexpected number of files matched '%s'  There should only be one\" % base_input_path)\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    split_wikiner(base_output_path, input_files[0], prefix=short_name, remap={\"OUT\": \"O\"})\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_my_ucsy(paths, short_name):\n",
        "    assert short_name == \"my_ucsy\"\n",
        "    language = \"my\"\n",
        "\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], short_name)\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    convert_my_ucsy.convert_my_ucsy(base_input_path, base_output_path)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_fa_arman(paths, short_name):\n",
        "    \"\"\"\n",
        "    Converts fa_arman dataset\n",
        "\n",
        "    The conversion is quite simple, actually.\n",
        "    Just need to split the train file and then convert bio -> json\n",
        "    \"\"\"\n",
        "    assert short_name == \"fa_arman\"\n",
        "    language = \"fa\"\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"PersianNER\")\n",
        "    train_input_file = os.path.join(base_input_path, \"train_fold1.txt\")\n",
        "    test_input_file = os.path.join(base_input_path, \"test_fold1.txt\")\n",
        "    if not os.path.exists(train_input_file) or not os.path.exists(test_input_file):\n",
        "        full_corpus_file = os.path.join(base_input_path, \"ArmanPersoNERCorpus.zip\")\n",
        "        if os.path.exists(full_corpus_file):\n",
        "            raise FileNotFoundError(\"Please unzip the file {}\".format(full_corpus_file))\n",
        "        raise FileNotFoundError(\"Cannot find the arman corpus in the expected directory: {}\".format(base_input_path))\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    test_output_file = os.path.join(base_output_path, \"%s.test.bio\" % short_name)\n",
        "\n",
        "    split_wikiner(base_output_path, train_input_file, prefix=short_name, train_fraction=0.8, test_section=False)\n",
        "    shutil.copy2(test_input_file, test_output_file)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_sv_suc3licensed(paths, short_name):\n",
        "    \"\"\"\n",
        "    The .zip provided for SUC3 includes train/dev/test splits already\n",
        "\n",
        "    This extracts those splits without needing to unzip the original file\n",
        "    \"\"\"\n",
        "    assert short_name == \"sv_suc3licensed\"\n",
        "    language = \"sv\"\n",
        "    train_input_file = os.path.join(paths[\"NERBASE\"], short_name, \"SUC3.0.zip\")\n",
        "    if not os.path.exists(train_input_file):\n",
        "        raise FileNotFoundError(\"Cannot find the officially licensed SUC3 dataset in %s\" % train_input_file)\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    suc_conll_to_iob.process_suc3(train_input_file, short_name, base_output_path)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_sv_suc3shuffle(paths, short_name):\n",
        "    \"\"\"\n",
        "    Uses an externally provided script to read the SUC3 XML file, then splits it\n",
        "    \"\"\"\n",
        "    assert short_name == \"sv_suc3shuffle\"\n",
        "    language = \"sv\"\n",
        "    train_input_file = os.path.join(paths[\"NERBASE\"], short_name, \"suc3.xml.bz2\")\n",
        "    if not os.path.exists(train_input_file):\n",
        "        train_input_file = train_input_file[:-4]\n",
        "    if not os.path.exists(train_input_file):\n",
        "        raise FileNotFoundError(\"Unable to find the SUC3 dataset in {}.bz2\".format(train_input_file))\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    train_output_file = os.path.join(base_output_path, \"sv_suc3shuffle.bio\")\n",
        "    suc_to_iob.main([train_input_file, train_output_file])\n",
        "    split_wikiner(base_output_path, train_output_file, prefix=short_name)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_da_ddt(paths, short_name):\n",
        "    \"\"\"\n",
        "    Processes Danish DDT dataset\n",
        "\n",
        "    This dataset is in a conll file with the \"name\" attribute in the\n",
        "    misc column for the NER tag.  This function uses a script to\n",
        "    convert such CoNLL files to .bio\n",
        "    \"\"\"\n",
        "    assert short_name == \"da_ddt\"\n",
        "    language = \"da\"\n",
        "    IN_FILES = (\"ddt.train.conllu\", \"ddt.dev.conllu\", \"ddt.test.conllu\")\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    OUT_FILES = [os.path.join(base_output_path, \"%s.%s.bio\" % (short_name, shard)) for shard in SHARDS]\n",
        "\n",
        "    zip_file = os.path.join(paths[\"NERBASE\"], \"da_ddt\", \"ddt.zip\")\n",
        "    if os.path.exists(zip_file):\n",
        "        for in_filename, out_filename, shard in zip(IN_FILES, OUT_FILES, SHARDS):\n",
        "            conll_to_iob.process_conll(in_filename, out_filename, zip_file)\n",
        "    else:\n",
        "        for in_filename, out_filename, shard in zip(IN_FILES, OUT_FILES, SHARDS):\n",
        "            in_filename = os.path.join(paths[\"NERBASE\"], \"da_ddt\", in_filename)\n",
        "            if not os.path.exists(in_filename):\n",
        "                raise FileNotFoundError(\"Could not find zip in expected location %s and could not file %s file in %s\" % (zip_file, shard, in_filename))\n",
        "\n",
        "            conll_to_iob.process_conll(in_filename, out_filename)\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "\n",
        "def process_norne(paths, short_name):\n",
        "    \"\"\"\n",
        "    Processes Norwegian NorNE\n",
        "\n",
        "    Can handle either Bokmål or Nynorsk\n",
        "\n",
        "    Converts GPE_LOC and GPE_ORG to GPE\n",
        "    \"\"\"\n",
        "    language, name = short_name.split(\"_\", 1)\n",
        "    assert language in ('nb', 'nn')\n",
        "    assert name == 'norne'\n",
        "\n",
        "    if language == 'nb':\n",
        "        IN_FILES = (\"nob/no_bokmaal-ud-train.conllu\", \"nob/no_bokmaal-ud-dev.conllu\", \"nob/no_bokmaal-ud-test.conllu\")\n",
        "    else:\n",
        "        IN_FILES = (\"nno/no_nynorsk-ud-train.conllu\", \"nno/no_nynorsk-ud-dev.conllu\", \"nno/no_nynorsk-ud-test.conllu\")\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    OUT_FILES = [os.path.join(base_output_path, \"%s.%s.bio\" % (short_name, shard)) for shard in SHARDS]\n",
        "\n",
        "    CONVERSION = { \"GPE_LOC\": \"GPE\", \"GPE_ORG\": \"GPE\" }\n",
        "\n",
        "    for in_filename, out_filename, shard in zip(IN_FILES, OUT_FILES, SHARDS):\n",
        "        in_filename = os.path.join(paths[\"NERBASE\"], \"norne\", \"ud\", in_filename)\n",
        "        if not os.path.exists(in_filename):\n",
        "            raise FileNotFoundError(\"Could not find %s file in %s\" % (shard, in_filename))\n",
        "\n",
        "        conll_to_iob.process_conll(in_filename, out_filename, conversion=CONVERSION)\n",
        "\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_ja_gsd(paths, short_name):\n",
        "    \"\"\"\n",
        "    Convert ja_gsd from MegagonLabs\n",
        "\n",
        "    for example, can download from https://github.com/megagonlabs/UD_Japanese-GSD/releases/tag/r2.9-NE\n",
        "    \"\"\"\n",
        "    language, name = short_name.split(\"_\", 1)\n",
        "    assert language == 'ja'\n",
        "    assert name == 'gsd'\n",
        "\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    output_files = [os.path.join(base_output_path, \"%s.%s.bio\" % (short_name, shard)) for shard in SHARDS]\n",
        "\n",
        "    search_path = os.path.join(paths[\"NERBASE\"], \"ja_gsd\", \"UD_Japanese-GSD-r2.*-NE\")\n",
        "    versions = glob.glob(search_path)\n",
        "    max_version = None\n",
        "    base_input_path = None\n",
        "    version_re = re.compile(\"GSD-r2.([0-9]+)-NE$\")\n",
        "\n",
        "    for ver in versions:\n",
        "        match = version_re.search(ver)\n",
        "        if not match:\n",
        "            continue\n",
        "        ver_num = int(match.groups(1)[0])\n",
        "        if max_version is None or ver_num > max_version:\n",
        "            max_version = ver_num\n",
        "            base_input_path = ver\n",
        "\n",
        "    if base_input_path is None:\n",
        "        raise FileNotFoundError(\"Could not find any copies of the NE conversion of ja_gsd here: {}\".format(search_path))\n",
        "    print(\"Most recent version found: {}\".format(base_input_path))\n",
        "\n",
        "    input_files = [\"ja_gsd-ud-train.ne.conllu\", \"ja_gsd-ud-dev.ne.conllu\", \"ja_gsd-ud-test.ne.conllu\"]\n",
        "\n",
        "    def conversion(x):\n",
        "        if x[0] == 'L':\n",
        "            return 'E' + x[1:]\n",
        "        if x[0] == 'U':\n",
        "            return 'S' + x[1:]\n",
        "        # B, I unchanged\n",
        "        return x\n",
        "\n",
        "    for in_filename, out_filename, shard in zip(input_files, output_files, SHARDS):\n",
        "        in_path = os.path.join(base_input_path, in_filename)\n",
        "        if not os.path.exists(in_path):\n",
        "            in_spacy = os.path.join(base_input_path, \"spacy\", in_filename)\n",
        "            if not os.path.exists(in_spacy):\n",
        "                raise FileNotFoundError(\"Could not find %s file in %s or %s\" % (shard, in_path, in_spacy))\n",
        "            in_path = in_spacy\n",
        "\n",
        "        conll_to_iob.process_conll(in_path, out_filename, conversion=conversion, allow_empty=True, attr_prefix=\"NE\")\n",
        "\n",
        "    convert_bio_to_json(base_output_path, base_output_path, short_name)\n",
        "\n",
        "def process_starlang(paths, short_name):\n",
        "    \"\"\"\n",
        "    Process a Turkish dataset from Starlang\n",
        "    \"\"\"\n",
        "    assert short_name == 'tr_starlang'\n",
        "\n",
        "    PIECES = [\"TurkishAnnotatedTreeBank-15\",\n",
        "              \"TurkishAnnotatedTreeBank2-15\",\n",
        "              \"TurkishAnnotatedTreeBank2-20\"]\n",
        "\n",
        "    chunk_paths = [os.path.join(paths[\"CONSTITUENCY_BASE\"], \"turkish\", piece) for piece in PIECES]\n",
        "    datasets = convert_starlang_ner.read_starlang(chunk_paths)\n",
        "\n",
        "    write_dataset(datasets, paths[\"NER_DATA_DIR\"], short_name)\n",
        "\n",
        "def remap_germeval_tag(tag):\n",
        "    \"\"\"\n",
        "    Simplify tags for GermEval2014 using a simple rubric\n",
        "\n",
        "    all tags become their parent tag\n",
        "    OTH becomes MISC\n",
        "    \"\"\"\n",
        "    if tag == \"O\":\n",
        "        return tag\n",
        "    if tag[1:5] == \"-LOC\":\n",
        "        return tag[:5]\n",
        "    if tag[1:5] == \"-PER\":\n",
        "        return tag[:5]\n",
        "    if tag[1:5] == \"-ORG\":\n",
        "        return tag[:5]\n",
        "    if tag[1:5] == \"-OTH\":\n",
        "        return tag[0] + \"-MISC\"\n",
        "    raise ValueError(\"Unexpected tag: %s\" % tag)\n",
        "\n",
        "def process_de_germeval2014(paths, short_name):\n",
        "    \"\"\"\n",
        "    Process the TSV of the GermEval2014 dataset\n",
        "    \"\"\"\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"germeval2014\")\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    datasets = []\n",
        "    for shard in SHARDS:\n",
        "        in_file = os.path.join(in_directory, \"NER-de-%s.tsv\" % shard)\n",
        "        sentences = read_tsv(in_file, 1, 2, remap_fn=remap_germeval_tag)\n",
        "        datasets.append(sentences)\n",
        "    tags = get_tags(datasets)\n",
        "    print(\"Found the following tags: {}\".format(sorted(tags)))\n",
        "    write_dataset(datasets, base_output_path, short_name)\n",
        "\n",
        "def process_hiner(paths, short_name):\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"hindi\", \"HiNER\", \"data\", \"original\")\n",
        "    convert_bio_to_json(in_directory, paths[\"NER_DATA_DIR\"], short_name, suffix=\"conll\", shard_names=(\"train\", \"validation\", \"test\"))\n",
        "\n",
        "def process_hinercollapsed(paths, short_name):\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"hindi\", \"HiNER\", \"data\", \"collapsed\")\n",
        "    convert_bio_to_json(in_directory, paths[\"NER_DATA_DIR\"], short_name, suffix=\"conll\", shard_names=(\"train\", \"validation\", \"test\"))\n",
        "\n",
        "def process_lst20(paths, short_name, include_space_char=True):\n",
        "    convert_lst20.convert_lst20(paths, short_name, include_space_char)\n",
        "\n",
        "def process_nner22(paths, short_name, include_space_char=True):\n",
        "    convert_nner22.convert_nner22(paths, short_name, include_space_char)\n",
        "\n",
        "def process_mr_l3cube(paths, short_name):\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"marathi\", \"MarathiNLP\", \"L3Cube-MahaNER\", \"IOB\")\n",
        "    input_files = [\"train_iob.txt\", \"valid_iob.txt\", \"test_iob.txt\"]\n",
        "    input_files = [os.path.join(in_directory, x) for x in input_files]\n",
        "    for input_file in input_files:\n",
        "        if not os.path.exists(input_file):\n",
        "            raise FileNotFoundError(\"Could not find the expected piece of the l3cube dataset %s\" % input_file)\n",
        "\n",
        "    datasets = [convert_mr_l3cube.convert(input_file) for input_file in input_files]\n",
        "    write_dataset(datasets, base_output_path, short_name)\n",
        "\n",
        "def process_bn_daffodil(paths, short_name):\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"bangla\", \"Bengali-NER\")\n",
        "    out_directory = paths[\"NER_DATA_DIR\"]\n",
        "    convert_bn_daffodil.convert_dataset(in_directory, out_directory)\n",
        "\n",
        "def process_pl_nkjp(paths, short_name):\n",
        "    out_directory = paths[\"NER_DATA_DIR\"]\n",
        "    candidates = [os.path.join(paths[\"NERBASE\"], \"Polish-NKJP\"),\n",
        "                  os.path.join(paths[\"NERBASE\"], \"polish\", \"Polish-NKJP\"),\n",
        "                  os.path.join(paths[\"NERBASE\"], \"polish\", \"NKJP-PodkorpusMilionowy-1.2.tar.gz\"),]\n",
        "    for in_path in candidates:\n",
        "        if os.path.exists(in_path):\n",
        "            break\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Could not find %s  Looked in %s\" % (short_name, \" \".join(candidates)))\n",
        "    convert_nkjp.convert_nkjp(in_path, out_directory)\n",
        "\n",
        "def process_kk_kazNERD(paths, short_name):\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"kazakh\", \"KazNERD\", \"KazNERD\")\n",
        "    out_directory = paths[\"NER_DATA_DIR\"]\n",
        "    convert_kk_kazNERD.convert_dataset(in_directory, out_directory, short_name)\n",
        "\n",
        "def process_masakhane(paths, dataset_name):\n",
        "    \"\"\"\n",
        "    Converts Masakhane NER datasets to Stanza's .json format\n",
        "\n",
        "    If we let N be the length of the first sentence, the NER files\n",
        "    (in version 2, at least) are all of the form\n",
        "\n",
        "    word tag\n",
        "    ...\n",
        "    word tag\n",
        "      (blank line for sentence break)\n",
        "    word tag\n",
        "    ...\n",
        "\n",
        "    Once the dataset is git cloned in $NERBASE, the directory structure is\n",
        "\n",
        "    $NERBASE/masakhane-ner/MasakhaNER2.0/data/$lcode/{train,dev,test}.txt\n",
        "\n",
        "    The only tricky thing here is that for some languages, we treat\n",
        "    the 2 letter lcode as canonical thanks to UD, but Masakhane NER\n",
        "    uses 3 letter lcodes for all languages.\n",
        "    \"\"\"\n",
        "    language, dataset = dataset_name.split(\"_\")\n",
        "    lcode = lang_to_langcode(language)\n",
        "    if lcode in two_to_three_letters:\n",
        "        masakhane_lcode = two_to_three_letters[lcode]\n",
        "    else:\n",
        "        masakhane_lcode = lcode\n",
        "\n",
        "    mn_directory = os.path.join(paths[\"NERBASE\"], \"masakhane-ner\")\n",
        "    if not os.path.exists(mn_directory):\n",
        "        raise FileNotFoundError(\"Cannot find Masakhane NER repo.  Please check the setting of NERBASE or clone the repo to %s\" % mn_directory)\n",
        "    data_directory = os.path.join(mn_directory, \"MasakhaNER2.0\", \"data\")\n",
        "    if not os.path.exists(data_directory):\n",
        "        raise FileNotFoundError(\"Apparently found the repo at %s but the expected directory structure is not there - was looking for %s\" % (mn_directory, data_directory))\n",
        "\n",
        "    in_directory = os.path.join(data_directory, masakhane_lcode)\n",
        "    if not os.path.exists(in_directory):\n",
        "        raise UnknownDatasetError(dataset_name, \"Found the Masakhane repo, but there was no %s in the repo at path %s\" % (dataset_name, in_directory))\n",
        "    convert_bio_to_json(in_directory, paths[\"NER_DATA_DIR\"], \"%s_masakhane\" % lcode, \"txt\")\n",
        "\n",
        "def process_sd_siner(paths, short_name):\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"sindhi\", \"SiNER-dataset\")\n",
        "    if not os.path.exists(in_directory):\n",
        "        raise FileNotFoundError(\"Cannot find SiNER checkout in $NERBASE/sindhi  Please git clone to repo in that directory\")\n",
        "    in_filename = os.path.join(in_directory, \"SiNER-dataset.txt\")\n",
        "    if not os.path.exists(in_filename):\n",
        "        in_filename = os.path.join(in_directory, \"SiNER dataset.txt\")\n",
        "        if not os.path.exists(in_filename):\n",
        "            raise FileNotFoundError(\"Found an SiNER directory at %s but the directory did not contain the dataset\" % in_directory)\n",
        "    convert_sindhi_siner.convert_sindhi_siner(in_filename, paths[\"NER_DATA_DIR\"], short_name)\n",
        "\n",
        "def process_en_worldwide_4class(paths, short_name):\n",
        "    simplify_en_worldwide.main(args=['--simplify'])\n",
        "\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"en_worldwide\", \"4class\")\n",
        "    out_directory = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    destination_file = os.path.join(paths[\"NERBASE\"], \"en_worldwide\", \"en-worldwide-newswire\", \"regions.txt\")\n",
        "    prefix_map = read_prefix_file(destination_file)\n",
        "\n",
        "    random_shuffle_by_prefixes(in_directory, out_directory, short_name, prefix_map)\n",
        "\n",
        "def process_en_worldwide_9class(paths, short_name):\n",
        "    simplify_en_worldwide.main(args=['--no_simplify'])\n",
        "\n",
        "    in_directory = os.path.join(paths[\"NERBASE\"], \"en_worldwide\", \"9class\")\n",
        "    out_directory = paths[\"NER_DATA_DIR\"]\n",
        "\n",
        "    destination_file = os.path.join(paths[\"NERBASE\"], \"en_worldwide\", \"en-worldwide-newswire\", \"regions.txt\")\n",
        "    prefix_map = read_prefix_file(destination_file)\n",
        "\n",
        "    random_shuffle_by_prefixes(in_directory, out_directory, short_name, prefix_map)\n",
        "\n",
        "def process_en_ontonotes(paths, short_name):\n",
        "    ner_input_path = paths['NERBASE']\n",
        "    ontonotes_path = os.path.join(ner_input_path, \"english\", \"en_ontonotes\")\n",
        "    ner_output_path = paths['NER_DATA_DIR']\n",
        "    convert_ontonotes.process_dataset(\"en_ontonotes\", ontonotes_path, ner_output_path)\n",
        "\n",
        "def process_zh_ontonotes(paths, short_name):\n",
        "    ner_input_path = paths['NERBASE']\n",
        "    ontonotes_path = os.path.join(ner_input_path, \"chinese\", \"zh_ontonotes\")\n",
        "    ner_output_path = paths['NER_DATA_DIR']\n",
        "    convert_ontonotes.process_dataset(short_name, ontonotes_path, ner_output_path)\n",
        "\n",
        "def process_en_conll03(paths, short_name):\n",
        "    ner_input_path = paths['NERBASE']\n",
        "    conll_path = os.path.join(ner_input_path, \"english\", \"en_conll03\")\n",
        "    ner_output_path = paths['NER_DATA_DIR']\n",
        "    convert_en_conll03.process_dataset(\"en_conll03\", conll_path, ner_output_path)\n",
        "\n",
        "def process_en_conll03_worldwide(paths, short_name):\n",
        "    \"\"\"\n",
        "    Adds the training data for conll03 and worldwide together\n",
        "    \"\"\"\n",
        "    print(\"============== Preparing CoNLL 2003 ===================\")\n",
        "    process_en_conll03(paths, \"en_conll03\")\n",
        "    print(\"========== Preparing 4 Class Worldwide ================\")\n",
        "    process_en_worldwide_4class(paths, \"en_worldwide-4class\")\n",
        "    print(\"============== Combined Train Data ====================\")\n",
        "    input_files = [os.path.join(paths['NER_DATA_DIR'], \"en_conll03.train.json\"),\n",
        "                   os.path.join(paths['NER_DATA_DIR'], \"en_worldwide-4class.train.json\")]\n",
        "    output_file = os.path.join(paths['NER_DATA_DIR'], \"%s.train.json\" % short_name)\n",
        "    combine_files(output_file, *input_files)\n",
        "    shutil.copyfile(os.path.join(paths['NER_DATA_DIR'], \"en_conll03.dev.json\"),\n",
        "                    os.path.join(paths['NER_DATA_DIR'], \"%s.dev.json\" % short_name))\n",
        "    shutil.copyfile(os.path.join(paths['NER_DATA_DIR'], \"en_conll03.test.json\"),\n",
        "                    os.path.join(paths['NER_DATA_DIR'], \"%s.test.json\" % short_name))\n",
        "\n",
        "def process_en_ontonotes_ww_multi(paths, short_name):\n",
        "    \"\"\"\n",
        "    Combine the worldwide data with the OntoNotes data in a multi channel format\n",
        "    \"\"\"\n",
        "    print(\"=============== Preparing OntoNotes ===============\")\n",
        "    process_en_ontonotes(paths, \"en_ontonotes\")\n",
        "    print(\"========== Preparing 9 Class Worldwide ================\")\n",
        "    process_en_worldwide_9class(paths, \"en_worldwide-9class\")\n",
        "    # TODO: pass in options?\n",
        "    ontonotes_multitag.build_multitag_dataset(paths['NER_DATA_DIR'], short_name, True, True)\n",
        "\n",
        "def process_en_combined(paths, short_name):\n",
        "    \"\"\"\n",
        "    Combine WW, OntoNotes, and CoNLL into a 3 channel dataset\n",
        "    \"\"\"\n",
        "    print(\"================= Preparing OntoNotes =================\")\n",
        "    process_en_ontonotes(paths, \"en_ontonotes\")\n",
        "    print(\"========== Preparing 9 Class Worldwide ================\")\n",
        "    process_en_worldwide_9class(paths, \"en_worldwide-9class\")\n",
        "    print(\"=============== Preparing CoNLL 03 ====================\")\n",
        "    process_en_conll03(paths, \"en_conll03\")\n",
        "    build_en_combined.build_combined_dataset(paths['NER_DATA_DIR'], short_name)\n",
        "\n",
        "\n",
        "def process_en_conllpp(paths, short_name):\n",
        "    \"\"\"\n",
        "    This is ONLY a test set\n",
        "\n",
        "    the test set has entities start with I- instead of B- unless they\n",
        "    are in the middle of a sentence, but that should be find, as\n",
        "    process_tags in the NER model converts those to B- in a BIOES\n",
        "    conversion\n",
        "    \"\"\"\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"acl2023_conllpp\", \"dataset\", \"conllpp.txt\")\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    sentences = read_tsv(base_input_path, 0, 3, separator=None)\n",
        "    sentences = [sent for sent in sentences if len(sent) > 1 or sent[0][0] != '-DOCSTART-']\n",
        "    write_dataset([sentences], base_output_path, short_name, shard_names=[\"test\"], shards=[\"test\"])\n",
        "\n",
        "def process_armtdp(paths, short_name):\n",
        "    assert short_name == 'hy_armtdp'\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"armenian\", \"ArmTDP-NER\")\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    convert_hy_armtdp.convert_dataset(base_input_path, base_output_path, short_name)\n",
        "    for shard in SHARDS:\n",
        "        input_filename = os.path.join(base_output_path, f'{short_name}.{shard}.tsv')\n",
        "        if not os.path.exists(input_filename):\n",
        "            raise FileNotFoundError('Cannot find %s component of %s in %s' % (shard, short_name, input_filename))\n",
        "        output_filename = os.path.join(base_output_path, '%s.%s.json' % (short_name, shard))\n",
        "        prepare_ner_file.process_dataset(input_filename, output_filename)\n",
        "\n",
        "def process_toy_dataset(paths, short_name):\n",
        "    convert_bio_to_json(os.path.join(paths[\"NERBASE\"], \"English-SAMPLE\"), paths[\"NER_DATA_DIR\"], short_name)\n",
        "\n",
        "def process_ar_aqmar(paths, short_name):\n",
        "    base_input_path = os.path.join(paths[\"NERBASE\"], \"arabic\", \"AQMAR\", \"AQMAR_Arabic_NER_corpus-1.0.zip\")\n",
        "    base_output_path = paths[\"NER_DATA_DIR\"]\n",
        "    convert_ar_aqmar.convert_shuffle(base_input_path, base_output_path, short_name)\n",
        "\n",
        "def process_en_cell(paths, short_name):\n",
        "    # in_directory = os.path.join(paths[\"NERBASE\"], \"bangla\", \"Bengali-NER\")\n",
        "    in_directory = \"/content/stanza/data/\"\n",
        "    out_directory = \"/content/stanza/data/ner\"\n",
        "    convert_bn_daffodil.convert_dataset(in_directory, out_directory)\n",
        "\n",
        "DATASET_MAPPING = {\n",
        "    \"ar_aqmar\":          process_ar_aqmar,\n",
        "    \"bn_daffodil\":       process_bn_daffodil,\n",
        "    \"da_ddt\":            process_da_ddt,\n",
        "    \"de_germeval2014\":   process_de_germeval2014,\n",
        "    \"en_conll03\":        process_en_conll03,\n",
        "    \"en_conll03ww\":      process_en_conll03_worldwide,\n",
        "    \"en_conllpp\":        process_en_conllpp,\n",
        "    \"en_ontonotes\":      process_en_ontonotes,\n",
        "    \"en_ontonotes-ww-multi\": process_en_ontonotes_ww_multi,\n",
        "    \"en_combined\":       process_en_combined,\n",
        "    \"en_worldwide-4class\": process_en_worldwide_4class,\n",
        "    \"en_worldwide-9class\": process_en_worldwide_9class,\n",
        "    \"fa_arman\":          process_fa_arman,\n",
        "    \"fi_turku\":          process_turku,\n",
        "    \"hi_hiner\":          process_hiner,\n",
        "    \"hi_hinercollapsed\": process_hinercollapsed,\n",
        "    \"hi_ijc\":            process_ijc,\n",
        "    \"hu_nytk\":           process_nytk,\n",
        "    \"hu_combined\":       process_hu_combined,\n",
        "    \"hy_armtdp\":         process_armtdp,\n",
        "    \"it_fbk\":            process_it_fbk,\n",
        "    \"ja_gsd\":            process_ja_gsd,\n",
        "    \"kk_kazNERD\":        process_kk_kazNERD,\n",
        "    \"mr_l3cube\":         process_mr_l3cube,\n",
        "    \"my_ucsy\":           process_my_ucsy,\n",
        "    \"pl_nkjp\":           process_pl_nkjp,\n",
        "    \"sd_siner\":          process_sd_siner,\n",
        "    \"sv_suc3licensed\":   process_sv_suc3licensed,\n",
        "    \"sv_suc3shuffle\":    process_sv_suc3shuffle,\n",
        "    \"tr_starlang\":       process_starlang,\n",
        "    \"th_lst20\":          process_lst20,\n",
        "    \"th_nner22\":         process_nner22,\n",
        "    \"zh-hans_ontonotes\": process_zh_ontonotes,\n",
        "    \"en_cell\":           process_en_cell,\n",
        "}\n",
        "\n",
        "def main(dataset_name):\n",
        "    paths = default_paths.get_default_paths()\n",
        "    print(\"Processing %s\" % dataset_name)\n",
        "\n",
        "    random.seed(1234)\n",
        "\n",
        "    if dataset_name in DATASET_MAPPING:\n",
        "        DATASET_MAPPING[dataset_name](paths, dataset_name)\n",
        "    elif dataset_name in ('uk_languk', 'Ukranian_languk', 'Ukranian-languk'):\n",
        "        process_languk(paths, dataset_name)\n",
        "    elif dataset_name.endswith(\"FIRE2013\") or dataset_name.endswith(\"fire2013\"):\n",
        "        process_fire_2013(paths, dataset_name)\n",
        "    elif dataset_name.endswith('WikiNER'):\n",
        "        process_wikiner(paths, dataset_name)\n",
        "    elif dataset_name.startswith('hu_rgai'):\n",
        "        process_rgai(paths, dataset_name)\n",
        "    elif dataset_name.endswith(\"_bsnlp19\"):\n",
        "        process_bsnlp(paths, dataset_name)\n",
        "    elif dataset_name.endswith(\"_nchlt\"):\n",
        "        process_nchlt(paths, dataset_name)\n",
        "    elif dataset_name in (\"nb_norne\", \"nn_norne\"):\n",
        "        process_norne(paths, dataset_name)\n",
        "    elif dataset_name == 'en_sample':\n",
        "        process_toy_dataset(paths, dataset_name)\n",
        "    elif dataset_name.lower().endswith(\"_masakhane\"):\n",
        "        process_masakhane(paths, dataset_name)\n",
        "    else:\n",
        "        raise UnknownDatasetError(dataset_name, f\"dataset {dataset_name} currently not handled by prepare_ner_dataset\")\n",
        "    print(\"Done processing %s\" % dataset_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(sys.argv[1])\n"
      ],
      "metadata": {
        "id": "dpP4KRXRr-PZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b0204c-b01a-4b6a-bb75-430285dda7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting stanza/utils/datasets/ner/prepare_ner_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the BIO files to a Stanza input format."
      ],
      "metadata": {
        "id": "H4sBU7Z6q2vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 stanza/utils/datasets/ner/prepare_ner_dataset.py en_cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07c2maH7hvz-",
        "outputId": "79306859-fdb4-4e3b-f90e-c3605a28dccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing en_cell\n",
            "Converting /content/stanza/data/ner/en_cell.train.bio to /content/stanza/data/ner/en_cell.train.json\n",
            "1426 examples loaded from /content/stanza/data/ner/en_cell.train.bio\n",
            "Generated json file /content/stanza/data/ner/en_cell.train.json\n",
            "Converting /content/stanza/data/ner/en_cell.dev.bio to /content/stanza/data/ner/en_cell.dev.json\n",
            "159 examples loaded from /content/stanza/data/ner/en_cell.dev.bio\n",
            "Generated json file /content/stanza/data/ner/en_cell.dev.json\n",
            "Converting /content/stanza/data/ner/en_cell.test.bio to /content/stanza/data/ner/en_cell.test.json\n",
            "344 examples loaded from /content/stanza/data/ner/en_cell.test.bio\n",
            "Generated json file /content/stanza/data/ner/en_cell.test.json\n",
            "Done processing en_cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/stanza/stanza/utils/training/run_ner.py\n",
        "\"\"\"\n",
        "Trains or scores an NER model.\n",
        "\n",
        "Will attempt to guess the appropriate word vector file if none is\n",
        "specified, and will use the charlms specified in the resources\n",
        "for a given dataset or language if possible.\n",
        "\n",
        "Example command line:\n",
        "  python3 -m stanza.utils.training.run_ner.py hu_combined\n",
        "\n",
        "This script expects the prepared data to be in\n",
        "  data/ner/{lang}_{dataset}.train.json, {lang}_{dataset}.dev.json, {lang}_{dataset}.test.json\n",
        "\n",
        "If those files don't exist, it will make an attempt to rebuild them\n",
        "using the prepare_ner_dataset script.  However, this will fail if the\n",
        "data is not already downloaded.  More information on where to find\n",
        "most of the datasets online is in that script.  Some of the datasets\n",
        "have licenses which must be agreed to, so no attempt is made to\n",
        "automatically download the data.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from stanza.models import ner_tagger\n",
        "from stanza.resources.common import DEFAULT_MODEL_DIR\n",
        "from stanza.utils.datasets.ner import prepare_ner_dataset\n",
        "from stanza.utils.training import common\n",
        "from stanza.utils.training.common import Mode, add_charlm_args, build_charlm_args, choose_charlm, find_wordvec_pretrain\n",
        "\n",
        "from stanza.resources.default_packages import default_charlms, default_pretrains, ner_charlms, ner_pretrains\n",
        "\n",
        "# extra arguments specific to a particular dataset\n",
        "DATASET_EXTRA_ARGS = {\n",
        "    \"da_ddt\":   [ \"--dropout\", \"0.6\" ],\n",
        "    \"fa_arman\": [ \"--dropout\", \"0.6\" ],\n",
        "    \"vi_vlsp\":  [ \"--dropout\", \"0.6\",\n",
        "                  \"--word_dropout\", \"0.1\",\n",
        "                  \"--locked_dropout\", \"0.1\",\n",
        "                  \"--char_dropout\", \"0.1\" ],\n",
        "    \"en_cell\":   [ \"--max_steps\", \"200\",\n",
        "                   \"--batch_size\", \"4\"],\n",
        "}\n",
        "\n",
        "logger = logging.getLogger('stanza')\n",
        "\n",
        "def add_ner_args(parser):\n",
        "    add_charlm_args(parser)\n",
        "\n",
        "    parser.add_argument('--use_bert', default=False, action=\"store_true\", help='Use the default transformer for this language')\n",
        "\n",
        "\n",
        "def build_pretrain_args(language, dataset, charlm=\"default\", command_args=None, extra_args=None, model_dir=DEFAULT_MODEL_DIR):\n",
        "    \"\"\"\n",
        "    Returns one list with the args for this language & dataset's charlm and pretrained embedding\n",
        "    \"\"\"\n",
        "    charlm = choose_charlm(language, dataset, charlm, default_charlms, ner_charlms)\n",
        "    charlm_args = build_charlm_args(language, charlm, model_dir=model_dir)\n",
        "\n",
        "    wordvec_args = []\n",
        "    if extra_args is None or '--wordvec_pretrain_file' not in extra_args:\n",
        "        # will throw an error if the pretrain can't be found\n",
        "        wordvec_pretrain = find_wordvec_pretrain(language, default_pretrains, ner_pretrains, dataset, model_dir=model_dir)\n",
        "        wordvec_args = ['--wordvec_pretrain_file', wordvec_pretrain]\n",
        "\n",
        "    bert_args = common.choose_transformer(language, command_args, extra_args, warn=False)\n",
        "\n",
        "    return charlm_args + wordvec_args + bert_args\n",
        "\n",
        "\n",
        "# TODO: refactor?  tagger and depparse should be pretty similar\n",
        "def build_model_filename(paths, short_name, command_args, extra_args):\n",
        "    short_language, dataset = short_name.split(\"_\", 1)\n",
        "\n",
        "    # TODO: can avoid downloading the charlm at this point, since we\n",
        "    # might not even be training\n",
        "    pretrain_args = build_pretrain_args(short_language, dataset, command_args.charlm, command_args, extra_args)\n",
        "\n",
        "    dataset_args = DATASET_EXTRA_ARGS.get(short_name, [])\n",
        "\n",
        "    train_args = [\"--shorthand\", short_name,\n",
        "                  \"--mode\", \"train\"]\n",
        "    train_args = train_args + pretrain_args + dataset_args + extra_args\n",
        "    if command_args.save_name is not None:\n",
        "        train_args.extend([\"--save_name\", command_args.save_name])\n",
        "    if command_args.save_dir is not None:\n",
        "        train_args.extend([\"--save_dir\", command_args.save_dir])\n",
        "    args = ner_tagger.parse_args(train_args)\n",
        "    save_name = ner_tagger.model_file_name(args)\n",
        "    return save_name\n",
        "\n",
        "\n",
        "# Technically NER datasets are not necessarily treebanks\n",
        "# (usually not, in fact)\n",
        "# However, to keep the naming consistent, we leave the\n",
        "# method which does the training as run_treebank\n",
        "# TODO: rename treebank -> dataset everywhere\n",
        "def run_treebank(mode, paths, treebank, short_name,\n",
        "                 temp_output_file, command_args, extra_args):\n",
        "    ner_dir = paths[\"NER_DATA_DIR\"]\n",
        "    language, dataset = short_name.split(\"_\")\n",
        "\n",
        "    train_file = os.path.join(ner_dir, f\"{treebank}.train.json\")\n",
        "    dev_file   = os.path.join(ner_dir, f\"{treebank}.dev.json\")\n",
        "    test_file  = os.path.join(ner_dir, f\"{treebank}.test.json\")\n",
        "\n",
        "    # if any files are missing, try to rebuild the dataset\n",
        "    # if that still doesn't work, we have to throw an error\n",
        "    missing_file = [x for x in (train_file, dev_file, test_file) if not os.path.exists(x)]\n",
        "    if len(missing_file) > 0:\n",
        "        logger.warning(f\"The data for {treebank} is missing or incomplete.  Cannot find {missing_file}  Attempting to rebuild...\")\n",
        "        try:\n",
        "            prepare_ner_dataset.main(treebank)\n",
        "        except Exception as e:\n",
        "            raise FileNotFoundError(f\"An exception occurred while trying to build the data for {treebank}  At least one portion of the data was missing: {missing_file}  Please correctly build these files and then try again.\") from e\n",
        "\n",
        "    pretrain_args = build_pretrain_args(language, dataset, command_args.charlm, command_args, extra_args)\n",
        "\n",
        "    if mode == Mode.TRAIN:\n",
        "        # VI example arguments:\n",
        "        #   --wordvec_pretrain_file ~/stanza_resources/vi/pretrain/vtb.pt\n",
        "        #   --train_file data/ner/vi_vlsp.train.json\n",
        "        #   --eval_file data/ner/vi_vlsp.dev.json\n",
        "        #   --lang vi\n",
        "        #   --shorthand vi_vlsp\n",
        "        #   --mode train\n",
        "        #   --charlm --charlm_shorthand vi_conll17\n",
        "        #   --dropout 0.6 --word_dropout 0.1 --locked_dropout 0.1 --char_dropout 0.1\n",
        "        dataset_args = DATASET_EXTRA_ARGS.get(short_name, [])\n",
        "\n",
        "        train_args = ['--train_file', train_file,\n",
        "                      '--eval_file', dev_file,\n",
        "                      '--shorthand', short_name,\n",
        "                      '--mode', 'train']\n",
        "        train_args = train_args + pretrain_args + dataset_args + extra_args\n",
        "        logger.info(\"Running train step with args: {}\".format(train_args))\n",
        "        ner_tagger.main(train_args)\n",
        "\n",
        "    if mode == Mode.SCORE_DEV or mode == Mode.TRAIN:\n",
        "        dev_args = ['--eval_file', dev_file,\n",
        "                    '--shorthand', short_name,\n",
        "                    '--mode', 'predict']\n",
        "        dev_args = dev_args + pretrain_args + extra_args\n",
        "        logger.info(\"Running dev step with args: {}\".format(dev_args))\n",
        "        ner_tagger.main(dev_args)\n",
        "\n",
        "    if mode == Mode.SCORE_TEST or mode == Mode.TRAIN:\n",
        "        test_args = ['--eval_file', test_file,\n",
        "                     '--shorthand', short_name,\n",
        "                     '--mode', 'predict']\n",
        "        test_args = test_args + pretrain_args + extra_args\n",
        "        logger.info(\"Running test step with args: {}\".format(test_args))\n",
        "        ner_tagger.main(test_args)\n",
        "\n",
        "\n",
        "def main():\n",
        "    common.main(run_treebank, \"ner\", \"nertagger\", add_ner_args, ner_tagger.build_argparse(), build_model_filename=build_model_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pco0AZhStKCn",
        "outputId": "08966217-6333-48dc-ac66-3f21efaf19b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/stanza/stanza/utils/training/run_ner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the NER training component and evluate the results on the test set.\n",
        "Given the amount of data used in training and the computational resources on Colab. I set the training hyper-parameter as follows:\n",
        "*   batch size = 4\n",
        "*   max step = 200\n",
        "\n",
        "We can see that the model perforamce is very low. We can process more data, adjust the hyper-parameters if more computational resources are available."
      ],
      "metadata": {
        "id": "-d5ll-4Fq_j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m stanza.utils.training.run_ner en_cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoCRPZFHm4qe",
        "outputId": "f7ea5e13-f972-41bc-95aa-402260c85b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-28 01:33:44 INFO: Training program called with:\n",
            "/content/stanza/stanza/utils/training/run_ner.py en_cell\n",
            "2024-02-28 01:33:44 DEBUG: en_cell: en_cell\n",
            "2024-02-28 01:33:44 INFO: Using model /root/stanza_resources/en/forward_charlm/1billion.pt for forward charlm\n",
            "2024-02-28 01:33:44 INFO: Using model /root/stanza_resources/en/backward_charlm/1billion.pt for backward charlm\n",
            "2024-02-28 01:33:44 INFO: Using default pretrain for language, found in /root/stanza_resources/en/pretrain/conll17.pt  To use a different pretrain, specify --wordvec_pretrain_file\n",
            "2024-02-28 01:33:44 INFO: en_cell: saved_models/ner/en_cell_charlm_nertagger.pt does not exist, training new model\n",
            "2024-02-28 01:33:44 INFO: Using model /root/stanza_resources/en/forward_charlm/1billion.pt for forward charlm\n",
            "2024-02-28 01:33:44 INFO: Using model /root/stanza_resources/en/backward_charlm/1billion.pt for backward charlm\n",
            "2024-02-28 01:33:44 INFO: Using default pretrain for language, found in /root/stanza_resources/en/pretrain/conll17.pt  To use a different pretrain, specify --wordvec_pretrain_file\n",
            "2024-02-28 01:33:44 INFO: Running train step with args: ['--train_file', 'data/ner/en_cell.train.json', '--eval_file', 'data/ner/en_cell.dev.json', '--shorthand', 'en_cell', '--mode', 'train', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/root/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/root/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/root/stanza_resources/en/pretrain/conll17.pt', '--max_steps', '200', '--batch_size', '4']\n",
            "2024-02-28 01:33:44 INFO: Running NER tagger in train mode\n",
            "2024-02-28 01:33:44 INFO: ARGS USED AT TRAINING TIME:\n",
            "batch_size: 4\n",
            "bert_finetune: False\n",
            "bert_hidden_layers: None\n",
            "bert_learning_rate: 1.0\n",
            "bert_model: None\n",
            "char: True\n",
            "char_dropout: 0\n",
            "char_emb_dim: 100\n",
            "char_hidden_dim: 100\n",
            "char_lowercase: False\n",
            "char_num_layers: 1\n",
            "char_rec_dropout: 0\n",
            "charlm: True\n",
            "charlm_backward_file: /root/stanza_resources/en/backward_charlm/1billion.pt\n",
            "charlm_forward_file: /root/stanza_resources/en/forward_charlm/1billion.pt\n",
            "charlm_save_dir: saved_models/charlm\n",
            "charlm_shorthand: en_1billion\n",
            "connect_output_layers: False\n",
            "data_dir: data/ner\n",
            "device: cpu\n",
            "dropout: 0.5\n",
            "emb_finetune: True\n",
            "emb_finetune_known_only: False\n",
            "eval_file: data/ner/en_cell.dev.json\n",
            "eval_interval: 500\n",
            "eval_output_file: None\n",
            "finetune: False\n",
            "finetune_load_name: None\n",
            "gradient_checkpointing: False\n",
            "hidden_dim: 256\n",
            "ignore_tag_scores: None\n",
            "input_transform: True\n",
            "locked_dropout: 0.0\n",
            "log_norms: False\n",
            "log_step: 20\n",
            "lora_alpha: 128\n",
            "lora_dropout: 0.1\n",
            "lora_modules_to_save: []\n",
            "lora_rank: 64\n",
            "lora_target_modules: ['query', 'value', 'output.dense', 'intermediate.dense']\n",
            "lowercase: True\n",
            "lr: 0.1\n",
            "lr_decay: 0.5\n",
            "max_grad_norm: 5.0\n",
            "max_steps: 200\n",
            "max_steps_no_improve: 2500\n",
            "min_lr: 0.0001\n",
            "mode: train\n",
            "momentum: 0\n",
            "num_layers: 1\n",
            "optim: sgd\n",
            "patience: 3\n",
            "predict_tagset: None\n",
            "pretrain_max_vocab: 100000\n",
            "rec_dropout: 0\n",
            "sample_train: 1.0\n",
            "save_dir: saved_models/ner\n",
            "save_name: en_cell_charlm_nertagger.pt\n",
            "scheme: bioes\n",
            "second_bert_learning_rate: 0\n",
            "second_lr: 0.005\n",
            "second_optim: None\n",
            "seed: 1234\n",
            "shorthand: en_cell\n",
            "train_classifier_only: False\n",
            "train_file: data/ner/en_cell.train.json\n",
            "train_scheme: None\n",
            "use_peft: False\n",
            "wandb: False\n",
            "wandb_name: None\n",
            "word_dropout: 0.01\n",
            "word_emb_dim: 100\n",
            "wordvec_dir: extern_data/word2vec\n",
            "wordvec_file: \n",
            "wordvec_pretrain_file: /root/stanza_resources/en/pretrain/conll17.pt\n",
            "\n",
            "2024-02-28 01:33:45 DEBUG: Loaded pretrain from /root/stanza_resources/en/pretrain/conll17.pt\n",
            "2024-02-28 01:33:45 INFO: Using pretrained contextualized char embedding\n",
            "2024-02-28 01:33:45 INFO: Loading training data with batch size 4 from data/ner/en_cell.train.json\n",
            "2024-02-28 01:33:46 INFO: Loaded 1426 sentences of training data\n",
            "2024-02-28 01:33:46 DEBUG: BIO tagging scheme found in input at column 0; converting into BIOES scheme...\n",
            "2024-02-28 01:33:46 DEBUG: Creating delta vocab of size 5767\n",
            "2024-02-28 01:33:47 DEBUG: 357 batches created.\n",
            "2024-02-28 01:33:47 INFO: Loading dev data from data/ner/en_cell.dev.json\n",
            "2024-02-28 01:33:47 INFO: Loaded 159 sentences of dev data\n",
            "2024-02-28 01:33:47 DEBUG: BIO tagging scheme found in input at column 0; converting into BIOES scheme...\n",
            "2024-02-28 01:33:47 DEBUG: 40 batches created.\n",
            "2024-02-28 01:33:47 INFO: Training data has 1 columns of tags\n",
            "2024-02-28 01:33:47 INFO: Tags present in training set at column 0:\n",
            "  Tags without BIES markers: O\n",
            "  Tags with B-, I-, E-, or S-: Cell\n",
            "2024-02-28 01:33:47 INFO: Training tagger...\n",
            "2024-02-28 01:33:47 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
            "2024-02-28 01:33:48 INFO: NERTagger(\n",
            "  (word_emb): Embedding(250000, 100, padding_idx=0)\n",
            "  (delta_emb): Embedding(5767, 100, padding_idx=0)\n",
            "  (charmodel_forward): CharacterLanguageModel(\n",
            "    (char_emb): Embedding(948, 100)\n",
            "    (charlstm): PackedLSTM(\n",
            "      (lstm): LSTM(100, 1024, batch_first=True)\n",
            "    )\n",
            "    (decoder): Linear(in_features=1024, out_features=948, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "    (char_dropout): SequenceUnitDropout(p=1e-05, replacement_id=1)\n",
            "  )\n",
            "  (charmodel_backward): CharacterLanguageModel(\n",
            "    (char_emb): Embedding(948, 100)\n",
            "    (charlstm): PackedLSTM(\n",
            "      (lstm): LSTM(100, 1024, batch_first=True)\n",
            "    )\n",
            "    (decoder): Linear(in_features=1024, out_features=948, bias=True)\n",
            "    (dropout): Dropout(p=0.05, inplace=False)\n",
            "    (char_dropout): SequenceUnitDropout(p=1e-05, replacement_id=1)\n",
            "  )\n",
            "  (input_transform): Linear(in_features=2148, out_features=2148, bias=True)\n",
            "  (taggerlstm): PackedLSTM(\n",
            "    (lstm): LSTM(2148, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (tag_clfs): ModuleList(\n",
            "    (0): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            "  (crits): ModuleList(\n",
            "    (0): CRFLoss()\n",
            "  )\n",
            "  (drop): Dropout(p=0.5, inplace=False)\n",
            "  (worddrop): WordDropout(p=0.01)\n",
            "  (lockeddrop): LockedDropout(p=0.0)\n",
            ")\n",
            "2024-02-28 01:34:27 INFO: 2024-02-28 01:34:27: step 20/200, loss = 5.507203 (1.121 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:35:06 INFO: 2024-02-28 01:35:06: step 40/200, loss = 5.192265 (1.610 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:35:47 INFO: 2024-02-28 01:35:47: step 60/200, loss = 6.857685 (2.193 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:36:22 INFO: 2024-02-28 01:36:22: step 80/200, loss = 4.257799 (1.810 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:36:58 INFO: 2024-02-28 01:36:58: step 100/200, loss = 3.926456 (2.349 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:37:37 INFO: 2024-02-28 01:37:37: step 120/200, loss = 1.437283 (1.496 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:38:14 INFO: 2024-02-28 01:38:14: step 140/200, loss = 8.158539 (2.253 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:38:51 INFO: 2024-02-28 01:38:51: step 160/200, loss = 3.054527 (1.514 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:39:30 INFO: 2024-02-28 01:39:30: step 180/200, loss = 2.089928 (2.355 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:40:02 INFO: 2024-02-28 01:40:02: step 200/200, loss = 3.026592 (1.329 sec/batch), lr: 0.100000\n",
            "2024-02-28 01:40:02 INFO: stopping...\n",
            "2024-02-28 01:40:02 INFO: Training ended with 200 steps.\n",
            "2024-02-28 01:40:02 INFO: Dev set never evaluated.  Saving final model.\n",
            "2024-02-28 01:40:03 INFO: Model saved to saved_models/ner/en_cell_charlm_nertagger.pt\n",
            "2024-02-28 01:40:03 INFO: Running dev step with args: ['--eval_file', 'data/ner/en_cell.dev.json', '--shorthand', 'en_cell', '--mode', 'predict', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/root/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/root/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/root/stanza_resources/en/pretrain/conll17.pt']\n",
            "2024-02-28 01:40:03 INFO: Running NER tagger in predict mode\n",
            "2024-02-28 01:40:03 DEBUG: Loaded pretrain from /root/stanza_resources/en/pretrain/conll17.pt\n",
            "2024-02-28 01:40:04 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
            "2024-02-28 01:40:04 DEBUG: Loaded model for eval from saved_models/ner/en_cell_charlm_nertagger.pt\n",
            "2024-02-28 01:40:04 DEBUG: Using the 0 tagset for evaluation\n",
            "2024-02-28 01:40:04 INFO: Loading data with batch size 32...\n",
            "2024-02-28 01:40:05 DEBUG: BIO tagging scheme found in input at column 0; converting into BIOES scheme...\n",
            "2024-02-28 01:40:05 DEBUG: 5 batches created.\n",
            "2024-02-28 01:40:05 INFO: Start evaluation...\n",
            "2024-02-28 01:40:36 INFO: Score by entity:\n",
            "Prec.\tRec.\tF1\n",
            "85.71\t5.04\t9.52\n",
            "2024-02-28 01:40:36 INFO: Score by token:\n",
            "Prec.\tRec.\tF1\n",
            "85.19\t7.32\t13.49\n",
            "2024-02-28 01:40:36 INFO: Weighted f1 for non-O tokens: 0.124511\n",
            "2024-02-28 01:40:36 INFO: NER tagger score: en_cell saved_models/ner/en_cell_charlm_nertagger.pt data/ner/en_cell.dev.json 9.52\n",
            "2024-02-28 01:40:36 INFO: NER Entity F1 scores:\n",
            "  Cell: 9.52\n",
            "2024-02-28 01:40:36 INFO: NER token confusion matrix:\n",
            "      t\\p       O B-Cell E-Cell I-Cell S-Cell\n",
            "         O   5006      2      2      0      0\n",
            "    B-Cell     59     11      0      0      0\n",
            "    E-Cell     59      0     11      0      0\n",
            "    I-Cell      6      0      0      0      0\n",
            "    S-Cell    167      0      0      0      1\n",
            "2024-02-28 01:40:36 INFO: Running test step with args: ['--eval_file', 'data/ner/en_cell.test.json', '--shorthand', 'en_cell', '--mode', 'predict', '--charlm', '--charlm_shorthand', 'en_1billion', '--charlm_forward_file', '/root/stanza_resources/en/forward_charlm/1billion.pt', '--charlm_backward_file', '/root/stanza_resources/en/backward_charlm/1billion.pt', '--wordvec_pretrain_file', '/root/stanza_resources/en/pretrain/conll17.pt']\n",
            "2024-02-28 01:40:36 INFO: Running NER tagger in predict mode\n",
            "2024-02-28 01:40:37 DEBUG: Loaded pretrain from /root/stanza_resources/en/pretrain/conll17.pt\n",
            "2024-02-28 01:40:37 DEBUG: Building SGD with lr=0.100000, momentum=0.000000\n",
            "2024-02-28 01:40:37 DEBUG: Loaded model for eval from saved_models/ner/en_cell_charlm_nertagger.pt\n",
            "2024-02-28 01:40:37 DEBUG: Using the 0 tagset for evaluation\n",
            "2024-02-28 01:40:37 INFO: Loading data with batch size 32...\n",
            "2024-02-28 01:40:38 DEBUG: BIO tagging scheme found in input at column 0; converting into BIOES scheme...\n",
            "2024-02-28 01:40:38 DEBUG: 11 batches created.\n",
            "2024-02-28 01:40:38 INFO: Start evaluation...\n",
            "2024-02-28 01:41:45 INFO: Score by entity:\n",
            "Prec.\tRec.\tF1\n",
            "74.29\t5.64\t10.48\n",
            "2024-02-28 01:41:45 INFO: Score by token:\n",
            "Prec.\tRec.\tF1\n",
            "73.53\t8.33\t14.97\n",
            "2024-02-28 01:41:45 INFO: Weighted f1 for non-O tokens: 0.134431\n",
            "2024-02-28 01:41:45 INFO: NER tagger score: en_cell saved_models/ner/en_cell_charlm_nertagger.pt data/ner/en_cell.test.json 10.48\n",
            "2024-02-28 01:41:45 INFO: NER Entity F1 scores:\n",
            "  Cell: 10.48\n",
            "2024-02-28 01:41:45 INFO: NER token confusion matrix:\n",
            "      t\\p       O B-Cell E-Cell I-Cell S-Cell\n",
            "         O  10981      7      8      0      0\n",
            "    B-Cell    107     24      0      0      0\n",
            "    E-Cell    106      1     24      0      0\n",
            "    I-Cell      8      0      0      0      0\n",
            "    S-Cell    326      1      1      0      2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcK94HifT/y+ta6vUpNGLx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}