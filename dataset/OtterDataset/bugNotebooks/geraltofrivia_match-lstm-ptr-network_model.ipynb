{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QA over unstructured data\n",
    "\n",
    "Using Match LSTM, Pointer Networks, as mentioned in paper https://arxiv.org/pdf/1608.07905.pdf\n",
    "\n",
    "We start with the pre-processing provided by https://github.com/MurtyShikhar/Question-Answering to clean up the data and make neat para, ques files.\n",
    "\n",
    "\n",
    "### @TODOs:\n",
    "\n",
    "1. [done] _Figure out how to put in real, pre-trained embeddings in embeddings layer._\n",
    "2. [done] _Explicitly provide batch size when instantiating model_\n",
    "3. [done] is ./val.ids.* validation set or test set?: **validation**\n",
    "4. [done:em] emInstead of test loss, calculate test acc metrics\n",
    "    1. todo: new metrics like P, R, F1\n",
    "5. [done] Update unit test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from pandas_ml import ConfusionMatrix as confusion_matrix\n",
    "from progressbar import ProgressBar\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from io import open\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import traceback\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import pylab\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from networks import Encoder, MatchLSTMEncoder, PointerDecoder\n",
    "\n",
    "\n",
    "# Importing models from - https://github.com/laddie132/Match-LSTM\n",
    "\n",
    "from models import *\n",
    "from models.loss import MyNLLLoss, RLLoss\n",
    "from utils.load_config import init_logging, read_config\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug Legend\n",
    "\n",
    "- 5: Print everything that goes in every tensor.\n",
    "- 4: ??\n",
    "- 3: Check every model individually\n",
    "- 2: Print things in training loops\n",
    "- 1: ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Macros \n",
    "DATA_LOC = './data/squad'\n",
    "MODEL_LOC = './models/mlstms/squad/'\n",
    "DEBUG = 1\n",
    "\n",
    "# nn Macros\n",
    "QUES_LEN, PARA_LEN =  30, 30\n",
    "VOCAB_SIZE = 120000\n",
    "# VOCAB_SIZE = glove_file.shape[1]               # @TODO: get actual size\n",
    "HIDDEN_DIM = 150\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 80                  # Might have total 100 batches.\n",
    "EPOCHS = 30\n",
    "TEST_EVERY_ = 1\n",
    "LR = 0.001\n",
    "CROP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder': {'add_features': 110,\n",
      "             'bidirection': True,\n",
      "             'char_cnn_filter_num': [75, 75, 75, 75],\n",
      "             'char_cnn_filter_size': [2, 3, 4, 5],\n",
      "             'char_embedding_size': 64,\n",
      "             'char_encode_type': 'LSTM',\n",
      "             'char_layers': 1,\n",
      "             'char_trainable': True,\n",
      "             'enable_char': True,\n",
      "             'mix_encode': False,\n",
      "             'word_embedding_size': 300,\n",
      "             'word_layers': 1},\n",
      " 'global': {'dropout_p': 0.4,\n",
      "            'emb_dropout_p': 0.1,\n",
      "            'hidden_mode': 'GRU',\n",
      "            'hidden_size': 150,\n",
      "            'layer_norm': False},\n",
      " 'interaction': {'birnn_after_self': True,\n",
      "                 'enable_self_match': False,\n",
      "                 'gated_attention': True,\n",
      "                 'match_lstm_bidirection': True,\n",
      "                 'question_match': False,\n",
      "                 'self_gated': False,\n",
      "                 'self_match_bidirection': True},\n",
      " 'output': {'answer_search': True,\n",
      "            'init_ptr_hidden': 'linear',\n",
      "            'num_hops': 1,\n",
      "            'ptr_bidirection': False,\n",
      "            'scales': [1]}}\n"
     ]
    }
   ],
   "source": [
    "global_config = read_config('config/base_model.yaml')\n",
    "pprint(global_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGloveEmbedding(layers.GloveEmbedding):\n",
    "    \n",
    "    def __init__(self, n_embeddings, len_embedding, weights):\n",
    "        super(layers.GloveEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(num_embeddings=n_embeddings, embedding_dim=len_embedding,\n",
    "                                                  _weight=weights)\n",
    "        self.embedding_layer.weight.requires_grad = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMatchLSTM(MatchLSTM):\n",
    "    def __init__(self, n_embeddings, len_embedding, weights):\n",
    "        super(MatchLSTM, self).__init__()\n",
    "                # set config\n",
    "        hidden_size = 150\n",
    "        dropout_p = 0.4\n",
    "        emb_dropout_p = 0.1\n",
    "        enable_layer_norm = False\n",
    "        hidden_mode = 'LSTM'\n",
    "\n",
    "        word_embedding_size = 300\n",
    "        encoder_bidirection = True\n",
    "        encoder_direction_num = 2 if encoder_bidirection else 1\n",
    "\n",
    "        match_lstm_bidirection = True\n",
    "        match_rnn_direction_num = 2 if match_lstm_bidirection else 1\n",
    "\n",
    "        ptr_bidirection = True\n",
    "        self.enable_search = True\n",
    "        \n",
    "        self.embedding = CustomGloveEmbedding(n_embeddings, len_embedding, weights)\n",
    "\n",
    "        self.encoder = layers.MyRNNBase(mode=hidden_mode,\n",
    "                                 input_size=word_embedding_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 bidirectional=encoder_bidirection,\n",
    "                                 dropout_p=emb_dropout_p)\n",
    "        encode_out_size = hidden_size * encoder_direction_num\n",
    "\n",
    "        self.match_rnn = layers.MatchRNN(mode=hidden_mode,\n",
    "                                  hp_input_size=encode_out_size,\n",
    "                                  hq_input_size=encode_out_size,\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  bidirectional=match_lstm_bidirection,\n",
    "                                  gated_attention=True,\n",
    "                                  dropout_p=dropout_p,\n",
    "                                  enable_layer_norm=enable_layer_norm)\n",
    "        match_rnn_out_size = hidden_size * match_rnn_direction_num\n",
    "\n",
    "        self.pointer_net = layers.BoundaryPointer(mode=hidden_mode,\n",
    "                                           input_size=match_rnn_out_size,\n",
    "                                           hidden_size=hidden_size,\n",
    "                                           bidirectional=ptr_bidirection,\n",
    "                                           dropout_p=dropout_p,\n",
    "                                           enable_layer_norm=enable_layer_norm)\n",
    "    def forward(self, context, question, context_char=None, question_char=None, context_f=None, question_f=None):\n",
    "        \"\"\"\n",
    "        context_char and question_char not used\n",
    "        \"\"\"\n",
    "\n",
    "        # get embedding: (seq_len, batch, embedding_size)\n",
    "        context_vec, context_mask = self.embedding.forward(context)\n",
    "        question_vec, question_mask = self.embedding.forward(question)\n",
    "\n",
    "        # encode: (seq_len, batch, hidden_size)\n",
    "        context_encode, _ = self.encoder.forward(context_vec, context_mask)\n",
    "        question_encode, _ = self.encoder.forward(question_vec, question_mask)\n",
    "\n",
    "        # match lstm: (seq_len, batch, hidden_size)\n",
    "        print(\"context mask is \", context_mask)\n",
    "        qt_aware_ct, qt_aware_last_hidden, match_para = self.match_rnn.forward(context_encode, context_mask,\n",
    "                                                                               question_encode, question_mask)\n",
    "        vis_param = {'match': match_para}\n",
    "\n",
    "        # pointer net: (answer_len, batch, context_len)\n",
    "        ans_range_prop = self.pointer_net.forward(qt_aware_ct, context_mask)\n",
    "        ans_range_prop = ans_range_prop.transpose(0, 1)\n",
    "\n",
    "        # answer range\n",
    "        if not self.training and self.enable_search:\n",
    "            ans_range = answer_search(ans_range_prop, context_mask)\n",
    "        else:\n",
    "            _, ans_range = torch.max(ans_range_prop, dim=2)\n",
    "\n",
    "        return ans_range_prop, ans_range, vis_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding init\n",
      "context mask is  tensor([[ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "DEBUG = 5\n",
    "if DEBUG > 4:\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        macros = {\n",
    "        \"ques_len\": QUES_LEN,\n",
    "        \"hidden_dim\": HIDDEN_DIM, \n",
    "        \"vocab_size\": VOCAB_SIZE, \n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"para_len\": PARA_LEN,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"lr\": LR,\n",
    "        \"debug\":5,\n",
    "        \"device\":device\n",
    "    }\n",
    "        \n",
    "    glove_file = torch.randn((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "#     glove_emb = CustomGloveEmbedding(VOCAB_SIZE,EMBEDDING_DIM,glove_file)\n",
    "    match_lstm = CustomMatchLSTM(VOCAB_SIZE,EMBEDDING_DIM,glove_file)\n",
    "    print(\"embedding init\")\n",
    "    paragraph = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,)).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,)).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    a,b,c = match_lstm.forward(paragraph,question)\n",
    "    \n",
    "#     print(glove_emb.forward(torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,)).view(BATCH_SIZE,PARA_LEN).long()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 14,  13,  24,  14,  28,  10,  28,  18,  10,   9,  24,   4,\n",
      "         26,  13,  16,  15,  26,  26,  20,  23,  27,  29,  24,  11,\n",
      "          5,  19,  25,  11,   4,  15,  26,   5,  28,  13,   6,   7,\n",
      "         24,  13,   5,  25,  21,  22,  16,  23,  26,   2,  14,  15,\n",
      "         28,  10,   4,  15,  15,   6,  24,  29,   6,  19,   7,   6,\n",
      "         26,   5,  26,  10,  28,  11,  10,   7,   8,  13,  18,  15,\n",
      "         13,  29,  21,   8,   1,  16,  20,  18])\n",
      "tensor([ 14,  13,  24,  14,  28,  10,  28,  18,  10,   9,  24,   4,\n",
      "         26,  13,  16,  15,  26,  26,  20,  23,  27,  29,  24,  11,\n",
      "          5,  19,  25,  11,   4,  15,  26,   5,  28,  13,   6,   7,\n",
      "         24,  13,   5,  25,  21,  22,  16,  23,  26,   2,  14,  15,\n",
      "         28,  10,   4,  15,  15,   6,  24,  29,   6,  19,   7,   6,\n",
      "         26,   5,  26,  10,  28,  11,  10,   7,   8,  13,  18,  15,\n",
      "         13,  29,  21,   8,   1,  16,  20,  18])\n",
      "tensor([[ 14,  14],\n",
      "        [ 13,  13],\n",
      "        [ 24,  24],\n",
      "        [ 14,  14],\n",
      "        [ 28,  28],\n",
      "        [ 10,  10],\n",
      "        [ 28,  28],\n",
      "        [ 18,  18],\n",
      "        [ 10,  10],\n",
      "        [  9,   9],\n",
      "        [ 24,  24],\n",
      "        [  4,   4],\n",
      "        [ 26,  26],\n",
      "        [ 13,  13],\n",
      "        [ 16,  16],\n",
      "        [ 15,  15],\n",
      "        [ 26,  26],\n",
      "        [ 26,  26],\n",
      "        [ 20,  20],\n",
      "        [ 23,  23],\n",
      "        [ 27,  27],\n",
      "        [ 29,  29],\n",
      "        [ 24,  24],\n",
      "        [ 11,  11],\n",
      "        [  5,   5],\n",
      "        [ 19,  19],\n",
      "        [ 25,  25],\n",
      "        [ 11,  11],\n",
      "        [  4,   4],\n",
      "        [ 15,  15],\n",
      "        [ 26,  26],\n",
      "        [  5,   5],\n",
      "        [ 28,  28],\n",
      "        [ 13,  13],\n",
      "        [  6,   6],\n",
      "        [  7,   7],\n",
      "        [ 24,  24],\n",
      "        [ 13,  13],\n",
      "        [  5,   5],\n",
      "        [ 25,  25],\n",
      "        [ 21,  21],\n",
      "        [ 22,  22],\n",
      "        [ 16,  16],\n",
      "        [ 23,  23],\n",
      "        [ 26,  26],\n",
      "        [  2,   2],\n",
      "        [ 14,  14],\n",
      "        [ 15,  15],\n",
      "        [ 28,  28],\n",
      "        [ 10,  10],\n",
      "        [  4,   4],\n",
      "        [ 15,  15],\n",
      "        [ 15,  15],\n",
      "        [  6,   6],\n",
      "        [ 24,  24],\n",
      "        [ 29,  29],\n",
      "        [  6,   6],\n",
      "        [ 19,  19],\n",
      "        [  7,   7],\n",
      "        [  6,   6],\n",
      "        [ 26,  26],\n",
      "        [  5,   5],\n",
      "        [ 26,  26],\n",
      "        [ 10,  10],\n",
      "        [ 28,  28],\n",
      "        [ 11,  11],\n",
      "        [ 10,  10],\n",
      "        [  7,   7],\n",
      "        [  8,   8],\n",
      "        [ 13,  13],\n",
      "        [ 18,  18],\n",
      "        [ 15,  15],\n",
      "        [ 13,  13],\n",
      "        [ 29,  29],\n",
      "        [ 21,  21],\n",
      "        [  8,   8],\n",
      "        [  1,   1],\n",
      "        [ 16,  16],\n",
      "        [ 20,  20],\n",
      "        [ 18,  18]])\n"
     ]
    }
   ],
   "source": [
    "print(b[:,1])\n",
    "print(b[:,0])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoder \n",
    "Use a simple lstm class to have encoder for question and paragraph. \n",
    "The output of these will be used in the match lstm\n",
    "\n",
    "$H^p = LSTM(P)$ \n",
    "\n",
    "\n",
    "$H^q = LSTM(Q)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputlen, macros, glove_file, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Catch dim\n",
    "        self.inputlen = inputlen\n",
    "        self.hiddendim = macros['hidden_dim']\n",
    "        self.embeddingdim =  macros['embedding_dim']\n",
    "        self.vocablen = macros['vocab_size']\n",
    "#         self.device = macros['device']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        # Embedding Layer\n",
    "#         self.embedding = nn.Embedding(len(glove_file), self.embeddingdim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_file))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "       \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(self.embeddingdim, self.hiddendim, bidirectional=True, batch_first=False)\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \n",
    "        # Returns a new hidden layer var for LSTM\n",
    "        return (torch.zeros((2, batch_size, self.hiddendim), device=device), \n",
    "                torch.zeros((2, batch_size, self.hiddendim), device=device))\n",
    "    \n",
    "    def forward(self, x, h, device):\n",
    "        \n",
    "        # Input: x (batch, len ) (current input)\n",
    "        # Hidden: h (1, batch, hiddendim) (last hidden state)\n",
    "        \n",
    "        # Batchsize: b int (inferred)\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        if self.debug > 4: print(\"x:\\t\", x.shape)\n",
    "        if self.debug > 4: print(\"h:\\t\", h[0].shape, h[1].shape)\n",
    "        \n",
    "        x_emb = self.embedding(x)\n",
    "        if self.debug > 4: \n",
    "            print(\"x_emb:\\t\", x_emb.shape)\n",
    "#             print(\"x_emb_wrong:\\t\", x_emb.transpose(1,0).shape)           \n",
    "            \n",
    "        ycap, h = self.lstm(x_emb.transpose(1,0), h)\n",
    "        if self.debug > 4: \n",
    "            print(\"ycap:\\t\", ycap.shape)\n",
    "        \n",
    "        return ycap, h\n",
    "    \n",
    "    \n",
    "# # with torch.no_grad():\n",
    "# #     print (\"Trying out question encoder LSTM\")\n",
    "# #     model = Encoder(QUES_LEN, HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE)\n",
    "# #     dummy_x = torch.tensor([22,45,12], dtype=torch.long)\n",
    "# #     hidden = model.init_hidden()\n",
    "# #     ycap, h = model(dummy_x, hidden)\n",
    "    \n",
    "# #     print(ycap.shape)\n",
    "# #     print(h[0].shape, h[1].shape)\n",
    "\n",
    "\n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        macros = {\n",
    "        \"ques_len\": QUES_LEN,\n",
    "        \"hidden_dim\": HIDDEN_DIM, \n",
    "        \"vocab_size\": VOCAB_SIZE, \n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"para_len\": PARA_LEN,\n",
    "        \"embedding_dim\": EMBEDDING_DIM,\n",
    "        \"lr\": LR,\n",
    "        \"debug\":5,\n",
    "        \"device\":device\n",
    "    }\n",
    "\n",
    "        dummy_para = torch.randint(0,VOCAB_SIZE-1,(PARA_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,PARA_LEN).long()\n",
    "    #     print (dummy_para.shape)\n",
    "        dummy_question = torch.randint(0,VOCAB_SIZE-1,(QUES_LEN*BATCH_SIZE,), device=device).view(BATCH_SIZE,QUES_LEN).long()\n",
    "    #     print (dummy_question.shape)\n",
    "        glove_file = torch.randn((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "    #     print(\"LSTM with batches\")\n",
    "        ques_model = Encoder(QUES_LEN, macros, glove_file, device).cuda(device)\n",
    "        para_model = Encoder(QUES_LEN, macros, glove_file, device).cuda(device)\n",
    "        ques_hidden = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        para_hidden = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        ques_embedded,hidden_ques = ques_model(dummy_question,ques_hidden, device)\n",
    "        para_embedded,hidden_para = para_model(dummy_para,para_hidden, device)\n",
    "        \n",
    "#         print (ques_embedded.shape) # question_length,batch,embedding_dim\n",
    "#         print (para_embedded.shape) # para_length,batch,embedding_dim\n",
    "#         print (hidden_para[0].shape,hidden_para[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Match LSTM\n",
    "\n",
    "Use a match LSTM to compute a **summarized sequential vector** for the paragraph w.r.t the question.\n",
    "\n",
    "Consider the summarized vector ($H^r$) as the output of a new decoder, where the inputs are $H^p, H^q$ computed above. \n",
    "\n",
    "1. Attend the para word $i$ with the entire question ($H^q$)\n",
    "  \n",
    "    1. $\\vec{G}_i = tanh(W^qH^q + repeat(W^ph^p_i + W^r\\vec{h^r_{i-1} + b^p}))$\n",
    "    \n",
    "    2. *Computing it*: Here, $\\vec{G}_i$ is equivalent to `energy`, computed differently.\n",
    "    \n",
    "    3. Use a linear layer to compute the content within the $repeat$ fn.\n",
    "    \n",
    "    4. Add with another linear (without bias) with $H_q$\n",
    "    \n",
    "    5. $tanh$ the bloody thing\n",
    "  \n",
    "  \n",
    "2. Softmax over it to get $\\alpha$ weights.\n",
    "\n",
    "    1. $\\vec{\\alpha_i} = softmax(w^t\\vec{G}_i + repeat(b))$\n",
    "    \n",
    "3. Use the attention weight vector $\\vec{\\alpha_i}$ to obtain a weighted version of the question and concat it with the current token of the passage to form a vector $\\vec{z_i}$\n",
    "\n",
    "4. Use $\\vec{z_i}$ to compute the desired $h^r_i$:\n",
    "\n",
    "    1. $ h^r_i = LSTM(\\vec{z_i}, h^r_{i-1}) $\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros, device):\n",
    "        \n",
    "        super(MatchLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.ques_len = macros['ques_len']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.debug = macros['debug']    \n",
    "        \n",
    "        # Catch lens and params\n",
    "        self.lin_g_repeat_a_dense = nn.Linear(2*self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_g_repeat_b_dense = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        self.lin_g_nobias = nn.Linear(2*self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.alpha_i_w = nn.Parameter(torch.rand((self.hidden_dim, 1)))\n",
    "        self.alpha_i_b = nn.Parameter(torch.rand((1)))\n",
    "        \n",
    "        self.lstm_summary = nn.LSTM((self.ques_len+1)*2*self.hidden_dim, self.hidden_dim, batch_first=False)\n",
    "                                      \n",
    "    \n",
    "    def forward(self, H_p, h_ri, H_q, hidden, device):\n",
    "        \"\"\"\n",
    "            Ideally, we would have manually unrolled the lstm \n",
    "            but due to memory constraints, we do it in the module.\n",
    "        \"\"\"\n",
    "        # Find batchsize\n",
    "        batch_size = H_p.shape[1]\n",
    "        H_r = []\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print(\"H_p:\\t\", H_p.shape[0])\n",
    "            print(\"h_ri:\\t\", h_ri.shape[0])\n",
    "            print(\"H_q:\\t\", H_q.shape[0])\n",
    "            print(\"hidden:\\t\", hidden.shape[0])\n",
    "        \n",
    "        for i in range(H_p.shape[0]):\n",
    "            \n",
    "            j = H_p.shape[0] - 1 - i\n",
    "            \n",
    "            # Get the G's first input\n",
    "            G_input_a = self.lin_g_repeat_a_dense(H_q)\n",
    "            \n",
    "            #\n",
    "            G_input_b_input_a_fwd = \n",
    "            \n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "#                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "\n",
    "\n",
    "\n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        macros = {\n",
    "            \"ques_len\": QUES_LEN,\n",
    "            \"hidden_dim\": HIDDEN_DIM, \n",
    "            \"vocab_size\": VOCAB_SIZE, \n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"para_len\": PARA_LEN,\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lr\": LR,\n",
    "            \"debug\":5,\n",
    "            \"device\":device\n",
    "        }\n",
    "            \n",
    "        matchLSTMEncoder = MatchLSTMEncoder(macros,device).cuda(device)\n",
    "        hidden = matchLSTMEncoder.init_hidden(BATCH_SIZE,device)\n",
    "        para_embedded = torch.rand((PARA_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "        ques_embedded = torch.rand((QUES_LEN, BATCH_SIZE, 2*HIDDEN_DIM), device=device)\n",
    "        h_ri = torch.randn(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    #     if DEBUG:\n",
    "    #         print (\"init h_ri shape is: \", h_ri.shape)\n",
    "    #         print (\"the para length is \", len(para_embedded))\n",
    "        H_r = matchLSTMEncoder(para_embedded.view(-1,BATCH_SIZE,2*HIDDEN_DIM),\n",
    "                               h_ri, \n",
    "                               ques_embedded, \n",
    "                               hidden,\n",
    "                               device)\n",
    "        print(\"H_r: \", H_r.shape)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointer Network\n",
    "\n",
    "Using a ptrnet over $H_r$ to unfold and get most probable spans.\n",
    "We use the **boundry model** to do that (predict start and end of seq).\n",
    "\n",
    "- We first calculate $\\vec{F}_k$ (energy) vector which will be softmaxed to get the attention weights ($\\beta$).\n",
    "\n",
    "  - $F_k = tanh(VH^r + (W^ah^a_{k-1} + b^a).repeat(P))$\n",
    "\n",
    "  - $\\beta_k = softmax(v^TF_k + c.repeat(P))$\n",
    "\n",
    "- Using the $\\beta$ values, we annotate the hidden states of the match-lstm output, which we directly throw to a decoder.\n",
    "\n",
    "- Finally, the $\\beta$ values are used as the **desired** output pointer.\n",
    "\n",
    "- We unroll the decoder twice (manually) to get a **start** and an **end** pointer.\n",
    "\n",
    "A simple energy -> softmax -> decoder. Where softmaxed energy is supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PointerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, macros, device):\n",
    "        super(PointerDecoder, self).__init__()\n",
    "        \n",
    "        # Keep args\n",
    "        self.hidden_dim = macros['hidden_dim']\n",
    "        self.batch_size = macros['batch_size']\n",
    "        self.para_len = macros['para_len']\n",
    "        self.debug = macros['debug']\n",
    "        \n",
    "        self.lin_f_repeat = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_f_nobias = nn.Linear(self.hidden_dim, self.hidden_dim, bias=False)\n",
    "        \n",
    "        self.beta_k_w = nn.Parameter(torch.randn(self.hidden_dim, 1))\n",
    "        self.beta_k_b = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_dim*self.para_len, self.hidden_dim, batch_first=False)\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return torch.zeros((1, batch_size, self.hidden_dim), device=device)\n",
    "#                 torch.zeros((1, batch_size, self.hidden_dim), device=device))\n",
    "    \n",
    "    def forward(self, h_ak, H_r, hidden, device):\n",
    "        \n",
    "        # h_ak (current decoder's last op) (1,batch,hiddendim)\n",
    "        # H_r (weighted summary of para) (P, batch, hiddendim)\n",
    "        batch_size = H_r.shape[1]\n",
    "        \n",
    "        if self.debug > 4:\n",
    "            print(\"h_ak:\\t\\t\\t\", h_ak.shape)\n",
    "            print(\"H_r:\\t\\t\\t\", H_r.shape)\n",
    "            print(\"hidden:\\t\\t\\t\", hidden.shape)\n",
    "            \n",
    "        # Prepare inputs for the tanh used to compute energy\n",
    "        f_input_b = self.lin_f_repeat(h_ak)\n",
    "        if self.debug > 4: print(\"f_input_b unrepeated:  \", f_input_b.shape)\n",
    "        \n",
    "        #H_r shape is ([PARA_LEN, BATCHSIZE, EmbeddingDIM])\n",
    "        f_input_b = f_input_b.repeat(H_r.shape[0], 1, 1)\n",
    "        if self.debug > 4: print(\"f_input_b repeated:\\t\", f_input_b.shape)\n",
    "            \n",
    "        f_input_a = self.lin_f_nobias(H_r)\n",
    "        if self.debug > 4: print(\"f_input_a:\\t\\t\", f_input_a.shape)\n",
    "            \n",
    "        # Send it off to tanh now\n",
    "        F_k = F.tanh(f_input_a+f_input_b)\n",
    "        if self.debug > 4: print(\"F_k:\\t\\t\\t\", F_k.shape) #PARA_LEN,BATCHSIZE,EmbeddingDim\n",
    "        \n",
    "        # Attention weights\n",
    "        beta_k_input_a = F_k.transpose(1,0).matmul(self.beta_k_w).view(batch_size, 1, -1)\n",
    "        if self.debug > 4: print(\"beta_k_input_a:\\t\\t\", beta_k_input_a.shape)\n",
    "            \n",
    "        beta_k_input = beta_k_input_a.add_(self.beta_k_b.repeat(1,1,self.para_len))\n",
    "        if self.debug > 4: print(\"beta_k_input:\\t\\t\", beta_k_input.shape)\n",
    "            \n",
    "        beta_k = F.softmax(beta_k_input, dim=-1)\n",
    "        if self.debug > 4: print(\"beta_k:\\t\\t\\t\", beta_k.shape)\n",
    "        \n",
    "        lstm_input_a = H_r.transpose(1,0) * (beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))\n",
    "        if self.debug > 4: print(\"lstm_input_a:\\t\\t\", lstm_input_a.shape)\n",
    "        \n",
    "        _, (h_ak, hidden) = self.lstm(lstm_input_a.transpose(1,0).contiguous().view(1, batch_size, -1), (h_ak, hidden))\n",
    "        \n",
    "        return h_ak, hidden, torch.log(beta_k)\n",
    "            \n",
    "if DEBUG > 4:\n",
    "    with torch.no_grad():\n",
    "        macros = {\n",
    "            \"ques_len\": QUES_LEN,\n",
    "            \"hidden_dim\": HIDDEN_DIM, \n",
    "            \"vocab_size\": VOCAB_SIZE, \n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"para_len\": PARA_LEN,\n",
    "            \"embedding_dim\": EMBEDDING_DIM,\n",
    "            \"lr\": LR,\n",
    "            \"debug\":5,\n",
    "            \"device\":device\n",
    "        }\n",
    "        \n",
    "        pointerDecoder = PointerDecoder(macros, device).cuda(device)\n",
    "        h_ak = torch.randn(1,BATCH_SIZE,HIDDEN_DIM, device=device)\n",
    "        H_r = torch.randn(PARA_LEN, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "        hidden = pointerDecoder.init_hidden(BATCH_SIZE, device)\n",
    "        h_ak, hidden, beta_k = pointerDecoder(h_ak, H_r, hidden, device)\n",
    "        print (beta_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_r.transpose(1,0) * \\\n",
    "(beta_k.view(batch_size, self.para_len, -1).repeat(1,1,self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the real data from disk.\n",
    "\n",
    "Files stored in `./data/squad/train.ids.*`\n",
    "Pull both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loc, macros, crop=None):\n",
    "    \"\"\"\n",
    "        Given the dataloc and the data available in a specific format, it would pick the data up, and make trainable matrices,\n",
    "        Harvest train_P, train_Q, train_Y, test_P, test_Q, test_Y matrices in this format\n",
    "        \n",
    "        If crop given, will trim the data at a certain length\n",
    "        \n",
    "        **return_type**: np matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpacking macros\n",
    "    PARA_LEN = macros['para_len']\n",
    "    QUES_LEN = macros['ques_len']\n",
    "    \n",
    "    train_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.question')))])\n",
    "    train_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.ids.context')))])\n",
    "    train_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'train.span')))])\n",
    "\n",
    "    test_q = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.question')))])\n",
    "    test_p = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.ids.context')))])\n",
    "    test_y = np.asarray([[int(x) for x in datum.split()] for datum in list(open(os.path.join(data_loc, 'val.span')))])\n",
    "\n",
    "    if macros['debug'] > 3:\n",
    "        print(\"Train Q: \", train_q.shape)\n",
    "        print(\"Train P: \", train_p.shape)\n",
    "        print(\"Train Y: \", train_y.shape)\n",
    "        print(\"Test Q: \", test_q.shape)\n",
    "        print(\"Test P: \", test_p.shape)\n",
    "        print(\"Test Y: \", test_y.shape)\n",
    "    \n",
    "    \"\"\"\n",
    "        Parse the semi-raw data:\n",
    "            - shuffle\n",
    "            - pad, prepare\n",
    "            - dump useless vars\n",
    "    \"\"\"\n",
    "    # Shuffle data\n",
    "    \n",
    "    if crop:\n",
    "        index_train, index_test = np.random.choice(np.arange(len(train_p)), crop), \\\n",
    "                                  np.random.choice(np.arange(len(test_p)), crop)\n",
    "    else:\n",
    "        index_train, index_test = np.arange(len(train_p)), np.arange(len(test_p))\n",
    "        np.random.shuffle(index_train)\n",
    "        np.random.shuffle(index_test)\n",
    "\n",
    "    train_p, train_q, train_y = train_p[index_train], train_q[index_train], train_y[index_train]\n",
    "    test_p, test_q, test_y = test_p[index_test], test_q[index_test], test_y[index_test]\n",
    "\n",
    "#     sanity_check(train_p, train_y)\n",
    "\n",
    "    if macros['debug'] >= 5:\n",
    "        print(\"Max q len: \", max(len(q) for q in train_q))\n",
    "        \n",
    "    \n",
    "    # Pad and prepare\n",
    "    train_P = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Q = np.zeros((len(train_q), QUES_LEN))\n",
    "    train_Y_start = np.zeros((len(train_p), PARA_LEN))\n",
    "    train_Y_end = np.zeros((len(train_p), PARA_LEN))\n",
    "\n",
    "    test_P = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Q = np.zeros((len(test_q), QUES_LEN))\n",
    "    test_Y_start = np.zeros((len(test_p), PARA_LEN))\n",
    "    test_Y_end = np.zeros((len(test_p), PARA_LEN))\n",
    "    \n",
    "#     print(train_P.shape)\n",
    "\n",
    "    crop_train = []    # Remove these rows from training\n",
    "    for i in range(len(train_p)):\n",
    "        p = train_p[i]\n",
    "        q = train_q[i]\n",
    "        y = train_y[i]\n",
    "        \n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_train.append(i)\n",
    "            continue\n",
    "\n",
    "\n",
    "        train_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        train_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        train_Y_start[i, y[0]] = 1\n",
    "        train_Y_end[i, y[1]] = 1\n",
    "\n",
    "    crop_test = []\n",
    "    for i in range(len(test_p)):\n",
    "        p = test_p[i]\n",
    "        q = test_q[i]\n",
    "        y = test_y[i]\n",
    "\n",
    "        # First see if you can keep this example or not (due to size)\n",
    "        if y[0] >= PARA_LEN or y[1] >= PARA_LEN:\n",
    "            crop_test.append(i)\n",
    "            continue\n",
    "\n",
    "        test_P[i, :min(PARA_LEN, len(p))] = p[:min(PARA_LEN, len(p))]\n",
    "        test_Q[i, :min(QUES_LEN, len(q))] = q[:min(QUES_LEN, len(q))]\n",
    "        test_Y_start[i, y[0]] = 1\n",
    "        test_Y_end[i, y[1]] = 1\n",
    "        \n",
    "        \n",
    "    # Remove the instances which are in crop_train\n",
    "    train_P = np.delete(train_P, crop_train, axis=0)\n",
    "    train_Q = np.delete(train_Q, crop_train, axis=0)\n",
    "    train_Y_start = np.delete(train_Y_start, crop_train, axis=0)\n",
    "    train_Y_end = np.delete(train_Y_end, crop_train, axis=0)\n",
    "    \n",
    "    test_P = np.delete(test_P, crop_test, axis=0)\n",
    "    test_Q = np.delete(test_Q, crop_test, axis=0)\n",
    "    test_Y_start = np.delete(test_Y_start, crop_test, axis=0)\n",
    "    test_Y_end = np.delete(test_Y_end, crop_test, axis=0)\n",
    "\n",
    "    if macros['debug'] >= 1:\n",
    "        print(\"Train Q: \", train_Q.shape)\n",
    "        print(\"Train P: \", train_P.shape)\n",
    "        print(\"Train Y: \", train_Y_start.shape)\n",
    "        print(\"Test Q: \", test_Q.shape)\n",
    "        print(\"Test P: \", test_P.shape)\n",
    "        print(\"Test Y: \", test_Y_start.shape)\n",
    "        print(\"Crop_train: \", len(crop_train))\n",
    "        print(\"Crop_test: \", len(crop_test))\n",
    "    # Let's free up some memory now\n",
    "    train_p, train_q, train_y, test_p, test_q, test_y = None, None, None, None, None, None\n",
    "    \n",
    "    # Load embedding matrics\n",
    "    vectors = np.load(os.path.join(data_loc, 'glove.new.trimmed.300.npy'))\n",
    "    \n",
    "    return train_P[:10000], train_Q[:10000], train_Y_start[:10000], train_Y_end[:10000], test_P, test_Q, test_Y_start, test_Y_end, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macros = {\n",
    "#     \"ques_len\": QUES_LEN,\n",
    "#     \"hidden_dim\": HIDDEN_DIM, \n",
    "#     \"vocab_size\": VOCAB_SIZE, \n",
    "#     \"batch_size\": BATCH_SIZE,\n",
    "#     \"para_len\": PARA_LEN,\n",
    "#     \"embedding_dim\": EMBEDDING_DIM,\n",
    "#     \"debug\": 5\n",
    "# } \n",
    "\n",
    "# a = prepare_data(DATA_LOC, macros=macros, crop=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, and running the model\n",
    "- Write a train fn\n",
    "- Write a training loop invoking it\n",
    "- Fill in real data\n",
    "\n",
    "----------\n",
    "\n",
    "Feats:\n",
    "- Function to test every n epochs.\n",
    "- Report train accuracy every epoch\n",
    "- Store the train, test accuracy for every instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(loc, models, epochs=0, optimizer=None):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            loc: str of the folder where the models are to be saved\n",
    "            models: dict of 'model_name': model_object\n",
    "            epochs, optimizers are int, torch.optims (discarded right now).\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(models) is dict and len(models.keys()) == 4\n",
    "    \n",
    "    # Assumes four models. Doesn't save device/epochs/optimizer right now.\n",
    "    \n",
    "    for name in models:\n",
    "        torch.save(models[name], os.path.join(loc, name+'.torch'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(para_batch,\n",
    "          ques_batch,\n",
    "          answer_start_batch,\n",
    "          answer_end_batch,\n",
    "          ques_model,\n",
    "          para_model,\n",
    "          mlstm_model,\n",
    "          pointer_decoder_model,\n",
    "          optimizer, \n",
    "          loss_fn,\n",
    "          macros,\n",
    "          debug=2,\n",
    "          train=True):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param mlstm_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "        # Temporarily infer batch size, use and then at last, put this value back\n",
    "        BATCH_SIZE = macros['batch_size']\n",
    "        macros['batch_size'] = para_batch.shape[0]\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_paraenc = para_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(macros['batch_size'], device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(macros['batch_size'], device)\n",
    "        h_ri = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, macros['batch_size'], macros['hidden_dim']), dtype=torch.float, device=device)\n",
    "        if debug >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "\n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device=device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device=device)\n",
    "        if debug >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "    #         raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, macros['batch_size'], 2*macros['hidden_dim']), h_ri, H_q, hidden_mlstm, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        h_ak, hidden_ptrnet, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device=device)\n",
    "        if debug >= 2: print(\"------------Passed through pointernet------------\")\n",
    "\n",
    "\n",
    "        # For crossentropy\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "#         print(beta_k_start.view(-1, macros['para_len']).shape, answer_start_batch.view(-1).shape)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(beta_k_start.view(-1, macros['para_len']), answer_start_batch)\n",
    "        loss += loss_fn(beta_k_end.view(-1, macros['para_len']), answer_end_batch)\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "\n",
    "        if train:\n",
    "            #optimization step\n",
    "            optimizer.step()\n",
    "            if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "        \n",
    "        macros['batch_size'] = BATCH_SIZE\n",
    "        return beta_k_start, beta_k_end, loss\n",
    "    \n",
    "    except: \n",
    "        macros['batch_size'] = BATCH_SIZE\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(para_batch,\n",
    "                 ques_batch,\n",
    "                 answer_start_batch,\n",
    "                 answer_end_batch,\n",
    "                 matchlstm_model,\n",
    "                 optimizer, \n",
    "                 loss_fn,\n",
    "                 macros,\n",
    "                 debug=2,\n",
    "                 train=True):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    :param para_batch: paragraphs (batch, max_seq_len_para) \n",
    "    :param ques_batch: questions corresponding to para (batch, max_seq_len_ques)\n",
    "    :param answer_start_batch: one-hot vector denoting pos of span start (batch, max_seq_len_para)\n",
    "    :param answer_end_batch: one-hot vector denoting pos of span end (batch, max_seq_len_para)\n",
    "    \n",
    "    # Models\n",
    "    :param ques_model: model to encode ques\n",
    "    :param para_model: model to encode para\n",
    "    :param mlstm_model: model to match para, ques to get para summary\n",
    "    :param pointer_decoder_model: model to get a pointer over start and end span pointer\n",
    "    \n",
    "    # Loss and Optimizer.\n",
    "    :param loss_fn: \n",
    "    :param optimizer: \n",
    "    \n",
    "    :return: \n",
    "    \n",
    "    \n",
    "    NOTE: When using MSE, \n",
    "        - target labels are one-hot\n",
    "        - target label is float tensor\n",
    "        - shape (batch, 1, len)\n",
    "        \n",
    "        When using CrossEntropy\n",
    "        - target is not onehot\n",
    "        - long\n",
    "        - shape (batch, )\n",
    "    \"\"\"\n",
    "    try:    \n",
    "        # Temporarily infer batch size, use and then at last, put this value back\n",
    "        BATCH_SIZE = macros['batch_size']\n",
    "        macros['batch_size'] = para_batch.shape[0]\n",
    "\n",
    "        if debug >=2: \n",
    "            print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "            print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "            print(\"\\tanswer_start_batch:\\t\", answer_start_batch.shape)\n",
    "            print(\"\\tanswer_end_batch:\\t\\t\", answer_end_batch.shape)\n",
    "\n",
    "        # Wiping all gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # passing th data through matchLSTM\n",
    "        ans_range_prop, beta, _ = model.forward(ques_batch,para_batch)\n",
    "        \n",
    "        # creating batch answer\n",
    "        _, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "        _, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "        answer_start_batch = answer_start_batch.view(-1).long()\n",
    "        answer_end_batch = answer_end_batch.view(-1).long()\n",
    "        \n",
    "        # TODO: Check if the logic works correctly.\n",
    "        answer_range_batch = np.hstack((answer_start_batch,answer_end_batch))\n",
    "        \n",
    "        # passing it through custom loss function\n",
    "        if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "        loss = criterion.forward(ans_range_prop, answer_range_batch)\n",
    "        \n",
    "        \n",
    "        if train:\n",
    "            if debug >= 2: print(\"------------Calculated Gradients------------\")\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_max)  # fix gradient explosion\n",
    "            if debug >= 2: print(\"------------Updated weights.------------\")\n",
    "            optimizer.step()\n",
    "        \n",
    "        macros['batch_size'] = BATCH_SIZE\n",
    "        return beta[:,0], beta[:,1], loss\n",
    "    \n",
    "    except: \n",
    "        macros['batch_size'] = BATCH_SIZE\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function (no grad, no eval)\n",
    "def predict(para_batch,\n",
    "            ques_batch,\n",
    "            ques_model,\n",
    "            para_model,\n",
    "            mlstm_model,\n",
    "            pointer_decoder_model,\n",
    "            macros,\n",
    "            loss_fn=None,\n",
    "            debug=DEBUG):\n",
    "    \"\"\"\n",
    "        Function which returns the model's output based on a given set of P&Q's. \n",
    "        Does not convert to strings, gives the direct model output.\n",
    "        \n",
    "        Expects:\n",
    "            four models\n",
    "            data\n",
    "            misc macros\n",
    "    \"\"\"\n",
    "    \n",
    "#     BATCH_SIZE = macros['batch_size']\n",
    "    BATCH_SIZE = ques_batch.shape[0]\n",
    "    HIDDEN_DIM = macros['hidden_dim']\n",
    "    DEBUG = debug\n",
    "    \n",
    "    if debug >=2: \n",
    "        print(\"\\tpara_batch:\\t\\t\", para_batch.shape)\n",
    "        print(\"\\tques_batch:\\t\\t\", ques_batch.shape)\n",
    "        \n",
    "    with torch.no_grad():    \n",
    "\n",
    "        # Initializing all hidden states.\n",
    "        hidden_quesenc = ques_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_paraenc = para_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_mlstm = mlstm_model.init_hidden(BATCH_SIZE, device)\n",
    "        hidden_ptrnet = pointer_decoder_model.init_hidden(BATCH_SIZE, device)\n",
    "        h_ri = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        h_ak = torch.zeros((1, BATCH_SIZE, HIDDEN_DIM), dtype=torch.float, device=device)\n",
    "        if DEBUG >= 2: print(\"------------Instantiated hidden states------------\")\n",
    "            \n",
    "        #passing the data through LSTM pre-processing layer\n",
    "        H_q, ques_model_hidden = ques_model(ques_batch, hidden_quesenc, device)\n",
    "        H_p, para_model_hidden = para_model(para_batch, hidden_paraenc, device)\n",
    "        if DEBUG >= 2: \n",
    "            print(\"\\tH_q:\\t\\t\", H_q.shape)\n",
    "            print(\"\\tH_p:\\t\\t\", H_p.shape)\n",
    "            print(\"\\tH_ri:\\t\\t\", h_ri.shape)\n",
    "#             raw_input(\"Check memory and ye shall continue\")\n",
    "            print(\"------------Encoded hidden states------------\")\n",
    "\n",
    "        H_r = mlstm_model(H_p.view(-1, BATCH_SIZE, 2*HIDDEN_DIM), h_ri, H_q, hidden_mlstm, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through matchlstm------------\")\n",
    "\n",
    "        #Passing the paragraph embddin via pointer network to generate final answer pointer.\n",
    "        h_ak, hidden_ptrnet, beta_k_start = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        _, _, beta_k_end = pointer_decoder_model(h_ak, H_r, hidden_ptrnet, device)\n",
    "        if DEBUG >= 2: print(\"------------Passed through pointernet------------\")\n",
    "                            \n",
    "        # For crossentropy\n",
    "#         _, answer_start_batch = answer_start_batch.max(dim=2)[1]\n",
    "#         _, answer_end_batch = answer_end_batch.max(dim=2)[1]\n",
    "#         print(\"labels: \", answer_start_batch.shape)[1]\n",
    "            \n",
    "#         #How will we manage batches for loss.\n",
    "#         loss = loss_fn(beta_k_start, answer_start_batch)\n",
    "#         loss += loss_fn(beta_k_end, answer_end_batch)\n",
    "#         if debug >= 2: print(\"------------Calculated loss------------\")\n",
    "            \n",
    "        return (beta_k_start, beta_k_end, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval function (no grad no eval no nothing)\n",
    "def eval(y_cap, y):\n",
    "    \"\"\" \n",
    "        Returns the exact-match (em) metric by default.\n",
    "        Can specifiy more in a list (TODO)\n",
    "        \n",
    "        Inputs:\n",
    "        - y_cap: list of two tensors (start, end) of dim [BATCH_SIZE, PARA_LEN] each\n",
    "        - y: list of two tensors (start, end) of dim [BATCH_SIZE, 1] each\n",
    "    \"\"\"\n",
    "    metrics={'em':None, 'p':None, 'r':None, 'f1':None}\n",
    "    \n",
    "#     y_cap= torch.argmax(y_cap[0], dim=1).float(), torch.argmax(y_cap[1], dim=1).float()\n",
    "#     y = torch.argmax(y[0], dim=1).float(), torch.argmax(y[1], dim=1).float()\n",
    "    \n",
    "    \n",
    "    # If we want f1 and haven't specified that we want p and q, fuck it and add it there\n",
    "    if 'f1' in metrics.keys():\n",
    "        metrics['p'] = None \n",
    "        metrics['r'] = None\n",
    "    \n",
    "    # Convert to numpy arrays of size (batch, 2)\n",
    "    y_cap= np.vstack((torch.argmax(y_cap[0], dim=1), torch.argmax(y_cap[1], dim=1))).transpose()\n",
    "    y = np.vstack((torch.argmax(y[0], dim=1), torch.argmax(y[1], dim=1))).transpose()\n",
    "      \n",
    "    # First, if start > end, fix that (we're cool that way.)\n",
    "    for i in range(y_cap.shape[0]):\n",
    "        if y_cap[i][0] > y_cap[i][1]: \n",
    "            y_cap[i] = y_cap[i][[1,0]]\n",
    "            \n",
    "    # First, if start > end, fix that (we're cool that way.)\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i][0]> y[i][1]: \n",
    "            y[i] = y[i][[1,0]]\n",
    "            \n",
    "            \n",
    "    if \"em\" in metrics.keys():\n",
    "        metrics['em'] = np.mean(np.logical_and(np.equal(y[:,0], y_cap[:,0]),np.equal(y[:,1], y_cap[:,1])))\n",
    "            \n",
    "    if 'f1' in metrics.keys():\n",
    "        \n",
    "        f1, pr, rk = [], [], []\n",
    "        for i in range(y.shape[0]):\n",
    "            \n",
    "            if y[i][0] == y[i][1]:\n",
    "                _y = [int(y[i][0])]\n",
    "            else:\n",
    "                _y = range(y[i][0], y[i][1])\n",
    "                \n",
    "            if y_cap[i][0] == y_cap[i][1]:\n",
    "                _y_cap = [int(y_cap[i][0])]\n",
    "            else:\n",
    "                _y_cap = range(y_cap[i][0], y_cap[i][1])\n",
    "            \n",
    "            intersection = len(set(_y).intersection(_y_cap))\n",
    "            \n",
    "            positives = float(len(_y_cap))\n",
    "            truth = float(len(_y))\n",
    "            \n",
    "            try:\n",
    "                p = intersection/positives\n",
    "                r = intersection/truth\n",
    "            except ZeroDivisionError:\n",
    "                print(\"Ran into zero division error. Here are the inputs.\")\n",
    "                print(_y)\n",
    "                print(_y_cap)\n",
    "                \n",
    "                p = 0\n",
    "                r = 0\n",
    "            \n",
    "            f = (2*p*r)/(p+r) if p > 0 and r > 0 else 0.0\n",
    "            \n",
    "            f1.append(f)\n",
    "            pr.append(p)\n",
    "            rk.append(r)\n",
    "            \n",
    "        f1 = np.mean(f1)\n",
    "        pr = np.mean(pr)\n",
    "        rk = np.mean(rk)\n",
    "        \n",
    "        metrics['f1'] = f1\n",
    "        metrics['p'] = pr\n",
    "        metrics['r'] = rk\n",
    "            \n",
    "    if DEBUG >= 3: \n",
    "        print(\"Test performance: \", metrics)\n",
    "        print(\"------------Evaluated------------\")\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "if True:\n",
    "    # Testing this function\n",
    "    metrics = {'em':None}\n",
    "#     y = torch.tensor([[3]]).float(), torch.tensor([[4]]).float()\n",
    "    y = torch.tensor([[0,0,3,0], [0,2,0,0]]), torch.tensor([[0,0,0,3], [0,0,0,3]])\n",
    "    y_cap = torch.tensor([[3,0,0,0],[0,0,3,0]]), torch.tensor([[0,2,0,1],[0,1,0,0]])\n",
    "#     y = torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float(), torch.randint(0, PARA_LEN, (BATCH_SIZE,)).float()\n",
    "#     y_cap = torch.rand((BATCH_SIZE, PARA_LEN)), torch.rand((BATCH_SIZE, PARA_LEN))\n",
    "    print(eval(y_cap, y))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(_models, _data, _macros, _epochs, _save=0, _test_eval=0, _train_eval=0, _debug=2):\n",
    "    \"\"\"\n",
    "        > Instantiate models\n",
    "        > Instantiate loss, optimizer\n",
    "        > Instantiate ways to store loss\n",
    "\n",
    "        > Per epoch\n",
    "            > sample batch and give to train fn\n",
    "            > get loss\n",
    "            > if epoch %k ==0: get test accuracy\n",
    "\n",
    "        > have fn to calculate test accuracy\n",
    "        \n",
    "        > _save: int\n",
    "            > 0: dont\n",
    "            > 1+: save every _save epoch (overwrite)\n",
    "            > -1 -> save best (turned to 1 if test evals dont happen.)\n",
    "        \n",
    "        > Save the model at every epoch if we don't test on test. \n",
    "            > else save on the best performning mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack data\n",
    "    DEBUG = _debug\n",
    "    train_P = _data['train']['P']\n",
    "    train_Q = _data['train']['Q']\n",
    "    train_Y_start = _data['train']['Ys']\n",
    "    train_Y_end = _data['train']['Ye']\n",
    "    test_P = _data['test']['P']\n",
    "    test_Q = _data['test']['Q']\n",
    "    test_Y_start = _data['test']['Ys']\n",
    "    test_Y_end = _data['test']['Ye']\n",
    "\n",
    "    ques_model, para_model, mlstm_model, pointer_decoder_model = _models\n",
    "    _data = None\n",
    "\n",
    "    # Instantiate Loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, ques_model.parameters())) + \n",
    "                             list(filter(lambda p: p.requires_grad, para_model.parameters())) + \n",
    "                             list(mlstm_model.parameters()) + \n",
    "                             list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "#         optimizer = optim.Adam(list(ques_model.parameters()) + \\\n",
    "#                                list(para_model.parameters()) + \\\n",
    "#                                list(mlstm_model.parameters()) + \\\n",
    "#                               list(pointer_decoder_model.parameters()), lr=macros['lr'])\n",
    "\n",
    "    # Losses\n",
    "    train_losses = []\n",
    "    train_em = []\n",
    "    train_f = []\n",
    "    test_losses = []\n",
    "    test_em = []\n",
    "    test_f = []\n",
    "    best_test = 0.0\n",
    "    found_best_test = False\n",
    "    traces_test = None\n",
    "    traces_train = None\n",
    "    try: \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(_epochs):\n",
    "            print(\"Epoch: \", epoch, \"/\", _epochs)\n",
    "\n",
    "            epoch_loss = []\n",
    "            epoch_train_em = []\n",
    "            epoch_train_f = []\n",
    "            epoch_time = time.time()\n",
    "            epoch_traces = {'true':{'start':[], 'end':[]}, 'pred':{'start':[], 'end':[]}}\n",
    "            epoch_traces_train = {'true':{'start':[], 'end':[]}, 'pred':{'start':[], 'end':[]}}\n",
    "            \n",
    "            for iter in range(int(len(train_P)/BATCH_SIZE)):\n",
    "#             for iter in range(2):\n",
    "\n",
    "                batch_time = time.time()\n",
    "\n",
    "                # Sample batch and train on it\n",
    "                sample_index = np.random.randint(0, len(train_P), _macros['batch_size'])\n",
    "            \n",
    "                y_cap_start, y_cap_end, loss = train(\n",
    "                    para_batch = torch.tensor(train_P[sample_index], dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(train_Q[sample_index], dtype=torch.long, device=device),\n",
    "                    answer_start_batch = torch.tensor(train_Y_start[sample_index], dtype=torch.float, device=device).view( _macros['batch_size'], 1, _macros['para_len']),\n",
    "                    answer_end_batch = torch.tensor(train_Y_end[sample_index], dtype=torch.float, device=device).view(_macros['batch_size'], 1, _macros['para_len']),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    optimizer = optimizer, \n",
    "                    loss_fn= loss_fn,\n",
    "                    macros=_macros,\n",
    "                    debug=_macros['debug']\n",
    "                )\n",
    "                \n",
    "                epoch_traces_train['true']['start'].append(train_Y_start[sample_index])\n",
    "                epoch_traces_train['true']['end'].append(train_Y_end[sample_index])\n",
    "                epoch_traces_train['pred']['start'].append(y_cap_start.cpu().detach().numpy())\n",
    "                epoch_traces_train['pred']['end'].append(y_cap_end.cpu().detach().numpy())\n",
    "\n",
    "                if _train_eval: \n",
    "\n",
    "                    # Calculate train accuracy for this minibatch\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(train_Y_start[sample_index], device=device),\n",
    "                            torch.tensor(train_Y_end[sample_index], device=device)),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "                    epoch_train_em.append(metrics['em'])\n",
    "                    epoch_train_f.append(metrics['f1'])\n",
    "    \n",
    "                epoch_loss.append(loss.item())\n",
    "    \n",
    "#                 grad_new = sum([x.grad.sum().item() for x in params])\n",
    "\n",
    "                print(\"Batch:\\t%d\" % iter,\"/%d\\t\\b: \" % (len(train_P)/_macros['batch_size']),\n",
    "                      str(\"%s\" % (time.time() - batch_time))[:8], \n",
    "                      str(\"\\t\\b%s\" % (time.time() - epoch_time))[:10], \n",
    "                      \"\\tl:%f\" % loss.item(),\n",
    "                      \"\\tem:%f\" % epoch_train_em[-1] if _train_eval else \"\",\n",
    "                     \"\\t\\bf1:%f\" % epoch_train_f[-1] if _train_eval else \"\")\n",
    "#                      \"\\t\\b\\b%s\" % grad_new - grad_old)\n",
    "#                      end=None if iter+1 == int(len(train_P)/BATCH_SIZE) else \"\\r\")\n",
    "\n",
    "            train_losses.append(epoch_loss)\n",
    "        \n",
    "            if _train_eval: \n",
    "                train_em.append(epoch_train_em)\n",
    "                train_f.append(epoch_train_f)\n",
    "\n",
    "            # TEMP\n",
    "            # Save model now\n",
    "            models = { 'ques_model': ques_model,\n",
    "                   'para_model': para_model,\n",
    "                   'mlstm_model':  mlstm_model,\n",
    "                   'pointer_decoder_model': pointer_decoder_model\n",
    "                 }\n",
    "                \n",
    "            save_model(macros['save_model_loc'], models,\n",
    "                          epochs=epoch,\n",
    "                           optimizer=optimizer)\n",
    "                \n",
    "            print(\"Saving new model on epoch %d\" % epoch)\n",
    "                \n",
    "                \n",
    "            if _test_eval and epoch % _test_eval == 0:\n",
    "            \n",
    "                metrics_epoch_test = []\n",
    "                for i_batch in range(0, len(test_P),_macros['batch_size']):\n",
    "                    test_p = test_P[i_batch: i_batch+_macros['batch_size']]\n",
    "                    test_q = test_Q[i_batch: i_batch+_macros['batch_size']]\n",
    "                    test_y_start = test_Y_start[i_batch: i_batch+_macros['batch_size']]\n",
    "                    test_y_end =  test_Y_end[i_batch: i_batch+_macros['batch_size']]\n",
    "\n",
    "#                     y_cap_start, y_cap_end, test_loss = train(\n",
    "#                         para_batch = torch.tensor(test_p, dtype=torch.long, device=device),\n",
    "#                         ques_batch = torch.tensor(test_q, dtype=torch.long, device=device),\n",
    "#                         ques_model = ques_model,\n",
    "#                         para_model = para_model,\n",
    "#                         mlstm_model = mlstm_model,\n",
    "#                         pointer_decoder_model = pointer_decoder_model,\n",
    "#                         macros = _macros,\n",
    "#                         loss_fn = loss_fn,\n",
    "#                         debug = _macros['debug'],\n",
    "                        \n",
    "#                     )\n",
    "                    y_cap_start, y_cap_end, test_loss = train(\n",
    "                        para_batch = torch.tensor(test_p, dtype=torch.long, device=device),\n",
    "                        ques_batch = torch.tensor(test_q, dtype=torch.long, device=device),\n",
    "                        answer_start_batch = torch.tensor(test_y_start, dtype=torch.float, device=device).view( -1, 1, _macros['para_len']),\n",
    "                        answer_end_batch = torch.tensor(test_y_end, dtype=torch.float, device=device).view(-1, 1, _macros['para_len']),\n",
    "                        ques_model = ques_model,\n",
    "                        para_model = para_model,\n",
    "                        mlstm_model = mlstm_model,\n",
    "                        pointer_decoder_model = pointer_decoder_model,\n",
    "                        optimizer = optimizer, \n",
    "                        loss_fn= loss_fn,\n",
    "                        macros=_macros,\n",
    "                        debug=_macros['debug'],\n",
    "                        train=False\n",
    "                    )\n",
    "                    metrics = eval(\n",
    "                        y=(torch.tensor(test_y_start, device=device),\n",
    "                            torch.tensor(test_y_end, device=device)),\n",
    "                        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "                    metrics_epoch_test.append(metrics)\n",
    "                    \n",
    "                    epoch_traces['true']['start'].append(test_y_start)\n",
    "                    epoch_traces['true']['end'].append(test_y_end)\n",
    "                    epoch_traces['pred']['start'].append(y_cap_start.cpu().detach().numpy())\n",
    "                    epoch_traces['pred']['end'].append(y_cap_end.cpu().detach().numpy())\n",
    "                \n",
    "                # Find em and f1\n",
    "                em = np.mean([metric['em'] for metric in metrics_epoch_test])\n",
    "                f1 = np.mean([metric['f1'] for metric in metrics_epoch_test])\n",
    "\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_em.append(em)\n",
    "                test_f.append(f1)\n",
    "                \n",
    "                # Check if we outperformed the best one.\n",
    "                if f1 > best_test:\n",
    "                    \n",
    "                    # Set flag\n",
    "                    found_best_test = True\n",
    "                    \n",
    "                    # Update value\n",
    "                    best_test = f1\n",
    "                    \n",
    "                    # Update traces for the best one.\n",
    "                    traces_train = epoch_traces_train\n",
    "                    traces_test = epoch_traces\n",
    "\n",
    "            # Saving logic\n",
    "            if _save == 0:\n",
    "                pass\n",
    "            elif ( _save>0 and epoch % _save == 0) or \\\n",
    "            ( _save == -1 and found_best_test ):\n",
    "                models = { 'ques_model': ques_model,\n",
    "                           'para_model': para_model,\n",
    "                           'mlstm_model':  mlstm_model,\n",
    "                           'pointer_decoder_model': pointer_decoder_model\n",
    "                         }\n",
    "                \n",
    "                save_model(macros['save_model_loc'], models,\n",
    "                          epochs=epoch,\n",
    "                           optimizer=optimizer)\n",
    "                \n",
    "                print(\"Saving new model on epoch %d\" % epoch)\n",
    "            \n",
    "            # Reset flags\n",
    "            found_best_test = False\n",
    "            \n",
    "#             At the end of every epoch, do print the average epoch loss, and other stat\n",
    "            print(\"\\nEpoch performance: \",\n",
    "                  \"%ssec\" % str(time.time() - epoch_time)[:6],\n",
    "                  \"Trl:%f\" % np.mean(epoch_loss, axis=0),\n",
    "                  \"Tel:%f\" % test_losses[-1],\n",
    "                  \"\\n\\tTrem:%f\" % np.mean(epoch_train_em) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTrf1:%f\" % np.mean(epoch_train_f) if _train_eval and epoch % _train_eval == 0 else \"\",\n",
    "                  \"\\tTeem:%f\" % test_em[-1] if _test_eval and epoch % _test_eval == 0 else \"\",\n",
    "                  \"\\tTef1:%f\\n\" % test_f[-1] if _test_eval and epoch % _test_eval == 0 else \"\\n\")\n",
    "\n",
    "        return train_losses, train_em, test_losses, test_em\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        # someone called a ctrl+c on it. Let' return the things computed so far atlest.\n",
    "        print(\"Found keyboard interrupt. Stopping training loop\")\n",
    "        \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:       \n",
    "        return train_losses, train_em, train_f, test_losses, test_em, test_f, best_test, traces_train, traces_test\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(loss, loss2=None, _label=\"Some label\", _label2=\"Some other label\", _name=\"Generic Name\", _only_epoch=True):\n",
    "    \"\"\"\n",
    "        Fn to visualize loss.\n",
    "        Expects either\n",
    "            - [int, int] for epoch level stuff\n",
    "            - [ [int, int], [int, int] ] for batch level data. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 8] \n",
    "    \n",
    "    # Detect input format\n",
    "    if type(loss[0]) is not list: #in [int, float, long]:\n",
    "        \n",
    "#         print(\"here\")\n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    elif type(loss[0]) == list:\n",
    "        \n",
    "        if _only_epoch:\n",
    "            loss = [ np.mean(x) for x in loss ]\n",
    "            if loss2 is not None: \n",
    "                loss2 = [ np.mean(x) for x in loss2 ]\n",
    "            \n",
    "        else:\n",
    "            loss = [ y for x in loss for y in x ]\n",
    "            if loss2 is not None: loss2 = [ y for x in loss2 for y in x ]\n",
    "            \n",
    "        plt.plot(loss, '-b', label=_label)\n",
    "        if loss2 is not None: plt.plot(loss2, '-r', label=_label2)\n",
    "        plt.ylabel(_name)\n",
    "        pylab.legend(loc='upper left')\n",
    "        plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_traces(trace):\n",
    "    print('Start')\n",
    "#     x = [ np.argmax(x, axis=1) for x in trace['true']['start']]\n",
    "#     for a in x:\n",
    "#         print(a.shape)\n",
    "# #     b = np.hstack([ np.argmax(x.reshape(-1, macros['para_len']), axis=1) for x in trace['true']['start'][:-1]])\n",
    "# #     a =\n",
    "#     print(a.shape)\n",
    "#     print(b.shape)\n",
    "#     print(x.shape)\n",
    "#     print(x.reshape(-1, macros['para_len']).shape)\n",
    "#     print(np.argmax(x.reshape(-1, macros['para_len']), axis=1).shape)\n",
    "    cm = confusion_matrix(\n",
    "        np.hstack([ np.argmax(x.reshape(-1, macros['para_len']), axis=1) for x in trace['true']['start'][:-1]]),\n",
    "        np.hstack([ np.argmax(x.reshape(-1, macros['para_len']), axis=1) for x in trace['pred']['start'][:-1]]))\n",
    "    cm.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    print('End')\n",
    "    cm = confusion_matrix(\n",
    "        np.hstack([ np.argmax(x.reshape(-1, macros['para_len']), axis=1) for x in trace['true']['end'][:-1]]),\n",
    "        np.hstack([ np.argmax(x.reshape(-1, macros['para_len']), axis=1) for x in trace['pred']['end'][:-1]]))\n",
    "    cm.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator\n",
    "\n",
    "One cell which instantiates and runs everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Cell which pulls everything together.\n",
    "\n",
    "    > init models\n",
    "    > get data prepared\n",
    "    > pass models and data to training loop\n",
    "    > gets trained models and loss\n",
    "    > saves models\n",
    "    > visualizes loss?\n",
    "\n",
    "No other function but this one ever sees global macros!\n",
    "\"\"\"\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":DEBUG,\n",
    "    \"save_model_loc\": MODEL_LOC,\n",
    "    \"epochs\": EPOCHS\n",
    "#     \"device\": device\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train':{}, 'test':{}}\n",
    "data['train']['P'], data['train']['Q'], data['train']['Ys'], data['train']['Ye'], \\\n",
    "data['test']['P'], data['test']['Q'], data['test']['Ys'], data['test']['Ye'], vectors = \\\n",
    "    prepare_data(DATA_LOC, macros, crop=CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(index,test=False):\n",
    "    if test:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate models\n",
    "ques_model = Encoder(QUES_LEN, macros, vectors, device).cuda(device)\n",
    "para_model = Encoder(PARA_LEN, macros, vectors, device).cuda(device)\n",
    "mlstm_model = MatchLSTMEncoder(macros, device).cuda(device)\n",
    "pointer_decoder_model = PointerDecoder(macros, device).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op = training_loop(_models=[ques_model, para_model, mlstm_model, pointer_decoder_model],\n",
    "                   _data=data,\n",
    "                   _debug=macros['debug'],\n",
    "                   _save=-1,\n",
    "                   _test_eval=1,\n",
    "                   _train_eval=1,\n",
    "                   _epochs=macros['epochs'],\n",
    "                   _macros=macros)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "So far, we plot the training losss. \n",
    "Shall we superimpose test loss on it too? We don't calculate test loss per batch though (fortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "print(\"Training Loss\")\n",
    "visualize_loss(loss=op[0], _name=\"train loss\", _only_epoch=True)\n",
    "# loss2=op[3], _label=\"train loss\", _label2=\"test_loss\",\n",
    "\n",
    "print(\"Test Loss\")\n",
    "visualize_loss(loss=op[4], _name=\"test loss\", _only_epoch=True)\n",
    "\n",
    "# if len(op[1]) > 0:\n",
    "\n",
    "print(\"Exact Match\")\n",
    "visualize_loss(loss=op[1], loss2=op[4], _label=\"train\", _label2=\"test\", _name=\"Exact Match\", _only_epoch=True)\n",
    "# visualize_loss(loss=op[1], _label=\"train em\", _label2=\"test em\", _only_epoch=True)\n",
    "\n",
    "print(\"F-Measure\")\n",
    "visualize_loss(loss=op[2], loss2=op[5], _label=\"train\", _label2=\"test\", _name=\"F-Measure\")\n",
    "\n",
    "# op[3]\n",
    "# print(op[1])\n",
    "print(\"Conf Mat Train\")\n",
    "visualize_traces(op[7])\n",
    "\n",
    "print(\"Conf Mat Test\")\n",
    "visualize_traces(op[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = op[-1]\n",
    "b = a['true']['start']\n",
    "np.argmax(b[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(op, open('./performance/domain-noglove-17-07-2018/op.dump', 'wb+'))\n",
    "print(\"Best F1: \", op[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing (temp)\n",
    "# models = { 'ques_model': ques_model,\n",
    "#            'para_model': para_model,\n",
    "#            'mlstm_model':  mlstm_model,\n",
    "#            'pointer_decoder_model': pointer_decoder_model\n",
    "#          }\n",
    "# save_model(loc=macros['save_model_loc'], models=models, epochs=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Try loading the model\n",
    "ques_model = torch.load(os.path.join(macros['save_model_loc'], 'ques_model.torch'))\n",
    "print(\"Ques Model\\n\", ques_model)\n",
    "\n",
    "para_model = torch.load(os.path.join(macros['save_model_loc'], 'para_model.torch'))\n",
    "print(\"Para Model\\n\", para_model)\n",
    "\n",
    "mlstm_model = torch.load(os.path.join(macros['save_model_loc'], 'mlstm_model.torch'))\n",
    "print(\"MLSTM Model\\n\", mlstm_model)\n",
    "\n",
    "pointer_decoder_model = torch.load(os.path.join(macros['save_model_loc'], 'pointer_decoder_model.torch'))\n",
    "print(\"Pointer Decoder model\\n\", pointer_decoder_model)\n",
    "\n",
    "# # Create dummy data for testing the predict fn\n",
    "# q = np.random.randint(0, len(vectors), (1, 30))\n",
    "# p = np.random.randint(0, len(vectors), (1, 200))\n",
    "# qa = np.repeat(q,macros['batch_size'],axis=0)\n",
    "# pa = np.repeat(p,macros['batch_size'],axis=0)\n",
    "\n",
    "# qb = np.repeat(q,10,axis=0)\n",
    "# pb = np.repeat(p,10,axis=0)\n",
    "\n",
    "# # print(p_repeat.shape)\n",
    "# # print(q_repeat.shape)\n",
    "\n",
    "# ysa, yea, _ = predict(torch.tensor(pa, dtype=torch.long, device=device), \n",
    "#                                    torch.tensor(qa, dtype=torch.long, device=device),\n",
    "#                                    ques_model=ques_model.eval(),\n",
    "#                                    para_model=para_model.eval(),\n",
    "#                                    mlstm_model=mlstm_model.eval(),\n",
    "#                                    pointer_decoder_model=pointer_decoder_model.eval(),\n",
    "#                                     macros=macros,\n",
    "#                                     debug=macros['debug'])\n",
    "\n",
    "# ysb, yeb, _ = predict(torch.tensor(pb, dtype=torch.long, device=device), \n",
    "#                                    torch.tensor(qb, dtype=torch.long, device=device),\n",
    "#                                    ques_model=ques_model.eval(),\n",
    "#                                    para_model=para_model.eval(),\n",
    "#                                    mlstm_model=mlstm_model.eval(),\n",
    "#                                    pointer_decoder_model=pointer_decoder_model.eval(),\n",
    "#                                     macros=macros,\n",
    "#                                     debug=macros['debug'])\n",
    "\n",
    "# print(torch.argmax(ysa.squeeze(), dim=1))\n",
    "# print(torch.argmax(ysb.squeeze(), dim=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h = ques_model.init_hidden(100, device)\n",
    "q = torch.randint(0, 10, (100, 30), device=device)\n",
    "p = torch.randint(0, 10, (100, 200), device=device)\n",
    "\n",
    "_q, _ = ques_model(q.long(), h, device)\n",
    "_p, _ = ques_model(p.long(), h, device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "h1 = ques_model.init_hidden(50, device)\n",
    "p1, p2 = p[:50], p[50:]\n",
    "q1, q2 = q[:50], q[50:]\n",
    "_p1, _ = ques_model(p1.long(), h1, device)\n",
    "_p2, _ = ques_model(p2.long(), h1, device)\n",
    "\n",
    "print(p1.shape)\n",
    "print(torch.mean(p))\n",
    "print(torch.mean(p1), torch.mean(p2))\n",
    "print(torch.mean(_p).item())\n",
    "print((torch.mean(_p1).item()+torch.mean(_p2).item())/2.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "macros = {\n",
    "    \"ques_len\": QUES_LEN,\n",
    "    \"hidden_dim\": HIDDEN_DIM, \n",
    "    \"vocab_size\": VOCAB_SIZE, \n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"para_len\": PARA_LEN,\n",
    "    \"embedding_dim\": EMBEDDING_DIM,\n",
    "    \"lr\": LR,\n",
    "    \"debug\":DEBUG,\n",
    "    \"save_model_loc\": MODEL_LOC\n",
    "#     \"device\": device\n",
    "} \n",
    "\n",
    "macros['batch_size'] = 100\n",
    "\n",
    "s, e, _ = predict(\n",
    "                    para_batch = torch.tensor(p, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(q, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = macros,\n",
    "                    loss_fn= None,\n",
    "                    debug = macros['debug']\n",
    "                )\n",
    "\n",
    "macros['batch_size'] = 50\n",
    "s1, e1, _ = predict(\n",
    "                    para_batch = torch.tensor(p1, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(q1, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = macros,\n",
    "                    loss_fn= None,\n",
    "                    debug = macros['debug']\n",
    "                )\n",
    "s2, e2, _ = predict(\n",
    "                    para_batch = torch.tensor(p2, dtype=torch.long, device=device),\n",
    "                    ques_batch = torch.tensor(q2, dtype=torch.long, device=device),\n",
    "                    ques_model = ques_model,\n",
    "                    para_model = para_model,\n",
    "                    mlstm_model = mlstm_model,\n",
    "                    pointer_decoder_model = pointer_decoder_model,\n",
    "                    macros = macros,\n",
    "                    loss_fn= None,\n",
    "                    debug = macros['debug']\n",
    "                )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(torch.mean(s).item())\n",
    "print((torch.mean(s1).item() + torch.mean(s2).item())/2.0)\n",
    "\n",
    "print(torch.argmax(s.squeeze(), dim=1))\n",
    "print(torch.stack((torch.argmax(s1.squeeze(), dim=1), torch.argmax(s2.squeeze(), dim=1))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s1 = torch.argmax(s1.squeeze(), dim=1)\n",
    "s2 = torch.argmax(s2.squeeze(), dim=1)\n",
    "s = torch.argmax(s.squeeze(), dim=1)[:50]\n",
    "e1 = torch.argmax(e1.squeeze(), dim=1)\n",
    "e2 = torch.argmax(e2.squeeze(), dim=1)\n",
    "e = torch.argmax(e.squeeze(), dim=1)[:50]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s,e)\n",
    "print(s1, e1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred = np.vstack((s1, e1)).transpose()\n",
    "y_true = np.vstack((s, e)).transpose()\n",
    "\n",
    "# First, if start > end, fix that (we're cool that way.)\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i][0] > y_pred[i][1]: \n",
    "        y_pred[i] = y_pred[i][[1,0]]\n",
    "        \n",
    "for i in range(y_true.shape[0]):\n",
    "    if y_true[i][0] > y_true[i][1]: \n",
    "        y_true[i] = y_true[i][[1,0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "np.mean(np.logical_and(np.equal(y_true[:,0], y_pred[:,0]),np.equal(y_true[:,1], y_pred[:,1])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_data = data\n",
    "train_P = _data['train']['P']\n",
    "train_Q = _data['train']['Q']\n",
    "train_Y_start = _data['train']['Ys']\n",
    "train_Y_end = _data['train']['Ye']\n",
    "test_P = _data['test']['P']\n",
    "test_Q = _data['test']['Q']\n",
    "test_Y_start = _data['test']['Ys']\n",
    "test_Y_end = _data['test']['Ye']\n",
    "_macros = macros\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "macros['batch_size'] = 100\n",
    "metrics_all = []\n",
    "p = ProgressBar()\n",
    "for i_batch in p(range(0, len(train_P[:200]),macros['batch_size'])):\n",
    "#     test_p = test_P[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_q = test_Q[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_y_start = test_Y_start[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_y_end =  test_Y_end[i_batch: i_batch+macros['batch_size']]\n",
    "\n",
    "    \n",
    "    test_p = train_P[i_batch: i_batch+macros['batch_size']]\n",
    "    test_q = train_Q[i_batch: i_batch+macros['batch_size']]\n",
    "    test_y_start = train_Y_start[i_batch: i_batch+macros['batch_size']]\n",
    "    test_y_end =  train_Y_end[i_batch: i_batch+macros['batch_size']]\n",
    "    \n",
    "    \n",
    "    y_cap_start, y_cap_end, test_loss = predict(\n",
    "        para_batch = torch.tensor(test_p, dtype=torch.long, device=device),\n",
    "        ques_batch = torch.tensor(test_q, dtype=torch.long, device=device),\n",
    "        ques_model = ques_model,\n",
    "        para_model = para_model,\n",
    "        mlstm_model = mlstm_model,\n",
    "        pointer_decoder_model = pointer_decoder_model,\n",
    "        macros = _macros,\n",
    "        loss_fn= loss_fn,\n",
    "        debug = _macros['debug']\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    metrics = eval(\n",
    "        y=(torch.tensor(test_y_start, device=device),\n",
    "            torch.tensor(test_y_end, device=device)),\n",
    "        y_cap=[y_cap_start.squeeze(), y_cap_end.squeeze()])\n",
    "\n",
    "    metrics_all.append(metrics)\n",
    "\n",
    "em = np.mean([metric['em'] for metric in metrics_all])\n",
    "f1 = np.mean([metric['f1'] for metric in metrics_all])\n",
    "print(em,f1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "answer_start_batch, answer_end_batch = test_y_start, test_y_end\n",
    "answer_start_batch = torch.tensor(answer_start_batch, dtype=torch.long, device=device).view(a.shape[0], 1, -1)\n",
    "answer_end_batch = torch.tensor(answer_end_batch, dtype=torch.long, device=device).view(a.shape[0], 1, -1)\n",
    "# # For crossentropy\n",
    "_, answer_start_batch = answer_start_batch.max(dim=2)\n",
    "_, answer_end_batch = answer_end_batch.max(dim=2)\n",
    "answer_start_batch = answer_start_batch.view(-1).long()\n",
    "answer_end_batch = answer_end_batch.view(-1).long()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(answer_start_batch)\n",
    "print(answer_end_batch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tr = data['test']['P']\n",
    "t = data['train']['P']\n",
    "# p = [[11,0],[0,2],[11,0],[43,213]]\n",
    "tt= np.unique(t)\n",
    "ttr = np.unique(tr)\n",
    "\n",
    "count=0\n",
    "for key in ttr:\n",
    "    if key not in tt:\n",
    "        count += 1\n",
    "\n",
    "\n",
    "# u=np.unique(np.sum(p, axis=1))\n",
    "# u.shape\n",
    "ttr.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i_batch = 8\n",
    "macros['batch_size'] = 6\n",
    "\n",
    "test_p = test_P[i_batch: i_batch+macros['batch_size']]\n",
    "test_q = test_Q[i_batch: i_batch+macros['batch_size']]\n",
    "test_y_start = test_Y_start[i_batch: i_batch+macros['batch_size']]\n",
    "test_y_end =  test_Y_end[i_batch: i_batch+macros['batch_size']]\n",
    "\n",
    "\n",
    "#     test_p = train_P[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_q = train_Q[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_y_start = train_Y_start[i_batch: i_batch+macros['batch_size']]\n",
    "#     test_y_end =  train_Y_end[i_batch: i_batch+macros['batch_size']]\n",
    "\n",
    "\n",
    "s12, e12, test_loss = predict(\n",
    "    para_batch = torch.tensor(test_p, dtype=torch.long, device=device),\n",
    "    ques_batch = torch.tensor(test_q, dtype=torch.long, device=device),\n",
    "    ques_model = ques_model,\n",
    "    para_model = para_model,\n",
    "    mlstm_model = mlstm_model,\n",
    "    pointer_decoder_model = pointer_decoder_model,\n",
    "    macros = _macros,\n",
    "    loss_fn= loss_fn,\n",
    "    debug = _macros['debug']\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_cap_start.squeeze().shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "s1[:3] = torch.tensor(x1[:3]).unsqueeze(1)\n",
    "e1[:3] = torch.tensor(y1[:3]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(torch.argmax(e1.squeeze(), dim=1))\n",
    "# print(torch.argmax(e11.squeeze(), dim=1), torch.argmax(e12.squeeze(), dim=1))\n",
    "\n",
    "x1 = test_Y_start[0: 14]\n",
    "y1 =  test_Y_end[0: 14]\n",
    "m1 = eval(y=(torch.tensor(x1, device=device),\n",
    "            torch.tensor(y1, device=device)),\n",
    "        y_cap=[s1.squeeze(), e1.squeeze()])\n",
    "\n",
    "x11 = test_Y_start[0: 8]\n",
    "y11 =  test_Y_end[0: 8]\n",
    "m11 = eval(y=(torch.tensor(x11, device=device).view( -1, _macros['para_len']),\n",
    "            torch.tensor(y11, device=device).view(-1, _macros['para_len'])),\n",
    "        y_cap=[s11.squeeze(), e11.squeeze()])\n",
    "\n",
    "x12 = test_Y_start[8: 14]\n",
    "y12 =  test_Y_end[8: 14]\n",
    "m12 = eval(y=(torch.tensor(x12, device=device).view( -1, _macros['para_len']),\n",
    "            torch.tensor(y12, device=device).view(-1, _macros['para_len'])),\n",
    "        y_cap=[s12.squeeze(), e12.squeeze()])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "m12"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.argmax(x1, axis=1))\n",
    "print(np.argmax(s1.squeeze(), axis=1))\n",
    "# print(y1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a,b,c,d = torch.randint(0,10,(10,14)),torch.randint(0,10,(10,14)),torch.randint(0,10,(10,14)),torch.randint(0,10,(10,14))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a,b,c,d = torch.argmax(a,dim=1), torch.argmax(b,dim=1), torch.argmax(c,dim=1), torch.argmax(d,dim=1)\n",
    "a[:3] = c[:3]\n",
    "b[:3] = d[:3]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s_ab = np.vstack((a,b))\n",
    "s_cd = np.vstack((c,d))\n",
    "print(s_ab,s_cd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s_ab = s_ab.transpose()\n",
    "s_cd = s_cd.transpose()\n",
    "print(s_ab,s_cd)\n",
    "print(s_ab[:,0])\n",
    "print(np.equal(s_ab[:,0],s_cd[:,0]))\n",
    "\n",
    "print(\"col 0 i \", s_ab[:,0])\n",
    "print(\"col 1 i \", s_ab[:,1])\n",
    "# print(s_cd[:,1])\n",
    "# print(np.equal(s_ab[:,1],s_cd[:,1]))\n",
    "\n",
    "\n",
    "\n",
    "em = np.logical_and(np.equal(s_ab[:,0], s_cd[:,0]),np.equal(s_ab[:,1], s_cd[:,1]))\n",
    "print(em)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
