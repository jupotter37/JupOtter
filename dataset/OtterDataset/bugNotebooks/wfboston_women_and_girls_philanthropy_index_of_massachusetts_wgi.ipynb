{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c05e2f-8a36-4257-bb98-dec7ca0b628a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.31.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting xlsx2csv\n",
      "  Downloading xlsx2csv-0.8.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xlsx2csv, tqdm\n",
      "Successfully installed tqdm-4.66.1 xlsx2csv-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests tqdm\n",
    "xlsx2csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da6957f-4031-4bf3-9b05-0f0dfa3836a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from zipcodes import greater_boston_zipcodes\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from xlsx2csv import Xlsx2csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "#import logging\n",
    "\n",
    "\n",
    "WGI_URL = 'https://wgi.communityplatform.us/'\n",
    "WGI_API_BASE = 'https://wgi.communityplatform.us/platform-api/'\n",
    "STATE = 'MA'\n",
    "ORG_LIST_URL = f'{WGI_API_BASE}search/base-search?page=1&perPage=10000&orderBy=revenue&keywordType=all&resultType=all&states[]={STATE}'\n",
    "# https://wgi.communityplatform.us/platform-api/search/base-search?page=1&perPage=10000&orderBy=revenue&keywordType=all&resultType=all&states[]=MA\n",
    "IRS_SRC_URL = 'https://www.irs.gov/statistics/soi-tax-stats-annual-extract-of-tax-exempt-organization-financial-data'\n",
    "DEV_EMAIL = 'dhee.panwar@dell.com'\n",
    "#script_dir = Path(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "API = 'https://wgi.communityplatform.us/platform-api/search/base-search?page=1&perPage=40&orderBy=revenue&keywordType=all&resultType=all&states%5B%5D=MA&onlyFilers=true&searchView=map'\n",
    "\n",
    "IRS_ORG_LIST = 'https://www.irs.gov/charities-non-profits/exempt-organizations-business-master-file-extract-eo-bmf'\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "output_folder_path = Path(os.path.join(current_directory, 'output_files'))\n",
    "input_folder_path = Path(os.path.join(current_directory, 'input_files'))\n",
    "logs = Path(os.path.join(current_directory, 'logs'))\n",
    "WGI_file = input_folder_path/'WGI'/'WGI_MA_Only_11_6_23.csv'\n",
    "\n",
    "\n",
    "def download_file(link, filepath, force=False):\n",
    "    if not os.path.exists(filepath) or force:\n",
    "        response = requests.get(link, stream=True)\n",
    "        print(response)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            # 10MB chunk size\n",
    "            for chunk in tqdm(response.iter_content(chunk_size=10_000_000),\n",
    "                              desc=f'Downloading {filepath.name}'):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "\n",
    "def xlsx_to_csv(src, dest=None, force=False):\n",
    "    stem = src.stem\n",
    "    if dest is None:\n",
    "        dest = src.with_suffix('.csv')\n",
    "    if not os.path.exists(dest) or force:\n",
    "        print(f'Converting {stem}.xslx to {stem}.csv...')\n",
    "        print(f'This process can take upto 5-10 minutes. Please Wait!')\n",
    "        Xlsx2csv(str(src), outputencoding='utf-8').convert(str(dest))\n",
    "    #print(\"Dest\\n\")\n",
    "    #print(dest)\n",
    "    return dest\n",
    "\n",
    "\n",
    "def get_download_links():\n",
    "    \"\"\"Parse download links for excel forms from IRS website\n",
    "\n",
    "    Returns:\n",
    "        dict(int:list(dict{filename, link})): list of excel download links for each year\n",
    "    \"\"\"\n",
    "    r = requests.get(IRS_SRC_URL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    # get all <h2> that appear before tables\n",
    "    #pint(soup)\n",
    "    h2s = filter(\n",
    "        lambda tag: tag.text.startswith(\n",
    "            'Exempt Organization Returns Filed in Calendar Year'),\n",
    "        soup.find_all('h2'))\n",
    "    download_links = {}\n",
    "    for h2 in h2s:\n",
    "        year = int(re.search(r'\\d+', h2.text).group(0))\n",
    "        table = h2.find_next()\n",
    "        ##int(table)\n",
    "        links = [{\n",
    "            'name': f'{a_tag.text} ({year})',\n",
    "            'link': a_tag['href'],\n",
    "        } for a_tag in table.find_all('a')]\n",
    "        download_links[year] = links\n",
    "    #print(download_links)\n",
    "    return download_links\n",
    "\n",
    "\n",
    "def download_raw_data(year: int, force=False):\n",
    "    \"\"\"Download excel files from the IRS website corresponding to the year and convert to CSV\n",
    "    Example of exported file: `script_dir/2021/Form 990 Extract (2021).csv`\n",
    "\n",
    "    Raises RuntimeError if something went wrong\n",
    "    Raises ValueError if year is not available to download\n",
    "\n",
    "    Args:\n",
    "        year (int): year for which files to download\n",
    "    \"\"\"\n",
    "    # https://pythonprogramming.net/introduction-scraping-parsing-beautiful-soup-tutorial/\n",
    "    # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "    try:\n",
    "        download_links = get_download_links()\n",
    "    except requests.RequestException:\n",
    "        raise RuntimeError(f'Could not access IRS website ({IRS_SRC_URL})')\n",
    "    if not download_links:\n",
    "        raise RuntimeError('Could not parse IRS website to fetch links')\n",
    "    if year not in download_links:\n",
    "        raise ValueError(f'Year {year} is unavailable')\n",
    "\n",
    "    # download files into directory\n",
    "\n",
    "    download_folder = input_folder_path / f'{year}'\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    total_contrib = 0\n",
    "    for file in download_links[year]:\n",
    "    \n",
    "        name = download_folder / file['name']\n",
    "        xlsx_file = name.with_suffix('.xlsx')\n",
    "        download_file(file['link'], xlsx_file, force=force)\n",
    "        csv_file = str(xlsx_to_csv(xlsx_file, force=force))\n",
    "        if '990-EZ' in csv_file:\n",
    "            with open(csv_file) as f:\n",
    "                total_contrib += sum(int(r['totcntrbs']) for r in csv.DictReader(f))\n",
    "        elif 'Form 990 Extract' in csv_file:\n",
    "            with open(csv_file) as f:\n",
    "                total_contrib += sum(int(r['totcntrbgfts']) for r in csv.DictReader(f))\n",
    "        #comment below\n",
    "       ## print(f'{name.stem} headers: ', end='')\n",
    "       # with open(csv_file) as f:\n",
    "       #      csv_reader = csv.reader(f, delimiter = ',')\n",
    "       #      for row in csv_reader:\n",
    "       #          print(', '.join(row))\n",
    "       #          break\n",
    "    #comment above \n",
    "    print('Completed required download and CSV conversions')\n",
    "    print('Total Contribution:', total_contrib)\n",
    "    return total_contrib\n",
    "\n",
    "\n",
    "def get_latest_wgi(force=False):\n",
    "    r = requests.get(WGI_URL)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    a_tag = soup.find('a', string='Download The List')\n",
    "    if not a_tag:\n",
    "        raise RuntimeError('Could not download WGI list from ')\n",
    "    dl_link = a_tag['href']\n",
    "    wgi_dir = input_folder_path / 'WGI'\n",
    "    os.makedirs(wgi_dir, exist_ok=True)\n",
    "    xlsx_file = wgi_dir / Path(dl_link).name\n",
    "    download_file(dl_link, xlsx_file, force=force)\n",
    "    #print(\"Get latest Wgi done!\")\n",
    "    return xlsx_to_csv(xlsx_file)\n",
    "\n",
    "\n",
    "def get_org(org_id):\n",
    "\n",
    "    # https://wgi.communityplatform.us/platform-api/organization/1776515\n",
    "    url = f'{WGI_API_BASE}organization/{org_id}'\n",
    "    r = requests.get(url)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def get_gba_orgs():\n",
    "    \"\"\"\n",
    "    Get orgs from the Greater Boston Area\n",
    "    \"\"\"\n",
    "    # greater_boston_zipcodes\n",
    "    orgs = requests.get(ORG_LIST_URL).json()['data']\n",
    "    org_in_state = {}\n",
    "    wg_revenue = 0\n",
    "    \n",
    "    output_file = output_folder_path/'greater_boston_orgs.csv'\n",
    "    \n",
    "    try:\n",
    "        with open(output_file) as f:\n",
    "            wg_revenue = sum(int(r['revenue']) for r in csv.DictReader(f))\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        # https://wgi.communityplatform.us/platform-api/organization/1776515\n",
    "        for org in orgs:\n",
    "            org_zip = int(org['zip'])\n",
    "            if org_zip in greater_boston_zipcodes:\n",
    "                # clean data\n",
    "                org.pop('distance')\n",
    "                org.pop('icon')\n",
    "                org.pop('programId')\n",
    "                org.pop('programName')\n",
    "                org.pop('redirectUrl')\n",
    "                org.pop('relevance')\n",
    "                org['ein'] = ''\n",
    "                org_in_state[org['organizationId']] = org\n",
    "                revenue = org['revenue']\n",
    "                #print(\"Printing Revenue \\n\")\n",
    "                if isinstance(revenue, str):\n",
    "                    revenue = revenue.strip()\n",
    "                    if revenue[0] == '(' and revenue[-1] == ')':\n",
    "                        revenue = f'-{revenue[1:-1]}'\n",
    "                    if revenue == '-':\n",
    "                        revenue = 0\n",
    "                    revenue = int(revenue)\n",
    "                org['revenue'] = revenue\n",
    "                wg_revenue += revenue\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            future_to_org = {executor.submit(get_org, org_id): org_id for org_id in org_in_state}\n",
    "            # Order is not guaranteed even if you use a list. Use the value part above as an index\n",
    "            for future in tqdm(concurrent.futures.as_completed(future_to_org), total=len(future_to_org), desc='Downloading organization data'):\n",
    "                try:\n",
    "                    org_id = future_to_org[future]\n",
    "                    res = future.result()\n",
    "                    org_in_state[org_id]['ein'] = res['ein']\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "\n",
    "        with open(output_file, 'w') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=list(next(iter(org_in_state.values())).keys()))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(org_in_state.values())\n",
    "    print('Organization data for Greater Boston Area found in:', output_file)\n",
    "    print('Revenue W&G organizations:', wg_revenue)\n",
    "    return wg_revenue\n",
    "\n",
    "\n",
    "def get_ma_orgs_list(force=False):\n",
    "    #Download Massachusetts Orgnaization Data from IRS\n",
    "    try:\n",
    "        r = requests.get(IRS_ORG_LIST)\n",
    "    except: \n",
    "        print(\"Unable to access IRS website check this URL {IRS_ORG_LIST}\")\n",
    "        print(r)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    a_tag = soup.find('a', string='Massachusetts')      \n",
    "    if not a_tag:\n",
    "        raise RuntimeError('Could not download Massachusetts Org list from ')       \n",
    "    dl_link = a_tag['href']\n",
    "    wgi_dir = input_folder_path \n",
    "    os.makedirs(wgi_dir, exist_ok=True)\n",
    "    csv_file = wgi_dir / Path(dl_link).name\n",
    "    download_file(dl_link, csv_file, force=force)\n",
    "    return csv_file\n",
    "\n",
    "\n",
    "def update_ein_header(data_frame):\n",
    "    updated_df = data_frame.rename(columns={'ein':'EIN'},inplace=True)\n",
    "    return updated_df\n",
    "\n",
    "###########This is for MA ####################\n",
    "#Read eo_ma file, extract EIN, Name, City Zip (Zip needs to be seprated to) \n",
    "#Create a CSV file with this file \n",
    "#Create a list data structure with EIN ( Name, City, Zip)\n",
    "#Compare the EIN and with IRS data base both 900 and 900EZ file and update the above list with, \"totcntrbs\" in 900 and ezfile \"totcntrbgfts\"\n",
    "#Generate a CSV file with this info\n",
    "\n",
    "\n",
    "def update_ma_orgs_file(file_name=None):\n",
    "    print(file_name)\n",
    "    columns_to_capture = ['EIN','NAME','STREET', 'CITY', 'STATE', 'ZIP']\n",
    "    df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "    df[['ZIP_PART_1', 'ZIP_PART_2']] = df['ZIP'].str.split('-', expand=True)\n",
    "    df = df.drop('ZIP', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_irs_990_extract_file(file_name):\n",
    "    #print(\"Processing IRS 990 Extract File\")\n",
    "    try:\n",
    "        #print(\"trying one method!\")\n",
    "        columns_to_capture = ['ein','totcntrbgfts'] # add function to update this file\n",
    "        df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "        df.rename(columns={'ein':'EIN'},inplace=True)\n",
    "    except: \n",
    "        #print(\"trying 2nd method!\")\n",
    "        columns_to_capture = ['EIN','totcntrbgfts'] # add function to update this file\n",
    "        df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "    #print(\"Processing IRS 990 Extract file done!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_irs_990_ez_file(file_name):\n",
    "    #print(\"Processing IRS 990 EX file\")\n",
    "    try:\n",
    "        #print(\"trying one method!\")\n",
    "        columns_to_capture = ['ein','totcntrbs'] # add function to update this file\n",
    "        df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "        df.rename(columns={'ein':'EIN'},inplace=True)\n",
    "    except: \n",
    "        #print(\"trying 2nd method!\")\n",
    "        columns_to_capture = ['EIN','totcntrbs'] # add function to update this file\n",
    "        df = pd.read_csv(file_name, usecols=columns_to_capture)\n",
    "        #df.head()\n",
    "        \n",
    "    #print(\"Processing IRS 990 EX file done!\")\n",
    "    return df\n",
    "\n",
    "    \n",
    "def merge_df(first_df, second_df):\n",
    "    merged_df = pd.merge(first_df, second_df, on='EIN', how='left')\n",
    "    return merged_df    \n",
    "\n",
    "\n",
    "###########This is for Greater Boston Area####################\n",
    "# Create a list with EIN greater boston  \n",
    "# Convert index file in csv if needed\n",
    "# Read the above MA file compare it with index zip file if the zip is there in MA fill \n",
    "# Update the List with filtered info\n",
    "# Create a CSV file  with the filtered info\n",
    "\n",
    "\n",
    "def generate_gb_report(year, gb_dataframe, irs_990_extract_dataframe, irs_990_ez_dataframe):\n",
    "    try:\n",
    "        gb_dataframe.rename(columns={'ein':'EIN'},inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    gb_dataframe = merge_df(gb_dataframe, irs_990_extract_dataframe)\n",
    "    gb_dataframe = merge_df(gb_dataframe, irs_990_ez_dataframe)\n",
    "    print(\"Genrating list of organizations in Greater Boston Area...\\n\")\n",
    "    output_folder_path = Path(os.path.join(current_directory, f'output_files/{year}/'))\n",
    "    if not output_folder_path.exists():\n",
    "        try: \n",
    "            os.makedirs(output_folder_path, exist_ok=True)\n",
    "        except:\n",
    "            print(\"File name already exists!\")\n",
    "    output_file = os.path.join(output_folder_path, f'greater_boston_report{year}.csv')\n",
    "    gb_dataframe.to_csv(output_file)\n",
    "    print(f\"Greater Boston file generated!! File location {output_file} \")\n",
    "    return gb_dataframe\n",
    "\n",
    "###########This is for Womens only in GB  ####################\n",
    "# Create a list with EIN Womens only  GB\n",
    "# Read the above MA file compare it with Given V2 April_WSO_GSO_MA.xlsx if the zip is there in MA fill \n",
    "# Update the List with filtered info\n",
    "# Create a CSV file  with the filtered info\n",
    "\n",
    "def generate_wgi_in_gb_report(year, gb_dataframe, wgi_file):\n",
    "    columns_to_capture_in_gb=['organizationName','id', 'name', 'description', 'address', 'categories','revenue','EIN', 'totcntrbgfts', 'totcntrbs']\n",
    "    columns_to_capture_in_wgi=['EIN','Name']\n",
    "    wgi_df= pd.read_csv(wgi_file, usecols = columns_to_capture_in_wgi)\n",
    "    output_folder_path = Path(os.path.join(current_directory, f'output_files/{year}'))\n",
    "    wgi_in_gb_df = merge_df(gb_dataframe, wgi_df)\n",
    "    wgi_in_gb_df['w&g_organization'] = 'No'\n",
    "    wgi_in_gb_df.loc[wgi_in_gb_df['Name'].notnull(), 'w&g_organization'] = 'Yes'\n",
    "    wgi_in_gb_df = wgi_in_gb_df.drop(['Name'], axis=1)\n",
    "    output_folder_path = Path(os.path.join(current_directory, f'output_files/{year}/'))\n",
    "    output_file = os.path.join(output_folder_path, f'wgi_greater_boston_report{year}.csv')\n",
    "    wgi_in_gb_df.to_csv(output_file)\n",
    "    print(f\"W&G file generated!! File location {output_file} \")\n",
    "\n",
    "def is_valid_year(year_str):\n",
    "    current_year = datetime.now().year\n",
    "    try:\n",
    "        year = int(year_str)\n",
    "        return 2018 <= year < current_year\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def get_valid_year():\n",
    "    while True:\n",
    "        year_str = input(\"Enter the year you would like to download the file : \")\n",
    "        if is_valid_year(year_str):\n",
    "            return int(year_str)\n",
    "        else:\n",
    "            print(\"Invalid year. Please enter a valid year.\")\n",
    "            \n",
    "def generate_report(year, irs_990_extract_file, irs_990_ez_file, ma_orgs_file, greater_boston_orgs_file):\n",
    "    try: \n",
    "        ma_orgs_data = update_ma_orgs_file(ma_orgs_file)\n",
    "        irs_990_extract_dataframe = process_irs_990_extract_file(irs_990_extract_file)\n",
    "        irs_990_ez_dataframe = process_irs_990_ez_file(irs_990_ez_file)\n",
    "        ma_orgs_dataframe = merge_df(ma_orgs_data, irs_990_extract_dataframe)\n",
    "        ma_orgs_dataframe = merge_df(ma_orgs_dataframe, irs_990_ez_dataframe)\n",
    "        output_folder_path = Path(os.path.join(current_directory, f'output_files/{year}/'))\n",
    "        if not output_folder_path.exists():\n",
    "            try: \n",
    "                os.makedirs(output_folder_path, exist_ok=True)\n",
    "            except:\n",
    "                print(f\"File name already exists!\")\n",
    "        output_file = os.path.join(output_folder_path, f'MA_orgs_report{year}.csv')\n",
    "        ma_orgs_dataframe.to_csv(output_file)\n",
    "        print(f\"MA organizations report generated Location {output_file}\")\n",
    "        #ma_orgs_dataframe.head()\n",
    "        gb_dataframe = pd.read_csv(greater_boston_orgs_file)\n",
    "        #print(gb_dataframe)\n",
    "        gb_report_dataframe = generate_gb_report(year, gb_dataframe, irs_990_extract_dataframe, irs_990_ez_dataframe) \n",
    "        generate_wgi_in_gb_report(year, gb_report_dataframe, WGI_file)\n",
    "    except: \n",
    "        print(f\"Unable to generate report contact {DEV_EMAIL}\")\n",
    "        \n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758b1543-8e4c-4e39-ba2f-78d12716f77a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the year you would like to download the file :  2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organization data for Greater Boston Area found in: C:\\Users\\Dhee\\Desktop\\dell_Lalit\\dell_Lalit\\bwg\\output_files\\greater_boston_orgs.csv\n",
      "Revenue W&G organizations: 1007594668\n",
      "Downloading latest revenue data\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Extract Documentation (2018).xlsx: 1it [00:00, 73.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Extract Documentation (2018).xslx to Extract Documentation (2018).csv...\n",
      "This process can take upto 5-10 minutes. Please Wait!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Form 990 Extract (2018).xlsx: 34it [00:17,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Form 990 Extract (2018).xslx to Form 990 Extract (2018).csv...\n",
      "This process can take upto 5-10 minutes. Please Wait!\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Form 990-EZ Extract (2018).xlsx: 8it [00:03,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Form 990-EZ Extract (2018).xslx to Form 990-EZ Extract (2018).csv...\n",
      "This process can take upto 5-10 minutes. Please Wait!\n",
      "Completed required download and CSV conversions\n",
      "Total Contribution: 509767610899\n",
      "Percent contribution: 0.2 %\n",
      "Processing data to generate MA Orgs, Great Boston Orgs and W&G Orgs in Great Boston for year 2018\n",
      "C:\\Users\\Dhee\\Desktop\\dell_Lalit\\dell_Lalit\\bwg\\input_files\\eo_ma.csv\n",
      "MA organizations report generated Location C:\\Users\\Dhee\\Desktop\\dell_Lalit\\dell_Lalit\\bwg\\output_files\\2018\\MA_orgs_report2018.csv\n",
      "Genrating list of organizations in Greater Boston Area...\n",
      "\n",
      "Greater Boston file generated!! File location C:\\Users\\Dhee\\Desktop\\dell_Lalit\\dell_Lalit\\bwg\\output_files\\2018\\greater_boston_report2018.csv \n",
      "W&G file generated!! File location C:\\Users\\Dhee\\Desktop\\dell_Lalit\\dell_Lalit\\bwg\\output_files\\2018\\wgi_greater_boston_report2018.csv \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if not os.path.exists(output_folder_path):    \n",
    "            os.makedirs('output_files', exist_ok=True)\n",
    "        if not os.path.exists(input_folder_path):  \n",
    "            os.makedirs('input_files', exist_ok=True)\n",
    "        if not os.path.exists('logs'):  \n",
    "            os.makedirs('logs', exist_ok=True)   \n",
    "       # logging.basicConfig(filename='logs/script.log', encoding='utf-8', level=logging.DEBUG)\n",
    "        xlsx_key_list = input_folder_path/'V2_April_22_WSO_GSO_MA.xlsx'\n",
    "        #xlsx_key_list = script_dir / 'keys' / 'V2 April 22_WSO_GSO_MA.xlsx'\n",
    "        while not os.path.exists(xlsx_key_list):\n",
    "            print(f'Warning file not found: {xlsx_key_list}')\n",
    "            xlsx_key_list = Path(input('Enter keys source file: '))\n",
    "        csv_key_list = xlsx_key_list.with_suffix('.csv')\n",
    "        if not os.path.exists(csv_key_list):\n",
    "            print('Converting', xlsx_key_list.name, 'to csv')\n",
    "            Xlsx2csv(str(xlsx_key_list),\n",
    "                    outputencoding='utf-8').convert(str(csv_key_list))\n",
    "        year=get_valid_year()\n",
    "        wg_revenue = get_gba_orgs()\n",
    "        print(\"Downloading latest revenue data\")\n",
    "        wgi_latest = get_latest_wgi()\n",
    "        total_revenue = download_raw_data(year)\n",
    "        print('Percent contribution:', round(wg_revenue / total_revenue * 100, 2), '%')\n",
    "        print(f'Processing data to generate MA Orgs, Great Boston Orgs and W&G Orgs in Great Boston for year {year}')\n",
    "       \n",
    "        ma_orgs_file=get_ma_orgs_list()\n",
    "        file_990_extract_name = f'Form 990 Extract ({year}).csv'\n",
    "        file_990_ez_name = f'Form 990-EZ Extract ({year}).csv'\n",
    "        irs_990_extract_file = input_folder_path / str(year) /file_990_extract_name\n",
    "        irs_990_ez_file = input_folder_path / str(year) /file_990_ez_name\n",
    "        greater_boston_orgs_file = Path(os.path.join(current_directory, 'output_files/greater_boston_orgs.csv')) \n",
    "        try:\n",
    "            generate_report(year, irs_990_extract_file, irs_990_ez_file, ma_orgs_file, greater_boston_orgs_file)\n",
    "        except:\n",
    "            print(\"Script not working!! Please contact Dhee Panwar <{DEV_EMAIL}>\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        exc_type, exc_tb = sys.exc_info()[0], sys.exc_info()[2]\n",
    "        print(e.__repr__())\n",
    "        print(f'\\nThe error above was encountered on line {exc_tb.tb_lineno}. Please contact Dhee Panwar <{DEV_EMAIL}>')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
