{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c7b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f6fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e210003",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ARXIV_ENCODING_TENSOR_FILE = './train/train_encodings'\n",
    "TRAIN_ARXIV_LABEL_TENSOR_FILE = './train/train_labels'\n",
    "\n",
    "TEST_ARXIV_ENCODING_TENSOR_FILE = './test/test_encodings'\n",
    "TEST_ARXIV_LABEL_TENSOR_FILE = './test/test_labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eff1c",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "981e5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/merges.txt HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/tokenizer.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784af5d3",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac75a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_arxiv_tensor = True if len(os.listdir('./train')) != 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2403c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/scientific_papers/scientific_papers.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 \"HEAD /huggingface/datasets/1.6.2/datasets/scientific_papers/scientific_papers.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 \"HEAD /huggingface/datasets/1.6.2/datasets/scientific_papers/dataset_infos.json HTTP/1.1\" 200 0\n",
      "WARNING:datasets.builder:Reusing dataset scientific_papers (/home/changheng/.cache/huggingface/datasets/scientific_papers/arxiv/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"
     ]
    }
   ],
   "source": [
    "arxiv = None if has_arxiv_tensor else load_dataset('scientific_papers', 'arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc105a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(tensor_file, data, tokenizer):\n",
    "    if not Path(f'{tensor_file}.pt').is_file():\n",
    "        log.debug('-- No tensor file found, start tokenizing --')\n",
    "        tensor = tokenizer.batch_encode_plus(data, truncation=True, padding=True, max_length=1024, return_tensors='pt')\n",
    "        log.debug(f'-- Save tokenized tensor to {tensor_file} --')\n",
    "        torch.save(tensor, f'{tensor_file}.pt')\n",
    "    else:\n",
    "        log.debug(f'-- Load tokenized tensor from {tensor_file} --')\n",
    "        tensor = torch.load(f'{tensor_file}.pt')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1192701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary_dataset(Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings\n",
    "    self.labels = labels\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels['input_ids'][index])\n",
    "    return item\n",
    "  \n",
    "\n",
    "  def __len__(self):\n",
    "    return self.encodings['input_ids'].size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c47eb74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:-- Load tokenized tensor from ./test/test_encodings --\n",
      "DEBUG:__main__:-- Load tokenized tensor from ./test/test_labels --\n"
     ]
    }
   ],
   "source": [
    "test_encodings = load_tensor(\n",
    "    TEST_ARXIV_ENCODING_TENSOR_FILE,\n",
    "    None if has_arxiv_tensor else list(map(lambda x: x['article'], arxiv['test'])),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_labels = load_tensor(\n",
    "    TEST_ARXIV_LABEL_TENSOR_FILE,\n",
    "    None if has_arxiv_tensor else list(map(lambda x: x['abstract'], arxiv['test'])),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = Summary_dataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716e2f0",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = 20\n",
    "EPOCH = 1\n",
    "LR_RATE = 5e-5\n",
    "TRAIN_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LR_RATE)\n",
    "\n",
    "batch_size = 0 if has_arxiv_tensor else len(arxiv['train']) // SPLITS\n",
    "articles = None if has_arxiv_tensor else list(map(lambda x: x['article'], arxiv['train']))\n",
    "abstracts = None if has_arxiv_tensor else list(map(lambda x: x['abstract'], arxiv['train']))\n",
    "\n",
    "for i in range(SPLITS):\n",
    "    model.train()\n",
    "    \n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size if i != SPLITS - 1 else len(arxiv['train']) - 1\n",
    "    \n",
    "    train_encodings = load_tensor(\n",
    "        f'{TRAIN_ARXIV_ENCODING_TENSOR_FILE}_{i}',\n",
    "        None if has_arxiv_tensor else articles[i * batch_size : (i + 1) * batch_size],\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    train_labels = load_tensor(\n",
    "        f'{TRAIN_ARXIV_LABEL_TENSOR_FILE}_{i}',\n",
    "        None if has_arxiv_tensor else abstracts[i * batch_size : (i + 1) * batch_size],\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    train_dataset = Summary_dataset(train_encodings, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    for j in range(EPOCH):\n",
    "        with tqdm(train_loader, unit=\"batch\") as epoch:\n",
    "            for batch in epoch:\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                total_loss = torch.tensor(0, dtype=torch.float, device=device)\n",
    "                \n",
    "                # gradient accumulation\n",
    "                for idx in range(TRAIN_BATCH_SIZE):\n",
    "                    input_ids = batch['input_ids'][idx].view(1, -1).to(device)\n",
    "                    attention_mask = batch['attention_mask'][idx].view(1, -1).to(device)\n",
    "                    labels = batch['labels'][idx].view(1, -1).to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs[0] / TRAIN_BATCH_SIZE\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    total_loss += loss\n",
    "                epoch.set_postfix(loss=total_loss)\n",
    "                \n",
    "                optimizer.step()\n",
    "    del train_encodings\n",
    "    del train_labels\n",
    "    del train_dataset\n",
    "    del train_loader\n",
    "    gc.collect()\n",
    "    log.info(f'-- Saving #{i} model checkpoint --')\n",
    "    torch.save(model.state_dict(), f'bart_split_{i}.pt')\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0121142",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### BART Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb6c9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "CPU_COUNT = psutil.cpu_count(logical = True)\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5933e973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/bart-large-cnn/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02fde20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "predicts, labels = [], arxiv['test']['abstract'][:500]\n",
    "\n",
    "for batch in DataLoader(test_dataset, batch_size=1, drop_last=True, num_workers=CPU_COUNT):\n",
    "    predicts.append(\n",
    "        tokenizer.decode(\n",
    "            model.generate(batch['input_ids'].to(device).view(1, -1)).squeeze(0)\n",
    "        )\n",
    "    )\n",
    "    if len(predicts) == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "866ac6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.25636286683601944,\n",
       "  'p': 0.432389182280891,\n",
       "  'r': 0.19238319264202147},\n",
       " 'rouge-2': {'f': 0.0761504181802766,\n",
       "  'p': 0.1288743815556388,\n",
       "  'r': 0.056975345464353316},\n",
       " 'rouge-l': {'f': 0.23734391309236852,\n",
       "  'p': 0.33479190615244336,\n",
       "  'r': 0.19148226445524558}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(predicts, labels, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20515b8",
   "metadata": {},
   "source": [
    "#### Original articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba2bda15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \\n the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \\n a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \\n it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \\n however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7001b5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for about 20 years the problem of properties of short - term changes of solar activity has been considered extensively .\\nmany investigators studied the short - term periodicities of the various indices of solar activity .\\nseveral periodicities were detected , but the periodicities about 155 days and from the interval of @xmath3 $ ] days ( @xmath4 $ ] years ) are mentioned most often .\\nfirst of them was discovered by @xcite in the occurence rate of gamma - ray flares detected by the gamma - ray spectrometer aboard the _ solar maximum mission ( smm ) .\\nthis periodicity was confirmed for other solar flares data and for the same time period @xcite .\\nit was also found in proton flares during solar cycles 19 and 20 @xcite , but it was not found in the solar flares data during solar cycles 22 @xcite .\\n_    several autors confirmed above results for the daily sunspot area data . @xcite studied the sunspot data from 18741984 .\\nshe found the 155-day periodicity in data records from 31 years .\\nthis periodicity is always characteristic for one of the solar hemispheres ( the southern hemisphere for cycles 1215 and the northern hemisphere for cycles 1621 ) .\\nmoreover , it is only present during epochs of maximum activity ( in episodes of 13 years ) .\\nsimilarinvestigationswerecarriedoutby + @xcite .\\nthey applied the same power spectrum method as lean , but the daily sunspot area data ( cycles 1221 ) were divided into 10 shorter time series .\\nthe periodicities were searched for the frequency interval 57115 nhz ( 100200 days ) and for each of 10 time series .\\nthe authors showed that the periodicity between 150160 days is statistically significant during all cycles from 16 to 21 .\\nthe considered peaks were remained unaltered after removing the 11-year cycle and applying the power spectrum analysis .\\n@xcite used the wavelet technique for the daily sunspot areas between 1874 and 1993 .\\nthey determined the epochs of appearance of this periodicity and concluded that it presents around the maximum activity period in cycles 16 to 21 .\\nmoreover , the power of this periodicity started growing at cycle 19 , decreased in cycles 20 and 21 and disappered after cycle 21 .\\nsimilaranalyseswerepresentedby + @xcite , but for sunspot number , solar wind plasma , interplanetary magnetic field and geomagnetic activity index @xmath5 .\\nduring 1964 - 2000 the sunspot number wavelet power of periods less than one year shows a cyclic evolution with the phase of the solar cycle.the 154-day period is prominent and its strenth is stronger around the 1982 - 1984 interval in almost all solar wind parameters .\\nthe existence of the 156-day periodicity in sunspot data were confirmed by @xcite .\\nthey considered the possible relation between the 475-day ( 1.3-year ) and 156-day periodicities .\\nthe 475-day ( 1.3-year ) periodicity was also detected in variations of the interplanetary magnetic field , geomagnetic activity helioseismic data and in the solar wind speed @xcite .\\n@xcite concluded that the region of larger wavelet power shifts from 475-day ( 1.3-year ) period to 620-day ( 1.7-year ) period and then back to 475-day ( 1.3-year ) .\\nthe periodicities from the interval @xmath6 $ ] days ( @xmath4 $ ] years ) have been considered from 1968 .\\n@xcite mentioned a 16.3-month ( 490-day ) periodicity in the sunspot numbers and in the geomagnetic data .\\n@xcite analysed the occurrence rate of major flares during solar cycles 19 .\\nthey found a 18-month ( 540-day ) periodicity in flare rate of the norhern hemisphere .\\n@xcite confirmed this result for the @xmath7 flare data for solar cycles 20 and 21 and found a peak in the power spectra near 510540 days .\\n@xcite found a 17-month ( 510-day ) periodicity of sunspot groups and their areas from 1969 to 1986 .\\nthese authors concluded that the length of this period is variable and the reason of this periodicity is still not understood .\\n@xcite and + @xcite obtained statistically significant peaks of power at around 158 days for daily sunspot data from 1923 - 1933 ( cycle 16 ) . in this paper the problem of the existence of this periodicity for sunspot data from cycle 16 is considered .\\nthe daily sunspot areas , the mean sunspot areas per carrington rotation , the monthly sunspot numbers and their fluctuations , which are obtained after removing the 11-year cycle are analysed . in section 2 the properties of the power spectrum methods are described . in section 3 a new approach to the problem of aliases in the power spectrum analysis\\nis presented . in section 4 numerical results of the new method of the diagnosis of an echo - effect for sunspot area data are discussed . in section 5 the problem of the existence of the periodicity of about 155 days during the maximum activity period for sunspot data from the whole solar disk and from each solar hemisphere separately is considered .\\nto find periodicities in a given time series the power spectrum analysis is applied . in this paper\\ntwo methods are used : the fast fourier transformation algorithm with the hamming window function ( fft ) and the blackman - tukey ( bt ) power spectrum method @xcite .\\nthe bt method is used for the diagnosis of the reasons of the existence of peaks , which are obtained by the fft method .\\nthe bt method consists in the smoothing of a cosine transform of an autocorrelation function using a 3-point weighting average .\\nsuch an estimator is consistent and unbiased .\\nmoreover , the peaks are uncorrelated and their sum is a variance of a considered time series . the main disadvantage of this method is a weak resolution of the periodogram points , particularly for low frequences .\\nfor example , if the autocorrelation function is evaluated for @xmath8 , then the distribution points in the time domain are : @xmath9 thus , it is obvious that this method should not be used for detecting low frequency periodicities with a fairly good resolution .\\nhowever , because of an application of the autocorrelation function , the bt method can be used to verify a reality of peaks which are computed using a method giving the better resolution ( for example the fft method ) .\\nit is valuable to remember that the power spectrum methods should be applied very carefully .\\nthe difficulties in the interpretation of significant peaks could be caused by at least four effects : a sampling of a continuos function , an echo - effect , a contribution of long - term periodicities and a random noise .\\nfirst effect exists because periodicities , which are shorter than the sampling interval , may mix with longer periodicities . in result , this effect can be reduced by an decrease of the sampling interval between observations .\\nthe echo - effect occurs when there is a latent harmonic of frequency @xmath10 in the time series , giving a spectral peak at @xmath10 , and also periodic terms of frequency @xmath11 etc .\\nthis may be detected by the autocorrelation function for time series with a large variance .\\ntime series often contain long - term periodicities , that influence short - term peaks .\\nthey could rise periodogram s peaks at lower frequencies .\\nhowever , it is also easy to notice the influence of the long - term periodicities on short - term peaks in the graphs of the autocorrelation functions .\\nthis effect is observed for the time series of solar activity indexes which are limited by the 11-year cycle .    to find statistically significant periodicities\\nit is reasonable to use the autocorrelation function and the power spectrum method with a high resolution . in the case of a stationary time\\nseries they give similar results .\\nmoreover , for a stationary time series with the mean zero the fourier transform is equivalent to the cosine transform of an autocorrelation function @xcite .\\nthus , after a comparison of a periodogram with an appropriate autocorrelation function one can detect peaks which are in the graph of the first function and do not exist in the graph of the second function .\\nthe reasons of their existence could be explained by the long - term periodicities and the echo - effect .\\nbelow method enables one to detect these effects .\\n( solid line ) and the 95% confidence level basing on thered noise ( dotted line ) .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series ( solid line ) .\\nthe dotted lines represent two standard errors of the autocorrelation function .\\nthe dashed horizontal line shows the zero level .\\nthe autocorrelation values are shown in the right axis . ]     because the statistical tests indicate that the time series is a white noise the confidence level is not marked . ]    . ]\\nthe method of the diagnosis of an echo - effect in the power spectrum ( de ) consists in an analysis of a periodogram of a given time series computed using the bt method .\\nthe bt method bases on the cosine transform of the autocorrelation function which creates peaks which are in the periodogram , but not in the autocorrelation function .\\nthe de method is used for peaks which are computed by the fft method ( with high resolution ) and are statistically significant .\\nthe time series of sunspot activity indexes with the spacing interval one rotation or one month contain a markov - type persistence , which means a tendency for the successive values of the time series to remember their antecendent values .\\nthus , i use a confidence level basing on the red noise of markov @xcite for the choice of the significant peaks of the periodogram computed by the fft method .\\nwhen a time series does not contain the markov - type persistence i apply the fisher test and the kolmogorov - smirnov test at the significance level @xmath12 @xcite to verify a statistically significance of periodograms peaks . the fisher test checks the null hypothesis that the time series is white noise agains the alternative hypothesis that the time series contains an added deterministic periodic component of unspecified frequency . because the fisher test tends to be severe in rejecting peaks as insignificant the kolmogorov - smirnov test is also used .\\nthe de method analyses raw estimators of the power spectrum .\\nthey are given as follows    @xmath13    for @xmath14 + where @xmath15 for @xmath16 + @xmath17 is the length of the time series @xmath18 and @xmath19 is the mean value .\\nthe first term of the estimator @xmath20 is constant .\\nthe second term takes two values ( depending on odd or even @xmath21 ) which are not significant because @xmath22 for large m. thus , the third term of ( 1 ) should be analysed .\\nlooking for intervals of @xmath23 for which @xmath24 has the same sign and different signs one can find such parts of the function @xmath25 which create the value @xmath20 .\\nlet the set of values of the independent variable of the autocorrelation function be called @xmath26 and it can be divided into the sums of disjoint sets : @xmath27 where + @xmath28 + @xmath29 @xmath30 @xmath31 + @xmath32 + @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 @xmath38\\n@xmath39 @xmath40    well , the set @xmath41 contains all integer values of @xmath23 from the interval of @xmath42 for which the autocorrelation function and the cosinus function with the period @xmath43 $ ] are positive .\\nthe index @xmath44 indicates successive parts of the cosinus function for which the cosinuses of successive values of @xmath23 have the same sign .\\nhowever , sometimes the set @xmath41 can be empty .\\nfor example , for @xmath45 and @xmath46 the set @xmath47 should contain all @xmath48 $ ] for which @xmath49 and @xmath50 , but for such values of @xmath23 the values of @xmath51 are negative .\\nthus , the set @xmath47 is empty .    .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series .\\nthe autocorrelation values are shown in the right axis . ]\\nlet us take into consideration all sets \\\\{@xmath52 } , \\\\{@xmath53 } and \\\\{@xmath41 } which are not empty . because numberings and power of these sets depend on the form of the autocorrelation function of the given time series , it is impossible to establish them arbitrary .\\nthus , the sets of appropriate indexes of the sets \\\\{@xmath52 } , \\\\{@xmath53 } and \\\\{@xmath41 } are called @xmath54 , @xmath55 and @xmath56 respectively . for example\\nthe set @xmath56 contains all @xmath44 from the set @xmath57 for which the sets @xmath41 are not empty .\\nto separate quantitatively in the estimator @xmath20 the positive contributions which are originated by the cases described by the formula ( 5 ) from the cases which are described by the formula ( 3 ) the following indexes are introduced : @xmath58 @xmath59 @xmath60 @xmath61 where @xmath62 @xmath63 @xmath64 taking for the empty sets \\\\{@xmath53 } and \\\\{@xmath41 } the indices @xmath65 and @xmath66 equal zero .\\nthe index @xmath65 describes a percentage of the contribution of the case when @xmath25 and @xmath51 are positive to the positive part of the third term of the sum ( 1 ) .\\nthe index @xmath66 describes a similar contribution , but for the case when the both @xmath25 and @xmath51 are simultaneously negative .\\nthanks to these one can decide which the positive or the negative values of the autocorrelation function have a larger contribution to the positive values of the estimator @xmath20 .\\nwhen the difference @xmath67 is positive , the statement the @xmath21-th peak really exists can not be rejected .\\nthus , the following formula should be satisfied : @xmath68    because the @xmath21-th peak could exist as a result of the echo - effect , it is necessary to verify the second condition :    @xmath69\\\\in c_m.\\\\ ] ]    .\\nthe periodogram values are presented on the left axis .\\nthe lower curve illustrates the autocorrelation function of the same time series ( solid line ) .\\nthe dotted lines represent two standard errors of the autocorrelation function .\\nthe dashed horizontal line shows the zero level .\\nthe autocorrelation values are shown in the right axis . ]    to verify the implication ( 8) firstly it is necessary to evaluate the sets @xmath41 for @xmath70 of the values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath71 $ ] are positive and the sets @xmath72 of values of @xmath23 for which the autocorrelation function and the cosine function with the period @xmath43 $ ] are negative .\\nsecondly , a percentage of the contribution of the sum of products of positive values of @xmath25 and @xmath51 to the sum of positive products of the values of @xmath25 and @xmath51 should be evaluated . as a result the indexes @xmath65 for each set\\n@xmath41 where @xmath44 is the index from the set @xmath56 are obtained .\\nthirdly , from all sets @xmath41 such that @xmath70 the set @xmath73 for which the index @xmath65 is the greatest should be chosen .    the implication ( 8) is true when the set @xmath73 includes the considered period @xmath43 $ ] .\\nthis means that the greatest contribution of positive values of the autocorrelation function and positive cosines with the period @xmath43 $ ] to the periodogram value @xmath20 is caused by the sum of positive products of @xmath74 for each @xmath75-\\\\frac{m}{2k},[\\\\frac{2m}{k}]+\\\\frac{m}{2k})$ ] .    when the implication ( 8) is false , the peak @xmath20 is mainly created by the sum of positive products of @xmath74 for each @xmath76-\\\\frac{m}{2k},\\\\big [ \\\\frac{2m}{n}\\\\big ] + \\\\frac{m}{2k } \\\\big ) $ ] , where @xmath77 is a multiple or a divisor of @xmath21 .\\nit is necessary to add , that the de method should be applied to the periodograms peaks , which probably exist because of the echo - effect .\\nit enables one to find such parts of the autocorrelation function , which have the significant contribution to the considered peak .\\nthe fact , that the conditions ( 7 ) and ( 8) are satisfied , can unambiguously decide about the existence of the considered periodicity in the given time series , but if at least one of them is not satisfied , one can doubt about the existence of the considered periodicity .\\nthus , in such cases the sentence the peak can not be treated as true should be used .    using the de method\\nit is necessary to remember about the power of the set @xmath78 .\\nif @xmath79 is too large , errors of an autocorrelation function estimation appear .\\nthey are caused by the finite length of the given time series and as a result additional peaks of the periodogram occur . if @xmath79 is too small , there are less peaks because of a low resolution of the periodogram . in applications\\n@xmath80 is used . in order to evaluate the value\\n@xmath79 the fft method is used .\\nthe periodograms computed by the bt and the fft method are compared .\\nthe conformity of them enables one to obtain the value @xmath79 .    .\\nthe fft periodogram values are presented on the left axis .\\nthe lower curve illustrates the bt periodogram of the same time series ( solid line and large black circles ) .\\nthe bt periodogram values are shown in the right axis . ]\\nin this paper the sunspot activity data ( august 1923 - october 1933 ) provided by the greenwich photoheliographic results ( gpr ) are analysed .\\nfirstly , i consider the monthly sunspot number data . to eliminate the 11-year trend from these data ,\\nthe consecutively smoothed monthly sunspot number @xmath81 is subtracted from the monthly sunspot number @xmath82 where the consecutive mean @xmath83 is given by @xmath84 the values @xmath83 for @xmath85 and @xmath86 are calculated using additional data from last six months of cycle 15 and first six months of cycle 17 .    because of the north - south asymmetry of various solar indices @xcite , the sunspot activity is considered for each solar hemisphere separately .\\nanalogously to the monthly sunspot numbers , the time series of sunspot areas in the northern and southern hemispheres with the spacing interval @xmath87 rotation are denoted . in order to find periodicities ,\\nthe following time series are used : + @xmath88  \\n+ @xmath89\\n   + @xmath90     + in the lower part of figure [ f1 ] the autocorrelation function of the time series for the northern hemisphere @xmath88 is shown .\\nit is easy to notice that the prominent peak falls at 17 rotations interval ( 459 days ) and @xmath25 for @xmath91 $ ] rotations ( [ 81 , 162 ] days ) are significantly negative .\\nthe periodogram of the time series @xmath88 ( see the upper curve in figures [ f1 ] ) does not show the significant peaks at @xmath92 rotations ( 135 , 162 days ) , but there is the significant peak at @xmath93 ( 243 days ) .\\nthe peaks at @xmath94 are close to the peaks of the autocorrelation function .\\nthus , the result obtained for the periodicity at about @xmath0 days are contradict to the results obtained for the time series of daily sunspot areas @xcite .    for the southern hemisphere ( the lower curve in figure [ f2 ] ) @xmath25 for @xmath95 $ ] rotations ( [ 54 , 189 ] days ) is not positive except @xmath96 ( 135 days ) for which @xmath97 is not statistically significant .\\nthe upper curve in figures [ f2 ] presents the periodogram of the time series @xmath89 .\\nthis time series does not contain a markov - type persistence .\\nmoreover , the kolmogorov - smirnov test and the fisher test do not reject a null hypothesis that the time series is a white noise only .\\nthis means that the time series do not contain an added deterministic periodic component of unspecified frequency .\\nthe autocorrelation function of the time series @xmath90 ( the lower curve in figure [ f3 ] ) has only one statistically significant peak for @xmath98 months ( 480 days ) and negative values for @xmath99 $ ] months ( [ 90 , 390 ] days ) .\\nhowever , the periodogram of this time series ( the upper curve in figure [ f3 ] ) has two significant peaks the first at 15.2 and the second at 5.3 months ( 456 , 159 days ) .\\nthus , the periodogram contains the significant peak , although the autocorrelation function has the negative value at @xmath100 months .    to explain\\nthese problems two following time series of daily sunspot areas are considered : + @xmath101  \\n+ @xmath102     + where @xmath103    the values @xmath104 for @xmath105 and @xmath106 are calculated using additional daily data from the solar cycles 15 and 17 .     and the cosine function for @xmath45 ( the period at about 154 days ) .\\nthe horizontal line ( dotted line ) shows the zero level .\\nthe vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath108 ) are searched .\\nthe percentage values show the index @xmath65 for each @xmath41 for the time series @xmath102 ( in parentheses for the time series @xmath101 ) . in the right bottom corner\\nthe values of @xmath65 for the time series @xmath102 , for @xmath109 are written . ]\\n( the 500-day period ) ]    the comparison of the functions @xmath25 of the time series @xmath101 ( the lower curve in figure [ f4 ] ) and @xmath102 ( the lower curve in figure [ f5 ] ) suggests that the positive values of the function @xmath110 of the time series @xmath101 in the interval of @xmath111 $ ] days could be caused by the 11-year cycle .\\nthis effect is not visible in the case of periodograms of the both time series computed using the fft method ( see the upper curves in figures [ f4 ] and [ f5 ] ) or the bt method ( see the lower curve in figure [ f6 ] ) . moreover , the periodogram of the time series @xmath102 has the significant values at @xmath112 days , but the autocorrelation function is negative at these points .\\n@xcite showed that the lomb - scargle periodograms for the both time series ( see @xcite , figures 7 a - c ) have a peak at 158.8 days which stands over the fap level by a significant amount . using the de method the above discrepancies are obvious . to establish the @xmath79 value the periodograms computed by the fft and\\nthe bt methods are shown in figure [ f6 ] ( the upper and the lower curve respectively ) .\\nfor @xmath46 and for periods less than 166 days there is a good comformity of the both periodograms ( but for periods greater than 166 days the points of the bt periodogram are not linked because the bt periodogram has much worse resolution than the fft periodogram ( no one know how to do it ) ) . for @xmath46 and @xmath113\\nthe value of @xmath21 is 13 ( @xmath71=153 $ ] ) .\\nthe inequality ( 7 ) is satisfied because @xmath114 .\\nthis means that the value of @xmath115 is mainly created by positive values of the autocorrelation function .\\nthe implication ( 8) needs an evaluation of the greatest value of the index @xmath65 where @xmath70 , but the solar data contain the most prominent period for @xmath116 days because of the solar rotation .\\nthus , although @xmath117 for each @xmath118 , all sets @xmath41 ( see ( 5 ) and ( 6 ) ) without the set @xmath119 ( see ( 4 ) ) , which contains @xmath120 $ ] , are considered . this situation is presented in figure [ f7 ] . in this figure\\ntwo curves @xmath121 and @xmath122 are plotted .\\nthe vertical dotted lines evaluate the intervals where the sets @xmath107 ( for @xmath123 ) are searched . for such @xmath41 two numbers\\nare written : in parentheses the value of @xmath65 for the time series @xmath101 and above it the value of @xmath65 for the time series @xmath102 . to make this figure clear the curves are plotted for the set @xmath124 only .\\n( in the right bottom corner information about the values of @xmath65 for the time series @xmath102 , for @xmath109 are written . )\\nthe implication ( 8) is not true , because @xmath125 for @xmath126 .\\ntherefore , @xmath43=153\\\\notin c_6=[423,500]$ ] .\\nmoreover , the autocorrelation function for @xmath127 $ ] is negative and the set @xmath128 is empty .\\nthus , @xmath129 . on the basis of these information one can state , that the periodogram peak at @xmath130 days of the time series @xmath102 exists because of positive @xmath25 , but for @xmath23 from the intervals which do not contain this period .\\nlooking at the values of @xmath65 of the time series @xmath101 , one can notice that they decrease when @xmath23 increases until @xmath131 .\\nthis indicates , that when @xmath23 increases , the contribution of the 11-year cycle to the peaks of the periodogram decreases .\\nan increase of the value of @xmath65 is for @xmath132 for the both time series , although the contribution of the 11-year cycle for the time series @xmath101 is insignificant .\\nthus , this part of the autocorrelation function ( @xmath133 for the time series @xmath102 ) influences the @xmath21-th peak of the periodogram .\\nthis suggests that the periodicity at about 155 days is a harmonic of the periodicity from the interval of @xmath1 $ ] days .\\n( solid line ) and consecutively smoothed sunspot areas of the one rotation time interval @xmath134 ( dotted line ) .\\nboth indexes are presented on the left axis .\\nthe lower curve illustrates fluctuations of the sunspot areas @xmath135 .\\nthe dotted and dashed horizontal lines represent levels zero and @xmath136 respectively .\\nthe fluctuations are shown on the right axis . ]\\nthe described reasoning can be carried out for other values of the periodogram .\\nfor example , the condition ( 8) is not satisfied for @xmath137 ( 250 , 222 , 200 days ) .\\nmoreover , the autocorrelation function at these points is negative .\\nthese suggest that there are not a true periodicity in the interval of [ 200 , 250 ] days .\\nit is difficult to decide about the existence of the periodicities for @xmath138 ( 333 days ) and @xmath139 ( 286 days ) on the basis of above analysis . the implication ( 8) is not satisfied for @xmath139 and the condition ( 7 ) is not satisfied for @xmath138 , although the function @xmath25 of the time series @xmath102 is significantly positive for @xmath140 .\\nthe conditions ( 7 ) and ( 8) are satisfied for @xmath141 ( figure [ f8 ] ) and @xmath142 . therefore , it is possible to exist the periodicity from the interval of @xmath1 $ ] days .\\nsimilar results were also obtained by @xcite for daily sunspot numbers and daily sunspot areas .\\nshe considered the means of three periodograms of these indexes for data from @xmath143 years and found statistically significant peaks from the interval of @xmath1 $ ] ( see @xcite , figure 2 ) .\\n@xcite studied sunspot areas from 1876 - 1999 and sunspot numbers from 1749 - 2001 with the help of the wavelet transform .\\nthey pointed out that the 154 - 158-day period could be the third harmonic of the 1.3-year ( 475-day ) period .\\nmoreover , the both periods fluctuate considerably with time , being stronger during stronger sunspot cycles .\\ntherefore , the wavelet analysis suggests a common origin of the both periodicities . this conclusion confirms the de method result which indicates that the periodogram peak at @xmath144 days is an alias of the periodicity from the interval of @xmath1 $ ]\\nin order to verify the existence of the periodicity at about 155 days i consider the following time series : + @xmath145     + @xmath146\\n   + @xmath147  \\n+ the value @xmath134 is calculated analogously to @xmath83 ( see sect .\\nthe values @xmath148 and @xmath149 are evaluated from the formula ( 9 ) . in the upper part of figure [ f9 ] the time series of sunspot areas @xmath150 of the one rotation time interval from the whole solar disk and the time series of consecutively smoothed sunspot areas @xmath151\\nare showed . in the lower part of figure [ f9 ]\\nthe time series of sunspot area fluctuations @xmath145 is presented .\\non the basis of these data the maximum activity period of cycle 16 is evaluated .\\nit is an interval between two strongest fluctuations e.a .\\n@xmath152 $ ] rotations .\\nthe length of the time interval @xmath153 is 54 rotations .\\nif the about @xmath0-day ( 6 solar rotations ) periodicity existed in this time interval and it was characteristic for strong fluctuations from this time interval , 10 local maxima in the set of @xmath154 would be seen .\\nthen it should be necessary to find such a value of p for which @xmath155 for @xmath156 and the number of the local maxima of these values is 10 .\\nas it can be seen in the lower part of figure [ f9 ] this is for the case of @xmath157 ( in this figure the dashed horizontal line is the level of @xmath158 ) .\\nfigure [ f10 ] presents nine time distances among the successive fluctuation local maxima and the horizontal line represents the 6-rotation periodicity .\\nit is immediately apparent that the dispersion of these points is 10 and it is difficult to find even few points which oscillate around the value of 6 .\\nsuch an analysis was carried out for smaller and larger @xmath136 and the results were similar .\\ntherefore , the fact , that the about @xmath0-day periodicity exists in the time series of sunspot area fluctuations during the maximum activity period is questionable .    .\\nthe horizontal line represents the 6-rotation ( 162-day ) period . ]    ]    ]    to verify again the existence of the about @xmath0-day periodicity during the maximum activity period in each solar hemisphere separately , the time series @xmath88 and @xmath89 were also cut down to the maximum activity period ( january 1925december 1930 ) .\\nthe comparison of the autocorrelation functions of these time series with the appriopriate autocorrelation functions of the time series @xmath88 and @xmath89 , which are computed for the whole 11-year cycle ( the lower curves of figures [ f1 ] and [ f2 ] ) , indicates that there are not significant differences between them especially for @xmath23=5 and 6 rotations ( 135 and 162 days ) ) .\\nthis conclusion is confirmed by the analysis of the time series @xmath146 for the maximum activity period .\\nthe autocorrelation function ( the lower curve of figure [ f11 ] ) is negative for the interval of [ 57 , 173 ] days , but the resolution of the periodogram is too low to find the significant peak at @xmath159 days .\\nthe autocorrelation function gives the same result as for daily sunspot area fluctuations from the whole solar disk ( @xmath160 ) ( see also the lower curve of figures [ f5 ] ) . in the case of\\nthe time series @xmath89 @xmath161 is zero for the fluctuations from the whole solar cycle and it is almost zero ( @xmath162 ) for the fluctuations from the maximum activity period .\\nthe value @xmath163 is negative .\\nsimilarly to the case of the northern hemisphere the autocorrelation function and the periodogram of southern hemisphere daily sunspot area fluctuations from the maximum activity period @xmath147 are computed ( see figure [ f12 ] ) .\\nthe autocorrelation function has the statistically significant positive peak in the interval of [ 155 , 165 ] days , but the periodogram has too low resolution to decide about the possible periodicities .\\nthe correlative analysis indicates that there are positive fluctuations with time distances about @xmath0 days in the maximum activity period .\\nthe results of the analyses of the time series of sunspot area fluctuations from the maximum activity period are contradict with the conclusions of @xcite .\\nshe uses the power spectrum analysis only .\\nthe periodogram of daily sunspot fluctuations contains peaks , which could be harmonics or subharmonics of the true periodicities .\\nthey could be treated as real periodicities .\\nthis effect is not visible for sunspot data of the one rotation time interval , but averaging could lose true periodicities .\\nthis is observed for data from the southern hemisphere .\\nthere is the about @xmath0-day peak in the autocorrelation function of daily fluctuations , but the correlation for data of the one rotation interval is almost zero or negative at the points @xmath164 and 6 rotations .\\nthus , it is reasonable to research both time series together using the correlative and the power spectrum analyses .\\nthe following results are obtained :    1 .\\na new method of the detection of statistically significant peaks of the periodograms enables one to identify aliases in the periodogram .\\n2 .   two effects cause the existence of the peak of the periodogram of the time series of sunspot area fluctuations at about @xmath0 days : the first is caused by the 27-day periodicity , which probably creates the 162-day periodicity ( it is a subharmonic frequency of the 27-day periodicity ) and the second is caused by statistically significant positive values of the autocorrelation function from the intervals of @xmath165 $ ] and @xmath166 $ ] days .\\nthe existence of the periodicity of about @xmath0 days of the time series of sunspot area fluctuations and sunspot area fluctuations from the northern hemisphere during the maximum activity period is questionable .\\nthe autocorrelation analysis of the time series of sunspot area fluctuations from the southern hemisphere indicates that the periodicity of about 155 days exists during the maximum activity period .\\ni appreciate valuable comments from professor j. jakimiec .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6123f769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we study the detectability of circular polarization in a stochastic gravitational wave background from various sources such as supermassive black hole binaries , cosmic strings , and inflation in the early universe with pulsar timing arrays . \\n we calculate generalized overlap reduction functions for the circularly polarized stochastic gravitational wave background . \\n we find that the circular polarization can not be detected for an isotropic background . however , there is a chance to observe the circular polarization for an anisotropic gravitational wave background . \\n we also show how to separate polarized gravitational waves from unpolarized gravitational waves . '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['abstract'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7cead36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is believed that the direct detection of gravitational waves ( gws ) will bring the era of gravitational wave astronomy .\\nthe interferometer detectors are now under operation and awaiting the first signal of gws  @xcite .\\nit is also known that pulsar timing arrays ( ptas ) can be used as a detector for gws @xcite .\\nthese detectors are used to search for very low frequency ( @xmath0 ) gravitational waves , where the lower limit of the observable frequencies is determined by the inverse of total observation time @xmath1 .\\nindeed , the total observation time has a crucial role in ptas , because ptas are most sensitive near the lower edge of observable frequencies @xcite . taking into account its sensitivity ,\\nthe first direct detection of the gravitational waves might be achieved by ptas .\\nthe main target of ptas is the stochastic gravitational wave background ( sgwb ) generated by a large number of unresolved sources with the astrophysical origin or the cosmological origin in the early universe .\\nthe promising sources are super massive black hole binaries  @xcite , cosmic ( super)string  @xcite , and inflation  @xcite .\\nprevious studies have assumed that the sgwb is isotropic and unpolarized  @xcite .\\nthese assumptions are reasonable for the primary detection of the sgwb , but the deviation from the isotropy and the polarizations should have rich information of sources of gravitational waves .\\nrecently , the cross - correlation formalism has been generalized to deal with anisotropy in the sgwb @xcite .\\nresult of this work enables us to consider arbitrary levels of anisotropy , and a bayesian approach was performed by using this formalism @xcite . on the other hand , for the anisotropy of the sgwb , the cross - correlation formalism has been also developed in the case of interferometer detectors  @xcite .\\nas to the polarization , there are works including the ones motivated by the modified gravity  @xcite\\n. we can envisage supermassive black hole binaries emit circularly polarized sgwb due to the chern - simons term  @xcite .\\nthere may also exist cosmological sgwb with circular polarization in the presence of parity violating term in gravity sector  @xcite .    in this paper\\n, we investigate the detectability of circular polarization in the sgwb by ptas .\\nwe characterize sgwb by the so called stokes @xmath2 parameter  @xcite and calculate generalized overlap reduction functions ( orfs ) so that we can probe the circular polarization of the sgwb .\\nwe also discuss a method to separate the intensity ( @xmath3 mode ) and circular polarization ( @xmath2 mode ) of the sgwb .\\nthe paper is organized as follows . in section [ sec :\\nstokes parameters for a plane gravitational wave ] , we introduce the stokes parameters for monochromatic plane gravitational waves , and clarify the physical meaning of the stokes parameters @xmath3 and @xmath2 . in section [ sec : formulation ] , we formulate the cross - correlation formalism for anisotropic circularly polarized sgwb with ptas .\\nthe basic framework is essentially a combination of the formalism of @xcite , and the polarization decomposition formula of the sgwb derived in @xcite . in section [ sec : the generalized overlap reduction function for circular polarization ] , we calculate the generalized orfs for the @xmath2 mode .\\nthe results for @xmath3 mode are consistent with the previous work  @xcite . in section [ sec : separation method ] , we give a method for separation between the @xmath3 mode and @xmath2 mode of the sgwb .\\nthe final section is devoted to the conclusion . in appendixes , we present analytic results for the generalized overlap reduction functions . in this paper\\n, we will use the gravitational units @xmath4 .\\nlet us consider the stokes parameters for plane waves traveling in the direction @xmath5 , which can be described by @xmath6 \\\\\\n, \\\\\\\\ & & h_{xy}(t , z)=h_{yx}(t , z)={\\\\rm re}[b_{\\\\times}\\\\mathrm{e}^{-iw(t - z ) } ] \\\\ .\\\\end{aligned}\\\\ ] ] for an idealized monochromatic plane wave , complex amplitudes @xmath7 and @xmath8 are constants .\\npolarization of the plane gws is characterized by the tensor , ( see @xcite and also electromagnetic case @xcite ) @xmath9 where @xmath10 take @xmath11 .\\nany @xmath12 hermitian matrix can be expanded by the pauli and the unit matrices with real coefficients .\\nhence , the @xmath13 hermitian matrix @xmath14 can be written as @xmath15 where @xmath16 by analogy with electromagnetic cases , @xmath17 and @xmath2 are called stokes parameters . comparing with , we can read off the stokes parameters as @xmath18= b_{+}^{\\\\ast}b_{\\\\times}+ b_{\\\\times}^{\\\\ast}b_{+},\\\\\\\\ v&=&-2{\\\\rm i m } [ b_{+}^{\\\\ast}b_{\\\\times}]=i ( b_{+}^{\\\\ast}b_{\\\\times}- b_{\\\\times}^{\\\\ast}b_{+}).\\\\label{stv}\\\\end{aligned}\\\\ ] ] apparently , the real parameter @xmath3 is the intensity of gws . in order to reveal the physical meaning of the real parameter @xmath2 , we define the circular polarization bases @xcite @xmath19 from the relation @xmath20 we see @xmath21\\nthus , we can rewrite the stokes parameters - as @xmath22 from the above expression , we see that the real parameter @xmath2 characterizes the asymmetry of circular polarization amplitudes .\\nthe other parameters @xmath23 and @xmath24 have additional information about linear polarizations by analogy with the electromagnetic cases .\\nalternatively , we can also define the tensor @xmath25 in circular polarization bases @xmath26 where @xmath27 .\\nnote that the stokes parameters satisfy a relation @xmath28    next , we consider the transformation of the stokes parameters under rotations around the @xmath5 axis . the rotation around the @xmath5 axis is given by @xmath29 where @xmath30 is the angle of the rotation .\\nthe gws traveling in the direction @xmath5 @xmath31 transform as @xmath32 where we took the transverse traceless gauge @xmath33 after a short calculation , we obtain @xmath34 using and , the four stokes parameters ( [ sti])-([stv ] ) transform as @xmath35 as you can see , the parameters @xmath23 and @xmath24 depend on the rotation angle @xmath30 .\\nthis reflects the fact that @xmath23 and @xmath24 parameters characterize linear polarizations .\\nnote that this transformation is similar to the transformation of electromagnetic case except for the angle @xmath36 and can be rewritten as @xmath37\\nin this section , we study anisotropic distribution of sgwb and focus on the detectability of circular polarizations with pulsar timing arrays .\\nwe combine the analysis of @xcite and that of @xcite . in sec.[subsec : the spectral ] , we derive the power spectral density for anisotropic circularly polarized sgwb @xmath38 .\\nthen we also derive the dimensionless density parameter @xmath39 which is expressed by the frequency spectrum of intensity @xmath40  @xcite . in sec.[subsec : the signal ] , we extend the generalized orfs to cases with circular polarizations characterized by the parameter @xmath2 . for simplicity ,\\nwe consider specific anisotropic patterns with @xmath41 expressed by the spherical harmonics @xmath42 .      in the transverse traceless gauge , metric perturbations @xmath43 with a given propagation direction @xmath44\\ncan be expanded as @xcite @xmath45 where the fourier amplitude satisfies @xmath46 as a consequence of the reality of @xmath43 , @xmath47 , @xmath48 is the frequency of the gws , @xmath49 are spatial indices , @xmath50 label polarizations .\\nnote that the fourier amplitude @xmath51 satisfies the relation @xmath52 where @xmath53 was defined by .\\nthe polarized tensors @xmath54 are defined by @xmath55 where @xmath56 and @xmath57 are unit orthogonal vectors perpendicular to @xmath58 .\\nthe polarization tensors satisfy @xmath59 with polar coordinates , the direction @xmath44 can be represented by @xmath60 and the polarization basis vectors read @xmath61    we assume the fourier amplitudes @xmath62 are random variables , which is stationary and gaussian .\\nhowever , they are not isotropic and unpolarized .\\nthe ensemble average of fourier amplitudes can be written as @xcite @xmath63 where @xmath64 here , the bracket @xmath65 represents an ensemble average , and @xmath66 is the dirac delta function on the two - sphere .\\nthe gw power spectral density @xmath38 is a hermitian matrix , and satisfies @xmath67 because of the relation @xmath46 .\\ntherefore , we have the relations @xmath68 note that the stokes parameters are not exactly the same as the expression of , but they have the relation and characterize the same polarization .\\nwe further assume that the sgwbs satisfy @xmath69 we also assume the directional dependence of the sgwb is frequency independent @xcite .\\nthis implies the gw power spectral density is factorized into two parts , one of which depends on the direction while the other depends on the frequency .\\nbecause of the transformations - , the parameters @xmath3 and @xmath2 have spin 0 and the parameters @xmath70 have spin @xmath71  @xcite . to analyze the sgwb on the sky , it is convenient to expand the stokes parameters by spherical harmonics @xmath72\\n. however , since @xmath70 parameters have spin @xmath71 , they have to be expanded by the spin - weighted harmonics @xmath73 @xcite .\\nthus , we obtain @xmath74 in this paper , we study specific anisotropic patterns with @xmath41 for simplicity .\\ntherefore , we can neglect @xmath23 and @xmath24 from now on .\\nthus , the gw power spectral density becomes @xmath75 where @xmath76 so , we focus on the parameters @xmath3 and @xmath2 . in what follows , we will use the following shorthand notation @xmath77    next , we consider the dimensionless density parameter  @xcite @xmath78 where @xmath79 is the critical density , @xmath80 is the present value of the hubble parameter , @xmath81 is the energy density of gravitational waves , and @xmath82 is the energy density in the frequency range @xmath48 to @xmath83 .\\nthe bracket @xmath65 represents the ensemble average .\\nhowever , actually , we take a spatial average over the wave lengths @xmath84 of gws or a temporal average over the periods @xmath85 of gws . here\\n, we assumed the ergodicity , namely , the ensemble average can be replaced by the temporal average .\\nusing , , , as well as @xmath46 and @xmath86 , we get @xmath87 then we define @xmath88 hence , the dimensionless quantity @xmath39 in is given by @xmath89 where the spherical harmonics are orthogonal and normalized as @xmath90 using @xmath91 , we obtain @xmath92 without loss of generality , we normalize the monopole moment as @xmath93 so , becomes @xmath94      the time of arrival of radio pulses from the pulsar is affected by gws .\\nconsider a pulsar with frequency @xmath95 located in the direction @xmath96 . to detect the sgwb ,\\nlet us consider the redshift of the pulse from a pulsar @xcite @xmath97 where @xmath98 is a frequency detected at the earth and @xmath96 is the direction to the pulsar .\\nthe unit vector @xmath44 represents the direction of propagation of gravitational plane waves .\\nwe also defined the difference between the metric perturbations at the pulsar @xmath99 and at the earth @xmath100 as @xmath101 the gravitational plane waves at each point is defined as @xmath102 for the sgwb , the redshift have to be integrated over the direction of propagation of the gravitational waves @xmath44 : @xmath103 we choose a coordinate system @xmath104 and assume that the amplitudes of the metric perturbation at the pulsar and the earth are the same .\\nthen becomes @xmath105 and therefore , reads @xmath106 where we have defined the pattern functions for pulsars @xmath107 note that our convention for the fourier transformation is @xmath108 therefore , the fourier transformation of can be written as @xmath109    in the actual signals from a pulsar , there exist noises .\\nhence , we need to use the correlation analysis .\\nwe consider the signals from two pulsars @xmath110 where @xmath111 labels the pulsar . here\\n, @xmath112 denotes the signal from the pulsar and @xmath113 denotes the noise intrinsic to the measurement .\\nwe assume the noises are stationary , gaussian and are not correlated between the two pulsars .\\nto correlate the signals of two measurements , we define @xmath114 where @xmath1 is the total observation time and @xmath115 is a real filter function which should be optimal to maximize signal - to - noise ratio . in the case of interferometer\\n, the optimal filter function falls to zero for large @xmath116 compered to the travel time of the light between the detecters .\\nsince the signals of two detectors are expected to correlate due to the same effect of the gravitational waves , the optimal filter function should behave this way .\\nthen , typically one of the detectors is very close to the other compared to the total observation time @xmath1 .\\ntherefore , the total observation time @xmath1 can be extended to @xmath117 @xcite .\\nin contrast , in the case of pta , it is invalid that @xmath1 is very large compered to the travel time of the light between the pulsars .\\nnevertheless , we can assume that one of the two @xmath1 can be expanded to @xmath117 , because in situations @xmath118 and @xmath119 it is known that we can ignore the effect of the distance @xmath120 of pulsars .\\nin this case , it is clear that any locations of the pulsars are optimal and optimal filter function should behave like as the interferometer case @xcite .    using these assumptions @xmath118 and @xmath119 , we can rewrite as @xmath121 where @xmath122 note that @xmath123 satisfies @xmath124 , because @xmath125 is real .\\nmoreover , to deal with the unphysical region @xmath126 we require @xmath127 .\\nthus , @xmath123 becomes real .\\ntaking the ensemble average , using @xmath128 , @xmath118 , and assuming the noises in the two measurements are not correlated , we get @xmath129\\\\ , \\\\label{s2}\\\\end{aligned}\\\\ ] ] where we have defined @xmath130 the functions @xmath131 and @xmath132 are called the generalized orfs , which describe the angular sensitivity of the pulsars for the sgwb . note that , as we already mentioned , we consider the cases of @xmath41 for simplicity\\n. then we have assumed @xmath118 and @xmath119 , this assumption implies that approximately becomes @xmath133 due to the rapid oscillation of the phase factor .\\ntherefore , the distance @xmath120 of the pulsars does not appear in the generalized orfs , and hence the generalized orfs do not depend on the frequency .    as you can see from ,\\nthe correlation of the two measurements involve both the total intensity and the circular polarization .\\nhowever , the degeneracy can be disentangled by using separation method , which will be discussed in the section [ sec : separation method ] .\\nin this section , we consider the generalized orfs for circular polarizations : @xmath134 where we defined @xmath135 in the above , we have used and the fact that the generalized orfs do not depend on frequency . for computation of the generalized orfs for circular polarizations ,\\nit is convenient to use the computational frame @xcite defined by @xmath136 where @xmath137 is the angular separation between the two pulsars . using - , , and\\n, one can easily show that @xmath138 we therefore get @xmath139 the explicit form of the spherical harmonics reads @xmath140 where @xmath141 is the normalization factor .\\nthe associated legendre functions are given by @xmath142 and @xmath143 with the legendre functions @xmath144\\\\ .\\\\label{pl}\\\\end{aligned}\\\\ ] ] using the spherical harmonics , becomes @xmath145 where we have used the fact that the function of @xmath146 is odd parity in the case of @xmath147 and is even parity in the case of @xmath148 .\\nnote that the generalized orfs for circular polarizations are real functions . in the case of @xmath149 and/or @xmath150 ,\\nthe integrand in vanishes .\\ntherefore , we can not detect circular polarizations for these cases .\\nthis fact for @xmath151 implies that we do not need to consider auto - correlation for a single pulsar .\\nthis is the reason why we neglected auto - correlation term in .    integrating ,\\nwe get the following form for @xmath152 : @xmath153 for @xmath154 , we have obtained @xmath155 \\\\ , \\\\\\\\ \\\\gamma^{v}_{1 - 1}&=&\\\\gamma^{v}_{11 } \\\\ , \\\\end{aligned}\\\\ ] ] recall that @xmath156 .\\nthe derivation of this formula for @xmath154 can be found in appendix [ sec : angular integral of the generalized overlap reduction function for dipole circular polarization ] .    for @xmath157 , we derived the following : @xmath158\\\\ , \\\\\\\\ \\\\gamma^{v}_{2 - 1}&=&\\\\gamma^{v}_{21}\\\\ , \\\\\\\\\\n\\\\gamma^{v}_{22}&=&-\\\\frac{\\\\sqrt{30\\\\pi}}{6}(1-\\\\cos\\\\xi)\\\\left[2-\\\\cos\\\\xi+6\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{v}_{2 - 2}&=&-\\\\gamma^{v}_{22}\\\\ , \\\\end{aligned}\\\\ ] ] for @xmath159 , the results are @xmath160\\\\ , \\\\\\\\ \\\\gamma^{v}_{3 - 1}&=&\\\\gamma^{v}_{31}\\\\ , \\\\\\\\ \\\\gamma^{v}_{32}&=&\\\\frac{\\\\sqrt{210\\\\pi}}{24}(1-\\\\cos\\\\xi)\\\\left[8 - 5\\\\cos\\\\xi-\\\\cos^2\\\\xi+24\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{v}_{3 - 2}&=&-\\\\gamma^{v}_{3 - 2}\\\\ , \\\\\\\\ \\\\gamma^{v}_{33}&=&-\\\\frac{\\\\sqrt{35\\\\pi}}{16}\\\\sin\\\\xi\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\left[11 - 6\\\\cos\\\\xi-\\\\cos^2\\\\xi+32\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{v}_{3 - 3}&=&\\\\gamma^{v}_{33}\\\\ .\\\\end{aligned}\\\\ ] ] in fig .\\n[ gv ] , we plotted these generalized orfs as a function of the angular separation between the two pulsars @xmath137 .\\nit is apparent that considering the @xmath2 mode does not make sense when we only consider the isotropic ( @xmath152 ) orf . on the other hand ,\\nwhen we consider anisotropic ( @xmath161 ) orfs , it is worth taking into account polarizations .\\nthe polarizations of the sgwb would give us rich information both of super massive black hole binaries and of inflation in the early universe .\\nas a function of the angular separation between the two pulsars @xmath137 . in fig .\\n[ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig .\\n[ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig .\\n[ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig .\\n[ gv](d ) , the orfs for the octupole ( l=3 ) are plotted .\\nthe black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=\"fig:\",width=340 ] ( a ) @xmath152     as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig .\\n[ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig .\\n[ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig .\\n[ gv](d ) , the orfs for the octupole ( l=3 ) are plotted .\\nthe black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=\"fig:\",width=340 ] ( b ) @xmath154     as a function of the angular separation between the two pulsars @xmath137 . in fig . [ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig .\\n[ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig .\\n[ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig .\\n[ gv](d ) , the orfs for the octupole ( l=3 ) are plotted .\\nthe black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=\"fig:\",width=340 ] ( c ) @xmath157     as a function of the angular separation between the two pulsars @xmath137 . in fig .\\n[ gv](a ) , we find the orf for the monopole ( l=0 ) is trivial . in fig .\\n[ gv](b ) , the orfs for the dipole ( l=1 ) are shown . in fig .\\n[ gv](c ) , the orfs for the quadrupole ( l=2 ) are depicted . in fig .\\n[ gv](d ) , the orfs for the octupole ( l=3 ) are plotted .\\nthe black solid curve , the blue dashed curve , the red dotted curve , the dark - red space - dotted curve , and the green long - dashed curve represent @xmath149 , @xmath162 , @xmath163 , @xmath164 , @xmath165 , respectively.,title=\"fig:\",width=340 ] ( d ) @xmath159    using the same procedure described in the above to derive the generalized orfs for circular polarizations , we can also derive the generalized orfs for the intensity @xmath166 where @xmath167 the angular integral in this case was performed in @xcite .\\nthe results are summarized in appendix [ sec : the generalized overlap reduction function for intensity ] .\\nin this section , we separate the @xmath3 mode and @xmath2 mode of the sgwb with correlation analysis @xcite . to this aim ,\\nwe use four pulsars ( actually we need at least three pulsars ) , and define correlations of @xmath168 @xmath169 where @xmath170 label the pulsars . comparing with , we obtain @xmath171 \\\\ ,\\n\\\\label{1c12}\\\\\\\\ & & c_{34}(f)=\\\\sum_{lm}^{l=3}\\\\left[c_{lm}^{i}i(f)\\\\gamma_{lm,34}^{i}+c_{lm}^{v}v(f)\\\\gamma_{lm,34}^{v}\\\\right ] \\\\ .\\\\label{1c34}\\\\end{aligned}\\\\ ] ] if the @xmath3 mode and @xmath2 mode of the sgwb are dominated by a certain @xmath172 and @xmath173 , and become @xmath174 \\\\ , \\\\label{2c12 } \\\\\\\\ & & c_{34}(f)=\\\\left[c _ { l m}^{i}i(f)\\\\gamma _ { l m,34}^{i}+c _ { l \\' m\\'}^{v}v(f)\\\\gamma _ { l \\' m\\',34}^{v}\\\\right ] \\\\ .\\\\label{2c34}\\\\end{aligned}\\\\ ] ] to separate the intensity and the circular polarization , we take the following linear combinations @xmath175 where we defined coefficients @xmath176 as you can see , @xmath177 contains only @xmath40 , and @xmath178 contains only @xmath179 .\\nfor the signal @xmath180 , the formulas corresponding to and are given by @xmath181 \\\\ , \\\\label{sp}\\\\end{aligned}\\\\ ] ] where @xmath182 denotes @xmath3 and @xmath2 .\\nwe assume @xmath183 and that the noise in the four pulsars are not correlated .\\nwe also assume that the ensemble average of fourier amplitudes of the noises @xmath184 is of the form @xmath185 where @xmath186 is the noise power spectral density .\\nthe reality of @xmath187 gives rise to @xmath188 and therefore we obtain @xmath189 . without loss of generality\\n, we can assume @xmath190 then we obtain corresponding noises @xmath191 : @xmath192\\\\ , \\\\label{np}\\\\end{aligned}\\\\ ] ] where @xmath193^{1/2 } \\\\label{sn12 } \\\\ , \\\\quad s_{n,34}(f ) \\\\equiv [ s_{n,3}(f)s_{n,4}(f)]^{1/2 } \\\\label{sn34 } \\\\ .\\\\end{aligned}\\\\ ] ] using the inner product @xmath194 \\\\ , \\\\end{aligned}\\\\ ] ] we can rewrite , as @xmath195 therefore , the optimal filter function can be chosen as @xmath196 using , we get optimal signal - to - noise ratio @xmath197^{1/2}\\\\ .\\\\label{snr}\\\\end{aligned}\\\\ ] ] plugging , , and into , we obtain @xmath198^{1/2}\\\\ , \\\\\\\\\\n{ \\\\rm snr}_{v}&=&\\\\left[t\\\\int_{-\\\\infty}^{\\\\infty}df\\\\,\\\\,\\\\frac{\\\\left(c^{v}_{{l}\\'{m}\\'}\\\\right)^{2}v^{2}(f)\\\\left(\\\\gamma_{{l}\\'{m}\\',34}^{v}\\\\gamma^{i}_{{l}{m},12}-\\\\gamma_{{l}\\'{m}\\',12}^{v}\\\\gamma^{i}_{{l}{m},34}\\\\right)^2}{\\\\left(\\\\gamma^{i}_{{l}{m},12}\\\\right)^2s^{2}_{n,34}(f)+\\\\left(\\\\gamma^{i}_{{l}{m},34}\\\\right)^2s^{2}_{n,12}(f)}\\\\right]^{1/2}\\\\ .\\\\end{aligned}\\\\ ] ] if we assume all of the noise power spectral densities are the same , becomes @xmath199 thus , the compiled orfs can be defined as @xmath200^{1/2}}\\\\ , \\\\\\\\ \\\\gamma_{12:34}^{v}&\\\\equiv&\\\\frac{\\\\gamma_{{l}\\'{m}\\',34}^{v}\\\\gamma^{i}_{{l}{m},12}-\\\\gamma_{{l}\\'{m}\\',12}^{v}\\\\gamma^{i}_{{l}{m},34}}{\\\\left[\\\\left(\\\\gamma^{i}_{{l}{m},12}\\\\right)^2+\\\\left(\\\\gamma^{i}_{{l}{m},34}\\\\right)^2\\\\right]^{1/2}}\\\\ .\\\\end{aligned}\\\\ ] ] this compiled orfs @xmath201 and @xmath202 describe the angular sensitivity of the four pulsars for the pure @xmath3 and @xmath2 mode of the sgwb , respectively .\\nnote that , to do this separation , we must know a priori the coefficients @xmath203 and @xmath204 .\\nif we do not assume , the generalized orfs depend on the frequency . in this case\\n, it seems difficult to calculate these coefficients .\\nwe next consider the case that @xmath3 mode and/or @xmath2 mode dominant in two or more @xmath205 . in this case , if we have a priori knowledge of the values of @xmath206 in each of @xmath205 for coefficients\\n@xmath203 and @xmath204 , we can separate @xmath3 mode and @xmath2 mode . for example , assume that @xmath3 mode is dominated by @xmath207 , while @xmath2 mode is dominated by @xmath208 , then and become @xmath209\\\\ , \\\\label{3c12}\\\\\\\\ & & c_{34}(f)=\\\\left[c^{i}_{00}i(f)\\\\left(\\\\gamma_{00,34}^{i}+\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,34}^{i}\\\\right)+c_{11}^{v}v(f)\\\\gamma_{11,34}^{v}\\\\right]\\\\ .\\\\label{3c34}\\\\end{aligned}\\\\ ] ] thus , we can separate @xmath3 mode and @xmath2 mode by using linear combinations @xmath210\\\\ , \\\\\\\\\\nd_{v}&\\\\equiv&a_{v}c_{34}(f)+b_{v}c_{12}(f ) \\\\nonumber\\\\\\\\ & = & c_{11}^{v}v(f)\\\\left[\\\\gamma_{11,34}^{v}\\\\left(\\\\gamma_{00,12}^{i}+\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,12}^{i}\\\\right)-\\\\gamma_{11,12}^{v}\\\\left(\\\\gamma_{00,34}^{i}+\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,34}^{i}\\\\right)\\\\right]\\\\ , \\\\end{aligned}\\\\ ] ] where @xmath211 as in the previous calculations , we can get the compiled orfs @xmath212^{1/2}}\\\\ , \\\\label{gi1234}\\\\\\\\ \\\\gamma_{12:34}^{v}&\\\\equiv&\\\\frac{\\\\gamma_{11,34}^{v}\\\\left(\\\\gamma_{00,12}^{i}+\\\\displaystyle\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,12}^{i}\\\\right)-\\\\gamma_{11,12}^{v}\\\\left(\\\\gamma_{00,34}^{i}+\\\\displaystyle\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,34}^{i}\\\\right)}{\\\\left[\\\\left(\\\\gamma_{00,12}^{i}+\\\\displaystyle\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,12}^{i}\\\\right)^2+\\\\left(\\\\gamma_{00,34}^{i}+\\\\displaystyle\\\\frac{c^{i}_{11}}{c^{i}_{00}}\\\\gamma_{11,34}^{i}\\\\right)^2\\\\right]^{1/2}}\\\\ .\\\\label{gv1234}\\\\end{aligned}\\\\ ] ]    [ cols=\"^,^ \" , ]     in fig .\\n[ cg ] we show some compiled orfs @xmath213 ( left panels ) and @xmath214 ( right panels ) as a function of the two angular separations @xmath137 and @xmath215 for two pulsar pairs , respectively .\\nwe used the expressions of @xmath2 mode and @xmath3 mode ( see appendix [ sec : the generalized overlap reduction function for intensity ] ) , and we assumed @xmath216 for simplicity . in fig .\\n[ cg](a ) and [ cg](b ) , the @xmath3 mode is dominated by @xmath217 and @xmath2 mode is dominated by @xmath218 . in fig .\\n[ cg](c ) and [ cg](d ) , the @xmath3 mode is dominated by @xmath219 and @xmath2 mode is dominated by @xmath218 . in fig .\\n[ cg](e ) and [ cg](f ) , the @xmath3 mode is dominated by @xmath207 and @xmath2 mode is dominated by @xmath218 . in fig .\\n[ cg](e ) and [ cg](f ) , the @xmath3 mode is dominated by @xmath220 and @xmath2 mode is dominated by @xmath218 . by definition , in the case of @xmath221 ,\\nthe compiled orfs are zero .\\nwe have studied the detectability of the stochastic gravitational waves with ptas . in most of the previous works ,\\nthe isotropy of sgwb has been assumed for the analysis .\\nrecently , however , a stochastic gravitational wave background with anisotropy have been considered .\\nthe information of the anisotropic pattern of the distribution should contain important information of the sources such as supermassive black hole binaries and the sources in the early universe .\\nit is also intriguing to take into account the polarization of sgwb in the pta analysis .\\ntherefore , we extended the correlation analysis to circularly polarized sgwb and calculated generalized overlap reduction functions for them .\\nit turned out that the circular polarization can not be detected for an isotropic background .\\nhowever , when the distribution has anisotropy , we have shown that there is a chance to observe circular polarizations in the sgwb .\\nwe also discussed how to separate polarized modes from unpolarized modes of gravitational waves .\\nif we have a priori knowledge of the abundance ratio for each mode in each of @xmath205 , we can separate @xmath3 mode and @xmath2 mode in general .\\nthis would be possible if we start from fundamental theory and calculate the spectrum of sgwb .\\nin particular , in the case that the signal of lowest @xmath222 is dominant , we performed the separation of @xmath3 mode and @xmath2 mode explicitly .\\nthis work was supported by grants - in - aid for scientific research ( c ) no.25400251 and \" mext grant - in - aid for scientific research on innovative areas no.26104708 and `` cosmic acceleration\\'\\'(no.15h05895 ) .\\nin this appendix , we perform angular integration of the generalized orf for dipole ( @xmath154 ) circular polarization ( see @xcite ) : @xmath223 where we have defined @xmath224 .\\nit is obvious that in the case of @xmath225 , integrand of the generalized orf is zero , because of @xmath226 , then we obtain @xmath227 then , using - , we calculate @xmath228 and we find @xmath229    therefore we only have to consider the dipole generalized orf in the case of @xmath154 , @xmath230 : @xmath231 where @xmath232 first , to calculate @xmath233 , we use contour integral in the complex plane . defining @xmath234 and substituting @xmath235 into , we can rewrite @xmath233 as @xmath236 } \\\\ , \\\\end{aligned}\\\\ ] ] where @xmath237 denotes a unit circle .\\nwe can factorize the denominator of the integrand and get @xmath238 where @xmath239 hereafter , the upper sign applies when @xmath240 and the lower one applies when @xmath241 .\\nnote that we only consider the region @xmath242 , so we have used the relation @xmath243 in above expression . in the region\\n@xmath244 , @xmath245 is inside the unit circle @xmath237 except for @xmath246 and @xmath247 is outside the unit circle @xmath237 .\\nnow , we can perform the integral using the residue theorem @xmath248 where @xmath249 the residues inside the unit circle @xmath237 can be evaluated as @xmath250\\\\right\\\\ }             = \\\\frac{i(z_{+}+z_{-})}{2\\\\sqrt{1-x^2}\\\\sin\\\\xi } \\\\ , \\\\end{aligned}\\\\ ] ] @xmath251 thus , we obtain @xmath252 next , we consider @xmath253 defined in\\n. using , we can calculate @xmath253 as @xmath254    similarly , we can evaluate @xmath255 given in . to calculate @xmath255 in the complex plane , we again substitute into and obtain @xmath256 we use the residue theorem @xmath257 where @xmath258 the residues inside the unit circle @xmath237 can be calculated as @xmath259\\\\right\\\\ }        = \\\\frac{i(z_{+}^2+z_{-}^2)}{4\\\\sqrt{1-x^2}\\\\sin\\\\xi } \\\\ , \\\\end{aligned}\\\\ ] ] @xmath260 therefore , @xmath255 becomes @xmath261 substituting to , we can calculate @xmath262 : @xmath263    finally , substituting and into , we get the generalized orf for @xmath264 @xmath265\\\\ .\\\\end{aligned}\\\\ ] ]     as a function of the angular separation between the two pulsars @xmath137 .\\n[ gi](a ) shows monopole ( l=0 ) , fig .\\n[ gi](b ) shows dipole ( l=1 ) , fig .\\n[ gi](c ) shows quadrupole ( l=2 ) and fig .\\n[ gi](d ) shows octupole ( l=3 ) .\\nthe black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=\"fig:\",width=340 ] ( a ) @xmath152     as a function of the angular separation between the two pulsars @xmath137 .\\n[ gi](a ) shows monopole ( l=0 ) , fig .\\n[ gi](b ) shows dipole ( l=1 ) , fig .\\n[ gi](c ) shows quadrupole ( l=2 ) and fig .\\n[ gi](d ) shows octupole ( l=3 ) .\\nthe black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=\"fig:\",width=340 ] ( b ) @xmath154     as a function of the angular separation between the two pulsars @xmath137 . fig . [ gi](a ) shows monopole ( l=0 ) , fig .\\n[ gi](b ) shows dipole ( l=1 ) , fig .\\n[ gi](c ) shows quadrupole ( l=2 ) and fig .\\n[ gi](d ) shows octupole ( l=3 ) .\\nthe black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=\"fig:\",width=340 ] ( c ) @xmath157     as a function of the angular separation between the two pulsars @xmath137 .\\n[ gi](a ) shows monopole ( l=0 ) , fig .\\n[ gi](b ) shows dipole ( l=1 ) , fig .\\n[ gi](c ) shows quadrupole ( l=2 ) and fig .\\n[ gi](d ) shows octupole ( l=3 ) . the black solid curve , the blue dashed curve , the dark - blue dash - dotted curve , the red dotted curve , the green long - dashed curve , the dark - green space - dashed curve represent @xmath149 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 , respectively.,title=\"fig:\",width=340 ] ( d ) @xmath159\\nin this appendix , we show orfs for the intensity @xcite .\\nthe following form for @xmath152 was derived in @xcite , and our expressions are identical to their expressions : @xmath271\\\\ , \\\\end{aligned}\\\\ ] ] for , @xmath154 , we calculated as @xmath272\\\\ , \\\\\\\\ \\\\gamma^{i}_{11}&=&\\\\frac{\\\\sqrt{6\\\\pi}}{12}\\\\sin\\\\xi\\\\left[1 + 3(1-\\\\cos\\\\xi)\\\\left\\\\{1+\\\\frac{4}{1+\\\\cos\\\\xi}\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right\\\\}\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{1 - 1}&=&-\\\\gamma^{i}_{11}\\\\ , \\\\end{aligned}\\\\ ] ] for @xmath157 , we obtain @xmath273\\\\ , \\\\\\\\ \\\\gamma^{i}_{21}&=&-\\\\frac{\\\\sqrt{30\\\\pi}}{60}\\\\sin\\\\xi\\\\left[21 - 15\\\\cos\\\\xi-5\\\\cos^2\\\\xi+60\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{2 - 1}&=&-\\\\gamma^{i}_{2 - 1}\\\\ , \\\\\\\\ \\\\gamma^{i}_{22}&=&\\\\frac{\\\\sqrt{30\\\\pi}}{24}(1-\\\\cos\\\\xi)\\\\left[9 - 4\\\\cos\\\\xi-\\\\cos^2\\\\xi+24\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{2 - 2}&=&\\\\gamma^{i}_{22}\\\\ ,\\n\\\\end{aligned}\\\\ ] ] for @xmath159 , it is straightforward to reach the following @xmath274\\\\ , \\\\\\\\ \\\\gamma^{i}_{31}&=&\\\\frac{\\\\sqrt{21\\\\pi}}{48}\\\\sin\\\\xi(1-\\\\cos\\\\xi)\\\\left[34 + 15\\\\cos\\\\xi+5\\\\cos^2\\\\xi+\\\\frac{96}{1+\\\\cos\\\\xi}\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{3 - 1}&=&-\\\\gamma^{i}_{31}\\\\ , \\\\\\\\ \\\\gamma^{i}_{32}&=&-\\\\frac{\\\\sqrt{210\\\\pi}}{48}(1-\\\\cos\\\\xi)\\\\left[17 - 9\\\\cos\\\\xi-3\\\\cos^2\\\\xi-\\\\cos^3\\\\xi+48\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{3 - 2}&=&\\\\gamma^{i}_{32}\\\\ , \\\\\\\\ \\\\gamma^{i}_{33}&=&\\\\frac{\\\\sqrt{35\\\\pi}}{48}\\\\frac{(1-\\\\cos\\\\xi)^2}{\\\\sin\\\\xi}\\\\left[34 - 17\\\\cos\\\\xi-4\\\\cos^2\\\\xi-\\\\cos^3\\\\xi+96\\\\left(\\\\frac{1-\\\\cos\\\\xi}{1+\\\\cos\\\\xi}\\\\right)\\\\log\\\\left(\\\\sin\\\\frac{\\\\xi}{2}\\\\right)\\\\right]\\\\ , \\\\\\\\ \\\\gamma^{i}_{3 - 3}&=&-\\\\gamma^{i}_{33}\\\\ .\\\\end{aligned}\\\\ ] ] these are plotted in fig .\\nthe generalized orfs of total intensity are different from that of circular polarization in that the value for @xmath149 is non - trivial .\\nthen the @xmath3 mode orfs for @xmath275 have value even in the case of @xmath151 .\\nthis implies that we can consider auto - correlation for a single pulsar .\\n99 j.  aasi _ et al .\\n_ [ ligo scientific collaboration ] , class .\\ngrav .   * 32 * , 074001 ( 2015 ) doi:10.1088/0264 - 9381/32/7/074001 [ arxiv:1411.4547 [ gr - qc ] ] .\\nf.  acernese _ et al . _\\n[ virgo collaboration ] , class .\\ngrav .   * 32 * , no . 2 , 024001 ( 2015 ) doi:10.1088/0264 - 9381/32/2/024001 [ arxiv:1408.3978 [ gr - qc ] ] . k.  somiya [ kagra collaboration ] , class .\\n* 29 * , 124007 ( 2012 ) doi:10.1088/0264 - 9381/29/12/124007 [ arxiv:1111.7185 [ gr - qc ] ] .\\ns.  l.  detweiler , astrophys .\\nj.   * 234 * , 1100 ( 1979 ) .\\ndoi:10.1086/157593 romani , r. w. ( 1989 ) .\\ntiming a millisecond pulsar array . timing neutron stars , 113 - 117 .\\nl.  lentati _ et al .\\n_ , mon .  not .\\nsoc .   * 453 * , 2576 ( 2015 ) [ arxiv:1504.03692 [ astro-ph.co ] ] .\\nz.  arzoumanian _ et al .\\n_ [ nanograv collaboration ] , arxiv:1508.03024 [ astro-ph.ga ] . e.  s.  phinney , astro - ph/0108028 . a.  vilenkin , phys .\\nrept .   * 121 * , 263 ( 1985 ) .\\ns.  kuroyanagi , k.  miyamoto , t.  sekiguchi , k.  takahashi and j.  silk , phys .\\nd * 87 * , no . 2 , 023522 ( 2013 ) [ phys .\\nd * 87 * , no .\\n6 , 069903 ( 2013 ) ] [ arxiv:1210.2829 [ astro-ph.co ] ] .\\nm.  maggiore , gr - qc/0008027 .\\nl.  p.  grishchuk , phys .\\n* 48 * , 1235 ( 2005 ) doi:10.1070/pu2005v048n12abeh005795 [ gr - qc/0504018 ] .\\nb.  allen and j.  d.  romano , phys .\\nd * 59 * , 102001 ( 1999 ) [ gr - qc/9710117 ] .\\nc.  m.  f.  mingarelli , t.  sidery , i.  mandel and a.  vecchio , phys .\\nd * 88 * , no . 6 , 062005 ( 2013 ) [ arxiv:1306.5394 [ astro-ph.he ] ] .\\ns.  r.  taylor and j.  r.  gair , phys .\\nd * 88 * , 084001 ( 2013 ) [ arxiv:1306.5395 [ gr - qc ] ] .\\nn.  seto and a.  taruya , phys .\\nlett .   * 99 * , 121101 ( 2007 ) doi:10.1103/physrevlett.99.121101 [ arxiv:0707.0535 [ astro - ph ] ] . n.  seto and a.  taruya , phys .  rev .\\nd * 77 * , 103001 ( 2008 ) [ arxiv:0801.4185 [ astro - ph ] ] . s.  j.  chamberlin and x.  siemens , phys .\\nd * 85 * , 082001 ( 2012 ) doi:10.1103/physrevd.85.082001 [ arxiv:1111.5661 [ astro-ph.he ] ] . j.  r.  gair , j.  d.  romano and s.  r.  taylor , phys .  rev .\\nd * 92 * , no .\\n10 , 102003 ( 2015 ) doi:10.1103/physrevd.92.102003 [ arxiv:1506.08668 [ gr - qc ] ] .\\nr.  jackiw and s.  y.  pi , phys .\\nd * 68 * , 104012 ( 2003 ) doi:10.1103/physrevd.68.104012 [ gr - qc/0308071 ] . m.  satoh , s.  kanno and j.  soda , phys .\\nd * 77 * , 023526 ( 2008 ) doi:10.1103/physrevd.77.023526 [ arxiv:0706.3585 [ astro - ph ] ] . c.  r.  contaldi , j.  magueijo and l.  smolin , phys .\\n* 101 * ( 2008 ) 141101 doi:10.1103/physrevlett.101.141101 [ arxiv:0806.3082 [ astro - ph ] ] .\\nt.  takahashi and j.  soda , phys .\\nlett .   * 102 * , 231301 ( 2009 ) doi:10.1103/physrevlett.102.231301 [ arxiv:0904.0554 [ hep - th ] ] .\\nj.  l.  cook and l.  sorbo , phys .\\nd * 85 * , 023534 ( 2012 ) [ phys .\\nd * 86 * , 069901 ( 2012 ) ] doi:10.1103/physrevd.86.069901 , 10.1103/physrevd.85.023534 [ arxiv:1109.0022 [ astro-ph.co ] ] .\\ni.  obata , t.  miura and j.  soda , phys .\\nd * 92 * , no .\\n6 , 063516 ( 2015 ) doi:10.1103/physrevd.92.063516 [ arxiv:1412.7620 [ hep - ph ] ] .\\na. p. lightman , w. h. press , r. h. price , and s. a. teukolski , problem book in relativity and gravitation , 2nd ed .\\n( princeton university press , 1979 ) .\\nm. maggiore , gravitational waves , vol . 1 : theory and experiments ( oxford university press , 2008 )\\n. g. b. rybicki , a. p. lightman , radiative processes in astrophysics ( wiley - interscience , 1979 ) .\\nl. d. landau and e. m. lifshitz , the classical theory ( pergamon press , 1975 ) . c. misner , k. thorne and j. wheeler , gravitation , ( freeman 1973 ) .\\nb.  allen and a.  c.  ottewill , phys .\\nd * 56 * , 545 ( 1997 ) doi:10.1103/physrevd.56.545 [ gr - qc/9607068 ] .\\nu.  seljak and m.  zaldarriaga , phys .\\nlett .   * 78 * , 2054 ( 1997 ) doi:10.1103/physrevlett.78.2054 [ astro - ph/9609169 ] .\\nj. n. goldberg et al .\\n, journal of mathematical physics * 8 * , 2155 , 1967 . m.  anholm , s.  ballmer , j.  d.  e.  creighton , l.  r.  price and x.  siemens , phys .\\nd * 79 * , 084030 ( 2009 ) doi:10.1103/physrevd.79.084030 [ arxiv:0809.0701 [ gr - qc ] ] . l.  g.  book and e.  e.  flanagan , phys .\\nd * 83 * , 024024 ( 2011 ) doi:10.1103/physrevd.83.024024 [ arxiv:1009.4192 [ astro-ph.co ] ] . f.  a.  jenet and j.  d.  romano , am .\\nj.  phys .\\n* 83 * , 635 ( 2015 ) doi:10.1119/1.4916358 [ arxiv:1412.1142 [ gr - qc ] ] .\\nr.  w.  hellings and g.  s.  downs , astrophys .\\nj.   * 265 * , l39 ( 1983 ) . doi:10.1086/183954'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97268bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' starting from the wkb approximation , a new barrier penetration formula is proposed for potential barriers containing a long - range coulomb interaction . \\n this formula is especially proper for the barrier penetration with penetration energy much lower than the coulomb barrier . \\n the penetrabilities calculated from the new formula agree well with the results from the wkb method . as a first attempt , \\n this new formula is used to evaluate @xmath0 decay half - lives of atomic nuclei and a good agreement with the experiment is obtained . '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['abstract'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bff7e809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as a common quantum phenomenon , the tunneling through a potential barrier plays a very important role in the microscopic world and has been studied extensively since the birth of quantum mechanics .\\none of the earliest applications of quantum tunneling is the explanation of @xmath0 decays in atomic nuclei .\\nthe quantum tunneling effect governs also many other nuclear processes such as fission and fusion .\\nin particular , a lot of new features are revealed in sub - barrier fusion reactions which are closely connected with the tunneling phenomena  @xcite .    for most of the potential barriers , the penetrability can not be calculated analytically  @xcite . among those potentials for which analytical solutions can be obtained ,\\nthe parabolic potential  @xcite is the mostly used in the study of nuclear fusion . by approximating the coulomb barrier to a parabola\\n, wong derived an analytic expression for the fusion cross section  @xcite which is widely adopted today in the study of heavy ion reactions ( see , e.g. , recent refs .\\nthe parabolic approximation works remarkably well both for the penetrability and for the fusion cross section at energies around or above the coulomb barrier  @xcite .\\napparently the parabolic approximation breaks down at energies much smaller than the barrier height due to the long - range coulomb interaction .\\none may calculate the penetration probability numerically by using the path integral method or the wkb approximation .\\nhowever , it is highly desirable to have an analytical expression for the barrier penetrability when one introduces an energy - dependent one - dimensional potential barrier  @xcite or barrier distribution functions  @xcite .    in the present work\\n, we derived a new barrier penetration formula based on the wkb approximation .\\nthe influence of the long coulomb tail in the barrier potential is taken into accout properly .\\ntherefore this formula is especially applicable to the barrier penetration with penetration energy much lower than the coulomb barrier .    as a first attempt and a test study\\n, we apply this new formula to evaluate @xmath0 decay half - lives of atomic nuclei . for the @xmath0 decay ,\\nthe penetrability is usually calculated with the wkb approach  @xcite , in other words , integrating numerically the wave number within two turning points at which the interaction potential is equal to the @xmath1-value of the @xmath0 decay .\\nwe will show that the present analytical formula reproduces the experimental results very well , especially for spherical nuclei .\\nthe paper is organized as follows . in sec .\\n[ sec : formalism ] we present the new barrier penetration formula .\\nthe validity of the new formula is investigated and its application to @xmath0 decays are given in sec .\\n[ sec : results ] . finally in sec .\\n[ sec : summary ] we summarize our work . in the appendix ,\\nthe detailed derivation of the new penetration formula is given .\\nwhen the penetration energy is well below the coulomb barrier , the barrier penetrability formula derived from the wkb approximation reads , @xmath2 ,   \\\\label{eq : wkb}\\\\ ] ] where the potential usually consists of three parts , the nuclear , the coulomb , and the centrifugal potentials , @xmath3 @xmath4 and @xmath5 are the inner and outer turning points determined by the relation @xmath6 .    by approximating @xmath7 to a parabola with the height @xmath8 and the width @xmath9 , eq .\\n( [ eq : wkb ] ) is reduced as @xmath10 ,   \\\\label{eq : hw}\\\\ ] ] which has been widely used in the study of heavy ion reactions .    because of the long - range coulomb interaction , the coulomb barrier given in eq .\\n( [ eq : potential ] ) has a long tail and is asymmetric .\\nthus for the penetration well below the barrier , the parabolic approximation is not valid .\\nwe may divide the potential barrier into two parts at the barrier position @xmath11 .\\nthe first part of @xmath7 with @xmath12 could still be approximated by half of a parabola and we need to evaluate the integration in eq .\\n( [ eq : wkb ] ) in the range @xmath13 only . for s wave , the integral in eq .\\n( [ eq : wkb ] ) is evaluated as , @xmath14 ,   \\\\label{eq : x1x2}\\\\ ] ] with @xmath15 under the parabolic approximation and @xmath16   \\\\nonumber \\\\\\\\   &    & \\\\mbox { }   +   \\\\frac { k a } { \\\\sqrt { \\\\tau - 1 } } \\\\frac{v_0}{e }   \\\\ln [ 1 + e^ { ( r_0 - r_b ) / a } ]   \\\\label{eq : new }   , \\\\end{aligned}\\\\ ] ] where @xmath17 and @xmath18 .\\nthe details of the derivation of eq .\\n( [ eq : new ] ) are given in the appendix .\\nit should be mentioned that in the derivation of eq .\\n( [ eq : new ] ) , a woods - saxon form is used for @xmath19 .\\nin this section , we use the new formula to study the typical barrier penetration problem , @xmath0 decays of atomic nuclei .\\nthe @xmath0 decay half - life is related to the decay width @xmath20 by @xcite @xmath21 the decay width @xmath20 is calculated as  @xcite @xmath22 where @xmath23 is the assaults frequency of @xmath0 particle on the barrier , @xmath24 the spectroscopic or preformation factor and @xmath25 the penetrability with @xmath1 the @xmath0 decay q - value . for spherical nuclei ,\\n@xmath26 is parametrized as  @xcite @xmath27 and the penetrability will be calculated with eqs .\\n( [ eq : x1x2 ] ) , ( [ eq : left ] ) , and ( [ eq : new ] ) .\\n( color online ) the barrier potential between the @xmath0 and the daughter nucleus for @xmath28po and @xmath29nd .\\nthe solid curve shows the exact potential @xmath7 and the dashed curve stands for the effective potential given in eq .\\n( [ eq : veff ] ) associated with the parabolic approximation eq .\\n( [ eq : left ] ) and the new barrier penetration formula eq .\\n( [ eq : new ] ) .\\nnote that the two curves are almost identical to each other . , title=\"fig:\",scaledwidth=45.0% ]   ( color online ) the barrier potential between the @xmath0 and the daughter nucleus for @xmath28po and @xmath29nd .\\nthe solid curve shows the exact potential @xmath7 and the dashed curve stands for the effective potential given in eq .\\n( [ eq : veff ] ) associated with the parabolic approximation eq .\\n( [ eq : left ] ) and the new barrier penetration formula eq .\\n( [ eq : new ] ) .\\nnote that the two curves are almost identical to each other .\\n, title=\"fig:\",scaledwidth=45.0% ]    for the @xmath0-nuclear interaction , we adopt the coulomb and the woods - saxon potentials and parameters proposed in ref .\\n@xcite , @xmath30 , & r \\\\le r_m ,   \\\\end{cases }   \\\\label{eq : coulomb}\\\\ ] ] and @xmath31 }   , \\\\end{aligned}\\\\ ] ] with @xmath32 and @xmath33 the mass and charge numbers of the daughter nucleus and @xmath1 the @xmath0 decay energy . the parameters in these potentials and given in eq .\\n( [ eq : prefomation ] ) were obtained by fitting @xmath0 decay half lives and cross section data for several fusion reactions .\\nit can be easily verified that the position of the coulomb barrier @xmath11 is larger than @xmath34 thus the use of the coulomb force given in eq .\\n( [ eq : coul ] ) is valid .\\nbefore the new formula is used to study alpha decays , we investigate in details its validity .\\nfirst we examine how the effective potential connected with the new formula eq .\\n( [ eq : new ] ) is close to the exact one .\\ntwo extreme examples are chosen for this purpose , @xmath28po which has a very short half - life @xmath35 s and @xmath29nd which has a quite long half - life @xmath36 s  @xcite .\\nthe barrier potential @xmath7 is shown in fig .\\n[ fig : potential ] for these two systems .\\nthe effective potential ,    @xmath37    is also shown for comparison .\\n@xmath38 fm is the radial position outside of which the nuclear part of the @xmath0-nucleus potential could be neglected ( see the appendix for more details ) . in our calculations , the width of the parabolic potential is obtained by fitting the barrier potential from the inner turning point @xmath39 to the position of the barrier @xmath11 . unlike the full parabolic approximation ,\\nthe effective potential is asymmetric and coincides with the exact potential very well , especially the outer side of the barrier which critically influences @xmath0 decays .\\n.[tab : deviation ] comparison of the results for the barrier penetration probability for @xmath0 decays in po isotopes ( charge and mass numbers of the @xmath0 emitter are listed in the first and the second entries ) .\\nthe meaning of @xmath40 is given in eq .\\n( [ eq : x1x2 ] ) .\\nthe superscript `` wkb \\'\\' means the penetrability calculated from the wkb approach , `` para \\'\\' from the parabolic approximation in eq .\\n( [ eq : left ] ) , and `` new \\'\\' from the new formulas eq .\\n( [ eq : new ] ) . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     the deformation influences the @xmath0 decay life time both on the preformation mechanism and on the penetration process  @xcite . in the present work , we have assumed the barrier potential to be spherical . in 68 of these 344 nuclei\\n, the spherical potential assumption is met well ( with @xmath41 for the daughter nucleus  @xcite ) . in table\\n[ tab : spherical ] the calculated and experimental values of the @xmath0 decay half lives for these nuclei are given . the statistical summary is also shown in the last line of table  [ tab : stati ] .\\nit is found that the new formula gives very good results for these spherical nuclei . in most cases , the differences between the calculated and the experimental values of @xmath42 are smaller than 0.5 .\\nthe root mean square deviation between @xmath43 $ ] and @xmath44 $ ] is 0.34 .\\nin the study of barrier penetration in nuclear physics , the parabolic approximation is usually adopted because an analytical solution exists for the penetrability of a parabola barrier potential .\\nthe parabola approximation works indeed well both for the penetrability and for the fusion cross section at energies around or above the coulomb barrier .\\nbut it fails at energies much smaller than the barrier height due to the long - range coulomb interaction .    in the present work\\n, we derived a new barrier penetration formula , eq .\\n( [ eq : new ] ) , based on the wkb approximation .\\nwe took into account the influence of the long coulomb tail in the barrier potential properly .\\ntherefore this formula is especially applicable to the barrier penetration with penetration energy much lower than the coulomb barrier .\\nwe have shown that the present analytical formula reproduces the wkb results very well .\\nthis new penetration formula is used to calculate @xmath0 decay half - lives of 344 nuclei with the @xmath0-nucleus potential given in ref .\\nsatisfactory agreement between the present calculation and the experiment is achieved . for spherical and even - even nuclei ,\\nthe results are particularly good .\\ntherefore , the new formula could be used in the study of barrier penetration at energies much smaller than the barrier height .\\nfurthermore , we expect that the new formula will facilitate the study of the barrier penetrability where one has to introduce an energy - dependent one - dimensional potential barrier or a barrier distribution function .\\nthis work was partly supported by the national natural science foundation ( 10575036 , 10705014 , and 10875157 ) , the major state basic research development program of china ( 2007cb815000 ) , the knowledge innovation project of cas ( kjcx3-syw - n02 and kjcx2-sw - n17 ) , and deutsche forschungsgemeinschaft .\\nthe computation of this work was supported by supercomputing center , cnic , cas .\\nin order to evaluate the integration @xmath45 in eq .\\n( [ eq : x1x2 ] ) , we divide the potential between the position of the barrier @xmath11 and the outer turning point @xmath5 into two parts , @xmath46 and @xmath47 .\\n@xmath48 should be large enough so that the nuclear potential vanishes for @xmath49 . for s\\nwave , @xmath50 it has been verified that when @xmath48 is not very close to @xmath5 , @xmath51 , therefore , @xmath52\\\\ dr   \\\\nonumber \\\\\\\\   &    & \\\\mbox { } +   2 \\\\int^{r _ \\\\mathrm{out } } _ { r_\\\\mathrm{v } }      \\\\sqrt { \\\\frac{2\\\\mu } { \\\\hbar^2 } \\\\left ( v_\\\\mathrm{c}(r ) - e \\\\right ) } \\\\ dr   \\\\nonumber   \\\\end{aligned}\\\\ ] ] @xmath53 since the coulomb potential outside the barrier ( @xmath54 ) is well described by [ c.f .\\n( [ eq : coulomb ] ] , @xmath55 the first term in the above equation can be evaluated easily as , @xmath56   , \\\\end{aligned}\\\\ ] ] with @xmath17 and @xmath18 . for the evaluation of the second term in eq .\\n( [ eq : x1x2_approx ] ) , we adopt a woods - saxon form for the nuclear part of the barrier potential , @xmath57 }   , \\\\end{aligned}\\\\ ] ] and replace @xmath58 in the denominator by @xmath59 , @xmath60   \\\\right\\\\ } \\\\right|_{r_b}^ { r _ { \\\\mathrm{v } } }   \\\\nonumber \\\\\\\\   & \\\\approx &   \\\\frac { k } { \\\\sqrt { \\\\tau - 1 } } \\\\frac{v_0}{e }   \\\\ {   r_0- r_b + a \\\\ln [ 1 + e^ { ( r_b - r_0 ) / a } ]   \\\\ }   \\\\nonumber \\\\\\\\   & = &   \\\\frac { k a } { \\\\sqrt { \\\\tau - 1 } } \\\\frac{v_0}{e }   \\\\ln [ 1 + e^ { ( r_0 - r_b ) / a } ]   .\\\\end{aligned}\\\\ ] ] in the above derivation , we have used the fact that @xmath61 \\\\gg 1 $ ] for @xmath0 decay and penetration well below the coulomb barrier .\\nfinally , we have an analytical expression for @xmath45 , @xmath62   \\\\nonumber \\\\\\\\   &    & \\\\mbox { }   +   \\\\frac { k a } { \\\\sqrt { \\\\tau - 1 } } \\\\frac{v_0}{e }   \\\\ln [ 1 + e^ { ( r_0 - r_b ) / a } ]   .\\\\end{aligned}\\\\ ] ]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['article'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbb3ecad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we study a novel class of numerical integrators , the adapted nested force - gradient schemes , used within the molecular dynamics step of the hybrid monte carlo ( hmc ) algorithm . \\n we test these methods in the schwinger model on the lattice , a well known benchmark problem . \\n we derive the analytical basis of nested force - gradient type methods and demonstrate the advantage of the proposed approach , namely reduced computational costs compared with other numerical integration schemes in hmc . '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['abstract'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c85e1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for the hybrid monte carlo algorithm ( hmc)@xcite , often used to study quantum chromodynamics ( qcd ) on the lattice , one is interested in efficient numerical time integration schemes which are optimal in terms of computational costs per trajectory for a given acceptance rate . high order\\nnumerical methods allow the use of larger step sizes , but demand a larger computational effort per step ; low order schemes do not require such large computational costs per step , but need more steps per trajectory .\\nso there is a need to balance these opposing effects .\\nomelyan integration schemes @xcite of a force - gradient type have proved to be an efficient choice , since it is easy to obtain higher order schemes that demand a small additional computational effort .\\nthese schemes use higher - order information from force - gradient terms to both increase the convergence of the method and decrease the size of the leading error coefficient . other ideas to achieve better efficiency for numerical time integrators are given by multirate or nested approaches .\\nthese schemes do not increase the order but reduce the computational costs per path by recognizing the different dynamical time - scales generated by different parts of the action .\\nslow forces , which are usually expensive to evaluate , need only to be sampled at low frequency while fast forces which are usually cheap to evaluate need to be sampled at a high frequency . a natural way to inherit the advantages from both force - gradient type schemes and multirate approaches would be to combine these two ideas .    previously , we studied the behavior of the adapted nested force - gradient scheme for the example of the @xmath0-body problem @xcite and would like to learn more about their usefulness for lattice field theory calculations . due to the huge computational effort required for qcd simulations ,\\nit is natural to attempt an intermediate step first .\\nwe chose the model problem of quantum electrodynamics ( qed ) in two dimensions , the schwinger model @xcite , since it is well - suited as a test case for new concepts and ideas which can be subsequently applied to more computationally demanding problems @xcite .\\nas a lattice quantum field theory , it has many of the properties of more sophisticated models such as qcd , for example the numerical cost is still dominated by the fermion part of the action . the fact that this model , with far fewer degrees of freedom , does not require such large computational effort makes it the perfect choice for testing purposes .\\nwe compare the behavior of numerical time integration schemes currently used for hmc @xcite with the nested force - gradient integrator @xcite and the adapted version introduced in @xcite .\\nwe investigate the computational costs needed to perform numerical calculations , as well as the effort required to achieve a satisfactory acceptance rate during the hmc evolution .\\nour goal is to find a numerical scheme for the hmc algorithm which would provide a sufficiently high acceptance rate while not drastically increasing the simulation time .\\nthe paper is organized as follows . in section 2\\nwe give a short overview of the hmc algorithm and numerical schemes for time integration , which are used in hmc . in section 3\\nwe present the 2-dimensional schwinger model and introduce the idea of the force - gradient approach and the resulting novel class of numerical schemes .\\nsection 4 is devoted to the results of a comparison between widely used algorithms and the new approach and section 5 draws our conclusion .\\nin this section we provide a general overview of the hmc algorithm @xcite to introduce our novel integrator .\\nwe also present some standard numerical time integrating methods used in hmc , as well state - of - the - art numerical schemes , which we later compare by applying them to the two - dimensional schwinger model .      in the hybrid monte carlo algorithm ,\\nthe quantum lattice field theory is embedded in a higher - dimensional classical system through the introduction of a fictitious ( simulation ) time @xcite .\\nthe gauge field @xmath1 is associated with its ( fictitious ) conjugate momenta @xmath2 , and the classical system is described by the hamiltonian , @xmath3 + { \\\\mathcal{b}}[u],\\\\ ] ] where @xmath4 $ ] and @xmath5 $ ] represent the kinetic and potential energy respectively .    for a given configuration @xmath1 ,\\na new configuration @xmath6 is generated by performing an hmc update @xmath7 , which consists of two steps :    * * molecular dynamics trajectory : * evolve the gauge fields @xmath1 , elements of a lie group , and the momenta @xmath2 , elements of the corresponding lie algebra , in a fictitious computer time @xmath8 according to hamilton s equations of motions @xmath9 since analytical solutions are not available in general , numerical methods must be used to solve the system of eqn .  .\\nthe discrete updates of @xmath1 and @xmath2 with an integration step @xmath10 are @xmath11 leading to a first - order approximation at time @xmath12 .\\nsince the momenta @xmath2 are elements of lie algebra , we have an additive update of @xmath2 .\\non the other hand , the links @xmath1 must be elements of the lie group , therefore an exponential update is used for @xmath1 to preserve the underlying group structure . * * metropolis step : * accept or reject the new configuration @xmath13 with probability @xmath14 where @xmath15 .      in this paper\\nwe are concerned with numerical time integration schemes , which preserve the fundamental properties of geometric integration , time - reversibility and volume - preservation .\\nall numerical schemes presented below possess these necessary properties .\\n* basic schemes : * well - known , commonly used integration schemes in molecular dynamics are given by    * the leap - frog method , a 3-stage composition scheme of the discrete updates defined above : @xmath16 * and a 5-stage extension widely used in qcd computations : @xmath17    * force gradient schemes : * force - gradient schemes increase accuracy by using additional information from the force gradient term @xmath18 , with @xmath19 defining lie brackets . the 5-stage force - gradient scheme proposed by omelyan et\\nal @xcite is the simplest ; @xmath20 here we also test the modification of the force - gradient method proposed in @xcite , where the force - gradient term @xmath21 is approximated via a taylor expansion .\\nan extension is given by the 11-stage decomposition @xcite , recently implemented as the integrator in the open source code openqcd as one of the standard options @xcite\\n@xmath22where @xmath23 , @xmath24 , @xmath25 and @xmath26 are parameters from equation ( 71 ) in ref .\\n@xcite .    *\\nnested schemes : * qed and qcd problems usually lead to hamiltonians with the following fine structure @xmath27 + { \\\\mathcal{b}}_{1}[u]+ { \\\\mathcal{b}}_{2}[u],\\\\ ] ] where the action of the system can be split into two parts : a fast action @xmath28 such as the gauge action , and a slow part @xmath29 , for example , the fermion action .\\nthis allows us to apply the idea of multirate schemes ( an idea known as nested integration in physics literature)@xcite in order to reduce the computational effort . at\\nfirst we consider the nested version of the leap - frog method @xmath30 where the inner cheaper system @xmath31+{\\\\mathcal{b}}_{1}[u]$ ] is solved by @xmath32 with @xmath33 being a number of iterations for the fast part of the action .\\nour main goal is to compare the above - mentioned methods with more elaborated nested schemes : in @xcite , a similar 5-stage decomposition scheme has been recently introduced : @xmath34    a nested version of , which has been used in  @xcite reads @xmath35 where @xmath36 with @xmath37 and @xmath38 . in the limit @xmath39\\nwe have @xmath40 .\\nnote that this approach uses force - gradient information at all levels , i.e. , the high computational cost of high order schemes appears at all levels .\\none may overcome this problem by using schemes of different order at the different levels without losing the effective high order of the overall multirate scheme .\\nfor the latter , we include appropriate force gradient information as we explain in the following for the case of two time levels , where the gauge action plays the role of the fast and cheap part , and the fermionic action plays the role of the slow and expensive part .\\nthe reasoning is as follows : if one uses the 5-stage sexton - weingarten integrator of second order for the slow action , and approximates the fast action by @xmath41 leap - frog steps of step size @xmath42 , the error of the overall multirate scheme will be of order @xmath43 . with the use of force gradient information only at the slowest level it is possible to cancel the leading error term of order @xmath44 .\\nas @xmath45 usually holds in the multirate setting , the overall order is then given by the leading error term of order @xmath46 , i.e. , the scheme has an effective order of four .\\none example for such a scheme for problems of type is given by the 5-stage nested force - gradient scheme introduced in @xcite @xmath47 to summarize , the adapted scheme   differs from the original one   in two perspectives :    * the force gradient scheme for the fast action is replaced by a leap - frog scheme . *\\nonly the part @xmath48 of the full force gradient @xmath49 is needed to gain the effective order of four .\\nthe numerical schemes - and - are second order convergent schemes . methods - and - have the fourth order of convergence .\\nwe do not consider integrators of higher order than four since the computational costs are too high .\\nthe schemes of the same convergence order differ from each other by the number of stages ( updates of momenta and links per time step ) .\\nusually methods with more stages have smaller leading error coefficients and therefore have better accuracy , but higher computational costs .\\nwe would like to determine which integrator would represent the best compromise between high accuracy and computational efficiency\\n.    we will apply all these numerical integration schemes  to the two - dimensional schwinger model .\\nthe most challenging task from the theoretical point of view is to derive the force - gradient term @xmath21 . in the next section\\nwe introduce the schwinger model and explain how to obtain the force - gradient term .\\nthe 2 dimensional schwinger model is defined by the following hamiltonian function @xmath50 = \\\\frac{1}{2}\\\\sum_{n=1,\\\\mu=1}^{v,2 } p_{n,\\\\mu}^2 + s_g[u ] + s_f[u].}\\\\ ] ] with @xmath51 the volume of the lattice . unlike qcd , where @xmath52 and @xmath53 , for this qed problem , the links @xmath1 are the elements of the lie group @xmath54 and the momenta @xmath55 belong to @xmath56 , which represents the lie algebra of the group @xmath54 .\\nthis makes this test example very cheap in terms of the computational time .\\nthis together with the fact that the schwinger model also shares many of the features of qcd simulations , makes the schwinger model an excellent test example when considering numerical integrators : a fast dynamics given by the computationally cheap gauge part @xmath57 $ ] of the action demanding small step sizes , and a slow dynamics given by the computationally expensive fermion part @xmath58 $ ] allowing large step sizes .\\nthe pure gauge part of the action @xmath59 sums up over all plaquettes @xmath60 in the two - dimensional lattice with @xmath61 and is given by @xmath62 the links @xmath1 can be written in the form @xmath63 and connect the sites @xmath0 and @xmath64 on the lattice ; @xmath65 $ ] , @xmath66 , @xmath67 @xmath68 are respectively space and time directions and @xmath69 is a coupling constant .\\nnote that from now on we will set the lattice spacing @xmath70 .\\nthe fermion part of the action @xmath71 is given by @xmath72 where @xmath26 is a complex pseudofermion field . here , @xmath73 denotes the wilson  dirac operator given by @xmath74 where @xmath75 are the pauli matrices @xmath76 @xmath77 is the mass parameter and the kronecker delta @xmath78 acts on the pseudofermion field by @xmath79 with @xmath80 the pseudofermion field , a vector in the two - dimensional spinor space taking values at each lattice point @xmath0 . in order to proceed with the numerical integration we need to obtain the force @xmath81 and the force gradient term @xmath21 .\\nthe force term @xmath82 with respect to the link @xmath83 is given by the first derivative of the action @xmath84 and can be written as @xmath85 since the numerical schemes  use the multi - rate approach , the shifts in the momenta updates are split on @xmath86 and @xmath87 and we can consider them separately .\\nthe force terms @xmath86 and @xmath87 are obtained by differentiation over @xmath54 group elements , which for the schwinger model is the standard differentiation .    the force associated with link @xmath88 from the gauge action\\nis given by @xmath89 the force term of the fermion part is given by @xmath90 \\\\,\\\\ ] ] where vectors @xmath91 and @xmath92 are given @xmath93    for the numerical methods and we need to find the force gradient term @xmath94 with respect to the link @xmath83 . in case of the schwinger model this term reads @xmath95    for simplicity we decompose the force gradient term in four parts @xmath96 this decomposition is also useful since the numerical integrator only uses the term @xmath97 by construction .\\nas shown in @xcite , to obtain the fourth order convergent scheme from the second order convergent method we must eliminate the leading error term , which is exactly represented by @xmath97 . for completeness we discuss all 4 parts below .\\nthe @xmath98 part of the force - gradient term is @xmath99 \\\\end{aligned}\\\\ ] ] with the set of plaquettes @xmath100 then by using the vectors @xmath101 defined in we obtain the @xmath102 piece of the force - gradient term given by @xmath103 . }\\n\\\\end{aligned}\\\\ ] ] the second derivative of the fermion action is @xmath104 \\\\xi + } \\\\nonumber \\\\\\\\ & & { 2 \\\\operatorname{re } \\\\chi^\\\\dagger \\\\frac{\\\\partial d}{\\\\partial q_\\\\mu(n ) } ( d^\\\\dagger d)^{-1 } \\\\frac{\\\\partial d^\\\\dagger}{\\\\partial q_\\\\nu(m ) } \\\\chi \\\\ , , } \\\\nonumber \\\\\\\\ & & { = 2 \\\\operatorname{re } \\\\left [ z_{1,m,\\\\nu}^\\\\dagger \\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } \\\\xi +   \\\\chi^\\\\dagger\\n\\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } d^{-1 } w_{2,m,\\\\nu } -   \\\\chi^\\\\dagger   \\\\frac{\\\\partial^2 d}{\\\\partial q_{\\\\nu}(m ) \\\\partial q_{\\\\mu}(n ) } \\\\xi +   \\\\chi^\\\\dagger\\n\\\\frac{\\\\partial d}{\\\\partial q_{\\\\mu}(n ) } d^{-1 } z_{1,m,\\\\nu}\\\\right ] } \\\\nonumber \\\\\\\\ & & { = 2 \\\\textrm{re } \\\\left [ z_{1,m,\\\\nu}^\\\\dagger w_{2,n,\\\\mu } +   w_{1,n,\\\\mu}^\\\\dagger z_{2,m,\\\\nu } -   \\\\chi^\\\\dagger   \\\\frac{\\\\partial^2 d}{\\\\partial q_{\\\\nu}(m ) \\\\partial q_{\\\\mu}(n ) } \\\\xi \\\\right ] } \\\\end{aligned}\\\\ ] ] in terms of the vectors @xmath105 and @xmath92 defined in .\\nnow the fields @xmath106 and @xmath107 are given by @xmath108 with @xmath109    in order to calculate @xmath110 and @xmath111 it is possible to perform the summation of @xmath112 before the inversions of @xmath73 and @xmath113 to get @xmath114 and @xmath115 which save @xmath116 additional inversions for the force gradient terms .\\nit follows for the force gradient term @xmath111 @xmath117\\\\ ] ] with @xmath118 + z_1 \\\\right ) \\\\ , .\\n\\\\end{aligned}\\\\ ] ] the expression for @xmath110 can be obtained from the one for @xmath111 by replacing in and the vector @xmath119 with @xmath120 defined in .\\nit is important to mention that the computationally most demanding part of the numerical integration of the schwinger model and quantum field theory in general is the inverse of the dirac operator @xmath121 .\\nevery momenta update , which includes fermion action requires 2 inversions of the dirac operator , the addition of the force - gradient term @xmath21 requires 4 more inversions .\\ntherefore leap - frog based methods and need 4 computations of @xmath121 per time step ; schemes and 6 times ; force - gradient based methods 8 for and , 10 for and the 11 stage method has 12 inversions of the dirac operator . since we use the multi - rate approach for schemes , and , which leads generally to fewer macro time steps needed than for the standard schemes we expect the integrator will be the most efficient choice among the methods considered . in the next section we present numerical tests of this prediction .\\nin this section we apply the numerical integrators  to compute the molecular dynamics step for the schwinger model when studied with the hmc algorithm .\\nwe consider a @xmath122 by @xmath122 lattice with a coupling constant @xmath123 and mass @xmath124 .\\nthe parameters were taken from @xcite and correspond to the scaling variable @xmath125 defined in @xcite.we have chosen them to simulate close to the scaling limit with light fermions and also to increase the impact of the fermion part of the action .\\nwe use one thermalised gauge configuration . for each integrator and value of the step - size\\nwe generate @xmath126 independent sets of momenta and integrate the equations of motion on a trajectory of length @xmath127 .\\nwe compute the absolute error @xmath128 and estimate its statistical error from the standard deviation .\\nalso the parameter @xmath33 is chosen in such a way to make micro step size to be @xmath129 times smaller than the macro step size @xmath10 .\\nfigure [ fig:1 ] presents the comparison between the numerical integrators  .\\nit shows the absolute error @xmath128 versus the step - size of the numerical scheme . here\\nthe multi - rate schemes , , and outperform their standard versions as expected .\\nalso it is easy to see that the scheme has the best accuracy and the nested force - gradient method just slightly edges the adapted nested force - gradient scheme .\\nfigure [ fig:2 ] presents the cpu time , required for the proposed integrators \\n, versus the achieved accuracy .\\nwe can observe that the nested force - gradient method and adapted nested force- gradient method show much better results in terms of a computational efficiency than the integrators and ; and even compared to the 11 stage scheme .\\nhere we can see that the modification of proposed in @xcite also performs better than its original version .\\nit shows almost similar computational costs as nested versions of the force - gradient approach - , since it has the same number of @xmath121  ( see table [ tab:1 ] ) .\\nbut it is less efficient because the proposed nested approach is more precise .\\n.step - sizes and number of inversions of @xmath73 per step and per trajectory for acceptance rate of 90% [ cols=\"^,^,^,^,^\",options=\"header \" , ]     table [ tab:1 ] shows the number of inversions of the dirac operator @xmath73 , which is needed to reach 90% acceptance rate of the hmc . since @xmath121 is the most computationally demanding part it is important to see how many of these inversions are required per each trajectory . from table\\n[ tab:1 ] it easy to see that the adapted nested force - gradient method and nested force - gradient method need the least number of @xmath121 per trajectory to reach the chosen acceptance rate @xmath130 .\\nwe can also claim that methods and have a potential to perform even better with respect to the computational effort in the case of lattice qcd problems , since the impact of the fermion action and the computational time to obtain the inversion of the dirac operator @xmath73 is much more significant .\\nwe presented the nested force - gradient approach and its adapted version applied to a model problem in quantum field theory , the two - dimensional schwinger model .\\nthe derivation of the force - gradient terms was given and the schwinger model was introduced .\\nnested force - gradient schemes seem to be an optimal choice with relatively high convergence order and low computational effort .\\nalso it would be possible to improve the algorithm by measuring the poisson brackets of the shadow hamiltonian of the proposed integrator and then tuning the set of optimal parameters , e.  g. micro and macro step sizes .\\n+ in future work we will apply this approach to the hmc algorithm for numerical integration in lattice qcd . here\\nwe expect the adapted nested - force gradient scheme to outperform the original one , if we further partition the action into more than two parts , by using techniques to factorize the fermion determinant : less force - gradient information is needed for the most expensive action , and only leap - frog steps are needed for the high frequency parts of the action .\\nthis work is part of project b5 within the sfb / transregio 55 _ hadronenphysik mit gitter - qcd _ funded by dfg ( deutsche forschungsgemeinschaft ) .\\ns.  duane , a.d .\\nkennedy , b.j .\\npendleton , d.  roweth , hybrid monte carlo , phys .\\nb195 ( 1987 ) , pp .\\ne.  hairer , c.  lubich , g.  wanner , geometric numerical integration : structure - preserving algorithms for ordinary differential equations , springer , berlin , 2002 .\\nomelyan , i.m .\\nmryglod , r.  folk , symplectic analytically integrable decomposition algorithms : classification , derivation , and application to molecular dynamics , quantum and celestial mechanics , comput .\\n151 ( 2003 ) , pp .'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['article'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05158860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' new methods for obtaining functional equations for feynman integrals are presented . \\n application of these methods for finding functional equations for various one- and two- loop integrals described in detail . \\n it is shown that with the aid of functional equations feynman integrals in general kinematics can be expressed in terms of simpler integrals .    \\n pacs numbers : 02.30.gp , 02.30.ks , 12.20.ds , 12.38.bx + keywords : feynman integrals , functional equations     +    derivation of functional equations for feynman integrals + from algebraic relations   +    * o.v .  \\n tarasov * +   ii . \\n institut fr theoretische physik , universitt hamburg , + luruper chaussee 149 , 22761 hamburg , germany + and + joint institute for nuclear research , + 141980 dubna , russian federation + : otarasov@jinr.ru + '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['abstract'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa9ce082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"recently it was discovered that feynman integrals obey functional equations @xcite , @xcite .\\ndifferent examples of functional equations were presented in refs .\\n@xcite , @xcite,@xcite . in these articles\\nonly one - loop integrals were considered .    in the present paper\\nwe propose essentially new methods for deriving functional equations .\\nthese methods are based on algebraic relations between propagators and they are suitable for deriving functional equations for multi - loop integrals . also these methods can be used to derive functional equations for integrals with some propagators raised to non - integer powers .\\nour paper is organized as follows . in sec .\\n2 . the method proposed in ref .  @xcite is shortly reviewed .    in sec .\\n3 . a method for finding algebraic relations between products of propagators is formulated .\\nwe describe in detail derivation of explicit relations for products of two , three and four propagators .\\nalso algebraic relation for products of arbitrary number of proparators is given .\\nthese relations are used in sec.4 . to obtain functional equations for some one- , as well as two- loop integrals .\\nin particular functional equation for the massless one - loop vertex type integral is presented . also functional equation for the two - loop vertex type integral with arbitrary masses\\nis given .    in sec .\\nanother method for obtaining functional equations is proposed .\\nthe method is based on finding algebraic relations for ` deformed propagators ' and further conversion of integrals with ` deformed propagators ' to usual feynman integrals by imposing conditions on deformation parameters . to perform such a conversion the @xmath0- parametric representation for both types of integrals\\nis exploited .\\nthe method was used to derive functional equation for the two - loop vacuum type integral with arbitrary masses . as a by product , from this functional equation we obtained new hypergeometric representation for the one - loop massless vertex integral .    in conclusion\\nwe formulate our vision of the future applications and developments of the proposed methods .\\nthe method for deriving functional equations proposed in ref .\\n@xcite is based on the use different kind of recurrence relations . in particular in refs .\\n@xcite , @xcite , @xcite , generalized recurrence relations @xcite were utilized to obtain functional equations for one - loop feynman integrals . in general such recurrence relations\\nconnect a combination of some number of integrals @xmath1 corresponding to diagrams , say , with @xmath2 lines and integrals corresponding to diagrams with fewer number of lines . diagrams with fewer number of lines can be obtained by contracting some lines in integrals with @xmath2 lines .\\nintegrals corresponding to such diagrams depend on fewer number of kinematical variables and masses compared to integrals with @xmath2 lines .\\nsuch recurrence relations can be written in the following form : @xmath3 where @xmath4 and @xmath5 are ratios of polynomials depending on masses @xmath6 , scalar products @xmath7 of external momenta , powers of propagators @xmath8 and parameter of the space time dimension @xmath9 . at the left hand - side of eq .\\n( [ nconnectr ] ) we combined integrals with @xmath2 lines and on the right hand - side integrals with fewer number of lines .    in accordance with the method of ref .\\n@xcite , to obtain functional equation from eq .\\n( [ nconnectr ] ) one should eliminate terms on the left hand - side by defining some kinematical variables from the set of equations : @xmath10 if there is a nontrivial solution of this system and for this solution some @xmath11 are different from zero then the right - hand side of eq .\\n( [ nconnectr ] ) will represent functional equation .\\nfor the one - loop integrals with @xmath2 propagators @xmath12 where @xmath13 different types of recurrence relations were given in refs .\\n@xcite , @xcite .\\ndiagram corresponding to this integral is given in figure 1 .\\nexternal legs ]    in refs .\\n@xcite , @xcite the following relation was derived : @xmath14 where the operators @xmath15 shift index of propagators by one unit @xmath16 , @xmath17\\n@xmath18 @xmath19 here @xmath20 are external momenta going through lines @xmath21 respectively , and @xmath22 is mass attributed to @xmath23-th line .\\ngram determinant @xmath24 and modified cayley determinant @xmath25 are polynomials depending on scalar products and masses .\\nit is assumed that these scalar products are made of @xmath9 dimensional vectors and @xmath24 and @xmath25 are not subject to any restriction or condition specific to some integer values of @xmath9 .\\n( [ reducedtod ] ) is written in the form corresponding to eq .\\n( [ nconnectr ] ) . to eliminate integrals with @xmath2 lines on the left hand - side of eq .\\n( [ reducedtod ] ) the following conditions to be hold : @xmath26    eq .\\n( [ reducedtod ] ) is valid for arbitrary kinematical variables and masses .\\nsolution of eqs .\\n( [ sharik ] ) can be easily done with respect to two kinematical variables or masses .\\nstarting from @xmath27 substitution of such solutions into eq .\\n( [ reducedtod ] ) gives nontrivial functional equations .\\nthe method for obtaining functional equations by eliminating complicated integrals from recurrence relations is quite general one .\\nhowever for multi loop integrals , depending on several kinematical variables , derivation of equations like eq .\\n( [ reducedtod ] ) is computationally challenging . in the next sections we will describe easier and more powerful methods that can be used for deriving functional equations for multi - loop integrals .\\nsetting @xmath28 in eq .\\n( [ reducedtod ] ) and imposing conditions ( [ sharik ] ) leads to the following equation : @xmath29 in eq .\\n( [ fe_n_points ] ) integrands of @xmath30 are products of @xmath31 propagators depending on different external momenta , i.e. each term in this relation corresponds to the same function but with different arguments .\\nin fact functional equations considered in refs .\\n@xcite are of the same form as eq .\\n( [ fe_n_points ] ) .\\nthe question naturally arises : this relationship holds for integrals or it can be obtained as the consequence of a relationship between integrands ?    by inspecting eq .\\n( [ fe_n_points ] ) , one can suggest the following form of the relation between products of propagators of integrands : @xmath32 where @xmath33 in what follows we will omit @xmath34 term assuming that all masses have such a correction .\\nadditionally we assume that vectors @xmath35 are linearly dependent , i.e. the gram determinant for the set of vectors @xmath36 is equal to zero .\\nsuch a condition is valid for all examples considered in refs .\\n@xcite , @xcite .\\nnow let s consider in detail implementation of our prescription for products of 2,3 and 4 propagators . at @xmath37 relation ( [ usual_props ] )\\nreads : @xmath38 where @xmath39 according to our assumption three vectors @xmath40,@xmath41,@xmath42 are linearly dependent . without loss of generality we may assume that @xmath43 furthermore , we assume that @xmath44 will be integration momentum and scalar quantities @xmath45,@xmath46 , @xmath47 , @xmath47 do not depend on @xmath44 .\\nputting all terms in eq .\\n( [ 2prop_relation ] ) over a common denominator and then equating to zero the coefficients in front of various products of @xmath48 , @xmath49,@xmath50 yields the following system of equations : @xmath51 solution of this system of equations is : @xmath52 where @xmath53 is a root of the equation @xmath54 with @xmath55 this solution can be rewritten in an explicit form : @xmath56 where @xmath57    now let s find algebraic relation for the products of three propagators . at @xmath27 eq .\\n( [ usual_props ] ) reads : @xmath58 where @xmath59 , @xmath60 , @xmath61 are defined in eq.([p1p2p3 ] ) and @xmath62 in complete analogy with the previous case we can represent one momentum as a combination of other ones . without loss of generality we may write @xmath63 where @xmath64 for the time being are arbitrary coefficients . putting all terms in eq .\\n( [ 3prop_relation ] ) over a common denominator and then equating to zero the coefficients in front of various products of @xmath65 , @xmath66 , @xmath67 , @xmath68 yields the following system of equations : @xmath69 solving these equations for @xmath45 , @xmath46 , @xmath70 , @xmath71 , @xmath72 we have @xmath73 where @xmath74 is solution of the equation @xmath75 here @xmath76    let us now turn to the derivation of algebraic relation for the product of four propagators . at @xmath77\\n( [ usual_props ] ) reads : @xmath78 where @xmath59 , @xmath60 , @xmath61,@xmath79 are defined in eqs .\\n( [ p1p2p3 ] ) , ( [ d4 ] ) , @xmath80 and @xmath81 is a linear combination of vectors @xmath40,  ,@xmath82 , @xmath83 putting all terms in eq .\\n( [ 4prop_relation ] ) over a common denominator and then equating to zero the coefficients in front of different products of @xmath48 , @xmath84 yields system of equations : @xmath85 solving this system for @xmath45 , @xmath46 , @xmath70,@xmath86 , @xmath87 , @xmath88 we have @xmath89 where @xmath90 is a solution of the equation @xmath91 with @xmath92    eqs .\\n( [ 3prop_relation ] ) , ( [ 3prop_relation ] ) and ( [ 4prop_relation ] ) will be used in the next sections to derive functional equations for the propagator , vertex and box type of integrals .\\nrelations between products of five and more propagators can be easily derived in the same way as as it was done for products of two- , three- and four- propagators . from eq .\\n( [ usual_props ] ) one can derive system of equations and find its solution for arbitrary @xmath2 .\\nmultiplying both sides of eq .\\n( [ usual_props ] ) by the product of @xmath93 propagators @xmath94 yields @xmath95 or @xmath96 since we assume linear dependence of vectors @xmath97 , without loss of generality we may write : @xmath98 substituting ( [ pnp1 ] ) into eq.([ini_equ ] ) , collecting terms in front of @xmath48 , @xmath84 and terms without @xmath44 , equating them to zero after some simplifications yields the following system of @xmath99 equations : @xmath100 solving eq .\\n( [ sumy ] ) for one of the @xmath64 an substituting this solution into eq .\\n( [ kwadraticy ] ) gives quadratic equation for the remaining @xmath64 .\\nthis quadratic equation can be solved with respect to one of the parameters @xmath64 .\\nthus the solution of the system of equations ( [ xequs ] ) , ( [ sumy ] ) , ( [ kwadraticy ] ) will depend on @xmath101 arbitrary parameters @xmath64 and one arbitrary mass @xmath102 .\\nit is interesting to note that for any @xmath2 , functional equations for integrals with all masses equal to zero and functional equations for integrals with all masses equal are the same . in case of equal masses , two mass dependent terms in eq .\\n( [ kwadraticy ] ) cancel each other due to eq .\\n( [ sumy ] ) . in both cases systems of equations for @xmath103 ,\\n@xmath104 are the same and therefore arguments of integrals are the same .\\n( [ 2prop_relation ] ) is analogous to the equation for splitting propagators presented in ref .\\n( [ 3prop_relation ] ) is a generalization of eq .\\n( [ 2prop_relation ] ) . indeed , setting @xmath105 , canceling common factor @xmath61 on both sides of eq .\\n( [ 3prop_relation ] ) yields relation similar to ( [ 2prop_relation ] ) . in turn\\n( [ 4prop_relation ] ) is a generalization of ( [ 3prop_relation ] ) .\\nmultiplying algebraic relations ( [ 2prop_relation]),([3prop_relation ] ) , ( [ 4prop_relation ] ) by products of any number of propagators raised to arbitrary powers @xmath106 @xmath107^{\\\\nu_j}}\\\\ ] ] and integrating with respect to @xmath44 we get a functional equation for one - loop integrals .\\n( [ 2prop_relation ] ) , ( [ 3prop_relation ] ) , ( [ 4prop_relation ] ) also can be used to derive functional equations for integrals with any number of loops .\\nmultiplying algebraic relations for propagators by function corresponding to feynman integral depending on momentum @xmath44 and any number of external momenta and then integrating with respect to @xmath44 will produce functional equations .\\njust for demonstrational purposes we present graphically in figure 2 functional equation based on @xmath2 propagator relation .\\n- propagator functional equation ]    the blob on this picture correspond to either product of propagators raised to arbitrary powers or to an integral with any number of loops and external legs .\\none of the external momenta of this multi loop integral should be @xmath44 .\\nin this section several particular examples of functional equations resulting from algebraic relations for products of propagators will be considered .\\nfirst , we consider the simplest case , namely , functional equation for the integral @xmath108 : @xmath109           [ ( k_1-p_k)^2-m_k^2]}.\\\\ ] ] integrating both sides of eq .\\n( [ 2prop_relation ] ) with respect to @xmath44 , we get : @xmath110 the arguments @xmath111 , @xmath112 of integrals on the right hand - side depend on @xmath113 , @xmath47 @xmath114 substituting solution for @xmath64 from eq .\\n( [ y_for_2prop ] ) into eq .\\n( [ p13_p23 ] ) yields : @xmath115 in this equation @xmath116 is an arbitrary parameter and can be taken at will . functional equation ( [ prop_fe ] )\\nis in agreement with the result presented in refs .\\n@xcite,@xcite .\\nfunctional equations for the vertex type integral @xmath117           [ ( k_1-p_2)^2-m_2 ^ 2 ]       [ ( k_1-p_3)^2-m_3 ^ 2 ]       } , \\\\label{i3definition }      \\\\end{aligned}\\\\ ] ] can be obtained from eq .\\n( [ 2prop_relation ] ) as well as from eq .\\n( [ 3prop_relation ] ) .\\nmultiplying eq .\\n( [ 2prop_relation ] ) with the factor @xmath118 where @xmath119 and integrating over @xmath44 leads to the equation : @xmath120 this equation in terms of integrals @xmath121 reads @xmath122 two more functional equations can be obtained from eq .\\n( [ fe_for_vertex ] ) by symmetric permutations @xmath123 and @xmath124 .\\nanother functional equation for the vertex type integral can be obtained by integrating eq .\\n( [ 3prop_relation ] ) with respect to @xmath44 : @xmath125 where @xmath126 there is an essential difference between functional equation eq .\\n( [ fei3massiv ] ) obtained from eq .\\n( [ 2prop_relation ] ) and functional equation ( [ fei3massive ] ) derived from eq .\\n( [ 3prop_relation ] ) .\\nfor example , at @xmath127 , eq .  ( [ fei3massiv ] ) becomes trivial while from eq .\\n( [ fei3massive ] ) for the integral @xmath128 we obtain nontrivial functional equation : @xmath129 where @xmath74 is a root of the quadratic equation @xmath130    if one argument of @xmath131 is zero then by applying functional equation ( [ fe_triangle ] ) such an integral can be expressed in terms of integrals @xmath121 with two arguments equal to zero .\\nfor example , at @xmath132 and @xmath133 the relation ( [ fe_triangle ] ) becomes : @xmath134 this is a typical example how functional equations can be used to simplify evaluation of an integral by reducing it to a combination of integrals with fewer number of arguments .    at @xmath135 , similar to the previous case , eq.([fei3massiv ] ) degenerate while from eq.([fei3massive ] ) for the integral @xmath136 we obtain nontrivial functional equation : @xmath137 where @xmath74 is a root of the quadratic equation @xmath138 eqs .\\n( [ fe_triangle_eqm ] ) , ( [ l3_eqm ] ) are identical to eqs .\\n( [ fe_triangle]),([lambda3_zero_masses ] ) respectively and therefore functional equation for the integral with massless propagators and functional equation for the integral with all masses equal are the same .\\n( [ fe_triangle_eqm ] ) at @xmath132 and @xmath133 leads to the relation similar to ( [ 1zero_2zeros ] ) : @xmath139 this is not surprising because coefficients of the eq .\\n( [ fe_triangle_eqm ] ) are mass independent and in the integrand @xmath140 and @xmath34 appear in the covariant combination @xmath141 .\\nfor this reason the similarity of functional equations for massless integrals and integrals with all masses equal take place for integrals with more external legs and more loops .\\nfunctional equations for the box type integrals can be obtained by multiplying relation ( [ 2prop_relation ] ) by two propagators , or by multiplying relation ( [ 3prop_relation ] ) by one propagator and then integrating over momentum @xmath44 . yet\\nanother relation can be obtained just by integrating eq .\\n( [ 4prop_relation ] ) over momentum @xmath44 : @xmath142 here @xmath90 is defined in eq .\\n( [ lambda4 ] ) and @xmath143,@xmath144 , @xmath145 are arbitrary parameters and @xmath146 arbitrary parameters in this functional equation can be chosen from the requirement of simplicity of evaluation of integrals on the right hand - side of eq .\\n( [ box_func_equ ] ) or from some other requirements .\\nfor example , one can choose these parameters by transforming arguments to a certain kinematical region needed for analytic continuation of the original integral .      the method described in the previous section can be applied to multi loop integrals .\\nconsider , for example , integral corresponding to the diagram given in figure 3 .\\n]    if we multiply eq .\\n( [ 2prop_relation ] ) by the one - loop integral depending on @xmath44 @xmath147 [ ( k_1-k_2)^2-m_5 ^ 2]}\\\\ ] ] and integrate with respect to momentum @xmath44\\nthen we obtain functional equation @xmath148 where @xmath149[(k_2-q_2)^2-m_2 ^ 2 ] [ k_1 ^ 2-m_3 ^ 2][(k_1-k_2)^2-m_4 ^ 2 ] } , \\\\label{rdefinition}\\\\end{aligned}\\\\ ] ] @xmath150 integrals of this type arise , for example , in calculations of two - loop radiative corrections in the electroweak theory . instead of the integral @xmath151 one can consider derivative of @xmath152 with respect to @xmath116 which is uv finite : @xmath153[(k_2-q_2)^2-m_2 ^ 2 ] [ k_1 ^ 2-m_3 ^\\n2]^2[(k_1-k_2)^2-m_4 ^ 2]}. \\\\label{r3definition}\\\\end{aligned}\\\\ ] ] integral @xmath154 satisfy the following functional equations : @xmath155 this relation can be used for computing basis integral arising in calculation of two - loop radiative correction to the ortho -positronium lifetime . in particular one of these basis integrals corresponds to kinematics @xmath135 , @xmath156 , @xmath157 . in this case relation ( [ fe3 ] ) reads @xmath158 integral on the right hand - side is in fact propagator type integral with one massless line . applying recurrence relations given in ref .\\n@xcite this integral can be reduced to simpler integral : @xmath159 ^ 2[(k_1+q_1)^2-m^2 ] } \\\\nonumber \\\\\\\\ & & = \\\\frac{2}{3(d-3 ) } j_{111}^{(d-2)}(m^2),\\\\end{aligned}\\\\ ] ] where @xmath160 [ ( k_2-q)^2-m^2]}.\\\\ ] ] at @xmath161 , the result for @xmath162 is known @xcite : @xmath163 } } + ( d-3){\\\\,{}_3f_2}{{\\\\!\\\\!\\\\left[\\\\begin{array}{c}1,\\\\frac{4-d}{2},\\\\frac{d-1}{2}\\\\,;\\\\\\\\#2\\\\,;\\\\end{array}1\\\\right]}},\\\\end{aligned}\\\\ ] ] and it can be used for the @xmath164 expansion of @xmath152 and @xmath154 . as was already mentioned at @xmath156 , @xmath157 integrals on the right hand - side of eq.([fe_for_r ] ) correspond to propagator type integrals .\\nanalytic result for @xmath152 reads @xmath165,\\\\end{aligned}\\\\ ] ] where @xmath166 we checked that several first terms in the @xmath167 expansion of @xmath152 and @xmath154 are in agreement with results of @xcite .\\nthe main profit from functional equations for @xmath152 and @xmath154 comes from the fact that vertex integrals were expressed in terms of simpler , propagator type integrals .\\nthe method described in the previous section does not work for deriving functional equations for all kinds of feynman integrals .\\nfor example , we did not found functional equation for the two - loop vacuum type integral given in figure 4 .        in this section\\nwe shall describe another method that extends the class of integrals for which we can obtain functional equations .\\nthe method is based on transformation of functional equations for some auxiliary integrals depending on arbitrary parameters into functional equations for integrals of interest .\\nsuch functional equations will be derived from algebraic relations for ` deformed propagators ' which will be defined in the next section .\\nthese auxiliary integrals will be transformed into @xmath0 parametric representation . in general characteristic polynomials of these integrals in @xmath0 parametric representation\\ndiffer from those for the investigated integral . functional equation for the integral of interest\\ncan be obtained in case when it will be possible to map characteristic polynomials of auxiliary integrals with ` deformed propagators ' to characteristic polynomials of this integral .\\nsuch a mapping will be performed by rescaling @xmath0 parameters and appropriate choice of arbitrary ` deforming parameters ' .      in the previous section to derive functional equation we added to our consideration a propagator with combination of external momenta taken with arbitrary scalar coefficient .\\nnow we consider generalization of this method .    to find functional equation for @xmath168-loop feynman integral depending on @xmath169- external momenta we start from the relation of the form @xmath170 where @xmath171 is defined as : @xmath172 with @xmath173 and @xmath174 , @xmath175 for the time being are arbitrary scalar parameters .\\nsome of these parameters as well as @xmath176 will be fixed from the equation ( [ x_parameters ] ) .\\nanother part of these parameters will be fixed from the requirement that the product of propagators in ( [ x_parameters ] ) should correspond to the integrand of the integral with the considered topology .\\nwe would like to remark that instead of deformation of propagators proposed in eqs .\\n( [ deformed_prop]),([deformed_momentum ] ) one can use other deformations .\\nfor example , all terms in denominators of propagators can be taken with arbitrary scalar coefficients : @xmath177    to establish algebraic relation ( [ x_parameters ] ) we put all terms over a common denominator and then equate coefficients in front scalar products depending on integration momenta .\\nsolving obtained system of equations gives some restrictions on the scalar parameters .\\nin general integrals obtained by integrating products of ` deformed propagators ' will not correspond to usual feynman integrals .\\nfurther restrictions on parameters should be imposed in order to obtain relations between integrals corresponding to feynman integrals coming from a realistic quantum field theory models .\\nas an example , let us consider derivation of functional equation for the two - loop vacuum type integral given in figure 4 : @xmath178 analytic expression for this integral was presented in ref @xcite . instead of this integral\\nwe will first consider an auxiliary integral with integrand made from ` deformed propagators ' defined in eqs.([deformed_prop ] ) , ( [ deformed_momentum ] ) : @xmath179 where @xmath180 for the product of three deformed propagators one can try to find an algebraic relation of the form : @xmath181 where @xmath182 , @xmath183,@xmath184 are defined in eq.([d123_for2loop_bubble ] ) and @xmath185 here @xmath186 are arbitrary masses , @xmath187 , @xmath188 , @xmath189 , @xmath190 , @xmath191 are undetermined parameters and @xmath44 , @xmath192 will be integration momenta\\n.      we would like to notice that eq .\\n( [ equ_with_redefined_masses2 ] ) is valid for integrals but not for their integrands .\\nthis is due to the fact that the factor in front of integral that comes from the scaling of @xmath0 parameters in parametric integral is not fully compensated by scaling momenta given in eq .\\n( [ scaling_momenta ] ) .    at @xmath246\\nthe dependence on all parameters @xmath247,@xmath248,@xmath249 in eqs .\\n( [ equ_with_redefined_masses_mm4nz ] ) , ( [ equ_with_redefined_masses2 ] ) drops out and the integral @xmath250 reduces to a comination of simpler integrals : @xmath251    analytic expression for the integral @xmath201 with one mass equal to zero is known @xcite . under assumption that @xmath252 it reads @xmath253}}. \\\\label{j0_hgf}\\\\ ] ] from functional equation ( [ j0_mm4_zero ] ) as a by - product\\none can get a new hypergeometric representation for the one - loop massless vertex type integral . in ref .\\n@xcite an interesting relation between the dimensionally regularized one - loop vertex type integral @xmath254 and the two - dimensional integral @xmath255 was discovered @xmath256 functional equation ( [ j0_mm4_zero ] ) with @xmath257 defined in eq .\\n( [ j0_hgf ] ) provide us a new hypergeometric representation for the integral @xmath121 with massless propagators .\\nformula for the one - loop massless vertex integral in terms of other gauss hypergeometric functions is given in ref .\\nfinally , we summarize what we have accomplished in this paper .\\nfirst of all , we formulated new methods for deriving functional equations for feynman integrals .\\nthese methods are rather simple and do not use any kind of integration by parts techniques .\\nsecond , it was shown that integrals with many kinematic arguments can be reduced to a combination of simpler integrals with fewer arguments . in our future publications\\nwe are going to demonstrate that in some cases applying functional equations one can reduce , the so - called , master integrals to a combination of simpler integrals from , what we would like to call , a ` universal ' basis of integrals .\\nthe method based on algebraic relations for ` deformed propagators ' can be used not only for vacuum type of integrals but also for integrals depending on external momenta . in the present paper we considered rather particular cases of functional equations .\\nthe systematic investigation and classification of the proposed functional equations requires application of the methods of algebraic geometry and group theory .    at the present moment\\nit is not quite clear whether functional equations derivable from recurrence relations can be reproduced by the methods of algebraic relations between products of propagators described in section 3 and section 5 .\\na detailed consideration of our functional equations and their application to the one - loop integrals with four , five and six external legs as well as to some two- and three- loop feynman integrals will be presented in future publications .\\nthis work was supported by the german science foundation ( dfg ) within the collaborative research center 676 _ particle , strings and the early universe : the structure of matter and space - time_. i am thankful to o.l .\\nveretin for providing results for integrals contributing to ortho - positronium lifetime described in ref.@xcite .\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv['test']['article'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a39ec",
   "metadata": {},
   "source": [
    "#### Sample summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "923bc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s>For about 20 years the problem of properties of short - term changes of solar activity has been considered extensively. Several periodicities were detected, but the periodicities about 155 days and from the interval of @xmath3 $ ] days are mentioned most often. The periodicity between 150160 days is statistically significant during all cycles from 16 to 21. The power of this periodicity started growing at cycle 19, decreased in cycles 20 and 21 and disappered after cycle 21.</s>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73e9a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s>It is believed that the direct detection of gravitational waves ( gws) will bring the era of gravitational wave astronomy. Pulsar timing arrays ( ptas) can be used as a detector for gws. The main target of ptas is the stochastic gravitational wave background ( sgwb)</s>'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d35f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s>The tunneling through a potential barrier plays a very important role in the microscopic world and has been studied extensively since the birth of quantum mechanics. For most of the potential barriers, the penetrability can not be calculated analytically. In the present work, we derived a new barrier penetration formula based on the wkb approximation. We apply this new formula to evaluate the half - lives of atomic nuclei.</s>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a1d8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s>for the hybrid monte carlo algorithm ( hmc)@xcite, often used to study quantum chromodynamics ( qcd ) on the lattice, one is interested in efficient numerical time integration schemes. high ordernumerical methods allow the use of larger step sizes, but demand a larger computational effort per step. low order schemes do not require such large computational costs per step, but need more steps per trajectory. A natural way to inherit the advantages from force - gradient type schemes and multirate approaches would be to combine these two ideas.</s>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21abdabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s>recently it was discovered that feynman integrals obey functional equations. In the present paper we propose essentially new methods for derivingfunctional equations. These methods are based on algebraic relations between propagators. They can be used to derive functional equations for integrals with some propagators raised to non - integer powers. We formulate our vision of the future applications of the proposed methods.</s>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a2ff1",
   "metadata": {},
   "source": [
    "### BART fine-tune on Arxiv scientific dataset after 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "855f522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "model.load_state_dict(torch.load(f'bart_split_{15}.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c5419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "predicts, labels = [], arxiv['test']['abstract'][:500]\n",
    "\n",
    "for batch in DataLoader(test_dataset, batch_size=1, drop_last=True, num_workers=CPU_COUNT):\n",
    "    predicts.append(\n",
    "        tokenizer.decode(\n",
    "            model.generate(batch['input_ids'].to(device).view(1, -1)).squeeze(0)\n",
    "        )\n",
    "    )\n",
    "    if len(predicts) == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ade8b0",
   "metadata": {},
   "source": [
    "#### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7fabc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.37150027317634876,\n",
       "  'p': 0.4994501188160528,\n",
       "  'r': 0.3140735726099545},\n",
       " 'rouge-2': {'f': 0.13533945533327682,\n",
       "  'p': 0.18288610584670895,\n",
       "  'r': 0.11403439901011984},\n",
       " 'rouge-l': {'f': 0.31408308485193576,\n",
       "  'p': 0.3838079853883263,\n",
       "  'r': 0.27784543247788374}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(predicts, labels, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f235d2",
   "metadata": {},
   "source": [
    "#### Sample summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba8659fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s><s> the problem of the existence of a short - term periodicity in the sunspot data from cycle 16. \\n the daily sunspot area, the mean sunspot areas per carrington rotation, the monthly sunspot numbers and their fluctuations are obtained after removing the 11-year cycle. the properties of the power spectrum methods \\n are analysed and the periodicity is found to be statistically significant during all cycles from 16 to 21.    _ \\n keywords _ : solar activity, sunspot number, solar wind plasma, interplanetary magnetic field </s>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f80aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s> we investigate the detectability of circular polarization in the stochastic gravitational wave background ( sgwb ) generated by a large number of unresolved sources with the astrophysical origin or the cosmological origin in the early universe by pulsar timing arrays ( ptas ). \\n we characterize the stokes parameters for monochromatic plane gravitational waves and calculate generalized overlap reduction functions ( orfs ) so that we can probe the circular polarization of the sgWB.    in this paper \\n, we formulate the cross - correlation formalism for anisotropic circularly polarized sgswb with ptas. the basic framework is essentially a combination of the formalism of @xc</s>'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de3d322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s> we derive a new barrier penetration formula based on the wkb approximation. \\n the influence of the long - range coulomb tail in the barrier potential is taken into accout properly. as a first attempt and a test study \\n, we apply this new formula to evaluate @xmath0 decay half - lives of atomic nuclei and show that the present analytical formula reproduces the experimental results very well. </s>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "807f8806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s> the hybrid monte carlo algorithm ( hmc ) is often used to study quantum chromodynamics ( qcd ) on the lattice. \\n we present a novel class of numerical time integration schemes for the hmc algorithm, based on the idea of force - gradient integrators, which use higher - order information from force gradients to both increase the convergence of the method and decrease the size of the leading error coefficient. in this paper \\n, we study the computational costs needed to perform numerical calculations, as well as the effort required to achieve a satisfactory acceptance rate during the evolution of hmc. </s>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc2540cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s><s> we propose essentially new methods for deriving functional equations for multi - loop integrals. \\n these methods are based on algebraic relations between products of propagators and they can be used to derive functional equation for integrals with some propagators raised to non - integer powers. in particular functional equation is presented for the massless one - loop vertex type integral with arbitrary masses. as a by product, from this functional equation \\n we obtained new hypergeometric representation for the one-loop massless vertex integral. </s>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce1a00",
   "metadata": {},
   "source": [
    "### Pegasus fine-tune on Arxiv scientific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c4bcaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/spiece.model HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/special_tokens_map.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/pegasus-arxiv/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "model_name = 'google/pegasus-arxiv'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03bf17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts, labels = [], arxiv['test']['abstract'][:500]\n",
    "\n",
    "articles = tokenizer.batch_encode_plus(arxiv['test']['article'][:500], max_length=1024, return_tensors='pt', truncation=True, padding=\"longest\").to(device)\n",
    "for batch in articles['input_ids']:\n",
    "    predicts.append(\n",
    "        tokenizer.decode(\n",
    "            model.generate(batch.to(device).view(1, -1)).squeeze(0)\n",
    "        )\n",
    "    )\n",
    "    if len(predicts) == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05a219",
   "metadata": {},
   "source": [
    "#### ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d65c09fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.30449826577267997,\n",
       "  'p': 0.5238095238095238,\n",
       "  'r': 0.2146341463414634},\n",
       " 'rouge-2': {'f': 0.11149825372846596,\n",
       "  'p': 0.1927710843373494,\n",
       "  'r': 0.0784313725490196},\n",
       " 'rouge-l': {'f': 0.324675319919042,\n",
       "  'p': 0.4166666666666667,\n",
       "  'r': 0.26595744680851063}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(predict, label, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965a9c6",
   "metadata": {},
   "source": [
    "#### Sample summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a0da6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the daily sunspot areas, the mean sunspot areas per carrington rotation, the monthly sunspot numbers and their fluctuations, which are obtained after removing the 11-year cycle, are analysed. <n> a new approach to the problem of aliases is proposed. <n> the power spectrum method is applied for the analysis of the sunspot data. <n> it is shown that the sunspot data from cycle 16 present a short - term periodicity with the period of @xmath0 days ( @xmath1 $ ] years ). <n> this periodicity presents around the maximum activity period in cycles 16 to 21 and disappered after cycle 21. <n> the period of this periodicity is @xmath2 days for the mean sunspot areas per carrington rotation, @xmath2 days for the monthly sunspot numbers and @xmath2 days for the fluctuations of the sunspot numbers. <n> [ firstpage ] sun : activity sunspot : fluctuations'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688253a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we investigate the detectability of circular polarization in the stochastic gravitational wave background ( sgwb ) by pulsar timing arrays ( ptas ). <n> we characterize sgwb by the so called stokes parameters and calculate generalized overlap reduction functions ( orfs ) so that we can probe the circular polarization of the sgwb. <n> we also discuss a method to separate the intensity ( @xmath0 mode ) and circular polarization ( @xmath1 mode ) of the sgwb.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "824d14b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a new analytical formula for the barrier penetrability is derived based on the wkb approximation. <n> the influence of the long coulomb tail in the barrier potential is taken into account properly. <n> this formula is especially applicable to the barrier penetration with penetration energy much lower than the coulomb barrier. as a first attempt and a test study, we apply this new formula to evaluate @xmath0 decay half - lives of atomic nuclei. <n> we show that the present analytical formula reproduces the experimental results very well, especially for spherical nuclei.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4abb3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we introduce a new numerical time integration scheme for the hybrid monte carlo algorithm ( hmc ), which combines the advantages of force - gradient type integrators and multirate approaches. <n> the new scheme is tested on the two - dimensional schwinger model and is found to provide a sufficiently high acceptance rate while not significantly increasing the simulation time.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db527c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new methods for deriving functional equations for feynman integrals are proposed. <n> these methods are based on algebraic relations between propagators and they are suitable for deriving functional equations for multi - loop integrals. <n> also these methods can be used to derive functional equations for integrals with some propagators raised to non - integer powers.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75130e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this paper we study in details the problem of sensitivity loss due to discretization of parameters and to the needs to limit the computing cost, with hough procedures. in particular, we propose and study the characteristics of a frequency hough procedure, designed mainly to reduce the discretization problem, and we compare it with the sky hough procedure, which is actually used in the virgo collaboration.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9427042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this review focuses specifically on what we have learned about the progenitors of core - collapse supernovae ( cc sne ) by examining images of the supernova ( sn ) sites taken prior to the explosion. by registering pre - sn and post - sn images, usually taken at high resolution using either space - based optical detectors, or ground - based infrared detectors equipped with laser guide star adaptive optics systems ( lgs - ao ), about one dozen cc sne have now been directly detected ( i.e., shown to be spatially coincident with the sn ) in pre - sn images, with roughly two dozen upper limits derived from non - detections. <n> one example from each of the following three categories of progenitor studies is provided : ( 1 ) no progenitor star detected in pre - sn image(s ) ; ( 2 ) likely progenitor star identified via spatial coincidence in pre - sn image(s ) ; ( 3 ) progenitor star detected in pre - sn image(s ) and subsequently confirmed by demonstrating its absence in images taken after the sn has faded beyond detection. <n> a summary of overall results to date for each sn type is then given'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "128e5713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'single - transverse spin asymmetries ( ssas ) play a fundamental role for our understanding of qcd in high - energy hadronic scattering. <n> they may be obtained for reactions in, for example, lepton - proton or proton - proton scattering with one transversely polarized initial proton, by dividing the difference of the cross sections for the two settings of the transverse polarization by their sum. <n> a crucial feature is that the distribution functions and the soft factor in this factorization are not integrated over the transverse momenta of partons, because these in fact generate the observed transverse momentum @xmath0. <n> a particularly interesting feature is that the sivers effect is not universal in the usual sense, that is, it is not represented by universal probability functions convoluted with partonic hard - scattering cross sections. <n> however, the non - universality has in fact a clear physical origin, and its closer investigation has turned out to be an extremely important and productive development in qcd. in a nutshell, in order not to be forced to vanish because of the time - reversal symmetry of qcd, single - spin asymmetries require the presence of a strong -'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c984d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the kingman coalescent is a random tree with infinitely many leaves arising in large population genetic models. at time @xmath0 it comes down from infinitely many lines to @xmath1 lines. <n> it is known that for large @xmath1 a randomly chosen @xmath2 is approximately exponentially distributed with mean @xmath3. in this paper <n> we prove a large deviation principle for the distributions of @xmath4. as a byproduct <n> we derive a large deviation principle for the distributions of @xmath5, where @xmath6 is the time to the most recent common ancestor of the infinite population of leaves.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d39a575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the main purpose of this paper is to overview several different physical examples of multi - mode and/or multi - frequency solitary waves that occur for the pulse or beam propagation in nonlinear optical fibers and waveguides. for these purposes, we select three different cases : multi - wavelength solitary waves in bit - parallel - wavelength optical fiber links, multi - colour spatial solitons due to multistep cascading in optical waveguides with quadratic nonlinearities, and quasiperiodic solitons in the fibonacci superlattices. <n> we believe these examples display both the diversity and richness of the multi - mode soliton systems, and they will allow further progress to be made in the study of nonlinear waves in multi - component nonintegrable physical models.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60207257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
