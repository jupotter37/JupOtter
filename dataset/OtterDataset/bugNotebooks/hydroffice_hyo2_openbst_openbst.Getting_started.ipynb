{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.hydroffice.org/openbst/\"><img src=\"images/openbst.png\" alt=\"XSF\" title=\"Open OpenBST home page\" align=\"center\" width=\"12%\" alt=\"XSF logo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>OpenBST: An interactive and open library for Backscatter Processing </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OpenBST:*** The <strong>Open</strong>  <strong>B</strong>ack <strong>S</strong>catter <strong>T</strong> oolchain project is an open and collaborative effort that provides an interactive environment to explore backscatter data and processing algorithms. The project is designed as a virtual town square where developers, researchers and users can come to discuss the latest state of the art in backscatter processing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenBST uses Jupyter Notebooks as the interactive front end. There are two types of cells used in the notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Cells:\n",
    "This is a markdown cell. It contains texual information and directions for interacting with the notebook and project. An empty markdown cell is shown below. Type something and then run the cell. You can view the markup text buy pressing `enter`\n",
    "\n",
    "*hint: to run a cell, press `shift + enter`. This will run the cell and select the next cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Cells:\n",
    "Code cells let you run python commands. The real advantage of the Notebooks is that as you move through the processing workflow, you are able to interact with the public API and test various functionalities and methodologies. Below is a code cell with a bit of code. Feel free to test out other python commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola Mundo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hola Mundo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is plenty more about Jupyter Notebooks to learn and they really are a neat way of programming. But for this project you now know all you need to know about Notebooks. Now lets get Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can now start to interact with a standard notebook. Each section interacts with the public API that runs in the background. The only cell that needs to be run first is the \"Setup the Notebook Environment\". After that you're free to skip cells. This is because we don't want to impose an order of operations to backscatter processing unless everyone agrees there should be one. Of course, there are sections that may need information from other sections. If that's the case, the code will tell you what you need to run.\n",
    "\n",
    "Have at it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you must execute the cell below. This code sets up the namespace of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from hyo2.openbst.lib.openbst import OpenBST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an OpenBST Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we need to do is create a project. Interacting with the project lets you add files, test a processing algorithm, and visualize the results. \n",
    "\n",
    "By default, project files are placed in the local AppData directory. When creating a new project make sure to use a unique name. Each notebook will have a suggested project name, but you are the boss so make it your own.\n",
    "\n",
    "If you want to continue with an old project, enter the old project title to open that project where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Path: C:\\Users\\msmith\\AppData\\Local\\HydrOffice\\OpenBST\\projects\\getting_started.openbst\n",
      "Project Version: 0.1.1\n",
      "Last Modified: 2019-11-20 21:32:29.400157 UTC\n"
     ]
    }
   ],
   "source": [
    "project_title = \"getting_started\"\n",
    "bst = OpenBST(prj_name=project_title).prj\n",
    "print(\"Project Path: \" + str(bst.info.project_path))\n",
    "print(\"Project Version: \" + str(bst.info.version))\n",
    "print(\"Last Modified: \" + str(bst.info.modified) + \" UTC\")\n",
    "bst.open_project_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above should have opened the your directory `prj_name.openbst`. This is where all the project files are stored. \n",
    "Take a moment look over the directory structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Directories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking at your project structure you should see three directories. Below is a short description of what gets stored in each of these:\n",
    "\n",
    "\n",
    "* **Raws** - When a new raw file (.all, .s7k, etc.) is added to the project, a copy is made and stored at a netCDF file\n",
    "\n",
    "\n",
    "* **Processing** - For each data file, a netCDF file is created to store the results of a processing step and the processing metadata\n",
    "\n",
    "\n",
    "* **Products** - This folder contains netCDF files which store and recall the data visualizations generated at the processing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF Files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the main data storage contained mentioned above is a netCDF file. Additionally you should be able to see the `info.nc` file in the project directory. `info.nc` is a netcdf file used to maintain information about the project such as what files have been added or removed, processed, etc.\n",
    "\n",
    "netCDF files are well equipped to handle sonar data, and the CF convention allows for easy metadata coupling. Additionally, the CF convention is well known and data exploration is possible with other computing environments (c++, python, Matlab) and existing softwares (Arc, QGIS). This is a major benefit if you want to take the processing results and inspect it with your prefered method.\n",
    "\n",
    "A free an convenient application to explore the data is NASA's [Panoply](https://www.giss.nasa.gov/tools/panoply/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the widget below to add files to the project. You can add sonar files at any time. However, adding files in the middle of processing requires all steps up to that point. You can add as many files as you like.\n",
    "\n",
    "As the title of the notebooks says, this notebook is focused on the T50-P. We will yell at you if load data from different systems. So do not try anything funny!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets are not in the MVP at the moment. But will be added as time permits. For now the cell below will suffice for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Testing Path: C:\\PythonCode\\hyo2_openbst\n",
      "Raw File Path: C:\\PythonCode\\hyo2_openbst\\data\\download\\reson\\20190730_144835.s7k\n"
     ]
    }
   ],
   "source": [
    "from hyo2.abc.lib.testing_paths import TestingPaths\n",
    "testing = TestingPaths(root_folder=Path().cwd().parents[2].resolve())\n",
    "print(\"Root Testing Path: \" + str(testing.root_folder))\n",
    "raw_path = testing.download_data_folder().joinpath('reson','20190730_144835.s7k')\n",
    "print(\"Raw File Path: \" + str(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "added = bst.add_raw(raw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Project Health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing, the project checks to make sure that it is in a healthy state. A healthy project is a happy project.  \n",
    "This means checking that all the raw files have been imported into the project, there are no broken files or anything missing.\n",
    "\n",
    "\n",
    "After running this section you should be able to explore the raw data file created in the raws folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.check_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now walk through the processing workflow. Each subsection will conduct some processing action that is part of the backscatter processing toolchain. At each step, a number of methodologies can be utilized and compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open Data File and Map the datagrams\n",
    "infile = prr.x7kRead(str(raw_input))\n",
    "infile.mapfile(verbose=False)\n",
    "\n",
    "# Extract data from the datafile\n",
    "# - Get the sonar setting data\n",
    "dg_type = '7000'\n",
    "nr_dg_runtime = len(infile.map.packdir[dg_type])\n",
    "# 39 is the number of fields in the runtime datagram\n",
    "data_runtime = np.empty((nr_dg_runtime, 39), dtype=float)\n",
    "for i in range(nr_dg_runtime):\n",
    "    try:\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        # this cast is dangerous\n",
    "        data_runtime[i, :] = np.asarray(dg.header, dtype=float)\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error while reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# - Get the receiver beam widths\n",
    "dg_type = '7004'\n",
    "nr_dg_beamgeo = len(infile.map.packdir[dg_type])\n",
    "# -- determine how many rx beams system was set to. Assume constant through dg\n",
    "dg = infile.getrecord(dg_type, 1)\n",
    "nr_rx_beams = dg.header[1]\n",
    "# -- reading all the 4 fields in the data section\n",
    "data_beamgeo = np.empty((nr_dg_beamgeo, nr_rx_beams, 4))\n",
    "for i in range(nr_dg_beamgeo):\n",
    "    try:\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        data_beamgeo[i, :, :] = dg.data.transpose()\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error while reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# - Get the sonar bathy/Intensity data\n",
    "dg_type = '7027'\n",
    "nr_pings = len(infile.map.packdir[dg_type])\n",
    "\n",
    "data_bathy = np.empty((nr_pings, nr_rx_beams, 7))  # 7 because of selecting some of the fields\n",
    "data_bathy[:] = np.nan\n",
    "time_bathy = np.empty(nr_pings)\n",
    "time_bathy[:] = np.nan\n",
    "for i in range(nr_pings):\n",
    "    try:\n",
    "        time_bathy[i] = infile.map.packdir[dg_type][i, 1]\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        beam_indices = dg.data[:, 0].astype(int)\n",
    "        data_bathy[i, beam_indices, 1:4] = dg.data[:, [1, 2, 6]]\n",
    "        # change to avoid expensive matrix multiplication\n",
    "        data_bathy[i, :, 0] = np.tile(np.array(dg.header[1], copy=True), nr_rx_beams)\n",
    "        data_bathy[i, :, 4] = np.tile(np.array(dg.header[7], copy=True), nr_rx_beams)\n",
    "        data_bathy[i, :, 5] = np.tile(np.array(dg.header[8], copy=True), nr_rx_beams)\n",
    "        data_bathy[i, :, 6] = np.tile(np.array(dg.header[9], copy=True), nr_rx_beams)\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error while reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# - Get the TVG\n",
    "dg_type = '7010'\n",
    "nr_dg_tvg = len(infile.map.packdir[dg_type])\n",
    "data_tvg = list()\n",
    "for i in range(nr_dg_tvg):\n",
    "    try:\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        data_tvg.append(dg.data)\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# - Get the position data\n",
    "dg_type = '1003'\n",
    "nr_dg_position = len(infile.map.packdir[dg_type])\n",
    "time_position = np.empty(nr_dg_position)\n",
    "position_measured = np.empty((nr_dg_position, 3))  # 3 because we read just 3 of the fields\n",
    "for i in range(nr_dg_position):\n",
    "    try:\n",
    "        time_position[i] = infile.map.packdir[dg_type][i, 1]\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        if dg.header[0] != 0:\n",
    "            logger.debug(\"Warning: Datum is not WGS84\")\n",
    "        lat_rad, lon_rad = dg.header[2:4]\n",
    "        height_datum = dg.header[4]\n",
    "        lat_degree = np.rad2deg(lat_rad)\n",
    "        lon_degree = np.rad2deg(lon_rad)\n",
    "        position_measured[i, :] = [lat_degree, lon_degree, height_datum]\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error reading %s #%d\" % (dg_type, i))\n",
    "# -- Interpolate the position to the ping time\n",
    "lat_interp_object = interp_sp.interp1d(time_position, position_measured[:, 0], fill_value='extrapolate')\n",
    "lat_ping = lat_interp_object(time_bathy)\n",
    "\n",
    "lon_interp_object = interp_sp.interp1d(time_position, position_measured[:, 1], fill_value='extrapolate')\n",
    "lon_ping = lon_interp_object(time_bathy)\n",
    "\n",
    "# height_interp_object = interp_sp.interp1d(time_position, position_measured[:, 2], fill_value='extrapolate')\n",
    "# height_ping = height_interp_object(time_bathy)\n",
    "\n",
    "# data_position = np.array([lat_ping, lon_ping, height_ping])\n",
    "# data_position = data_position.transpose()\n",
    "\n",
    "# - Get the attitude data\n",
    "# -- Get the roll, pitch, heave / Values are in radians\n",
    "dg_type = '1012'\n",
    "nr_dg_rph = len(infile.map.packdir[dg_type])\n",
    "time_rph = np.empty(nr_dg_rph)\n",
    "rph_measured = np.empty((nr_dg_rph, 3))\n",
    "for i in range(nr_dg_rph):\n",
    "    try:\n",
    "        time_rph[i] = infile.map.packdir[dg_type][i, 1]\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        rph_measured[i, :] = dg.header\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# -- Get the heading\n",
    "dg_type = '1013'\n",
    "nr_dg_heading = len(infile.map.packdir[dg_type])\n",
    "time_heading = np.empty(nr_dg_heading)\n",
    "heading_measured = np.empty(nr_dg_heading)\n",
    "for i in range(nr_dg_heading):\n",
    "    try:\n",
    "        time_heading[i] = infile.map.packdir[dg_type][i, 1]\n",
    "        dg = infile.getrecord(dg_type, i)\n",
    "        heading_measured[i] = np.array(dg.header)\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"Error reading %s #%d\" % (dg_type, i))\n",
    "\n",
    "# --Interpolate the attitude to the ping times\n",
    "# roll_interp_object = interp_sp.interp1d(time_rph, rph_measured[:, 0], fill_value='extrapolate')\n",
    "# roll_ping = roll_interp_object(time_bathy)\n",
    "#\n",
    "# pitch_interp_object = interp_sp.interp1d(time_rph, rph_measured[:, 1], fill_value='extrapolate')\n",
    "# pitch_ping = pitch_interp_object(time_bathy)\n",
    "#\n",
    "# heave_interp_object = interp_sp.interp1d(time_rph, rph_measured[:, 2], fill_value='extrapolate')\n",
    "# heave_ping = heave_interp_object(time_bathy)\n",
    "\n",
    "# TODO: Determine if extrapolate is good or bad\n",
    "heading_interp_object = interp_sp.interp1d(time_heading, heading_measured[:], fill_value='extrapolate')\n",
    "heading_ping = heading_interp_object(time_bathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Processing Workflow\n",
    "    # - Convert digital value to dB\n",
    "    digital_value_db = 20 * np.log10(data_bathy[:, :, 3])\n",
    "\n",
    "    # -- Plot the per beam reflectivity\n",
    "    frequency = data_runtime[0, 3]\n",
    "    title_str = \"Initiial raw Relfectivity\\n Reson T50-P @ %d kHz\" % (frequency / 1000)\n",
    "    fig_raw = data_plt.plot_ping_beam(digital_value_db, title=title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # - Calculate the estimated slant range\n",
    "    bottom_detect_sample = data_bathy[:, :, 1]\n",
    "    surface_sound_speed = data_runtime[:, 36]\n",
    "    sample_rate = data_bathy[:, :, 4]\n",
    "    range_m = bottom_detect_sample / sample_rate * surface_sound_speed[:, np.newaxis] / 2\n",
    "\n",
    "    # -- Plot the range and check it's within reason\n",
    "    cmap = 'gist_rainbow'\n",
    "    title_str = \"Preliminary Slant Range\"\n",
    "    clabel_str = \"Range [m]\"\n",
    "    fig_range = data_plt.plot_ping_beam(range_m, colormap=cmap, title=title_str, clabel=clabel_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Remove the static gain\n",
    "    rx_fixed_gain = data_runtime[:, 15]\n",
    "    datacorr_fixed_gain = digital_value_db - rx_fixed_gain[:, np.newaxis]\n",
    "\n",
    "    # -- Plot the adjusted gain\n",
    "    title_str = \"Static Gain [%.1f dB] Correction Product\\nReson T50-P @ %d kHz\" % (rx_fixed_gain[0], frequency/1000)\n",
    "    fig_fixedgain = data_plt.plot_ping_beam(datacorr_fixed_gain, title=title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Remove the TVG\n",
    "    tvg_gain = np.empty((nr_dg_tvg, nr_rx_beams))\n",
    "\n",
    "    for i in range(nr_dg_tvg):  # TODO: Make something better than a nested for loop\n",
    "        for j in range(nr_rx_beams):\n",
    "            if np.isnan(bottom_detect_sample[i, j]):\n",
    "                tvg_gain[i, j] = np.nan\n",
    "            else:\n",
    "                # TODO: verify rounding logic\n",
    "                tvg_index = int(np.round(bottom_detect_sample[i, j]))\n",
    "                tvg_curve = data_tvg[i]\n",
    "                tvg_gain[i, j] = tvg_curve[tvg_index]\n",
    "\n",
    "    alpha = data_runtime[:, 35] / 1000\n",
    "    spreading = data_runtime[:, 37]\n",
    "    tvg_bswg = (spreading[:, np.newaxis] * np.log10(range_m)) + 2 * (alpha[:, np.newaxis] * range_m)\n",
    "    datacorr_tvg_gain = digital_value_db - tvg_gain\n",
    "\n",
    "    # -- Plot a comparison between the calculated and estimated tvg values\n",
    "    mplt.figure()\n",
    "    mplt.plot(np.nanmean(tvg_gain, axis=0))\n",
    "    mplt.plot(np.nanmean(tvg_bswg, axis=0))\n",
    "    mplt.grid()\n",
    "    mplt.title(\n",
    "        \"Comparison between RESON TVG and BSWG TVG\\nSpreading = %d dB / Absorption = %0.2f dB/m\"\n",
    "        % (spreading[0], alpha[0]))\n",
    "    mplt.xlabel(\"Beam [#]\")\n",
    "    mplt.ylabel(\"Average TVG value [dB}\")\n",
    "    mplt.legend([\"Reson TVG\", \"BSWG TVG\"])\n",
    "\n",
    "    # -- Plot the corrected values using the reson tvg\n",
    "    title_str = \"TVG Correction Product\\nReson T50-P @ %dHz\" % (frequency/1000)\n",
    "    fig_tvgcorr = data_plt.plot_ping_beam(datacorr_tvg_gain, title=title_str)\n",
    "    mplt.imshow(datacorr_tvg_gain, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Correct for the Source Level\n",
    "source_level = data_runtime[:, 14]\n",
    "datacorr_sourcelevel = datacorr_tvg_gain - source_level[:, np.newaxis]\n",
    "\n",
    "# -- Plot the corrected values\n",
    "title_str = \"Source Level Correction Product\\nReson T50-P @ %dkHz\" % (frequency/1000)\n",
    "fig_sourcelevel = data_plt.plot_ping_beam(datacorr_sourcelevel, title=title_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Apply Relative Calibration Correction\n",
    "# -- Load calibration curve\n",
    "calibration_data = np.genfromtxt(fname=calib_input, dtype=float, delimiter=',')\n",
    "\n",
    "# -- Generate a 4th Order Fit to the curve data\n",
    "poly_coefficents = np.polyfit(calibration_data[:, 0], calibration_data[:, 1], 4)\n",
    "calibration_curve = np.arange(-75, 75, 0.1)[:, np.newaxis]\n",
    "calibration_curve = np.tile(calibration_curve, [1, 2])\n",
    "calibration_curve[:, 1] = np.polyval(poly_coefficents, calibration_curve[:, 0])\n",
    "\n",
    "# -- Obtain Calibration Correction for each beam angle of each ping\n",
    "rx_angle = data_bathy[:, :, 2]\n",
    "calibration_correction = np.empty((nr_pings, nr_rx_beams))\n",
    "for i in range(nr_pings):\n",
    "    calibration_correction[i, :] = np.interp(np.rad2deg(rx_angle[i, :]),\n",
    "                                             calibration_curve[:, 0], calibration_curve[:, 1])\n",
    "# -- Apply calibration values\n",
    "datacorr_echolevel = datacorr_sourcelevel - calibration_correction\n",
    "\n",
    "# -- Plot the corrected values\n",
    "title_str = \"Echo Level Product\\nReson T50-P @ %dkHz\" % (frequency/1000)\n",
    "clabel_str = \"Intensity Value [dB re 1$mu$Pa]\"\n",
    "fig_echo = data_plt.plot_ping_beam(datacorr_echolevel, title=title_str, clabel=clabel_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Correct for the Transmission Loss\n",
    "# TODO: current assumption is spherical spreading\n",
    "transmission_loss = 40 * np.log10(range_m) + 2 * (alpha[:, np.newaxis] / 1000) * range_m\n",
    "datacorr_transmissionloss = datacorr_sourcelevel + transmission_loss\n",
    "\n",
    "# -- Plot the corrected values\n",
    "title_str = \"Transmission Loss Correction Product\\nReson T50-P @ %dkHz\" % (frequency/1000)\n",
    "clabel_str = \"Intensity Value [dB re 1$mu$Pa]\"\n",
    "fig_transmission = data_plt.plot_ping_beam(datacorr_transmissionloss, title=title_str, clabel=clabel_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Correct for Ensonified Area\n",
    "    # -- Calculate the Ensonified area using a flat seafloor assumption\n",
    "    beamwidth_rx_across = data_beamgeo[:, :, 3]\n",
    "    beamwidth_tx_along = data_runtime[:, 20]\n",
    "    pulse_length = data_runtime[:, 6]\n",
    "    ind_zero = np.where(rx_angle == 0)\n",
    "    rx_angle[ind_zero[0], ind_zero[1]] = np.deg2rad(0.1)  # Nudge 0 values\n",
    "    area_beamlimited = beamwidth_rx_across * beamwidth_tx_along[:,\n",
    "                                             np.newaxis] * range_m ** 2  # TODO: Verify that the 7004 dg is accounting for the cosine of the angle\n",
    "    area_pulselimited = ((surface_sound_speed[:, np.newaxis] * pulse_length[:, np.newaxis]) / (\n",
    "            2 * np.sin(np.abs(rx_angle)))) \\\n",
    "                        * (beamwidth_tx_along[:, np.newaxis] * range_m)\n",
    "\n",
    "    # TODO: justify why taking the minimum\n",
    "    area_correction = 10 * np.log10(np.minimum(area_beamlimited, area_pulselimited))\n",
    "\n",
    "    # -- Correct the data\n",
    "    datacorr_radiometric = datacorr_transmissionloss - area_correction\n",
    "\n",
    "    # -- Plot area regions\n",
    "    fig_areacorr = mplt.figure()\n",
    "    mplt.plot(10 * np.log10(np.nanmean(area_beamlimited, axis=0)))\n",
    "    mplt.plot(10 * np.log10(np.nanmean(area_pulselimited, axis=0)))\n",
    "    mplt.plot(10 * np.log10(np.nanmean(np.minimum(area_pulselimited, area_beamlimited), axis=0)))\n",
    "    mplt.grid(which='minor')\n",
    "    mplt.xlabel(\"Beam [#]\")\n",
    "    mplt.ylabel(\"Area Correction [dB]\")\n",
    "    mplt.title(\"Comparison of Area Corrections\")\n",
    "\n",
    "    # Plot the area corrected data\n",
    "    title_str = \"Seafloor Backscatter Product\\nReson T50-P @ %dkHz\" % (frequency/1000)\n",
    "    clabel_str = r'$S(\\theta)_b$  [dB re $1\\mu$Pa]'\n",
    "    fig_radiometric = data_plt.plot_ping_beam(datacorr_radiometric, title=title_str, clabel=clabel_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
