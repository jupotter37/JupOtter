{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessors\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables\n",
    "\n",
    "## Maximum processes for multip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum nb of process for the multiprocessing Pool.\n",
    "MAX_PROCESSES = 17 # 17 ok if you're not fuzzing around with other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_DOCS_PATH = \"../../data/languages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init spacy and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sp = None\n",
    "\n",
    "def get_spacy_model(language):\n",
    "    global sp\n",
    "    if sp is not None:\n",
    "        return sp\n",
    "    if language == \"fr\":\n",
    "        sp = spacy.load(\"fr_core_news_sm\")\n",
    "    elif language == \"en\":\n",
    "        sp = spacy.load('en_core_web_sm')\n",
    "    else:\n",
    "        print(\"Unknown spacy language %s\" % language)\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors pipeline\n",
    "\n",
    "### Pipeblock\n",
    "\n",
    "FIXME : pipeline multip def\n",
    "\n",
    "* Generalize for all dataset a pipeline of processes.\n",
    "* Easier to multiprocess.\n",
    "\n",
    "=> Base \"abstract\" classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod #abstract base class\n",
    "\n",
    "class PipeBlock():\n",
    "    @abstractmethod\n",
    "    def __init__():\n",
    "        ...\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __call__(self, doc):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def _biased_vocab_wrapper(doc_url, biased_vocab, pipeline):\n",
    "    for name, transform in pipeline.items():\n",
    "        if name == \"vectorizer\":\n",
    "            break\n",
    "        for tag, voc in biased_vocab.items():\n",
    "            biased_vocab[tag] = transform(voc)\n",
    "    return (doc_url, biased_vocab)\n",
    "\n",
    "def _doc_wrapper(doc_url, doc, pipeline):\n",
    "    for name, transform in pipeline.items():\n",
    "        doc = transform(doc)\n",
    "        # Remove document that are too short.\n",
    "        if (name == \"tokenizer\" or name == \"cleaner\") and len(doc) < transform.min_doc_len:\n",
    "            return (doc_url, None)\n",
    "    return (doc_url, doc)\n",
    "\n",
    "def run_pipeline(corpus, wrapper, pipeline=None, verbose=0):\n",
    "    \"\"\"\n",
    "    Loop over the dataset.\n",
    "    On each document, apply the transformation of each block in pipeline (in order).\n",
    "    \n",
    "    :param corpus:   Dictionary mapping a doc_key to its content (array of strings, 1 string = 1 sentence)\n",
    "    :param wrapper:  The wrapper is a function to adapt the behavior of the block to the dataset shape/structure.\n",
    "    :param pipeline: If none, u'r looking for troubles my friend.\n",
    "                     An array of actions to perform on a document of the corpus.\n",
    "                     Must follow the PipeBlock implementation.\n",
    "    \"\"\"\n",
    "    with Pool(MAX_PROCESSES) as ps:\n",
    "        corpus_res = ps.starmap(wrapper, [(doc_url, doc_sents, pipeline) for doc_url, doc_sents in corpus.items()])\n",
    "        corpus_res = list(filter(lambda x : x[1] != None, corpus_res))\n",
    "        corpus = dict(corpus_res)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def multip_for(func, it_dict, args):\n",
    "    \"\"\"\n",
    "    Apply a function on a dictionary.\n",
    "    \n",
    "    :param func:     Action to perform on the dictionary items. (key and/or value)/\n",
    "    :param it_dict:  Dictionary on wich to perform the action.\n",
    "    :param args:     Arguments to pass to the function in addition to the dictionary item.\n",
    "    \n",
    "    :return:    A new dictionary.\n",
    "    \"\"\"\n",
    "    with Pool(MAX_PROCESSES) as ps:\n",
    "        res = ps.starmap(func, [(key, value, *args) for key, value in it_dict.items()])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalisation\n",
    "\n",
    "Define interface inorder to multiprocess the full pipeline.\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_cleaner(sentence) : \n",
    "    #Method used to cleand a sentence of all diacritics all characters likes these.\n",
    "    return re.sub(r\"\\s+\",\" \", \n",
    "                  re.sub(r\"[^a-zA-Z0-9]\",\" \",\n",
    "                  unicodedata.normalize('NFKD', sentence).encode('ASCII', 'ignore').decode(\"utf-8\")\n",
    "                )\n",
    "             ).lower().strip()\n",
    "\n",
    "def tokenizer_cleaner(doc, language='french') :\n",
    "    # Method to create cleaned sentences with nltk\n",
    "    sentences = sent_tokenize(doc, language=language)\n",
    "    cleaned = []\n",
    "    for sen in sentences:\n",
    "        cleaned_sen = generic_cleaner(sen)\n",
    "        if len(cleaned_sen.split()) > 1:\n",
    "            cleaned.append(cleaned_sen)\n",
    "    return cleaned\n",
    "\n",
    "def brutal_tokenizer(doc, n) :\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    toks = generic_cleaner(doc).split(\" \")\n",
    "    sentences = [\" \".join(toks[x*n:x*n+n]) for x in range(len(toks)//n)]\n",
    "    return sentences\n",
    "\n",
    "def overlap_tokenizer(doc, block_size, over_window):\n",
    "    #Create sentences by cutting the document in portions of n words\n",
    "    toks = generic_cleaner(doc).split(\" \")\n",
    "    sentences = []\n",
    "    if len(toks) >= block_size :\n",
    "        sentences = [\" \".join(toks[x*over_window:x*over_window+block_size+1])\n",
    "                     for x in range( (len(toks)-block_size)//over_window+1)]\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "def spacy_tokenizer(doc, language):\n",
    "    \"\"\"\n",
    "    Wrapper around the spacy tokenizer.\n",
    "    Adapts it to the corpus dictionay structure.\n",
    "    \"\"\"\n",
    "    sp = get_spacy_model(language)\n",
    "    toks = sp(doc)\n",
    "    #cleaned_doc = [generic_cleaner(sent.string) for sent in toks.sents]\n",
    "    #cleaned_doc = [sent for sent in cleaned_doc if len(sent) > 1]\n",
    "    #tok_doc = cleaned_doc\n",
    "    #tok_doc = [sent.string for sent in toks.sents]\n",
    "    tok_doc = toks\n",
    "    return tok_doc\n",
    "\n",
    "class Tokenizer(PipeBlock):\n",
    "    def __init__(self, language, method, len_sen=10, over=4, min_doc_len=0):\n",
    "        \"\"\"\n",
    "        Init a document tokenizer.\n",
    "\n",
    "        :param method:      String. Reference a tokenize method.\n",
    "                            'nltk'    ->\n",
    "                            'brutal'  ->\n",
    "                            'overlap' ->\n",
    "                            'spacy'   ->\n",
    "\n",
    "        :param len_sen:     int. Number of words in a sentence.\n",
    "                            Used by the 'brutal' and 'overlap' tokenizer.\n",
    "\n",
    "        :param over:        ???\n",
    "                            Someting used by the 'overlap' tokenizer.\n",
    "\n",
    "        :param min_doc_len: Int. Minimum of sentence a document must containe.\n",
    "    \n",
    "\n",
    "        :return:    Tokenized document. If once tokenized, nb of sentence <\n",
    "                    min_doc_len, return None.\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.language = language\n",
    "        self.len_sen = len_sen\n",
    "        self.over = over\n",
    "        self.min_doc_len = min_doc_len\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"\n",
    "        Tokenize documents.\n",
    "        \"\"\"\n",
    "        #Sentence Tokenization of the corpus\n",
    "        if self.method == 'nltk':\n",
    "            tokenized_doc = tokenizer_cleaner(doc)\n",
    "        elif self.method == 'brutal':\n",
    "            tokenized_doc = brutal_tokenizer(doc, self.len_sen)\n",
    "        elif self.method == 'overlap':\n",
    "            tokenized_doc = overlap_tokenizer(doc, self.len_sen, self.over)\n",
    "        elif self.method == 'spacy':\n",
    "            tokenized_doc = spacy_tokenizer(doc, self.language)\n",
    "        else :\n",
    "            print(\"Tokenizer method not accepted: %s\" % self.method)\n",
    "            exit(1)\n",
    "        return tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveStopWords():\n",
    "    def __init__(self, language, method):\n",
    "        self.method = method\n",
    "        if method == 'spacy':\n",
    "            sp = get_spacy_model(language)\n",
    "            spacy_model = spacy.lang.fr if language == \"french\" else spacy.lang.en\n",
    "            self.stop_words = spacy_model.stop_words.STOP_WORDS\n",
    "        else:\n",
    "            self.stop_words = nltk.corpus.stopwords.words(language)\n",
    "        assert(self.stop_words is not None)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        if self.method == \"spacy\":\n",
    "            doc = self.spacy_stop_w(doc)\n",
    "        else:\n",
    "            doc = self.nltk_stop_w(doc)\n",
    "        return doc\n",
    "        \n",
    "    def spacy_stop_w(self, doc):\n",
    "        #print(self.stop_words)\n",
    "        doc_res = []\n",
    "        for sent in doc:\n",
    "            sent_tmp = [w.string for w in sent if not w.is_stop]\n",
    "            doc_res.append(\" \".join(sent_tmp))\n",
    "        return doc_res\n",
    "    \n",
    "    def nltk_stop_w(self, doc):\n",
    "        doc_res = []\n",
    "        for sent in doc:\n",
    "            sent_tmp = [w for w in sent.split() if w not in self.stop_words]\n",
    "            doc_res.append(\" \".join(sent_tmp))\n",
    "        return doc_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(language, gen, docs, gold_sum_dict, biased_vocab=None):    \n",
    "    \"\"\"\n",
    "    Apply preprocessing to text.\n",
    "    \"\"\"\n",
    "    logging.debug(\"[Preprocessors][PREP CORPUS] Preprocessing pipeline\")\n",
    "    \n",
    "    # Preprocess the document\n",
    "    logging.debug(\"[Preprocessors][PREP CORPUS] docs\")\n",
    "    normalisation_pipeline = {\n",
    "        \"tokenizer\": Tokenizer(language, *gen, min_doc_len=3)\n",
    "    }\n",
    "    \n",
    "    docs = run_pipeline(docs, _doc_wrapper, normalisation_pipeline)\n",
    "    \n",
    "    \"\"\"\n",
    "    Biased vocabulary preprocessing\n",
    "    if biased_vocab is not None:\n",
    "        logging.debug(\"[Preprocessors][PREP CORPUS] Biased vocab\")    \n",
    "        biased_vocab = run_pipeline(biased_vocab, _biased_vocab_wrapper, normalisation_pipeline)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess the gold summaries\n",
    "    logging.debug(\"[Preprocessors][PREP CORPUS] gold sum\")\n",
    "    clean_tok = {\n",
    "        \"cleaner\": Tokenizer(language, 'nltk', min_doc_len=1)\n",
    "    }\n",
    "    \n",
    "    gold_sum_dict = run_pipeline(gold_sum_dict, _doc_wrapper, clean_tok)\n",
    "    logging.debug(\"[Preprocessors][PREP CORPUS] Done\")\n",
    "    \n",
    "    return docs, gold_sum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias on vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab_doc_bias(doc_url, tag_map, bias_weight, vocab):\n",
    "    doc_vocab_bias = defaultdict(lambda:0)\n",
    "    for tag, tokens in tag_map.items():\n",
    "        if (len(tokens) == 0):\n",
    "            continue\n",
    "        tokens = tokenizer_cleaner(tokens)\n",
    "        tokens = set(\" \".join(tokens).split())\n",
    "        for word in tokens:\n",
    "            if word not in vocab.keys():\n",
    "                continue\n",
    "            doc_vocab_bias[vocab[word]] += bias_weight[tag]\n",
    "    return (doc_url, dict(doc_vocab_bias))\n",
    "\n",
    "def build_vocab_bias(vocab, doc_bias_terms, bias_weight, vocab_bias_file = None):\n",
    "    \"\"\"\n",
    "    Transformed our string bias (dict doc to tag to words) to an item bias\n",
    "    dict doc to word_id to weight\n",
    "\n",
    "    :param vocab:           Dictionnary mapping a word to feature indices.\n",
    "    :param doc_bias_terms:  Dictionnay mapping a document key to a bias element key\n",
    "                            (ex html tag of interests). bias element key mapping to words,\n",
    "                            as they appear in the vocabulary.\n",
    "                            <=> biased_vocab quoi\n",
    "    :param bias_weight: Dictionnary mapping a bias element to its values (weight).\n",
    "    \"\"\"\n",
    "    \n",
    "    if vocab_bias_file is not None and os.path.exists(vocab_bias_file):\n",
    "        return pickle.load(open(vocab_bias_file, 'rb'))\n",
    "    \n",
    "    logging.debug(\"[Preprocessors][Bias] Building vocab bias for each document\")\n",
    "    s = time.time()\n",
    "    vocab_bias = multip_for(_build_vocab_doc_bias, doc_bias_terms, (bias_weight, vocab))\n",
    "    e = time.time()\n",
    "    logging.debug(\"[Preprocessors][Bias] Done, Time :\" + \"{:.2f}\".format(e-s))\n",
    "    \n",
    "    if vocab_bias_file is not None and not os.path.exists(vocab_bias_file):\n",
    "        pickle.dump(vocab_bias, open(vocab_bias_file, 'wb'))\n",
    "\n",
    "    return dict(vocab_bias)\n",
    " \n",
    "def apply_bias(doc, doc_bias):\n",
    "    for word_id, weight in doc_bias.items():\n",
    "        doc_col = doc[:,word_id]\n",
    "        weight_vec = [weight if k != 0 else 0 for sent_id, k in enumerate(doc_col)]\n",
    "        try:\n",
    "            doc_col += np.array(weight_vec).reshape(len(weight_vec), 1)\n",
    "        except:\n",
    "            logging.error('process id:', os.getpid(), \"ERROR\")\n",
    "            logging.error('process id:', os.getpid(), \"APPLY BIAS, doc.shape = \", doc.shape, \"doc type\", type(doc))\n",
    "            logging.error('process id:', os.getpid(), \"DOC COL SHAPE\", doc_col.shape)\n",
    "            logging.error('process id:', os.getpid(), \"WORD_ID\", word_id)\n",
    "            raise\n",
    "                \n",
    "    return doc\n",
    "    \n",
    "def _apply_vocab_doc_bias(doc_url, doc, vocab_bias):\n",
    "    \"\"\"\n",
    "    Given a vector representation of the document (each sentence is a vector of the vocab size), \n",
    "    adds to a word its bias weight.\n",
    "    \"\"\"\n",
    "    doc = apply_bias(doc, vocab_bias[doc_url])\n",
    "    # Normalisation ??\n",
    "    return (doc_url, doc)\n",
    "\n",
    "def apply_vocab_bias(docs, vocab_bias):\n",
    "    \"\"\"\n",
    "\n",
    "    :param vocab:           Dictionnary mapping a word to feature indices.\n",
    "    :param doc_bias_terms:  Dictionnay mapping a document key to a bias element key\n",
    "                            (ex html tag of interests). bias element key mapping to words,\n",
    "                            as they appear in the vocabulary.\n",
    "                            <=> biased_vocab quoi\n",
    "    :param bias_weight:     Dictionnary mapping a bias element to its values (weight).\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = multip_for(_apply_vocab_doc_bias, docs, [vocab_bias])\n",
    "        \n",
    "    return dict(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from itertools import islice\n",
    "from langdetect import detect\n",
    "\n",
    "def chunks(data, SIZE=50):\n",
    "    it = iter(data)\n",
    "    for i in range(0, len(data), SIZE):\n",
    "        yield {k:data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def serialize_by_lang(dic, biased_vocab, corpus):\n",
    "    DIR_PATH = os.path.join(DATA_PATH, \"languages/\" + corpus)\n",
    "    \n",
    "    tmp_dic = {}\n",
    "    for key, value in dic.items():\n",
    "        try:\n",
    "            lang = detect(value)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if lang not in tmp_dic:\n",
    "            tmp_dic[lang] = {}\n",
    "        tmp_dic[lang][key] = {\"docs\": value, \"biased_vocab\" : biased_vocab[key]}\n",
    "    \n",
    "    for key, value in tmp_dic.items():\n",
    "        DIR_NAME = os.path.join(DIR_PATH, key)\n",
    "        if not os.path.exists(DIR_NAME):\n",
    "            os.makedirs(DIR_NAME)\n",
    "        \n",
    "        gen = chunks(value)\n",
    "        for i, json in enumerate(gen):\n",
    "            FILE_NAME = key + '-part-{0:04}'.format(i)\n",
    "            with open(os.path.join(DIR_NAME, FILE_NAME), 'wb+') as handle:\n",
    "                pickle.dump(json, handle)\n",
    "\n",
    "def deserialize(lang, corpus, sampling):\n",
    "    DIR_PATH = os.path.join(DATA_PATH, \"languages/\" + corpus + \"/\" + lang)\n",
    "    \n",
    "    dic = {}\n",
    "    files = [file for file in os.listdir(DIR_PATH) if bool(re.match(r'..-part-[0-9]+', file))]\n",
    "    nb_files = int(sampling * len(files))\n",
    "    \n",
    "    for file in files[:nb_files]:\n",
    "        with open(os.path.join(DIR_PATH, file), 'rb') as handle:\n",
    "            dic.update(pickle.load(handle))\n",
    "    \n",
    "    docs, biased_vocab = {}, {}\n",
    "    for key, value in dic.items():\n",
    "        docs[key] = value[\"docs\"]\n",
    "        biased_vocab[key] = value[\"biased_vocab\"]\n",
    "    return docs, biased_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_PATH = \"/home/pfee/data\"\n",
    "docs = {'www.marseille.archi.fr/ecole/bibliotheque/': \"École nationale supérieure d'architecture de Marseille 184, avenue de Luminy - case 924 13288 Marseille cedex 9 Tél : 04 91 82 71 00 Fax : 04 91 82 71 80\", 'www.parisseveille.info/quelles-sont-les-meilleures-marques-dautoradio/': '/ /Quelles sont les meilleures marques d’autoradio ?', 'www.magicien-animateur.ch/contact/': \"Vous souhaitez en savoir plus ? Renvoyez-moi ce formulaire avec les infos de votre événement. Vous n'êtes pas obligés de tout remplir, mais indiquez svp au moins votre nom et votre e-mail. Et plus vous m'en dites, plus l'offre que je vous enverrai sera précise ;-) Une proposition vous parviendra par e-mail dans les meilleurs délais ! Si vous préférez, vous pouvez aussi me contacter directement par e-mail sur : Illusion.ch - Magic Management - Patrick Waltrick Chemin de la Malice 36 - CH - 1228 Plan-les-Ouates / Genève Tel ++41.22 910 16 80 -\", 'ivoirtv.net/modules.html': \"Nicaise Blé et Brigitte Yodé ''LaFabuleuse SBY'' se sont dit « OUI » depuis le 28 avril 20... Le design et les couleurs chatoyantes interpellent les passants. Et quand on a accès à l'i... À proposLet'swelcome Ivory Coast Reggae Superstar RAMSES DE KIMON THE PHARAOH. His African... Zaena Morisho est une jeune chanteuse congolaise (RDC). Elle vit à Houston, la capitale du... A l'Etat civil, elle se nomme Opportune Aka et dans le milieu artistique elle répond au ps...\"}\n",
    "bias = {'www.marseille.archi.fr/ecole/bibliotheque/': {'h1': '', 'title': '', 'bold': '', 'b': '', 'i': '', 'em': '', 'mark': ''}, 'www.parisseveille.info/quelles-sont-les-meilleures-marques-dautoradio/': {'h1': '', 'title': '', 'bold': '', 'b': '', 'i': '', 'em': '', 'mark': ''}, 'www.magicien-animateur.ch/contact/': {'h1': '', 'title': '', 'bold': '', 'b': '', 'i': '', 'em': '', 'mark': ''}, 'ivoirtv.net/modules.html': {'h1': '', 'title': '', 'bold': '', 'b': '', 'i': '', 'em': '', 'mark': ''}}\n",
    "\n",
    "\"\"\"test = serialize_by_lang(docs, bias)\n",
    "print(json.dumps(test, indent=4, sort_keys=True))\"\"\"\n",
    "\n",
    "docs, biased_vocab = deserialize(\"fr\", \"dmoz-html\", sampling = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
