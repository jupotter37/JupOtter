{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb24d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16908615680\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "from os.path import expanduser\n",
    "import splitfolders\n",
    "import shutil\n",
    "from define_path import Def_Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as func\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import albumentations as A # Library for augmentations\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import transforms, utils, engine, train\n",
    "from utils import collate_fn\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "print(t)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "print(r)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "print(a)\n",
    "# f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac7fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/2023-10-13/\n"
     ]
    }
   ],
   "source": [
    "# to generalize home directory. User can change their parent path without entering their home directory\n",
    "path = Def_Path()\n",
    "\n",
    "parent_path =  path.home + \"/Pictures/\" + \"Data/\"\n",
    "\n",
    "root_dir = parent_path + path.year + \"-\" + path.month + \"-\" + path.day + \"/\"\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd833b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9, 0)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02a22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fucntion tranforms an input image for diverseifying data for training\n",
    "def train_transform():\n",
    "    return A.Compose([\n",
    "        A.Sequential([\n",
    "            A.RandomRotate90(p=1), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1), \n",
    "        ], p=1),\n",
    "        A.Resize(640, 480),  # Resize every image to 640x480 after all other transformations\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy'),\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92e1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to split the dataset into train, test and validation folder.\n",
    "def train_test_split(src_dir):\n",
    "    dst_dir_img = src_dir + \"images\"\n",
    "    dst_dir_anno = src_dir + \"annotations\"\n",
    "    \n",
    "    if os.path.exists(dst_dir_img) and os.path.exists(dst_dir_anno):\n",
    "        print(\"folders exist\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir_img)\n",
    "        os.mkdir(dst_dir_anno)\n",
    "        \n",
    "    for jpgfile in glob.iglob(os.path.join(src_dir, \"*.jpg\")):\n",
    "        shutil.copy(jpgfile, dst_dir_img)\n",
    "\n",
    "    for jsonfile in glob.iglob(os.path.join(src_dir, \"*.json\")):\n",
    "        shutil.copy(jsonfile, dst_dir_anno)\n",
    "        \n",
    "    output = parent_path + \"split_folder_output\" + \"-\" + path.year + \"-\" + path.month + \"-\" + path.day \n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    splitfolders.ratio(src_dir, # The location of dataset\n",
    "                   output=output, # The output location\n",
    "                   seed=42, # The number of seed\n",
    "                   ratio=(.7, .2, .1), # The ratio of split dataset\n",
    "                   group_prefix=None, # If your dataset contains more than one file like \".jpg\", \".pdf\", etc\n",
    "                   move=False # If you choose to move, turn this into True\n",
    "                   )\n",
    "    \n",
    "    shutil.rmtree(dst_dir_img)\n",
    "    shutil.rmtree(dst_dir_anno)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfdc7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, demo=False):                \n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.demo = demo \n",
    "        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n",
    "        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n",
    "        \n",
    "        with open(annotations_path) as f:\n",
    "            data = json.load(f)\n",
    "            bboxes_original = data['bboxes']\n",
    "            keypoints_original = data['keypoints']\n",
    "            \n",
    "            # All objects are keypoints on the arm\n",
    "            bboxes_labels_original = [] \n",
    "            bboxes_labels_original.append('base_kp')\n",
    "            bboxes_labels_original.append('joint1')\n",
    "            bboxes_labels_original.append('joint2')\n",
    "            bboxes_labels_original.append('joint3')\n",
    "            bboxes_labels_original.append('joint4')\n",
    "            bboxes_labels_original.append('joint5')\n",
    "\n",
    "        if self.transform:\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "            img = transformed['image']\n",
    "            bboxes = transformed['bboxes']\n",
    "            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,1,2)).tolist()\n",
    "            \n",
    "            keypoints = []\n",
    "            for o_idx, obj in enumerate(keypoints_transformed_unflattened):\n",
    "                obj_keypoints = []\n",
    "                for k_idx, kp in enumerate(obj):\n",
    "                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                keypoints.append(obj_keypoints)\n",
    "        else:\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original  \n",
    "\n",
    "            # Convert everything into a torch tensor        \n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n",
    "        target = {}\n",
    "        labels = [1, 2, 3, 4, 5, 6]   \n",
    "#         labels = [1, 2, 3, 4]\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are joint positions\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)        \n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64) # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n",
    "        img_original = F.to_tensor(img_original)\n",
    "\n",
    "\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_files)                     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ae5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_geometric_data(G):\n",
    "    x = []\n",
    "    edge_index = []\n",
    "    for _, node_data in G.nodes(data=True):\n",
    "        x.append([node_data['x'], node_data['y']])\n",
    "    for edge in G.edges():\n",
    "        edge_index.append(edge)\n",
    "    return Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c16e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_graph(keypoints, labels):\n",
    "    G = nx.DiGraph()\n",
    "    for i, kp in enumerate(keypoints):\n",
    "        x, y = kp\n",
    "        G.add_node(i, x=x, y=y, label=labels[i])\n",
    "    edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "        # Debugging print:\n",
    "    for node, data in G.nodes(data=True):\n",
    "        print(node, data)\n",
    "        \n",
    "    return to_geometric_data(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6025e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_graph(predicted_keypoints, predicted_labels=None):\n",
    "    G = nx.DiGraph()\n",
    "    placeholder = [-9999, -9999]  # Sentinel value for missing keypoints\n",
    "    mask = []\n",
    "\n",
    "    total_keypoints = 6  # Assuming you have 6 keypoints as per your edges\n",
    "\n",
    "    for i in range(total_keypoints):\n",
    "        if i < len(predicted_keypoints) and predicted_keypoints[i] is not None:\n",
    "            # If the keypoint is present and within the bounds of predicted_keypoints\n",
    "            x, y = predicted_keypoints[i][0:2]\n",
    "            mask.append(1.)\n",
    "            label = predicted_labels[i] if predicted_labels else -1\n",
    "        else:\n",
    "            # If keypoint is out of bounds of predicted_keypoints or missing\n",
    "            x, y = placeholder\n",
    "            mask.append(0.)\n",
    "            label = -1\n",
    "        \n",
    "        G.add_node(i, x=x, y=y, label=label)\n",
    "\n",
    "    # Add edges to the graph\n",
    "    edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    return to_geometric_data(G), torch.tensor(mask, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15817fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class SimpleGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleGNNLayer, self).__init__(aggr='add')  # 'add' aggregation\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Add self loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Transform node feature matrix.\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        self.layer1 = SimpleGNNLayer(in_channels, hidden_channels)\n",
    "        self.layer2 = SimpleGNNLayer(hidden_channels, hidden_channels)\n",
    "#         self.layer3 = SimpleGNNLayer(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = torch.relu(self.layer1(x, edge_index))\n",
    "        x = self.layer2(x, edge_index)\n",
    "#         x = torch.relu(self.layer3(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d08d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "class KeypointGNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointGNN, self).__init__()\n",
    "        \n",
    "        # Define GNN layers\n",
    "        self.conv1 = geom_nn.GCNConv(2, 128)\n",
    "        self.conv2 = geom_nn.GCNConv(128, 64)\n",
    "        self.regressor = nn.Linear(64, 2)  # Predicts x,y for each keypoint\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to(device), data.edge_index.to(device)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.regressor(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ae2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True, num_keypoints=6, num_classes=7)\n",
    "        self.kp_gnn = KeypointGNN()    \n",
    "\n",
    "    def forward(self, images, targets=None, train=False):\n",
    "        if train:\n",
    "            output = self.keypoint_rcnn(images, targets)\n",
    "            return output\n",
    "    \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.keypoint_rcnn.eval()\n",
    "                output = self.keypoint_rcnn(images)\n",
    "                self.keypoint_rcnn.train()\n",
    "                \n",
    "                keypoints = output[0]['keypoints'].detach().cpu().numpy()\n",
    "                kp_score = output[0]['keypoints_scores'].detach().cpu().numpy()\n",
    "                labels = output[0]['labels'].detach().cpu().numpy()\n",
    "                unique_labels = list(set(labels))\n",
    "                scores = output[0]['scores'].detach().cpu().numpy()\n",
    "                print(\"labels\", unique_labels)\n",
    "                kps = []\n",
    "                kp_scores = []\n",
    "                ulabels = []\n",
    "\n",
    "                for label in unique_labels:\n",
    "                    indices = [j for j, x in enumerate(labels) if x == label]\n",
    "                    scores_for_label = [scores[j] for j in indices]\n",
    "                    max_score_index = indices[scores_for_label.index(max(scores_for_label))]\n",
    "                    kp_score_label = kp_score[max_score_index].tolist()\n",
    "                    kps.append(keypoints[max_score_index][kp_score_label.index(max(kp_score_label))])\n",
    "                    ulabels.append(label)\n",
    "\n",
    "                kps = [torch.tensor(kp, dtype=torch.float32) for kp in kps]\n",
    "                keypoints = torch.stack(kps)\n",
    "                \n",
    "                pred_data, mask = create_pred_graph(keypoints, unique_labels)\n",
    "                pred_kps = self.kp_gnn(pred_data)\n",
    "                        \n",
    "            print(\"All keypoints\", pred_kps)\n",
    "\n",
    "            return pred_kps, mask, pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbd28da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_classification_loss(pred_graph, gt_graph):\n",
    "    # Assuming pred_graph and gt_graph are in a format where .edge_index provides the pair of nodes connected by an edge\n",
    "    pred_edges = set(tuple(e.cpu().numpy()) for e in pred_graph.edge_index.t())\n",
    "    gt_edges = set(tuple(e.cpu().numpy()) for e in gt_graph.edge_index.t())\n",
    "\n",
    "    # Compute missing and spurious edges\n",
    "    missing_edges = gt_edges - pred_edges\n",
    "    spurious_edges = pred_edges - gt_edges\n",
    "\n",
    "    loss = len(missing_edges) + len(spurious_edges)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "554d44dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/split_folder_output-2023-10-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 18343.90 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/split_folder_output-2023-10-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 18677.40 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jc-merlab/Pictures/Data/split_folder_output-2023-10-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2662 files [00:00, 19291.92 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[112.0801, 257.9522],\n",
      "        [195.9869, 257.9597],\n",
      "        [271.8108, 282.1697],\n",
      "        [265.5089, 301.9530],\n",
      "        [348.2030, 356.1391],\n",
      "        [367.1588, 368.3154]], device='cuda:0') tensor([1, 2, 3, 4, 5, 6], device='cuda:0')\n",
      "0 {'x': tensor(112.0801, device='cuda:0'), 'y': tensor(257.9522, device='cuda:0'), 'label': tensor(1, device='cuda:0')}\n",
      "1 {'x': tensor(195.9869, device='cuda:0'), 'y': tensor(257.9597, device='cuda:0'), 'label': tensor(2, device='cuda:0')}\n",
      "2 {'x': tensor(271.8108, device='cuda:0'), 'y': tensor(282.1697, device='cuda:0'), 'label': tensor(3, device='cuda:0')}\n",
      "3 {'x': tensor(265.5089, device='cuda:0'), 'y': tensor(301.9530, device='cuda:0'), 'label': tensor(4, device='cuda:0')}\n",
      "4 {'x': tensor(348.2030, device='cuda:0'), 'y': tensor(356.1391, device='cuda:0'), 'label': tensor(5, device='cuda:0')}\n",
      "5 {'x': tensor(367.1588, device='cuda:0'), 'y': tensor(368.3154, device='cuda:0'), 'label': tensor(6, device='cuda:0')}\n",
      "labels [1, 2, 4, 5]\n",
      "All keypoints tensor([[-541.5291, -430.9223],\n",
      "        [-176.6318, -138.7135],\n",
      "        [  -1.4576,    2.0048],\n",
      "        [  -2.0218,    2.1771],\n",
      "        [-177.9821, -139.0747],\n",
      "        [-542.3153, -431.4558]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# GNN for gt in the forward pass\u001b[39;00m\n\u001b[1;32m     43\u001b[0m gt_loss \u001b[38;5;241m=\u001b[39m criterion(refined_gt_keypoints, gt_keypoints)            \n\u001b[0;32m---> 44\u001b[0m refined_pred_keypoints, mask, pred_data \u001b[38;5;241m=\u001b[39m model(images[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(refined_pred_keypoints)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(gt_keypoints)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gnn = KeypointGNN().to(device)\n",
    "model = CombinedModel().to(device)\n",
    "optimizer = torch.optim.Adam(list(gnn.parameters()) + list(model.parameters()), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "KEYPOINTS_FOLDER_TRAIN = train_test_split(root_dir) +\"/train\" #train_test_split(root_dir) +\"/train\"\n",
    "KEYPOINTS_FOLDER_VAL = train_test_split(root_dir) +\"/val\"\n",
    "KEYPOINTS_FOLDER_TEST = train_test_split(root_dir) +\"/test\"\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 4\n",
    "\n",
    "dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n",
    "dataset_val = ClassDataset(KEYPOINTS_FOLDER_VAL, transform=None, demo=False)\n",
    "dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(data_loader_train):\n",
    "        images, targets = batch\n",
    "        images = torch.stack(images).to(device)  \n",
    "        # Move targets to GPU\n",
    "        for target in targets:\n",
    "            for key, val in target.items():\n",
    "                target[key] = val.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        indiviual_losses = []\n",
    "        output_train = model(images, targets=targets, train=True)\n",
    "        for i in range(len(images)):\n",
    "            gt_keypoints = targets[i]['keypoints'].to(device).squeeze()[:,:2]\n",
    "            gt_labels = targets[i]['labels'].to(device).squeeze()\n",
    "            print(gt_keypoints, gt_labels)\n",
    "            # Craeting GT graph\n",
    "            gt_data = create_gt_graph(gt_keypoints.to(device), gt_labels.to(device))\n",
    "            refined_gt_keypoints = gnn(gt_data)\n",
    "            # GNN for gt in the forward pass\n",
    "            gt_loss = criterion(refined_gt_keypoints, gt_keypoints)            \n",
    "            refined_pred_keypoints, mask, pred_data = model(images[i].unsqueeze(0), train=False)\n",
    "            print(refined_pred_keypoints)\n",
    "            print(gt_keypoints)\n",
    "            pred_loss = criterion(refined_pred_keypoints[0], gt_keypoints)            \n",
    "            graph_loss = graph_classification_loss(pred_data, gt_data)\n",
    "            loss = gt_loss + pred_loss + graph_loss\n",
    "            individual_losses.sppend(lodd.item())\n",
    "        \n",
    "        # Aggregate the individual losses to get a scalar loss\n",
    "        scalar_loss = sum(individual_losses) / len(individual_losses)      \n",
    "        loss_keypoint = output_train['loss_keypoint']\n",
    "        total_loss = 0.01*loss_keypoint + scalar_loss\n",
    "        optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch:{epoch} and Loss:{total_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialization\n",
    "# keypoint_rcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
    "# keypoint_rcnn = keypoint_rcnn.to(device)\n",
    "# gnn = KeypointGNN().to(device)\n",
    "# optimizer = torch.optim.Adam(list(keypoint_rcnn.parameters()) + list(gnn.parameters()), lr=0.001)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(epochs):\n",
    "#     for img, target, gt_graph, pred_graph in dataloader:\n",
    "        \n",
    "#         # 1. Keypoint R-CNN forward pass\n",
    "#         predictions = keypoint_rcnn(img)\n",
    "        \n",
    "#         # Loss for Keypoint R-CNN\n",
    "#         rcnn_loss = compute_rcnn_loss(predictions, target)  # Assuming you have this\n",
    "        \n",
    "#         # 2. GNN forward pass with GT keypoints\n",
    "#         refined_gt_keypoints = gnn(gt_graph)\n",
    "#         gt_loss = criterion(refined_gt_keypoints, torch.stack([k[\"keypoints\"] for k in target]))\n",
    "        \n",
    "#         # 3. GNN forward pass with predicted keypoints\n",
    "#         refined_pred_keypoints = gnn(pred_graph)\n",
    "#         pred_loss = criterion(refined_pred_keypoints, torch.stack([k[\"keypoints\"] for k in target]))\n",
    "        \n",
    "#         # Combine losses\n",
    "#         total_loss = rcnn_loss + gt_loss + pred_loss\n",
    "        \n",
    "#         # Backpropagation\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_model = HybridModel().to(device)\n",
    "weights_path = '/home/jc-merlab/Pictures/Data/trained_models/keypointsrcnn_weights_sim_b1_e25_v0.pth'\n",
    "cnn_model = torch.load(weights_path).to(device)\n",
    "image = '/home/jc-merlab/Pictures/Data/2023-08-14-Occluded/002654.rgb.jpg'\n",
    "image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "def predict_keypoints(cnn_model, gnn_model, image):\n",
    "    gnn_model.eval()\n",
    "    cnn_model.eval()\n",
    "    image = F.to_tensor(image).to(device)\n",
    "#     image = list(image)    \n",
    "    with torch.no_grad():\n",
    "        output = cnn_model([image])  \n",
    "        scores = output[0]['scores'].detach().cpu().numpy()\n",
    "        high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n",
    "        post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() \n",
    "        keypoints = []\n",
    "        labels = []\n",
    "        for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            keypoints.append(list(map(int, kps[0,0:2])))        \n",
    "        for label in output[0]['labels'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n",
    "            labels.append(label)\n",
    "        initial_keypoints = [x for _,x in sorted(zip(labels,keypoints))]\n",
    "        print(initial_keypoints)\n",
    "        data = construct_graph_for_prediction(initial_keypoints)\n",
    "        data = data.to(device)\n",
    "        predicted_keypoints = gnn_model(image.unsqueeze(0), data).cpu().numpy()\n",
    "    print(predicted_keypoints)\n",
    "    return predicted_keypoints, initial_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_keypoints, gt_keypoints = predict_keypoints(cnn_model, gnn_model, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_keypoints(image_path, keypoints, gt_keypoints):\n",
    "    \"\"\"\n",
    "    Visualize the keypoints on an image.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Path to the image.\n",
    "    - keypoints (np.array): Array of keypoints, assumed to be in (x, y) format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the image\n",
    "#     img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # Display the image\n",
    "    ax.imshow(image_path)\n",
    "    print(type(keypoints))\n",
    "    # Extract the x and y coordinates\n",
    "    x_coords = keypoints[:, 0]\n",
    "    y_coords = keypoints[:, 1]\n",
    "    \n",
    "    print(type(gt_keypoints))\n",
    "    gt_keypoints = np.array(gt_keypoints)\n",
    "    \n",
    "    x_gt = gt_keypoints[:, 0]\n",
    "    y_gt = gt_keypoints[:, 1]\n",
    "    \n",
    "    # Plot the keypoints\n",
    "    ax.scatter(x_coords, y_coords, c='r', s=40, label=\"Keypoints\")\n",
    "    ax.scatter(x_gt, y_gt, c='b', s=40, label=\"gt_keypoints\")\n",
    "    \n",
    "    # Show the image with keypoints\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(image, predicted_keypoints, gt_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660fad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
