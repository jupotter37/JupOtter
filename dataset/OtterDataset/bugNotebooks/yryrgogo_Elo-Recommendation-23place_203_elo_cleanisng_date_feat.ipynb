{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gc\n",
    "import re\n",
    "import pandas as pd\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "import os\n",
    "import sys\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/library')\n",
    "import utils\n",
    "from utils import get_categorical_features, get_numeric_features, reduce_mem_usage, elo_save_feature\n",
    "from preprocessing import get_dummies\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.listdir('../input/')\n",
    "key = 'card_id'\n",
    "target = 'target'\n",
    "ignore_list = [key, target, 'merchant_id', 'first_avtive_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 109.97it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 206.03it/s]\n",
      "100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_train = utils.read_df_pkl('../input/train0*')\n",
    "df_test = utils.read_df_pkl('../input/test0*')\n",
    "df_train.set_index(key, inplace=True)\n",
    "df_test.set_index(key, inplace=True)\n",
    "train_test = pd.concat([df_train, df_test], axis=0)\n",
    "# df_hist = utils.read_df_pkl('../input/hist_clean*')\n",
    "# df_new = utils.read_df_pkl('../input/new_clean*')\n",
    "df_hist = utils.read_df_pkl('../input/histori*0*')\n",
    "df_new = utils.read_df_pkl('../input/new_mercha*0*')\n",
    "\n",
    "df_train = utils.reduce_mem_usage(df_train)\n",
    "df_test  = utils.reduce_mem_usage(df_test )\n",
    "df_hist  = utils.reduce_mem_usage(df_hist )\n",
    "df_new   = utils.reduce_mem_usage(df_new  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_hist, df_new]:\n",
    "#     df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "#     df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "#     df['yyyymm'] = df['purchase_date'].map(lambda x: str(x)[:7])\n",
    "    df['yyyymm'] = df['purchase_date'].map(lambda x: str(x)[:7])\n",
    "    df['yyyymmdd'] = df['purchase_date'].map(lambda x: str(x)[:10])\n",
    "# df_hist = df_hist.set_index(key).join(train_test['first_active_month']).reset_index()\n",
    "# df_new = df_new.set_index(key).join(train_test['first_active_month']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_utils import date_add_days\n",
    "\n",
    "hist_max_date = df_hist.groupby(key)['purchase_date'].max()\n",
    "hist_max_date.name = 'hist_purchase_date_max'\n",
    "hist_max_date = hist_max_date.to_frame()\n",
    "# month_maxは翌月の数字にする\n",
    "hist_max_date['hist_purchase_month_max'] = hist_max_date['hist_purchase_date_max'].map(lambda x: date_add_days(x, 32) if int(str(x)[8:10])<=15 else date_add_days(x, 20))\n",
    "hist_max_date['hist_purchase_month_max'] = hist_max_date['hist_purchase_month_max'].map(lambda x: str(x)[:6])\n",
    "\n",
    "hist_min_date = df_hist.groupby(key)['purchase_date'].min()\n",
    "hist_min_date.name = 'hist_purchase_date_min'\n",
    "hist_min_date = hist_min_date.to_frame()\n",
    "hist_min_date['hist_purchase_month_min'] = hist_min_date['hist_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n",
    "\n",
    "new_max_date = df_new.groupby(key)['purchase_date'].max()\n",
    "new_max_date.name = 'new_purchase_date_max'\n",
    "new_max_date = new_max_date.to_frame()\n",
    "new_max_date['new_purchase_month_max'] = new_max_date['new_purchase_date_max'].map(lambda x: date_add_days(x, 32) if int(str(x)[8:10])<=15 else date_add_days(x, 20))\n",
    "new_max_date['new_purchase_month_max'] = new_max_date['new_purchase_month_max'].map(lambda x: str(x)[:6])\n",
    "\n",
    "new_min_date = df_new.groupby(key)['purchase_date'].min()\n",
    "new_min_date.name = 'new_purchase_date_min'\n",
    "new_min_date = new_min_date.to_frame()\n",
    "\n",
    "# month_maxは翌月の数字にする\n",
    "new_min_date['new_purchase_month_min'] = new_min_date['new_purchase_date_min'].map(lambda x: str(x)[:7])\n",
    "\n",
    "df_hist.set_index(key, inplace=True)\n",
    "df_new.set_index(key, inplace=True)\n",
    "df_hist = df_hist.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "df_new = df_new.join(hist_max_date).join(hist_min_date).join(new_max_date).join(new_min_date)\n",
    "\n",
    "df_hist['hist_purchase_month_max'] = df_hist['hist_purchase_month_max'].map(lambda x: str(x)[:4] + '-' + str(x)[4:])\n",
    "df_hist['new_purchase_month_max'] = df_hist['new_purchase_month_max'].map(lambda x: str(x)[:4] + '-' + str(x)[4:])\n",
    "df_hist['new_purchase_month_max'] = df_hist['new_purchase_month_max'].replace('nan-', np.nan)\n",
    "\n",
    "df_hist['hist_purchase_month_max'] = pd.to_datetime(df_hist['hist_purchase_month_max'])\n",
    "df_hist['hist_purchase_month_min'] = pd.to_datetime(df_hist['hist_purchase_month_min'])\n",
    "df_hist['new_purchase_month_max'] = pd.to_datetime(df_hist['new_purchase_month_max'])\n",
    "df_hist['new_purchase_month_min'] = pd.to_datetime(df_hist['new_purchase_month_min'])\n",
    "\n",
    "df_new['hist_purchase_month_max'] = df_new['hist_purchase_month_max'].map(lambda x: str(x)[:4] + '-' + str(x)[4:])\n",
    "df_new['new_purchase_month_max'] = df_new['new_purchase_month_max'].map(lambda x: str(x)[:4] + '-' + str(x)[4:])\n",
    "df_new['new_purchase_month_max'] = df_new['new_purchase_month_max'].replace('nan-', np.nan)\n",
    "\n",
    "df_new['new_purchase_month_max'] = pd.to_datetime(df_new['new_purchase_month_max'])\n",
    "df_new['new_purchase_month_min'] = pd.to_datetime(df_new['new_purchase_month_min'])\n",
    "df_new['hist_purchase_month_max'] = pd.to_datetime(df_new['hist_purchase_month_max'])\n",
    "df_new['hist_purchase_month_min'] = pd.to_datetime(df_new['hist_purchase_month_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26595452, 23)\n",
      "(2516909, 23)\n"
     ]
    }
   ],
   "source": [
    "df_new_lag1 = df_new[df_new['month_lag']==1]\n",
    "df_new_lag2 = df_new[df_new['month_lag']==2]\n",
    "\n",
    "df = df_hist\n",
    "\n",
    "auth1 = df[df.authorized_flag==1]\n",
    "auth0 = df[df.authorized_flag==0]\n",
    "print(auth1.shape)\n",
    "print(auth0.shape)\n",
    "\n",
    "cat1_0 = False\n",
    "# cat1_0 = True\n",
    "if cat1_0:\n",
    "    auth1_cat1 = auth1[auth1.category_1==1]\n",
    "    auth1_cat0 = auth1[auth1.category_1==0]\n",
    "    auth0_cat1 = auth0[auth0.category_1==1]\n",
    "    auth0_cat0 = auth0[auth0.category_1==0]\n",
    "    new_cat1 = df_new[df_new.category_1==1]\n",
    "    new_cat0 = df_new[df_new.category_1==0]\n",
    "#     del auth1, auth0\n",
    "#     gc.collect()\n",
    "\n",
    "auth1_lag0 = auth1[auth1['month_lag']==0]\n",
    "auth1_lag1 = auth1[auth1['month_lag']==-1]\n",
    "auth1_lag2 = auth1[auth1['month_lag']==-2]\n",
    "auth1_lag02 = auth1[auth1['month_lag']>=-2]\n",
    "auth1_lag05 = auth1[auth1['month_lag']>=-5]\n",
    "\n",
    "auth0_lag0 = auth0[auth0['month_lag']==0]\n",
    "auth0_lag1 = auth0[auth0['month_lag']==-1]\n",
    "auth0_lag2 = auth0[auth0['month_lag']==-2]\n",
    "auth0_lag02 = auth0[auth0['month_lag']>=-2]\n",
    "auth0_lag05 = auth0[auth0['month_lag']>=-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 100.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 172.30it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(1963031, 31)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 1/3 [00:28<00:56, 28.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 102.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 176.45it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(1027617, 31)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 2/3 [00:54<00:27, 27.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 92.28it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 167.42it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(935414, 31)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 3/3 [01:21<00:00, 27.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "prefix = '203_pst'\n",
    "prefix = '303_pst'\n",
    "prefix = '403_pst'\n",
    "new_df_list = [df_new, df_new_lag1, df_new_lag2]\n",
    "new_fname_list = ['new', 'new_lag1', 'new_lag2']\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    train_test = train_test.head(10000)\n",
    "        \n",
    "\n",
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# 集計が必要なFeature\n",
    "#========================================================================\n",
    "for df, fname in zip(tqdm(new_df_list), new_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "#     df.drop(['level_0', 'index'], axis=1, inplace=True)\n",
    "#     sys.exit()\n",
    "    \n",
    "    df_train = utils.read_df_pkl('../input/train0*')\n",
    "    df_test = utils.read_df_pkl('../input/test0*')\n",
    "    df_train.set_index(key, inplace=True)\n",
    "    df_test.set_index(key, inplace=True)\n",
    "    train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    # new\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    \n",
    "    df['diff_date_from_new_min_month'] = (df['new_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_new_max_month'] = (df['new_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_min_month'] = (df['hist_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_max_month'] = (df['hist_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "\n",
    "    aggs = {}\n",
    "    aggs['diff_date_from_new_min_month'] = ['mean', 'max', 'min', 'std' ]\n",
    "    aggs['diff_date_from_new_max_month'] = ['mean', 'max', 'min', 'std' ]\n",
    "    aggs['diff_date_from_hist_min_month'] = ['mean', 'max', 'min', 'std']\n",
    "    aggs['diff_date_from_hist_max_month'] = ['mean', 'max', 'min', 'std']\n",
    "#     aggs['diff_date_from_new_min_month'] = ['skew']\n",
    "#     aggs['diff_date_from_new_max_month'] = ['skew']\n",
    "#     aggs['diff_date_from_hist_min_month'] = ['skew']\n",
    "#     aggs['diff_date_from_hist_max_month'] = ['skew']\n",
    "\n",
    "#     col_unique =['subsector_id', 'merchant_id', 'merchant_category_id', 'yyyymmdd', 'yyyy_week']\n",
    "    col_unique =['yyyymmdd']\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    \n",
    "#     df = pd.get_dummies(df, columns=['category_2', 'category_3'])\n",
    "#     aggs = {\n",
    "#     'category_1': ['sum', 'mean'],\n",
    "#     'category_2_1.0': ['mean'],\n",
    "#     'category_2_2.0': ['mean'],\n",
    "#     'category_2_3.0': ['mean'],\n",
    "#     'category_2_4.0': ['mean'],\n",
    "#     'category_2_5.0': ['mean'],\n",
    "#     'category_3_A': ['mean'],\n",
    "#     'category_3_B': ['mean'],\n",
    "#     'category_3_C': ['mean'],\n",
    "#     }\n",
    "    \n",
    "#     for col in col_unique:\n",
    "#         aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['installments'] = ['sum', 'max','mean','var']\n",
    "#     aggs['month_lag'] = ['max','min','mean','var']\n",
    "    aggs['month_diff'] = ['mean','var']\n",
    "    aggs['card_id'] = ['count']\n",
    "    \n",
    "    if ('level_0' and 'index' not in df.columns) and key not in df.columns:\n",
    "        df.reset_index(inplace=True)\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    \n",
    "    print('Aggregation Start!')\n",
    "    print(df.shape)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    # add feature\n",
    "#     df_agg[f'{fname}_amount_sum_per_installments_sum'] = df_agg[f'{fname}_purchase_amount_sum'] / (df_agg[f'{fname}_installments_sum'] + 1.0)\n",
    "#     df_agg[f'{fname}_amount_mean_per_installments_mean'] = df_agg[f'{fname}_purchase_amount_mean'] / (df_agg[f'{fname}_installments_mean'] + 1.0)\n",
    "    \n",
    "    \n",
    "    #========================================================================\n",
    "    # monthly agg\n",
    "    #========================================================================\n",
    "    new_columns = get_new_columns(fname + '_monthly_avg', aggs)\n",
    "    month_agg = df.groupby([key, 'yyyymm']).agg(aggs)\n",
    "        \n",
    "    month_agg.columns = new_columns\n",
    "    month_agg = month_agg.reset_index().drop('yyyymm', axis=1).groupby([key]).mean()\n",
    "    \n",
    "#     train_test = train_test.join(df_agg)\n",
    "    train_test = train_test.join(df_agg).join(month_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "    elo_save_feature(prefix, train_test, feat_check=False)\n",
    "print('Complete!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 102.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 172.40it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(26595452, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  8%|▊         | 1/12 [00:52<09:36, 52.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 96.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 167.24it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(3178354, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 2/12 [01:20<07:31, 45.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 98.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 181.85it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(3326861, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 25%|██▌       | 3/12 [01:48<06:00, 40.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 94.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 166.74it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(3523114, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 4/12 [02:17<04:52, 36.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 96.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 174.16it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(10028329, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 42%|████▏     | 5/12 [02:53<04:15, 36.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 95.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 172.15it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(17438896, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 6/12 [03:36<03:51, 38.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 101.06it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 173.90it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(2516909, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 58%|█████▊    | 7/12 [04:06<02:59, 35.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 95.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 169.94it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(292592, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 8/12 [04:28<02:06, 31.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 96.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 172.62it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(300569, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 75%|███████▌  | 9/12 [04:49<01:25, 28.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 88.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 158.53it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(334162, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 10/12 [05:11<00:53, 26.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 99.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 176.94it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(927323, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 92%|█████████▏| 11/12 [05:37<00:26, 26.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 97.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 171.08it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Start!\n",
      "(1617002, 29)\n",
      "(325540, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 12/12 [06:04<00:00, 26.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!!\n"
     ]
    }
   ],
   "source": [
    "prefix = '203_pst'\n",
    "prefix = '303_pst'\n",
    "prefix = '403_pst'\n",
    "hist_df_list = [auth1, auth1_lag0, auth1_lag1, auth1_lag2, auth1_lag02, auth1_lag05, auth0, auth0_lag0, auth0_lag1, auth0_lag2, auth0_lag02, auth0_lag05] \n",
    "hist_fname_list = ['auth1', 'auth1_lag0', 'auth1_lag1', 'auth1_lag2', 'auth1_lag02', 'auth1_lag05', 'auth0', 'auth0_lag0', 'auth0_lag1', 'auth0_lag2', 'auth0_lag02', 'auth0_lag05']\n",
    "\n",
    "for df, fname in zip(tqdm(hist_df_list), hist_fname_list):\n",
    "    if debug:\n",
    "        df = df.head(3000)\n",
    "    \n",
    "    df_train = utils.read_df_pkl('../input/train0*')\n",
    "    df_test = utils.read_df_pkl('../input/test0*')\n",
    "    df_train.set_index(key, inplace=True)\n",
    "    df_test.set_index(key, inplace=True)\n",
    "    train_test = pd.concat([df_train, df_test], axis=0)\n",
    "    \n",
    "    # new\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    \n",
    "    df['diff_date_from_new_min_month'] = (df['new_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_new_max_month'] = (df['new_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_min_month'] = (df['hist_purchase_month_min'] - df['purchase_date']).dt.days\n",
    "    df['diff_date_from_hist_max_month'] = (df['hist_purchase_month_max'] - df['purchase_date']).dt.days\n",
    "\n",
    "    aggs = {}\n",
    "    aggs['diff_date_from_new_min_month'] = ['mean', 'max', 'min', 'std' ]\n",
    "    aggs['diff_date_from_new_max_month'] = ['mean', 'max', 'min', 'std' ]\n",
    "    aggs['diff_date_from_hist_min_month'] = ['mean', 'max', 'min', 'std']\n",
    "    aggs['diff_date_from_hist_max_month'] = ['mean', 'max', 'min', 'std']\n",
    "#     aggs['diff_date_from_new_min_month'] = ['skew']\n",
    "#     aggs['diff_date_from_new_max_month'] = ['skew']\n",
    "#     aggs['diff_date_from_hist_min_month'] = ['skew']\n",
    "#     aggs['diff_date_from_hist_max_month'] = ['skew']\n",
    "\n",
    "#     col_unique =['subsector_id', 'merchant_id', 'merchant_category_id', 'yyyymmdd', 'yyyy_week']\n",
    "    col_unique =['yyyymmdd']\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    \n",
    "#     df = pd.get_dummies(df, columns=['category_2', 'category_3'])\n",
    "#     aggs = {\n",
    "#     'category_1': ['sum', 'mean'],\n",
    "#     'category_2_1.0': ['mean'],\n",
    "#     'category_2_2.0': ['mean'],\n",
    "#     'category_2_3.0': ['mean'],\n",
    "#     'category_2_4.0': ['mean'],\n",
    "#     'category_2_5.0': ['mean'],\n",
    "#     'category_3_A': ['mean'],\n",
    "#     'category_3_B': ['mean'],\n",
    "#     'category_3_C': ['mean'],\n",
    "#     }\n",
    "    \n",
    "#     for col in col_unique:\n",
    "#         aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "    aggs['installments'] = ['sum', 'max','mean','var']\n",
    "#     aggs['month_lag'] = ['max','min','mean','var']\n",
    "    aggs['month_diff'] = ['mean','var']\n",
    "    aggs['card_id'] = ['count']\n",
    "    \n",
    "    if ('level_0' and 'index' not in df.columns) and key not in df.columns:\n",
    "        df.reset_index(inplace=True)\n",
    "    \n",
    "    new_columns = get_new_columns(fname, aggs)\n",
    "    \n",
    "    print('Aggregation Start!')\n",
    "    print(df.shape)\n",
    "    df_agg = df.groupby(key).agg(aggs)\n",
    "    df_agg.columns = new_columns\n",
    "    \n",
    "    # add feature\n",
    "#     df_agg[f'{fname}_amount_sum_per_installments_sum'] = df_agg[f'{fname}_purchase_amount_sum'] / (df_agg[f'{fname}_installments_sum'] + 1.0)\n",
    "#     df_agg[f'{fname}_amount_mean_per_installments_mean'] = df_agg[f'{fname}_purchase_amount_mean'] / (df_agg[f'{fname}_installments_mean'] + 1.0)\n",
    "    \n",
    "    \n",
    "    #========================================================================\n",
    "    # monthly agg\n",
    "    #========================================================================\n",
    "    new_columns = get_new_columns(fname + '_monthly_avg', aggs)\n",
    "    month_agg = df.groupby([key, 'yyyymm']).agg(aggs)\n",
    "        \n",
    "    month_agg.columns = new_columns\n",
    "    month_agg = month_agg.reset_index().drop('yyyymm', axis=1).groupby([key]).mean()\n",
    "    \n",
    "#     train_test = train_test.join(df_agg)\n",
    "    train_test = train_test.join(df_agg).join(month_agg)\n",
    "    print(train_test.shape)\n",
    "    \n",
    "    elo_save_feature(prefix, train_test, feat_check=False)\n",
    "print('Complete!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 67.98it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(325540, 703)\n"
     ]
    }
   ],
   "source": [
    "prefix = '303_pst'\n",
    "prefix = '403_pst'\n",
    "\n",
    "import glob\n",
    "# #========================================================================\n",
    "# # これは作成済のFeatureを読み込んで計算する\n",
    "# #========================================================================\n",
    "combi_list = [ ['new', 'new_lag1'] ,['new', 'auth1'] ,['new', 'auth1_lag0'] ,['new', 'auth1_lag02'] ,['new', 'auth1_lag05'] ,['new', 'auth0_lag0'] ,\n",
    "['new', 'auth0_lag02'] ,['new', 'auth0_lag05'] ,['auth1_lag0', 'auth1_lag02'] ,['auth1_lag0', 'auth1_lag05'] ,['auth1_lag02', 'auth1_lag05'] ,['auth0_lag02', 'auth0_lag05']\n",
    "]\n",
    "\n",
    "comp_cols = [\n",
    "    'amount_sum_per_installments_sum'\n",
    "    ,'amount_mean_per_installments_mean'\n",
    "    ,'purchase_amount_sum' \n",
    "    ,'purchase_amount_mean' \n",
    "    ,'purchase_amount_max' \n",
    "    ,'purchase_amount_min' \n",
    "    ,'installments_max' \n",
    "    ,'installments_var' \n",
    "    ,'category_1_sum' \n",
    "    ,'category_1_mean' \n",
    "    ,'month_lag_mean' \n",
    "    ,'month_lag_var' \n",
    "    ,'month_lag_skew' \n",
    "    ,'month_diff_mean' \n",
    "    ,'month_diff_var' \n",
    "    ,'month_diff_skew' \n",
    "    ,'card_id_size' \n",
    "    ,'yyyymmdd_nunique'\n",
    "    ,'subsector_id_nunique'\n",
    "    ,'merchant_id_nunique'\n",
    "    ,'merchant_category_id_nunique'\n",
    "     'diff_date_from_new_min_month_mean'\n",
    "     ,'diff_date_from_hist_max_month_mean'\n",
    "     ,'diff_date_from_new_min_month_max'\n",
    "     ,'diff_date_from_hist_max_month_max'\n",
    "     ,'diff_date_from_new_min_month_min'\n",
    "     ,'diff_date_from_hist_min_month_min'\n",
    "]\n",
    "\n",
    "tmp_feature_list = glob.glob(f'../features/1_first_valid/{prefix}*.gz')\n",
    "# tmp_feature_list = glob.glob(f'../features/8_ensemble/{prefix}*.gz') + glob.glob(f'../features/4_winner/{prefix}*.gz')\n",
    "feature_list = []\n",
    "for f in tmp_feature_list:\n",
    "    if f.count('pst_ratio_') or f.count('pst_diff_'):continue\n",
    "    for col in comp_cols:\n",
    "        if f.count(col):\n",
    "            feature_list.append(f)\n",
    "            \n",
    "base = utils.read_df_pkl('../input/base_first*0*')\n",
    "p_list = utils.parallel_load_data(path_list=feature_list)\n",
    "df_feat = pd.concat(p_list, axis=1)\n",
    "train_test = pd.concat([base[key], df_feat], axis=1)\n",
    "\n",
    "for (fm1, fm2) in tqdm(combi_list):\n",
    "    for col in comp_cols:\n",
    "#         203_pst_auth0_lag02_monthly_avg_purchase_amount_min\n",
    "        try:\n",
    "            train_test[f\"ratio_{fm1}_{fm2}_{col}\"] = train_test[f\"{prefix}_{fm1}_{col}@\"] / train_test[f\"{prefix}_{fm2}_{col}@\"]\n",
    "            train_test[f\"diff_{fm1}_{fm2}_{col}\"] = train_test[f\"{prefix}_{fm1}_{col}@\"] - train_test[f\"{prefix}_{fm2}_{col}@\"]\n",
    "        except KeyError:\n",
    "#             print(fm1, fm2, col)\n",
    "            continue\n",
    "        \n",
    "print(train_test.shape)\n",
    "\n",
    "ratio_diff_cols = [col for col in train_test.columns if col[:5]=='ratio' or col[:4]=='diff']\n",
    "\n",
    "elo_save_feature(prefix, train_test[ratio_diff_cols], feat_check=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
