{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Checking 0/5...\n",
      "Checking 1/5...\n",
      "Checking 2/5...\n",
      "Checking 3/5...\n",
      "Checking 4/5...\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add               │ 78             │ \u001b[1;32m38              \u001b[0m │\n",
      "│ ArgMin            │ 1              │ 1                │\n",
      "│ Cast              │ 96             │ \u001b[1;32m1               \u001b[0m │\n",
      "│ Ceil              │ 20             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Clip              │ 20             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Concat            │ 33             │ \u001b[1;32m1               \u001b[0m │\n",
      "│ Constant          │ 638            │ \u001b[1;32m279             \u001b[0m │\n",
      "│ ConstantOfShape   │ 10             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Conv              │ 260            │ 260              │\n",
      "│ Div               │ 40             │ \u001b[1;32m4               \u001b[0m │\n",
      "│ Equal             │ 4              │ 4                │\n",
      "│ Flatten           │ 2              │ 2                │\n",
      "│ Gather            │ 32             │ \u001b[1;32m6               \u001b[0m │\n",
      "│ GlobalAveragePool │ 2              │ 2                │\n",
      "│ Identity          │ 156            │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Mul               │ 228            │ \u001b[1;32m208             \u001b[0m │\n",
      "│ NonZero           │ 4              │ 4                │\n",
      "│ Pad               │ 10             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Pow               │ 2              │ 2                │\n",
      "│ ReduceMean        │ 56             │ 56               │\n",
      "│ ReduceSum         │ 1              │ 1                │\n",
      "│ Reshape           │ 24             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Resize            │ 2              │ 2                │\n",
      "│ Shape             │ 28             │ \u001b[1;32m0               \u001b[0m │\n",
      "│ Sigmoid           │ 208            │ 208              │\n",
      "│ Slice             │ 16             │ \u001b[1;32m2               \u001b[0m │\n",
      "│ Squeeze           │ 4              │ 4                │\n",
      "│ Sub               │ 73             │ \u001b[1;32m3               \u001b[0m │\n",
      "│ Transpose         │ 14             │ \u001b[1;32m4               \u001b[0m │\n",
      "│ Unsqueeze         │ 82             │ \u001b[1;32m2               \u001b[0m │\n",
      "│ Model Size        │ 41.0MiB        │ \u001b[1;32m40.9MiB         \u001b[0m │\n",
      "└───────────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!onnxsim ./full.onnx ./full_opt.onnx 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[11:07:06] :46: ONNX Model ir version: 8\n",
      "[11:07:06] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ image, ]\n",
      "outputTensors: [ embeddings, ]\n",
      "gen Static Model ... \n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./full_static_opt.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode full_static_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --saveStaticModel --MNNModel ./full_static_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[15:21:01] :46: ONNX Model ir version: 8\n",
      "[15:21:01] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "The Convolution use shared weight, may increase the model size\n",
      "inputTensors : [ query_images, support_images, ]\n",
      "outputTensors: [ predictions, ]\n",
      "gen Static Model ... \n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./full_opt.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode full_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --saveStaticModel --MNNModel ./full_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[23:39:28] :46: ONNX Model ir version: 8\n",
      "[23:39:28] :47: ONNX Model opset version: 17\n",
      "[23:39:28] :133: Check it out ==> /Resize_output_0 has empty input, the index is 1\n",
      "[23:39:28] :133: Check it out ==> /Resize_output_0 has empty input, the index is 2\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ image, ]\n",
      "outputTensors: [ embeddings, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./full_dynamic_opt.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode full_dynamic_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./full_dynamic_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[12:52:20] :46: ONNX Model ir version: 8\n",
      "[12:52:20] :47: ONNX Model opset version: 17\n",
      "[12:52:20] :133: Check it out ==> /Resize_output_0 has empty input, the index is 1\n",
      "[12:52:20] :133: Check it out ==> /Resize_output_0 has empty input, the index is 2\n",
      "[12:52:20] :133: Check it out ==> /model/neck/upsample0/Resize_output_0 has empty input, the index is 1\n",
      "[12:52:20] :133: Check it out ==> /model/neck/upsample0/Resize_output_0 has empty input, the index is 2\n",
      "[12:52:20] :133: Check it out ==> /model/neck/upsample1/Resize_output_0 has empty input, the index is 1\n",
      "[12:52:20] :133: Check it out ==> /model/neck/upsample1/Resize_output_0 has empty input, the index is 2\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ det_box, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile /Users/arif/Downloads/yolov6lite_l.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode yolov6lite_l_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./yolov6lite_l_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[14:46:36] :46: ONNX Model ir version: 8\n",
      "[14:46:36] :47: ONNX Model opset version: 17\n",
      "[14:46:36] :133: Check it out ==> /Resize_output_0 has empty input, the index is 1\n",
      "[14:46:36] :133: Check it out ==> /Resize_output_0 has empty input, the index is 2\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ det_box, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile /Users/arif/Downloads/yolov6n.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode yolov6n_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./yolov6n_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[11:07:17] :46: ONNX Model ir version: 8\n",
      "[11:07:17] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ z_support, support_labels, ]\n",
      "outputTensors: [ z_proto, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./support_proto/ZProto_opt.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode ZProto_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./support_proto/ZProto_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[19:09:35] :46: ONNX Model ir version: 8\n",
      "[19:09:35] :47: ONNX Model opset version: 17\n",
      "[19:09:35] :133: Check it out ==> /Resize_output_0 has empty input, the index is 1\n",
      "[19:09:35] :133: Check it out ==> /Resize_output_0 has empty input, the index is 2\n",
      "Start to Optimize the MNN Net...\n",
      "The Convolution use shared weight, may increase the model size\n",
      "Shape of NonMaxSupression's input is unknown. Please confirm version of MNN engine is new enough and use V3 Module API to run it correctly\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ det_box, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./support_proto/yolov6l6.onnx --optimizeLevel 1 --optimizePrefer 2 --bizCode yolov6l6_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./support_proto/yolov6l6_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[00:30:03] :46: ONNX Model ir version: 8\n",
      "[00:30:03] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ z_query, z_proto, ]\n",
      "outputTensors: [ dists, prediction, ]\n",
      "gen Static Model ... \n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./MinEuclid.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode MinEuclid --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --saveStaticModel --MNNModel ./MinEuclid.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[14:38:10] :46: ONNX Model ir version: 8\n",
      "[14:38:10] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ query_image, z_proto, ]\n",
      "outputTensors: [ dists, predictions, ]\n",
      "gen Static Model ... \n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./ProdStatic_opt.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode ProdStatic_opt --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --saveStaticModel --batch 2 --MNNModel ./ProdStatic_opt.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[21:19:16] :46: ONNX Model ir version: 8\n",
      "[21:19:16] :47: ONNX Model opset version: 17\n",
      "Start to Optimize the MNN Net...\n",
      "Unsupported data type!Unsupported data type!Unsupported data type!Unsupported data type!Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Unsupported data type!Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Unsupported data type!Unsupported data type!Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "Reshape error: 1 -> 0\n",
      "[21:19:16] :105: These Op Not Support: ONNX::SkipLayerNormalization \n",
      "Converted Failed!\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/arif/anaconda3/envs/mnn/bin/mnnconvert\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('MNN==2.8.1', 'console_scripts', 'mnnconvert')())\n",
      "  File \"/Users/arif/anaconda3/envs/mnn/lib/python3.9/site-packages/MNN-2.8.1-py3.9-macosx-11.1-arm64.egg/MNN/tools/mnnconvert.py\", line 49, in main\n",
      "    dst_model_size = os.path.getsize(arg_dict[\"MNNModel\"]) / 1024.0 / 1024.0\n",
      "  File \"/Users/arif/anaconda3/envs/mnn/lib/python3.9/genericpath.py\", line 50, in getsize\n",
      "    return os.stat(filename).st_size\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './ProdStatic_opt_o.mnn'\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile models/ProdStatic_opt_o.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode ProdStatic_opt_o --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --batch 2 --MNNModel ./ProdStatic_opt_o.mnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "\n",
      "Usage:\n",
      "  MNNConvert [OPTION...]\n",
      "\n",
      "  -h, --help                    Convert Other Model Format To MNN Model\n",
      "\n",
      "  -v, --version                 show current version\n",
      "  -f, --framework arg           model type, ex:\n",
      "                                [TF,CAFFE,ONNX,TFLITE,MNN,JSON]\n",
      "      --modelFile arg           tensorflow Pb or caffeModel, ex:\n",
      "                                *.pb,*caffemodel\n",
      "      --batch arg               if model input's batch is not set, set as the\n",
      "                                batch size you set\n",
      "      --keepInputFormat         keep input dimension format or not, default:\n",
      "                                false\n",
      "      --optimizeLevel arg       graph optimize option, 0: don't run\n",
      "                                optimize(only support for MNN source), 1: use graph\n",
      "                                optimize only for every input case is right, 2:\n",
      "                                normally right but some case may be wrong,\n",
      "                                default 1\n",
      "      --optimizePrefer arg      graph optimize option, 0 for normal, 1 for\n",
      "                                smalleset, 2 for fastest\n",
      "      --prototxt arg            only used for caffe, ex: *.prototxt\n",
      "      --MNNModel arg            MNN model, ex: *.mnn\n",
      "      --fp16                    save Conv's weight/bias in half_float data\n",
      "                                type\n",
      "      --benchmarkModel          Do NOT save big size data, such as Conv's\n",
      "                                weight,BN's gamma,beta,mean and variance etc.\n",
      "                                Only used to test the cost of the model\n",
      "      --bizCode arg             MNN Model Flag, ex: MNN\n",
      "      --debug                   Enable debugging mode.\n",
      "      --forTraining             whether or not to save training ops BN and\n",
      "                                Dropout, default: false\n",
      "      --weightQuantBits arg     save conv/matmul/LSTM float weights to int8\n",
      "                                type, only optimize for model size, 2-8 bits,\n",
      "                                default: 0, which means no weight quant\n",
      "      --weightQuantAsymmetric   the default weight-quant uses SYMMETRIC quant\n",
      "                                method, which is compatible with old MNN\n",
      "                                versions. you can try set --weightQuantAsymmetric\n",
      "                                to use asymmetric quant method to improve\n",
      "                                accuracy of the weight-quant model in some cases,\n",
      "                                but asymmetric quant model cannot run on old\n",
      "                                MNN versions. You will need to upgrade MNN to\n",
      "                                new version to solve this problem. default:\n",
      "                                false\n",
      "      --compressionParamsFile arg\n",
      "                                The path of the compression parameters that\n",
      "                                stores activation, weight scales and zero\n",
      "                                points for quantization or information for\n",
      "                                sparsity.\n",
      "      --OP                      print framework supported op\n",
      "      --saveStaticModel         save static model with fix shape, default:\n",
      "                                false\n",
      "      --targetVersion arg       compability for old mnn engine, default the\n",
      "                                same as converter\n",
      "      --customOpLibs arg        custom op libs ex: libmy_add.so;libmy_sub.so\n",
      "      --info                    dump MNN's model info\n",
      "      --authCode arg            code for model authentication.\n",
      "      --inputConfigFile arg     set input config file for static model, ex:\n",
      "                                ~/config.txt\n",
      "      --testdir arg             set test dir, mnn will convert model and then\n",
      "                                check the result\n",
      "      --testconfig arg          set test config json, example:\n",
      "                                tools/converter/forward.json\n",
      "      --thredhold arg           if set test dir, thredhold mean the max rate\n",
      "                                permit for run MNN model and origin error\n",
      "      --JsonFile arg            if input model is MNN and give jsonfile,\n",
      "                                while Dump MNN model to the JsonFile.\n",
      "      --alignDenormalizedValue arg\n",
      "                                if 1, converter would align denormalized\n",
      "                                float(|x| < 1.18e-38) as zero, because of in\n",
      "                                ubuntu/protobuf or android/flatbuf, system\n",
      "                                behaviors are different. default: 1, range: {0, 1}\n",
      "      --detectSparseSpeedUp arg\n",
      "                                if 1 converter would detect weights sparsity\n",
      "                                and check sparse speedup. default: 1, range :\n",
      "                                {0, 1}\n",
      "      --saveExternalData        save weight to extenal bin file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hw.cpufamily: 458787763 , size = 4\n",
      "The device support i8sdot:1, support fp16:1, support i8mm: 0\n",
      "Start to Convert Other Model Format To MNN Model..., target version: 2.8\n",
      "[18:00:21] :46: ONNX Model ir version: 8\n",
      "[18:00:21] :47: ONNX Model opset version: 18\n",
      "Start to Optimize the MNN Net...\n",
      "The Convolution use shared weight, may increase the model size\n",
      "Shape of NonMaxSupression's input is unknown. Please confirm version of MNN engine is new enough and use V3 Module API to run it correctly\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ output_0, output_1, output_2, output_3, ]\n",
      "Converted Success!\n"
     ]
    }
   ],
   "source": [
    "!mnnconvert -f ONNX --modelFile ./od_tf.onnx --optimizeLevel 1 --optimizePrefer 2 --fp16 --weightQuantBits 8 --weightQuantAsymmetric --bizCode od_tf --alignDenormalizedValue 1 --detectSparseSpeedUp 1 --MNNModel ./od_tf.mnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=\"/Users/arif/Documents/Pet_Feeder/Software/AI/cat_feeder_project/od_tf2/efficientdet-d5_frozen.pb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m cProfile -s time \"/Users/arif/Documents/Pet_Feeder/Software/AI/cat_feeder_project/cv_bb_prod_final_rpi.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 ./deploy/ONNX/export_onnx.py \\\n",
    "--weights /Users/arif/Downloads/yolov6l6.pt \\\n",
    "    --img 1600 \\\n",
    "    --batch 1 \\\n",
    "    --end2end \\\n",
    "    --ort \\\n",
    "--inplace --simplify --topk-all 10 --conf-thres 0.4 --with-preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 ./deploy/ONNX/export_onnx.py \\\n",
    "--weights /Users/arif/Downloads/yolov6lite_l.pt \\\n",
    "    --img 320 \\\n",
    "    --batch 1 \\\n",
    "    --end2end \\\n",
    "    --ort \\\n",
    "--inplace --simplify --topk-all 10 --conf-thres 0.4 --with-preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 ./export.py --weights /Users/arif/Downloads/yolov7.pt --grid --end2end --simplify\\\n",
    "        --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --max-wh 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from tqdm import tqdm\n",
    "\n",
    "from easyfsl.samplers import TaskSampler\n",
    "\n",
    "import MNN.nn as nn\n",
    "import MNN.cv as cv2\n",
    "import MNN.numpy as np\n",
    "import MNN.expr as expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image dtype.uint8 [959, 884, 3] 84.82652282714844 46.131614685058594\n",
      "image dtype.float [1, 3, 959, 884] 84.84130859375 46.14801025390625\n",
      "array([  10,   32, 1549, 1544], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_mnn(image):\n",
    "    # mean = np.asarray([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    # std = np.asarray([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # print(\"image cvt\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    # image = cv2.resize(image, (640, 640), interpolation=cv2.INTER_CUBIC)\n",
    "    # print(\"image rsz\", image.dtype, image.shape, image.mean(), image.std())\n",
    "\n",
    "    # w = h = 300\n",
    "    # y, x = (image.shape[0] - h) // 2, (image.shape[1] - w) // 2\n",
    "    # image = image[y:y + h, x:x + w]\n",
    "\n",
    "    # image = image.astype(np.float32)\n",
    "    image = expr.cast(image)\n",
    "    # image = image.astype(np.float32) / 255.0\n",
    "    # print(\"image 255\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    # image = (image - mean) / std\n",
    "    # print(\"image norm\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    image = expr.expand_dims(image, 0)\n",
    "    image = expr.convert(image, expr.NC4HW4)\n",
    "    return image\n",
    "\n",
    "\n",
    "image = cv2.imread(\"CatDataset/support/jico/lp_image.jpeg\", cv2.IMREAD_COLOR)\n",
    "# image = cv2.imread(\"CatDataset/support/jico/lp_image.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "# image = cv2.imread(\"/Users/arif/Downloads/beignets-task-guide.png\", cv2.IMREAD_GRAYSCALE)\n",
    "print(\"image\", image.dtype, image.shape, image.mean(), image.std())\n",
    "image = preprocess_mnn(image)\n",
    "\n",
    "print(\"image\", image.dtype, image.shape, image.mean(), image.std())\n",
    "\n",
    "# inputTensors = []\n",
    "inputTensors = [\"images\"]\n",
    "# outputTensors = [ '/end2end/TopK_output_0', 'det_boxes', 'det_classes', 'num_dets']\n",
    "# outputTensors = [ 'det_boxes', 'det_classes', 'det_scores', 'num_dets']\n",
    "# outputTensors = [ 'num_dets', 'det_boxes', 'det_scores', 'det_classes']\n",
    "outputTensors = [\"det_box_score\"]\n",
    "outputTensors = []\n",
    "# , runtime_manager=rt\n",
    "net = nn.load_module_from_file(\"./yolov6n_opt.mnn\", inputTensors, outputTensors)\n",
    "# net = nn.load_module_from_file('./full_dynamic_opt.mnn', ['image'], ['embeddings'])\n",
    "r = net.forward(image)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image dtype.uint8 [959, 884, 3] 84.82652282714844 46.131614685058594\n",
      "image dtype.uint8 [1, 3, 959, 884] 84.84130859375 46.14801025390625\n",
      "array([[37., 37., 37., 37., 77., 77., 37., 34., 37., 37.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# def pad_to_square(image, color=(114, 114, 114)):\n",
    "#     \"\"\"Pad an image to a square shape with the specified color.\n",
    "\n",
    "#     Args:\n",
    "#         image (numpy.ndarray): The input image.\n",
    "#         color (tuple): The color for padding (B, G, R).\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: The padded image.\n",
    "#     \"\"\"\n",
    "#     # Get the image height and width\n",
    "#     h, w = image.shape[:2]\n",
    "\n",
    "#     # Determine the size for the square (the max of height and width)\n",
    "#     square_size = max(h, w)\n",
    "\n",
    "#     # Calculate the padding for height and width\n",
    "#     top_pad = (square_size - h) // 2\n",
    "#     bottom_pad = square_size - h - top_pad\n",
    "#     left_pad = (square_size - w) // 2\n",
    "#     right_pad = square_size - w - left_pad\n",
    "\n",
    "#     # Pad the image\n",
    "#     padded_image = cv2.copyMakeBorder(\n",
    "#         image,\n",
    "#         top_pad,\n",
    "#         bottom_pad,\n",
    "#         left_pad,\n",
    "#         right_pad,\n",
    "#         cv2.BORDER_CONSTANT,\n",
    "#         value=color,\n",
    "#     )\n",
    "\n",
    "#     return padded_image\n",
    "\n",
    "\n",
    "def preprocess_mnn(image):\n",
    "    # mean = np.asarray([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    # std = np.asarray([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # print(\"image cvt\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    # image = cv2.resize(image, (1280, 1280), interpolation=cv2.INTER_CUBIC)\n",
    "    # print(\"image rsz\", image.dtype, image.shape, image.mean(), image.std())\n",
    "\n",
    "    # w = h = 300\n",
    "    # y, x = (image.shape[0] - h) // 2, (image.shape[1] - w) // 2\n",
    "    # image = image[y:y + h, x:x + w]\n",
    "\n",
    "    # image = image.astype(np.float32)\n",
    "    # image = expr.cast(image)\n",
    "    # image = image.astype(np.float32) / 255.0\n",
    "    # print(\"image 255\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    # image = (image - mean) / std\n",
    "    # print(\"image norm\", image.dtype, image.shape, image.mean(), image.std())\n",
    "    image = expr.expand_dims(image, 0)\n",
    "    image = expr.convert(image, expr.NC4HW4)\n",
    "    return image\n",
    "\n",
    "\n",
    "image = cv2.imread(\"CatDataset/support/jico/lp_image.jpeg\", cv2.IMREAD_COLOR)\n",
    "# image = cv2.imread(\"CatDataset/support/jico/lp_image.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "# image = cv2.imread(\"/Users/arif/Downloads/beignets-task-guide.png\", cv2.IMREAD_GRAYSCALE)\n",
    "print(\"image\", image.dtype, image.shape, image.mean(), image.std())\n",
    "image = preprocess_mnn(image)\n",
    "\n",
    "print(\"image\", image.dtype, image.shape, image.mean(), image.std())\n",
    "\n",
    "inputTensors = []\n",
    "# inputTensors = [ 'images' ]\n",
    "# outputTensors = [ '/end2end/TopK_output_0', 'det_boxes', 'det_classes', 'num_dets']\n",
    "# outputTensors = [ 'det_boxes', 'det_classes', 'det_scores', 'num_dets']\n",
    "# outputTensors = [ 'num_dets', 'det_boxes', 'det_scores', 'det_classes']\n",
    "# outputTensors = [ 'det_box_score']\n",
    "outputTensors = [\"output_2\"]\n",
    "# , runtime_manager=rt\n",
    "net = nn.load_module_from_file(\"./od_tf.mnn\", inputTensors, outputTensors)\n",
    "# net = nn.load_module_from_file('./full_dynamic_opt.mnn', ['image'], ['embeddings'])\n",
    "r = net.forward(image)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python3 ./deploy/ONNX/export_onnx.py \\\n",
    "--weights /Users/arif/Downloads/yolov6n.pt \\\n",
    "    --img 1152 \\\n",
    "    --batch 1 \\\n",
    "    --end2end \\\n",
    "    --ort \\\n",
    "--inplace --simplify --topk-all 10 --conf-thres 0.5 --with-preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  4.7543335,   3.0682983,   0.       ],\n",
       "        [ 14.586975 ,   6.470459 ,   0.       ],\n",
       "        [639.354    , 636.8819   ,   0.       ],\n",
       "        [637.2987   , 636.2532   ,   0.       ]]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocess(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unique_vars(var_array):\n",
    "#     # print(\"var_array\", var_array)\n",
    "#     unique_set = {var_array[i] for i in range(len(var_array))}\n",
    "#     return unique_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = OxfordIIITPet(\n",
    "    root=\"/Users/arif/Downloads/data\",\n",
    "    split=\"trainval\",\n",
    "    target_types=\"category\",\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "# test_set = OxfordIIITPet(\n",
    "#     root=\"/Users/arif/Downloads/data\",\n",
    "#     split= \"test\",\n",
    "#     target_types = \"category\",\n",
    "#     download=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNN use low precision\n"
     ]
    }
   ],
   "source": [
    "# # 配置执行后端，线程数，精度等信息；key-vlaue请查看API介绍\n",
    "config = {}\n",
    "config[\"precision\"] = \"low\"  # 当硬件支持（armv8.2）时使用fp16推理\n",
    "config[\"backend\"] = 0  # CPU\n",
    "config[\"numThread\"] = 8  # 线程数\n",
    "# config['memory_mode'] = 'high'\n",
    "# config['power_mode'] = 'high'\n",
    "\n",
    "rt = nn.create_runtime_manager((config,))\n",
    "# 加载模型创建_Module\n",
    "inputTensors = [\n",
    "    \"query_images\",\n",
    "    \"support_images\",\n",
    "]\n",
    "outputTensors = [\"predictions\"]\n",
    "\n",
    "net = nn.load_module_from_file(\n",
    "    \"./full_static_in_opt.mnn\", inputTensors, outputTensors, runtime_manager=rt\n",
    ")\n",
    "# inputTensors = [\n",
    "#     \"query_images\",\n",
    "#     \"support_images\",\n",
    "#     \"support_labels\",\n",
    "# ]\n",
    "# outputTensors = [\"predictions\"]\n",
    "\n",
    "# net = nn.load_module_from_file(\n",
    "#     \"./full_opt.mnn\", inputTensors, outputTensors, runtime_manager=rt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    # image = cv.resize(image, (260, 260), mean=[0.485, 0.456, 0.406], norm=[0.229, 0.224, 0.225])\n",
    "    # input_var = np.expand_dims(image, 0)\n",
    "    input_var = expr.convert(image, expr.NC4HW4)\n",
    "    return input_var\n",
    "\n",
    "\n",
    "def postprocess(output_var):\n",
    "    output_var = expr.convert(output_var, expr.NHWC)\n",
    "    return output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_mean_for_label(z_support, support_labels, label):\n",
    "#     # Select the elements of z_support corresponding to the current label and calculate their mean\n",
    "#     return np.mean(z_support[support_labels[support_labels == label]], axis=0)\n",
    "#     # return z_support[np.nonzero(support_labels == label)].mean(0)\n",
    "\n",
    "# def fsl_model(support_net, query_net, support_images, support_labels, query_images):\n",
    "#     # print(support_labels.shape, support_labels)\n",
    "#     support_images = preprocess(support_images)\n",
    "#     query_images = preprocess(query_images)\n",
    "#     # print(support_images.shape, query_images.shape)\n",
    "#     print(\"support_images\", support_images.mean(), support_images.std())\n",
    "#     print(\"query_images\", query_images.mean(), query_images.std())\n",
    "\n",
    "#     z_support = support_net.forward(support_images)\n",
    "#     z_query = query_net.forward(query_images)\n",
    "#     print(z_support.shape, z_query.shape)\n",
    "#     print(\"z_support\", z_support.mean(), z_support.std())\n",
    "#     print(\"z_query\", z_query.mean(), z_query.std())\n",
    "\n",
    "#     z_support = postprocess(z_support)\n",
    "#     z_query = postprocess(z_query)\n",
    "#     # print(z_support.shape, z_query.shape)\n",
    "\n",
    "#     # Infer the number of different classes from the labels of the support set\n",
    "#     # n_way = len(np2.unique(support_labels))\n",
    "#     n_way = len(unique_vars(support_labels))\n",
    "#     # print(n_way)\n",
    "\n",
    "#     z_proto = Parallel(n_jobs=1)(delayed(calculate_mean_for_label)(z_support, support_labels, label) for label in range(n_way))\n",
    "#     z_proto = np.stack(z_proto)\n",
    "#     # z_proto = np.stack(\n",
    "#     #     [\n",
    "#     #         # z_support[np.nonzero(support_labels == label)].mean(0)\n",
    "#     #         np.mean(z_support[np.nonzero(support_labels == label)], 0)\n",
    "#     #         for label in range(n_way)\n",
    "#     #     ]\n",
    "#     # )\n",
    "#     print(\"z_proto\", z_proto.mean(), z_proto.std())\n",
    "#     print(\"z_proto\", z_proto)\n",
    "#     # print(z_proto.shape)\n",
    "#     # Compute the euclidean distance from queries to prototypes\n",
    "#     # dists = cdist(z_query, z_proto)\n",
    "#     print(z_query.shape, z_proto.shape)\n",
    "#     dists = np.linalg.norm(z_query[:, np.newaxis] - z_proto, axis=2)\n",
    "#     # dists = np.sqrt(np.sum((z_query[:, np.newaxis, :] - z_proto[np.newaxis, :, :]) ** 2, axis=2))\n",
    "#     print(dists.shape)\n",
    "#     # return z_support, z_proto\n",
    "#     print(\"dists\", dists.mean(), dists.std())\n",
    "#     print(\"dists\", dists)\n",
    "#     # print(dists.shape)\n",
    "\n",
    "#     # And here is the super complicated operation to transform those distances into classification scores!\n",
    "#     scores = -dists\n",
    "#     print(\"scores\", scores.mean(), scores.std())\n",
    "#     print(\"scores\", scores)\n",
    "#     # print(scores.shape, scores)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY = 4  # Number of classes in a task\n",
    "N_SHOT = 8  # Number of images per class in the support set\n",
    "N_QUERY = 30  # Number of images per class in the query set\n",
    "N_EVALUATION_TASKS = 10\n",
    "\n",
    "# The sampler needs a dataset with a \"get_labels\" method. Check the code if you have any doubt!\n",
    "test_set.get_labels = lambda: test_set._labels\n",
    "test_sampler = TaskSampler(\n",
    "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:46<02:58, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)\n",
      "array([2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:55<01:52, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:04<01:19, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:13<00:59, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:29<00:52, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:40<00:38, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:49<00:22, 11.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:01<00:11, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:17<00:00, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tested on 10 tasks.\n",
      "Accuracy: 37.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_one_task(\n",
    "    support_images, support_labels, query_images, query_labels, n_way\n",
    "):\n",
    "    print(support_labels)\n",
    "    support_images = preprocess(support_images)\n",
    "    query_images = preprocess(query_images)\n",
    "    # [ x.149, x.1, onnx::Cast_1, ]\n",
    "    predictions = net.forward([query_images, support_images])[0]\n",
    "    # predictions = net.forward([query_images, support_images, support_labels])[0] OLDD\n",
    "    # print(\"predictions\", predictions)\n",
    "    # print(\"query_labels\", query_labels)\n",
    "    accuracy = np.sum(predictions == query_labels) / len(query_labels)\n",
    "\n",
    "    return accuracy * 100\n",
    "\n",
    "\n",
    "def evaluate(data_loader):\n",
    "    mean_accuracy = 0\n",
    "    for episode_index, (\n",
    "        support_images,\n",
    "        support_labels,\n",
    "        query_images,\n",
    "        query_labels,\n",
    "        class_ids,\n",
    "    ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        accuracy = evaluate_on_one_task(\n",
    "            support_images, support_labels, query_images, query_labels, len(class_ids)\n",
    "        )\n",
    "        # break\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    print(f\"Model tested on {len(data_loader)} tasks.\")\n",
    "    print(f\"Accuracy: {mean_accuracy / len(data_loader):.2f}%\")\n",
    "    # return mean_accuracy / len(data_loader)\n",
    "\n",
    "\n",
    "evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
