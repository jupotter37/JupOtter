{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08632d6c-ba25-4443-a7f5-c66e17d5a548",
   "metadata": {},
   "source": [
    "# Finetuning falcon-1b with Axolotl+QLoRA\n",
    "\n",
    "This notebook makes it easy to try out finetuning falcon-1b with Axolotl+QLoRA on LambdaLabs\n",
    "\n",
    "If you run into any issues, welcome to report [here](https://github.com/OpenAccess-AI-Collective/axolotl/pull/132) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcfde552-6451-40e5-aadb-f51063200240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEBIAN_FRONTEND=noninteractive\n"
     ]
    }
   ],
   "source": [
    "%env DEBIAN_FRONTEND=noninteractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e18117-72c3-40bb-b763-a119dd73e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdb879b-86d2-4dcb-ba95-934609150e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 23.0.1 from /home/ubuntu/.local/lib/python3.8/site-packages/pip (python 3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57475efd-98d3-416c-b2a0-6616b424876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3eac6b-da03-4fc9-b1a8-6b3cacec0f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libpython3.10-minimal libpython3.10-stdlib python3.10-minimal\n",
      "Suggested packages:\n",
      "  python3.10-venv binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  libpython3.10-minimal libpython3.10-stdlib python3.10 python3.10-minimal\n",
      "0 upgraded, 4 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 5225 kB of archives.\n",
      "After this operation, 20.2 MB of additional disk space will be used.\n",
      "Err:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.10-minimal amd64 3.10.10-1+focal1\n",
      "  404  Not Found [IP: 185.125.190.52 443]\n",
      "Err:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.10-minimal amd64 3.10.10-1+focal1\n",
      "  404  Not Found [IP: 185.125.190.52 443]\n",
      "Err:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.10-stdlib amd64 3.10.10-1+focal1\n",
      "  404  Not Found [IP: 185.125.190.52 443]\n",
      "Err:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.10 amd64 3.10.10-1+focal1\n",
      "  404  Not Found [IP: 185.125.190.52 443]\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/pool/main/p/python3.10/libpython3.10-minimal_3.10.10-1+focal1_amd64.deb  404  Not Found [IP: 185.125.190.52 443]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/pool/main/p/python3.10/python3.10-minimal_3.10.10-1+focal1_amd64.deb  404  Not Found [IP: 185.125.190.52 443]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/pool/main/p/python3.10/libpython3.10-stdlib_3.10.10-1+focal1_amd64.deb  404  Not Found [IP: 185.125.190.52 443]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mFailed to fetch https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu/pool/main/p/python3.10/python3.10_3.10.10-1+focal1_amd64.deb  404  Not Found [IP: 185.125.190.52 443]\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install -yq python3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f0f468-e2b2-49ff-873f-ad72e968553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libpython3.9-minimal libpython3.9-stdlib python3.9-minimal\n",
      "Suggested packages:\n",
      "  python3.9-venv binfmt-support\n",
      "The following NEW packages will be installed:\n",
      "  libpython3.9-minimal libpython3.9-stdlib python3.9 python3.9-minimal\n",
      "0 upgraded, 4 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 5023 kB of archives.\n",
      "After this operation, 19.7 MB of additional disk space will be used.\n",
      "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.9-minimal amd64 3.9.16-1+focal1 [804 kB]\n",
      "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.9-minimal amd64 3.9.16-1+focal1 [2062 kB]\n",
      "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.9-stdlib amd64 3.9.16-1+focal1 [1662 kB]\n",
      "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.9 amd64 3.9.16-1+focal1 [495 kB]\n",
      "Fetched 5023 kB in 6s (889 kB/s)\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libpython3.9-minimal:amd64.\n",
      "(Reading database ... 279084 files and directories currently installed.)\n",
      "Preparing to unpack .../libpython3.9-minimal_3.9.16-1+focal1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libpython3.9-minimal:amd64 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package python3.9-minimal.\n",
      "Preparing to unpack .../python3.9-minimal_3.9.16-1+focal1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking python3.9-minimal (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libpython3.9-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3.9-stdlib_3.9.16-1+focal1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libpython3.9-stdlib:amd64 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package python3.9.\n",
      "Preparing to unpack .../python3.9_3.9.16-1+focal1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking python3.9 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Setting up libpython3.9-minimal:amd64 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up python3.9-minimal (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libpython3.9-stdlib:amd64 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up python3.9 (3.9.16-1+focal1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[Jupdate-alternatives: using /usr/bin/python3.9 to provide /usr/bin/python (python) in auto mode\n",
      "There is only one alternative in link group python (providing /usr/bin/python): /usr/bin/python3.9\n",
      "Nothing to configure.\n",
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install -yq python3.9\n",
    "!sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.9 1\n",
    "!sudo update-alternatives --config python # pick 3.9 if given option\n",
    "!python -V # should be 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5141e6a-2525-477e-8daf-907a6db75f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-03 15:42:40--  https://bootstrap.pypa.io/get-pip.py\n",
      "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.192.175, 151.101.128.175, 151.101.64.175, ...\n",
      "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.192.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2578580 (2.5M) [text/x-python]\n",
      "Saving to: ‘get-pip.py’\n",
      "\n",
      "get-pip.py          100%[===================>]   2.46M  --.-KB/s    in 0.009s  \n",
      "\n",
      "2023-06-03 15:42:41 (284 MB/s) - ‘get-pip.py’ saved [2578580/2578580]\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "Successfully installed pip-23.1.2\n"
     ]
    }
   ],
   "source": [
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python get-pip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0e7bd4-4315-4454-b94a-01d7c7babe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3adbbfa2-f905-400f-901f-5c7cba0c6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                  1.13.1\n",
      "torchvision            0.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 list|grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e3859-017a-40c6-a3c3-d595e7ea5a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch) (2.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (2.10.1)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.local/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in ./.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04a4fbfe-721d-491a-983c-98c65fabb711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'axolotl'...\n",
      "remote: Enumerating objects: 2526, done.\u001b[K\n",
      "remote: Counting objects: 100% (1269/1269), done.\u001b[K\n",
      "remote: Compressing objects: 100% (385/385), done.\u001b[K\n",
      "remote: Total 2526 (delta 923), reused 1100 (delta 818), pack-reused 1257\u001b[K\n",
      "Receiving objects: 100% (2526/2526), 1.30 MiB | 12.59 MiB/s, done.\n",
      "Resolving deltas: 100% (1562/1562), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenAccess-AI-Collective/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933d68aa-e5b7-4234-bee1-3054f3fb467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/axolotl\n"
     ]
    }
   ],
   "source": [
    "%cd axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2441ccfb-18b7-45ed-9c0f-a8faa34b7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/ubuntu/axolotl\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-mt8imf30/transformers_e7d69891845841ad93145f052bc3f2f5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-mt8imf30/transformers_e7d69891845841ad93145f052bc3f2f5\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 539e2281cd97c35ef4122757f26c88f44115fa94\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting PyYAML==6.0 (from axolotl==0.1)\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.8/661.8 kB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate (from axolotl==0.1)\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting addict (from axolotl==0.1)\n",
      "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting bert-score==0.3.13 (from axolotl==0.1)\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes>=0.39.0 (from axolotl==0.1)\n",
      "  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from axolotl==0.1)\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting einops (from axolotl==0.1)\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate==0.4.0 (from axolotl==0.1)\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fire (from axolotl==0.1)\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-score==0.1.2 (from axolotl==0.1)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn==1.2.2 (from axolotl==0.1)\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m153.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/lib/python3/dist-packages (from axolotl==0.1) (1.3.3)\n",
      "Collecting sentencepiece (from axolotl==0.1)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb (from axolotl==0.1)\n",
      "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xformers (from axolotl==0.1)\n",
      "  Downloading xformers-0.0.20-cp39-cp39-manylinux2014_x86_64.whl (109.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.1+cu118)\n",
      "Collecting pandas>=1.0.1 (from bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading pandas-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m175.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/lib/python3/dist-packages (from bert-score==0.3.13->axolotl==0.1) (1.17.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from bert-score==0.3.13->axolotl==0.1) (2.22.0)\n",
      "Collecting tqdm>=4.31.1 (from bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from bert-score==0.3.13->axolotl==0.1) (3.1.2)\n",
      "Collecting packaging>=20.9 (from bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0 (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19 (from evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: absl-py in /usr/lib/python3/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (0.15.0)\n",
      "Collecting nltk (from rouge-score==0.1.2->axolotl==0.1)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.14.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.2.2->axolotl==0.1)\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.2.2->axolotl==0.1)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate->axolotl==0.1) (5.5.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->axolotl==0.1)\n",
      "  Downloading pyarrow-12.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->axolotl==0.1)\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.0.12)\n",
      "Collecting regex!=2019.12.17 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
      "  Downloading regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m174.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor in /usr/lib/python3/dist-packages (from fire->axolotl==0.1) (1.1.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (7.0)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl==0.1)\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.1)\n",
      "  Downloading sentry_sdk-1.25.0-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.1)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools (from wandb->axolotl==0.1)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle (from wandb->axolotl==0.1)\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (45.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (1.4.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (3.10.0.2)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.15.0 (from wandb->axolotl==0.1)\n",
      "  Downloading protobuf-4.23.2-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyre-extensions==0.0.29 (from xformers->axolotl==0.1)\n",
      "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
      "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.10.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (15.0.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets->axolotl==0.1) (19.3.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->axolotl==0.1)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from bert-score==0.3.13->axolotl==0.1)\n",
      "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting urllib3>=1.25.10 (from responses<0.19->evaluate==0.4.0->axolotl==0.1)\n",
      "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from sentry-sdk>=1.0.0->wandb->axolotl==0.1) (2019.11.28)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets->axolotl==0.1) (2.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.2.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: rouge-score, transformers, fire, pathtools\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=8648fe266bbf97c65a9f3e675f9f094ad480623064e182ca46be38642a389e61\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7160079 sha256=d907e1ad91a8145e286873787b759255627d08029167f9092958066a94b6567c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p71kyqo1/wheels/f7/92/8c/752ff3bfcd3439805d8bbf641614da38ef3226e127ebea86ee\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=a8b9f22c736b704dd34da9a6ac343b725ac2b90031fd9930df10816437ac70cc\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=041d16a945a1361c1d8e2d5323368993d62f317ce7225d098485ab16338c9e2f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built rouge-score transformers fire pathtools\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pytz, pathtools, bitsandbytes, addict, xxhash, urllib3, tzdata, tqdm, threadpoolctl, smmap, setproctitle, regex, PyYAML, python-dateutil, protobuf, packaging, numpy, mypy-extensions, multidict, joblib, fsspec, frozenlist, fire, einops, docker-pycreds, dill, charset-normalizer, async-timeout, yarl, typing-inspect, sentry-sdk, scikit-learn, responses, pyarrow, pandas, nltk, multiprocess, huggingface-hub, gitdb, aiosignal, transformers, rouge-score, pyre-extensions, GitPython, aiohttp, wandb, datasets, evaluate, xformers, bert-score, accelerate, axolotl\n",
      "  Running setup.py develop for axolotl\n",
      "Successfully installed GitPython-3.1.31 PyYAML-6.0 accelerate-0.19.0 addict-2.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 axolotl bert-score-0.3.13 bitsandbytes-0.39.0 charset-normalizer-3.1.0 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.1 evaluate-0.4.0 fire-0.5.0 frozenlist-1.3.3 fsspec-2023.5.0 gitdb-4.0.10 huggingface-hub-0.15.1 joblib-1.2.0 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 nltk-3.8.1 numpy-1.24.3 packaging-23.1 pandas-2.0.2 pathtools-0.1.2 protobuf-4.23.2 pyarrow-12.0.0 pyre-extensions-0.0.29 python-dateutil-2.8.2 pytz-2023.3 regex-2023.5.5 responses-0.18.0 rouge-score-0.1.2 safetensors-0.3.1 scikit-learn-1.2.2 sentencepiece-0.1.99 sentry-sdk-1.25.0 setproctitle-1.3.2 smmap-5.0.0 threadpoolctl-3.1.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.0.dev0 typing-inspect-0.9.0 tzdata-2023.3 urllib3-2.0.2 wandb-0.15.3 xformers-0.0.20 xxhash-3.2.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bac4e4d-5e14-4011-bc1a-1de6406908d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/accelerate\", line 5, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/__init__.py\", line 3, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/accelerator.py\", line 39, in <module>\n",
      "    from .tracking import LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/tracking.py\", line 42, in <module>\n",
      "    from torch.utils import tensorboard\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py\", line 12, in <module>\n",
      "    from .writer import FileWriter, SummaryWriter  # noqa: F401\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\", line 9, in <module>\n",
      "    from tensorboard.compat.proto.event_pb2 import SessionLog\n",
      "  File \"/usr/lib/python3/dist-packages/tensorboard/compat/proto/event_pb2.py\", line 15, in <module>\n",
      "    from tensorboard.compat.proto import summary_pb2 as tensorboard_dot_compat_dot_proto_dot_summary__pb2\n",
      "  File \"/usr/lib/python3/dist-packages/tensorboard/compat/proto/summary_pb2.py\", line 15, in <module>\n",
      "    from tensorboard.compat.proto import histogram_pb2 as tensorboard_dot_compat_dot_proto_dot_histogram__pb2\n",
      "  File \"/usr/lib/python3/dist-packages/tensorboard/compat/proto/histogram_pb2.py\", line 34, in <module>\n",
      "    _descriptor.FieldDescriptor(\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/google/protobuf/descriptor.py\", line 561, in __new__\n",
      "    _message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n"
     ]
    }
   ],
   "source": [
    "!accelerate config --config_file configs/accelerate/default_config.yaml default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93397822-6165-4939-bcad-3055db9ff9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/axolotl\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffa82804-01a2-434b-9038-d88c0e1b2081",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/ubuntu/axolotl\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-48bo5v0h/transformers_137884897c0d4fd8ba70023bd1155185\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-48bo5v0h/transformers_137884897c0d4fd8ba70023bd1155185\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 539e2281cd97c35ef4122757f26c88f44115fa94\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML==6.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (6.0)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.19.0)\n",
      "Requirement already satisfied: addict in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (2.4.0)\n",
      "Requirement already satisfied: bert-score==0.3.13 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.3.13)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.39.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (2.12.0)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.6.1)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.4.0)\n",
      "Requirement already satisfied: fire in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.5.0)\n",
      "Requirement already satisfied: rouge-score==0.1.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from axolotl==0.1) (1.3.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.1.99)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.15.3)\n",
      "Requirement already satisfied: xformers in /home/ubuntu/.local/lib/python3.9/site-packages (from axolotl==0.1) (0.0.20)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.1+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.2)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (1.24.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from bert-score==0.3.13->axolotl==0.1) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from bert-score==0.3.13->axolotl==0.1) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (23.1)\n",
      "Requirement already satisfied: dill in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (2023.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.15.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/.local/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /usr/lib/python3/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (0.15.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (3.1.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate->axolotl==0.1) (5.5.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from datasets->axolotl==0.1) (12.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.9/site-packages (from datasets->axolotl==0.1) (3.8.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.3.1)\n",
      "Requirement already satisfied: termcolor in /usr/lib/python3/dist-packages (from fire->axolotl==0.1) (1.1.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (7.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (3.1.31)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.25.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (45.2.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (1.4.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from wandb->axolotl==0.1) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from wandb->axolotl==0.1) (4.23.2)\n",
      "Requirement already satisfied: pyre-extensions==0.0.29 in /home/ubuntu/.local/lib/python3.9/site-packages (from xformers->axolotl==0.1) (0.0.29)\n",
      "Requirement already satisfied: typing-inspect in /home/ubuntu/.local/lib/python3.9/site-packages (from pyre-extensions==0.0.29->xformers->axolotl==0.1) (0.9.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.10.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (15.0.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets->axolotl==0.1) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (4.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/ubuntu/.local/lib/python3.9/site-packages (from responses<0.19->evaluate==0.4.0->axolotl==0.1) (2.0.2)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from sentry-sdk>=1.0.0->wandb->axolotl==0.1) (2019.11.28)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (5.0.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets->axolotl==0.1) (2.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1) (1.0.0)\n",
      "Installing collected packages: axolotl\n",
      "  Attempting uninstall: axolotl\n",
      "    Found existing installation: axolotl 0.1\n",
      "    Uninstalling axolotl-0.1:\n",
      "      Successfully uninstalled axolotl-0.1\n",
      "  Running setup.py develop for axolotl\n",
      "Successfully installed axolotl\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.2\n",
      "    Uninstalling protobuf-4.23.2:\n",
      "      Successfully uninstalled protobuf-4.23.2\n",
      "Successfully installed protobuf-3.20.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.22.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.9/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from requests) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2019.11.28)\n",
      "Installing collected packages: requests\n",
      "Successfully installed requests-2.31.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psutil\n",
      "Successfully installed psutil-5.9.5\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (1.3.3)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/ubuntu/.local/lib/python3.9/site-packages (from scipy) (1.24.3)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.10.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-3755vabw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-3755vabw\n",
      "  Resolved https://github.com/huggingface/peft.git to commit fcff23f005fc7bfb816ad1f55360442c170cd5f5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (4.30.0.dev0)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.9/site-packages (from peft==0.4.0.dev0) (0.19.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.10.0.2)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.10.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/ubuntu/.local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (15.0.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.9/site-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2019.11.28)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.2.1)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=57784 sha256=537d985f2ccbf80c6a275d4257085a4225af3e6ddcbcde6bece95cb9c3f2f925\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zzasgnxp/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.4.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -e . # change depend on needs\n",
    "!pip3 install protobuf==3.20.3\n",
    "!pip3 install -U requests\n",
    "!pip3 install -U --ignore-installed psutil\n",
    "!pip3 install -U scipy\n",
    "!pip3 install git+https://github.com/huggingface/peft.git # not for gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "184e5d29-9baf-4ac5-870c-4a831539eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d265ff-1ffa-4ed6-9816-00c36247a8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu\n"
     ]
    }
   ],
   "source": [
    "%env LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f784d37-a9b3-4442-b08d-06c2e3e89d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate configuration saved at /home/ubuntu/.cache/huggingface/accelerate/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbc92a5c-a024-4f34-b642-d2f624db8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-03 15:51:47--  https://raw.githubusercontent.com/utensil/axolotl/falcon-7b-qlora/examples/falcon/config-7b-qlora.yml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2238 (2.2K) [text/plain]\n",
      "Saving to: ‘config-7b-qlora.yml’\n",
      "\n",
      "config-7b-qlora.yml 100%[===================>]   2.19K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-06-03 15:51:47 (102 MB/s) - ‘config-7b-qlora.yml’ saved [2238/2238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/utensil/axolotl/falcon-7b-qlora/examples/falcon/config-7b-qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dd4a7db-fb0b-40c4-b788-913f692870ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp config-7b-qlora.yml config-1b-qlora.yml\n",
    "!sed -i -e 's/falcon-7b/falcon-rw-1b/g' -e 's/wandb_project: falcon-qlora/wandb_project: /g' -e 's/bf16: true/bf16: false/g'  -e 's/tf32: true/tf32: false/g' config-1b-qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf48c28b-28ba-47d9-b5b1-c3a89b33d3b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1b: tiiuae/falcon-rw-1b\n",
      "# 40b: tiiuae/falcon-40b\n",
      "base_model: tiiuae/falcon-rw-1b\n",
      "base_model_config: tiiuae/falcon-rw-1b\n",
      "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-rw-1b/tree/main \n",
      "trust_remote_code: true\n",
      "model_type: AutoModelForCausalLM\n",
      "tokenizer_type: AutoTokenizer\n",
      "load_in_8bit: false\n",
      "# enable 4bit for QLoRA\n",
      "load_in_4bit: true\n",
      "gptq: false\n",
      "strict: false\n",
      "push_dataset_to_hub:\n",
      "datasets:\n",
      "  - path: QingyiSi/Alpaca-CoT\n",
      "    data_files:\n",
      "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
      "    type: \"alpaca:chat\"\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.01\n",
      "# enable QLoRA\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "sequence_len: 2048\n",
      "max_packed_sequence_len:\n",
      "\n",
      "# hyperparameters from QLoRA paper Appendix B.2\n",
      "# \"We find hyperparameters to be largely robust across datasets\"\n",
      "lora_r: 64\n",
      "lora_alpha: 16\n",
      "# 0.1 for models up to 13B\n",
      "# 0.05 for 33B and 65B models\n",
      "lora_dropout: 0.05\n",
      "# add LoRA modules on all linear layers of the base model\n",
      "lora_target_modules:\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "\n",
      "wandb_project: \n",
      "wandb_watch:\n",
      "wandb_run_id:\n",
      "wandb_log_model:\n",
      "output_dir: ./qlora-out\n",
      "\n",
      "# QLoRA paper Table 9\n",
      "# - 16 for 7b & 13b\n",
      "# - 32 for 33b, 64 for 64b\n",
      "# Max size tested on A6000\n",
      "# - 7b: 40\n",
      "# - 40b: 4\n",
      "# decrease if OOM, increase for max VRAM utilization\n",
      "micro_batch_size: 1\n",
      "gradient_accumulation_steps: 2\n",
      "num_epochs: 3\n",
      "# Optimizer for QLoRA\n",
      "optimizer: paged_adamw_32bit\n",
      "torchdistx_path:\n",
      "lr_scheduler: cosine\n",
      "# QLoRA paper Table 9\n",
      "# - 2e-4 for 7b & 13b\n",
      "# - 1e-4 for 33b & 64b\n",
      "learning_rate: 0.0002\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: false\n",
      "fp16: false\n",
      "tf32: false\n",
      "gradient_checkpointing: true\n",
      "# stop training after this many evaluation losses have increased in a row\n",
      "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
      "early_stopping_patience: 3\n",
      "resume_from_checkpoint:\n",
      "auto_resume_from_checkpoints: true\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention: true\n",
      "flash_attention:\n",
      "gptq_groupsize:\n",
      "gptq_model_v1:\n",
      "warmup_steps: 10\n",
      "eval_steps: 5\n",
      "save_steps: 10\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.000001\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  pad_token: \"<|endoftext|>\"\n",
      "  bos_token: \">>ABSTRACT<<\"\n",
      "  eos_token: \"<|endoftext|>\"\n"
     ]
    }
   ],
   "source": [
    "!cat config-1b-qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7155f96-5dbc-486b-8446-6b986acb16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=offline\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_MODE=offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92146bda-52fa-4f31-8160-23b4c7e40c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/lib/x86_64-linux-gnu/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ubuntu/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
      "INFO:root:loading tokenizer... tiiuae/falcon-rw-1b\n",
      "INFO:filelock:Lock 139620631419392 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/31a4ebf2104713af5922ab75c18e61597a6f076e.lock\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 255/255 [00:00<00:00, 63.1kB/s]\n",
      "INFO:filelock:Lock 139620631419392 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/31a4ebf2104713af5922ab75c18e61597a6f076e.lock\n",
      "INFO:filelock:Lock 139620631419152 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 798k/798k [00:00<00:00, 3.37MB/s]\n",
      "INFO:filelock:Lock 139620631419152 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock\n",
      "INFO:filelock:Lock 139620631419392 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/6636bda4a1fd7a63653dffb22683b8162c8de956.lock\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 2.57MB/s]\n",
      "INFO:filelock:Lock 139620631419392 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/6636bda4a1fd7a63653dffb22683b8162c8de956.lock\n",
      "INFO:filelock:Lock 139620631419152 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/6dc5e94ca994de07d374e33698489de9cff48a02.lock\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 2.11M/2.11M [00:00<00:00, 11.6MB/s]\n",
      "INFO:filelock:Lock 139620631419152 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/6dc5e94ca994de07d374e33698489de9cff48a02.lock\n",
      "INFO:filelock:Lock 139620631419392 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6.lock\n",
      "Downloading (…)cial_tokens_map.json: 100%|████| 99.0/99.0 [00:00<00:00, 146kB/s]\n",
      "INFO:filelock:Lock 139620631419392 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6.lock\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Unable to find prepared dataset in last_run_prepared/0ecc5b78e3ce4254b22e749b093712b4\n",
      "INFO:root:Loading raw datasets...\n",
      "Downloading readme: 100%|██████████████████| 8.26k/8.26k [00:00<00:00, 31.1MB/s]\n",
      "Downloading and preparing dataset json/QingyiSi--Alpaca-CoT to /home/ubuntu/.cache/huggingface/datasets/QingyiSi___json/QingyiSi--Alpaca-CoT-2953efcfeb19f105/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|█████████████████████| 4.45M/4.45M [00:00<00:00, 116MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2560.63it/s]\n",
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/QingyiSi___json/QingyiSi--Alpaca-CoT-2953efcfeb19f105/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 983.89it/s]\n",
      "INFO:root:tokenizing, merging, and shuffling master dataset\n",
      "INFO:root:Saving merged prepared dataset to disk... last_run_prepared/0ecc5b78e3ce4254b22e749b093712b4\n",
      "INFO:root:loading model and peft_config...                                      \n",
      "INFO:filelock:Lock 139620622432576 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/c6e815d7380adbb2b091fd7bc7580d72dee0bc03.lock\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 665/665 [00:00<00:00, 788kB/s]\n",
      "INFO:filelock:Lock 139620622432576 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/c6e815d7380adbb2b091fd7bc7580d72dee0bc03.lock\n",
      "INFO:filelock:Lock 139621809190704 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/ab845c0d34462b3b3391d3b8119c11816e9c02ce.lock\n",
      "Downloading (…)/configuration_RW.py: 100%|█| 2.61k/2.61k [00:00<00:00, 3.72MB/s]\n",
      "INFO:filelock:Lock 139621809190704 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/ab845c0d34462b3b3391d3b8119c11816e9c02ce.lock\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- configuration_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "INFO:filelock:Lock 139625672846544 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/3e4e76c47352197b871178c8c508bc8c5942a2f8.lock\n",
      "Downloading (…)main/modelling_RW.py: 100%|█| 47.5k/47.5k [00:00<00:00, 67.3MB/s]\n",
      "INFO:filelock:Lock 139625672846544 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/3e4e76c47352197b871178c8c508bc8c5942a2f8.lock\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- modelling_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "INFO:filelock:Lock 139621809986576 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/3a0d68f0309c8f7ec913f51edf8bf2eca849477c9f53db727f49ffa2f6019251.lock\n",
      "Downloading pytorch_model.bin: 100%|████████| 2.62G/2.62G [00:13<00:00, 188MB/s]\n",
      "INFO:filelock:Lock 139621809986576 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/3a0d68f0309c8f7ec913f51edf8bf2eca849477c9f53db727f49ffa2f6019251.lock\n",
      "INFO:filelock:Lock 139621809191040 acquired on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/2cb1ddd9ee691eb96ad0baa19b67447cd7be0275.lock\n",
      "Downloading (…)neration_config.json: 100%|██████| 111/111 [00:00<00:00, 133kB/s]\n",
      "INFO:filelock:Lock 139621809191040 released on /home/ubuntu/.cache/huggingface/hub/models--tiiuae--falcon-rw-1b/blobs/2cb1ddd9ee691eb96ad0baa19b67447cd7be0275.lock\n",
      "INFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "INFO:root:found linear modules: ['dense_4h_to_h', 'query_key_value', 'dense_h_to_4h', 'dense']\n",
      "trainable params: 50331648 || all params: 757911552 || trainable%: 6.6408339953630895\n",
      "INFO:root:Compiling torch model\n",
      "INFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/__main__.py\", line 1, in <module>\n",
      "    from wandb.cli import cli\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/cli/cli.py\", line 933, in <module>\n",
      "    def launch_sweep(\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 1234, in decorator\n",
      "    cmd = command(*args, **kwargs)(f)\n",
      "  File \"/usr/lib/python3/dist-packages/click/decorators.py\", line 115, in decorator\n",
      "    cmd = _make_command(f, name, attrs, cls)\n",
      "  File \"/usr/lib/python3/dist-packages/click/decorators.py\", line 88, in _make_command\n",
      "    return cls(name=name or f.__name__.lower().replace('_', '-'),\n",
      "TypeError: __init__() got an unexpected keyword argument 'no_args_is_help'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/axolotl/scripts/finetune.py\", line 295, in <module>\n",
      "    fire.Fire(train)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/home/ubuntu/axolotl/scripts/finetune.py\", line 282, in train\n",
      "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 1661, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/trainer.py\", line 1870, in _inner_training_loop\n",
      "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/trainer_callback.py\", line 353, in on_train_begin\n",
      "    return self.call_event(\"on_train_begin\", args, state, control)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/trainer_callback.py\", line 397, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/integrations.py\", line 774, in on_train_begin\n",
      "    self.setup(args, state, model, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/transformers/integrations.py\", line 748, in setup\n",
      "    self._wandb.init(\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1169, in init\n",
      "    raise e\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1146, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 172, in setup\n",
      "    self._wl = wandb_setup.setup(settings=setup_settings)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 327, in setup\n",
      "    ret = _setup(settings=settings)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 320, in _setup\n",
      "    wl = _WandbSetup(settings=settings)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 303, in __init__\n",
      "    _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 114, in __init__\n",
      "    self._setup()\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 250, in _setup\n",
      "    self._setup_manager()\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_setup.py\", line 277, in _setup_manager\n",
      "    self._manager = wandb_manager._Manager(settings=self._settings)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_manager.py\", line 145, in __init__\n",
      "    self._service.start()\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/service/service.py\", line 199, in start\n",
      "    self._launch_server()\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/service/service.py\", line 193, in _launch_server\n",
      "    _sentry.reraise(e)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/analytics/sentry.py\", line 146, in reraise\n",
      "    raise exc.with_traceback(sys.exc_info()[2])\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/service/service.py\", line 191, in _launch_server\n",
      "    self._wait_for_ports(fname, proc=internal_proc)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/wandb/sdk/service/service.py\", line 116, in _wait_for_ports\n",
      "    raise ServiceStartProcessError(\n",
      "wandb.sdk.service.service.ServiceStartProcessError: The wandb service process exited with 1. Ensure that `sys.executable` is a valid python interpreter. You can override it with the `_executable` setting or with the `WANDB__EXECUTABLE` environment variable.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 918, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 580, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', 'scripts/finetune.py', 'config-1b-qlora.yml']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py config-1b-qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "246171b3-c49f-4cbe-8eeb-3eff5f581226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (7.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: click\n",
      "Successfully installed click-8.1.3\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/explosion/spaCy/issues/7160\n",
    "!pip3 install click --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e0516c-ab96-4620-abf2-79bd2b8a8d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ubuntu/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/lib/x86_64-linux-gnu/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ubuntu/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
      "INFO:root:loading tokenizer... tiiuae/falcon-rw-1b\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Loading prepared dataset from disk at last_run_prepared/0ecc5b78e3ce4254b22e749b093712b4...\n",
      "INFO:root:Prepared dataset loaded from disk...\n",
      "INFO:root:loading model and peft_config...\n",
      "INFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "INFO:root:found linear modules: ['dense_h_to_4h', 'dense_4h_to_h', 'dense', 'query_key_value']\n",
      "trainable params: 50331648 || all params: 757911552 || trainable%: 6.6408339953630895\n",
      "INFO:root:Compiling torch model\n",
      "INFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 1.858, 'learning_rate': 2e-05, 'epoch': 0.0}                           \n",
      "{'loss': 1.6529, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
      "{'loss': 1.642, 'learning_rate': 6e-05, 'epoch': 0.0}                           \n",
      "{'loss': 1.7997, 'learning_rate': 8e-05, 'epoch': 0.0}                          \n",
      "{'loss': 1.7844, 'learning_rate': 0.0001, 'epoch': 0.0}                         \n",
      "  0%|                                       | 5/11097 [00:03<2:01:58,  1.52it/s]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.01it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.86it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.59it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.74it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:04, 12.87it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.49it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.63it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 12.07it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.96it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.58it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.23it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 12.15it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.76it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.74it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.67it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.35it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.50it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.52it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.56it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.97it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.53it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:03<00:02, 11.00it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.99it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.42it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:01, 10.53it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 11.07it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:04<00:01, 11.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.27it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 11.00it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:00, 11.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.62it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.88it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:05<00:00, 12.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 12.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.92901611328125, 'eval_runtime': 6.4241, 'eval_samples_per_second': 11.675, 'eval_steps_per_second': 11.675, 'epoch': 0.0}\n",
      "  0%|                                       | 5/11097 [00:10<2:01:58,  1.52it/s]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 12.00it/s]\u001b[A\n",
      "{'loss': 2.3614, 'learning_rate': 0.00012, 'epoch': 0.0}                        \u001b[A\n",
      "{'loss': 2.0078, 'learning_rate': 0.00014, 'epoch': 0.0}                        \n",
      "{'loss': 1.4112, 'learning_rate': 0.00016, 'epoch': 0.0}                        \n",
      "{'loss': 1.7171, 'learning_rate': 0.00018, 'epoch': 0.0}                        \n",
      "{'loss': 1.4504, 'learning_rate': 0.0002, 'epoch': 0.0}                         \n",
      "  0%|                                      | 10/11097 [00:13<3:13:53,  1.05s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.15it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.71it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.46it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.58it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:04, 12.66it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.36it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.31it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.55it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 12.03it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.44it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.17it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 11.99it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.65it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.63it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.55it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.24it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.35it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.35it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.37it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.77it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.37it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.84it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.87it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.30it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.39it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.95it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:04<00:01, 10.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.24it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.99it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:00, 11.05it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.65it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.90it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:05<00:00, 12.25it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 12.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6972100734710693, 'eval_runtime': 6.4667, 'eval_samples_per_second': 11.598, 'eval_steps_per_second': 11.598, 'epoch': 0.0}\n",
      "  0%|                                      | 10/11097 [00:19<3:13:53,  1.05s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 12.06it/s]\u001b[A\n",
      "{'loss': 1.5844, 'learning_rate': 0.00019999999598540582, 'epoch': 0.0}         \u001b[A\n",
      "{'loss': 1.3097, 'learning_rate': 0.00019999998394162357, 'epoch': 0.0}         \n",
      "{'loss': 2.4036, 'learning_rate': 0.00019999996386865424, 'epoch': 0.0}         \n",
      "{'loss': 1.8435, 'learning_rate': 0.0001999999357664994, 'epoch': 0.0}          \n",
      "{'loss': 2.3936, 'learning_rate': 0.00019999989963516136, 'epoch': 0.0}         \n",
      "  0%|                                      | 15/11097 [00:24<3:52:23,  1.26s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.38it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.74it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.46it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.62it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:04, 12.76it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.45it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.30it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.57it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.98it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.49it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:03, 12.26it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 12.03it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.70it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.70it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.59it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.26it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.38it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.40it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.42it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.80it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.40it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.84it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.83it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.42it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.97it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:04<00:01, 10.95it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.18it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.96it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:00, 11.04it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.55it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.81it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:05<00:00, 12.14it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 12.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5616062879562378, 'eval_runtime': 6.4677, 'eval_samples_per_second': 11.596, 'eval_steps_per_second': 11.596, 'epoch': 0.0}\n",
      "  0%|                                      | 15/11097 [00:30<3:52:23,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.97it/s]\u001b[A\n",
      "{'loss': 1.1337, 'learning_rate': 0.000199999855474643, 'epoch': 0.0}           \u001b[A\n",
      "{'loss': 1.6135, 'learning_rate': 0.00019999980328494788, 'epoch': 0.0}         \n",
      "{'loss': 1.7764, 'learning_rate': 0.00019999974306608012, 'epoch': 0.0}         \n",
      "{'loss': 1.2131, 'learning_rate': 0.00019999967481804467, 'epoch': 0.01}        \n",
      "{'loss': 1.5954, 'learning_rate': 0.00019999959854084692, 'epoch': 0.01}        \n",
      "  0%|                                      | 20/11097 [00:33<3:26:42,  1.12s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.16it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.70it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.45it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.62it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:04, 12.65it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.34it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.53it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.99it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.85it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.44it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.16it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 12.00it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.62it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.62it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.20it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.32it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.31it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.35it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.75it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.29it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.79it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.79it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.24it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.34it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.91it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:04<00:01, 10.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.11it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.87it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.91it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.49it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.76it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:05<00:00, 12.13it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4697062969207764, 'eval_runtime': 6.5029, 'eval_samples_per_second': 11.533, 'eval_steps_per_second': 11.533, 'epoch': 0.01}\n",
      "  0%|                                      | 20/11097 [00:40<3:26:42,  1.12s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.91it/s]\u001b[A\n",
      "{'loss': 1.4434, 'learning_rate': 0.00019999951423449303, 'epoch': 0.01}        \u001b[A\n",
      "{'loss': 1.276, 'learning_rate': 0.00019999942189898975, 'epoch': 0.01}         \n",
      "{'loss': 1.5983, 'learning_rate': 0.00019999932153434453, 'epoch': 0.01}        \n",
      "{'loss': 1.2556, 'learning_rate': 0.00019999921314056539, 'epoch': 0.01}        \n",
      "{'loss': 1.2912, 'learning_rate': 0.00019999909671766103, 'epoch': 0.01}        \n",
      "  0%|                                      | 25/11097 [00:44<3:39:53,  1.19s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.06it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.67it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.38it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.53it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.59it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.31it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.23it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.45it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.88it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.35it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.08it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 11.89it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.52it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.53it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.41it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.11it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.24it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.27it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.26it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.66it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.23it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.72it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.76it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.27it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.81it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.09it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.81it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.86it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.44it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.68it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.99it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4360172748565674, 'eval_runtime': 6.5483, 'eval_samples_per_second': 11.453, 'eval_steps_per_second': 11.453, 'epoch': 0.01}\n",
      "  0%|                                      | 25/11097 [00:50<3:39:53,  1.19s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.80it/s]\u001b[A\n",
      "{'loss': 1.4833, 'learning_rate': 0.00019999897226564083, 'epoch': 0.01}        \u001b[A\n",
      "{'loss': 1.5938, 'learning_rate': 0.0001999988397845148, 'epoch': 0.01}         \n",
      "{'loss': 1.4827, 'learning_rate': 0.00019999869927429353, 'epoch': 0.01}        \n",
      "{'loss': 1.195, 'learning_rate': 0.0001999985507349883, 'epoch': 0.01}          \n",
      "{'loss': 1.2247, 'learning_rate': 0.00019999839416661106, 'epoch': 0.01}        \n",
      "  0%|                                      | 30/11097 [00:53<3:36:20,  1.17s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.00it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.47it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.31it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.44it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:04, 12.60it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.31it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.43it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.90it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.82it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.44it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.17it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 11.92it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.60it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.57it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:02<00:03, 11.19it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.32it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.31it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.31it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.67it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.26it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.75it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.76it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.18it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.29it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.84it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:04<00:01, 10.86it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.14it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.87it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.90it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.45it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.70it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 12.04it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3883757591247559, 'eval_runtime': 6.532, 'eval_samples_per_second': 11.482, 'eval_steps_per_second': 11.482, 'epoch': 0.01}\n",
      "  0%|                                      | 30/11097 [01:00<3:36:20,  1.17s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.84it/s]\u001b[A\n",
      "{'loss': 1.0079, 'learning_rate': 0.00019999822956917439, 'epoch': 0.01}        \u001b[A\n",
      "{'loss': 1.6677, 'learning_rate': 0.00019999805694269145, 'epoch': 0.01}        \n",
      "{'loss': 1.4063, 'learning_rate': 0.0001999978762871762, 'epoch': 0.01}         \n",
      "{'loss': 1.9272, 'learning_rate': 0.00019999768760264303, 'epoch': 0.01}        \n",
      "{'loss': 1.1411, 'learning_rate': 0.00019999749088910718, 'epoch': 0.01}        \n",
      "  0%|                                      | 35/11097 [01:04<3:51:36,  1.26s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 22.06it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.53it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.32it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.43it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.45it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:04, 12.20it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.12it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.32it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.80it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.65it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.24it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 12.00it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 11.85it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.49it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.49it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.36it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 11.03it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.20it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.22it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.61it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.15it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.66it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.66it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 11.11it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.21it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.76it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.75it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 11.04it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.81it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.83it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.37it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.60it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.78it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.372732162475586, 'eval_runtime': 6.5827, 'eval_samples_per_second': 11.393, 'eval_steps_per_second': 11.393, 'epoch': 0.01}\n",
      "  0%|                                      | 35/11097 [01:11<3:51:36,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.77it/s]\u001b[A\n",
      "{'loss': 1.5162, 'learning_rate': 0.0001999972861465844, 'epoch': 0.01}         \u001b[A\n",
      "{'loss': 1.8269, 'learning_rate': 0.00019999707337509115, 'epoch': 0.01}        \n",
      "{'loss': 1.079, 'learning_rate': 0.00019999685257464446, 'epoch': 0.01}         \n",
      "{'loss': 1.5693, 'learning_rate': 0.00019999662374526215, 'epoch': 0.01}        \n",
      "{'loss': 1.2695, 'learning_rate': 0.00019999638688696254, 'epoch': 0.01}        \n",
      "  0%|▏                                     | 40/11097 [01:14<3:32:50,  1.16s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.90it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.27it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.14it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.28it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.40it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 12.15it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 12.00it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.21it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.71it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.54it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.21it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.94it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:03, 11.76it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.40it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.35it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.36it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 11.06it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.18it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.17it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.10it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.53it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.61it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.57it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.98it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.09it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.65it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.70it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.93it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.67it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.31it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.55it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.90it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3437153100967407, 'eval_runtime': 6.6275, 'eval_samples_per_second': 11.316, 'eval_steps_per_second': 11.316, 'epoch': 0.01}\n",
      "  0%|▏                                     | 40/11097 [01:20<3:32:50,  1.16s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.73it/s]\u001b[A\n",
      "{'loss': 1.1865, 'learning_rate': 0.00019999614199976465, 'epoch': 0.01}        \u001b[A\n",
      "{'loss': 1.5142, 'learning_rate': 0.00019999588908368817, 'epoch': 0.01}        \n",
      "{'loss': 0.9165, 'learning_rate': 0.00019999562813875334, 'epoch': 0.01}        \n",
      "{'loss': 1.0933, 'learning_rate': 0.00019999535916498116, 'epoch': 0.01}        \n",
      "{'loss': 0.9505, 'learning_rate': 0.00019999508216239322, 'epoch': 0.01}        \n",
      "  0%|▏                                     | 45/11097 [01:25<3:56:41,  1.29s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.63it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.24it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.12it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.28it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.31it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 12.14it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 11.97it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.23it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.74it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.52it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.20it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.91it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:04, 11.70it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.36it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.34it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.22it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 10.98it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.09it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.11it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.50it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 11.07it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.56it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.58it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.98it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.09it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.63it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.61it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.89it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.67it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.28it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.49it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.68it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.328573226928711, 'eval_runtime': 6.6502, 'eval_samples_per_second': 11.278, 'eval_steps_per_second': 11.278, 'epoch': 0.01}\n",
      "  0%|▏                                     | 45/11097 [01:32<3:56:41,  1.29s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.66it/s]\u001b[A\n",
      "{'loss': 1.103, 'learning_rate': 0.0001999947971310118, 'epoch': 0.01}          \u001b[A\n",
      "{'loss': 1.3633, 'learning_rate': 0.0001999945040708597, 'epoch': 0.01}         \n",
      "{'loss': 1.6129, 'learning_rate': 0.00019999420298196055, 'epoch': 0.01}        \n",
      "{'loss': 1.291, 'learning_rate': 0.00019999389386433843, 'epoch': 0.01}         \n",
      "{'loss': 0.9853, 'learning_rate': 0.00019999357671801818, 'epoch': 0.01}        \n",
      "  0%|▏                                     | 50/11097 [01:35<3:39:31,  1.19s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.71it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.32it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 12.10it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.25it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.27it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 12.02it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 11.93it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.16it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.60it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.43it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:01<00:04, 11.08it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.84it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:04, 11.71it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:03, 11.28it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.30it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 10.92it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.07it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:02, 11.03it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.39it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 10.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.49it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.49it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02, 10.05it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.57it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.84it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.57it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.62it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.17it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:05<00:00, 11.44it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.62it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3150309324264526, 'eval_runtime': 6.6927, 'eval_samples_per_second': 11.206, 'eval_steps_per_second': 11.206, 'epoch': 0.01}\n",
      "  0%|▏                                     | 50/11097 [01:42<3:39:31,  1.19s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.57it/s]\u001b[A\n",
      "{'loss': 1.2576, 'learning_rate': 0.00019999325154302535, 'epoch': 0.01}        \u001b[A\n",
      "{'loss': 1.3121, 'learning_rate': 0.00019999291833938597, 'epoch': 0.01}        \n",
      "{'loss': 1.105, 'learning_rate': 0.0001999925771071268, 'epoch': 0.01}          \n",
      "{'loss': 1.1401, 'learning_rate': 0.00019999222784627523, 'epoch': 0.01}        \n",
      "{'loss': 1.2429, 'learning_rate': 0.00019999187055685935, 'epoch': 0.01}        \n",
      "  0%|▏                                     | 55/11097 [01:46<4:03:31,  1.32s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.64it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.03it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 11.91it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.17it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.24it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 12.01it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 11.83it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.07it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.53it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.41it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:02<00:04, 11.11it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.77it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:04, 11.60it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:04, 11.21it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.22it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.18it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 10.93it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 11.04it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 11.01it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:03, 10.99it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.35it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 10.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.48it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.47it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02,  9.99it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.54it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.81it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.52it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.55it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.14it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:06<00:00, 11.39it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.72it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.59it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.295883297920227, 'eval_runtime': 6.7224, 'eval_samples_per_second': 11.157, 'eval_steps_per_second': 11.157, 'epoch': 0.01}\n",
      "  0%|▏                                     | 55/11097 [01:53<4:03:31,  1.32s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.49it/s]\u001b[A\n",
      "{'loss': 0.9401, 'learning_rate': 0.0001999915052389078, 'epoch': 0.02}         \u001b[A\n",
      "{'loss': 1.1006, 'learning_rate': 0.00019999113189244993, 'epoch': 0.02}        \n",
      "{'loss': 1.0839, 'learning_rate': 0.00019999075051751572, 'epoch': 0.02}        \n",
      "{'loss': 1.5278, 'learning_rate': 0.0001999903611141358, 'epoch': 0.02}         \n",
      "{'loss': 1.161, 'learning_rate': 0.00019998996368234142, 'epoch': 0.02}         \n",
      "  1%|▏                                     | 60/11097 [01:56<3:33:59,  1.16s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.40it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.00it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 11.91it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.08it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.20it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 11.91it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 11.81it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.01it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.48it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.35it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:02<00:04, 11.00it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.74it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:04, 11.51it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:04, 11.10it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.16it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.06it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 10.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 10.94it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 10.92it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:03, 10.95it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.33it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:03<00:02, 10.88it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.39it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.37it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02,  9.97it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.49it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.72it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.47it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.08it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:06<00:00, 11.31it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.280275583267212, 'eval_runtime': 6.7603, 'eval_samples_per_second': 11.094, 'eval_steps_per_second': 11.094, 'epoch': 0.02}\n",
      "  1%|▏                                     | 60/11097 [02:03<3:33:59,  1.16s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:06<00:00, 11.45it/s]\u001b[A\n",
      "{'loss': 1.6926, 'learning_rate': 0.00019998955822216448, 'epoch': 0.02}        \u001b[A\n",
      "{'loss': 1.0253, 'learning_rate': 0.00019998914473363754, 'epoch': 0.02}        \n",
      "{'loss': 1.2151, 'learning_rate': 0.00019998872321679386, 'epoch': 0.02}        \n",
      "{'loss': 1.1427, 'learning_rate': 0.00019998829367166718, 'epoch': 0.02}        \n",
      "{'loss': 1.3025, 'learning_rate': 0.00019998785609829205, 'epoch': 0.02}        \n",
      "  1%|▏                                     | 65/11097 [02:07<3:49:55,  1.25s/it]\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:03, 21.53it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:00<00:04, 14.07it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:00<00:05, 11.93it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:00<00:05, 12.10it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:00<00:05, 12.13it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:01<00:05, 11.89it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:01<00:04, 11.81it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:01<00:04, 12.01it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:01<00:04, 11.48it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:01<00:04, 11.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:02<00:04, 11.01it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:02<00:04, 11.72it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:02<00:04, 11.50it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:02<00:04, 11.11it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:02<00:03, 11.16it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:02<00:03, 11.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:03<00:03, 10.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:03<00:03, 10.93it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:03<00:03, 10.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:03<00:03, 10.90it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:03<00:02, 11.29it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:04<00:02, 10.81it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:04<00:02, 10.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:04<00:02, 10.37it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:04<00:02, 10.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:04<00:02,  9.92it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:04<00:01, 10.44it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:05<00:01, 10.47it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:05<00:01, 10.71it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:05<00:01, 10.46it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:05<00:01, 10.53it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:05<00:00, 11.05it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:06<00:00, 11.28it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:06<00:00, 11.61it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:06<00:00, 11.51it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:06<00:00, 11.40it/s]\u001b[A^C\n",
      "  1%|▏                                     | 65/11097 [02:14<6:21:22,  2.07s/it]\n",
      "\n",
      "                                                                                \u001b[A"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py config-1b-qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3684d4-23bb-434d-8849-4bcb5d7bedcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
