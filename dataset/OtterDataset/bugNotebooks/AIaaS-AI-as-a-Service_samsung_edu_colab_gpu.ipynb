{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "51bQlVgjmnWf"
      },
      "source": [
        "# **YoloV5(PyTorch) ëª¨ë¸ ì‹¤ìŠµ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. ì‚¬ì „ í™˜ê²½ ì„¸íŒ…**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EIUMAdGXm46J"
      },
      "source": [
        "### **1-1. PyTorch ì •ìƒ ì„¤ì¹˜ í™•ì¸**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "oyrsdB9THu-8",
        "outputId": "a0b04911-7396-4807-af0d-04975315ac77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxD-M-g4nZf_"
      },
      "source": [
        "### **1-2. Pre-trained weightë¡œ ì¶”ë¡  ì§„í–‰**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "xymK200gmV25",
        "outputId": "f7c19fab-4f8b-4d5c-eed3-5b70c148065e"
      },
      "outputs": [],
      "source": [
        "# pretrained weight ì¸ yolov5l.pt ë¥¼ í™œìš©í•´ data/bus.jpg ì´ë¯¸ì§€ì— ëŒ€í•´ ì¶”ë¡  ì§„í–‰\n",
        "!python detect.py --weights yolov5l.pt --source /workspace/data/images/bus.jpg\n",
        "                                            #    img.jpg                         # image\n",
        "                                            #    vid.mp4                         # video\n",
        "                                            #    screen                          # screenshot\n",
        "                                            #    path/                           # directory\n",
        "                                            #    list.txt                        # list of images\n",
        "                                            #    list.streams                    # list of streams\n",
        "                                            #    'path/*.jpg'                    # glob\n",
        "                                            #    'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                                            #    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì¶”ë¡  ê²°ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•œ ëª¨ë“ˆ ë¡œë“œ\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# runs/detects/ í´ë” ë‚´ ê°€ì¥ ìµœì‹  exp í™•ì¸\n",
        "exp_dirs = sorted(glob.glob('runs/detect/exp*'), key=os.path.getmtime)\n",
        "latest_exp_dir = exp_dirs[-1] if exp_dirs else None\n",
        "\n",
        "# í´ë” ë‚´ ì¶”ë¡ ëœ ì´ë¯¸ì§€ê°€ ìˆì„ ê²½ìš° ì¶œë ¥\n",
        "if latest_exp_dir:\n",
        "    result_images = glob.glob(os.path.join(latest_exp_dir, '*.jpg'))\n",
        "    \n",
        "    if result_images:\n",
        "        display(Image(filename=result_images[0]))\n",
        "    else:\n",
        "        print(\"No result images found in the latest directory.\")\n",
        "else:\n",
        "    print(\"No 'exp*' directories found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySGg4Ols02rX"
      },
      "source": [
        "## **2. PASCAL VOC ë°ì´í„° í™œìš© ëª¨ë¸ í•™ìŠµ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tlgBiU4ZsZY5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from utils.general import check_dataset, check_img_size, increment_path, colorstr\n",
        "from utils.torch_utils import select_device\n",
        "from utils.dataloaders import create_dataloader\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ PYTHONPATHì— ì¶”ê°€ (ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰ ì‹œ í•„ìš”)\n",
        "FILE = Path(__file__).resolve() if \"__file__\" in globals() else Path.cwd()\n",
        "ROOT = FILE.parents[0]\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
        "def load_data(data_path, img_size, batch_size, hyp):\n",
        "    \"\"\"\n",
        "    ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n",
        "    Args:\n",
        "        data_path (str): ë°ì´í„°ì…‹ yaml íŒŒì¼ ê²½ë¡œ\n",
        "        img_size (int): ì´ë¯¸ì§€ í¬ê¸°\n",
        "        batch_size (int): ë°°ì¹˜ í¬ê¸°\n",
        "        hyp (dict): í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬\n",
        "    Returns:\n",
        "        train_loader (DataLoader): í•™ìŠµ ë°ì´í„° ë¡œë”\n",
        "        dataset (Dataset): ë°ì´í„°ì…‹ ê°ì²´\n",
        "    \"\"\"\n",
        "    # ë°ì´í„°ì…‹ ê²½ë¡œ ë° ì •ë³´ ë¡œë“œ\n",
        "    data_dict = check_dataset(data_path)  \n",
        "    train_path, val_path = data_dict['train'], data_dict['val']  # í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° ê²½ë¡œ\n",
        "\n",
        "    # ë°ì´í„° ë¡œë” ìƒì„±\n",
        "    train_loader, dataset = create_dataloader(\n",
        "        train_path,\n",
        "        img_size,\n",
        "        batch_size,\n",
        "        stride=32,  # YOLO ëª¨ë¸ ê¸°ë³¸ stride\n",
        "        hyp=hyp,\n",
        "        augment=True,  # ë°ì´í„° ì¦ê°•\n",
        "        cache=None,    # ë°ì´í„° ìºì‹± ë¹„í™œì„±í™”\n",
        "        rect=False,    # ì§ì‚¬ê°í˜• í¬ê¸° ë§ì¶”ê¸° ë¹„í™œì„±í™”\n",
        "        rank=-1,       # DDP ë¹„í™œì„±í™”\n",
        "        workers=8,     # ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜\n",
        "        image_weights=False,  # ì´ë¯¸ì§€ ê°€ì¤‘ì¹˜ ë¹„í™œì„±í™”\n",
        "        prefix=colorstr('train: ')\n",
        "    )\n",
        "    \n",
        "    return train_loader, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-2. ëª¨ë¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# yolo ëª¨ë¸ í˜¸ì¶œ\n",
        "from models.yolo import Model\n",
        "\n",
        "def define_model(cfg, nc, hyp, device):\n",
        "    \"\"\"\n",
        "    YOLOv5 ëª¨ë¸ì„ ì •ì˜í•˜ëŠ” í•¨ìˆ˜\n",
        "    Args:\n",
        "        cfg (str): ëª¨ë¸ êµ¬ì„± íŒŒì¼ ê²½ë¡œ (.yaml)\n",
        "        nc (int): í´ë˜ìŠ¤ ìˆ˜\n",
        "        hyp (dict): í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬\n",
        "        device (torch.device): ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cpu' ë˜ëŠ” 'cuda')\n",
        "    Returns:\n",
        "        model (Model): YOLOv5 ëª¨ë¸ ê°ì²´\n",
        "    \"\"\"\n",
        "    # ëª¨ë¸ ìƒì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "    model = Model(cfg, ch=3, nc=nc).to(device)  # YOLO ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    model.hyp = hyp  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
        "def get_hyperparameters():\n",
        "    \"\"\"\n",
        "    YOLOv5 ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜\n",
        "    Returns:\n",
        "        hyp (dict): í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬\n",
        "    \"\"\"\n",
        "    hyp = {\n",
        "        'lr0': 0.01,  # ì´ˆê¸° í•™ìŠµë¥ \n",
        "        'momentum': 0.937,  # ëª¨ë©˜í…€\n",
        "        'weight_decay': 0.0005,  # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
        "        'box': 0.05,  # ë°•ìŠ¤ ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
        "        'cls': 0.5,  # í´ë˜ìŠ¤ ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
        "        'obj': 1.0,  # ê°ì²´ ì†ì‹¤ ê°€ì¤‘ì¹˜\n",
        "        'iou_t': 0.2,  # IoU ì„ê³„ê°’\n",
        "        'anchor_t': 4.0,  # ì•µì»¤ ì„ê³„ê°’\n",
        "        'fl_gamma': 0.0,  # Focal Loss ê°ë§ˆ ê°’\n",
        "        'hsv_h': 0.015,  # HSV ìƒ‰ìƒ ë³€í™”\n",
        "        'hsv_s': 0.7,  # HSV ì±„ë„ ë³€í™”\n",
        "        'hsv_v': 0.4,  # HSV ëª…ë„ ë³€í™”\n",
        "        'degrees': 0.0,  # íšŒì „ ê°ë„\n",
        "        'translate': 0.1,  # ì´ë™ ë³€í™˜\n",
        "        'scale': 0.5,  # ìŠ¤ì¼€ì¼ ë³€í™˜\n",
        "        'shear': 0.0,  # ì™œê³¡ ë³€í™˜\n",
        "        'cls_pw': 1.0,  # í´ë˜ìŠ¤ ì†ì‹¤ ì–‘ì„± ê°€ì¤‘ì¹˜\n",
        "        'obj_pw': 1.0,  # ê°ì²´ ì†ì‹¤ ì–‘ì„± ê°€ì¤‘ì¹˜\n",
        "        'mosaic': 1.0,  # ëª¨ìì´í¬ ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "        'copy_paste': 0.0,  # Copy-paste ì¦ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "        'perspective': 0.0,  # ì›ê·¼ ë³€í™˜\n",
        "        'mixup': 0.0,  # Mixup ì¦ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "        'flipud': 0.0,  # ìˆ˜ì§ í”Œë¦½ í™•ë¥ \n",
        "        'fliplr': 0.5,  # ìˆ˜í‰ í”Œë¦½ í™•ë¥ \n",
        "    }\n",
        "    return hyp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-3. ëª¨ë¸ í•™ìŠµ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 ğŸš€ 2024-8-31 Python-3.10.9 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12042MiB)\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7073569 parameters, 7073569 gradients, 16.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/torch/hub/datasets/VOC/labels/train2007.cache... 16551 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16551/16551 [00:00<?, ?it/s]\n",
            "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1035/1035 [01:30<00:00, 11.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 finished.\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ë° í•™ìŠµ ì§„í–‰\n",
        "\n",
        "def train(hyp, opt, device):\n",
        "    # ê²½ë¡œ ì„¤ì •\n",
        "    save_dir = Path(opt.save_dir)  # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "    epochs, batch_size, weights, data = opt.epochs, opt.batch_size, opt.weights, opt.data\n",
        "\n",
        "    # ë°ì´í„°ì…‹ ë° ëª¨ë¸ configuration í˜¸ì¶œ\n",
        "    data_dict = check_dataset(data)  # data.yaml í˜¸ì¶œ(VOC.yaml)\n",
        "    train_path, val_path = data_dict['train'], data_dict['val']\n",
        "    nc = int(data_dict['nc'])  # í´ë˜ìŠ¤ ê°œìˆ˜\n",
        "    names = data_dict['names']  # í´ë˜ìŠ¤ ëª…\n",
        "\n",
        "    # ëª¨ë¸\n",
        "    model = Model(opt.cfg, ch=3, nc=nc).to(device)  # ëª¨ë¸ ì‹ ê·œ ì •ì˜\n",
        "    model.hyp = hyp  # ì‚¬ì „ ì •ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜ì˜\n",
        "    imgsz = check_img_size(opt.imgsz, s=32)  # ì´ë¯¸ì§€ í¬ê¸° ê²€ì¦\n",
        "\n",
        "    # ë°ì´í„°\n",
        "    train_loader, dataset = create_dataloader(train_path,\n",
        "                                              imgsz,\n",
        "                                              batch_size,\n",
        "                                              32,\n",
        "                                              hyp=hyp,\n",
        "                                              augment=True,\n",
        "                                              cache=None,\n",
        "                                              rect=False,\n",
        "                                              rank=-1,\n",
        "                                              workers=8,\n",
        "                                              image_weights=False,\n",
        "                                              prefix=colorstr('train: ')\n",
        "                                              )\n",
        "\n",
        "    # optimizer, ì†ì‹¤í•¨ìˆ˜ ì •ì˜\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=hyp['lr0'], momentum=hyp['momentum'], weight_decay=hyp['weight_decay'])\n",
        "    compute_loss = ComputeLoss(model)\n",
        "\n",
        "    # í•™ìŠµ ë£¨í”„\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for imgs, targets, paths, _ in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'): # ë°ì´í„° ë£¨íŠ¸ í´ë” ë‚´ ì´ë¯¸ì§€ listì— ëŒ€í•œ ë£¨í”„\n",
        "            imgs = imgs.to(device).float() / 255.0  # ì´ë¯¸ì§€ ì •ê·œí™”\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(imgs)\n",
        "            loss, loss_items = compute_loss(pred, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} finished.')\n",
        "\n",
        "    # ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•  ë•Œ ëª¨ë¸ì˜ ì „ì²´ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ì˜ˆì‹œ\n",
        "    ckpt = {\n",
        "        'epoch': epoch,\n",
        "        'best_fitness': None,  # ìµœìƒì˜ í”¼íŠ¸ë‹ˆìŠ¤ ì ìˆ˜ (í•„ìš”ì‹œ ì¶”ê°€)\n",
        "        'model': model,  # ì „ì²´ ëª¨ë¸ ê°ì²´ë¥¼ ì €ì¥\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'hyp': hyp,  # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "        'names': names,  # í´ë˜ìŠ¤ ì´ë¦„\n",
        "        'ema': None,  # EMA ê°ì²´ (í•„ìš”ì‹œ ì¶”ê°€)\n",
        "        'updates': None,  # EMA ì—…ë°ì´íŠ¸ ìˆ˜\n",
        "    }\n",
        "\n",
        "    torch.save(ckpt, save_dir / 'last.pt')\n",
        "\n",
        "# í•™ìŠµ ì˜µì…˜ ì„¤ì • í´ë˜ìŠ¤\n",
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.weights = ''  # ì´ˆê¸° ê°€ì¤‘ì¹˜ ê²½ë¡œ, ê¸°ë³¸ì ìœ¼ë¡œëŠ” ë¹ˆ ë¬¸ìì—´ (ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ)\n",
        "        self.cfg = 'models/yolov5s.yaml'  # ëª¨ë¸ êµ¬ì„± íŒŒì¼ ê²½ë¡œ\n",
        "        self.data = '/workspace/data/VOC.yaml'  # ë°ì´í„°ì…‹ êµ¬ì„± íŒŒì¼ ê²½ë¡œ\n",
        "        self.epochs = 1  # ì´ í•™ìŠµ ì—í­ ìˆ˜\n",
        "        self.batch_size = 16  # ë°°ì¹˜ í¬ê¸°\n",
        "        self.imgsz = 640  # í•™ìŠµ ë° ê²€ì¦ ì‹œ ì‚¬ìš©í•  ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€)\n",
        "        self.save_dir = 'runs/train'  # í•™ìŠµ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘\n",
        "if __name__ == '__main__':\n",
        "    opt = Options()\n",
        "    device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
        "    opt.save_dir = increment_path(Path(opt.save_dir) / 'exp')\n",
        "    opt.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    hyp = get_hyperparameters()\n",
        "    train(hyp, opt, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgtlCN1m1TKG"
      },
      "source": [
        "## **3. ì‹ ê·œ í•™ìŠµ weightë¥¼ í™œìš©í•œ ì¶”ë¡ **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "colab_type": "code",
        "id": "wok7x44vNYuP",
        "outputId": "45e152d6-041a-48c9-813d-9d1588d5b4b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect2: \u001b[0mweights=['/workspace/runs/train/exp17/last.pt'], source=/workspace/data/images/bus.jpg, data=data/VOC.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 ğŸš€ v7.0-361-gc5ffbbf1 Python-3.10.9 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12042MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/detect2.py\", line 437, in <module>\n",
            "    main(opt)\n",
            "  File \"/workspace/detect2.py\", line 432, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/workspace/detect2.py\", line 166, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/workspace/models/common.py\", line 489, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/workspace/models/experimental.py\", line 99, in attempt_load\n",
            "    ckpt = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()  # FP32 model\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/workspace/models/yolo.py\", line 208, in _apply\n",
            "    self = super()._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ì‹ ê·œ í•™ìŠµ weight ì¸ yolov5s.pt ë¥¼ í™œìš©í•´ data/bus.jpg ì´ë¯¸ì§€ì— ëŒ€í•´ ì¶”ë¡  ì§„í–‰\n",
        "!python detect.py --weights /workspace/runs/train/exp17/last.pt --source /workspace/data/images/bus.jpg --data data/VOC.yaml\n",
        "                                            #    img.jpg                         # image\n",
        "                                            #    vid.mp4                         # video\n",
        "                                            #    screen                          # screenshot\n",
        "                                            #    path/                           # directory\n",
        "                                            #    list.txt                        # list of images\n",
        "                                            #    list.streams                    # list of streams\n",
        "                                            #    'path/*.jpg'                    # glob\n",
        "                                            #    'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                                            #    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "esTFKZzX7jYj"
      },
      "outputs": [],
      "source": [
        "# ì¶”ë¡  ê²°ê³¼ ì‹œê°í™”ë¥¼ ìœ„í•œ ëª¨ë“ˆ ë¡œë“œ\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# runs/detects/ í´ë” ë‚´ ê°€ì¥ ìµœì‹  exp í™•ì¸\n",
        "exp_dirs = sorted(glob.glob('runs/detect/exp*'), key=os.path.getmtime)\n",
        "latest_exp_dir = exp_dirs[-1] if exp_dirs else None\n",
        "\n",
        "# í´ë” ë‚´ ì¶”ë¡ ëœ ì´ë¯¸ì§€ê°€ ìˆì„ ê²½ìš° ì¶œë ¥\n",
        "if latest_exp_dir:\n",
        "    result_images = glob.glob(os.path.join(latest_exp_dir, '*.jpg'))\n",
        "    \n",
        "    if result_images:\n",
        "        display(Image(filename=result_images[0]))\n",
        "    else:\n",
        "        print(\"No result images found in the latest directory.\")\n",
        "else:\n",
        "    print(\"No 'exp*' directories found.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "colab_gpu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
