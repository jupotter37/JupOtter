{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "51bQlVgjmnWf"
      },
      "source": [
        "# **YoloV5(PyTorch) 모델 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. 사전 환경 세팅**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EIUMAdGXm46J"
      },
      "source": [
        "### **1-1. PyTorch 정상 설치 확인**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "oyrsdB9THu-8",
        "outputId": "a0b04911-7396-4807-af0d-04975315ac77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxD-M-g4nZf_"
      },
      "source": [
        "### **1-2. Pre-trained weight로 추론 진행**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "xymK200gmV25",
        "outputId": "f7c19fab-4f8b-4d5c-eed3-5b70c148065e"
      },
      "outputs": [],
      "source": [
        "# pretrained weight 인 yolov5l.pt 를 활용해 data/bus.jpg 이미지에 대해 추론 진행\n",
        "!python detect.py --weights yolov5l.pt --source /workspace/data/images/bus.jpg\n",
        "                                            #    img.jpg                         # image\n",
        "                                            #    vid.mp4                         # video\n",
        "                                            #    screen                          # screenshot\n",
        "                                            #    path/                           # directory\n",
        "                                            #    list.txt                        # list of images\n",
        "                                            #    list.streams                    # list of streams\n",
        "                                            #    'path/*.jpg'                    # glob\n",
        "                                            #    'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                                            #    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 결과 시각화를 위한 모듈 로드\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# runs/detects/ 폴더 내 가장 최신 exp 확인\n",
        "exp_dirs = sorted(glob.glob('runs/detect/exp*'), key=os.path.getmtime)\n",
        "latest_exp_dir = exp_dirs[-1] if exp_dirs else None\n",
        "\n",
        "# 폴더 내 추론된 이미지가 있을 경우 출력\n",
        "if latest_exp_dir:\n",
        "    result_images = glob.glob(os.path.join(latest_exp_dir, '*.jpg'))\n",
        "    \n",
        "    if result_images:\n",
        "        display(Image(filename=result_images[0]))\n",
        "    else:\n",
        "        print(\"No result images found in the latest directory.\")\n",
        "else:\n",
        "    print(\"No 'exp*' directories found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ySGg4Ols02rX"
      },
      "source": [
        "## **2. PASCAL VOC 데이터 활용 모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-1. 데이터 로드 및 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tlgBiU4ZsZY5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from utils.general import check_dataset, check_img_size, increment_path, colorstr\n",
        "from utils.torch_utils import select_device\n",
        "from utils.dataloaders import create_dataloader\n",
        "\n",
        "# 프로젝트 루트 디렉토리를 PYTHONPATH에 추가 (노트북에서 실행 시 필요)\n",
        "FILE = Path(__file__).resolve() if \"__file__\" in globals() else Path.cwd()\n",
        "ROOT = FILE.parents[0]\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터셋 로드 및 전처리 함수 정의\n",
        "def load_data(data_path, img_size, batch_size, hyp):\n",
        "    \"\"\"\n",
        "    데이터셋을 로드하고 전처리를 수행하는 함수\n",
        "    Args:\n",
        "        data_path (str): 데이터셋 yaml 파일 경로\n",
        "        img_size (int): 이미지 크기\n",
        "        batch_size (int): 배치 크기\n",
        "        hyp (dict): 하이퍼파라미터 딕셔너리\n",
        "    Returns:\n",
        "        train_loader (DataLoader): 학습 데이터 로더\n",
        "        dataset (Dataset): 데이터셋 객체\n",
        "    \"\"\"\n",
        "    # 데이터셋 경로 및 정보 로드\n",
        "    data_dict = check_dataset(data_path)  \n",
        "    train_path, val_path = data_dict['train'], data_dict['val']  # 학습 및 검증 데이터 경로\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    train_loader, dataset = create_dataloader(\n",
        "        train_path,\n",
        "        img_size,\n",
        "        batch_size,\n",
        "        stride=32,  # YOLO 모델 기본 stride\n",
        "        hyp=hyp,\n",
        "        augment=True,  # 데이터 증강\n",
        "        cache=None,    # 데이터 캐싱 비활성화\n",
        "        rect=False,    # 직사각형 크기 맞추기 비활성화\n",
        "        rank=-1,       # DDP 비활성화\n",
        "        workers=8,     # 데이터 로딩 워커 수\n",
        "        image_weights=False,  # 이미지 가중치 비활성화\n",
        "        prefix=colorstr('train: ')\n",
        "    )\n",
        "    \n",
        "    return train_loader, dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-2. 모델 및 하이퍼파라미터 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# yolo 모델 호출\n",
        "from models.yolo import Model\n",
        "\n",
        "def define_model(cfg, nc, hyp, device):\n",
        "    \"\"\"\n",
        "    YOLOv5 모델을 정의하는 함수\n",
        "    Args:\n",
        "        cfg (str): 모델 구성 파일 경로 (.yaml)\n",
        "        nc (int): 클래스 수\n",
        "        hyp (dict): 하이퍼파라미터 딕셔너리\n",
        "        device (torch.device): 사용할 디바이스 ('cpu' 또는 'cuda')\n",
        "    Returns:\n",
        "        model (Model): YOLOv5 모델 객체\n",
        "    \"\"\"\n",
        "    # 모델 생성 및 하이퍼파라미터 설정\n",
        "    model = Model(cfg, ch=3, nc=nc).to(device)  # YOLO 모델 초기화\n",
        "    model.hyp = hyp  # 하이퍼파라미터 설정\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 정의\n",
        "def get_hyperparameters():\n",
        "    \"\"\"\n",
        "    YOLOv5 모델 학습에 필요한 하이퍼파라미터를 정의하는 함수\n",
        "    Returns:\n",
        "        hyp (dict): 하이퍼파라미터 딕셔너리\n",
        "    \"\"\"\n",
        "    hyp = {\n",
        "        'lr0': 0.01,  # 초기 학습률\n",
        "        'momentum': 0.937,  # 모멘텀\n",
        "        'weight_decay': 0.0005,  # 가중치 감쇠\n",
        "        'box': 0.05,  # 박스 손실 가중치\n",
        "        'cls': 0.5,  # 클래스 손실 가중치\n",
        "        'obj': 1.0,  # 객체 손실 가중치\n",
        "        'iou_t': 0.2,  # IoU 임계값\n",
        "        'anchor_t': 4.0,  # 앵커 임계값\n",
        "        'fl_gamma': 0.0,  # Focal Loss 감마 값\n",
        "        'hsv_h': 0.015,  # HSV 색상 변화\n",
        "        'hsv_s': 0.7,  # HSV 채도 변화\n",
        "        'hsv_v': 0.4,  # HSV 명도 변화\n",
        "        'degrees': 0.0,  # 회전 각도\n",
        "        'translate': 0.1,  # 이동 변환\n",
        "        'scale': 0.5,  # 스케일 변환\n",
        "        'shear': 0.0,  # 왜곡 변환\n",
        "        'cls_pw': 1.0,  # 클래스 손실 양성 가중치\n",
        "        'obj_pw': 1.0,  # 객체 손실 양성 가중치\n",
        "        'mosaic': 1.0,  # 모자이크 데이터 증강 사용 여부\n",
        "        'copy_paste': 0.0,  # Copy-paste 증강 사용 여부\n",
        "        'perspective': 0.0,  # 원근 변환\n",
        "        'mixup': 0.0,  # Mixup 증강 사용 여부\n",
        "        'flipud': 0.0,  # 수직 플립 확률\n",
        "        'fliplr': 0.5,  # 수평 플립 확률\n",
        "    }\n",
        "    return hyp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2-3. 모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 🚀 2024-8-31 Python-3.10.9 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12042MiB)\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "YOLOv5s summary: 214 layers, 7073569 parameters, 7073569 gradients, 16.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/.cache/torch/hub/datasets/VOC/labels/train2007.cache... 16551 images, 0 backgrounds, 0 corrupt: 100%|██████████| 16551/16551 [00:00<?, ?it/s]\n",
            "Epoch 1/1: 100%|██████████| 1035/1035 [01:30<00:00, 11.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 finished.\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습 함수 정의 및 학습 진행\n",
        "\n",
        "def train(hyp, opt, device):\n",
        "    # 경로 설정\n",
        "    save_dir = Path(opt.save_dir)  # 결과 저장 경로 설정\n",
        "    epochs, batch_size, weights, data = opt.epochs, opt.batch_size, opt.weights, opt.data\n",
        "\n",
        "    # 데이터셋 및 모델 configuration 호출\n",
        "    data_dict = check_dataset(data)  # data.yaml 호출(VOC.yaml)\n",
        "    train_path, val_path = data_dict['train'], data_dict['val']\n",
        "    nc = int(data_dict['nc'])  # 클래스 개수\n",
        "    names = data_dict['names']  # 클래스 명\n",
        "\n",
        "    # 모델\n",
        "    model = Model(opt.cfg, ch=3, nc=nc).to(device)  # 모델 신규 정의\n",
        "    model.hyp = hyp  # 사전 정의 하이퍼파라미터 반영\n",
        "    imgsz = check_img_size(opt.imgsz, s=32)  # 이미지 크기 검증\n",
        "\n",
        "    # 데이터\n",
        "    train_loader, dataset = create_dataloader(train_path,\n",
        "                                              imgsz,\n",
        "                                              batch_size,\n",
        "                                              32,\n",
        "                                              hyp=hyp,\n",
        "                                              augment=True,\n",
        "                                              cache=None,\n",
        "                                              rect=False,\n",
        "                                              rank=-1,\n",
        "                                              workers=8,\n",
        "                                              image_weights=False,\n",
        "                                              prefix=colorstr('train: ')\n",
        "                                              )\n",
        "\n",
        "    # optimizer, 손실함수 정의\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=hyp['lr0'], momentum=hyp['momentum'], weight_decay=hyp['weight_decay'])\n",
        "    compute_loss = ComputeLoss(model)\n",
        "\n",
        "    # 학습 루프\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for imgs, targets, paths, _ in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'): # 데이터 루트 폴더 내 이미지 list에 대한 루프\n",
        "            imgs = imgs.to(device).float() / 255.0  # 이미지 정규화\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(imgs)\n",
        "            loss, loss_items = compute_loss(pred, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} finished.')\n",
        "\n",
        "    # 가중치를 저장할 때 모델의 전체 상태를 저장하는 예시\n",
        "    ckpt = {\n",
        "        'epoch': epoch,\n",
        "        'best_fitness': None,  # 최상의 피트니스 점수 (필요시 추가)\n",
        "        'model': model,  # 전체 모델 객체를 저장\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'hyp': hyp,  # 하이퍼파라미터\n",
        "        'names': names,  # 클래스 이름\n",
        "        'ema': None,  # EMA 객체 (필요시 추가)\n",
        "        'updates': None,  # EMA 업데이트 수\n",
        "    }\n",
        "\n",
        "    torch.save(ckpt, save_dir / 'last.pt')\n",
        "\n",
        "# 학습 옵션 설정 클래스\n",
        "class Options:\n",
        "    def __init__(self):\n",
        "        self.weights = ''  # 초기 가중치 경로, 기본적으로는 빈 문자열 (초기화되지 않음)\n",
        "        self.cfg = 'models/yolov5s.yaml'  # 모델 구성 파일 경로\n",
        "        self.data = '/workspace/data/VOC.yaml'  # 데이터셋 구성 파일 경로\n",
        "        self.epochs = 1  # 총 학습 에폭 수\n",
        "        self.batch_size = 16  # 배치 크기\n",
        "        self.imgsz = 640  # 학습 및 검증 시 사용할 이미지 크기 (픽셀)\n",
        "        self.save_dir = 'runs/train'  # 학습 결과 저장 디렉토리\n",
        "\n",
        "# 학습 시작\n",
        "if __name__ == '__main__':\n",
        "    opt = Options()\n",
        "    device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
        "    opt.save_dir = increment_path(Path(opt.save_dir) / 'exp')\n",
        "    opt.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    hyp = get_hyperparameters()\n",
        "    train(hyp, opt, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgtlCN1m1TKG"
      },
      "source": [
        "## **3. 신규 학습 weight를 활용한 추론**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "colab_type": "code",
        "id": "wok7x44vNYuP",
        "outputId": "45e152d6-041a-48c9-813d-9d1588d5b4b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect2: \u001b[0mweights=['/workspace/runs/train/exp17/last.pt'], source=/workspace/data/images/bus.jpg, data=data/VOC.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-361-gc5ffbbf1 Python-3.10.9 torch-2.0.0 CUDA:0 (NVIDIA GeForce RTX 3080 Ti, 12042MiB)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/detect2.py\", line 437, in <module>\n",
            "    main(opt)\n",
            "  File \"/workspace/detect2.py\", line 432, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/workspace/detect2.py\", line 166, in run\n",
            "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
            "  File \"/workspace/models/common.py\", line 489, in __init__\n",
            "    model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n",
            "  File \"/workspace/models/experimental.py\", line 99, in attempt_load\n",
            "    ckpt = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()  # FP32 model\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1145, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/workspace/models/yolo.py\", line 208, in _apply\n",
            "    self = super()._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 797, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 820, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1143, in convert\n",
            "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
            "RuntimeError: CUDA error: out of memory\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 신규 학습 weight 인 yolov5s.pt 를 활용해 data/bus.jpg 이미지에 대해 추론 진행\n",
        "!python detect.py --weights /workspace/runs/train/exp17/last.pt --source /workspace/data/images/bus.jpg --data data/VOC.yaml\n",
        "                                            #    img.jpg                         # image\n",
        "                                            #    vid.mp4                         # video\n",
        "                                            #    screen                          # screenshot\n",
        "                                            #    path/                           # directory\n",
        "                                            #    list.txt                        # list of images\n",
        "                                            #    list.streams                    # list of streams\n",
        "                                            #    'path/*.jpg'                    # glob\n",
        "                                            #    'https://youtu.be/LNwODJXcvt4'  # YouTube\n",
        "                                            #    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "esTFKZzX7jYj"
      },
      "outputs": [],
      "source": [
        "# 추론 결과 시각화를 위한 모듈 로드\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# runs/detects/ 폴더 내 가장 최신 exp 확인\n",
        "exp_dirs = sorted(glob.glob('runs/detect/exp*'), key=os.path.getmtime)\n",
        "latest_exp_dir = exp_dirs[-1] if exp_dirs else None\n",
        "\n",
        "# 폴더 내 추론된 이미지가 있을 경우 출력\n",
        "if latest_exp_dir:\n",
        "    result_images = glob.glob(os.path.join(latest_exp_dir, '*.jpg'))\n",
        "    \n",
        "    if result_images:\n",
        "        display(Image(filename=result_images[0]))\n",
        "    else:\n",
        "        print(\"No result images found in the latest directory.\")\n",
        "else:\n",
        "    print(\"No 'exp*' directories found.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "colab_gpu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
