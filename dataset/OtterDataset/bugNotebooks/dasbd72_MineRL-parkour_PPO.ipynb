{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from parkour_env import parkour_env\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': '0', 'y1': '1', 'z1': '0', 'x2': '0', 'y2': '1', 'z2': '3', 'type': 'obsidian'}\n",
      "{'x1': '0', 'y1': '1', 'z1': '5', 'x2': '0', 'y2': '1', 'z2': '6', 'type': 'obsidian'}\n",
      "{'x1': '0', 'y1': '1', 'z1': '6', 'x2': '5', 'y2': '1', 'z2': '6', 'type': 'obsidian'}\n",
      "{'x1': '5', 'y1': '1', 'z1': '6', 'x2': '5', 'y2': '1', 'z2': '7', 'type': 'obsidian'}\n",
      "{'x1': '5', 'y1': '1', 'z1': '9', 'x2': '5', 'y2': '1', 'z2': '10', 'type': 'obsidian'}\n",
      "{'x1': '5', 'y1': '2', 'z1': '10', 'x2': '5', 'y2': '2', 'z2': '11', 'type': 'obsidian'}\n",
      "{'x1': '5', 'y1': '3', 'z1': '11', 'x2': '5', 'y2': '3', 'z2': '13', 'type': 'obsidian'}\n",
      "{'x1': '5', 'y1': '3', 'z1': '13', 'x2': '2', 'y2': '3', 'z2': '13', 'type': 'obsidian'}\n",
      "{'x1': '2', 'y1': '3', 'z1': '13', 'x2': '2', 'y2': '3', 'z2': '10', 'type': 'obsidian'}\n",
      "{'x1': '2', 'y1': '3', 'z1': '10', 'x2': '1', 'y2': '3', 'z2': '10', 'type': 'obsidian'}\n",
      "{'x1': '-1', 'y1': '3', 'z1': '10', 'x2': '-1', 'y2': '3', 'z2': '13', 'type': 'obsidian'}\n",
      "{'x1': '-2', 'y1': '3', 'z1': '13', 'x2': '-2', 'y2': '3', 'z2': '15', 'type': 'obsidian'}\n",
      "{'x1': '-2', 'y1': '3', 'z1': '16', 'x2': '-2', 'y2': '3', 'z2': '16', 'type': 'gold_block'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/minerl/herobraine/hero/spaces.py:484: UserWarning: The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\n",
      "  warnings.warn(\"The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\")\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "FAST = True\n",
    "MPS = True\n",
    "RESOLUTION = (64, 64)\n",
    "env = parkour_env(resolution=RESOLUTION, map=\"bridge_hybrid2\", debug=DEBUG, fast=FAST, action_set=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_state = env.observation_space.sample()\n",
    "sample_action = env.action_space.sample()\n",
    "image_shape = sample_state['pov'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample action: 4\n",
      "Sample pov space (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample action:\", sample_action)\n",
    "print(\"Sample pov space\", image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.distributions as Categorical\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if (getattr(torch, 'has_mps', False) and MPS) else 'cpu'\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    import ipywidgets as widgets\n",
    "plt.ion()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, image_shape, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "                        #nn.BatchNorm2d(3),\n",
    "                        nn.Conv2d(3*4, 32, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(576, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, n_actions),\n",
    "                        # nn.Softmax(dim=-1)\n",
    "                        nn.Softmax(dim=-1)\n",
    "                    )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        #nn.BatchNorm2d(3),\n",
    "                        nn.Conv2d(3*4, 32, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        #nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "                        # nn.ReLU(),\n",
    "                        nn.Tanh(),\n",
    "                        nn.MaxPool2d(3, 1),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(576, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, 1)\n",
    "                    )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, image_shape, n_actions, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(image_shape, n_actions).to(device)\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(image_shape, n_actions).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            #state = torch.FloatTensor(state).to(device)\n",
    "            action, action_logprob = self.policy_old.act(state)\n",
    "        \n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, model_path):\n",
    "        torch.save(self.policy_old.state_dict(), model_path)\n",
    "   \n",
    "\n",
    "    def load(self, model_path):\n",
    "        try:\n",
    "            self.policy_old.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "            self.policy.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "        except:\n",
    "            'Failed Loading...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_state(obs):\n",
    "    x = torch.tensor(obs['pov'].copy(), dtype=torch.float32)\n",
    "    #x = torch.permute(x, (2,0,1)).unsqueeze(0) / 255\n",
    "    x = torch.permute(x, (2,0,1)) / 255  # for queue\n",
    "    return x\n",
    "class queue():\n",
    "    def __init__(self, n_stacked_img, dim, stride=1):\n",
    "        self.data = []\n",
    "        self.stride = stride\n",
    "        self.size = (n_stacked_img - 1) * stride + 1\n",
    "        for i in range(self.size):\n",
    "            self.data.append(torch.zeros(dim))\n",
    "    def push(self, img):\n",
    "        self.data[:-1] = self.data[1:]\n",
    "        self.data[-1] = img\n",
    "    def get(self):\n",
    "        return torch.cat(self.data[::self.stride]).unsqueeze(0).to(device)\n",
    "    def fill(self, img):\n",
    "        for i in range(self.size):\n",
    "            self.data[i] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "save_model_freq = int(100)          # save model frequency (in num timesteps)\n",
    "model_path = 'ppo_adamw2.pt'\n",
    "\n",
    "update_timestep = max_ep_len * 4    # update policy every n timesteps\n",
    "K_epochs = 40                       # update policy for K epochs\n",
    "eps_clip = 0.2                      # clip parameter for PPO\n",
    "gamma = 0.99                        # discount factor\n",
    "\n",
    "lr_actor = 0.0003                   # learning rate for actor network\n",
    "lr_critic = 0.001                   # learning rate for critic network\n",
    "\n",
    "random_seed = 0                     # set random seed if required (0 = no random seed)\n",
    "\n",
    "n_actions = env.n_actions\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(image_shape, n_actions, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "#ppo_agent.load(model_path)\n",
    "success_video = []\n",
    "reward_list = []\n",
    "total_epi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1afec57312040cabb5799a8ef85b563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7aaa49d5f84afe9e3019132b2a0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/minerl/herobraine/hero/spaces.py:484: UserWarning: The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\n",
      "  warnings.warn(\"The Text MineRLSpace class is not fully implemented. This may cause problems when sampling an action of this type (even when getting a noop).\")\n",
      "/home/user/.local/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n",
      "/opt/conda/lib/python3.9/runpy.py:127: RuntimeWarning: 'minerl.utils.process_watcher' found in sys.modules after import of package 'minerl.utils', but prior to execution of 'minerl.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting environment.\n",
      "Resetting environment.\n",
      "Resetting environment.\n"
     ]
    },
    {
     "ename": "timeout",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m     13\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     best_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     17\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/save/Work/MineRL-Parkour-Spec/parkour_env.py:198\u001b[0m, in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         reward -= 0.01 * self.t\n\u001b[1;32m    196\u001b[0m         return (obs, reward, done, info, success)\n\u001b[0;32m--> 198\u001b[0m     else:\n\u001b[1;32m    199\u001b[0m         # Random step result\n\u001b[1;32m    200\u001b[0m         return self.random_step()\n\u001b[1;32m    202\u001b[0m def reset(self) -> _ObservationType:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/wrappers/time_limit.py:27\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/minerl/env/_singleagent.py:22\u001b[0m, in \u001b[0;36m_SingleAgentEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 22\u001b[0m     multi_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m multi_obs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39magent_names[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/minerl/env/_multiagent.py:480\u001b[0m, in \u001b[0;36m_MultiAgentEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_finished \u001b[38;5;241m=\u001b[39m {agent: \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39magent_names}\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Start the Mission/Task, by sending the master mission XML over \u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# the pipe to these instances, and  update the agent xmls to get\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# the port/ip of the master agent send the remaining XMLS.\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_mission\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_xmls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_token\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep_uid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Master\u001b[39;00m\n\u001b[1;32m    481\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39magent_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/minerl/env/_multiagent.py:637\u001b[0m, in \u001b[0;36m_MultiAgentEnv._send_mission\u001b[0;34m(self, instance, mission_xml_etree, token_in)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m# TODO: This is odd, MAX_WAIT is usually a number of seconds but here\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m#  it's a number of retries. Probably needs to be fixed.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_retries \u001b[38;5;241m>\u001b[39m MAX_WAIT:\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout()\n\u001b[1;32m    638\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecieved a MALMOBUSY from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; trying again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(instance))\n\u001b[1;32m    639\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mtimeout\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "renderer = widgets.Output()\n",
    "display.display(renderer)\n",
    "plotter = widgets.Output()\n",
    "display.display(plotter)\n",
    "\n",
    "num_episodes = 5000\n",
    "total_time = 0\n",
    "time_step = 0\n",
    "que = queue(n_stacked_img=4, dim=(3, RESOLUTION[0], RESOLUTION[1]), stride=4)\n",
    "for i_episode in range(num_episodes):\n",
    "    start = time.time()\n",
    "    obs = env.reset()\n",
    "\n",
    "    best_reward = -100\n",
    "    total_reward = 0\n",
    "    reward = 0\n",
    "    tmp = extract_state(obs)\n",
    "    que.fill(tmp)\n",
    "    history = []\n",
    "    for t in range(max_ep_len):\n",
    "        env.render()\n",
    "        if t % 10 == 0 and FAST and is_ipython:\n",
    "            with renderer:\n",
    "                display.clear_output(wait=True)\n",
    "                plt.imshow( obs['pov'] )\n",
    "                plt.title(f'{total_epi} step: {t} tot_reward: {total_reward:.2f} time: {total_time/60:.2f} min')\n",
    "                plt.show()\n",
    "        \n",
    "        # select action with policy\n",
    "        state = extract_state(obs)\n",
    "        que.push(state)\n",
    "        action = ppo_agent.select_action(que.get())\n",
    "        obs, reward, done, info, success = env.step(action)\n",
    "        total_reward += reward\n",
    "        history.append((state, action, reward))\n",
    "        best_reward = max(reward, best_reward)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step += 1\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0 and not DEBUG:\n",
    "            ppo_agent.save(model_path)\n",
    "\n",
    "        if done:\n",
    "            reward_list.append(total_reward)\n",
    "            if is_ipython:\n",
    "                with plotter:\n",
    "                    display.clear_output(wait=True)\n",
    "                    plt.scatter(range(len(reward_list)), reward_list)\n",
    "                    plt.show()\n",
    "            break\n",
    "    total_epi += 1\n",
    "    end = time.time()\n",
    "    end = end - start\n",
    "    total_time += end\n",
    "    if success:\n",
    "        print(f\"{i_episode} success\")\n",
    "        success_video.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(reward_list)), reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython import display\n",
    "len(success_video)\n",
    "play = -7\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(1,1)\n",
    "t = 0\n",
    "A = ['forward', 'forward_jump', 'forward_sprint', 'camera_left', 'camera_right']\n",
    "ims = []\n",
    "def update(i):\n",
    "    state, action, reward = success_video[play][i]\n",
    "    title = ax.set_title(f'step: {i+1} reward: {float(reward):.2f} action: {A[action]}')\n",
    "    img = torch.permute(state, (1,2,0))*255\n",
    "    img = np.array(img.numpy(), dtype=int)\n",
    "    im = ax.imshow(img)\n",
    "    return im,\n",
    "anim = animation.FuncAnimation(fig, update, frames=len(success_video[play]), interval=50, repeat=True)\n",
    "anim.save(\"PPO_hybrid2-5.mp4\", dpi=80)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    ppo_agent.save(model_path)\n",
    "print(len(success_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class queue():\n",
    "    def __init__(self, n_stacked_img, dim, stride=1):\n",
    "        self.data = []\n",
    "        self.stride = stride\n",
    "        self.size = (n_stacked_img - 1) * stride + 1\n",
    "        for i in range(self.size):\n",
    "            self.data.append(torch.zeros(dim))\n",
    "    def push(self, img):\n",
    "        self.data[:-1] = self.data[1:]\n",
    "        self.data[-1] = img\n",
    "    def get(self):\n",
    "        return torch.cat(self.data[::self.stride]).unsqueeze(0).to(device)\n",
    "    def fill(self, img):\n",
    "        for i in range(self.size):\n",
    "            self.data[i] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4f7e2a837d202febde7ca43873e66ef03435144f2808f9a8051bdb7fbfc88b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
