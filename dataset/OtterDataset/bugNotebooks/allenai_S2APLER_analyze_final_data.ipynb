{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nfs2.s2-research/sergeyf/miniconda3/envs/s2apler/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-29 04:03:52,748 - s2apler - WARNING - You haven't set `main_data_dir` in data/path_config.json! Using data/ as default data directory.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from s2apler.data import PDData\n",
    "from s2apler.featurizer import FeaturizationInfo, many_pairs_featurize\n",
    "from s2apler.model import PairwiseModeler, Clusterer\n",
    "from s2apler.eval import pairwise_eval, cluster_eval\n",
    "from s2apler.consts import DEFAULT_CHUNK_SIZE, PROJECT_ROOT_PATH, CONFIG\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s2apler.eval import *\n",
    "\n",
    "\n",
    "def pairwise_eval(\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    classifier: Any,\n",
    "    figs_path: str,\n",
    "    title: str,\n",
    "    shap_feature_names: List[str],\n",
    "    thresh_for_f1: float = 0.5,\n",
    "    shap_plot_type: Optional[str] = \"dot\",\n",
    "    nameless_classifier: Optional[Any] = None,\n",
    "    nameless_X: Optional[np.array] = None,\n",
    "    nameless_feature_names: Optional[List[str]] = None,\n",
    "    skip_shap: bool = False,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Performs pairwise model evaluation, without using blocks.\n",
    "    Also writes plots to the provided file path\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "        Feature matrix of features to do eval on.\n",
    "    y: np.array\n",
    "        Feature matrix of labels to do eval on.\n",
    "    classifier: sklearn compatible classifier\n",
    "        Classifier to do eval on.\n",
    "    figs_path: string\n",
    "        Where to put the resulting evaluation figures.\n",
    "    title: string\n",
    "        Title to stick on all the plots and use for file name.\n",
    "    shap_feature_names: List[str]\n",
    "        List of feature names for the SHAP plots.\n",
    "    thresh_for_f1: float\n",
    "        Threshold for F1 computation. Defaults to 0.5.\n",
    "    shap_plot_type: str\n",
    "        Type of shap plot. Defaults to 'dot'.\n",
    "        Can also be: 'bar', 'violin', 'compact_dot'\n",
    "    nameless_classifier: sklearn compatible classifier\n",
    "        Classifier to do eval on that doesn't use name features.\n",
    "    nameless_X: np.array\n",
    "        Feature matrix of features to do eval on excluding name features.\n",
    "    nameless_feature_names: List[str]\n",
    "        List of feature names for the SHAP plots excluding name features.\n",
    "    skip_shap: bool\n",
    "        Whether to skip SHAP entirely.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict: A dictionary of common pairwise metrics.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(figs_path):\n",
    "        os.makedirs(figs_path)\n",
    "\n",
    "    # filename base will be title but lower and underscores\n",
    "    base_name = title.lower().replace(\" \", \"_\")\n",
    "    if hasattr(classifier, \"classifier\"):\n",
    "        classifier = classifier.classifier\n",
    "\n",
    "    if nameless_classifier is not None and hasattr(nameless_classifier, \"classifier\"):\n",
    "        nameless_classifier = nameless_classifier.classifier\n",
    "\n",
    "    if nameless_classifier is not None:\n",
    "        y_prob = (classifier.predict_proba(X)[:, 1] + nameless_classifier.predict_proba(nameless_X)[:, 1]) / 2\n",
    "    else:\n",
    "        y_prob = classifier.predict_proba(X)[:, 1]\n",
    "\n",
    "    # plot AUROC\n",
    "    fpr, tpr, _ = roc_curve(y, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(0, figsize=(15, 15))\n",
    "    plt.plot(fpr, tpr, lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve for {title}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(join(figs_path, base_name + \"_roc.png\"))\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # plot AUPR\n",
    "    precision, recall, _ = precision_recall_curve(y, y_prob)\n",
    "    avg_precision = average_precision_score(y, y_prob)\n",
    "\n",
    "    plt.figure(1, figsize=(15, 15))\n",
    "    plt.plot(\n",
    "        precision,\n",
    "        recall,\n",
    "        lw=2,\n",
    "        label=\"PR curve (average precision = %0.2f)\" % avg_precision,\n",
    "    )\n",
    "    plt.xlabel(\"Precision\")\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.title(f\"PR Curve for {title}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(join(figs_path, base_name + \"_pr.png\"))\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # plot SHAP\n",
    "    # note that SHAP doesn't support model stacking directly\n",
    "    # so we have to approximate by getting SHAP values for each\n",
    "    # of the models inside the stack\n",
    "    if not skip_shap:\n",
    "        from s2apler.model import VotingClassifier  # avoid circular import\n",
    "\n",
    "        if isinstance(classifier, VotingClassifier):\n",
    "            shap_values_all = []\n",
    "            for c in classifier.estimators:\n",
    "                if isinstance(c, CalibratedClassifierCV):\n",
    "                    shap_values_all.append(shap.TreeExplainer(c.base_estimator).shap_values(X)[1])\n",
    "                else:\n",
    "                    shap_values_all.append(shap.TreeExplainer(c).shap_values(X)[1])\n",
    "            shap_values = [np.mean(shap_values_all, axis=0)]\n",
    "        elif nameless_classifier is not None:\n",
    "            shap_values = []\n",
    "            for c, d in [(classifier, X), (nameless_classifier, nameless_X)]:\n",
    "                if isinstance(classifier, CalibratedClassifierCV):\n",
    "                    shap_values.append(shap.TreeExplainer(c.base_estimator).shap_values(d)[1])\n",
    "                else:\n",
    "                    shap_values.append(shap.TreeExplainer(c).shap_values(d)[1])\n",
    "        elif isinstance(classifier, CalibratedClassifierCV):\n",
    "            shap_values = shap.TreeExplainer(classifier.base_estimator).shap_values(X)[1]\n",
    "        else:\n",
    "            shap_values = shap.TreeExplainer(classifier).shap_values(X)[1]\n",
    "\n",
    "        if isinstance(shap_values, list):\n",
    "            for i, (shap_value, feature_names, d) in enumerate(\n",
    "                zip(\n",
    "                    shap_values,\n",
    "                    [shap_feature_names, nameless_feature_names],\n",
    "                    [X, nameless_X],\n",
    "                )\n",
    "            ):\n",
    "                assert feature_names is not None, \"neither feature_names should be None here\"\n",
    "                plt.figure(2 + i)\n",
    "                shap.summary_plot(\n",
    "                    shap_value,\n",
    "                    d,\n",
    "                    plot_type=shap_plot_type,\n",
    "                    feature_names=feature_names,\n",
    "                    show=False,\n",
    "                    max_display=len(feature_names),\n",
    "                )\n",
    "                # plt.title(f\"{i}: SHAP Values for {title}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(join(figs_path, base_name + f\"_shap_{i}.png\"))\n",
    "                plt.clf()\n",
    "                plt.close()\n",
    "        else:\n",
    "            plt.figure(2)\n",
    "            shap.summary_plot(\n",
    "                shap_values,\n",
    "                X,\n",
    "                plot_type=shap_plot_type,\n",
    "                feature_names=shap_feature_names,\n",
    "                show=False,\n",
    "                max_display=len(shap_feature_names),\n",
    "            )\n",
    "            # plt.title(f\"SHAP Values for {title}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(join(figs_path, base_name + \"_shap.png\"))\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "\n",
    "    # collect metrics and return\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y, y_prob > thresh_for_f1, beta=1.0, average=\"macro\")\n",
    "    metrics = {\n",
    "        \"AUROC\": np.round(roc_auc, 3),\n",
    "        \"Average Precision\": np.round(avg_precision, 3),\n",
    "        \"F1\": np.round(f1, 3),\n",
    "        \"Precision\": np.round(pr, 3),\n",
    "        \"Recall\": np.round(rc, 3),\n",
    "    }\n",
    "\n",
    "    return metrics, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [\n",
    "    \"author_similarity\",\n",
    "    \"venue_similarity\",\n",
    "    \"year_diff\",\n",
    "    \"title_similarity\",\n",
    "    \"abstract_similarity\",\n",
    "    \"paper_quality\",\n",
    "]\n",
    "\n",
    "nameless_features_to_use = [\n",
    "    feature_name\n",
    "    for feature_name in features_to_use\n",
    "    if feature_name not in {\"title_similarity\", \"abstract_similarity\"}\n",
    "]\n",
    "\n",
    "featurization_info = FeaturizationInfo(features_to_use=features_to_use)\n",
    "nameless_featurization_info = FeaturizationInfo(\n",
    "    features_to_use=nameless_features_to_use\n",
    ")\n",
    "\n",
    "feature_names = featurization_info.get_feature_names()\n",
    "nameless_feature_names = nameless_featurization_info.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 04:04:02,359 - s2apler - INFO - loading papers\n",
      "2023-06-29 04:06:31,254 - s2apler - INFO - loaded papers\n",
      "2023-06-29 04:06:31,259 - s2apler - INFO - loading clusters\n",
      "2023-06-29 04:06:58,300 - s2apler - INFO - loaded clusters, loading specter\n",
      "2023-06-29 04:06:58,301 - s2apler - INFO - loaded specter, loading cluster seeds\n",
      "2023-06-29 04:06:58,302 - s2apler - INFO - loaded cluster seeds\n",
      "2023-06-29 04:06:58,303 - s2apler - INFO - making paper to cluster id\n",
      "2023-06-29 04:07:00,147 - s2apler - INFO - made paper to cluster id\n",
      "2023-06-29 04:07:04,350 - s2apler - INFO - preprocessing papers\n",
      "Preprocessing papers: 100%|██████████| 1783603/1783603 [17:03<00:00, 1742.03it/s]\n",
      "2023-06-29 04:24:23,909 - s2apler - INFO - preprocessed papers\n"
     ]
    }
   ],
   "source": [
    "dataset = PDData(\n",
    "    join(CONFIG[\"main_data_dir\"], \"papers.json\"),\n",
    "    clusters=join(CONFIG[\"main_data_dir\"], \"clusters.json\"),\n",
    "    name=\"test_dataset\",\n",
    "    n_jobs=8,\n",
    "    balanced_pair_sample=False,\n",
    "    train_pairs_size=5000000,\n",
    "    val_pairs_size=5000000,\n",
    "    test_pairs_size=5000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_papers_dict,\n",
    "    val_papers_dict,\n",
    "    test_papers_dict,\n",
    ") = dataset.split_cluster_papers()  # type: ignore\n",
    "\n",
    "train_pairs, val_pairs, test_pairs = dataset.split_pairs(\n",
    "    train_papers_dict, val_papers_dict, test_papers_dict\n",
    ")\n",
    "\n",
    "inverse_papers_dict = {}\n",
    "for k, v in train_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k\n",
    "        \n",
    "for k, v in val_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k\n",
    "        \n",
    "for k, v in test_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = many_pairs_featurize(\n",
    "    train_pairs,\n",
    "    dataset,\n",
    "    featurization_info,\n",
    "    16,\n",
    "    False,\n",
    "    DEFAULT_CHUNK_SIZE,\n",
    "    nameless_featurization_info,\n",
    "    np.nan,\n",
    "    False,\n",
    ")\n",
    "\n",
    "val_features = many_pairs_featurize(\n",
    "    val_pairs,\n",
    "    dataset,\n",
    "    featurization_info,\n",
    "    16,\n",
    "    False,\n",
    "    DEFAULT_CHUNK_SIZE,\n",
    "    nameless_featurization_info,\n",
    "    np.nan,\n",
    "    False,\n",
    ")\n",
    "\n",
    "test_features = many_pairs_featurize(\n",
    "    test_pairs,\n",
    "    dataset,\n",
    "    featurization_info,\n",
    "    16,\n",
    "    False,\n",
    "    DEFAULT_CHUNK_SIZE,\n",
    "    nameless_featurization_info,\n",
    "    np.nan,\n",
    "    False,\n",
    ")\n",
    "\n",
    "with open(join(PROJECT_ROOT_PATH, \"data\", \"training_data_v2022_11_21.pickle\"), 'wb') as f:\n",
    "    pickle.dump((train_papers_dict, train_pairs, train_features, val_papers_dict, val_pairs, val_features, test_papers_dict, test_pairs, test_features), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot block sizes\n",
    "blocks = dataset.get_blocks()\n",
    "block_sizes = np.array([len(i) for i in blocks.values()])\n",
    "cluster_sizes_no_orphans = np.array([len(i['sourced_paper_ids']) for k, i in dataset.clusters.items() if not k.endswith('_orphans')])\n",
    "cluster_sizes_orphans = np.array([len(i['sourced_paper_ids']) for k, i in dataset.clusters.items() if k.endswith('_orphans')])\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(block_sizes, bins=100)\n",
    "plt.xlabel('block size')\n",
    "plt.ylabel('number of blocks')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(cluster_sizes_no_orphans, bins=range(1, 10))\n",
    "plt.xlabel('cluster size (excluding orphans)')\n",
    "plt.ylabel('number of clusters')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(cluster_sizes_orphans, bins=100)\n",
    "plt.xlabel('cluster size (orphans only)')\n",
    "plt.ylabel('number of clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(PROJECT_ROOT_PATH, \"data\", \"training_data_v2022_11_21.pickle\"), 'rb') as f:\n",
    "    train_papers_dict, train_pairs, train_features, val_papers_dict, val_pairs, val_features, test_papers_dict, test_pairs, test_features = pickle.load(f)\n",
    "    \n",
    "inverse_papers_dict = {}\n",
    "for k, v in train_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k\n",
    "        \n",
    "for k, v in val_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k\n",
    "        \n",
    "for k, v in test_papers_dict.items():\n",
    "    for v_i in v:\n",
    "        inverse_papers_dict[v_i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:43<00:00, 14.66s/trial, best loss: -0.9860376866240141]\n",
      "{'Feature Removed': 'author_names_similarity', 'AUROC': 0.9992, 'Average Precision': 0.9981, 'F1 (0.65 thresh)': 0.984, 'Precision': 0.9859, 'Recall': 0.9821, 'Accuracy (0.65 thresh)': 0.9864}\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.91s/trial, best loss: -0.98679813886249]\n",
      "{'Feature Removed': 'author_first_letter_compatibility', 'AUROC': 0.9993, 'Average Precision': 0.9983, 'F1 (0.65 thresh)': 0.9852, 'Precision': 0.9866, 'Recall': 0.9839, 'Accuracy (0.65 thresh)': 0.9875}\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.79s/trial, best loss: -0.987397287532993] \n",
      "{'Feature Removed': 'venue_similarity', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9859, 'Precision': 0.9876, 'Recall': 0.9842, 'Accuracy (0.65 thresh)': 0.988}\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.12s/trial, best loss: -0.9859664483495463]\n",
      "{'Feature Removed': 'year_diff', 'AUROC': 0.9992, 'Average Precision': 0.9982, 'F1 (0.65 thresh)': 0.984, 'Precision': 0.986, 'Recall': 0.9822, 'Accuracy (0.65 thresh)': 0.9865}\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.14s/trial, best loss: -0.9873192281332277]\n",
      "{'Feature Removed': 'title_character_similarity', 'AUROC': 0.9993, 'Average Precision': 0.9983, 'F1 (0.65 thresh)': 0.9855, 'Precision': 0.9868, 'Recall': 0.9842, 'Accuracy (0.65 thresh)': 0.9877}\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.68s/trial, best loss: -0.9872006223465752]\n",
      "{'Feature Removed': 'title_numeral_similarity', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9862, 'Precision': 0.9876, 'Recall': 0.9848, 'Accuracy (0.65 thresh)': 0.9883}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.50s/trial, best loss: -0.9874054384570017]\n",
      "{'Feature Removed': 'title_special_publication_word_similarity', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9859, 'Precision': 0.9875, 'Recall': 0.9843, 'Accuracy (0.65 thresh)': 0.988}\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.67s/trial, best loss: -0.9873773661856922]\n",
      "{'Feature Removed': 'title_year_similarity', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9861, 'Precision': 0.9877, 'Recall': 0.9846, 'Accuracy (0.65 thresh)': 0.9883}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.51s/trial, best loss: -0.9873637834881519]\n",
      "{'Feature Removed': 'title_levenshtein', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.986, 'Precision': 0.9876, 'Recall': 0.9844, 'Accuracy (0.65 thresh)': 0.9881}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.63s/trial, best loss: -0.9865569580954417]\n",
      "{'Feature Removed': 'title_prefix', 'AUROC': 0.9993, 'Average Precision': 0.9985, 'F1 (0.65 thresh)': 0.9853, 'Precision': 0.9873, 'Recall': 0.9833, 'Accuracy (0.65 thresh)': 0.9875}\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.18s/trial, best loss: -0.9876133278948935]\n",
      "{'Feature Removed': 'title_jaro', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.986, 'Precision': 0.9877, 'Recall': 0.9844, 'Accuracy (0.65 thresh)': 0.9882}\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.19s/trial, best loss: -0.986243536522418]\n",
      "{'Feature Removed': 'abstract_word_similarity', 'AUROC': 0.9993, 'Average Precision': 0.9983, 'F1 (0.65 thresh)': 0.9846, 'Precision': 0.9863, 'Recall': 0.9831, 'Accuracy (0.65 thresh)': 0.987}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.65s/trial, best loss: -0.9873576282621148]\n",
      "{'Feature Removed': 'paper_field_count_abstract', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.986, 'Precision': 0.9877, 'Recall': 0.9844, 'Accuracy (0.65 thresh)': 0.9882}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.47s/trial, best loss: -0.9874143120890602]\n",
      "{'Feature Removed': 'paper_field_count_authors', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9862, 'Precision': 0.9877, 'Recall': 0.9847, 'Accuracy (0.65 thresh)': 0.9883}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.40s/trial, best loss: -0.9874861837426209]\n",
      "{'Feature Removed': 'paper_field_count_venue', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9862, 'Precision': 0.9879, 'Recall': 0.9844, 'Accuracy (0.65 thresh)': 0.9883}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.52s/trial, best loss: -0.9870550424706921]\n",
      "{'Feature Removed': 'source_count_MergedPDFExtraction', 'AUROC': 0.9993, 'Average Precision': 0.9985, 'F1 (0.65 thresh)': 0.9859, 'Precision': 0.9875, 'Recall': 0.9843, 'Accuracy (0.65 thresh)': 0.9881}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.63s/trial, best loss: -0.987513506259645] \n",
      "{'Feature Removed': 'source_count_publisher', 'AUROC': 0.9993, 'Average Precision': 0.9985, 'F1 (0.65 thresh)': 0.9859, 'Precision': 0.9877, 'Recall': 0.9842, 'Accuracy (0.65 thresh)': 0.9881}\n",
      "100%|██████████| 3/3 [00:46<00:00, 15.62s/trial, best loss: -0.987131070384591]\n",
      "{'Feature Removed': 'sources_are_same', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9859, 'Precision': 0.9875, 'Recall': 0.9843, 'Accuracy (0.65 thresh)': 0.9881}\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.68s/trial, best loss: -0.9875767824583213]\n",
      "{'Feature Removed': 'None', 'AUROC': 0.9994, 'Average Precision': 0.9986, 'F1 (0.65 thresh)': 0.9863, 'Precision': 0.9878, 'Recall': 0.9848, 'Accuracy (0.65 thresh)': 0.9884}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Removed</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>F1 (0.65 thresh)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy (0.65 thresh)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author_names_similarity</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author_first_letter_compatibility</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.9852</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>venue_similarity</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.9880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>year_diff</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.9865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>title_character_similarity</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.9877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>title_numeral_similarity</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>title_special_publication_word_similarity</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>title_year_similarity</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>title_levenshtein</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>0.9881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>title_prefix</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>title_jaro</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>0.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abstract_word_similarity</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9831</td>\n",
       "      <td>0.9870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paper_field_count_abstract</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>0.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>paper_field_count_authors</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paper_field_count_venue</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>source_count_MergedPDFExtraction</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>source_count_publisher</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>0.9881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sources_are_same</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9859</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.9881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>0.9878</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.9884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Feature Removed   AUROC  Average Precision  \\\n",
       "0                     author_names_similarity  0.9992             0.9981   \n",
       "1           author_first_letter_compatibility  0.9993             0.9983   \n",
       "2                            venue_similarity  0.9994             0.9986   \n",
       "3                                   year_diff  0.9992             0.9982   \n",
       "4                  title_character_similarity  0.9993             0.9983   \n",
       "5                    title_numeral_similarity  0.9994             0.9986   \n",
       "6   title_special_publication_word_similarity  0.9994             0.9986   \n",
       "7                       title_year_similarity  0.9994             0.9986   \n",
       "8                           title_levenshtein  0.9994             0.9986   \n",
       "9                                title_prefix  0.9993             0.9985   \n",
       "10                                 title_jaro  0.9994             0.9986   \n",
       "11                   abstract_word_similarity  0.9993             0.9983   \n",
       "12                 paper_field_count_abstract  0.9994             0.9986   \n",
       "13                  paper_field_count_authors  0.9994             0.9986   \n",
       "14                    paper_field_count_venue  0.9994             0.9986   \n",
       "15           source_count_MergedPDFExtraction  0.9993             0.9985   \n",
       "16                     source_count_publisher  0.9993             0.9985   \n",
       "17                           sources_are_same  0.9994             0.9986   \n",
       "18                                       None  0.9994             0.9986   \n",
       "\n",
       "    F1 (0.65 thresh)  Precision  Recall  Accuracy (0.65 thresh)  \n",
       "0             0.9840     0.9859  0.9821                  0.9864  \n",
       "1             0.9852     0.9866  0.9839                  0.9875  \n",
       "2             0.9859     0.9876  0.9842                  0.9880  \n",
       "3             0.9840     0.9860  0.9822                  0.9865  \n",
       "4             0.9855     0.9868  0.9842                  0.9877  \n",
       "5             0.9862     0.9876  0.9848                  0.9883  \n",
       "6             0.9859     0.9875  0.9843                  0.9880  \n",
       "7             0.9861     0.9877  0.9846                  0.9883  \n",
       "8             0.9860     0.9876  0.9844                  0.9881  \n",
       "9             0.9853     0.9873  0.9833                  0.9875  \n",
       "10            0.9860     0.9877  0.9844                  0.9882  \n",
       "11            0.9846     0.9863  0.9831                  0.9870  \n",
       "12            0.9860     0.9877  0.9844                  0.9882  \n",
       "13            0.9862     0.9877  0.9847                  0.9883  \n",
       "14            0.9862     0.9879  0.9844                  0.9883  \n",
       "15            0.9859     0.9875  0.9843                  0.9881  \n",
       "16            0.9859     0.9877  0.9842                  0.9881  \n",
       "17            0.9859     0.9875  0.9843                  0.9881  \n",
       "18            0.9863     0.9878  0.9848                  0.9884  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, nameless_X_train = train_features\n",
    "X_val, y_val, nameless_X_val = val_features\n",
    "X_test, y_test, nameless_X_test = test_features\n",
    "\n",
    "all_metrics = []\n",
    "for i in range(X_train.shape[1]):\n",
    "    sub_inds = list(range(X_train.shape[1]))\n",
    "    sub_inds.remove(i)\n",
    "    model = PairwiseModeler(n_iter=3, n_jobs=15)\n",
    "    model.fit(X_train[:, sub_inds], y_train, X_val[:, sub_inds], y_val)\n",
    "    y_pred = model.predict_proba(X_test[:, sub_inds])\n",
    "    classifier = model.classifier\n",
    "    y_prob = classifier.predict_proba(X_test[:, sub_inds])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    avg_precision = average_precision_score(y_test, y_prob)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(y_test, y_prob > 0.75, beta=1.0, average=\"macro\")\n",
    "    metrics = {\n",
    "        \"Feature Removed\": feature_names[i],\n",
    "        \"AUROC\": np.round(roc_auc, 4),\n",
    "        \"Average Precision\": np.round(avg_precision, 4),\n",
    "        \"F1 (0.75 thresh)\": np.round(f1, 4),\n",
    "        \"Precision\": np.round(pr, 4),\n",
    "        \"Recall\": np.round(rc, 4),\n",
    "        \"Accuracy (0.75 thresh)\": np.round(np.mean(y_test == (y_prob > 0.75)), 4),\n",
    "    }\n",
    "    print(metrics)\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "# remove nothing\n",
    "sub_inds = list(range(X_train.shape[1]))\n",
    "model = PairwiseModeler(n_iter=3, n_jobs=15)\n",
    "model.fit(X_train[:, sub_inds], y_train, X_val[:, sub_inds], y_val)\n",
    "y_pred = model.predict_proba(X_test[:, sub_inds])\n",
    "classifier = model.classifier\n",
    "y_prob = classifier.predict_proba(X_test[:, sub_inds])[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "avg_precision = average_precision_score(y_test, y_prob)\n",
    "pr, rc, f1, _ = precision_recall_fscore_support(y_test, y_prob > 0.75, beta=1.0, average=\"macro\")\n",
    "metrics = {\n",
    "    \"Feature Removed\": \"None\",\n",
    "    \"AUROC\": np.round(roc_auc, 4),\n",
    "    \"Average Precision\": np.round(avg_precision, 4),\n",
    "    \"F1 (0.75 thresh)\": np.round(f1, 4),\n",
    "    \"Precision\": np.round(pr, 4),\n",
    "    \"Recall\": np.round(rc, 4),\n",
    "    \"Accuracy (0.75 thresh)\": np.round(np.mean(y_test == (y_prob > 0.75)), 4),\n",
    "}\n",
    "print(metrics)\n",
    "all_metrics.append(metrics)\n",
    "df_results = pd.DataFrame(all_metrics)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sergey/S2APLER/scripts/analyze_final_data.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mBest so far\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# let's do some debugging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# for each wrong row, get the papers and print them out for now\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m X_train, y_train, nameless_X_train \u001b[39m=\u001b[39m train_features\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m X_val, y_val, nameless_X_val \u001b[39m=\u001b[39m val_features\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bs2-elanding-24.reviz.ai2.in/home/sergey/S2APLER/scripts/analyze_final_data.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m X_test, y_test, nameless_X_test \u001b[39m=\u001b[39m test_features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Best so far\n",
    "\n",
    "before any changes: \n",
    "{'AUROC': 0.999, 'Average Precision': 0.999, 'F1': 0.987, 'Precision': 0.987, 'Recall': 0.988}\n",
    "\n",
    "adding unlatexing + a small bug in the publisher source list:\n",
    "{'AUROC': 0.999, 'Average Precision': 0.999, 'F1': 0.988, 'Precision': 0.987, 'Recall': 0.988}\n",
    "\"\"\"\n",
    "# let's do some debugging\n",
    "# for each wrong row, get the papers and print them out for now\n",
    "\n",
    "X_train, y_train, nameless_X_train = train_features\n",
    "X_val, y_val, nameless_X_val = val_features\n",
    "X_test, y_test, nameless_X_test = test_features\n",
    "\n",
    "model = PairwiseModeler(n_iter=25, n_jobs=8)\n",
    "model.fit(X_train, y_train, X_val, y_val)\n",
    "y_pred = model.predict_proba(X_test)\n",
    "y_pred_val = model.predict_proba(X_val)\n",
    "\n",
    "\n",
    "if len(X_test) > 50000:\n",
    "    # subsample for speed\n",
    "    np.random.seed(seed=0)\n",
    "    rand_ind = np.random.choice(len(X_test), 50000, replace=False)\n",
    "else:\n",
    "    # use all\n",
    "    rand_ind = np.arange(len(X_test))\n",
    "metrics, shap_values = pairwise_eval(\n",
    "    X_test[rand_ind, :], \n",
    "    y_test[rand_ind], \n",
    "    model, \n",
    "    join(PROJECT_ROOT_PATH, \"data\"), \n",
    "    'debug', \n",
    "    feature_names,\n",
    "    thresh_for_f1=0.5, \n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True)     16805\n",
      "(True, False)      123\n",
      "dtype: int64\n",
      "----------------\n",
      "Series([], dtype: int64)\n",
      "----------------\n",
      "(True, True)      10457\n",
      "(True, False)       349\n",
      "(False, False)        1\n",
      "dtype: int64\n",
      "----------------\n",
      "(True, True)    5200\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# how do our rules do? do they fix errors? do they cause errors?\n",
    "from s2apler.text import PUBLISHER_SOURCES\n",
    "preds = model.predict_proba(X_test)[:, 1] > 0.65\n",
    "doi_label = []\n",
    "pmid_label = []\n",
    "pdf_hash_label = []\n",
    "source_label = []\n",
    "{'3568424289','3758874283', '3754072317'}\n",
    "for pr, (paper_1_id, paper_2_id, label) in zip(preds, test_pairs):\n",
    "    paper_1 = dataset.papers[paper_1_id]\n",
    "    paper_2 = dataset.papers[paper_2_id]\n",
    "\n",
    "    if paper_1.doi is not None and paper_2.doi is not None and paper_1.doi == paper_2.doi:\n",
    "        doi_label.append((label == 1.0, pr == 1))\n",
    "    elif paper_1.pmid is not None and paper_2.pmid is not None and paper_1.pmid == paper_2.pmid:\n",
    "        pmid_label.append((label == 1.0, pr == 1))\n",
    "    elif paper_1.pdf_hash is not None and paper_2.pdf_hash is not None and paper_1.pdf_hash == paper_2.pdf_hash:\n",
    "        pdf_hash_label.append((label == 1.0, pr == 1))\n",
    "    elif (\n",
    "        paper_1.source_id is not None\n",
    "        and paper_2.source_id is not None\n",
    "        and paper_1.source in PUBLISHER_SOURCES\n",
    "        and paper_1.source == paper_2.source\n",
    "        and paper_1.source_id != paper_2.source_id\n",
    "    ):\n",
    "        source_label.append((label == 0.0, pr == 0))\n",
    "\n",
    "print(\n",
    "    pd.value_counts(doi_label),\n",
    "    pd.value_counts(pmid_label),\n",
    "    pd.value_counts(pdf_hash_label),\n",
    "    pd.value_counts(source_label),\n",
    "    sep='\\n----------------\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "df = pd.DataFrame({feat: X_train[:, feature_names.index(feat)] for feat in feature_names})\n",
    "df['y_train'] = y_train\n",
    "df.groupby(['title_character_similarity'])['y_train'].value_counts(dropna=False, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs_sub = np.array(test_pairs)[rand_ind]\n",
    "\n",
    "def simple_paper(p):\n",
    "    authors = ', '.join([a.author_info_full_name for a in p.authors])\n",
    "    abs = p.abstract[:100] + '...' if p.abstract is not None else None\n",
    "    return f\"ID: {p.sourced_paper_id}\\nSource: {p.source}\\nCluster ID: {dataset.paper_to_cluster_id[p.sourced_paper_id]}\\nTitle: {p.title}\\nAuthors: {authors}\\nVenue: {p.venue}\\nJournal: {p.journal_name}\\nYear: {p.year}\\nAbstract: {abs}\\nDOI: {p.doi}\"\n",
    "    \n",
    "\n",
    "def debug_print(ind):\n",
    "    x = X_test[rand_ind, :][ind, :]\n",
    "    sh = shap_values[ind, :]\n",
    "    p1 = dataset.papers[test_pairs_sub[ind][0]]\n",
    "    p2 = dataset.papers[test_pairs_sub[ind][1]]\n",
    "    print(\"Correct label:\", y_test[rand_ind][ind])\n",
    "    print(\"Predicted class probs:\", np.round(y_pred[rand_ind][ind, :], 3), '\\n')\n",
    "    df = pd.DataFrame({'feature': feature_names, 'value': x, 'shap': sh})\n",
    "    print(df)\n",
    "    print(\"\\nPaper 1\\n------\\n\" + simple_paper(p1), '\\n')\n",
    "    print(\"Paper 2\\n------\\n\" + simple_paper(p2))\n",
    "\n",
    "print(\"total wrong:\", np.sum(y_test[rand_ind] != (y_pred[rand_ind][:, 1] > 0.65)))\n",
    "rows_of_interest = (X_test[rand_ind, feature_names.index('title_character_similarity')] == 0) # & ((X_test[rand_ind, feature_names.index('sources_are_same')] == 2.0))\n",
    "wrong_inds = np.flatnonzero( (y_test[rand_ind] != (y_pred[rand_ind][:, 1] > 0.65)) )# & rows_of_interest)\n",
    "print(f\"total wrong where in rows of interest:\", len(wrong_inds))\n",
    "a = sample(list(wrong_inds), np.minimum(5, len(wrong_inds)))\n",
    "for i in a:\n",
    "    debug_print(i)\n",
    "    print('\\n\\n===========================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, nameless_X_train = train_features\n",
    "X_val, y_val, nameless_X_val = val_features\n",
    "X_test, y_test, nameless_X_test = test_features\n",
    "\n",
    "model = PairwiseModeler(n_iter=25, n_jobs=16)\n",
    "model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# using default threshold eps of 0.65\n",
    "cluster = Clusterer(\n",
    "    featurization_info,\n",
    "    model.classifier,\n",
    "    n_iter=25,\n",
    "    use_default_constraints_as_supervision=True,\n",
    "    n_jobs=16,\n",
    ")\n",
    "#cluster.fit(dataset)  \n",
    "cluster.eps = 0.65\n",
    "\n",
    "cluster_metrics_constraint, b3_metrics_per_signature_constraint = cluster_eval(\n",
    "    dataset,\n",
    "    cluster,\n",
    "    split=\"test\",\n",
    ")\n",
    "cluster_metrics_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip([i['misc']['vals']['eps'][0] for i in cluster.hyperopt_trials_store.trials], cluster.hyperopt_trials_store.losses())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default threshold eps of 0.65\n",
    "cluster = Clusterer(\n",
    "    featurization_info,\n",
    "    model.classifier,\n",
    "    use_default_constraints_as_supervision=False,\n",
    "    n_iter=25,\n",
    "    n_jobs=16,\n",
    ")\n",
    "cluster.eps = 0.65\n",
    "# cluster.fit(dataset)  \n",
    "\n",
    "cluster_metrics_no_constraint, b3_metrics_per_signature_no_constraint = cluster_eval(\n",
    "    dataset,\n",
    "    cluster,\n",
    "    split=\"test\",\n",
    ")\n",
    "cluster_metrics_no_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip([i['misc']['vals']['eps'][0] for i in cluster.hyperopt_trials_store.trials], cluster.hyperopt_trials_store.losses())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 prod: {'B3 (P, R, F1)': (1.0, 0.995, 0.997), 'Pred bigger ratio (mean, count)': (1.0, 34163), 'True bigger ratio (mean, count)': (1.97, 391)}\n",
      "Model (no constraints used): {'B3 (P, R, F1)': (0.993, 0.991, 0.992), 'Pred bigger ratio (mean, count)': (1.01, 33896), 'True bigger ratio (mean, count)': (1.94, 658)}\n",
      "Model (constraints used): {'B3 (P, R, F1)': (0.999, 0.998, 0.999), 'Pred bigger ratio (mean, count)': (1.0, 34429), 'True bigger ratio (mean, count)': (1.87, 125)}\n"
     ]
    }
   ],
   "source": [
    "cluster_metrics_s2, b3_metrics_per_signature_s2 = cluster_eval(\n",
    "    dataset,\n",
    "    cluster,\n",
    "    split=\"test\",\n",
    "    use_s2_clusters=True\n",
    ")\n",
    "print('S2 prod:', cluster_metrics_s2)\n",
    "print('Model (no constraints used):', cluster_metrics_no_constraint)\n",
    "print('Model (constraints used):', cluster_metrics_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(PROJECT_ROOT_PATH, \"data\", \"cluster_results_v2022_11_13.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((cluster, cluster_metrics_constraint, b3_metrics_per_signature_constraint, cluster_metrics_no_constraint, b3_metrics_per_signature_no_constraint, cluster_metrics_s2, b3_metrics_per_signature_s2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('theeconomicsoftheglobaldefenceindus', 8), ('thegenderofcaste', 7), ('emergingthreatstohumanrights', 7), ('minimalinvasiveundroboterassistiert', 6), ('sepsisquelleabdomenzwischeninterven', 6)]\n",
      "[('tumorimmunologyandcancerimmunothera', 12), ('scoreadjustmentsfordifferentialitem', 8), ('theeffectsofintravenousanestheticso', 7), ('incidenceoffirstprimarycentralnervo', 6), ('studiesontheactionofwettingagentson', 6)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_block_dict, val_block_dict, test_block_dict = dataset.split_blocks_helper(dataset.get_blocks())\n",
    "\n",
    "# find an example where we do better than S2 and where S2 does better than us\n",
    "s2_better = []\n",
    "me_better = []\n",
    "for i in b3_metrics_per_signature_s2.keys():\n",
    "    if b3_metrics_per_signature_s2[i][2] > b3_metrics_per_signature_constraint[i][2]:\n",
    "        s2_better.append(i)\n",
    "    elif b3_metrics_per_signature_s2[i][2] < b3_metrics_per_signature_constraint[i][2]:\n",
    "        me_better.append(i)\n",
    "len(s2_better), len(me_better)\n",
    "\n",
    "# find biggest blocks where each one is better\n",
    "s2_better_blocks = Counter()\n",
    "for i in s2_better:\n",
    "    s2_better_blocks[dataset.papers[i].block] += 1\n",
    "    \n",
    "me_better_blocks = Counter()\n",
    "for i in me_better:\n",
    "    me_better_blocks[dataset.papers[i].block] += 1\n",
    "    \n",
    "print(s2_better_blocks.most_common(5))\n",
    "print(me_better_blocks.most_common(5))\n",
    "\n",
    "# print out the truth for each block, as well as both clusterings, and then exclude \n",
    "block_dict = {i: test_block_dict[i] for i in list(s2_better_blocks) + list(me_better_blocks)}\n",
    "cluster_to_papers = dataset.construct_cluster_to_papers(block_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 17:57:13,429 - s2and - INFO - Making 179 distance matrices\n",
      "2022-11-15 17:57:13,432 - s2and - INFO - Initializing pairwise_probas\n",
      "2022-11-15 17:57:13,440 - s2and - INFO - Pairwise probas initialized, starting making all pairs\n",
      "2022-11-15 17:57:13,441 - s2and - INFO - Featurizing batch 0/1\n",
      "2022-11-15 17:57:13,442 - s2and - INFO - Getting constraints\n",
      "2022-11-15 17:57:14,344 - s2and - INFO - Creating 293081 pieces of work\n",
      "Creating work: 293081it [00:00, 972075.25it/s]\n",
      "2022-11-15 17:57:14,649 - s2and - INFO - Created pieces of work\n",
      "2022-11-15 17:57:14,650 - s2and - INFO - Cached changed, doing 288689 work in parallel\n",
      "Doing work: 100%|██████████| 288689/288689 [00:05<00:00, 49122.86it/s]\n",
      "2022-11-15 17:58:01,830 - s2and - INFO - Work completed\n",
      "2022-11-15 17:58:01,832 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-11-15 17:58:01,897 - s2and - INFO - Numpy arrays made\n",
      "2022-11-15 17:58:01,926 - s2and - INFO - Making predict flags\n",
      "2022-11-15 17:58:02,013 - s2and - INFO - Pairwise classification\n",
      "2022-11-15 17:58:02,172 - s2and - INFO - Starting to make matrices\n",
      "Writing matrices: 100%|██████████| 293081/293081 [00:00<00:00, 711110.32it/s] \n",
      "2022-11-15 17:58:02,588 - s2and - INFO - 179 distance matrices made\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.966, 0.96, 0.963) (0.985, 0.842, 0.908)\n"
     ]
    }
   ],
   "source": [
    "cluster.use_default_constraints_as_supervision = True  # constraints help a lot\n",
    "pred_clusters_us, dists_us = cluster.predict(block_dict, dataset, use_s2_clusters=False, dists=None)\n",
    "pred_clusters_s2, dists_s2 = cluster.predict(block_dict, dataset, use_s2_clusters=True, dists=None)\n",
    "\n",
    "(\n",
    "    b3_p,\n",
    "    b3_r,\n",
    "    b3_f1,\n",
    "    b3_metrics_per_paper,\n",
    "    pred_bigger_ratios,\n",
    "    true_bigger_ratios,\n",
    ") = b3_precision_recall_fscore(cluster_to_papers, pred_clusters_s2)\n",
    "metrics_s2 = (b3_p, b3_r, b3_f1)\n",
    "\n",
    "(\n",
    "    b3_p,\n",
    "    b3_r,\n",
    "    b3_f1,\n",
    "    b3_metrics_per_paper,\n",
    "    pred_bigger_ratios,\n",
    "    true_bigger_ratios,\n",
    ") = b3_precision_recall_fscore(cluster_to_papers, pred_clusters_us)\n",
    "metrics_us = (b3_p, b3_r, b3_f1)\n",
    "\n",
    "print(metrics_us, metrics_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On this block, S2 did better than we did on: theeconomicsoftheglobaldefenceindus\n",
      "True clustering: {'theeconomicsoftheglobaldefenceindus_2812410': ['3608588136', '1374001738436055040', '3485549719', '1437544349930885120'], 'theeconomicsoftheglobaldefenceindus_orphans': ['3202649882', '3538414372', '1440375106416152576', '3047027388', '2133287537'], 'theeconomicsoftheglobaldefenceindus_2457483': ['3140141835', '1438960302761709568', '3607612433', '3607861970']}\n",
      "S2 clustering: {219745050: ['3608588136', '3538414372', '1374001738436055040', '3485549719', '1437544349930885120'], 213847636: ['3202649882', '3140141835', '1438960302761709568', '3607612433', '3607861970'], 201334819: ['1440375106416152576', '3047027388', '2133287537']}\n",
      "Our clustering: {'theeconomicsoftheglobaldefenceindus_1': ['3608588136', '3538414372', '3485549719', '1437544349930885120'], 'theeconomicsoftheglobaldefenceindus_2': ['3202649882', '3140141835', '1438960302761709568', '1374001738436055040', '3607612433', '1440375106416152576', '3607861970', '3047027388', '2133287537']}\n",
      "\n",
      "On this block, we did better than S2: tumorimmunologyandcancerimmunothera\n",
      "True clustering: {'tumorimmunologyandcancerimmunothera_orphans': ['214945212', '1995004859', '3309370857', '1814946132', '1407156129', '210639262', '3310378919', '1567105080', '1418569576', '1814899500', '8990931', '1528161423', '1442499973', '3286834943'], 'tumorimmunologyandcancerimmunothera_6275855': ['3503541453', '3675032085', '2037796073', '3552204913', '3553114278'], 'tumorimmunologyandcancerimmunothera_6481313': ['1995421400', '3675033053', '2034478702', '3552398037', '3503541579', '3553477743', '3288687240']}\n",
      "S2 clustering: {13257425: ['214945212', '3503541453', '3675032085', '1995004859', '3309370857', '1407156129', '210639262', '2037796073', '1418569576', '1814899500', '8990931', '3288687240', '3552204913', '1528161423', '1442499973', '3286834943', '3553114278'], 52854370: ['1814946132', '1995421400', '3675033053', '3310378919', '1567105080', '2034478702', '3552398037', '3503541579', '3553477743']}\n",
      "Our clustering: {'tumorimmunologyandcancerimmunothera_1': ['214945212', '1814946132', '1995421400', '3675033053', '1407156129', '3310378919', '1567105080', '2034478702', '3552398037', '3503541579', '3553477743', '3288687240'], 'tumorimmunologyandcancerimmunothera_2': ['3503541453', '3675032085', '1995004859', '3309370857', '210639262', '2037796073', '1418569576', '1814899500', '8990931', '3552204913', '1528161423', '1442499973', '3286834943', '3553114278']}\n"
     ]
    }
   ],
   "source": [
    "def print_results(block):\n",
    "    papers_in_this_block = [k for k, v in dataset.papers.items() if v.block == block]\n",
    "    clusters_in_block = {k: v for k, v in cluster_to_papers.items() if k.startswith(block)}\n",
    "    print(\"True clustering:\", clusters_in_block)\n",
    "    clusters_in_block_s2 = {k: v for k, v in pred_clusters_s2.items() if len(set(papers_in_this_block).intersection(v)) > 0}\n",
    "    clusters_in_block_us = {k: v for k, v in pred_clusters_us.items() if len(set(papers_in_this_block).intersection(v)) > 0}\n",
    "    print(\"S2 clustering:\", clusters_in_block_s2)\n",
    "    print(\"Our clustering:\", clusters_in_block_us)\n",
    "    \n",
    "block = 'theeconomicsoftheglobaldefenceindus'\n",
    "print(f'On this block, S2 did better than we did on: {block}')\n",
    "print_results(block)\n",
    "print()\n",
    "block = 'tumorimmunologyandcancerimmunothera'\n",
    "print(f'On this block, we did better than S2: {block}')\n",
    "print_results(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Clusterer(\n",
    "    featurization_info,\n",
    "    model.classifier,\n",
    "    use_default_constraints_as_supervision=True,\n",
    "    n_iter=25,\n",
    "    n_jobs=16,\n",
    ")\n",
    "train_block_dict, val_block_dict, test_block_dict = dataset.split_blocks_helper(dataset.get_blocks())\n",
    "cluster_to_papers_test = dataset.construct_cluster_to_papers(test_block_dict)\n",
    "_, dists_temp = cluster.predict(test_block_dict, dataset, use_s2_clusters=False, dists=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 (0.99993, 0.95101, 0.97486)\n",
      "0.15000000000000002 (0.99993, 0.96465, 0.98197)\n",
      "0.20000000000000004 (0.99984, 0.97364, 0.98656)\n",
      "0.25000000000000006 (0.99984, 0.98077, 0.99021)\n",
      "0.30000000000000004 (0.99984, 0.98557, 0.99265)\n",
      "0.3500000000000001 (0.99979, 0.9902, 0.99497)\n",
      "0.40000000000000013 (0.9997, 0.99257, 0.99612)\n",
      "0.45000000000000007 (0.99955, 0.99422, 0.99688)\n",
      "0.5000000000000001 (0.99935, 0.99592, 0.99763)\n",
      "0.5500000000000002 (0.99926, 0.99666, 0.99796)\n",
      "0.6000000000000002 (0.99922, 0.99735, 0.99829)\n",
      "0.6500000000000001 (0.99914, 0.99785, 0.9985)\n",
      "0.7000000000000002 (0.99889, 0.99833, 0.99861)\n",
      "0.7500000000000002 (0.99862, 0.9986, 0.99861)\n",
      "0.8000000000000002 (0.99809, 0.99876, 0.99842)\n",
      "0.8500000000000002 (0.99755, 0.99887, 0.99821)\n"
     ]
    }
   ],
   "source": [
    "def b3_precision_recall_fscore(true_clus, pred_clus, skip_papers=None):\n",
    "    \"\"\"\n",
    "    Compute the B^3 variant of precision, recall and F-score.\n",
    "    Modified from: https://github.com/glouppe/beard/blob/master/beard/metrics/clustering.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_clus: Dict\n",
    "        dictionary with cluster id as keys and 1d array containing\n",
    "        the ground-truth paper id assignments as values.\n",
    "    pred_clus: Dict\n",
    "        dictionary with cluster id as keys and 1d array containing\n",
    "        the predicted paper id assignments as values.\n",
    "    skip_papers: List[string]\n",
    "        in the incremental setting blocks can be partially supervised,\n",
    "        hence those instances are not used for evaluation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: calculated precision\n",
    "    float: calculated recall\n",
    "    float: calculated F1\n",
    "    Dict: P/R/F1 per paper\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Amigo, Enrique, et al. \"A comparison of extrinsic clustering evaluation\n",
    "    metrics based on formal constraints.\" Information retrieval 12.4\n",
    "    (2009): 461-486.\n",
    "    \"\"\"\n",
    "\n",
    "    true_clusters = true_clus.copy()\n",
    "    pred_clusters = pred_clus.copy()\n",
    "\n",
    "    tcset = set()\n",
    "    for v in true_clusters.values():\n",
    "        tcset.update(v)\n",
    "    pcset = set()\n",
    "    for v in pred_clusters.values():\n",
    "        pcset.update(v)\n",
    "\n",
    "    if tcset != pcset:\n",
    "        raise ValueError(\"Predictions do not cover all the papers!\")\n",
    "\n",
    "    # incremental evaluation contains partially observed papers\n",
    "    # skip_papers are observed papers, which we skip for b3 calc.\n",
    "    if skip_papers is not None:\n",
    "        tcset = tcset.difference(skip_papers)\n",
    "\n",
    "    # anything from orphan clusters will also be skipped\n",
    "    # find orphan clusters, which are clusters that endwith ORPHAN_CLUSTER_KEY\n",
    "    orphan_clusters_names = [k for k in true_clusters.keys() if k.endswith(ORPHAN_CLUSTER_KEY)]\n",
    "    orphan_paper_ids = set()\n",
    "    for to_skip in orphan_clusters_names:\n",
    "        orphan_paper_ids.update(true_clusters[to_skip])\n",
    "\n",
    "    # need to remove orphans from true_clusters and pred_clusters\n",
    "    for cluster_id, cluster in true_clusters.items():\n",
    "        true_clusters[cluster_id] = frozenset(set(cluster).difference(orphan_paper_ids))\n",
    "\n",
    "    for cluster_id, cluster in pred_clusters.items():\n",
    "        pred_clusters[cluster_id] = frozenset(set(cluster).difference(orphan_paper_ids))\n",
    "\n",
    "    # invert lookups\n",
    "    reverse_true_clusters = {}\n",
    "    for k, v in true_clusters.items():\n",
    "        for vi in v:\n",
    "            reverse_true_clusters[vi] = k\n",
    "\n",
    "    reverse_pred_clusters = {}\n",
    "    for k, v in pred_clusters.items():\n",
    "        for vi in v:\n",
    "            reverse_pred_clusters[vi] = k\n",
    "\n",
    "    intersections = {}\n",
    "    per_paper_metrics = {}\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    n_samples = 0\n",
    "    true_bigger_ratios, pred_bigger_ratios = [], []\n",
    "    for item in sorted(tcset):\n",
    "        if item in orphan_paper_ids:\n",
    "            continue  # skipping orphans\n",
    "        pred_cluster_i = pred_clusters[reverse_pred_clusters[item]]\n",
    "        true_cluster_i = true_clusters[reverse_true_clusters[item]]\n",
    "\n",
    "        if len(pred_cluster_i) >= len(true_cluster_i):\n",
    "            pred_bigger_ratios.append(len(pred_cluster_i) / len(true_cluster_i))\n",
    "        else:\n",
    "            true_bigger_ratios.append(len(true_cluster_i) / len(pred_cluster_i))\n",
    "\n",
    "        if (pred_cluster_i, true_cluster_i) in intersections:\n",
    "            intersection = intersections[(pred_cluster_i, true_cluster_i)]\n",
    "        else:\n",
    "            intersection = pred_cluster_i.intersection(true_cluster_i)\n",
    "            intersections[(pred_cluster_i, true_cluster_i)] = intersection\n",
    "        _precision = len(intersection) / len(pred_cluster_i)\n",
    "        _recall = len(intersection) / len(true_cluster_i)\n",
    "        precision += _precision\n",
    "        recall += _recall\n",
    "        per_paper_metrics[item] = (\n",
    "            _precision,\n",
    "            _recall,\n",
    "            f1_score(_precision, _recall),\n",
    "        )\n",
    "        n_samples += 1\n",
    "\n",
    "    precision /= n_samples\n",
    "    recall /= n_samples\n",
    "\n",
    "    f_score = f1_score(precision, recall)\n",
    "\n",
    "    return (\n",
    "        np.round(precision, 5),\n",
    "        np.round(recall, 5),\n",
    "        np.round(f_score, 5),\n",
    "        per_paper_metrics,\n",
    "        pred_bigger_ratios,\n",
    "        true_bigger_ratios,\n",
    "    )\n",
    "\n",
    "print('s2', (0.99951, 0.99476, 0.99713))\n",
    "\n",
    "for eps in np.arange(0.6, 0.9, 0.025):\n",
    "    cluster.set_params({'eps': eps})\n",
    "    pred_clusters_temp, _ = cluster.predict(test_block_dict, dataset, use_s2_clusters=False, dists=dists_temp)\n",
    "\n",
    "    (\n",
    "        b3_p,\n",
    "        b3_r,\n",
    "        b3_f1,\n",
    "        b3_metrics_per_paper,\n",
    "        pred_bigger_ratios,\n",
    "        true_bigger_ratios,\n",
    "    ) = b3_precision_recall_fscore(cluster_to_papers_test, pred_clusters_temp)\n",
    "    print(np.round(eps, 3), (b3_p, b3_r, b3_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500000000000002 (0.99951, 0.99476, 0.99713)\n"
     ]
    }
   ],
   "source": [
    "pred_clusters_temp, _ = cluster.predict(test_block_dict, dataset, use_s2_clusters=True, dists=dists_temp)\n",
    "(\n",
    "    b3_p,\n",
    "    b3_r,\n",
    "    b3_f1,\n",
    "    b3_metrics_per_paper,\n",
    "    pred_bigger_ratios,\n",
    "    true_bigger_ratios,\n",
    ") = b3_precision_recall_fscore(cluster_to_papers_test, pred_clusters_temp)\n",
    "print(eps, (b3_p, b3_r, b3_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below we find blocks that are very easy that we can downsample to make the training set more balanced.\n",
    "Also find blocks where the labels are impossible to guess right.\n",
    "And finally, ones where having a lot of data is important, so we try to keep as many of those as we can.\n",
    "\"\"\"\n",
    "X_train, y_train, nameless_X_train = train_features\n",
    "X_val, y_val, nameless_X_val = val_features\n",
    "X_test, y_test, nameless_X_test = test_features\n",
    "\n",
    "X = np.vstack([X_train, X_val, X_test])\n",
    "y = np.hstack([y_train, y_val, y_test])\n",
    "\n",
    "# the model just gets confused here:\n",
    "# (1) they really look identical, but label is 1\n",
    "# (2) they really look different, but label is 0\n",
    "flag_remove = (\n",
    "    (y == 0)\n",
    "    & (X[:, feature_names.index('author_first_letter_compatibility')] == 1) \n",
    "    & (X[:, feature_names.index('author_names_similarity')] == 1) \n",
    "    & (X[:, feature_names.index('title_character_similarity')] == 1)\n",
    "    & (X[:, feature_names.index('year_diff')] == 0)\n",
    ") | (\n",
    "    (y == 1) \n",
    "    & (X[:, feature_names.index('title_character_similarity')] == 0)\n",
    ")\n",
    "\n",
    "# usually when the sources are the same, the label is y==0\n",
    "# so we need to keep all the cases where y == 1 and the sources are the same\n",
    "# because they are legit and informative\n",
    "flag_keep = (\n",
    "    (y == 1) \n",
    "    & (\n",
    "        (X[:, feature_names.index('source_count_MergedPDFExtraction')] == 2) \n",
    "        | (X[:, feature_names.index('sources_are_same')] == 1) \n",
    "        | (X[:, feature_names.index('source_count_publisher')] == 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# feats to remove in the fitting below -> should exclude the source stuff as it might generalize poorly\n",
    "feats_to_use = np.array([\n",
    "    feature_names.index(i) for i in feature_names if i not in \n",
    "    ['source_count_Crossref', 'source_count_MergedPDFExtraction', 'source_count_publisher', 'sources_are_same']\n",
    "])\n",
    "\n",
    "# goal is to find blocks in the data the held-out prediction is perfect\n",
    "# we don't really need these and they bloat the data\n",
    "blocks_array = np.array([inverse_papers_dict[i[0]] for i in train_pairs + val_pairs + test_pairs])\n",
    "group_kfold = GroupKFold(n_splits=4)\n",
    "blocks_to_accuracy = []\n",
    "blocks_to_size = []\n",
    "flags_remove = []\n",
    "flags_keep = []\n",
    "dfs = []\n",
    "splits = list(group_kfold.split(X, y, blocks_array))\n",
    "for train_index, test_index in splits:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X[train_index], y[train_index], test_size=0.25, random_state=42)\n",
    "    model = PairwiseModeler(n_iter=5, n_jobs=15)\n",
    "    model.fit(X_train[:, feats_to_use], y_train, X_val[:, feats_to_use], y_val)\n",
    "    y_pred = model.predict_proba(X[test_index, :][:, feats_to_use])\n",
    "    df = pd.DataFrame({'block': blocks_array[test_index], 'y_true': y[test_index], 'y_pred': y_pred[:, 1]})\n",
    "    df.loc[:, 'accuracy'] = df['y_true'] == (df['y_pred'] > 0.65)  # B3 performance is flat between 0.55 and 0.75\n",
    "    df.loc[:, 'flag_remove'] = flag_remove[test_index]\n",
    "    df.loc[:, 'flag_keep'] = flag_keep[test_index]\n",
    "    blocks_to_accuracy.append(df.groupby('block')['accuracy'].mean())\n",
    "    blocks_to_size.append(df.groupby('block')['accuracy'].count())\n",
    "    flags_remove.append(df.groupby('block')['flag_remove'].sum())\n",
    "    flags_keep.append(df.groupby('block')['flag_keep'].sum())\n",
    "    dfs.append(df)\n",
    "    \n",
    "df_acc = pd.concat(blocks_to_accuracy)\n",
    "df_size = pd.concat(blocks_to_size)\n",
    "df_remove = pd.concat(flags_remove)\n",
    "df_keep = pd.concat(flags_keep)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df_size_acc = pd.concat([df_acc, df_size, df_remove, df_keep], axis=1)\n",
    "df_size_acc.columns = ['accuracy', 'size', 'remove_count', 'keep_count']\n",
    "df_size_acc.to_csv(join(PROJECT_ROOT_PATH, \"data\", \"block_removal_candidates.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('s2apler')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae6f2dd2886d3fbfd790a7411a3dc3bf32bbb83c2c3b700fa2789f3541678dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
