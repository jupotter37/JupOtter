{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKxUDVaIA5f4"
      },
      "source": [
        "# Intro to TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tD3apHbd1hw_",
        "outputId": "e6ed2673-12cb-46be-844b-90985f5c1fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (5.5.6)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.3)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.45)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 815, in _parseNoCache\n",
            "    if self.mayIndexError or pre_loc >= len_instring:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIgv80AidV8N"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgfHKUkyA1Gi",
        "outputId": "551e744b-1539-4507-95fa-cb91fa6f1f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤\u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m0.7/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.4/2.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip install mitdeeplearning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clkSYJTNBQ_D"
      },
      "outputs": [],
      "source": [
        "import mitdeeplearning as mdl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R1ZmflnBeDU"
      },
      "source": [
        "Tensorflow controls the operations on tensors (multi-dimensional arrays). Tensors are n-dimensional arrays that have a base datatype (string, integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6BGWgT2BZ21",
        "outputId": "3538ec9e-a606-4bc7-b133-fd729085e1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sport is a 0-d tensor!\n"
          ]
        }
      ],
      "source": [
        "sport = tf.constant(\"Tennis\", tf.string)\n",
        "print(f\"sport is a {tf.rank(sport)}-d tensor!\") # Rank is used to get the number of dimensions (n-d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB5bmHMBTfLO",
        "outputId": "4ebf58c7-70c4-4ef0-cdb5-8dabeb9bf88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sports is a 1-d tensor!\n"
          ]
        }
      ],
      "source": [
        "sports = tf.constant([\"Tennis\", \"Basketball\"], tf.string)\n",
        "print(f\"sports is a {tf.rank(sports).numpy()}-d tensor!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaY6ggjtTsoa",
        "outputId": "749677a8-671c-4cf3-a099-8de0013c3fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scoresTensor is a 2-d tensor!\n"
          ]
        }
      ],
      "source": [
        "# Define a higher order (2-D) tensor!\n",
        "scores = [[97, 85, 89, 76],\n",
        "          [81, 75, 95, 84],\n",
        "          [78, 89, 98, 76]]\n",
        "scoresTensor = tf.constant(scores, tf.float64)\n",
        "print(f\"scoresTensor is a {tf.rank(scoresTensor).numpy()}-d tensor!\")\n",
        "\n",
        "# assert throws no error if the expression is true, otherwise the code results in an error.\n",
        "assert isinstance(scoresTensor, tf.Tensor)\n",
        "assert tf.rank(scoresTensor).numpy() == 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G4fHCGzUeA8",
        "outputId": "6ee3acdd-27c6-4c1a-a85b-8c18cf5ee32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images is a 4-d tensor!\n",
            "Shape of images is [10, 256, 256, 3].\n"
          ]
        }
      ],
      "source": [
        "# Create a 4D tensor using tf.zeros from TensorFlow\n",
        "# There are ten images and their dimensions are 256x256, 3 channels.\n",
        "images = tf.zeros((10, 256, 256, 3))\n",
        "print(f\"images is a {tf.rank(images)}-d tensor!\")\n",
        "print(f\"Shape of images is {tf.shape(images).numpy().tolist()}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv2fWPGvVhYB",
        "outputId": "1d8d54cb-1853-4ecb-f996-220b437bd018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[81.0, 75.0, 95.0, 84.0]\n",
            "[97.0, 81.0, 78.0]\n",
            "97.0\n"
          ]
        }
      ],
      "source": [
        "# Slicing arrays:\n",
        "scores = [[97, 85, 89, 76],\n",
        "          [81, 75, 95, 84],\n",
        "          [78, 89, 98, 76]]\n",
        "scoresTensor = tf.constant(scores, tf.float64)\n",
        "\n",
        "rowVector = scoresTensor[1]\n",
        "columnVector = scoresTensor[:, 0]\n",
        "singleScalar = scoresTensor[0, 0]\n",
        "print(rowVector.numpy().tolist())\n",
        "print(columnVector.numpy().tolist())\n",
        "print(singleScalar.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvJbT2GkWsBZ",
        "outputId": "70f2a453-afe9-43e0-e1b7-d4bc1c61aa37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "# Adding two tensors using TensorFlow\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(20)\n",
        "sum = tf.add(a, b)\n",
        "print(sum.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBYGfURHXnax",
        "outputId": "61f6cdeb-dba0-473c-c7a9-670f2719d1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "# Write a function to implement the following operations:\n",
        "'''\n",
        "a, b are defined.\n",
        "c = a + b\n",
        "d = b - 1\n",
        "e = c * d\n",
        "'''\n",
        "\n",
        "def myTensorOperations(a, b):\n",
        "    c = tf.add(a, b)\n",
        "    d = tf.subtract(b, 1)\n",
        "    e = tf.multiply(c, d)\n",
        "    return e\n",
        "\n",
        "a = tf.constant(4)\n",
        "b = tf.constant(3)\n",
        "e = myTensorOperations(a, b)\n",
        "print(e.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mJaGOgvZQA6"
      },
      "source": [
        "### Neural Networks in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wstmJCP3ZVtm"
      },
      "source": [
        "Tensors can flow through abstract types called Layers present in the Keras API. Layers are the building blocks of neural networks. They update weights, compute losses and define inter-layer connectivity. Let us define a single layer to implement a simple perceptron with a weights and input matrix and a bias and compute a non-linear activation function on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A1t7PK3ZPnJ",
        "outputId": "d9424bc6-5089-47fe-e6b8-dd61725ad56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.27064407 0.1826951  0.50374055]]\n",
            "[PASS] test_custom_dense_layer_output\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Defining a network layer:\n",
        "\n",
        "class OurDenseLayer(tf.keras.layers.Layer): # This class inherits from tf.keras.layers.Layer, which means it takes on its properties and methods.\n",
        "    def __init__(self, n_output_nodes):\n",
        "        super(OurDenseLayer, self).__init__() # Calls the constructor of the parent class to make sure the layer is properly initialized.\n",
        "        self.n_output_nodes = n_output_nodes\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        d = int(input_shape[-1])\n",
        "        # Initialize weights and biases.\n",
        "        self.W = self.add_weight(\"weight\", shape = [d, self.n_output_nodes])\n",
        "        self.b = self.add_weight(\"bias\", shape = [1, self.n_output_nodes])\n",
        "\n",
        "    def call(self, x):\n",
        "        z = tf.add(tf.matmul(x, self.W), self.b) # z = Wx + b\n",
        "        y = tf.sigmoid(z)\n",
        "        return y\n",
        "\n",
        "# Random seed is being used for reproduceability since weight initialization is also random.\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "layer = OurDenseLayer(3)\n",
        "layer.build((1, 2))\n",
        "x_input = tf.constant([1, 2.], shape=(1, 2))\n",
        "y = layer.call(x_input)\n",
        "\n",
        "print(y.numpy())\n",
        "mdl.lab1.test_custom_dense_layer_output(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EduGUJLCYwGR"
      },
      "outputs": [],
      "source": [
        "# Designing a neural network using the Sequential API from Keras.\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "n_output_nodes = 3\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Now create a Dense layer and then add it to the model:\n",
        "\n",
        "dense_layer = Dense(units = n_output_nodes,\n",
        "                    use_bias = True,\n",
        "                    activation = \"sigmoid\")\n",
        "\n",
        "model.add(dense_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdIPqgVudJrc",
        "outputId": "e849ea27-d4be-4488-e7b6-be4f5158539a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.18752205 0.23909675 0.3213029 ]], shape=(1, 3), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x_input = tf.constant([1, 2.], shape=(1, 2))\n",
        "model_output = model.call(x_input)\n",
        "print(model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNJcvNrkqs0E"
      },
      "source": [
        "You can also create a neural network by subclassing the Model class present in the Keras API. Subclassing provides the flexibility to define custom layers, custom training loops, custom activation functions, and custom models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQA8ucPjfKpe",
        "outputId": "4488d2c2-b817-48fe-b84a-4a32331771b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.82794297 0.15568398 0.567044  ]], shape=(1, 3), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class SubclassModel(tf.keras.Model):\n",
        "    # We define the model's layers in the constructor\n",
        "    def __init__(self, n_output_nodes):\n",
        "        super(SubclassModel, self).__init__()\n",
        "        self.dense_layer = Dense(units = n_output_nodes,\n",
        "                                use_bias = True,\n",
        "                                activation = \"sigmoid\")\n",
        "\n",
        "    def call(self, input):\n",
        "        return self.dense_layer(input)\n",
        "\n",
        "n_output_nodes = 3\n",
        "model = SubclassModel(n_output_nodes)\n",
        "x_input = tf.constant([[1, 2.]], shape = (1, 2))\n",
        "print(model.call(x_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3APxbwgsQ2F"
      },
      "source": [
        "To show how flexible Subclass Models are, define a boolean variable in the call method called isIdentity and simply output the input if isIdentity is true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrmcdBDcrBqj",
        "outputId": "521995fb-7ced-4053-bc91-c5b359302c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activated: [[0.5746787  0.6182241  0.88219965]]\n",
            "Identity: [[1. 2.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class IdentityModel(tf.keras.Model):\n",
        "    def __init__(self, n_output_nodes):\n",
        "        super(IdentityModel, self).__init__()\n",
        "        self.dense_layer = Dense(units = n_output_nodes,\n",
        "                                 activation = \"sigmoid\",\n",
        "                                 use_bias = True)\n",
        "\n",
        "    def call(self, input, isIdentity = False):\n",
        "        x = self.dense_layer(input)\n",
        "        if(isIdentity):\n",
        "            return input\n",
        "        return x\n",
        "\n",
        "n_output_nodes = 3\n",
        "model = IdentityModel(n_output_nodes)\n",
        "x_input = tf.constant([[1, 2.]], shape=(1, 2))\n",
        "out_activated = model.call(x_input, isIdentity = False)\n",
        "out_identity = model.call(x_input, isIdentity = True)\n",
        "print(f\"Activated: {out_activated}\\nIdentity: {out_identity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SBR_n6LuER9"
      },
      "source": [
        "### Backpropogation in TensorFlow using automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeRfMqEouNaV"
      },
      "source": [
        "Automatic differentiation is the most import part of TensorFlow, it is the backbone of training with back propogation in tensorflow. When a forward pass is made, all the computation are stored using the tf.GradientTape functionality, to back propogate, this tape is played backwards. A particular tf.GradientTape can compute only one gradient, any subsequent calls throw an error. To compute multiple gradients over time, we need to use a persistent gradient.\n",
        "\n",
        "To look at an example, let us define a simple function, y = x^2 and compute the gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc85E-YnuLbj",
        "outputId": "635bbdd8-0805-4d8d-d326-9ac274ff1bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(3.0)\n",
        "# Initiate the gradient tape\n",
        "with tf.GradientTape() as tape:\n",
        "    # Define the function\n",
        "    y = x**2\n",
        "# Access the gradient -- derivative of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx) # Answer must be 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEormYuZvrrz"
      },
      "source": [
        "Using this concept, now let us try to use differentiation and Stochastic Gradient Descent to minimize a loss function. Let us minimize the loss, 攝ｿ = (x - xActual)^2. Analytically, minimum loss would be at x = xActual, let us see how we use GradientTape to arrive at this minimum (if we can)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "xxKu4KFXvdla",
        "outputId": "555a1fc9-bce6-4482-e9b0-2f43e5072733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing x as: <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-1.1012201]], dtype=float32)>\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9ff0729b8bf2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute the derivative of loss wrt x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnew_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;31m# SGD update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3840\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3841\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3842\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3843\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6169\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6170\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6171\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6172\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6173\u001b[0m         transpose_b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Function minimization with automatic differentiation and SGD\n",
        "# Initialize a random variable for our initial x\n",
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print(f\"Initializing x as: {x}\")\n",
        "\n",
        "learning_rate = 1e-2 # Learning rate for SGD\n",
        "history = []\n",
        "# Target Value:\n",
        "x_final = 4\n",
        "\n",
        "# We will run the SGD for a number of iterations, for each iteration, we will compute the loss, compute the derivative of the loss with respect to x and update the SGD.\n",
        "\n",
        "for i in range(500):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = tf.matmul(tf.subtract(x, x_final), tf.subtract(x, x_final))\n",
        "    grad = tape.gradient(loss, x) # Compute the derivative of loss wrt x\n",
        "    new_x = x - learning_rate*grad # SGD update\n",
        "    x.assign(new_x) # Update the value of x\n",
        "    history.append(x.numpy()[0])\n",
        "\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500], [x_final, x_final])\n",
        "plt.legend((\"Predicted\", \"True\"))\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARGbap8Nypa5"
      },
      "source": [
        "# Music Generation using RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnOAm_gBzcHf"
      },
      "source": [
        "We are using Comet ML to track model development and training runs.\n",
        "\n",
        "`My COMET API = ____________________` # Rq for your own API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0ihKOVRA3p7z",
        "outputId": "59fd5e5f-428e-4444-a62c-5cb56155a92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (5.5.6)\n",
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.3)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
            "Installing collected packages: jedi, comm, ipykernel\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed comm-0.2.2 ipykernel-6.29.4 jedi-0.19.1\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJSxROlb3tDI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftsfOokP3vvl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "!pip install mitdeeplearning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L332DBvZysUC"
      },
      "outputs": [],
      "source": [
        "!pip install comet_ml > /dev/null 2>&1\n",
        "!pip install mitdeeplearning --quiet\n",
        "import comet_ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXLDAsZy0R-j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import mitdeeplearning as mdl\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "from scipy.io.wavfile import write\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "\n",
        "COMET_API_KEY = \"Provide your API key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYIvr9dr1Fm9"
      },
      "source": [
        "We have a dataset of thousands of Irish Folk songs in the ABC notation. Let us download and inspect it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5tF3HxQ1ANj"
      },
      "outputs": [],
      "source": [
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "example_song = songs[0]\n",
        "print(f\"Example song: {example_song}\")\n",
        "# The song file contains not just the ABC notation, but also the meta-data such as song title, key, tempo, instrument, etc. This is important when we generate a numerical representation for the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VVgZH4M1UNq"
      },
      "outputs": [],
      "source": [
        "# Playing the song from ABC notation:\n",
        "mdl.lab1.play_song(example_song)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOXnuxba_wK3",
        "outputId": "1118a30f-aa71-4f61-eefc-d56e3282e530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Unique Characters: 83\n"
          ]
        }
      ],
      "source": [
        "# Join all the songs into a single string having all songs:\n",
        "all_songs = \"\\n\\n\".join(songs)\n",
        "\n",
        "# Find all the unique characters in the dataset:\n",
        "unique_characters = sorted(set(all_songs))\n",
        "print(f\"Total Unique Characters: {len(unique_characters)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JVaUteiAQ8S",
        "outputId": "f4e95873-c082-4fa6-81fc-eac7a0da9f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59\n",
            "d\n"
          ]
        }
      ],
      "source": [
        "# We need to create numerical representation of our dataset, to do this, we will create two look-up tables, one to map characters to numbers and one to map numbers back to characters\n",
        "# Create a dictionary based on the unique_characters string:\n",
        "char2idx = {u:i for i,u in enumerate(unique_characters)}\n",
        "# Now, each character has a unique number representation to it.\n",
        "print(char2idx['d'])\n",
        "# Now, we also need each number to have a unique character:\n",
        "idx2char = np.array(unique_characters)\n",
        "#Simply use the index to get the value of the character:\n",
        "print(idx2char[59])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlh4vjjGA_hj"
      },
      "outputs": [],
      "source": [
        "# Taking a look at the numerical representation:\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6aY5ItYB8yp"
      },
      "outputs": [],
      "source": [
        "# Let us write a function to vectorize a given string of songs and return back an np.array of numerical representation of that song string.\n",
        "def vectorize_string(string):\n",
        "    idxList = []\n",
        "    for i in string:\n",
        "        idxList.append(char2idx[i])\n",
        "    np_array = np.array(idxList)\n",
        "    return np_array\n",
        "vectorized_songs = vectorize_string(all_songs)\n",
        "\n",
        "assert isinstance(vectorized_songs, np.ndarray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O4AVBMrL6eb",
        "outputId": "2f76dcbb-8057-4973-9b3d-b6c4a77ec9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PASS] test_batch_func_types\n",
            "[PASS] test_batch_func_shapes\n",
            "[PASS] test_batch_func_next_step\n",
            "======\n",
            "[PASS] passed all tests!\n"
          ]
        }
      ],
      "source": [
        "# Creating training samples:\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "    # The length of vectorized_song string.\n",
        "    n = vectorized_songs.shape[0] - 1\n",
        "    # Randomly choose the starting indices for the examples in the training batch:\n",
        "    idx = np.random.choice(n-seq_length, batch_size)\n",
        "\n",
        "    # Use these indices and create input and output lists:\n",
        "    input_batch = [vectorized_songs[i:i+seq_length] for i in idx]\n",
        "    output_batch = [vectorized_songs[i+1:i+seq_length+1] for i in idx] # Output is just one more character from the input sequence.\n",
        "\n",
        "    # Return the correct inputs and outputs for network training:\n",
        "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# Simple tests to make sure batching is running perfectly.\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args):\n",
        "   print(\"======\\n[FAIL] could not pass tests\")\n",
        "else:\n",
        "   print(\"======\\n[PASS] passed all tests!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDAWGFvPOunE",
        "outputId": "d7dc2d41-f804-46e6-b065-763bf555aba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 0\n",
            "Input: 5 (\"'\")\n",
            "Expected Output: 74 ('s')\n",
            "Step: 1\n",
            "Input: 74 ('s')\n",
            "Expected Output: 1 (' ')\n",
            "Step: 2\n",
            "Input: 1 (' ')\n",
            "Expected Output: 26 ('A')\n",
            "Step: 3\n",
            "Input: 26 ('A')\n",
            "Expected Output: 74 ('s')\n",
            "Step: 4\n",
            "Input: 74 ('s')\n",
            "Expected Output: 63 ('h')\n"
          ]
        }
      ],
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length = 5, batch_size = 1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "    print(f\"Step: {i}\")\n",
        "    print(f\"Input: {input_idx} ({repr(idx2char[input_idx])})\")\n",
        "    print(f\"Expected Output: {target_idx} ({repr(idx2char[target_idx])})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ePZtMm4Sll",
        "outputId": "c2bcd326-ef43-41ef-e4e8-5f299b897e61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "def LSTMInitializer(rnn_units):\n",
        "    return LSTM(rnn_units,\n",
        "                return_sequences = True,\n",
        "                recurrent_initializer = 'glorot_uniform',\n",
        "                recurrent_activation = 'sigmoid',\n",
        "                stateful = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RVrg4H37kG7",
        "outputId": "458ba2db-d68a-43ec-9268-a0dc703c1390"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
        "                        LSTMInitializer(rnn_units),\n",
        "                        tf.keras.layers.Dense(vocab_size)])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nei3oJB88twL",
        "outputId": "443cf184-e6b1-4e6a-ea54-f47be4b3b743"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "model = build_model(len(unique_characters), embedding_dim = 256, rnn_units = 1024, batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyDRMqqJ9Q7T",
        "outputId": "38cf6da7-e9de-423e-837e-35fa9fad3035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (32, None, 256)           21248     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (32, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (32, None, 83)            85075     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5353299 (20.42 MB)\n",
            "Trainable params: 5353299 (20.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieaWDgow9ThS",
        "outputId": "ec735197-12ec-4258-e275-ba48a67f16a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape:       (32, 100)  # (batch_size, sequence_length)\n",
            "Prediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
        "pred = model(x)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "# Prediction includes the vocab size too since it returns a probability distribution of the entire vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRs6pUJy9fxj",
        "outputId": "813923b9-ee1f-4eb3-93a0-5cba12541acc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([67, 63, 46, 30, 32, 68, 72, 19, 64, 77, 31, 43,  0, 61, 80, 62, 55,\n",
              "       77, 48, 16, 39, 63,  0, 47, 82, 37, 79, 31, 81, 78, 11, 77, 13, 61,\n",
              "       55, 33, 50,  1, 55, 14, 17,  4, 66, 20, 55, 13, 30, 81,  9, 76, 71,\n",
              "       34, 31, 63, 52, 69, 26, 75, 19, 40, 28, 57,  8,  1, 34, 37, 71, 14,\n",
              "       29, 22,  3, 50, 43, 53, 16, 18, 74, 40, 46, 18, 52, 30,  2, 15, 75,\n",
              "       13, 39,  9, 70, 19,  2, 78,  0, 75, 46, 27, 33, 11, 23,  2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are now sampling from the probabilistic distribution of the vocab, instead of simply taking argmax since it can get stuck in a loop.\n",
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1) # Sampling from the first prediction.\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdY8iitm-M0-",
        "outputId": "02976b18-5e1d-4702-bdaa-edd0a0653bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'gf efd cBA:|!\\n\\nX:16\\nT:Kid on the Mountain\\nZ: id:dc-slipjig-16\\nM:9/8\\nL:1/8\\nK:E Minor\\nEDE FEF G2F|EFE '\n",
            "\n",
            "Next Char Predictions: \n",
            " 'lhUEGmq7ivFR\\nfyg_vW4Nh\\nV|LxFzw/v1f_HY _25#k8_1Ez-upIFh[nAt7OCb, ILp2D:\"YR]46sOU6[E!3t1N-o7!w\\ntUBH/<!'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-Jx2tmz_BPr",
        "outputId": "3096e7a9-5174-4781-95d0-dccc7bfac226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n",
            "Scalar loss: 4.4191113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "# Let us define a function to calculate loss:\n",
        "def compute_loss(labels, logits):\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n",
        "    return loss\n",
        "\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Scalar loss:\", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-MR-qaX_piq",
        "outputId": "a1f37361-4144-4774-cac1-baa8e45a69dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "# Let us define some hyper-parameters to train the model:\n",
        "vocab_size = len(unique_characters)\n",
        "# Model Parameters:\n",
        "params = dict(num_training_iterations = 3000,\n",
        "              batch_size = 8,\n",
        "              seq_length = 100,\n",
        "              learning_rate = 5e-3,\n",
        "              embedding_dim = 256,\n",
        "              rnn_units = 1024)\n",
        "\n",
        "# Checkpoint location:\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'my_ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c71XSKgAu4e"
      },
      "source": [
        "We can set up experiment tracking with COMET, EXPERIMENT are the core objects in COMET and will help us in tracking training and model development. Let us write a short function to track our progress with COMET.\n",
        "\n",
        "Note that when your hyperparameters change, you can initiate a new experiment, all experiments defined with the same project name will live under that project in your COMET interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2cRENIQAhqV"
      },
      "outputs": [],
      "source": [
        "def create_experiment():\n",
        "    # End any prior experiments\n",
        "    if 'experiment' in locals():\n",
        "        experiment.end()\n",
        "    # Initiate the COMET experiment for tracking\n",
        "    experiment = comet_ml.Experiment(api_key = COMET_API_KEY,\n",
        "                                     project_name = 'MIT Deep Learning Lab 1')\n",
        "    # Log our hyperparameters to the experiment\n",
        "    for param, value in params.items():\n",
        "        experiment.log_parameter(param, value)\n",
        "    experiment.flush()\n",
        "    return experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzb9qv4qB_zC"
      },
      "outputs": [],
      "source": [
        "# Now we finally compile and train the model:\n",
        "model = build_model(vocab_size, params[\"embedding_dim\"], params['rnn_units'], params['batch_size'])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = params['learning_rate'])\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    # Use tf.GradientTape to record the computations for back propogation\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = model(x)\n",
        "        loss = compute_loss(y, y_hat)\n",
        "    grads = tape.gradient(loss, model.trainable_variables) # Calculating the gradient of the loss with respect to all the trainable parameters\n",
        "    # Apply the gradients to the optimizer so it can update the model accordingly:\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Begin the training now:\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec = 2, xlabel = \"Iterations\", ylabel = \"Loss\")\n",
        "experiment = create_experiment()\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # Clear instances if they exist\n",
        "for iter in tqdm(range(params[\"num_training_iterations\"])):\n",
        "    # Take a batch and propogate it through the network\n",
        "    x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_length\"], params[\"batch_size\"])\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    # Log the loss to the COMET interface so we can track it there.\n",
        "    experiment.log_metric(\"loss\", loss.numpy().mean(), step = iter)\n",
        "    # Update the progress bar and visualize it in the notebook:\n",
        "    history.append(loss.numpy().mean())\n",
        "    plotter.plot(history)\n",
        "    # Update the model with the changed weights:\n",
        "    if iter % 100 == 0:\n",
        "        model.save_weights(checkpoint_prefix)\n",
        "\n",
        "model.save_weights(checkpoint_prefix)\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDIkSyshJ-sE"
      },
      "source": [
        "Now that we finished training the model, let us try generating some music, we need to start with a seed since we can't start with nothing.\n",
        "\n",
        "Once we generate a seed, we can iteratively predict the successive character, using our trained RNN. Since our RNN outputs a softmax probability, we need to sample to add to our ABC notation.\n",
        "\n",
        "Then all we have to do is write it to a file and listen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbJ0dKy0Cffq",
        "outputId": "35c1f990-9725-4920-fee7-acbc681aef9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (1, None, 256)            21248     \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (1, None, 83)             85075     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5353299 (20.42 MB)\n",
            "Trainable params: 5353299 (20.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Restore the last checkpoint, and use a batch size of one, since RNN is passed from timestep to timestep, the model model will only be able to accept a fixed batch size once it is built.\n",
        "Therefore to run the model with a different batch_size, you need to rebuild the model.\n",
        "'''\n",
        "\n",
        "model = build_model(vocab_size, params[\"embedding_dim\"], params[\"rnn_units\"], batch_size = 1)\n",
        "# Since the model was rebuilt, we use the weights from the checkpoints after we had trained the model.\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac7ckQouLWxJ"
      },
      "outputs": [],
      "source": [
        "# Now, let us generate some text (ABC notation) for later converting it to waveform music:\n",
        "def generate_text(model, start_string, generation_length = 1000):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0) # inserting a dimension at 1 of the input vector\n",
        "    text_generated = [] # empty string to store the results\n",
        "    model.reset_states()\n",
        "    tqdm._instances.clear()\n",
        "    for i in tqdm(range(generation_length)):\n",
        "        predictions = model(input_eval)\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
        "        # Pass the prediction along with the previous hidden state as the next input to the models\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        # Add the predicted character to the generated text:\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string+''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjT6K5V_Q3jb",
        "outputId": "b791fb19-c734-4f86-da7c-bd2c65e6a80c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n",
            "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 1000/1000 [00:09<00:00, 110.43it/s]\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(model, start_string = 'X', generation_length = 1000) # Since ABC notations start with the letter X, it might be a good starting string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF1o6llvRkEg",
        "outputId": "59f4d7f3-86f1-4311-fddc-20c560606747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:6\n",
            "T:Blaney's Monaghan\n",
            "Z: id:dc-reel-279\n",
            "M:C\n",
            "L:1/8\n",
            "K:G Major\n",
            "D|GFGA Bgdg|bagf gfed|edef dBAd|Bdd^c dAGF|EGFE EDEF|!\n",
            "DEFA dBAG|FAdcABd fagf|dfec d2:|!\n",
            "ff|fedf gfed|cBBB BABc|defd cAFA|]!\n",
            "DBG_B^f|ABA AFA|ded BAF|AFF AFE|!\n",
            "FEF DFA|BAB d2A|BAB def|gfg dcB|AFD D2:|!\n",
            "\n",
            "X:54\n",
            "T:Tellowe^c|d2A2 d2A2|c2B2 d2:|!\n",
            "\n",
            "X:53\n",
            "T:Dullow's No. 1\n",
            "Z: id:dc-reel-102\n",
            "M:C\n",
            "L:1/8\n",
            "K:G Major\n",
            "ef|gBBc AGEe|fgfe d3e|f2cf afef|dBB2 A2Bc|!\n",
            "defg a2gf|edBd faaf|ecAc dBGB|cBAG A2:|!\n",
            "\n",
            "X:56\n",
            "T:Desty Walk\n",
            "Z: id:dc-hornpipe-48\n",
            "M:C|\n",
            "L:1/8\n",
            "K:D Major\n",
            "agf|dBB2 GBdB|cBAB cAFD|EGce dBGB|BAAG AcBA|GFGA BGAF|GdAG FDD:|!\n",
            "D|G2dG BGAF|G2\n",
            "M:6/8\n",
            "L:1/8\n",
            "K:D Mixolydian\n",
            "GA BG|EF GA|Bd BA|Bd df/e/|dB A2:|!\n",
            "\n",
            "X:5\n",
            "T:Barney Ped\n",
            "Z: id:dc-reel-71\n",
            "M:C\n",
            "L:1/8\n",
            "K:A Dorian\n",
            "E|E2B,E G3A|Beed BABB|BAG A/B/c|dF F2|!\n",
            "d>e for\n",
            "B|A2B2 d2B2|B2BA Bdgf|eaaf gedB|Aecd ecAB|dBAB G3:|!\n",
            "A3B defd|A2d2 edBA|Bdef gfge|Bage dBGB|!\n",
            "AGAB G2Bd|e2dB eBdB|AGAB cAGE|GABG AGEG|(3BBA GFG|d2G G2A|!\n",
            "d2e dBG A2B|d2e edB|dBG AGE|F3 DFA|!\n",
            "B2E G2A|BAB d2B|ABA G2A|BAG A2D|!\n",
            "cde dBG|A3 ABd|e2d \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  def _forward_input(self, allow_stdin=False):\n"
          ]
        }
      ],
      "source": [
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gS03VICpRo4n"
      },
      "outputs": [],
      "source": [
        "### Play back generated songs ###\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "\n",
        "for i, song in enumerate(generated_songs):\n",
        "  # Synthesize the waveform from a song\n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  # If its a valid song (correct syntax), lets play it!\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)\n",
        "\n",
        "    numeric_data = np.frombuffer(waveform.data, dtype=np.int16)\n",
        "    wav_file_path = f\"output_{i}.wav\"\n",
        "    write(wav_file_path, 88200, numeric_data)\n",
        "\n",
        "    # save your song to the Comet interface -- you can access it there\n",
        "    experiment.log_asset(wav_file_path)\n",
        "# when done, end the comet experiment\n",
        "experiment.end()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bKxUDVaIA5f4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
