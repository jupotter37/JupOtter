{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is used for:\n",
    "- Convert .mrd to a an h5 file.\n",
    "- Convert the ismrmrd object into a numpy matrix.\n",
    "- Add the headers to the h5 file\n",
    "- Remove the zero-padding from the UMCG data if needed\n",
    "- Remove the zero-padding from the RUMC (Radboud) data if needed\n",
    "- Transform the UMCG data (phase encoding) into the same dimensions as the NYU data\n",
    "- Visualize the UMCG averages. How are they structured and related to each other\n",
    "- Perform RSS reconstruction on UMCG and NYU data.\n",
    "\n",
    "Later: (todo)\n",
    "- It will later also add the segmentations to the h5 file.\n",
    "- Add the converted calibration data to the h5 file.\n",
    "- Add the DICOM images to the h5 file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| WORKDIR: WindowsPath('E:/quintin/umcg_ksp_ws')\n",
      "ic| TEMPDIR: WindowsPath('E:/quintin/umcg_ksp_ws/tmp')\n",
      "ic| H5DIR: WindowsPath('E:/quintin/umcg_ksp_ws/output/h5s')\n",
      "ic| os.getcwd(): 'c:\\\\Users\\\\LohuizenQY\\\\local_docs\\\\repos\\\\umcglib\\\\src\\\\umcglib\\\\kspace'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\LohuizenQY\\\\local_docs\\\\repos\\\\umcglib\\\\src\\\\umcglib\\\\kspace'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "import SimpleITK as sitk\n",
    "import datetime\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "DEBUG      = True\n",
    "KEY        = 'x'\n",
    "\n",
    "# Windows path external ssd: E:\\quintin\\umcg_ksp_ws\n",
    "WORKDIR = Path(\"E:/quintin/umcg_ksp_ws\")        # Windows, Quintin, external ssd\n",
    "\n",
    "TEMPDIR    = Path(WORKDIR, 'tmp')\n",
    "H5DIR      = Path(WORKDIR, 'output', 'h5s')\n",
    "\n",
    "your_path= Path(\"/mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace\")\n",
    "\n",
    "# # H5 files\n",
    "FPATH_UMCG_H5_SAMPLE = Path(WORKDIR, \"output\", \"h5s\", \"ANON2784451_pst_T2.h5\")\n",
    "FPATH_NYU_H5_SAMPLE  = Path(your_path, \"prostate_nyu/training_T2_1/file_prostate_AXT2_0001.h5\")\n",
    "\n",
    "# MRD files\n",
    "FPATH_UMCG_MRD_SAMPLE_CALIB = Path(your_path, '/output/anon_kspaces/0001_patient_umcg_done/meas_MID00202_FID688156_T2_TSE_tra_obl-out_1.mrd')\n",
    "FPATH_UMCG_MRD_SAMPLE       = Path(your_path, '/output/anon_kspaces/0001_patient_umcg_done/meas_MID00202_FID688156_T2_TSE_tra_obl-out_2.mrd')\n",
    "\n",
    "print(WORKDIR)\n",
    "print(TEMPDIR)\n",
    "print(H5DIR)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create H5s from MRD files (UMCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_interleaved_slices_to_sequential(npdata):\n",
    "    '''\n",
    "    Reorder the kspace data from interleaved to all even slices first and then all odd slices\n",
    "    Arguments:\n",
    "        - npdata: numpy array of kspace data in shape (navgs, nslices, ncoils, rNx, eNy + 1) complex\n",
    "    Returns:\n",
    "        - data: numpy array of kspace data in shape (navgs, nslices, ncoils, rNx, eNy + 1) complex\n",
    "    '''\n",
    "\n",
    "    assert npdata.ndim == 5, \"Input data must be 5D, with dimensions [navgs, nslices, ncoils, rNx, eNy + 1]\"\n",
    "\n",
    "    # Get the number of slices and calculate the half value (rounded up)\n",
    "    nslices = npdata.shape[1]\n",
    "    mid_slice = (nslices // 2) + 1\n",
    "\n",
    "    # Create output array of the same shape and data type as the input array \n",
    "    reordered_data = np.zeros_like(npdata, dtype=np.complex64)\n",
    "\n",
    "    # Even slices first (0, 2, 4, ...) then odd slices (1, 3, 5, ...)\n",
    "    reordered_data[:, ::2] = npdata[:, :mid_slice]\n",
    "    reordered_data[:, 1::2] = npdata[:, mid_slice:]\n",
    "\n",
    "    safe_rss_to_nifti_file(reordered_data)\n",
    "    input(\"Press Enter to continue...\")\n",
    "\n",
    "    return reordered_data\n",
    "\n",
    "\n",
    "def build_kspace_array_from_mrd_umcg(fpath: str, verbose=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        - fpath: path to the .mrd file\n",
    "    \n",
    "    Returns:\n",
    "        - kspace: numpy array of kspace data in shape (navgs, nslices, ncoils, rNx, eNy + 1) complex\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f\"Building kspace from .mrd file\")\n",
    "\n",
    "    # Check if the kspace already exists in the TEMPDIR\n",
    "    if os.path.exists(os.path.join(TEMPDIR, \"kspace_0003.npy\")):\n",
    "        print(f\"kspace already exists at {os.path.join(TEMPDIR, 'kspace_0003.npy')}\")\n",
    "        kspace = np.load(os.path.join(TEMPDIR, \"kspace_0003.npy\"))\n",
    "        return kspace\n",
    "    else:\n",
    "\n",
    "        # Read the header and get encoding information\n",
    "        dset = ismrmrd.Dataset(fpath, create_if_needed=False)\n",
    "        header = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "        enc = header.encoding[0]\n",
    "\n",
    "        # Determine some parameters of the acquisition\n",
    "        ncoils     = header.acquisitionSystemInformation.receiverChannels\n",
    "        nslices    = enc.encodingLimits.slice.maximum + 1 if enc.encodingLimits.slice is not None else 1\n",
    "        eNy        = enc.encodedSpace.matrixSize.y\n",
    "        rNx        = enc.reconSpace.matrixSize.x\n",
    "        # eTL        = 25 if DEBUG else echo_train_length(dset)\n",
    "        # eTC        = 11 if DEBUG else echo_train_count(dset, echo_train_len=eTL)\n",
    "        firstacq   = 1 #if DEBUG else get_first_acquisition(dset)\n",
    "        navgs      = 3 #if DEBUG else get_num_averages(firstacq=firstacq, dset=dset)\n",
    "\n",
    "        # Initialize the kspace data array\n",
    "        kspace = np.zeros((navgs, nslices, ncoils, rNx, eNy + 1), dtype=np.complex64)\n",
    "        print(f\"Filling kspace array from mrd object to shape {kspace.shape}...\\n Num Acq: {dset.number_of_acquisitions()}\")\n",
    "\n",
    "        # Loop through the rest of the acquisitions and fill the data array with the kspace data\n",
    "        for acqnum in range(firstacq, dset.number_of_acquisitions()):\n",
    "            \n",
    "            if acqnum % 1000 == 0:\n",
    "                print(f\"{acqnum/dset.number_of_acquisitions() * 100:.1f}%\", end=\" \", flush=True)\n",
    "            \n",
    "            acq    = dset.read_acquisition(acqnum)\n",
    "            slice1 = acq.idx.slice\n",
    "            y      = acq.idx.kspace_encode_step_1\n",
    "            avg    = acq._head.idx.average\n",
    "\n",
    "            # Each acquisition is a 2D array of shape (coil, rNx) complex\n",
    "            kspace[avg, slice1, :, :, y] = acq.data\n",
    "    \n",
    "    # store the kspace as npy file to the TEMPDIR if it does not exist yet\n",
    "    if not os.path.exists(os.path.join(TEMPDIR, \"kspace_0003.npy\")):\n",
    "        np.save(os.path.join(TEMPDIR, \"kspace_0003.npy\"), kspace)\n",
    "        print(f\"kspace saved to {os.path.join(TEMPDIR, 'kspace_0003.npy')}\")\n",
    "\n",
    "    # Reorder the data from even and odd number of slices being interleaved to all even slices first and then all odd slices\n",
    "\n",
    "    pat_dcm_dir = \"/mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace/input/sectra_export_pat0001_pat0010_anon_dicoms/ANON5046358/2022-05-10/T2W_TRA\"\n",
    "    # method 3 - Reorder based on the dicom slice numbers and their respective acquisition times\n",
    "    # remap_indices = get_slice_remap_instanceNumber_vs_imagePatientPosition(pat_dcm_dir, verbose=True)\n",
    "    # remap_indices = get_slice_remap_instanceNumber_vs_acquisitionTime(pat_dcm_dir, verbose=True)\n",
    "    # remap_indices = get_slice_remap_by_image_position(pat_dcm_dir, verbose=True)\n",
    "    # remap_indices = get_slice_remap_by_instace_number(pat_dcm_dir, verbose=True)\n",
    "    remap_indices = get_slice_remap_based_on_acquisition_time(pat_dcm_dir, verbose=True)\n",
    "\n",
    "    input(f\"in build_kspace_array_from_mrd_umcg v1:     enter to continue..\")\n",
    "    kspace = kspace[:, remap_indices, :, :, :]\n",
    "\n",
    "    safe_rss_to_nifti_file(kspace, fname_part=\"_reorder_from_dcms_instNum_vs_Patpos_\", do_round=True)\n",
    "    input(f\"in build_kspace_array_from_mrd_umcg v2:     enter to continue..\")\n",
    "\n",
    "\n",
    "    return reorder_interleaved_slices_to_sequential(kspace)\n",
    "\n",
    "\n",
    "def process_mrd_to_h5(\n",
    "        fpath: str,\n",
    "        fpath_hf: str,\n",
    "        max_mag_ref: float,\n",
    "        do_rm_zero_pad: bool,\n",
    "        do_norm_to_ref: bool,\n",
    "        pat_seq_id: str,\n",
    "        pat_anon_id: str,\n",
    "        phase_crop_shape: tuple = None,\n",
    "    ) -> None:\n",
    "    '''\n",
    "    Description:\n",
    "        This function creates an h5 file from a .mrd file. The h5 file contains the kspace and the ismrmrd header.\n",
    "        The kspace is cropped in the phase direction to the shape of the NYU dataset.\n",
    "        The kspace is normalized to the reference magnitude of the NYU dataset.\n",
    "        The h5 file is named with the anonymized patient id.\n",
    "    Args:\n",
    "        fpath (str): The path to the .mrd file.\n",
    "        fpath_hf (str): The path to the h5 file.\n",
    "        phase_crop_shape (tuple): The shape to crop the kspace to.\n",
    "        max_mag_ref (float): The reference magnitude.\n",
    "        do_rm_zero_pad (bool): If True, the zero padding is removed.\n",
    "        do_norm_to_ref (bool): If True, the magnitude is normalized to the reference magnitude.\n",
    "    '''\n",
    "    \n",
    "    max_for_now  = 0.0004   # change in the future to something that makes sense\n",
    "    norm_for_now = 0.12    # change in the future to something that makes sense\n",
    "    \n",
    "    # Construct the kspace array from the sequentail MRD object.\n",
    "    kspace = build_kspace_array_from_mrd_umcg(fpath)\n",
    "\n",
    "    if do_rm_zero_pad:\n",
    "        kspace = remove_zero_padding(kspace)\n",
    "\n",
    "    # Crop the kspace in the phase dir and obtain the transformed headers. Simply extracts the headers as is, if the crop shape is equal to the kspace shape.\n",
    "    kspace, trans_hdrs = crop_kspace_in_phase_direction(kspace, target_shape=phase_crop_shape, fpath_mrd=fpath)\n",
    "\n",
    "    if do_norm_to_ref:\n",
    "        kspace = normalize_to_reference(kspace, max_magni_ref=max_mag_ref)\n",
    "\n",
    "    with h5py.File(fpath_hf, 'r+') as hf:\n",
    "        if not has_kspace_key(fpath_hf):\n",
    "            hf.create_dataset('ismrmrd_header', data=trans_hdrs)\n",
    "            hf.create_dataset('kspace', data=kspace)\n",
    "\n",
    "        if not has_correct_shape(fpath_hf):\n",
    "            raise Exception(f\"kspace shape is not correct. Shape: {hf['kspace'].shape}\")\n",
    "\n",
    "        if len(dict(hf.attrs)) == 0:\n",
    "            hf.attrs['acquisition'] = 'AXT2'\n",
    "            hf.attrs['max'] = max_for_now\n",
    "            hf.attrs['norm'] = norm_for_now\n",
    "            hf.attrs['patient_id'] = pat_anon_id\n",
    "            hf.attrs['patient_id_seq'] = pat_seq_id\n",
    "\n",
    "\n",
    "def create_t2w_h5_from_mrd_umcg(\n",
    "        fpath: str,\n",
    "        workdir: str,\n",
    "        key: str,\n",
    "        do_rm_zero_pad   = True,\n",
    "        phase_crop_shape = None,  # This is the NYU shape.\n",
    "        do_norm_to_ref   = True,\n",
    "        max_mag_ref      = 0.006096669,  # Based on one patient of the NYU dataset the max_magnitude reference: # MUST IN THE FUTURE BE DETERMINED ON THE ENTIRE NYU dataset. Or return the NYU RIM model on normalized data.\n",
    ") -> None:\n",
    "    '''\n",
    "    Description:\n",
    "        This function creates an h5 file from a .mrd file. The h5 file contains the kspace and the ismrmrd header.\n",
    "        The kspace is cropped in the phase direction to the shape of the NYU dataset.\n",
    "        The kspace is normalized to the reference magnitude of the NYU dataset.\n",
    "        The h5 file is named with the anonymized patient id.\n",
    "    Args:\n",
    "        workdir (str): The path to the workdir.\n",
    "        key (str): The key to the mapping dictionary.\n",
    "        do_rm_zero_pad (bool): If True, the zero padding is removed.\n",
    "        phase_crop_shape (tuple): The shape to crop the kspace to.\n",
    "        do_norm_to_ref (bool): If True, the magnitude is normalized to the reference magnitude.\n",
    "        max_mag_ref (float): The reference magnitude.\n",
    "    '''\n",
    "    print(f\"!!!!!!max_mag_ref is set to {max_mag_ref}. This must be changed later to the max of the NYU dataset.\")\n",
    "    print(f\"!!!!!!norm and max of the h5 file must be calculated later and set properly.\")\n",
    "\n",
    "    pat_seq_id  = get_patient_id(fpath).strip()\n",
    "    pat_anon_id = get_anon_pat_id_from_pat_id(pat_seq_id, get_mapping_patient_ids(workdir, key))\n",
    "    print(f\"Processing patient {pat_seq_id} with pat_anon_id {pat_anon_id}\")\n",
    "\n",
    "    fpath_hf = os.path.join(workdir, \"output\", \"h5s\", f\"{pat_anon_id}_pst_T2.h5\")\n",
    "\n",
    "    if not os.path.exists(fpath_hf):\n",
    "        with h5py.File(fpath_hf, 'w') as hf:\n",
    "            print(f\"created h5 file at {fpath_hf}\")\n",
    "    else:\n",
    "        print(f\"\\tH5 file already exists: {fpath_hf}\")\n",
    "\n",
    "    process_mrd_to_h5(\n",
    "        fpath            = fpath,\n",
    "        fpath_hf         = fpath_hf,\n",
    "        phase_crop_shape = phase_crop_shape,\n",
    "        max_mag_ref      = max_mag_ref,\n",
    "        do_rm_zero_pad   = do_rm_zero_pad,\n",
    "        do_norm_to_ref   = do_norm_to_ref,\n",
    "        pat_seq_id       = pat_seq_id,\n",
    "        pat_anon_id      = pat_anon_id\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the paths to the t2 tra files\n",
    "t2_tra_paths = get_t2_tra_paths(WORKDIR, verbose=False)\n",
    "print(t2_tra_paths)\n",
    "\n",
    "t2_tra_paths = [t2_tra_paths[12]]   # patient 0003\n",
    "\n",
    "# loop over the paths and create the h5 files\n",
    "for f_idx, fpath in enumerate(t2_tra_paths):\n",
    "    create_t2w_h5_from_mrd_umcg(\n",
    "        fpath            = fpath,\n",
    "        workdir          = WORKDIR,\n",
    "        key              = KEY,\n",
    "        do_rm_zero_pad   = True, \n",
    "        phase_crop_shape = None, # (3, 30, 20, 640, 551) # same shape. We will do the phase cropping in the RIM code. Make a new dataset there with a transform that does this operation. At a later stage in this code it will also include kspace zero-padding.\n",
    "        do_norm_to_ref   = True,\n",
    "        max_mag_ref      = 0.006096669,  # Based on one patient of the NYU dataset the max_magnitude reference: # MUST IN THE FUTURE BE DETERMINED ON THE ENTIRE NYU dataset. Or return the NYU RIM model on normalized data.\n",
    "    )\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"debug mode enabled. breaking after 1 patient dir..\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the kspace from npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kspace loaded from /mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace/tmp/kspace_0003.npy\n"
     ]
    }
   ],
   "source": [
    "# load the numpy file if it exists\n",
    "if os.path.exists(os.path.join(TEMPDIR, \"kspace_0003.npy\")):\n",
    "    kspace = np.load(os.path.join(TEMPDIR, \"kspace_0003.npy\"))\n",
    "    print(f\"kspace loaded from {os.path.join(TEMPDIR, 'kspace_0003.npy')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average 1 and 2 together but keep the dimension\n",
    "kspace = np.sum(kspace, axis=0, keepdims=True)\n",
    "\n",
    "# add coil 1 and 2 together but keep the dimension\n",
    "kspace = np.sum(kspace, axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30, 1, 768, 814)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kspace.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load in the kspace as is and try to reorder it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ksapce shape: (1, 30, 1, 768, 814)\n",
      "Number of slices: 30\n",
      "Half index: 15\n",
      "safe_rss_to_nifti_file_reorder - kspace shape: ((1, 30, 1, 768, 814))\n",
      "safe_rss_to_nifti_file_reorder - Image  shape: (30, 768, 814)\n",
      "safe_rss_to_nifti_file_reorder - kspace shape collapsed from averages: (30, 1, 768, 814)\n",
      "Saved image to /mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace/tmp/_roerdered_ksp_25-14-42_rss_recon.nii.gz\n",
      "safe_rss_to_nifti_file_reorder - kspace shape: ((1, 30, 1, 768, 814))\n",
      "safe_rss_to_nifti_file_reorder - Image  shape: (30, 768, 814)\n",
      "safe_rss_to_nifti_file_reorder - kspace shape collapsed from averages: (30, 1, 768, 814)\n",
      "Saved image to /mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace/tmp/_not_reordered_25-14-42_rss_recon.nii.gz\n"
     ]
    }
   ],
   "source": [
    "def safe_rss_to_nifti_file(kspace: np.ndarray, fname_part, do_round=True) -> None:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Perform root sum of squares reconstruction on the given k-space.\n",
    "    Args:\n",
    "        kspace (np.ndarray): The k-space data. 5D array with dimensions [num_averages, num_slices, num_coils, num_readout_points, num_phase_encode_steps]\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    assert kspace.ndim == 5, \"image should have 5 dimensions: (n_avg, n_slices, n_coils, n_freq, n_phase)\"\n",
    "\n",
    "    rss_recon = rss_recon_from_ksp(kspace, averages=(0,1), verbose=True, printkey=\"safe_rss_to_nifti_file_reorder\")\n",
    "\n",
    "    if do_round:\n",
    "        rss_recon = np.round(rss_recon*1000, decimals=3)\n",
    "\n",
    "    save_numpy_rss_as_nifti(rss_recon, fname=f\"{fname_part}rss_recon\", dir=TEMPDIR)\n",
    "\n",
    "\n",
    "def get_slice_remap_based_instanceID_and_patPosition(dicom_dir: str, verbose=False):\n",
    "\n",
    "    instance_numbers = []\n",
    "    acquisition_times = []\n",
    "    for filename in os.listdir(dicom_dir):\n",
    "        ds = pydicom.dcmread(os.path.join(dicom_dir, filename))\n",
    "\n",
    "        instance_numbers.append(ds.InstanceNumber)\n",
    "        acquisition_times.append(ds.ImagePositionPatient[-1])\n",
    "\n",
    "    # sort both the instance numbers and the acquisition times based on acquistion times small first then large\n",
    "    sorted_instance_numbers = [x for _,x in sorted(zip(acquisition_times, instance_numbers))]\n",
    "    # nicely print the sorted lists again:\n",
    "\n",
    "    for i in range(len(sorted_instance_numbers)):\n",
    "        print(f\"{sorted_instance_numbers[i]} : {acquisition_times[i]}\")\n",
    "\n",
    "    return np.array(sorted_instance_numbers) - 1  # -1 to make it zero-based for Python array indexing\n",
    "\n",
    "\n",
    "def reorder_interleaved_slices_to_sequential(kspace_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reorder k-space slices from interleaved to sequential: all even slices first, followed by all odd slices.\n",
    "    \n",
    "    Parameters:\n",
    "        kspace_array: Numpy array of shape [num_averages, num_slices, num_coils, num_readout_points, num_phase_encode_steps].\n",
    "        \n",
    "    Returns:\n",
    "        reordered_k_space_data: Numpy array with reordered slices, maintaining the same shape as the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate that the input data has the expected 5D shape\n",
    "    assert kspace_array.ndim == 5, \"Input data must be a 5D array with dimensions [num_averages, num_slices, num_coils, num_readout_points, num_phase_encode_steps]\"\n",
    "    \n",
    "    # Log the reordering operation for debugging purposes\n",
    "    print(\"\\nReordering k-space slices...\")\n",
    "    \n",
    "    # Compute the mid-point index for reordering slices, rounding up for odd numbers\n",
    "    num_slices = kspace_array.shape[1]\n",
    "    mid_slice_idx = ((num_slices // 2) + 1) if num_slices % 2 == 1 else num_slices // 2\n",
    "    \n",
    "    # Initialize an output array with the same shape and data type as the input array\n",
    "    reordered_k_space_data = np.zeros_like(kspace_array, dtype=np.complex64)\n",
    "    \n",
    "    reordered_k_space_data[:, 1::2] = kspace_array[:, :mid_slice_idx]\n",
    "    reordered_k_space_data[:, ::2] = kspace_array[:, mid_slice_idx:]\n",
    "    \n",
    "    safe_rss_to_nifti_file(reordered_k_space_data, fname_part=\"_reorder_ori_tiago\")\n",
    "\n",
    "    return reordered_k_space_data\n",
    "\n",
    "\n",
    "def reorder_k_space_even_odd(k_space: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rearranges the k-space data by interleaving even and odd indexed slices.\n",
    "\n",
    "    Parameters:\n",
    "    - k_space (numpy.ndarray): The input k-space data to be rearranged.\n",
    "\n",
    "    Returns:\n",
    "    - interleaved_k_space (numpy.ndarray): The k-space data with interleaved slices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a new complex array to store the interleaved k-space data\n",
    "    interleaved_k_space = np.zeros_like(k_space, dtype=np.complex64)\n",
    "\n",
    "    # Calculate the middle index for slicing the array into two halves\n",
    "    num_slices = k_space.shape[1]\n",
    "    middle_index = (num_slices + 1) // 2  # Handles both odd and even cases\n",
    "\n",
    "    # Interleave even and odd indexed slices\n",
    "    interleaved_k_space[:, ::2] = k_space[:, middle_index:]  # Place the second half at even indices\n",
    "    interleaved_k_space[:, 1::2] = k_space[:, :middle_index]  # Place the first half at odd indices\n",
    "\n",
    "    return interleaved_k_space\n",
    "\n",
    "\n",
    "def reorder_kspace_inplace(ksp_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rearrange k-space slices in place based on interleaved order.\n",
    "\n",
    "    Parameters:\n",
    "    - ksp_array (numpy.ndarray): 5D array of shape [num_averages, num_slices, num_coils, num_readout_points, num_phase_encode_steps].\n",
    "\n",
    "    Returns:\n",
    "    - ksp_array (numpy.ndarray): Rearranged 5D array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate that the input data has the expected 5D shape\n",
    "    assert ksp_array.ndim == 5 or ksp_array.ndim==3, \"Input data must be a 3D, or 5D array\"\n",
    "\n",
    "    num_slices = ksp_array.shape[1]\n",
    "    half_idx = (num_slices + 1) // 2\n",
    "\n",
    "    # Compute indices for the reordered array\n",
    "    even_indices = np.arange(half_idx) * 2\n",
    "    odd_indices = even_indices + 1\n",
    "    interleaved_indices = np.zeros(num_slices, dtype=np.int32)\n",
    "\n",
    "    interleaved_indices[:half_idx] = even_indices\n",
    "    interleaved_indices[half_idx:num_slices] = odd_indices[:num_slices - half_idx]\n",
    "\n",
    "    # Rearrange the array based on computed indices\n",
    "    ksp_array[:, :] = ksp_array[:, interleaved_indices]\n",
    "\n",
    "    return ksp_array\n",
    "\n",
    "\n",
    "###############################\n",
    "print(f\" ksapce shape: {kspace.shape}\")\n",
    "\n",
    "pat_dcm_dir = \"/mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_ksp_umcg/workspace/input/sectra_export_pat0001_pat0010_anon_dicoms/ANON5046358/2022-05-10/T2W_TRA\"\n",
    "# remap_indices = get_slice_remap_instanceNumber_vs_imagePatientPosition(pat_dcm_dir, verbose=True)\n",
    "# remap_indices = get_slice_remap_instanceNumber_vs_acquisitionTime(pat_dcm_dir, verbose=True)\n",
    "# remap_indices = get_slice_remap_by_image_position(pat_dcm_dir, verbose=True)\n",
    "# remap_indices = get_slice_remap_by_instace_number(pat_dcm_dir, verbose=True)\n",
    "\n",
    "if True:\n",
    "    if False:   \n",
    "        # method 1 - Acquisition time\n",
    "        remap_indices = get_slice_remap_based_instanceID_and_patPosition(pat_dcm_dir, verbose=True)\n",
    "        kspace_reordered = kspace[:, remap_indices, :, :, :]\n",
    "\n",
    "    if True:\n",
    "        # method 2 - Simple zipping\n",
    "        kspace_reordered = reorder_k_space_even_odd(kspace)\n",
    "\n",
    "if False:\n",
    "    kspace_reordered = reorder_kspace_inplace(kspace)\n",
    "\n",
    "# create a timestamp with current date and time\n",
    "now = datetime.datetime.now()\n",
    "timestring = now.strftime(\"%d-%H-%M\")\n",
    "\n",
    "# save image space\n",
    "safe_rss_to_nifti_file(kspace_reordered, fname_part=f\"_roerdered_ksp_{timestring}_\", do_round=True)\n",
    "safe_rss_to_nifti_file(kspace, fname_part=f\"_not_reordered_{timestring}_\", do_round=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create H5s from MRD files (RUMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers_from_ismrmrd_rumc(fpath: str, verbose=False) -> dict:\n",
    "    '''\n",
    "    Description:\n",
    "        - get the headers from a .mrd file\n",
    "    Arguments:\n",
    "        - fpath: path to the .mrd file\n",
    "    Returns:\n",
    "        - headers: dictionary with the headers\n",
    "    '''\n",
    "    dset = ismrmrd.Dataset(fpath, 'dataset', create_if_needed=False)\n",
    "    # what is the type of dset\n",
    "    print(type(dset))\n",
    "    # what is within: ismrmrd.hdf5.Dataset\n",
    "    print(dir(dset))\n",
    "\n",
    "    print(\"doneeee\")\n",
    "    headers = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\tHEADERS found in .mrd file\")\n",
    "        print(f\"\\t\\tHEADERS.SUBJECTINFORMATION\")\n",
    "        print_headers(headers.subjectInformation)\n",
    "        print(f\"\\t\\tHEADERS.MEASUREMENTINFORMATION\")\n",
    "        print_headers(headers.measurementInformation)\n",
    "        print(f\"\\t\\tHEADERS.ACQUISITIONSYSTEMINFORMATION\")\n",
    "        print_headers(headers.acquisitionSystemInformation)\n",
    "        print(f\"\\t\\tHEADERS.SEQUENCEPARAMETERS\")\n",
    "        print_headers(headers.sequenceParameters)\n",
    "        print(f\"\\t\\tHEADERS.USERPARAMETERS\")\n",
    "        print_headers(headers.userParameters)\n",
    "        print(f\"\\t\\tHEADERS.EXPERIMENTALCONDITIONS\")\n",
    "        print_headers(headers.experimentalConditions)\n",
    "        print(f\"\\t\\tHEADERS.ENCODEDSPACE\")\n",
    "        print_headers(headers.encoding[0].encodedSpace)\n",
    "        print(f\"\\t\\tHEADERS.TRAJECTORY\")\n",
    "        print_headers(headers.encoding[0].trajectory)\n",
    "        print(f\"\\t\\tHEADERS.RECONSPACE\")\n",
    "        print_headers(headers.encoding[0].reconSpace)\n",
    "        print(f\"\\t\\tHEADERS.ENCODINGLIMITS\")\n",
    "        print_headers(headers.encoding[0].encodingLimits)\n",
    "        print(f\"\\t\\tHEADERS.PARALLELIMAGING\")\n",
    "        print_headers(headers.encoding[0].parallelImaging)\n",
    "        print(type(headers))\n",
    "\n",
    "    return headers\n",
    "\n",
    "\n",
    "def build_np_from_ismrmrd_rumc(fpath: str, DEBUG=False, verbose=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        - fpath: path to the .mrd file\n",
    "    \n",
    "    Returns:\n",
    "        - kspace: numpy array of kspace data in shape (navgs, nslices, ncoils, rNx, eNy + 1) complex\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f\"Building kspace from .mrd file\")\n",
    "\n",
    "    # lets time this whole function and print it\n",
    "    start = time.time()\n",
    "\n",
    "    # Read the header and get encoding information\n",
    "    dset = ismrmrd.Dataset(fpath, dataset_name='dataset_2', create_if_needed=False)\n",
    "    header = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "    enc = header.encoding[0]\n",
    "\n",
    "    # Determine some parameters of the acquisition\n",
    "    ncoils     = header.acquisitionSystemInformation.receiverChannels\n",
    "    nslices    = enc.encodingLimits.slice.maximum + 1 if enc.encodingLimits.slice is not None else 1\n",
    "    eNy        = enc.encodedSpace.matrixSize.y\n",
    "    rNx        = enc.reconSpace.matrixSize.x\n",
    "    eTL        = 25 if DEBUG else echo_train_length(dset)\n",
    "    eTC        = 11 if DEBUG else echo_train_count(dset, echo_train_len=eTL)\n",
    "    firstacq   = 1 if DEBUG else get_first_acquisition(dset)\n",
    "    navgs      = 3 if DEBUG else get_num_averages(firstacq=firstacq, dset=dset)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ncoils: {ncoils}\")\n",
    "        print(f\"nslices: {nslices}\")\n",
    "        print(f\"eNy: {eNy}\")\n",
    "        print(f\"rNx: {rNx}\")\n",
    "        print(f\"eTL: {eTL}\")\n",
    "        print(f\"eTC: {eTC}\")\n",
    "        print(f\"firstacq: {firstacq}\")\n",
    "        print(f\"navgs: {navgs}\")\n",
    "\n",
    "    # Initialize the kspace data array\n",
    "    kspace = np.zeros((navgs, nslices, ncoils, rNx, eNy + 1), dtype=np.complex64)\n",
    "\n",
    "    # Loop through the rest of the acquisitions and fill the data array with the kspace data\n",
    "    with tqdm(total=dset.number_of_acquisitions() - firstacq) as pbar:\n",
    "        for acqnum in range(firstacq, dset.number_of_acquisitions()):\n",
    "            acq    = dset.read_acquisition(acqnum)\n",
    "            slice1 = acq.idx.slice\n",
    "            y      = acq.idx.kspace_encode_step_1\n",
    "            avg    = acq._head.idx.average\n",
    "\n",
    "            # each acquisition is a 2D array of shape (coil, rNx) complex\n",
    "            kspace[avg, slice1, :, :, y] = acq.data\n",
    "            pbar.update(1)\n",
    "\n",
    "    # reorder the data from even and odd number of slices being interleaved to all even slices first and then all odd slices\n",
    "    kspace = reorder_kspace_slices(kspace)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"build_np_from_ismrmrd took {time.time() - start} seconds\")\n",
    "    \n",
    "    return kspace\n",
    "\n",
    "\n",
    "def get_patient_id_rumc(fpath):\n",
    "    '''\n",
    "    Get the patient id from the file path.\n",
    "    Assumes the file path has the format: /mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_rumc/pst_example/raw/<patient_id>/<date>/<file_name>.mrd\n",
    "    '''\n",
    "    # Convert fpath to a string, then split it using the appropriate separator\n",
    "    fpath_str = str(fpath)\n",
    "    patient_id = fpath_str.split(\"/\")[-3]\n",
    "    return patient_id\n",
    "\n",
    "\n",
    "def create_h5_from_mrd_rumc(fpath: str, workdir: str, verbose=False, debug=False) -> None:\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    print(f\"Processing patient T2W MRD file with path: {fpath}\")\n",
    "\n",
    "    # get the patient id from the file path\n",
    "    patient_id = get_patient_id_rumc(fpath)\n",
    "    print(f\"\\tPatient ID RUMC: {patient_id}\")\n",
    "\n",
    "    # create the h5 file path\n",
    "    fpath_hf = os.path.join(workdir, \"output\", \"h5s\", f\"{patient_id}_pst_T2.h5\")\n",
    "\n",
    "    # check if the h5 file already exists\n",
    "    if not os.path.exists(fpath_hf):\n",
    "        with h5py.File(fpath_hf, 'w') as hf:\n",
    "            print(f\"created h5 file at {fpath_hf}\")\n",
    "    else:\n",
    "        print(f\"\\tH5 file already exists: {fpath_hf}\")\n",
    "\n",
    "    # Verify and create missing components in the H5 file.\n",
    "    with h5py.File(fpath_hf, 'r+') as hf:\n",
    "\n",
    "        if not has_kspace_key(fpath_hf):\n",
    "            kspace = build_np_from_ismrmrd_rumc(fpath, DEBUG=False, verbose=True)\n",
    "            hf.create_dataset('kspace', data=kspace)\n",
    "            print(f\"\\tcreated kspace key and filled with kspace data in h5 file: {fpath_hf}\")\n",
    "\n",
    "        if not has_correct_shape(fpath_hf):\n",
    "            raise Exception(f\"kspace shape is not correct. Shape: {hf['kspace'].shape}\")\n",
    "        \n",
    "        # if the headers key is not present in the h5 file yet \n",
    "        # then add the headers key and fill it with headers data from the .mrd file\n",
    "        # if not has_headers_key(fpath_hf):\n",
    "        #     rumc_headers_mrd  = get_headers_from_ismrmrd_rumc(fpath, verbose=True)\n",
    "        #     rumcheaders_dict = convert_ismrmrd_headers_to_dict(rumc_headers_mrd)\n",
    "        #     header_bytes      = encode_umcg_header_to_bytes(umcg_to_nyu_dict=rumc_headers_dict)\n",
    "        #     hf.create_dataset('ismrmrd_header', data=header_bytes)\n",
    "\n",
    "\n",
    "# set workdir based on wsl execution RUMC\n",
    "if os.path.exists(\"/mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_rumc/workspace\"):\n",
    "    WORKDIR_RUMC = \"/mnt/c/Users/qvloh/Documents/phd_lok/datasets/prostate_rumc/workspace\"\n",
    "else: \n",
    "    WORKDIR_RUMC = \"C:\\\\Users\\\\qvloh\\\\Documents\\\\phd_lok\\\\datasets\\\\prostate_rumc\\\\workspace\"\n",
    "\n",
    "print(f\"workdir rumc: {WORKDIR_RUMC}\")\n",
    "\n",
    "create_h5_from_mrd_rumc(FPATH_RUMC_MRD_SAMPLE, WORKDIR_RUMC, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Kspaces (UMCG, NYU, or RUMC) into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp_umcg = h5py.File(FPATH_UMCG_H5_SAMPLE, 'r')[\"kspace\"][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp_nyu = h5py.File(FPATH_NYU_H5_SAMPLE, 'r')[\"kspace\"][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp_rumc = h5py.File(FPATH_RUMC_H5_SAMPLE, 'r')[\"kspace\"][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 - Remove the zero-padding from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ksp_umcg' in globals():\n",
    "    print(\"UMCG\")\n",
    "    ksp_umcg = remove_zero_padding(ksp_umcg, verbose=True)\n",
    "\n",
    "if 'ksp_nyu' in globals():\n",
    "    print(\"\\nNYU\")\n",
    "    ksp_nyu  = remove_zero_padding(ksp_nyu,  verbose=True)\n",
    "\n",
    "if 'ksp_rumc' in globals():\n",
    "    print(\"\\nRUMC\")\n",
    "    ksp_rumc = remove_zero_padding(ksp_rumc, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - Remove 50 lines left and 50 lines right of the UMCG kspace so that the dimensions are the same as NYU.\n",
    "Look at the UMCG kspace zero padding removed. We will crop in k-space because there seems to be a similar field of view for both NYU and UMCG. So we will go crop kspace for UMCG to 640x451. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp_umcg, new_hdrs = crop_kspace_in_phase_direction(ksp_umcg, target_shape=(3, 31, 20, 640, 451), fpath_mrd=FPATH_UMCG_MRD_SAMPLE, verbose=True)\n",
    "print(f\"Cropped kspace shape is now: {ksp_umcg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 - Normalize the UMCG data to the same range as NYU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on one patient of the NYU dataset the max_magnitude reference: \n",
    "max_magnitude_reference = 0.006096669    # comes from. but then i do not have to load in the reference kspace\n",
    "ksp_umcg = normalize_to_reference(ksp_umcg, max_magnitude_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 - Write to h5 together with the ismrmrd header. Add the changed header to the new h5 file, othwerise the RIM doesn't understand the phase dimension anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the patient id for the file name\n",
    "with h5py.File(FPATH_UMCG_H5_SAMPLE, 'r') as f:\n",
    "    pat_id = f.attrs[\"patient_id\"]\n",
    "\n",
    "# create the h5 file path\n",
    "fpath_fixed_ksp_umcg = Path(WORKDIR, \"tmp\", f\"{pat_id}_pst_T2W_transformed_v3.h5\")\n",
    "\n",
    "# create the h5 file if it does not exist\n",
    "if not os.path.exists(fpath_fixed_ksp_umcg):\n",
    "    with h5py.File(fpath_fixed_ksp_umcg, 'w') as hf:\n",
    "        print(f\"created h5 file at {fpath_fixed_ksp_umcg}\")\n",
    "        hf.create_dataset('kspace', data=ksp_umcg)\n",
    "        hf.create_dataset('ismrmrd_header', data=new_hdrs)\n",
    "\n",
    "        hf.attrs['acquisition'] = 'AXT2'\n",
    "        hf.attrs['max'] = 0.00054\n",
    "        hf.attrs['norm'] = 0.149\n",
    "        hf.attrs['patient_id'] = pat_id\n",
    "        hf.attrs['patient_id2'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the UMCG and NYU prostate kspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape, Dtype, Zero-Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMCG\n",
    "if 'ksp_umcg' in globals():\n",
    "    zp_umcg, zp_idxs_umcg = calculate_zero_padding_PE(ksp_umcg)\n",
    "    print(\"UMCG - kspace shape: \", ksp_umcg.shape)\n",
    "    print(\"UMCG - kspace dtype: \", ksp_umcg.dtype)\n",
    "    print(f\"UMCG - Amount of zero padding: {zp_umcg} in phase encoding direction.\")\n",
    "    print(f\"UMCG - Data is acquired until {ksp_umcg.shape[-1] - zp_umcg}\")\n",
    "    print()\n",
    "\n",
    "# NYU\n",
    "if 'ksp_nyu' in globals():\n",
    "    zp_nyu, zp_idxs_nyu = calculate_zero_padding_PE(ksp_nyu)\n",
    "    print(\"NYU  - kspace shape: \", ksp_nyu.shape)\n",
    "    print(\"NYU  - kspace dtype: \", ksp_nyu.dtype)\n",
    "    print(f\"NYU  - Amount of zero padding: {zp_nyu} in phase encoding direction.\")\n",
    "    print(f\"NYU  - Data is acquired until {ksp_nyu.shape[-1] - zp_nyu}\")\n",
    "    print()\n",
    "\n",
    "# RUMC\n",
    "if 'ksp_rumc' in globals():\n",
    "    zp_rumc, zp_idxs_rumc = calculate_zero_padding_PE(ksp_rumc)\n",
    "    print(\"RUMC - kspace shape: \", ksp_rumc.shape)\n",
    "    print(\"RUMC - kspace dtype: \", ksp_rumc.dtype)\n",
    "    print(f\"RUMC - Amount of zero padding: {zp_rumc} in phase encoding direction.\")\n",
    "    print(f\"RUMC - Data is acquired until {ksp_rumc.shape[-1] - zp_rumc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kspace Averages Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMCG\n",
    "if True:\n",
    "    print(\"UMCG\")\n",
    "    ft = f'\\n\\n\\n\\n\\nThere is a chunck of zero-padding in the phase direction.'\n",
    "    visualize_kspace_averages(ksp_umcg, slice_idx=0, coil_idx=0, outdir=TEMPDIR, fname_id=\"UMCG\", figtext=ft)\n",
    "\n",
    "# NYU\n",
    "if True:\n",
    "    print(\"\\nNYU\")\n",
    "    ft = f'\\n\\n\\n\\n\\nThere seems to be no zero padding.'\n",
    "    visualize_kspace_averages(ksp_nyu, slice_idx=0, coil_idx=0, outdir=TEMPDIR, fname_id=\"NYU\", figtext=ft)\n",
    "\n",
    "# RUMC\n",
    "if False:\n",
    "    print(\"\\nRUMC\")\n",
    "    # ft = f'\\n\\n\\n\\n\\nThere seems to be no zero padding.'\n",
    "    ft = \"\\n\\n\\n\\n\\nThere seems to be no zero padding.\"\n",
    "    ft += \"\\n\\n\\n\\n\\nThere seems to be only 2 averages.\"\n",
    "    visualize_kspace_averages(ksp_rumc, slice_idx=0, coil_idx=0, outdir=TEMPDIR, fname_id=\"RUMC\", figtext=ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIEW - UMCG headers in MRD format & NYU in ISMRMRD format (a Comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UMCG\")\n",
    "umcg_headers = get_headers_from_ismrmrd(FPATH_UMCG_MRD_SAMPLE, verbose=True)\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\nNYU\")\n",
    "nyu_headers  = get_headers_from_h5(FPATH_NYU_H5_SAMPLE, verbose=False)\n",
    "\n",
    "if False:\n",
    "    print(\"\\n\\n\\n\\n\\nRUMC\")\n",
    "    rumc_headers = get_headers_from_h5(FPATH_RUMC_H5_SAMPLE, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify that both H5 files - UMCG and NYU - have the same header format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umcg_headers = get_headers_from_h5(FPATH_UMCG_H5_SAMPLE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_headers  = get_headers_from_h5(FPATH_NYU_H5_SAMPLE, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS Reconstructions for UMCG, NYU and RUMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ksp_umcg' in globals():\n",
    "    print(\"UMCG\")\n",
    "    umcg_rss = rss_recon_from_ksp(ksp_umcg, averages=(0,1), verbose=True, printkey=\"UMCG\")\n",
    "    save_numpy_rss_as_nifti(umcg_rss, \"umcg_rss\", dir=TEMPDIR)\n",
    "\n",
    "if 'ksp_nyu' in globals():\n",
    "    print(\"\\nNYU\")\n",
    "    nyu_rss = rss_recon_from_ksp(ksp_nyu,  averages=(0,1), verbose=True, printkey=\"NYU\")\n",
    "    save_numpy_rss_as_nifti(nyu_rss, \"nyu_rss\", dir=TEMPDIR)\n",
    "\n",
    "if 'ksp_rumc' in globals():\n",
    "    print(\"\\nRUMC\")\n",
    "    rumc_rss = rss_recon_from_ksp(ksp_rumc,  averages=(0,1), verbose=True, printkey=\"RUMC\")\n",
    "    save_numpy_rss_as_nifti(rumc_rss, \"rumc_rss\", dir=TEMPDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the MAX and NORM of the RSS reconstruction and what are these values in the h5 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nyu_rss.shape)\n",
    "\n",
    "norm = np.linalg.norm(nyu_rss[:])\n",
    "max = np.max(nyu_rss[:])\n",
    "\n",
    "print(f\"max and norm of nyu_rss: {max} and {norm}\")\n",
    "\n",
    "with h5py.File(FPATH_NYU_H5_SAMPLE, 'r') as f:\n",
    "    print(f.attrs.keys())\n",
    "    # print the max and norm of the h5 file\n",
    "    print(f\"max and norm of nyu_rss: {f.attrs['max']} and {f.attrs['norm']}\")\n",
    "\n",
    "\n",
    "# Use center_crop_im(im_3d: np.ndarray, crop_to_size: Tuple[int, int]) -> np.ndarray: to do the cropping\n",
    "nyu_rss_crop = center_crop_im(nyu_rss, (320, 320))\n",
    "# calculate the norm and max of the cropped image\n",
    "norm = np.linalg.norm(nyu_rss_crop[:])\n",
    "max = np.max(nyu_rss_crop[:])\n",
    "print(f\"max and norm of nyu_rss_crop: {max} and {norm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize each institutes k-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ksp_umcg' in globals():\n",
    "    print(\"UMCG \")\n",
    "    safe_kspace_slice_to_png(ksp_umcg, TEMPDIR, titlepart=\"UMCG\", do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))\n",
    "\n",
    "if 'ksp_nyu' in globals():\n",
    "    print(\"\\nNYU \")\n",
    "    safe_kspace_slice_to_png(ksp_nyu,  TEMPDIR, titlepart=\"NYU\",  do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))\n",
    "\n",
    "if 'ksp_rumc' in globals():\n",
    "    print(\"\\nRUMC \")\n",
    "    safe_kspace_slice_to_png(ksp_rumc, TEMPDIR, titlepart=\"RUMC\", do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new visualizations of the zero-pad removed kspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ksp_umcg' in globals():\n",
    "    print(\"UMCG\")\n",
    "    safe_kspace_slice_to_png(ksp_umcg, TEMPDIR, titlepart=\"UMCG_nzp\", do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))\n",
    "    umcg_rss_nzp = rss_recon_from_ksp(ksp_umcg, averages=(0,1), verbose=True, printkey=\"UMCG\")\n",
    "    save_numpy_rss_as_nifti(umcg_rss_nzp, \"umcg_rss_nzp\", dir=TEMPDIR)\n",
    "\n",
    "if 'ksp_nyu' in globals():\n",
    "    print(\"\\nNYU\")\n",
    "    safe_kspace_slice_to_png(ksp_nyu,  TEMPDIR, titlepart=\"NYU_nzp\",  do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))\n",
    "    nyu_rss_nzp  = rss_recon_from_ksp(ksp_nyu,  averages=(0,1), verbose=True, printkey=\"NYU\")\n",
    "    save_numpy_rss_as_nifti(nyu_rss_nzp, \"nyu_rss_nzp\", dir=TEMPDIR)\n",
    "\n",
    "if 'ksp_rumc' in globals():\n",
    "    print(\"\\nRUMC\")\n",
    "    safe_kspace_slice_to_png(ksp_rumc, TEMPDIR, titlepart=\"RUMC_nzp\", do_log=True, slice_no=0, coil_no=0, avg_to_add=(0,1))\n",
    "    rumc_rss_nzp = rss_recon_from_ksp(ksp_rumc, averages=(0,1), verbose=True, printkey=\"RUMC\")\n",
    "    save_numpy_rss_as_nifti(rumc_rss_nzp, \"rumc_rss_nzp\", dir=TEMPDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the UMCG attributes exist correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(FPATH_UMCG_H5_SAMPLE, 'r') as f:\n",
    "    print(f\"keys in h5: {list(f.keys())}\")\n",
    "    attrs = dict(f.attrs)\n",
    "    print(\"attributes length = \", len(attrs))\n",
    "    for k, v in attrs.items():\n",
    "        print(f\"\\t{k}: {v}\")\n",
    "\n",
    "# get only the patient id\n",
    "with h5py.File(FPATH_UMCG_H5_SAMPLE, 'r') as f:\n",
    "    # get the patient_id attribute of the h5 file\n",
    "    pat_id = f.attrs[\"patient_id\"]\n",
    "    print(pat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the zero padding to the right side for a nice looking anatomy. NYU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary pad the NYU kspace to 640 phase and also the UMCG\n",
    "ksp_nyu_padded = np.pad(ksp_nyu, ((0,0), (0,0), (0,0), (0,0), (0, 640-ksp_nyu.shape[-1])), mode='constant')\n",
    "print(f\"ksp_nyu_padded.shape = {ksp_nyu_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do RSS and safe as nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the RSS of the zero-padded kspace and save it as a nifti\n",
    "nyu_nzp_reduced_rss = rss_recon_from_ksp(ksp_nyu_padded, averages=(0,1), verbose=True, printkey=\"nyu\")\n",
    "save_numpy_rss_as_nifti(nyu_nzp_reduced_rss, \"nyu_padded_for_img_space\", dir=TEMPDIR)\n",
    "del ksp_nyu_padded, nyu_nzp_reduced_rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Nifti of the UMCG DATA (RSS Recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: calculate the RSS of the reduced kspace and save it as a nifti\n",
    "umcg_nzp_reduced_rss = rss_recon_from_ksp(ksp_umcg, averages=(0,1), verbose=True, printkey=\"UMCG\")\n",
    "save_numpy_rss_as_nifti(umcg_nzp_reduced_rss, \"umcg_init_nzp_rss\", dir=TEMPDIR)\n",
    "\n",
    "# STEP 4: Pad the UMCG kspace to 640 phase\n",
    "ksp_umcg_padded = np.pad(ksp_umcg, ((0,0), (0,0), (0,0), (0,0), (0, 640-nyu_shape[-1])), mode='constant')\n",
    "print(f\"\\nSTEP 4: ksp_umcg_padded.shape = {ksp_umcg_padded.shape}\")\n",
    "\n",
    "# STEP 5: calculate the RSS of the reduced kspace and save it as a nifti\n",
    "umcg_nzp_reduced_rss = rss_recon_from_ksp(ksp_umcg_padded, averages=(0,1), verbose=True, printkey=\"UMCG\")\n",
    "save_numpy_rss_as_nifti(umcg_nzp_reduced_rss, \"umcg_padded_for_img_space\", dir=TEMPDIR)\n",
    "\n",
    "del ksp_umcg_padded, umcg_nzp_reduced_rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyize both the NYU and UMCG kspace and check for differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_kspace_3d_vol(ksp_umcg, \"K-Space UMCG\")\n",
    "hist_real_and_imag(ksp_umcg, \"K-Space UMCG\")\n",
    "\n",
    "analyze_kspace_3d_vol(ksp_nyu, \"K-Space NYU\")\n",
    "hist_real_and_imag(ksp_nyu, \"K-Space NYU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Normalization check that UMCG has similar ranges to NYU data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"UMCG\")\n",
    "analyze_kspace_3d_vol(ksp_umcg)\n",
    "\n",
    "print(f\"\\nNYU\")\n",
    "analyze_kspace_3d_vol(ksp_nyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The UMCG kspace now has correct dimensions it seems and also the correct real and imaginary ranges of values, so write to h5 together with the ismrmrd header. The header needs to be changed. Otherwise the RIM model doesnt understand the some things about the k-space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headers_from_mrd_wrapper(fpath_mrd: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Wrapper function to get the headers from a .mrd file and transform them to the NYU format.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the headers from the .mrd file\n",
    "    umcg_headers_mrd  = get_headers_from_ismrmrd(fpath_mrd, verbose=False)\n",
    "\n",
    "    # transform the headers a dictionary\n",
    "    umcg_headers_dict = convert_ismrmrd_headers_to_dict(umcg_headers_mrd)\n",
    "\n",
    "    # Set the correct matrix size and encoding limits for the NYU data\n",
    "    t = \"{http://www.ismrm.org/ISMRMRD}\"\n",
    "    umcg_headers_dict[f\"{t}ismrmrdHeader\"][f\"{t}encoding\"][f\"{t}encodedSpace\"][f\"{t}matrixSize\"][f\"{t}y\"] = str(450)\n",
    "    umcg_headers_dict[f\"{t}ismrmrdHeader\"][f\"{t}encoding\"][f\"{t}encodingLimits\"][f\"{t}kspace_encoding_step_1\"][f\"{t}maximum\"] = str(450)\n",
    "    umcg_headers_dict[f\"{t}ismrmrdHeader\"][f\"{t}encoding\"][f\"{t}encodingLimits\"][f\"{t}kspace_encoding_step_1\"][f\"{t}center\"] = str(225)\n",
    "\n",
    "    header_bytes = encode_umcg_header_to_bytes(umcg_to_nyu_dict=umcg_headers_dict)\n",
    "    return header_bytes\n",
    "\n",
    "# get the patient id.\n",
    "with h5py.File(FPATH_UMCG_H5_SAMPLE, 'r') as f:\n",
    "    pat_id = f.attrs[\"patient_id\"]\n",
    "\n",
    "# create the h5 file path\n",
    "fpath_fixed_ksp_umcg = Path(WORKDIR, \"tmp\", f\"{pat_id}_pst_T2W_transformed_v2.h5\")\n",
    "transformed_hdrs = get_headers_from_mrd_wrapper(FPATH_UMCG_MRD_SAMPLE)\n",
    "\n",
    "# create the h5 file if it does not exist\n",
    "if not os.path.exists(fpath_fixed_ksp_umcg):\n",
    "    with h5py.File(fpath_fixed_ksp_umcg, 'w') as hf:\n",
    "        print(f\"created h5 file at {fpath_fixed_ksp_umcg}\")\n",
    "        hf.create_dataset('kspace', data=ksp_umcg)\n",
    "        hf.create_dataset('ismrmrd_header', data=transformed_hdrs)\n",
    "\n",
    "        hf.attrs['acquisition'] = 'AXT2'\n",
    "        hf.attrs['max'] = 0.00054\n",
    "        hf.attrs['norm'] = 0.149\n",
    "        hf.attrs['patient_id'] = pat_id\n",
    "        hf.attrs['patient_id2'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the Actual Differences between the T2W dicom and the RSS Deep Learning Reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nifti(file_path, as_numpy=False, verbatim=False):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file using SimpleITK.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the NIfTI file.\n",
    "        as_numpy (bool): Whether to return the image as a numpy array.\n",
    "        verbatim (bool): Whether to print details about the loaded image/array.\n",
    "        \n",
    "    Returns:\n",
    "        SimpleITK.Image or numpy.ndarray: The loaded image.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found.\")\n",
    "    \n",
    "    sitk_img = sitk.ReadImage(file_path)\n",
    "    \n",
    "    if as_numpy:\n",
    "        array = sitk.GetArrayFromImage(sitk_img)\n",
    "\n",
    "        if verbatim:\n",
    "            print(f\"\\tshape: {array.shape}\")\n",
    "            print(f\"\\tmean:  {np.mean(array)}\")\n",
    "            print(f\"\\tstd:   {np.std(array)}\")\n",
    "            print(f\"\\tmin:   {np.min(array)}\")\n",
    "            print(f\"\\tmax:   {np.max(array)}\")\n",
    "        return array\n",
    "\n",
    "    else:\n",
    "        if verbatim:\n",
    "            print(f\"\\tDimensions: {sitk_img.GetDimension()}\")\n",
    "            print(f\"\\tSize:       {sitk_img.GetSize()}\")\n",
    "            print(f\"\\tSpacing:    {sitk_img.GetSpacing()}\")\n",
    "            print(f\"\\tOrigin:     {sitk_img.GetOrigin()}\")\n",
    "            print(f\"\\tDirection:  {sitk_img.GetDirection()}\")\n",
    "            print(f\"\\tPixel Type: {sitk_img.GetPixelIDTypeAsString()}\")\n",
    "\n",
    "    return sitk_img\n",
    "\n",
    "# Load the DICOM array of the correct patient\n",
    "print(pat_id)\n",
    "\n",
    "nifti_fpath = Path(WORKDIR, 'input', 'sectra_export_pat0001_pat0010_niftis', pat_id, '2022-04-07', 'tse2d1_25_1.3.12.2.1107.5.2.19.46133.2022040713122344044291325.0.0.0.nii.gz')\n",
    "print(nifti_fpath)\n",
    "\n",
    "# load the DICOM array of the same patient\n",
    "dcm_arr = load_nifti(nifti_fpath, as_numpy=True, verbatim=True)\n",
    "\n",
    "plt.imshow(dcm_arr[15, :, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# load the RSS DL array of the same patient\n",
    "\n",
    "# Perform analysis on the differences between the two.\n",
    "\n",
    "# What differences are observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small test to create a zero-padding function around each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad_kspace(ksp: np.ndarray, desired_shape: tuple, verbose=False) -> np.ndarray:\n",
    "    '''\n",
    "    Arguments:\n",
    "        - ksp: 2D numpy array of shape (readout, phase) complex\n",
    "        - desired_shape: tuple of desired shape (readout, phase)\n",
    "    Returns:\n",
    "        - padded_ksp: 2D numpy array of shape (readout, phase) complex\n",
    "    '''\n",
    "    diff = np.array(desired_shape) - np.array(ksp.shape)\n",
    "    pad_left  = diff // 2\n",
    "    pad_right = diff - pad_left\n",
    "    \n",
    "    padded_ksp = np.pad(ksp, ((pad_left[0], pad_right[0]), (pad_left[1], pad_right[1])), mode='constant')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"current shape:    {ksp.shape}\")\n",
    "        print(f\"desired shape:    {desired_shape}\")\n",
    "        print(f\"padded_ksp shape: {padded_ksp.shape}\")\n",
    "\n",
    "    return padded_ksp\n",
    "\n",
    "\n",
    "ksp = np.random.randn(640, 451) + 1j * np.random.randn(640, 451)\n",
    "ksp_zp = zero_pad_kspace(ksp, (1200, 1200), verbose=True)\n",
    "\n",
    "# visualize the original kspace and the zero padded kspace seperately\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(np.abs(ksp), cmap='gray')\n",
    "axs[0].set_title(\"Original k-space\")\n",
    "axs[1].imshow(np.abs(ksp_zp), cmap='gray')\n",
    "axs[1].set_title(\"Zero padded k-space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the created h5 file from MRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_h5_file = Path(WORKDIR, \"output\", \"h5s\", \"ANON2784451_pst_T2.h5\")\n",
    "\n",
    "with h5py.File(temp_h5_file, 'r') as f:\n",
    "    # print all keys\n",
    "    print(f\"keys in h5: {list(f.keys())}\")\n",
    "    print(f\"kspace shape = {f['kspace'].shape}\")\n",
    "    print(f\"kspace dtype = {f['kspace'].dtype}\")\n",
    "\n",
    "    # print all the attributes of the h5 file\n",
    "    attrs = dict(f.attrs)\n",
    "    print(\"attributes length = \", len(attrs))\n",
    "    for k, v in attrs.items():\n",
    "        print(f\"\\t{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at NYU RSS reconstruction.\n",
    "There is an RSS reconstruction in the h5 file. What is it's 'max' and 'norm'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(FPATH_NYU_H5_SAMPLE, 'r') as hf:\n",
    "    print(f\"keys in h5: {list(hf.keys())}\")\n",
    "\n",
    "    # load in the reconstruction called reconstruction_rss\n",
    "    nyu_rss = hf[\"reconstruction_rss\"][()]\n",
    "    print(f\"nyu_rss.shape = {nyu_rss.shape}\")\n",
    "    max = np.max(nyu_rss[:])\n",
    "    norm = np.linalg.norm(nyu_rss[:])\n",
    "    print(f\"max = {max}\")\n",
    "    print(f\"norm = {norm}\")\n",
    "\n",
    "    # print all attributes of the h5 file\n",
    "    attrs = dict(hf.attrs)\n",
    "    for k, v in attrs.items():\n",
    "        print(f\"\\t{k}: {v}\")\n",
    "\n",
    "    # pick a random slice index from the first dimension of the RSS reconstruction\n",
    "    sliceee = np.random.randint(0, nyu_rss.shape[0])\n",
    "    plt.imshow(np.abs(nyu_rss[sliceee, :, :]), cmap='gray')\n",
    "    plt.title(f\"Slice {sliceee} of the RSS reconstruction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the calibration data of the same patient - Try to convert it into a numpy array with a similar shape to the acquisition. (30, 20, 640, 32) for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that recursively prints all keys and values in enc.\n",
    "def print_all_keys_and_values(enc, level=0):\n",
    "    for k, v in enc.items():\n",
    "        print(f\"{' ' * level}{k}: {v}\")\n",
    "        if isinstance(v, dict):\n",
    "            print_all_keys_and_values(v, level=level+1)\n",
    "\n",
    "\n",
    "def build_calib_array_from_ismrmrd_umcg(fpath: str, DEBUG=False, verbose=True):\n",
    "\n",
    "    # get_headers_from_ismrmrd(fpath, verbose=True)\n",
    "    # return None\n",
    "\n",
    "    # Read the header and get encoding information\n",
    "    dset = ismrmrd.Dataset(fpath, create_if_needed=False)\n",
    "    header = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "    enc = header.encoding[0]\n",
    "\n",
    "    # Determine some parameters of the acquisition\n",
    "    ncoils     = header.acquisitionSystemInformation.receiverChannels\n",
    "    nslices    = enc.encodingLimits.kspace_encoding_step_1.maximum\n",
    "    firstacq   = 1 if DEBUG else get_first_acquisition(dset)\n",
    "\n",
    "    encspace_mat_x = enc.encodedSpace.matrixSize.x\n",
    "    encspace_mat_y = enc.encodedSpace.matrixSize.y\n",
    "    encspace_mat_z = enc.encodedSpace.matrixSize.z\n",
    "    recspace_mat_y = enc.reconSpace.matrixSize.y\n",
    "    recspace_mat_x = enc.reconSpace.matrixSize.x\n",
    "    recspace_mat_z = enc.reconSpace.matrixSize.z\n",
    "\n",
    "    print(f\"enc space x,y,z = {encspace_mat_x}, {encspace_mat_y}, {encspace_mat_z}\")\n",
    "    print(f\"rec space x,y,z = {recspace_mat_x}, {recspace_mat_y}, {recspace_mat_z}\")\n",
    "    print(f\"encspace_mat_y = {encspace_mat_y}\")\n",
    "    print(f\"encspace_mat_x = {encspace_mat_x}\")\n",
    "    print(f\"ncoils = {ncoils}\")\n",
    "    print(f\"nslices = {nslices}\")\n",
    "\n",
    "    # Initialize the kspace data array\n",
    "    calib = np.zeros((nslices, ncoils, encspace_mat_x, encspace_mat_y), dtype=np.complex64)\n",
    "    print(f\"Current initialized calibration shape = {calib.shape}\")\n",
    "\n",
    "    # Loop through the rest of the acquisitions and fill the data array with the kspace data\n",
    "    for acqnum in range(firstacq, dset.number_of_acquisitions()):\n",
    "        acq    = dset.read_acquisition(acqnum)\n",
    "        slice1 = acq.idx.slice\n",
    "        y      = acq.idx.kspace_encode_step_1\n",
    "\n",
    "        print(f\"acq num = {acqnum} with shape {acq.data.shape}, with slice1 = {slice1} and y = {y}\")\n",
    "\n",
    "        # each acquisition is a 2D array of shape (coil, rNx) complex\n",
    "        # kspace[slice1, :, :, y] = acq.data\n",
    "\n",
    "    # reorder the data from even and odd number of slices being interleaved to all even slices first and then all odd slices\n",
    "    # kspace = reorder_kspace_slices(kspace)\n",
    "    \n",
    "    return kspace\n",
    "\n",
    "\n",
    "kspace = build_calib_array_from_ismrmrd_umcg(FPATH_UMCG_MRD_SAMPLE_CALIB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDBOX TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_np_from_ismrmrd_umcg(fpath: str, DEBUG=False, verbose=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        - fpath: path to the .mrd file\n",
    "    \n",
    "    Returns:\n",
    "        - kspace: numpy array of kspace data in shape (navgs, nslices, ncoils, rNx, eNy + 1) complex\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(f\"Building kspace from .mrd file\")\n",
    "\n",
    "    # lets time this whole function and print it\n",
    "    start = time.time()\n",
    "\n",
    "    # Read the header and get encoding information\n",
    "    dset = ismrmrd.Dataset(fpath, create_if_needed=False)\n",
    "    header = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "    enc = header.encoding[0]\n",
    "\n",
    "    # Determine some parameters of the acquisition\n",
    "    ncoils     = header.acquisitionSystemInformation.receiverChannels\n",
    "    nslices    = enc.encodingLimits.slice.maximum + 1 if enc.encodingLimits.slice is not None else 1\n",
    "    eNy        = enc.encodedSpace.matrixSize.y\n",
    "    rNx        = enc.reconSpace.matrixSize.x\n",
    "    # eTL        = 25 if DEBUG else echo_train_length(dset)\n",
    "    # eTC        = 11 if DEBUG else echo_train_count(dset, echo_train_len=eTL)\n",
    "    firstacq   = 1 if DEBUG else get_first_acquisition(dset)\n",
    "    navgs      = 3 if DEBUG else get_num_averages(firstacq=firstacq, dset=dset)\n",
    "\n",
    "    print(f\"ncoils = {ncoils}\")\n",
    "    print(f\"nslices = {nslices}\")\n",
    "    print(f\"eNy = {eNy}\")\n",
    "    print(f\"rNx = {rNx}\")\n",
    "    # print(f\"eTL = {eTL}\")\n",
    "    # print(f\"eTC = {eTC}\")\n",
    "    print(f\"firstacq = {firstacq}\")\n",
    "    print(f\"navgs = {navgs}\")\n",
    "\n",
    "    # # Initialize the kspace data array\n",
    "    # kspace = np.zeros((navgs, nslices, ncoils, rNx, eNy + 1), dtype=np.complex64)\n",
    "\n",
    "    # # Loop through the rest of the acquisitions and fill the data array with the kspace data\n",
    "    # with tqdm(total=dset.number_of_acquisitions() - firstacq) as pbar:\n",
    "    #     for acqnum in range(firstacq, dset.number_of_acquisitions()):\n",
    "    #         acq    = dset.read_acquisition(acqnum)\n",
    "    #         slice1 = acq.idx.slice\n",
    "    #         y      = acq.idx.kspace_encode_step_1\n",
    "    #         avg    = acq._head.idx.average\n",
    "\n",
    "    #         # each acquisition is a 2D array of shape (coil, rNx) complex\n",
    "    #         kspace[avg, slice1, :, :, y] = acq.data\n",
    "    #         pbar.update(1)\n",
    "\n",
    "    # # reorder the data from even and odd number of slices being interleaved to all even slices first and then all odd slices\n",
    "    # kspace = reorder_kspace_slices(kspace)\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f\"build_np_from_ismrmrd took {time.time() - start} seconds\")\n",
    "    \n",
    "    # return kspace\n",
    "    return None\n",
    "    \n",
    "\n",
    "kspace = build_np_from_ismrmrd_umcg(FPATH_UMCG_MRD_SAMPLE, DEBUG=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the NYU calibration data in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(FPATH_NYU_H5_SAMPLE, 'r') as hf:\n",
    "    print(f\"keys in h5: {list(hf.keys())}\")\n",
    "\n",
    "    # print calibration shape\n",
    "    calib_shape = hf[\"calibration_data\"][()].shape\n",
    "    print(f\"calibration shape = {calib_shape}\")\n",
    "\n",
    "    # kspace_shape = hf[\"kspace\"][()].shape\n",
    "    # print(f\"kspace shape = {kspace_shape}\")\n",
    "    nyu_calib = hf[\"calibration_data\"][()]\n",
    "\n",
    "    # visualize the first slice 1 coil of the calibration data\n",
    "    print(nyu_calib.dtype)\n",
    "\n",
    "    calib_slice = nyu_calib[0, 0, ...]\n",
    "    print(calib_slice.shape)\n",
    "\n",
    "    # visualize this slice in grey scale with the absolute\n",
    "    plt.imshow(np.abs(calib_slice), cmap='gray')\n",
    "    plt.title(\"Calibration slice 0 coil 0\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the calib_slice with ifft\n",
    "    plt.imshow(np.abs(np.fft.ifftshift(np.fft.ifft2(np.fft.ifftshift(calib_slice)))), cmap='gray')\n",
    "    # save this on to tempdir\n",
    "    plt.savefig(Path(TEMPDIR, \"calib_slice_0_coil_0.png\"))\n",
    "\n",
    "    # visualize without all the shifts\n",
    "    plt.imshow(np.abs(np.fft.ifft2(calib_slice)), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate all the T2 TSE files into a list and tar them together for transfer to Habrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_t2_tra_paths_to_transfer_to_habrok(target_dir: Path, verbose=False) -> List[Path]:\n",
    "    pattern1 = str(Path(target_dir, \"*\", \"meas_*_T2_TSE_tra_obl-out*.mrd\"))\n",
    "    pattern2 = str(Path(target_dir, \"*\", \"meas_*_t2_tse_traobl*.mrd\"))\n",
    "\n",
    "    t2_tra_paths = case_insensitive_glob(pattern1)\n",
    "    t2_tra_paths += case_insensitive_glob(pattern2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Found {len(t2_tra_paths)} T2 tra paths\")\n",
    "        print(\"\\tT2 path: \", t2_tra_paths)\n",
    "\n",
    "    return t2_tra_paths\n",
    "\n",
    "target_dir = Path(WORKDIR, 'output', 'anon_kspaces')\n",
    "transfer_paths = get_all_t2_tra_paths_to_transfer_to_habrok(target_dir, verbose=True)\n",
    "\n",
    "fpath_tar = Path(WORKDIR, 'output', 'anon_kspaces_t2_tse_tra_1_2.tar')\n",
    "with tarfile.open(fpath_tar, \"w\") as tar:\n",
    "    for fpath in transfer_paths:\n",
    "        relative_path = os.path.relpath(fpath, start=target_dir)\n",
    "        tar.add(fpath, arcname=relative_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
