{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Learning\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The backpropagation algorithm will be implemented for neural networks and it will be applied to the task of hand-written digit recognition.\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "The backpropagation algorithm will be implemented to learn the parameters for the neural network.\n",
    "\n",
    "## Visualizing the Data\n",
    "\n",
    "Load the data and display it on a 2-dimensional plot by calling the function `displayData`.\n",
    "\n",
    "There are 5000 training examples in `ex3data1.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each of these training examples becomes a single row in the data matrix $X$. This gives a 5000 by 400 matrix $X$ where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$X=\\begin{bmatrix}\n",
    "    -(x^{(1)})^T-\\\\\n",
    "    -(x^{(2)})^T-\\\\\n",
    "    \\vdots\\\\\n",
    "    -(x^{(m)})^T-\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector $y$ that contains labels for the training set. To make things more compatible with Octave/MATLAB indexing, where there is no zero index, the digit zero has been mapped to the value ten. Therefore, a “0” digit is labeled as “10”, while the digits “1” to “9” are labeled as “1” to “9” in their natural order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = loadmat('ex3data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "def displayData(X):\n",
    "    \"\"\"\n",
    "    Select randomly 100 rows from X,\n",
    "    plot them as 20x20 pixel grayscale images,\n",
    "    and combine them to one figure with all\n",
    "    100 digits.\n",
    "    \"\"\"\n",
    "    # Create 100 subplots and remove the gaps\n",
    "    # between the subplots with gridspec.\n",
    "    fig, axarr = plt.subplots(10, 10,\n",
    "                             figsize=(6, 6),\n",
    "                             gridspec_kw={'wspace':0,\n",
    "                                          'hspace':0})\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Select random indices.\n",
    "            idx = np.random.choice(len(X), 1)\n",
    "            # Index the array X with the indices.\n",
    "            pixels = X[idx] # shape(1, 400)\n",
    "            pixels = pixels.reshape(-1, 20) # shape(20, 20)\n",
    "            axarr[i,j].imshow(pixels.T, cmap='gray_r')\n",
    "            # Remove ticks.\n",
    "            axarr[i,j].set_xticks([])\n",
    "            axarr[i,j].set_yticks([])\n",
    "            # Turn off axes.\n",
    "            axarr[i,j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "displayData(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation\n",
    "\n",
    "The neural network has 3 layers - an input layer, a hidden layer and an output layer. Recall that the inputs are pixel values of digit images. Since the images are of size $20x20$, this gives 400 input layer units (not counting the extra bias unit which always outputs +1). The training data will be loaded into the variables $X$ and $y$.\n",
    "\n",
    "A set of already trained network parameters $(\\Theta^{(1)}, \\Theta^{(2)})$ stored in `ex3weights.mat` will be loaded by `loadmat` into `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters.\n",
    "# 20x20 Input images of digits\n",
    "input_layer_size  = 400\n",
    "\n",
    "# 25 Hidden units\n",
    "hidden_layer_size = 25\n",
    "\n",
    "# 10 Labels, from 1 to 10\n",
    "# Note that \"0\" has been mapped to \"10\".\n",
    "num_labels = 10\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2.\n",
    "weights = loadmat('ex3weights.mat')\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "\n",
    "# Create a list of my Thetas.\n",
    "Thetas = [Theta1, Theta2]\n",
    "\n",
    "# Unroll parameters and then merge/concatenate.\n",
    "unrolled_Thetas = [Thetas[i].ravel() for i,_ in enumerate(Thetas)]\n",
    "nn_params = np.concatenate(unrolled_Thetas)\n",
    "\n",
    "print(\"Shape of Theta1: \", Theta1.shape)\n",
    "print(\"Shape of Theta2: \", Theta2.shape)\n",
    "print(\"Shape of nn_params: \", nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 depicts a simple neural network with 3 layers. The first input layer has three units, the input units $x_i$ and is expressed as:\n",
    "\n",
    "$$x=\\alpha^{(1)}=(\\alpha_1^{(1)},\\alpha_2^{(1)},\\alpha_3^{(1)})^T$$\n",
    "\n",
    "The second hidden layer is similarly:\n",
    "\n",
    "$$\\alpha^{(2)}=(\\alpha_1^{(2)},\\alpha_2^{(2)},\\alpha_3^{(2)})^T$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\alpha_1^{(2)}=g(\\Theta_{11}^{(1)}\\alpha_1^{(1)}+\\Theta_{12}^{(1)}\\alpha_2^{(1)}+\\Theta_{13}^{(1)}\\alpha_3^{(1)})$$\n",
    "$$\\alpha_2^{(2)}=g(\\Theta_{21}^{(1)}\\alpha_1^{(1)}+\\Theta_{22}^{(1)}\\alpha_2^{(1)}+\\Theta_{23}^{(1)}\\alpha_3^{(1)})$$\n",
    "$$\\alpha_3^{(2)}=g(\\Theta_{31}^{(1)}\\alpha_1^{(1)}+\\Theta_{32}^{(1)}\\alpha_2^{(1)}+\\Theta_{33}^{(1)}\\alpha_3^{(1)})$$\n",
    "\n",
    "and $g(z) = \\frac{1}{1 + e^{-z}}$ is the logistic function.\n",
    "\n",
    "\n",
    "The third output layer has only one unit, which returns the hypothesis function:\n",
    "\n",
    "$$h_{\\Theta}(x)=\\alpha^{(3)}=(\\alpha_1^{(3)})=g(\\Theta_{11}^{(2)}\\alpha_1^{(2)}+\\Theta_{12}^{(2)}\\alpha_2^{(2)}+\\Theta_{13}^{(2)}\\alpha_3^{(2)})$$\n",
    "\n",
    "![Figure 1](simple_nn.png \"Neural Network Model\")\n",
    "\n",
    "## Feedforward and Cost Function\n",
    "\n",
    "First the cost function and gradient for the neural network will be implemented. Create a `nnCostFunction` fuction to return the cost. Recall that the cost function for the neural network (without regularization) is\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K[-y_k^{(i)}log((h_\\theta(x^{(i)})_k)-(1-y_k^{(i)})log((1-h_\\theta (x^{(i)}))_k)]$$\n",
    "\n",
    "Note that $h_{\\theta}(x^{(i)})_k=\\alpha_k^{(3)}$ is the activation (output value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable $y$) were 1, 2, ..., 10, for the purpose of training a neural network, the labels should be recoded as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$y=\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    0\\\\\n",
    "\\end{bmatrix},\\hspace{0.5cm}y=\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    0\\\\\n",
    "\\end{bmatrix},...\\hspace{0.5cm}or\\hspace{0.5cm}\n",
    "y=\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ should be a 10-dimensional vector with $y_5=1$, and the other elements equal to 0.\n",
    "\n",
    "The feedforward computation that computes $h_{\\theta}(x^{(i)})$ should be implemented for every example $i$ and the cost over all examples should be summed. The code should also work for a dataset of any size, with any number of labels assuming that there are always at least $K≥3$ labels.\n",
    "\n",
    "** Note **\n",
    "\n",
    "The matrix $X$ contains the examples in rows (i.e., $X(i,:)$' is the i-th training example $x^{(i)}$, expressed as a $nx1$ vector). To complete the code in `nnCostFunction` function, we need to add the column of $1$'s to the $X$ matrix. The parameters for each unit in the neural network are represented in `Theta1` and `Theta2` as one row. Speciffically, the first row of Theta1 corresponds to the first hidden unit in the second layer. We can use a for-loop over the examples to compute the cost.\n",
    "\n",
    "## Regularized Cost Function\n",
    "\n",
    "For neural networks, the cost function with regularization is going to be slightly more complicated:\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K[-y_k^{(i)}log((h_\\theta(x^{(i)})_k)-(1-y_k^{(i)})log((1-h_\\theta (x^{(i)}))_k)]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}(\\Theta_{j,i}^{(1)})^2$$\n",
    "\n",
    "For the particular neural network with the 3 layers, the cost function is given by\n",
    "\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K[-y_k^{(i)}log((h_\\theta(x^{(i)})_k)-(1-y_k^{(i)})log((1-h_\\theta (x^{(i)}))_k)]+\\frac{\\lambda}{2m}[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\Theta_{j,k}^{(1)})^2+\\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\Theta_{j,k}^{(2)})^2]$$\n",
    "\n",
    "We can assume that the neural network will only have 3 layers - an input layer, a hidden layer and an output layer. However, the code can work for any number of input units, hidden units and outputs units. While we have explicitly listed the indices above for $\\Theta^{(1)}$ and $\\Theta^{(2)}$ for clarity, do note that **the code can in general work with $\\Theta^{(1)}$ and $\\Theta^{(2)}$ of any size.**\n",
    "\n",
    "Note that we should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix. Calling `nnCostFunction` using the loaded set of parameters for `Theta1` and `Theta2`, and $\\lambda=1$ we should see that the cost is about 0.383770.\n",
    "\n",
    "# Backpropagation\n",
    "\n",
    "To implement the backpropagation algorithm we have to compute the gradient for the neural network cost function. We need to complete the `nnCostFunction` so that it returns an appropriate value for grad. Once we have computed the gradient, we will be able to train the neural network by minimizing the cost function $J(\\Theta)$ using an advanced optimizer from `scipy` such as the `minimize` function.\n",
    "\n",
    "## Sigmoid Gradient\n",
    "\n",
    "Before the implementation of backpropagation, we need first to implement the sigmoid gradient function. The gradient for the sigmoid function can be computed as\n",
    "\n",
    "$$g'(z)=\\frac{d}{dz}g(z)=g(z)(1-g(z))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$sigmoid(z)=g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "When we are done, we can try testing a few values by calling `sigmoidGradient(z)`. For large values (both positive and negative) of $z$, the gradient should be close to $0$. When $z=0$, the gradient should be exactly $0.25$. The code can also work with vectors and matrices. For a matrix, the function can perform the sigmoid gradient function on every element.\n",
    "\n",
    "## Random Initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init},\\epsilon_{init}]$.\n",
    "\n",
    "We should use $\\epsilon_{init}=0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient. We need to implement a function `randInitializeWeights` that will initialize the weights for $\\Theta$.\n",
    "\n",
    "**Note** One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is\n",
    "\n",
    "$$\\epsilon_{init}=\\frac{\\sqrt{6}}{\\sqrt{L_{in}+L_{out}}}$$\n",
    "\n",
    "where $L_{in}=s_l$ and $L_{out}=s_{l}+1$ are the number of units in the layers adjacent to $\\Theta^{(l)}$.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Now, we can implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)},y^{(t)})$, we will first run a \"forward pass\" to compute all the activations throughout the network, including the output value of the hypothesis $h_{\\theta}(x)$. Then, for each node $j$ in layer $l$, we would like to compute an \"error term\" $\\delta_j^{(l)}$ that measures how much that node was \"responsible\" for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network's activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer $3$ is the output layer). For the hidden units, we will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$.\n",
    "\n",
    "In detail, the backpropagation algorithm is shown in Figure 2. To accomplish this we have to implement steps 1 to 4 in a loop that processes one example at a time. Concretely, we should implement a `for-loop` for $t=1:m$ and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)},y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "![Figure 2](backprop.png \"Backpropagation Algorithm\")\n",
    "\n",
    "1. Set the input layer's values $(a^{(1)})$ to the $t^{th}$ training example $x^{(t)}$. Perform a feedforward pass (see Figure 3), computing the activations $(z^{(2)},\\alpha^{(2)},z^{(3)}, \\alpha^{(3)})$ for layers $2$ and $3$. Note that we need to add a $+1$ term to ensure that the vectors of activations for layers $\\alpha^{(1)}$ and $\\alpha^{(2)}$ also include the bias unit. ![Figure 3](nnModel.png \"Feedforward Pass\")\n",
    "\n",
    "2. For each output unit $k$ in layer $3$ (the output layer), set $$\\delta_k^{(3)}=(\\alpha_k^{(3)}-y_k)$$ where $y_k\\in{\\{0,1\\}}$ indicates whether the current training example belongs to class $k(y_k=1)$, or if it belongs to a different class $(y_k=0)$.\n",
    "\n",
    "3. For the hidden layer $l=2$, set $$\\delta^{(2)}=(\\Theta^{(2)})^T\\delta^{(3)}.*g'(z^{(2)})$$ Intuitively, $\\delta_j^{(l)}$ is the \"error\" for $\\alpha_j^{(l)}$ (unit $j$ in layer $l$). More formally, the delta values are actually the derivative of the cost function, $\\delta_j^{(l)}=\\frac{\\partial}{\\partial{z_j^{(l)}}}cost(t)$. Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are.\n",
    "\n",
    "4. Accumulate the gradient from this example using the following formula. Note that we should skip or remove $\\delta_0^{(2)}$. $$\\Delta^{(l)}=\\Delta^{(l)}+\\delta^{(l+1)}(\\alpha^{(l)})^T$$\n",
    "\n",
    "5. Obtain the gradient for the neural network (with regularization) cost function by dividing the accumulated gradients by $\\frac{1}{m}$: $$\\frac{\\partial}{\\partial{\\Theta_{ij}^{(l)}}}J(\\Theta)=D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)}\\hspace{20pt}for\\hspace{20pt}j=0$$ $$\\frac{\\partial}{\\partial{\\Theta_{ij}^{(l)}}}J(\\Theta)=D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\frac{\\lambda}{m}\\Theta_{ij}^{(l)}\\hspace{20pt}for\\hspace{20pt}j\\geq1$$ Note that we should not be regularizing the first column of $\\Theta^{(l)}$ which is used for the bias term. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting from $1$, and $j$ is indexed starting from $0$. Thus, $$\\Theta^{(1)}=\\begin{bmatrix}\\Theta_{1,0}^{(i)}&\\Theta_{1,1}^{(l)}&\\dots\\\\\\Theta_{2,0}^{(i)}&\\Theta_{2,1}^{(l)}&\\dots\\\\\\vdots&\\vdots&\\ddots\\\\\\end{bmatrix}$$\n",
    "\n",
    "After we have implemented the backpropagation algorithm, we should implement gradient checking. The gradient checking will allow us to increase our confidence that our code is computing the gradients correctly. If the code is correct, we should expect to see a relative difference that is less than `1e-9`.\n",
    "\n",
    "### Summarizing the Algorithms\n",
    "\n",
    "In general, the vectorized algorithms to be applied are:\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "1. $\\alpha^{(1)}=x$\n",
    "2. $\\text{for }l=1,...,L-1:$\n",
    "3. $\\quad z^{(l+1)}=\\Theta^{(l)}a^{(l)}$\n",
    "4. $\\quad \\alpha^{(l+1)}=g(z^{(l+1)})$\n",
    "5. $\\text{end}$\n",
    "6. $h_{\\Theta}(x)=\\alpha^{(L)}$\n",
    "\n",
    "**Back Propagation**\n",
    "\n",
    "1. $\\delta^{(L)}=a^{(L)}-y$\n",
    "2. $\\text{for } l= L-1,...,2:$\n",
    "3. $\\quad \\delta^{(l)}=(\\Theta^{(l)})^T\\delta^{(l + 1)}.*g'(z^{(l)})$\n",
    "4. $\\text{end}$\n",
    "\n",
    "**Algorithm for the gradient of $J(\\Theta)$**\n",
    "\n",
    "1. $\\text{Set } \\Delta^{(l)}:=0, \\text{ for } l=1, ..., L-1$\n",
    "2. $\\text{For training example } t=1, ..., m:$\n",
    "3. $\\quad \\text{Set } \\alpha^{(1)}=x^{(t)}$\n",
    "4. $\\quad \\text{Perform forward propagation to compute } \\alpha^{(l)} \\text{ for } l=1,...,L$\n",
    "5. $\\quad \\text{Using } y^{(t)}, \\text{ compute } \\delta^{(L)}=\\alpha^{(L)}-y^{(t)}$\n",
    "6. $\\quad \\text{Perform back propagation computing } \\delta^{(L-1)},\\delta^{(L-2)},...,\\delta^{(2)} \\text{ using } \\delta^{(l)}=(\\Theta^{(l)})^T\\delta^{(l + 1)}.*g'(z^{(l)}) \\text{ where } g'(z^{(l)})=\\alpha^{(l)}.*(1-\\alpha^{(l)})$\n",
    "6. $\\quad \\Delta^{(l)}:=\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T \\text{ for } l=L-1, ..., 1$\n",
    "7. $\\text{end}$\n",
    "\n",
    "Hence we update our new $\\Delta$ matrix.\n",
    "\n",
    "$D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}\\text{ for }j=0$\n",
    "\n",
    "$D_{ij}^{(l)}:=\\frac{1}{m}(\\Delta_{ij}^{(l)}+\\lambda \\Theta_{ij}^{(l)})\\text{ for }j\\geq1$\n",
    "\n",
    "The capital-delta matrix $D$ is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative, $\\frac{\\partial}{\\partial{\\Theta_{ij}^{(l)}}}J(\\Theta)=D_{ij}^{(l)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic sigmoid function.\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid function.\n",
    "    Args:\n",
    "        z: float, vector, matrix\n",
    "    Returns:\n",
    "        sigmoid: float, vector, matrix\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "# Create the sigmoid gradient function.\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function\n",
    "    evaluated at z. This works regardless if z is\n",
    "    a matrix or a vector. In particular, if z is\n",
    "    a vector or matrix, it returns the gradient for\n",
    "    each element.\n",
    "    Args:\n",
    "        z: float, vector, matrix\n",
    "    Returns:\n",
    "        g_prime: float, vector, matrix\n",
    "    \"\"\"\n",
    "    g_prime = sigmoid(z) * (1 - sigmoid(z))\n",
    "    return g_prime\n",
    "\n",
    "# Create the neural network cost function.\n",
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_coef):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function for a two layer\n",
    "    neural network which performs classification.\n",
    "    Computes the cost and gradient of the neural network. The\n",
    "    parameters for the neural network are \"unrolled\" into the vector\n",
    "    nn_params and are converted back into the weight matrices.\n",
    "    The returned parameter grad is an \"unrolled\" vector of the\n",
    "    partial derivatives of the neural network.\n",
    "    Args:\n",
    "        nn_params: vector array\n",
    "        input_layer_size: int\n",
    "        hidden_layer_size: int\n",
    "        num_labels: int\n",
    "        X: array (m, input_layer_size)\n",
    "        y: array (m, num_labels)\n",
    "        lambda_coef: int\n",
    "    Returns:\n",
    "        J: float\n",
    "        grad: vector array\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2,\n",
    "    # the weight matrices for our 2 layer neural network.\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, input_layer_size + 1)) # (25,401)\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n",
    "                        (num_labels, hidden_layer_size + 1)) # (10,26)\n",
    "\n",
    "    # Get the number of training examples, m.\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Insert a 1's column for the bias unit.\n",
    "    X = np.insert(X, 0, 1, axis=1) # (5000,401)\n",
    "    \n",
    "    # Perform forward propagation to compute a(l) for l=1,...,L.\n",
    "    # z(l+1) = theta(l)a(l) and a(l+1) = g(z(l+1)).\n",
    "    z2 = np.dot(X, Theta1.T) # (5000, 25)\n",
    "    a2 = sigmoid(z2) # (5000, 25)\n",
    "    \n",
    "    # Add 1's for the bias unit.\n",
    "    a2 = np.insert(a2, 0, 1, axis=1) # (5000,26)\n",
    "    z3 = np.dot(a2, Theta2.T) # (5000, 10)\n",
    "    a3 = sigmoid(z3) # (5000, 10)\n",
    "    \n",
    "    # Create a y matrix of shape (m, K)\n",
    "    # for later use in recoding.\n",
    "    y_recoded = np.zeros((m, num_labels)) # (5000, 10)\n",
    "    \n",
    "    # Initialize Delta matrices.\n",
    "    D1 = np.zeros((hidden_layer_size, input_layer_size + 1)) # (25,401)\n",
    "    D2 = np.zeros((num_labels, hidden_layer_size + 1)) # (10,26)\n",
    "    \n",
    "    #############################################################\n",
    "    ########## Forward Propagation and Cost Computation #########\n",
    "    #############################################################\n",
    "    \n",
    "    # Initialize cost.\n",
    "    j = 0\n",
    "    # Fwd pass; for training example t = 1,...,m:\n",
    "    for t in range(m):\n",
    "        x_t = X[t]\n",
    "        \n",
    "        # Recode the categorical integer values of  y\n",
    "        # as vectors with all values set to zeros except\n",
    "        # for one value set to \"1\", which indicates whether\n",
    "        # it belongs to class k (yk = 1).\n",
    "        y_recoded[t, y[t] - 1] = 1\n",
    "            \n",
    "        # Compute cost for every training example.\n",
    "        j += np.sum(-y_recoded[t] * np.log(a3[t]) - (1 - y_recoded[t]) * np.log(1 - a3[t])) # float\n",
    "\n",
    "        ###############################################################\n",
    "        ########## Back Propagation and Gradients Computation #########\n",
    "        ###############################################################\n",
    "    \n",
    "        # Compute the error delta.\n",
    "        d_3 = a3[t] - y_recoded[t] # (10,)\n",
    "        d_3 = d_3.reshape(-1,1) # (10,1)\n",
    "        \n",
    "        # Perform back propagation.\n",
    "        # In the parameters Thetas_i,j, i are indexed\n",
    "        # starting from 1, and j is indexed starting from  0.\n",
    "        d_2 = np.dot(Theta2.T[1:,:], d_3) * sigmoidGradient(z2[t].reshape(-1,1)) # (25,10)x(10,1).*(25,1) = (25,1)\n",
    "        D1 += np.dot(d_2, x_t.reshape(-1, 1).T) # (25,401)\n",
    "        D2 += np.dot(d_3, a2[t].reshape(1,-1)) # (10,26)\n",
    "\n",
    "    # Compute total cost.\n",
    "    J = j / m # float\n",
    "    \n",
    "    # Compute the regularization term.\n",
    "    # We should not be regularizing the first column of theta,\n",
    "    # which is used for the bias term.\n",
    "    Theta1_sum = np.sum(np.square(Theta1[:,1:])) # float\n",
    "    Theta2_sum = np.sum(np.square(Theta2[:,1:])) # float\n",
    "    reg_term = Theta1_sum + Theta2_sum # float\n",
    "    \n",
    "    # Compute total cost with regularization.\n",
    "    J = J + (lambda_coef / (2 * m)) * reg_term # float\n",
    "    \n",
    "    # Update our new Delta matrices with regularization.\n",
    "    # We should not be regularizing the first column of theta,\n",
    "    # which is used for the bias term.\n",
    "    # First devide every value in Deltas with m.\n",
    "    D1 = D1 / m\n",
    "    D2 = D2 / m\n",
    "    D1[:,1:] = D1[:,1:] + (lambda_coef / m) * Theta1[:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (lambda_coef / m) * Theta2[:,1:]\n",
    "    \n",
    "    # Unroll gradients.\n",
    "    Deltas = [D1, D2]\n",
    "    unrolled_Deltas = [Deltas[i].ravel() for i,_ in enumerate(Deltas)]\n",
    "    grad = np.concatenate(unrolled_Deltas)\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "# Create the randomly weights-initialization function.\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer with L_in\n",
    "    incoming connections and L_out outgoing connections.\n",
    "    Note that the column row of W handles the \"bias\" terms.\n",
    "    Args:\n",
    "        L_in: int\n",
    "        L_out: int\n",
    "    Returns:\n",
    "        W: array (L_out, 1 + L_in)\n",
    "    \"\"\"\n",
    "    # Set the range epsilon.\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # Initialize the W matrix.\n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "    \n",
    "    # Produce the random values in the range [-epsilon, epsilon].\n",
    "    W = np.random.rand(L_out, 1 + L_in) * (2 * epsilon_init) - epsilon_init\n",
    "    return W\n",
    "\n",
    "nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "In our neural network, we are minimizing the cost function $J(\\Theta)$. To perform gradient checking on our parameters, we can imagine “unrolling” the parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ into a long vector $\\theta$. By doing so, we can think of the cost function being $J(\\theta)$ instead and use the following gradient checking procedure.\n",
    "\n",
    "Suppose we have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial{\\theta_i}}J(\\theta)$; We would like to check if $f_i$ is outputting correct derivative values.\n",
    "\n",
    "$$Let\\hspace{10pt}\\theta^{(i+)}=\\theta+\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    \\epsilon\\\\\n",
    "    \\vdots\\\\\n",
    "    0\\\\\n",
    "\\end{bmatrix}\\hspace{10pt}and\\hspace{10pt}\\theta^{(i-)}=\\theta-\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    \\vdots\\\\\n",
    "    \\epsilon\\\\\n",
    "    \\vdots\\\\\n",
    "    0\\\\\\end{bmatrix}$$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i^{th}$ element has been incremented by $\\epsilon$. Similarly, $\\theta^{(i-)}$ is the corresponding vector with the $i^{th}$ element decreased by $\\epsilon$. We can now numerically verify $f_i(\\theta)$’s correctness by checking, for each $i$, that:\n",
    "\n",
    "$$f_i(\\theta)\\approx\\frac{J(\\theta^{(i+)})-J(\\theta^{(i-)})}{2\\epsilon}$$\n",
    "\n",
    "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon=10^{−4}$, we’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
    "\n",
    "The implementation of the computation of the numerical gradient is achieved with the `computeNumericalGradient` function.\n",
    "\n",
    "Next, we can run the function `checkNNGradients` which will create a small neural network and dataset that will be used for checking our gradients. If the backpropagation implementation is correct, we should see a relative difference that is less than $10^{-9}$.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "1. When performing gradient checking, it is much more efficient to use a small neural network with a relatively small number of input units and hidden units, thus having a relatively small number of parameters. Each dimension of $\\theta$ requires two evaluations of the cost function and this can be expensive. In the function `checkNNGradients`, our code creates a small random model and dataset which is used with `computeNumericalGradient` for gradient checking. Furthermore, after we are confident that our gradient computations are correct, we should turn off gradient checking before running our learning algorithm.\n",
    "\n",
    "2. Gradient checking works for any function where we are computing the cost and the gradient. Concretely, we can use the same `computeNumericalGradient` function to check if our gradient implementations for the other cases are correct too, e.g. logistic regression’s cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "    \"\"\"\n",
    "    Initialize the weights of a layer with fan_in\n",
    "    incoming connections and fan_out outgoing\n",
    "    connections using a fix set of values. This will\n",
    "    help us in debugging.\n",
    "    Note that W should be set to a matrix of size\n",
    "    (1 + fan_in, fan_out) as the first row of W\n",
    "    handles the \"bias\" terms.\n",
    "    Args:\n",
    "        fan_in: int\n",
    "        fan_out: int\n",
    "    Returns:\n",
    "        W: array (fan_out, 1 + fan_in)\n",
    "    \"\"\"\n",
    "    # Initialize the W matrix.\n",
    "    W = np.zeros((fan_out, 1 + fan_in))\n",
    "    \n",
    "    # The \"sin\" function ensures that W is always of\n",
    "    # the same values and will be useful for debugging.\n",
    "    W = np.reshape(np.sin(range(W.size)), W.shape) / 10\n",
    "    \n",
    "    return W\n",
    "\n",
    "def computeNumericalGradient(J, theta):\n",
    "    \"\"\"\n",
    "    Computes the numerical gradient of the function J\n",
    "    around theta using \"finite differences\" and gives\n",
    "    a numerical estimate of the gradient.\n",
    "    Notes: The following code implements numerical\n",
    "           gradient checking, and returns the numerical\n",
    "           gradient. It sets numgrad(i) to (a numerical \n",
    "           approximation of) the partial derivative of J\n",
    "           with respect to the i-th input argument,\n",
    "           evaluated at theta. (i.e., numgrad(i) should\n",
    "           be the (approximately) the partial derivative\n",
    "           of J with respect to theta(i).)\n",
    "    Args:\n",
    "        J: function\n",
    "        theta: vector array\n",
    "    Returns:\n",
    "        numgrad: vector array\n",
    "    \"\"\"\n",
    "    # Initialize parameters.\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    e = 1e-4\n",
    "\n",
    "    for p in range(theta.size):\n",
    "        # Set the perturbation vector.\n",
    "        perturb.reshape(perturb.size)[p] = e\n",
    "        loss1, _ = J(theta - perturb)\n",
    "        loss2, _ = J(theta + perturb)\n",
    "        # Compute the Numerical Gradient.\n",
    "        numgrad.reshape(numgrad.size)[p] = (loss2 - loss1) / (2 * e)\n",
    "        perturb.reshape(perturb.size)[p] = 0\n",
    "        \n",
    "    return numgrad\n",
    "\n",
    "def checkNNGradients(lambda_coef):\n",
    "    \"\"\"\n",
    "    Creates a small neural network to check the\n",
    "    backpropagation gradients. It will output the\n",
    "    analytical gradients produced by the backprop\n",
    "    code and the numerical gradients (computed\n",
    "    using computeNumericalGradient). These two\n",
    "    gradient computations should result in very\n",
    "    similar values.\n",
    "    Args:\n",
    "        lambda_coef: int\n",
    "    \"\"\"\n",
    "    # Initialize a small network.\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "\n",
    "    # Generate some 'random' test data.\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "    \n",
    "    # Reuse debugInitializeWeights to generate X.\n",
    "    X  = debugInitializeWeights(m, input_layer_size - 1)\n",
    "    # np.mod returns element-wise remainder of division.\n",
    "    y  = 1 + np.mod(range(m), num_labels).T\n",
    "\n",
    "    # Unroll parameters.\n",
    "    Thetas = [Theta1, Theta2]\n",
    "    unrolled_Thetas = [Thetas[i].ravel() for i,_ in enumerate(Thetas)]\n",
    "    nn_params = np.concatenate(unrolled_Thetas)\n",
    "\n",
    "    # Create short hand for cost function.\n",
    "    def costFunc(p):\n",
    "        return nnCostFunction(p, input_layer_size, hidden_layer_size,\n",
    "                              num_labels, X, y, lambda_coef)\n",
    "\n",
    "    _, grad = costFunc(nn_params)\n",
    "    numgrad = computeNumericalGradient(costFunc, nn_params)\n",
    "\n",
    "    # Visually examine the two gradient computations.\n",
    "    # The two columns should be very similar. \n",
    "    for numerical, analytical in zip(numgrad, grad):\n",
    "        print('Numerical Gradient: {0:10f}, Analytical Gradient {1:10f}'.format(numerical, analytical))\n",
    "    print('\\nThe above two columns should be very similar.')\n",
    "\n",
    "    # Evaluate the norm of the difference between two solutions.  \n",
    "    # If we have a correct implementation, and assuming we used e = 0.0001 \n",
    "    # in computeNumericalGradient, then diff below should be less than 1e-9.\n",
    "    diff = np.linalg.norm(numgrad-grad) / np.linalg.norm(numgrad+grad)\n",
    "\n",
    "    print('If the backpropagation implementation is correct, then \\n' \\\n",
    "             'the relative difference will be small (less than 1e-9). \\n' \\\n",
    "             '\\nRelative Difference: {:.10E}'.format(diff))\n",
    "\n",
    "checkNNGradients(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Parameters\n",
    "\n",
    "After we have successfully implemented the neural network cost function and gradient computation, the next step is to use an optimization function such as `minimize` to learn a good set parameters.\n",
    "\n",
    "After the training completes, we can proceed to report the training accuracy of our classifier by computing the percentage of examples it got correct. If our implementation is correct, we should see a reported training accuracy of about $95\\%$ (this may vary by about $1%$ due to the random initialization). It is possible to get higher training accuracies by training the neural network for more iterations. We can try training the neural network for more iterations (e.g., set `MaxIter` to `400`) and also vary the regularization parameter $\\lambda$. With the right learning settings, it is possible to get the neural network to perfectly fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Create a cost function optimizer.\n",
    "def optimizeCost(maxiter, lambda_coef):\n",
    "    \"\"\"\n",
    "    Optimizes the Cost Function.\n",
    "    Args:\n",
    "        maxiter: int\n",
    "        lambda_coef: float\n",
    "    Returns:\n",
    "        thetas: vector array\n",
    "    \"\"\"\n",
    "    ##########################################\n",
    "    ########## Initialize nn_params ##########\n",
    "    ##########################################\n",
    "    Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "    Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "    # Unroll parameters.\n",
    "    Thetas = [Theta1, Theta2]\n",
    "    unrolled_Thetas = [Thetas[i].ravel() for i,_ in enumerate(Thetas)]\n",
    "    nn_params = np.concatenate(unrolled_Thetas)\n",
    "    \n",
    "    ##########################################\n",
    "    ##########       Optimize       ##########\n",
    "    ##########################################\n",
    "    # Optimize the cost function.\n",
    "    results = minimize(fun=nnCostFunction,\n",
    "                       x0=nn_params,\n",
    "                       args=(input_layer_size,\n",
    "                             hidden_layer_size,\n",
    "                             num_labels,\n",
    "                             X, y, lambda_coef),\n",
    "                       method='CG',\n",
    "                       jac=True,\n",
    "                       options={'maxiter':maxiter, 'disp': True})\n",
    "    thetas = results.x\n",
    "\n",
    "    return thetas\n",
    "\n",
    "# Train the neural network for 40 iterations.\n",
    "# Set regularization parameter to 1.\n",
    "nn_params = optimizeCost(40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prediction function\n",
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"\n",
    "    Predicts the label of an input X\n",
    "    given the trained weights of a\n",
    "    neural network (Theta1, Theta2).\n",
    "    Args:\n",
    "        Theta1: array (hidden_layer_size, 1 + input_layer_size)\n",
    "        Theta2: array (num_labels, 1 + hidden_layer_size)\n",
    "        X: array (m, input_layer_size)\n",
    "    Returns:\n",
    "        p_argmax: vector array (m,)\n",
    "    \"\"\"\n",
    "    # Get some useful values.\n",
    "    m = len(X)\n",
    "    num_labels = len(Theta2)\n",
    "    \n",
    "    # Initialize prediction p.\n",
    "    p = np.zeros((m, 1))\n",
    "    \n",
    "    #Insert a 1's column to X.\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    a2 = sigmoid(np.dot(X, Theta1.T))\n",
    "    #Insert a 1's column to a2.\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)\n",
    "    h_theta = sigmoid(np.dot(a2, Theta2.T))\n",
    "    p_argmax = np.argmax(h_theta, axis=1)\n",
    "    # Add 1 to fix for Python's zero indexed array.\n",
    "    p_argmax = p_argmax + 1\n",
    "    \n",
    "    return p_argmax\n",
    "\n",
    "# Create a function to output the accuracy score.\n",
    "def outputAccuracy(nn_params):\n",
    "    \"\"\"\n",
    "    Outputs the accuracy of the\n",
    "    trained neural network.\n",
    "    Args:\n",
    "        nn_params: vector array\n",
    "    \"\"\"\n",
    "    # After the training of the neural network, reshape\n",
    "    # nn_params back into the parameters Theta1 and Theta2,\n",
    "    # the weight matrices for our 2 layer neural network.\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, input_layer_size + 1))\n",
    "    Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n",
    "                        (num_labels, hidden_layer_size + 1))\n",
    "\n",
    "    predictions = predict(Theta1, Theta2, X)\n",
    "    print('\\nTraining Set Accuracy: {0:.2f}%'.format(100 * np.mean(predictions == y.reshape(-1))))\n",
    "\n",
    "outputAccuracy(nn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Hidden Layer\n",
    "\n",
    "One way to understand what our neural network is learning is to visualize what the representations captured by the hidden units. Informally, given a particular hidden unit, one way to visualize what it computes is to find an input $x$ that will cause it to activate (that is, to have an activation value $(\\alpha^{(l)})$ close to $1$). For the neural network we trained, notice that the $i^{th}$ row of $\\Theta^{(1)}$ is a 401-dimensional vector that represents the parameter for the $i^{th}$ hidden unit. If we discard the bias term, we get a 400-dimensional vector that represents the weights from each input pixel to the hidden unit.\n",
    "\n",
    "Thus, one way to visualize the “representation” captured by the hidden unit is to reshape this 400-dimensional vector into a $20x20$ image and display it. This can be done by using the `displayData` function, which will show an image with 25 units, each corresponding to one hidden unit in the network.\n",
    "\n",
    "In the trained network, it can be found that the hidden units corresponds roughly to detectors that look for strokes and other patterns in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayData(Theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play\n",
    "\n",
    "We can try out different learning settings for the neural network to see how the performance of the neural network varies with the regularization parameter $\\lambda$ and number of training steps (the `MaxIter` option when using `minimize`).\n",
    "\n",
    "Neural networks are very powerful models that can form highly complex decision boundaries. Without regularization, it is possible for a neural network to “overfit” a training set so that it obtains close to $100\\%$ accuracy on the training set but does not as well on new examples that it has not seen before. The regularization $\\lambda$ can be set to a smaller value and the `MaxIter` parameter to a higher number of iterations to see this.\n",
    "\n",
    "The changes in the visualizations of the hidden units can also be depicted when the learning parameters $\\lambda$ and `MaxIter` are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different cases of the learning parameters, lambda_coef and MaxIter.\n",
    "cases = {\"MaxIter = 10 and lambda_coef = 0.5\": (10, 0.5),\n",
    "         \"MaxIter = 10 and lambda_coef = 100\": (10, 100),\n",
    "         \"MaxIter = 10 and lambda_coef = 1000\": (10, 1000),\n",
    "         \"MaxIter = 30 and lambda_coef = 1\": (30, 1),\n",
    "         \"MaxIter = 200 and lambda_coef = 0\": (200, 0)}\n",
    "\n",
    "for k, v in cases.items():\n",
    "    print(60 * \"-\")\n",
    "    print()\n",
    "    print(k)\n",
    "    nn_params = optimizeCost(v[0], v[1])\n",
    "    outputAccuracy(nn_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
