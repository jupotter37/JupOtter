{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Benchmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zT0PYJ4K3lKF",
        "fd1BrdEi-ZCQ",
        "eyazRidLmnd9",
        "LoW5saMzfhmI",
        "hQ5FlBLEfjgK",
        "F2WkFxv1mkxp",
        "KNr0CYDgmrIa",
        "uoaqU4ynmtyU",
        "6KjDLyz1FpCr",
        "IVsbS5ixTu5i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "987e4c185af14779b1a9688d0a2bc125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e2503ec1deb4e4aa41c4ced1922cf67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe1291603da348dfb45c33e38df4bee1",
              "IPY_MODEL_bbac9cefac25486aae543a7dbab69e7a"
            ]
          }
        },
        "6e2503ec1deb4e4aa41c4ced1922cf67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe1291603da348dfb45c33e38df4bee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cff4f91a8e9d4a0290d5b8413a7ca215",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41cc1e1f89bd43159fe6af86d61b2065"
          }
        },
        "bbac9cefac25486aae543a7dbab69e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cc3aa620c7a4fe7be6780d42da4292f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 553/553 [00:00&lt;00:00, 13.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7b14d3fe8624cf1baca5b6cce05fb2a"
          }
        },
        "cff4f91a8e9d4a0290d5b8413a7ca215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41cc1e1f89bd43159fe6af86d61b2065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cc3aa620c7a4fe7be6780d42da4292f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7b14d3fe8624cf1baca5b6cce05fb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b1daa86aed74d6d957471140444a119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e749165993c147bbb159907d10936756",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8946a8add6fa455c98dfe441f1568e8e",
              "IPY_MODEL_7c860a4019c948a5acd0348165c26b00"
            ]
          }
        },
        "e749165993c147bbb159907d10936756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8946a8add6fa455c98dfe441f1568e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1d2321173c834262a49bff2fa9589c8e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b501173251246dfb0c3bce722d44c72"
          }
        },
        "7c860a4019c948a5acd0348165c26b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79c8b5ef4d4c48218daa456fdb6a2f0f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:00&lt;00:00, 17.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30eab6894b3844b7ac7a996073f0dae5"
          }
        },
        "1d2321173c834262a49bff2fa9589c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b501173251246dfb0c3bce722d44c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "79c8b5ef4d4c48218daa456fdb6a2f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30eab6894b3844b7ac7a996073f0dae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT0PYJ4K3lKF"
      },
      "source": [
        "## Framework evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd1BrdEi-ZCQ"
      },
      "source": [
        "##### Presets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbPu8bdYffBO",
        "outputId": "5071215a-a8aa-48e9-cc4c-4d02742debd2"
      },
      "source": [
        "!pip install transformers==3.0.0\n",
        "!pip install py3nvml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\r\u001b[K     |▍                               | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 30.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 34.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 23.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 24.8MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 25.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 26.8MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 27.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 28.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 174kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 184kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 194kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 204kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 215kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 245kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 256kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 266kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 276kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 286kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 296kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 317kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 337kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 348kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 358kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 368kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 389kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 399kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 409kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 419kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 430kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 440kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 460kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 471kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 481kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 491kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 501kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 512kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 522kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 532kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 542kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 552kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 563kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 573kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 583kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 593kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 604kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 614kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 624kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 634kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 645kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 655kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 665kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 675kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 686kB 28.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 696kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 706kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 716kB 28.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 727kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 737kB 28.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 747kB 28.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 757kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.0rc4 transformers-3.0.0\n",
            "Collecting py3nvml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/b3/cb30dd8cc1198ae3fdb5a320ca7986d7ca76e23d16415067eafebff8685f/py3nvml-0.2.6-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.3MB/s \n",
            "\u001b[?25hCollecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Installing collected packages: xmltodict, py3nvml\n",
            "Successfully installed py3nvml-0.2.6 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyazRidLmnd9"
      },
      "source": [
        "### CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoW5saMzfhmI"
      },
      "source": [
        "#### TFBenchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ofKsWgHfhUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "987e4c185af14779b1a9688d0a2bc125",
            "6e2503ec1deb4e4aa41c4ced1922cf67",
            "fe1291603da348dfb45c33e38df4bee1",
            "bbac9cefac25486aae543a7dbab69e7a",
            "cff4f91a8e9d4a0290d5b8413a7ca215",
            "41cc1e1f89bd43159fe6af86d61b2065",
            "4cc3aa620c7a4fe7be6780d42da4292f",
            "a7b14d3fe8624cf1baca5b6cce05fb2a"
          ]
        },
        "outputId": "4da6c5da-684a-48c5-c9ff-df496413c4cc"
      },
      "source": [
        "from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\n",
        "\n",
        "args = TensorFlowBenchmarkArguments(models=[\"bert-base-multilingual-cased\"], batch_sizes=[1, 8], sequence_lengths=[64, 128])\n",
        "benchmark = TensorFlowBenchmark(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "987e4c185af14779b1a9688d0a2bc125",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=553.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-394035a22b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlowBenchmarkArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhf_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlowBenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, configs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             self.config_dict = {\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             }\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/benchmark/benchmark_utils.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             self.config_dict = {\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             }\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mt5'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLw3ABXLxIkZ"
      },
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ0UxaQUfopa",
        "outputId": "f2211ba9-d3cc-4c2f-f897-fe70c6b86a3f"
      },
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64            0.309     \n",
            " bert-base-multilingual-cased        1              128            0.432     \n",
            " bert-base-multilingual-cased        8               64            1.355     \n",
            " bert-base-multilingual-cased        8              128            2.413     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64             1417     \n",
            " bert-base-multilingual-cased        1              128             1416     \n",
            " bert-base-multilingual-cased        8               64             1417     \n",
            " bert-base-multilingual-cased        8              128             1417     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================        ENVIRONMENT INFORMATION         ====================\n",
            "- transformers_version: 3.0.0\n",
            "- framework: TensorFlow\n",
            "- eager_mode: False\n",
            "- use_xla: False\n",
            "- framework_version: 2.5.0\n",
            "- python_version: 3.7.10\n",
            "- system: Linux\n",
            "- cpu: x86_64\n",
            "- architecture: 64bit\n",
            "- date: 2021-06-30\n",
            "- time: 11:01:12.952021\n",
            "- fp16: False\n",
            "- use_multiprocessing: True\n",
            "- only_pretrain_model: False\n",
            "- cpu_ram_mb: 12993\n",
            "- use_gpu: False\n",
            "- use_tpu: False\n",
            "\n",
            "BenchmarkOutput(time_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 0.3087412803999996, 128: 0.43194536329999667}, 8: {64: 1.3550332535000051, 128: 2.41315752349999}}}}, memory_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 1417, 128: 1416}, 8: {64: 1417, 128: 1417}}}}, time_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, memory_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, inference_summary=None, train_summary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ5FlBLEfjgK"
      },
      "source": [
        "#### TorchBenchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J02hetvCHaLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52eed1d5-819d-4b9f-bad1-00dc8b93fa3a"
      },
      "source": [
        "!pip show requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: requests\n",
            "Version: 2.23.0\n",
            "Summary: Python HTTP for Humans.\n",
            "Home-page: https://requests.readthedocs.io\n",
            "Author: Kenneth Reitz\n",
            "Author-email: me@kennethreitz.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: certifi, idna, urllib3, chardet\n",
            "Required-by: tweepy, torchtext, tensorflow-datasets, tensorboard, Sphinx, spacy, requests-oauthlib, pymystem3, pooch, panel, pandas-datareader, kaggle, gspread, google-colab, google-api-core, gdown, folium, fix-yahoo-finance, fastai, coveralls, community, CacheControl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpPoRp1kflvz"
      },
      "source": [
        "from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments\n",
        "\n",
        "args = PyTorchBenchmarkArguments(models=[\"bert-base-multilingual-cased\"], batch_sizes=[1, 8], sequence_lengths=[64, 128])\n",
        "benchmark = PyTorchBenchmark(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqgaGA5bftBG",
        "outputId": "79eda22f-dc6b-4f89-9d74-4084db3ba420"
      },
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64            0.188     \n",
            " bert-base-multilingual-cased        1              128            0.336     \n",
            " bert-base-multilingual-cased        8               64            1.033     \n",
            " bert-base-multilingual-cased        8              128            2.068     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64             1008     \n",
            " bert-base-multilingual-cased        1              128             1010     \n",
            " bert-base-multilingual-cased        8               64             1040     \n",
            " bert-base-multilingual-cased        8              128             1066     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================        ENVIRONMENT INFORMATION         ====================\n",
            "- transformers_version: 3.0.0\n",
            "- framework: PyTorch\n",
            "- use_torchscript: False\n",
            "- framework_version: 1.9.0+cu102\n",
            "- python_version: 3.7.10\n",
            "- system: Linux\n",
            "- cpu: x86_64\n",
            "- architecture: 64bit\n",
            "- date: 2021-06-30\n",
            "- time: 10:53:49.159025\n",
            "- fp16: False\n",
            "- use_multiprocessing: True\n",
            "- only_pretrain_model: False\n",
            "- cpu_ram_mb: 12993\n",
            "- use_gpu: False\n",
            "- use_tpu: False\n",
            "\n",
            "BenchmarkOutput(time_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 0.18808466540000382, 128: 0.3355523851999919}, 8: {64: 1.0329195015999972, 128: 2.067962154999998}}}}, memory_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 1008, 128: 1010}, 8: {64: 1040, 128: 1066}}}}, time_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, memory_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, inference_summary=None, train_summary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2WkFxv1mkxp"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNr0CYDgmrIa"
      },
      "source": [
        "#### TFBenchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5b1daa86aed74d6d957471140444a119",
            "e749165993c147bbb159907d10936756",
            "8946a8add6fa455c98dfe441f1568e8e",
            "7c860a4019c948a5acd0348165c26b00",
            "1d2321173c834262a49bff2fa9589c8e",
            "2b501173251246dfb0c3bce722d44c72",
            "79c8b5ef4d4c48218daa456fdb6a2f0f",
            "30eab6894b3844b7ac7a996073f0dae5"
          ]
        },
        "id": "3eiu5qmAgC4H",
        "outputId": "3ec26acd-38e5-4aef-b5c9-08fe917ce933"
      },
      "source": [
        "from transformers import TensorFlowBenchmark, TensorFlowBenchmarkArguments\n",
        "\n",
        "args = TensorFlowBenchmarkArguments(models=[\"bert-base-multilingual-cased\"], batch_sizes=[1, 8], sequence_lengths=[64, 128])\n",
        "benchmark = TensorFlowBenchmark(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b1daa86aed74d6d957471140444a119",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnJqhmP2mg8B",
        "outputId": "23751e83-ed0d-466f-c97a-73bb027d12eb"
      },
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64            0.006     \n",
            " bert-base-multilingual-cased        1              128            0.009     \n",
            " bert-base-multilingual-cased        8               64             0.03     \n",
            " bert-base-multilingual-cased        8              128            0.062     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64             2430     \n",
            " bert-base-multilingual-cased        1              128             2430     \n",
            " bert-base-multilingual-cased        8               64             2430     \n",
            " bert-base-multilingual-cased        8              128             2430     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================        ENVIRONMENT INFORMATION         ====================\n",
            "- transformers_version: 3.0.0\n",
            "- framework: TensorFlow\n",
            "- eager_mode: False\n",
            "- use_xla: False\n",
            "- framework_version: 2.5.0\n",
            "- python_version: 3.7.10\n",
            "- system: Linux\n",
            "- cpu: x86_64\n",
            "- architecture: 64bit\n",
            "- date: 2021-06-30\n",
            "- time: 11:08:19.740340\n",
            "- fp16: False\n",
            "- use_multiprocessing: True\n",
            "- only_pretrain_model: False\n",
            "- cpu_ram_mb: 12993\n",
            "- use_gpu: True\n",
            "- num_gpus: 1\n",
            "- gpu: Tesla T4\n",
            "- gpu_ram_mb: 15109\n",
            "- gpu_power_watts: 70.0\n",
            "- gpu_performance_state: 0\n",
            "- use_tpu: False\n",
            "\n",
            "BenchmarkOutput(time_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 0.006450660500000538, 128: 0.008904085700000054}, 8: {64: 0.029738316099999905, 128: 0.06169962150000004}}}}, memory_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 2430, 128: 2430}, 8: {64: 2430, 128: 2430}}}}, time_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, memory_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, inference_summary=None, train_summary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoaqU4ynmtyU"
      },
      "source": [
        "#### TorchBenchmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgzow5Qdmim1"
      },
      "source": [
        "from transformers import PyTorchBenchmark, PyTorchBenchmarkArguments\n",
        "\n",
        "args = PyTorchBenchmarkArguments(models=[\"bert-base-multilingual-cased\"], batch_sizes=[1, 8], sequence_lengths=[64, 128])\n",
        "benchmark = PyTorchBenchmark(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRRl2FN0m0hW",
        "outputId": "4b3c16c5-56c4-40b3-fe26-0606525041b6"
      },
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64            0.146     \n",
            " bert-base-multilingual-cased        1              128            0.249     \n",
            " bert-base-multilingual-cased        8               64            0.796     \n",
            " bert-base-multilingual-cased        8              128            1.601     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            " bert-base-multilingual-cased        1               64             1022     \n",
            " bert-base-multilingual-cased        1              128             1025     \n",
            " bert-base-multilingual-cased        8               64             1055     \n",
            " bert-base-multilingual-cased        8              128             1081     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================        ENVIRONMENT INFORMATION         ====================\n",
            "- transformers_version: 3.0.0\n",
            "- framework: PyTorch\n",
            "- use_torchscript: False\n",
            "- framework_version: 1.9.0+cu102\n",
            "- python_version: 3.7.10\n",
            "- system: Linux\n",
            "- cpu: x86_64\n",
            "- architecture: 64bit\n",
            "- date: 2021-06-30\n",
            "- time: 11:10:34.908350\n",
            "- fp16: False\n",
            "- use_multiprocessing: True\n",
            "- only_pretrain_model: False\n",
            "- cpu_ram_mb: 12993\n",
            "- use_gpu: True\n",
            "- num_gpus: 1\n",
            "- gpu: Tesla T4\n",
            "- gpu_ram_mb: 15109\n",
            "- gpu_power_watts: 70.0\n",
            "- gpu_performance_state: 8\n",
            "- use_tpu: False\n",
            "\n",
            "BenchmarkOutput(time_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 0.14601964910000048, 128: 0.24878431610000007}, 8: {64: 0.7958174344000014, 128: 1.6005236703999999}}}}, memory_inference_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {64: 1022, 128: 1025}, 8: {64: 1055, 128: 1081}}}}, time_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, memory_train_result={'bert-base-multilingual-cased': {'bs': [1, 8], 'ss': [64, 128], 'result': {1: {}, 8: {}}}}, inference_summary=None, train_summary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj1EXuNjm2F5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWhCWC0vlxiA"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "These are just examples, refer to https://github.com/Muennighoff/hf-translation-benchmark for the full code, results & instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KjDLyz1FpCr"
      },
      "source": [
        "### hf-translaton-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Z1Zg-AcdAX"
      },
      "source": [
        "!python main.py --sample 100 --model m2m --weights facebook/m2m100_418M --data gdp-top10 --out m2m100418-gdp.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4qmgHbjFJkL"
      },
      "source": [
        "!python main.py --sample 100 --model m2m --weights facebook/m2m100_1200M --data gdp-top10 --out m2m1001200-gdp.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_i6jblCFytf"
      },
      "source": [
        "!python main.py --sample 100 --model mbart --weights facebook/mbart-large-50-many-to-many-mmt --data gdp-top10 --out mbart-gdp.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc149FkCF4-9"
      },
      "source": [
        "!python main.py --sample 100 --model marianmt --weights Helsinki-NLP/opus-mt-en-zh --data en-zh --out m2m100418-gdp.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga2d186TFmrG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVsbS5ixTu5i"
      },
      "source": [
        "### GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY6adiibmMJt",
        "outputId": "c62a37a4-441b-469b-df7c-50fd15f3d7fe"
      },
      "source": [
        "!git clone https://github.com/Muennighoff/lm-evaluation-harness.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lm-evaluation-harness'...\n",
            "remote: Enumerating objects: 4285, done.\u001b[K\n",
            "remote: Counting objects: 100% (1738/1738), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1143/1143), done.\u001b[K\n",
            "remote: Total 4285 (delta 749), reused 1507 (delta 583), pack-reused 2547\u001b[K\n",
            "Receiving objects: 100% (4285/4285), 8.35 MiB | 15.61 MiB/s, done.\n",
            "Resolving deltas: 100% (2573/2573), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG4W1P1lRS4S",
        "outputId": "4030d737-b3b4-4a5f-e292-8c3516f31342"
      },
      "source": [
        "import logging\n",
        "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
        "# or \n",
        "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
        "\n",
        "!python main.py \\\n",
        "\t--model gpt2 \\\n",
        "\t--device cuda:0 \\\n",
        "\t--tasks wmt20-zh-en \\\n",
        "\t--num_fewshot 5 \\\n",
        "  --limit 500 \\\n",
        "  --output_path results.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-12 15:41:57.356120: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
            "Running greedy_until requests\n",
            "0it [00:00, ?it/s]\n",
            "{\n",
            "  \"results\": {\n",
            "    \"wmt14-en-fr\": {\n",
            "      \"bleu\": 0.17714518492970446,\n",
            "      \"chrf\": 0.06972214684754112,\n",
            "      \"ter\": 2.431654676258993\n",
            "    }\n",
            "  },\n",
            "  \"versions\": {\n",
            "    \"wmt14-en-fr\": 0\n",
            "  }\n",
            "}\n",
            "gpt2 (), limit: 5, provide_description: False, num_fewshot: 0, batch_size: None\n",
            "|   Task    |Version|Metric|Value |   |Stderr|\n",
            "|-----------|------:|------|-----:|---|------|\n",
            "|wmt14-en-fr|      0|bleu  |0.1771|   |      |\n",
            "|           |       |chrf  |0.0697|   |      |\n",
            "|           |       |ter   |2.4317|   |      |\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNRrd68jI6xG"
      },
      "source": [
        "### huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RECZZoJ3I7-a",
        "outputId": "e1afa2f5-6b37-436d-8c73-9f4b94f0d2de"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip install -e ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 78287, done.\u001b[K\n",
            "remote: Counting objects: 100% (1328/1328), done.\u001b[K\n",
            "remote: Compressing objects: 100% (617/617), done.\u001b[K\n",
            "remote: Total 78287 (delta 812), reused 1041 (delta 648), pack-reused 76959\u001b[K\n",
            "Receiving objects: 100% (78287/78287), 61.34 MiB | 24.08 MiB/s, done.\n",
            "Resolving deltas: 100% (55770/55770), done.\n",
            "/content/transformers\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (4.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0.dev0) (2021.5.30)\n",
            "Installing collected packages: tokenizers, pyyaml, sacremoses, huggingface-hub, transformers\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7TyouvJOOz",
        "outputId": "d11f9a9a-3809-4ec4-a52c-a01295ff4735"
      },
      "source": [
        "!pip install -q sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 26.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.9MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 18.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 9.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBJgXl6EJrIu",
        "outputId": "f442ce22-cbd4-49f4-b890-d8c3a1d55ca5"
      },
      "source": [
        "!pip install -q datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 266kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 16.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8IRj5sNKiwI",
        "outputId": "6dd42489-6e15-44a6-afa9-50d332b09857"
      },
      "source": [
        "!pip install -q sacrebleu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██████                          | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 18.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TiZZk4azmY2",
        "outputId": "c537efc8-a0c5-4bde-d222-86f4ecf18ea6"
      },
      "source": [
        "!python examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \\\n",
        "    --do_eval \\\n",
        "    --source_lang en \\\n",
        "    --target_lang ro \\\n",
        "    --dataset_name wmt16 \\\n",
        "    --dataset_config_name ro-en \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate \\\n",
        "    --max_eval_samples 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-15 15:37:43.915737: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 15:37:45 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "07/15/2021 15:37:45 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-translation/runs/Jul15_15-37-45_4b75c380b5b7,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-translation,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.py\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/dataset_infos.json\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.json\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found local import file from /root/.cache/huggingface/datasets/downloads/11e8085b332f0f35c8271b01b4e3af50448c111c8a226effa391bdeef0c08ece.fee4db31f6778f75af365027007023104d0d90ceeb53eaf0fade478dba9105a1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt_utils.py\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.py\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/dataset_infos.json\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.json\n",
            "07/15/2021 15:37:46 - INFO - datasets.load - Found local import file from /root/.cache/huggingface/datasets/downloads/11e8085b332f0f35c8271b01b4e3af50448c111c8a226effa391bdeef0c08ece.fee4db31f6778f75af365027007023104d0d90ceeb53eaf0fade478dba9105a1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt_utils.py\n",
            "07/15/2021 15:37:46 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 15:37:46 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/15/2021 15:37:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 15:37:46 - WARNING - datasets.builder - Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n",
            "07/15/2021 15:37:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "100% 3/3 [00:00<00:00, 526.97it/s]\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 15:37:47,134 >> loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 15:37:47,135 >> Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 15:37:47,734 >> loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 15:37:47,735 >> Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,842 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/source.spm from cache at /root/.cache/huggingface/transformers/1954839338efdfd7d7cd66bb676626f3fa5404a96e4c3fd9e0d777f08fb2f7fb.2a52a89183278824d0afa78f92263dc46ca0c520a60df7071b6ae4e00fc5bef7\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/target.spm from cache at /root/.cache/huggingface/transformers/1e4b193c82cb1b7fe168ad6685f3a053a2b1ab3482d2d12c306282e03ff379f0.afd22f83f2170348e28be1f817974819f568155c58acd1bb3800cd7937db36a1\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/bffa5c5398d82c200e0eeee10da8e37228615dc7fbabd429567d6ae242f5a615.5e73795abff6c04c6739872ed86c111983aa47f92d9648ac33f2d9d3643e1357\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e05a8eb93de5526dd8224c840b38615d3f79b0bf9636f74ade2807ec16ae347f.09f8e480ed5ba39c95bf16472d22478f2d1870f222433394dd0fee0ac9f1b907\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 15:37:49,843 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1270] 2021-07-15 15:37:50,298 >> loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ddea6571c8d435d3a95a0d3b003ce4ed9cd74380314ce1510adc1a2b80ec6541.ea59afa9489e25f746dc0446b0ef6874b3b9e704d9d077620684ec467f86be9d\n",
            "[INFO|modeling_utils.py:1509] 2021-07-15 15:37:52,846 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1518] 2021-07-15 15:37:52,846 >> All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-ro.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]07/15/2021 15:37:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-fd6c91da54d69fc9.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00, 308.75ba/s]\n",
            "07/15/2021 15:37:54 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu\n",
            "07/15/2021 15:37:54 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466\n",
            "07/15/2021 15:37:54 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py to /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.py\n",
            "07/15/2021 15:37:54 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/dataset_infos.json\n",
            "07/15/2021 15:37:54 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.json\n",
            "07/15/2021 15:37:54 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2163] 2021-07-15 15:37:54,040 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2165] 2021-07-15 15:37:54,040 >>   Num examples = 1\n",
            "[INFO|trainer.py:2168] 2021-07-15 15:37:54,040 >>   Batch size = 4\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "100% 1/1 [00:00<00:00, 89.19it/s]\n",
            "***** eval metrics *****\n",
            "  eval_bleu               =     4.5152\n",
            "  eval_gen_len            =       16.0\n",
            "  eval_loss               =      1.787\n",
            "  eval_runtime            = 0:00:00.98\n",
            "  eval_samples            =          1\n",
            "  eval_samples_per_second =      1.019\n",
            "  eval_steps_per_second   =      1.019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK671D68JUrS",
        "outputId": "09ae3f0e-caa1-436e-cf9f-dda4cb7a8245"
      },
      "source": [
        "!python examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \\\n",
        "    --do_eval \\\n",
        "    --source_lang en \\\n",
        "    --target_lang ro \\\n",
        "    --dataset_name wmt16 \\\n",
        "    --dataset_config_name ro-en \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-15 12:35:42.097423: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 12:35:44 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "07/15/2021 12:35:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-translation/runs/Jul15_12-35-44_92a5f4675b6a,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-translation,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.py\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/dataset_infos.json\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.json\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found local import file from /root/.cache/huggingface/datasets/downloads/11e8085b332f0f35c8271b01b4e3af50448c111c8a226effa391bdeef0c08ece.fee4db31f6778f75af365027007023104d0d90ceeb53eaf0fade478dba9105a1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt_utils.py\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.py\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/dataset_infos.json\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt16/wmt16.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt16.json\n",
            "07/15/2021 12:35:44 - INFO - datasets.load - Found local import file from /root/.cache/huggingface/datasets/downloads/11e8085b332f0f35c8271b01b4e3af50448c111c8a226effa391bdeef0c08ece.fee4db31f6778f75af365027007023104d0d90ceeb53eaf0fade478dba9105a1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/wmt_utils.py\n",
            "07/15/2021 12:35:44 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 12:35:44 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/15/2021 12:35:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "07/15/2021 12:35:44 - WARNING - datasets.builder - Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n",
            "07/15/2021 12:35:44 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a\n",
            "100% 3/3 [00:00<00:00, 459.36it/s]\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 12:35:45,110 >> loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 12:35:45,111 >> Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 12:35:45,393 >> loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 12:35:45,394 >> Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/source.spm from cache at /root/.cache/huggingface/transformers/1954839338efdfd7d7cd66bb676626f3fa5404a96e4c3fd9e0d777f08fb2f7fb.2a52a89183278824d0afa78f92263dc46ca0c520a60df7071b6ae4e00fc5bef7\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/target.spm from cache at /root/.cache/huggingface/transformers/1e4b193c82cb1b7fe168ad6685f3a053a2b1ab3482d2d12c306282e03ff379f0.afd22f83f2170348e28be1f817974819f568155c58acd1bb3800cd7937db36a1\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/bffa5c5398d82c200e0eeee10da8e37228615dc7fbabd429567d6ae242f5a615.5e73795abff6c04c6739872ed86c111983aa47f92d9648ac33f2d9d3643e1357\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e05a8eb93de5526dd8224c840b38615d3f79b0bf9636f74ade2807ec16ae347f.09f8e480ed5ba39c95bf16472d22478f2d1870f222433394dd0fee0ac9f1b907\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 12:35:46,378 >> loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1270] 2021-07-15 12:35:46,695 >> loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ddea6571c8d435d3a95a0d3b003ce4ed9cd74380314ce1510adc1a2b80ec6541.ea59afa9489e25f746dc0446b0ef6874b3b9e704d9d077620684ec467f86be9d\n",
            "[INFO|modeling_utils.py:1509] 2021-07-15 12:35:49,296 >> All model checkpoint weights were used when initializing MarianMTModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1518] 2021-07-15 12:35:49,296 >> All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-ro.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
            "07/15/2021 12:35:49 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-89ec593c50f62f34.arrow\n",
            "07/15/2021 12:35:50 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu\n",
            "07/15/2021 12:35:50 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466\n",
            "07/15/2021 12:35:50 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py to /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.py\n",
            "07/15/2021 12:35:50 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/dataset_infos.json\n",
            "07/15/2021 12:35:50 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.json\n",
            "07/15/2021 12:35:50 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2163] 2021-07-15 12:35:50,280 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2165] 2021-07-15 12:35:50,280 >>   Num examples = 1999\n",
            "[INFO|trainer.py:2168] 2021-07-15 12:35:50,280 >>   Batch size = 4\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "100% 500/500 [46:04<00:00,  5.53s/it]\n",
            "***** eval metrics *****\n",
            "  eval_bleu               =     29.749\n",
            "  eval_gen_len            =    33.3897\n",
            "  eval_loss               =     1.4059\n",
            "  eval_runtime            = 0:46:11.32\n",
            "  eval_samples            =       1999\n",
            "  eval_samples_per_second =      0.721\n",
            "  eval_steps_per_second   =       0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-ud3ykmJno3",
        "outputId": "99046657-ae6d-41a8-b7b4-74919e2d08f5"
      },
      "source": [
        "!python examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_418M \\\n",
        "    --do_eval \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zh \\\n",
        "    --dataset_name wmt20_mlqe_task1 \\\n",
        "    --dataset_config_name en-zh \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-15 13:37:43.040368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 13:37:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/15/2021 13:37:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-translation/runs/Jul15_13-37-44_a1f60b3aa509,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-translation,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 13:37:45 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 13:37:45 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:37:45 - INFO - datasets.builder - Generating dataset wmt20_mlqe_task1 (/root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9)\n",
            "Downloading and preparing dataset wmt20_mlqe_task1/en-zh (download: 2.44 MiB, generated: 4.07 MiB, post-processed: Unknown size, total: 6.51 MiB) to /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9...\n",
            "07/15/2021 13:37:45 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "  0% 0/2 [00:00<?, ?it/s]07/15/2021 13:37:45 - INFO - datasets.utils.file_utils - https://github.com/facebookresearch/mlqe/raw/master/data/en-zh.tar.gz not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpa3z640y6\n",
            "\n",
            "Downloading: 100% 2.26M/2.26M [00:00<00:00, 38.8MB/s]\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.file_utils - storing https://github.com/facebookresearch/mlqe/raw/master/data/en-zh.tar.gz in cache at /root/.cache/huggingface/datasets/downloads/c24c0a54e648eded6960209ad4039f48e2656f0bb2e702506166cca8c55c725f\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/c24c0a54e648eded6960209ad4039f48e2656f0bb2e702506166cca8c55c725f\n",
            " 50% 1/2 [00:00<00:00,  1.09it/s]07/15/2021 13:37:46 - INFO - datasets.utils.file_utils - https://github.com/facebookresearch/mlqe/raw/master/data/en-zh_test.tar.gz not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpzeh5vu8y\n",
            "\n",
            "Downloading: 100% 302k/302k [00:00<00:00, 26.9MB/s]\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.file_utils - storing https://github.com/facebookresearch/mlqe/raw/master/data/en-zh_test.tar.gz in cache at /root/.cache/huggingface/datasets/downloads/bb18a2f6ba3f0a4fc73a6968721fc8ab11a50d03537158fb14c01761b69b8ada\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/bb18a2f6ba3f0a4fc73a6968721fc8ab11a50d03537158fb14c01761b69b8ada\n",
            "100% 2/2 [00:01<00:00,  1.50it/s]\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 31.75it/s]\n",
            "07/15/2021 13:37:46 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "07/15/2021 13:37:46 - INFO - datasets.builder - Generating split train\n",
            "07/15/2021 13:37:47 - INFO - datasets.builder - Generating split test\n",
            "07/15/2021 13:37:47 - INFO - datasets.builder - Generating split validation\n",
            "07/15/2021 13:37:48 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset wmt20_mlqe_task1 downloaded and prepared to /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 511.21it/s]\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 13:37:48,255 >> loading configuration file https://huggingface.co/facebook/m2m100_418M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/50bb5d651679266e89ef029c2c7c8007ad28d2c18c28b12389c5699d03968ef2.1314f3af996386b77812f148074f08db19b3ff438a7069d95435e9c51ed4ecfa\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 13:37:48,255 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 13:37:48,393 >> loading configuration file https://huggingface.co/facebook/m2m100_418M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/50bb5d651679266e89ef029c2c7c8007ad28d2c18c28b12389c5699d03968ef2.1314f3af996386b77812f148074f08db19b3ff438a7069d95435e9c51ed4ecfa\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 13:37:48,394 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_418M\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/ade613fd056457e782173e344d733f4a5c9d9d079fbda7919f3b4094b7108c43.469e7a402998e9590fd24cd8fb7df9c9686ecb7fa85274136b18e4231be29d39\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/f777cdf96a7687a96482f207822e9ba5a0ef3d78012afd409d11cbb9f71fd226.f4e5901d4bbb508f3cc5d9b07c765fad081143b0cf6afc5b32bc1a0f3363aec6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/937ae9f76874386b5cdd1b4809e24cbc7a7077f871d32d0b64330d0b80b1cab2.489d57873f088736d37561efb4bca796694cf6482e103fb0b4bd4e69fdc1f524\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/f6743bd6a144094e748e829992606a87e17cf213a8260595bb08c8cad568eb93.7e80b600cf19d2df58762f762130440df8dc1efa138330ca0e49ae816ae3cb56\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:37:48,548 >> loading file https://huggingface.co/facebook/m2m100_418M/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1270] 2021-07-15 13:37:48,752 >> loading weights file https://huggingface.co/facebook/m2m100_418M/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f9eabc2ccf1b4ddafac5c7f6dc837130ab7122d75ee98a64ed0a446a20b84871.53192defd013a2942c1d27b5842eba64b84d0e49943b0892c8f71967bf053029\n",
            "[INFO|modeling_utils.py:1509] 2021-07-15 13:37:55,695 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1518] 2021-07-15 13:37:55,695 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_418M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]07/15/2021 13:37:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/cache-3bb7a26dcf05ad8c.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.16ba/s]\n",
            "07/15/2021 13:37:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvb7xhh_g\n",
            "Downloading: 5.40kB [00:00, 4.70MB/s]       \n",
            "07/15/2021 13:37:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py in cache at /root/.cache/huggingface/datasets/downloads/2d4ecb23afd3c6c17daa2b825c127352e88f5cecc90426106788a4595033f862.c22504a92244ae7412f7fa5f913f568c6c1603a3d71af289957a8f1e7c07a8d2.py\n",
            "07/15/2021 13:37:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/2d4ecb23afd3c6c17daa2b825c127352e88f5cecc90426106788a4595033f862.c22504a92244ae7412f7fa5f913f568c6c1603a3d71af289957a8f1e7c07a8d2.py\n",
            "07/15/2021 13:37:57 - INFO - datasets.load - Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu\n",
            "07/15/2021 13:37:57 - INFO - datasets.load - Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466\n",
            "07/15/2021 13:37:57 - INFO - datasets.load - Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py to /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.py\n",
            "07/15/2021 13:37:57 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/dataset_infos.json\n",
            "07/15/2021 13:37:57 - INFO - datasets.load - Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.json\n",
            "07/15/2021 13:38:10 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2163] 2021-07-15 13:38:10,360 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2165] 2021-07-15 13:38:10,360 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2168] 2021-07-15 13:38:10,360 >>   Batch size = 4\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "100% 250/250 [04:25<00:00,  1.06s/it]\n",
            "***** eval metrics *****\n",
            "  eval_bleu               =     1.7287\n",
            "  eval_gen_len            =     30.728\n",
            "  eval_loss               =     2.2567\n",
            "  eval_runtime            = 0:04:26.47\n",
            "  eval_samples            =       1000\n",
            "  eval_samples_per_second =      3.753\n",
            "  eval_steps_per_second   =      0.938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-rvi7t0aGdF",
        "outputId": "7b46b440-9503-4a01-b93e-bab23366db4b"
      },
      "source": [
        "!python examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_1.2B \\\n",
        "    --do_eval \\\n",
        "    --source_lang en \\\n",
        "    --target_lang zh \\\n",
        "    --dataset_name wmt20_mlqe_task1 \\\n",
        "    --dataset_config_name en-zh \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-15 13:43:47.331418: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 13:43:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/15/2021 13:43:49 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-translation/runs/Jul15_13-43-49_a1f60b3aa509,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-translation,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 13:43:49 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 13:43:49 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:43:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/15/2021 13:43:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 13:43:49 - WARNING - datasets.builder - Reusing dataset wmt20_mlqe_task1 (/root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9)\n",
            "07/15/2021 13:43:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "100% 3/3 [00:00<00:00, 492.83it/s]\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:49,943 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxzxbyymu\n",
            "Downloading: 100% 909/909 [00:00<00:00, 572kB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:43:49,968 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:43:49,968 >> creating metadata file for /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 13:43:49,969 >> loading configuration file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 13:43:49,969 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_1.2B/\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 8192,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 24,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 8192,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 24,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:49,996 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnwd7yecb\n",
            "Downloading: 100% 271/271 [00:00<00:00, 210kB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:43:50,023 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/e3aa86fd0f46d613a5c46e34fe3fbd52c18fbd1fd000788c8b0edb8bfd3a8f4a.e4ee0ef674e85119e46ae598ea7fe2c441cd9997d68e2ebbc0e534302e94a286\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:43:50,024 >> creating metadata file for /root/.cache/huggingface/transformers/e3aa86fd0f46d613a5c46e34fe3fbd52c18fbd1fd000788c8b0edb8bfd3a8f4a.e4ee0ef674e85119e46ae598ea7fe2c441cd9997d68e2ebbc0e534302e94a286\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 13:43:50,048 >> loading configuration file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 13:43:50,049 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_1.2B/\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 8192,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 24,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 8192,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 24,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:50,073 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnyjmnm_w\n",
            "Downloading: 100% 3.71M/3.71M [00:00<00:00, 50.9MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:43:50,202 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/91dc753114999c6c5701ac64121c56d8560e65429c027d556d66049a7001e2c3.469e7a402998e9590fd24cd8fb7df9c9686ecb7fa85274136b18e4231be29d39\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:43:50,203 >> creating metadata file for /root/.cache/huggingface/transformers/91dc753114999c6c5701ac64121c56d8560e65429c027d556d66049a7001e2c3.469e7a402998e9590fd24cd8fb7df9c9686ecb7fa85274136b18e4231be29d39\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:50,228 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqs7y5jif\n",
            "Downloading: 100% 2.42M/2.42M [00:00<00:00, 46.9MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:43:50,305 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/huggingface/transformers/6e5dd39698b72d08358927194497ce37909bd826d64b304ab283e722a2ebd440.f4e5901d4bbb508f3cc5d9b07c765fad081143b0cf6afc5b32bc1a0f3363aec6\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:43:50,305 >> creating metadata file for /root/.cache/huggingface/transformers/6e5dd39698b72d08358927194497ce37909bd826d64b304ab283e722a2ebd440.f4e5901d4bbb508f3cc5d9b07c765fad081143b0cf6afc5b32bc1a0f3363aec6\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:50,380 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk99w4geg\n",
            "Downloading: 100% 1.14k/1.14k [00:00<00:00, 1.14MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:43:50,404 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/d0ec7a51c9951b2f2f31f2d6a756d05d87c07e798a658fb7adb8a0b234400fc6.7e80b600cf19d2df58762f762130440df8dc1efa138330ca0e49ae816ae3cb56\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:43:50,404 >> creating metadata file for /root/.cache/huggingface/transformers/d0ec7a51c9951b2f2f31f2d6a756d05d87c07e798a658fb7adb8a0b234400fc6.7e80b600cf19d2df58762f762130440df8dc1efa138330ca0e49ae816ae3cb56\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/91dc753114999c6c5701ac64121c56d8560e65429c027d556d66049a7001e2c3.469e7a402998e9590fd24cd8fb7df9c9686ecb7fa85274136b18e4231be29d39\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/6e5dd39698b72d08358927194497ce37909bd826d64b304ab283e722a2ebd440.f4e5901d4bbb508f3cc5d9b07c765fad081143b0cf6afc5b32bc1a0f3363aec6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e3aa86fd0f46d613a5c46e34fe3fbd52c18fbd1fd000788c8b0edb8bfd3a8f4a.e4ee0ef674e85119e46ae598ea7fe2c441cd9997d68e2ebbc0e534302e94a286\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/d0ec7a51c9951b2f2f31f2d6a756d05d87c07e798a658fb7adb8a0b234400fc6.7e80b600cf19d2df58762f762130440df8dc1efa138330ca0e49ae816ae3cb56\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 13:43:50,431 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|file_utils.py:1623] 2021-07-15 13:43:50,636 >> https://huggingface.co/facebook/m2m100_1.2B/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7fymn2ha\n",
            "Downloading: 100% 4.96G/4.96G [01:37<00:00, 51.1MB/s]\n",
            "[INFO|file_utils.py:1627] 2021-07-15 13:45:27,782 >> storing https://huggingface.co/facebook/m2m100_1.2B/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/68002fb1a7773d8d8373f1a230588141964ef9f249db6987681f295dbe85356c.ee70663869b89be4f68eed03a21d5c3400b223cb544883f411e469aaea0a25f9\n",
            "[INFO|file_utils.py:1635] 2021-07-15 13:45:27,782 >> creating metadata file for /root/.cache/huggingface/transformers/68002fb1a7773d8d8373f1a230588141964ef9f249db6987681f295dbe85356c.ee70663869b89be4f68eed03a21d5c3400b223cb544883f411e469aaea0a25f9\n",
            "[INFO|modeling_utils.py:1270] 2021-07-15 13:45:27,782 >> loading weights file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/68002fb1a7773d8d8373f1a230588141964ef9f249db6987681f295dbe85356c.ee70663869b89be4f68eed03a21d5c3400b223cb544883f411e469aaea0a25f9\n",
            "[INFO|modeling_utils.py:1509] 2021-07-15 13:47:56,267 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1518] 2021-07-15 13:47:56,267 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_1.2B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]07/15/2021 13:47:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/cache-b2ec016571d5ef7a.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:01<00:00,  1.02s/ba]\n",
            "07/15/2021 13:47:59 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu\n",
            "07/15/2021 13:47:59 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466\n",
            "07/15/2021 13:47:59 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py to /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.py\n",
            "07/15/2021 13:47:59 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/dataset_infos.json\n",
            "07/15/2021 13:47:59 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.json\n",
            "07/15/2021 13:48:14 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2163] 2021-07-15 13:48:14,139 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2165] 2021-07-15 13:48:14,139 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2168] 2021-07-15 13:48:14,139 >>   Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "100% 1000/1000 [15:17<00:00,  1.09it/s]\n",
            "***** eval metrics *****\n",
            "  eval_bleu               =     1.8033\n",
            "  eval_gen_len            =     30.913\n",
            "  eval_loss               =     1.7804\n",
            "  eval_runtime            = 0:15:19.05\n",
            "  eval_samples            =       1000\n",
            "  eval_samples_per_second =      1.088\n",
            "  eval_steps_per_second   =      1.088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rer3ZVqtdNm5",
        "outputId": "158f985d-3cc6-4c6d-afdb-83b7e2f2d71d"
      },
      "source": [
        "!python examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path facebook/m2m100_1.2B \\\n",
        "    --do_eval \\\n",
        "    --source_lang zh \\\n",
        "    --target_lang en \\\n",
        "    --dataset_name wmt20_mlqe_task1 \\\n",
        "    --dataset_config_name en-zh \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-15 14:03:46.132184: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 14:03:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/15/2021 14:03:50 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/tst-translation/runs/Jul15_14-03-50_a1f60b3aa509,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=tst-translation,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.py\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/dataset_infos.json\n",
            "07/15/2021 14:03:50 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.9.0/datasets/wmt20_mlqe_task1/wmt20_mlqe_task1.py at /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/wmt20_mlqe_task1.json\n",
            "07/15/2021 14:03:50 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt20_mlqe_task1/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 14:03:50 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/15/2021 14:03:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "07/15/2021 14:03:50 - WARNING - datasets.builder - Reusing dataset wmt20_mlqe_task1 (/root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9)\n",
            "07/15/2021 14:03:50 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9\n",
            "100% 3/3 [00:00<00:00, 39.93it/s]\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 14:03:50,734 >> loading configuration file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 14:03:50,734 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_1.2B/\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 8192,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 24,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 8192,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 24,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:537] 2021-07-15 14:03:50,872 >> loading configuration file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c2ceb725ca403a905263b5841ed965960eafb478ca8fb96bf29aa95c59676b7.a9b5ac265e8c2b9ab924fe68c2d36cfb2a9c0e026c9d7533ba5e61bbe09cb115\n",
            "[INFO|configuration_utils.py:573] 2021-07-15 14:03:50,873 >> Model config M2M100Config {\n",
            "  \"_name_or_path\": \"hf_models/m2m100_1.2B/\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 8192,\n",
            "  \"decoder_layerdrop\": 0.05,\n",
            "  \"decoder_layers\": 24,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 8192,\n",
            "  \"encoder_layerdrop\": 0.05,\n",
            "  \"encoder_layers\": 24,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_beams\": 5,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"transformers_version\": \"4.9.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/91dc753114999c6c5701ac64121c56d8560e65429c027d556d66049a7001e2c3.469e7a402998e9590fd24cd8fb7df9c9686ecb7fa85274136b18e4231be29d39\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/6e5dd39698b72d08358927194497ce37909bd826d64b304ab283e722a2ebd440.f4e5901d4bbb508f3cc5d9b07c765fad081143b0cf6afc5b32bc1a0f3363aec6\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e3aa86fd0f46d613a5c46e34fe3fbd52c18fbd1fd000788c8b0edb8bfd3a8f4a.e4ee0ef674e85119e46ae598ea7fe2c441cd9997d68e2ebbc0e534302e94a286\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/d0ec7a51c9951b2f2f31f2d6a756d05d87c07e798a658fb7adb8a0b234400fc6.7e80b600cf19d2df58762f762130440df8dc1efa138330ca0e49ae816ae3cb56\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 14:03:51,009 >> loading file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_utils.py:1270] 2021-07-15 14:03:51,308 >> loading weights file https://huggingface.co/facebook/m2m100_1.2B/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/68002fb1a7773d8d8373f1a230588141964ef9f249db6987681f295dbe85356c.ee70663869b89be4f68eed03a21d5c3400b223cb544883f411e469aaea0a25f9\n",
            "[INFO|modeling_utils.py:1509] 2021-07-15 14:05:51,939 >> All model checkpoint weights were used when initializing M2M100ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1518] 2021-07-15 14:05:51,940 >> All the weights of M2M100ForConditionalGeneration were initialized from the model checkpoint at facebook/m2m100_1.2B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use M2M100ForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]07/15/2021 14:05:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt20_mlqe_task1/en-zh/1.1.0/651fac5e9d40e5249d8e50a2e9422c861bb4d88f759e2f411c35879ae1fb27b9/cache-123cb14f187b043a.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  1.17ba/s]\n",
            "07/15/2021 14:05:54 - INFO - datasets.load - Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu\n",
            "07/15/2021 14:05:54 - INFO - datasets.load - Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466\n",
            "07/15/2021 14:05:54 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py to /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.py\n",
            "07/15/2021 14:05:54 - INFO - datasets.load - Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/dataset_infos.json\n",
            "07/15/2021 14:05:54 - INFO - datasets.load - Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.9.0/metrics/sacrebleu/sacrebleu.py at /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/4dba4e29caa3766d885f0b9cde070fedb22ac3190c264a6454b8ea6703ddd466/sacrebleu.json\n",
            "07/15/2021 14:06:10 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2163] 2021-07-15 14:06:10,957 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2165] 2021-07-15 14:06:10,957 >>   Num examples = 1000\n",
            "[INFO|trainer.py:2168] 2021-07-15 14:06:10,957 >>   Batch size = 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            " 11% 114/1000 [01:35<10:41,  1.38it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUDC6AsDWnp3"
      },
      "source": [
        "\n",
        "python examples/pytorch/seq2seq/run_translation.py \\\n",
        "    --model_name_or_path facebook/mbart-large-en-ro  \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --dataset_name wmt16 \\\n",
        "    --dataset_config_name ro-en \\\n",
        "    --source_lang en_XX \\\n",
        "    --target_lang ro_RO \\\n",
        "    --output_dir /tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --predict_with_generate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}