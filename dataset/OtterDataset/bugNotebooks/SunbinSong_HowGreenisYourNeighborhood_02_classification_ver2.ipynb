{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9651aa-d64b-4a52-bc54-d425e517aaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(10000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from fiona import crs\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from shapely.geometry import Point\n",
    "from osgeo import gdal\n",
    "# from gdalconst import GA_ReadOnly\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, accuracy_score, balanced_accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from filelock import FileLock\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0) \n",
    "%autosave 10\n",
    "%matplotlib inline\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "##Supressing warnings\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pyproj import CRS\n",
    "\n",
    "# Suppress FutureWarning from pyproj\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyproj.crs.crs\")\n",
    "\n",
    "# Suppress SettingWithCopyWarning from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5db52c-2d12-4868-815a-b551f3562583",
   "metadata": {},
   "source": [
    "### Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d28209a-15eb-4a2d-bf70-5dc0d43ff3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_img = r'../Processing/green_space_classification\\dist_road\\dist_%s.tif'%ux\n",
    "bands=['ndti','ndre','ndvi','ndwi','mndwi','glcm','B2','B3','B4','B8'] #define the band names in the img\n",
    "\n",
    "# read in all the OSM features that may have vegetation\n",
    "veges = pd.read_excel(r'osm_vegetation_classes.xlsx')\n",
    "\n",
    "foi = ['ndti','ndre','ndvi','ndwi','mndwi','glcm']\n",
    "ind_selected = [bands.index(x) for x in foi]\n",
    "bands = foi\n",
    "\n",
    "accuracy_out = r'../Processing/accuracy_classification.csv'\n",
    "# fail_out = r'D:\\Work/Processing/green_space_classification\\fail_classification.csv'\n",
    "# print(ind_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241e35f-a485-41e7-a110-69e66ece4c36",
   "metadata": {},
   "source": [
    "### Define some global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a4869a-e5ec-4538-810f-af694e6dd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pnts(row):\n",
    "    # define a function to align samples with raster cell\n",
    "    geometry = row['geometry']\n",
    "    bounds = geometry.bounds\n",
    "    xmin, ymin, xmax, ymax = bounds[0], bounds[1], bounds[2], bounds[3]\n",
    "    x,y= np.mgrid[xmin:xmax+10:10,ymin:ymax+10:10]\n",
    "    x,y = np.vstack([x.ravel(), y.ravel()])\n",
    "    p = pd.DataFrame(list(zip(x,y)))\n",
    "    p[0]=np.floor((p[0]-row['xmin'])/row['xres'])*row['xres']+row['xmin']+row['xres']/2\n",
    "    p[1]=np.floor((p[1]-row['ymin'])/row['yres'])*row['yres']+row['ymin']+row['yres']/2\n",
    "    p['pnt'] = list(set(zip(p[0],p[1])))\n",
    "    p['pnt']  = p['pnt'].apply(Point)\n",
    "    p = gpd.GeoDataFrame(p['pnt'],geometry='pnt',crs=crs.from_epsg(27700))\n",
    "    p = p[p.within(geometry)]\n",
    "    return p['pnt'].apply(lambda x:[x.x,x.y]).values\n",
    "\n",
    "def sample_raster(row,img_array):\n",
    "    # define a function to sample the rasters\n",
    "    y = int(row['y_n'])\n",
    "    x = int(row['x_n'])\n",
    "    if 0 <= y < img_array.shape[1] and 0 <= x < img_array.shape[2]:\n",
    "        res = img_array[:, y, x]\n",
    "    else:\n",
    "        res = np.nan\n",
    "    if np.isnan(res).any():\n",
    "        res = np.nan\n",
    "    return res\n",
    "\n",
    "# def sample_raster(row,img_array):\n",
    "#     # define a function to sample the rasters\n",
    "#     y = int(row['y_n'])\n",
    "#     x = int(row['x_n'])\n",
    "#     res = img_array[:,y,x]\n",
    "#     if np.isnan(res).any():\n",
    "#         res = np.nan\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8beac-4de2-4a00-80fb-0df0bbb2e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the selected feature polygons\n",
    "def extract_OSM_polygons(OSM, city):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile= shapefile.to_crs({'init': 'epsg:27700'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:,'area_length']=(building.area/building.length).values\n",
    "    building.loc[:,'general'] = 'bldg'\n",
    "    shapefile = shapefile[shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0,len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    for i in veges.index:\n",
    "        sub = pd.DataFrame()\n",
    "        # key = veges.loc[i,'Key']\n",
    "        # value = veges.loc[i,'Value']\n",
    "        # sub['geometry'] = shapefile.loc[shapefile[key]==value,'geometry']\n",
    "        # sub['FID'] = shapefile.loc[shapefile[key]==value,'FID']\n",
    "        sub['geometry'] = shapefile.loc[shapefile['general']=='vegetation','geometry']\n",
    "        sub['FID'] = shapefile.loc[shapefile['general']=='vegetation','FID']\n",
    "        sub['key'] = 'general'\n",
    "        sub['value']='vegetation'\n",
    "        sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "        if len(sub.index)>0:\n",
    "            one_city=pd.concat([one_city,sub])\n",
    "    one_city['general']='vegetation'\n",
    "\n",
    "    if len(one_city.index)>0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city,geometry='geometry', crs=crs.from_epsg(27700))\n",
    "        one_city_gdf.loc[:,'shape_index']=(one_city_gdf.length/(4*np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[(one_city_gdf.area<=one_city_gdf.area.quantile(0.975))&\n",
    "              (one_city_gdf.area>=one_city_gdf.area.quantile(0.025))&\n",
    "              (one_city_gdf['shape_index']<one_city_gdf['shape_index'].quantile(0.9))]\n",
    "        \n",
    "        background=shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])),['geometry','FID']]\n",
    "        background['general']='other'\n",
    "        background.loc[:,'shape_index']=(background.length/(4*np.sqrt(background.area))).values\n",
    "        background = background.loc[(background.area<=background.area.quantile(0.975))&\n",
    "          (background.area>=background.area.quantile(0.025))&\n",
    "          (background['shape_index']<background['shape_index'].quantile(0.9))]\n",
    "        one_city_gdf = pd.concat([one_city_gdf,building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf,background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general','geometry','shape_index']],geometry='geometry', crs=crs.from_epsg(27700))\n",
    "    one_city_gdf.to_file(driver = 'ESRI Shapefile', filename= r\"../Processing/polygon_%s.shp\"%city)\n",
    "    return one_city_gdf\n",
    "\n",
    "def grid_search_wrapper(refit_score,clf,param_grid,scorers,X_train,X_test,y_train,y_test,fit_params,city):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring= ['f1','f1_weighted'], refit=refit_score,\n",
    "                           cv=skf, return_train_score=False,n_jobs=4,verbose=0)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city':city,'datetime':datetime.datetime.now(),\n",
    "                         'accuracy_balanced':balanced_accuracy_score(y_test,y_pred),\n",
    "                        'accuracy':accuracy_score(y_test,y_pred),\n",
    "                        'precision':precision_score(y_test,y_pred),\n",
    "                         'recall':recall_score(y_test,y_pred),\n",
    "                         'f1_score':f1_score(y_test,y_pred)}\n",
    "\n",
    "def generate_sample(one_city_gdf, img, city):\n",
    "    logging.info(\"Starting sample generation...\")\n",
    "    global bands\n",
    "    # sample points to raster grid\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts']=  one_city_gdf.apply(sample_pnts,axis=1)\n",
    "    # attach sample class\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        logging.debug(f\"Processing class: {i}\")\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general']==i,'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0],sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0],sample_df[1]),crs=\"epsg:27700\")\n",
    "        sample_gdf['class'] = i\n",
    "        logging.debug(f\"Generated {len(sample_gdf)} points for class {i}.\")\n",
    "        all_sample = pd.concat([all_sample,sample_gdf])\n",
    "\n",
    "\n",
    "    print(\"one_city_gdf values\")\n",
    "    print(one_city_gdf['general'].value_counts())\n",
    "    \n",
    "    print(\"all_sample values\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    # attach sample to img grid x,y\n",
    "    all_sample['x']=all_sample.geometry.x\n",
    "    all_sample['y']=all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0])/geoTransform[1]-0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3])/geoTransform[5]-0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    print(\"all_sample values second\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    # remove overlapped samples\n",
    "    land_sample = all_sample[all_sample['class']!='bldg'].copy()\n",
    "    print(\"land_sample\")\n",
    "    print(len(land_sample))\n",
    "    print(len(land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=False)))\n",
    "    # land_sample.drop_duplicates(subset=['x','y'], keep=False, inplace=True)\n",
    "    land_sample.drop_duplicates(subset=['x','y'], keep='first', inplace=True)\n",
    "    clean_sample = pd.concat([land_sample,all_sample[all_sample['class']=='bldg'].copy()])\n",
    "    clean_sample['bldg_drop']=0\n",
    "    clean_sample.loc[clean_sample['class']=='bldg','bldg_drop']=1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    clean_sample.drop_duplicates(subset=['x','y'], keep='last', inplace=True)\n",
    "   \n",
    "    # mask out pixels with nan in any band\n",
    "    img_array= np.array(raster.ReadAsArray())\n",
    "    # print(img_array.shape)\n",
    "    img_array = img_array[ind_selected,:,:]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d =  img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d,x_d*y_d)\n",
    "    img_array[:,np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d,x_d,y_d)\n",
    "    # use the samples to sample the img\n",
    "\n",
    "    print(\"clean_sample number before ndvi filtering\")\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x:sample_raster(x,img_array),axis=1)\n",
    "    #remove bad sample based on ndvi\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x:x[bands.index('ndvi')])\n",
    "\n",
    "    # drop any vegetation sample with NDVI less than 0.1\n",
    "    # 0.1 > 0.02 \n",
    "    clean_sample.loc[(clean_sample['class']=='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']<=0.02),'sample_value']=np.nan \n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "\n",
    "    print(\"clean_sample number after ndvi filtering\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # drop any non-vegetation sample with NDVI greater than median NDVI of vegetated samples\n",
    "    v_median = clean_sample.loc[(clean_sample['class']=='vegetation'),'mean_ndvi'].median()\n",
    "    print(\"v_median\")\n",
    "    print(v_median)\n",
    "    clean_sample.loc[(clean_sample['class']!='vegetation')\n",
    "                     &(clean_sample['mean_ndvi']>=v_median),'sample_value']=np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi','sample_value'],axis=1)\n",
    "\n",
    "    print(\"clean_sample number before pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # PCA transformation of the img\n",
    "    # min-max normalization first\n",
    "    for i in range(0,img_array.shape[0]):\n",
    "        v = img_array[i,:,:]\n",
    "        img_array[i,:,:]=(v-np.nanmin(v))/(np.nanmax(v)-np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1]*img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var=np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3)*100)\n",
    "    n_pc = sum(var<=90)+1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce,pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc,:,:])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0],img_reduce.shape[1]*img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0],img_reduce.shape[1],img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca,img_array_pca_valid,img_reduce_re,img_reduce,pca_reduce\n",
    "    # determine outliers in the samples\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x,img_array),axis=1)\n",
    "    PCAs = list(range(0,n_pc))\n",
    "    for PC in PCAs:\n",
    "        i  = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x:x[i])\n",
    "    clean_sample= clean_sample.drop('sample_value',axis=1)\n",
    "\n",
    "    print(\"clean_sample number after pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class']==i,PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier']=y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class']==i,'outlier_score'] = (outlier_score-outlier_score.min()) / (outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier']!=-1]\n",
    "    \n",
    "    print(\"clean_sample number after second pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    # random selection of samples\n",
    "    n_vege = len(clean_sample.loc[clean_sample['class']=='vegetation'])\n",
    "    n_other = len(clean_sample.loc[clean_sample['class']=='other'])\n",
    "    n_bldg = len(clean_sample.loc[clean_sample['class']=='bldg'])\n",
    "    n_sample = int(0.2*min(n_vege,n_other+n_bldg))\n",
    "    print(\"n_sample: \",n_sample)\n",
    "    print(\"n_vege: \",n_vege)\n",
    "    print(\"n_other: \",n_other)\n",
    "    print(\"n_bldg: \",n_bldg)\n",
    "    \n",
    "    if n_sample>=2500:\n",
    "        n_sample = 2500\n",
    "    if n_sample<200:\n",
    "        n_sample = int(1*min(n_vege,n_other+n_bldg))\n",
    "    sub_clean_sample = clean_sample.loc[clean_sample['class']=='vegetation'].sample(n=n_sample,random_state=0)\n",
    "    if n_other>(n_sample/2) and n_bldg>(n_sample/2):\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='bldg'].sample(n=int(n_sample/2),random_state=0)])\n",
    "        sub_clean_sample = pd.concat([sub_clean_sample,clean_sample.loc[clean_sample['class']=='other'].sample(n=int(n_sample/2),random_state=0)])\n",
    "    else:\n",
    "        if n_other > (n_sample / 2) and n_bldg < (n_sample / 2):\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'bldg'].sample(n=int(n_bldg), random_state=0)])\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'other'].sample(n=int(n_sample - n_bldg), random_state=0)])\n",
    "        else:\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'bldg'].sample(n=int(n_sample - n_other), random_state=0)])\n",
    "            sub_clean_sample = pd.concat([sub_clean_sample, clean_sample.loc[clean_sample['class'] == 'other'].sample(n=int(n_other), random_state=0)])\n",
    "\n",
    "    sub_clean_sample.loc[sub_clean_sample['class']=='bldg','class']='other'\n",
    "    ext=sub_clean_sample[['class','geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:27700'})\n",
    "    ext.to_file(r\"../Processing/all_sample_%s.shp\"% city)\n",
    "    # encouraging garbage collection\n",
    "    raster = None\n",
    "    return sub_clean_sample, img_array,PCAs\n",
    "\n",
    "def split_sample(sub_clean_sample,PCAs,city):\n",
    "    # training testing split\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "\n",
    "    sub_clean_sample['class'] = sub_clean_sample['class'].apply(lambda x:1 if x=='vegetation' else 0)\n",
    "    targets = sub_clean_sample['class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets,random_state=0)\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "    train_exp = X_train.merge(sub_clean_sample,how='inner')\n",
    "    test_exp = X_test.merge(sub_clean_sample,how='inner')\n",
    "    gpd.GeoDataFrame(train_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/train_sample_%s.shp\"%city)\n",
    "    gpd.GeoDataFrame(test_exp[['class','geometry']],geometry='geometry').to_file(driver = 'ESRI Shapefile',\n",
    "                                filename= r\"../Processing/test_sample_%s.shp\"%(city)\n",
    "    return X_train,X_test,y_train,y_test,X_train_weight\n",
    "\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import warnings\n",
    "from filelock import FileLock\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def process_city(city, tif_names):\n",
    "    try:\n",
    "        logging.info(f'Starting processing for {city}')\n",
    "        OSM = r'../Code/%s_OSM_combined.shp' % city\n",
    "        img = r'../Sample_image/LA_%s_2023.tif' % tif_names[city]\n",
    "        \n",
    "        if not os.path.exists(OSM):\n",
    "            logging.error(f'OSM file does not exist: {OSM}')\n",
    "            return\n",
    "        if not os.path.exists(img):\n",
    "            logging.error(f'Image file does not exist: {img}')\n",
    "            return\n",
    "        \n",
    "        one_city_gdf = extract_OSM_polygons(OSM, city) # get OSM polygons\n",
    "        print(\"one_city_gdf after extract\")\n",
    "        print(len(one_city_gdf))\n",
    "        sub_clean_sample, img_array, PCAs = generate_sample(one_city_gdf, img, city) # generate random samples\n",
    "        print(\"sub_clean_sample\")\n",
    "        print(len(sub_clean_sample))\n",
    "        print(sub_clean_sample.head())\n",
    "    \n",
    "        X_train, X_test, y_train, y_test, X_train_weight = split_sample(sub_clean_sample, PCAs,city) # sample values, train test split\n",
    "        scorers = {\n",
    "            'precision_score': make_scorer(precision_score),\n",
    "            'recall_score': make_scorer(recall_score),\n",
    "            'accuracy_score': make_scorer(accuracy_score),\n",
    "            'f1_score': make_scorer(f1_score)\n",
    "        }\n",
    "        fit_params = {'sample_weight': X_train_weight}\n",
    "        clf = SVC()\n",
    "        param_grid = {'C': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                      'gamma': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                      'random_state': [0],\n",
    "                      'class_weight': ['balanced']}\n",
    "        grid_search_clf, test_scores = grid_search_wrapper('f1_weighted', clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params,city)\n",
    "        \n",
    "        # save testing accuracy\n",
    "        logging.info(f'Test scores for {city}: {test_scores}')\n",
    "    \n",
    "        lock = FileLock(f\"{accuracy_out}.lock\")\n",
    "        with lock:\n",
    "            with open(accuracy_out, 'a') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter=',', lineterminator='\\n')\n",
    "                writer.writerow([f\"{city}\"] + list(zip(test_scores.keys(), test_scores.values())))\n",
    "        \n",
    "        csv_file.close()\n",
    "        \n",
    "        # save original img\n",
    "        img_array2 = np.copy(img_array)\n",
    "        img_re = img_array2.reshape((img_array2.shape[0], img_array2.shape[1] * img_array2.shape[2])).transpose()\n",
    "        img_pre = np.copy(img_re[~np.isnan(img_re).any(axis=1)])\n",
    "        img_pre = grid_search_clf.predict(img_pre)\n",
    "        img_pre = img_pre.astype(np.int16)\n",
    "        res = img_re[:, 0]\n",
    "        res[~np.isnan(res)] = img_pre\n",
    "        res[np.isnan(res)] = -32768\n",
    "        res = res.reshape(img_array[0, :, :].shape)\n",
    "        res = res.astype(np.int16)\n",
    "\n",
    "        org_img = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "        meta = {\n",
    "            'driver': 'GTiff',\n",
    "            'dtype': 'int16',\n",
    "            'nodata': -32768,\n",
    "            'width': res.shape[1],\n",
    "            'height': res.shape[0],\n",
    "            'count': 1,\n",
    "            # due to package update\n",
    "            # 'crs': CRS.from_dict(init='epsg:27700'),\n",
    "            'crs': CRS(\"EPSG:27700\"),\n",
    "            'transform': Affine(10, 0.0, org_img.GetGeoTransform()[0], 0, -10, org_img.GetGeoTransform()[-3]),\n",
    "            'compress': 'lzw',\n",
    "            'interleave': 'pixel'\n",
    "        }\n",
    "\n",
    "        # Save to a specific file \n",
    "        result_path = r'../Results/{}_{}.tif'.format(city)\n",
    "        with rio.open(result_path, 'w', **meta) as dst:\n",
    "            dst.write(res, 1)\n",
    "\n",
    "        logging.info(f'Processed and saved results for {city}')\n",
    "\n",
    "        #encouraging garbage collection\n",
    "        del img_array\n",
    "        org_img = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    tif_names = {\"Greater_Manchester\": 'Manchester', \"Greater_London\": 'London', \"West_Midlands\": 'Westmidlands'}\n",
    "    # cities = [\"London\"]\n",
    "    cities = [\"Greater_Manchester\", \"West_Midlands\", \"Greater_London\"]\n",
    "    cities = [\"Greater_London\"]\n",
    "    \n",
    "    max_workers = 3  # Adjust this number based on your system's CPU cores and memory\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_city = {\n",
    "            executor.submit(process_city, city, tif_names) for city in cities\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_city):\n",
    "            city = future_to_city[future]\n",
    "            try:\n",
    "                future.result()  # Raises exception if any occurred during processing\n",
    "                logging.info(f'Completed processing for {city}')\n",
    "                # gc.collect()\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "print('All jobs done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e7783-3408-4ca6-ac09-926af887656a",
   "metadata": {},
   "source": [
    "## GI classification to 5 levels\n",
    "\n",
    "#### Amendments Made\n",
    "\n",
    "- application of multiclass\n",
    "- various classifier\n",
    "- score comparison table change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d8faac-c3ad-44a7-8b65-cb377c2813a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 17:08:02,198 - INFO - Starting processing for West_Midlands\n",
      "2024-07-27 17:08:02,198 - INFO - Starting processing for Greater_London\n",
      "2024-07-27 17:08:02,198 - INFO - Starting processing for Greater_Manchester\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "2024-07-27 17:08:23,796 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "27436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 17:08:26,705 - INFO - Starting sample generation...\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "2024-07-27 17:08:48,433 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "27602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 17:08:51,383 - INFO - Starting sample generation...\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "2024-07-27 17:10:01,093 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "12794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 17:10:02,489 - INFO - Starting sample generation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sample values before filtering\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    241863\n",
      "4.0    114473\n",
      "5.0     58012\n",
      "3.0       419\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    241863\n",
      "4.0    114473\n",
      "5.0     58012\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    241863\n",
      "4.0    114473\n",
      "5.0     58012\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "2204828\n",
      "2041779\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "1.0    989226\n",
      "0.0    854834\n",
      "2.0    224673\n",
      "4.0     43495\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after ndvi filtering\n",
      "1411520\n",
      "          index           coordinates                       geometry  class  \\\n",
      "2204821  114466  (361105.0, 407735.0)  POINT (361105.000 407735.000)    4.0   \n",
      "2204822  114467  (361155.0, 407765.0)  POINT (361155.000 407765.000)    4.0   \n",
      "2204823  114468  (361155.0, 407755.0)  POINT (361155.000 407755.000)    4.0   \n",
      "2204824  114469  (361105.0, 407745.0)  POINT (361105.000 407745.000)    4.0   \n",
      "2204825  114470  (361165.0, 407765.0)  POINT (361165.000 407765.000)    4.0   \n",
      "\n",
      "                x         y    x_n     y_n  bldg_drop  \\\n",
      "2204821  361105.0  407735.0  957.0  1359.0          0   \n",
      "2204822  361155.0  407765.0  962.0  1356.0          0   \n",
      "2204823  361155.0  407755.0  962.0  1357.0          0   \n",
      "2204824  361105.0  407745.0  957.0  1358.0          0   \n",
      "2204825  361165.0  407765.0  963.0  1356.0          0   \n",
      "\n",
      "                                              sample_value  mean_ndvi  \n",
      "2204821  [0.11388923, 0.020127865, 0.06433876, -0.08534...   0.064339  \n",
      "2204822  [0.11247122, 0.019964095, 0.06800282, -0.08601...   0.068003  \n",
      "2204823  [0.115383245, 0.020942891, 0.073335655, -0.092...   0.073336  \n",
      "2204824  [0.11633238, 0.018615473, 0.067998536, -0.0917...   0.067999  \n",
      "2204825  [0.11247122, 0.021265695, 0.07124323, -0.08933...   0.071243  \n",
      "class\n",
      "1.0    797617\n",
      "0.0    489156\n",
      "2.0    103975\n",
      "4.0     20772\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.06451414\n",
      "clean_sample number before pca\n",
      "1411520\n",
      "          index           coordinates                       geometry  class  \\\n",
      "2204821  114466  (361105.0, 407735.0)  POINT (361105.000 407735.000)    4.0   \n",
      "2204822  114467  (361155.0, 407765.0)  POINT (361155.000 407765.000)    4.0   \n",
      "2204823  114468  (361155.0, 407755.0)  POINT (361155.000 407755.000)    4.0   \n",
      "2204824  114469  (361105.0, 407745.0)  POINT (361105.000 407745.000)    4.0   \n",
      "2204825  114470  (361165.0, 407765.0)  POINT (361165.000 407765.000)    4.0   \n",
      "\n",
      "                x         y    x_n     y_n  bldg_drop  \n",
      "2204821  361105.0  407735.0  957.0  1359.0          0  \n",
      "2204822  361155.0  407765.0  962.0  1356.0          0  \n",
      "2204823  361155.0  407755.0  962.0  1357.0          0  \n",
      "2204824  361105.0  407745.0  957.0  1358.0          0  \n",
      "2204825  361165.0  407765.0  963.0  1356.0          0  \n",
      "class\n",
      "1.0    797617\n",
      "0.0    489156\n",
      "2.0    103975\n",
      "4.0     20772\n",
      "Name: count, dtype: int64\n",
      "all_sample values before filtering\n",
      "class\n",
      "2.0    683450\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "4.0    186044\n",
      "5.0     35668\n",
      "3.0      7497\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "2.0    683450\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "4.0    186044\n",
      "5.0     35668\n",
      "3.0      7497\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "2.0    683450\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "4.0    186044\n",
      "5.0     35668\n",
      "3.0      7497\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "1952904\n",
      "1589121\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "2.0    614667\n",
      "0.0    602128\n",
      "1.0    461543\n",
      "4.0     71903\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after pca\n",
      "1411520\n",
      "class\n",
      "1.0    797617\n",
      "0.0    489156\n",
      "2.0    103975\n",
      "4.0     20772\n",
      "Name: count, dtype: int64\n",
      "all_sample values before filtering\n",
      "class\n",
      "2.0    418052\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "4.0    143958\n",
      "5.0     80459\n",
      "3.0         6\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "2.0    418052\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "4.0    143958\n",
      "5.0     80459\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "2.0    418052\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "4.0    143958\n",
      "5.0     80459\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "1261427\n",
      "946360\n",
      "clean_sample number after ndvi filtering\n",
      "1550669\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1952887  186027  (534135.0, 199515.0)  POINT (534135.000 199515.000)    4.0   \n",
      "1952888  186028  (534115.0, 199515.0)  POINT (534115.000 199515.000)    4.0   \n",
      "1952889  186029  (534095.0, 199555.0)  POINT (534095.000 199555.000)    4.0   \n",
      "1952890  186030  (534125.0, 199565.0)  POINT (534125.000 199565.000)    4.0   \n",
      "1952891  186031  (534135.0, 199545.0)  POINT (534135.000 199545.000)    4.0   \n",
      "\n",
      "                x         y     x_n    y_n  bldg_drop  \\\n",
      "1952887  534135.0  199515.0  3097.0  232.0          0   \n",
      "1952888  534115.0  199515.0  3095.0  232.0          0   \n",
      "1952889  534095.0  199555.0  3093.0  228.0          0   \n",
      "1952890  534125.0  199565.0  3096.0  227.0          0   \n",
      "1952891  534135.0  199545.0  3097.0  229.0          0   \n",
      "\n",
      "                                              sample_value  mean_ndvi  \n",
      "1952887  [0.0980918, 0.025912853, 0.09842499, -0.106984...   0.098425  \n",
      "1952888  [0.09711976, 0.025930258, 0.10011498, -0.11104...   0.100115  \n",
      "1952889  [0.10058096, 0.027860574, 0.09631361, -0.11060...   0.096314  \n",
      "1952890  [0.106, 0.027417311, 0.10124298, -0.10756225, ...   0.101243  \n",
      "1952891  [0.103906944, 0.026680466, 0.103524104, -0.103...   0.103524  \n",
      "class\n",
      "0.0    542274\n",
      "2.0    516437\n",
      "1.0    432146\n",
      "4.0     59812\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.07305606\n",
      "clean_sample number before pca\n",
      "1550669\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1952887  186027  (534135.0, 199515.0)  POINT (534135.000 199515.000)    4.0   \n",
      "1952888  186028  (534115.0, 199515.0)  POINT (534115.000 199515.000)    4.0   \n",
      "1952889  186029  (534095.0, 199555.0)  POINT (534095.000 199555.000)    4.0   \n",
      "1952890  186030  (534125.0, 199565.0)  POINT (534125.000 199565.000)    4.0   \n",
      "1952891  186031  (534135.0, 199545.0)  POINT (534135.000 199545.000)    4.0   \n",
      "\n",
      "                x         y     x_n    y_n  bldg_drop  \n",
      "1952887  534135.0  199515.0  3097.0  232.0          0  \n",
      "1952888  534115.0  199515.0  3095.0  232.0          0  \n",
      "1952889  534095.0  199555.0  3093.0  228.0          0  \n",
      "1952890  534125.0  199565.0  3096.0  227.0          0  \n",
      "1952891  534135.0  199545.0  3097.0  229.0          0  \n",
      "class\n",
      "0.0    542274\n",
      "2.0    516437\n",
      "1.0    432146\n",
      "4.0     59812\n",
      "Name: count, dtype: int64\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "0.0    398089\n",
      "2.0    362628\n",
      "1.0    297922\n",
      "4.0     38243\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after second pca\n",
      "1409766\n",
      "class\n",
      "1.0    796810\n",
      "0.0    488630\n",
      "2.0    103696\n",
      "4.0     20630\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "1.0    796810\n",
      "0.0    488630\n",
      "2.0    103696\n",
      "4.0     20630\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "525078  525078  (396775.0, 410225.0)  POINT (396775.000 410225.000)    0.0   \n",
      "655258  655258  (354225.0, 402975.0)  POINT (354225.000 402975.000)    0.0   \n",
      "358908  358908  (397755.0, 387005.0)  POINT (397755.000 387005.000)    0.0   \n",
      "7720      7720  (359365.0, 408305.0)  POINT (359365.000 408305.000)    0.0   \n",
      "755583  755583  (375855.0, 400615.0)  POINT (375855.000 400615.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "525078  396775.0  410225.0  4524.0  1110.0          0  0.094488  0.026617   \n",
      "655258  354225.0  402975.0   269.0  1835.0          0  0.057844 -0.000401   \n",
      "358908  397755.0  387005.0  4622.0  3432.0          0  0.175381  0.020845   \n",
      "7720    359365.0  408305.0   783.0  1302.0          0  0.145316 -0.026472   \n",
      "755583  375855.0  400615.0  2432.0  2071.0          0  0.038018 -0.006095   \n",
      "\n",
      "               2  outlier  outlier_score  \n",
      "525078  0.002955      1.0       0.981375  \n",
      "655258  0.008986      1.0       0.956096  \n",
      "358908 -0.002530      1.0       0.967619  \n",
      "7720   -0.000577      1.0       0.952394  \n",
      "755583 -0.000539      1.0       0.964870  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18498/734998215.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ft['outlier_score'] = sub_clean_sample['outlier_score']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_sample number after ndvi filtering\n",
      "855264\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1261419  143950  (413015.0, 277465.0)  POINT (413015.000 277465.000)    4.0   \n",
      "1261420  143951  (413015.0, 277455.0)  POINT (413015.000 277455.000)    4.0   \n",
      "1261421  143952  (406735.0, 299035.0)  POINT (406735.000 299035.000)    4.0   \n",
      "24           24  (428205.0, 278385.0)  POINT (428205.000 278385.000)    0.0   \n",
      "25           25  (428285.0, 278405.0)  POINT (428285.000 278405.000)    0.0   \n",
      "\n",
      "                x         y     x_n     y_n  bldg_drop  \\\n",
      "1261419  413015.0  277465.0  2691.0  2995.0          0   \n",
      "1261420  413015.0  277455.0  2691.0  2996.0          0   \n",
      "1261421  406735.0  299035.0  2063.0   838.0          0   \n",
      "24       428205.0  278385.0  4210.0  2903.0          0   \n",
      "25       428285.0  278405.0  4218.0  2901.0          0   \n",
      "\n",
      "                                              sample_value  mean_ndvi  \n",
      "1261419  [0.09119949, 0.01320692, 0.055593137, -0.06678...   0.055593  \n",
      "1261420  [0.088723935, 0.013921632, 0.050260786, -0.059...   0.050261  \n",
      "1261421  [0.08924717, 0.019121883, 0.070354424, -0.0727...   0.070354  \n",
      "24       [0.104112744, 0.021047026, 0.086986855, -0.091...   0.086987  \n",
      "25       [0.0987496, 0.019869514, 0.09100334, -0.088547...   0.091003  \n",
      "class\n",
      "0.0    308087\n",
      "2.0    277183\n",
      "1.0    238236\n",
      "4.0     31758\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.072385386\n",
      "clean_sample number before pca\n",
      "855264\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1261419  143950  (413015.0, 277465.0)  POINT (413015.000 277465.000)    4.0   \n",
      "1261420  143951  (413015.0, 277455.0)  POINT (413015.000 277455.000)    4.0   \n",
      "1261421  143952  (406735.0, 299035.0)  POINT (406735.000 299035.000)    4.0   \n",
      "24           24  (428205.0, 278385.0)  POINT (428205.000 278385.000)    0.0   \n",
      "25           25  (428285.0, 278405.0)  POINT (428285.000 278405.000)    0.0   \n",
      "\n",
      "                x         y     x_n     y_n  bldg_drop  \n",
      "1261419  413015.0  277465.0  2691.0  2995.0          0  \n",
      "1261420  413015.0  277455.0  2691.0  2996.0          0  \n",
      "1261421  406735.0  299035.0  2063.0   838.0          0  \n",
      "24       428205.0  278385.0  4210.0  2903.0          0  \n",
      "25       428285.0  278405.0  4218.0  2901.0          0  \n",
      "class\n",
      "0.0    308087\n",
      "2.0    277183\n",
      "1.0    238236\n",
      "4.0     31758\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after pca\n",
      "1550669\n",
      "class\n",
      "0.0    542274\n",
      "2.0    516437\n",
      "1.0    432146\n",
      "4.0     59812\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after pca\n",
      "855264\n",
      "class\n",
      "0.0    308087\n",
      "2.0    277183\n",
      "1.0    238236\n",
      "4.0     31758\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after second pca\n",
      "853361\n",
      "class\n",
      "0.0    307618\n",
      "2.0    276688\n",
      "1.0    237634\n",
      "4.0     31421\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "0.0    307618\n",
      "2.0    276688\n",
      "1.0    237634\n",
      "4.0     31421\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "20148    20148  (401625.0, 302435.0)  POINT (401625.000 302435.000)    0.0   \n",
      "362082  362082  (403755.0, 305375.0)  POINT (403755.000 305375.000)    0.0   \n",
      "105799  105799  (402655.0, 300125.0)  POINT (402655.000 300125.000)    0.0   \n",
      "200823  200823  (415105.0, 290565.0)  POINT (415105.000 290565.000)    0.0   \n",
      "211592  211592  (397205.0, 288975.0)  POINT (397205.000 288975.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "20148   401625.0  302435.0  1552.0   498.0          0  0.170077  0.000967   \n",
      "362082  403755.0  305375.0  1765.0   204.0          0  0.224082  0.002846   \n",
      "105799  402655.0  300125.0  1655.0   729.0          0  0.222858  0.003654   \n",
      "200823  415105.0  290565.0  2900.0  1685.0          0  0.051698  0.000847   \n",
      "211592  397205.0  288975.0  1110.0  1844.0          0  0.024840  0.004606   \n",
      "\n",
      "        outlier  outlier_score  \n",
      "20148       1.0       0.969046  \n",
      "362082      1.0       0.961511  \n",
      "105799      1.0       0.972335  \n",
      "200823      1.0       0.964999  \n",
      "211592      1.0       0.969223  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18498/734998215.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ft['outlier_score'] = sub_clean_sample['outlier_score']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_sample number after second pca\n",
      "1548105\n",
      "class\n",
      "0.0    541628\n",
      "2.0    515801\n",
      "1.0    431272\n",
      "4.0     59404\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "0.0    541628\n",
      "2.0    515801\n",
      "1.0    431272\n",
      "4.0     59404\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "75813    75813  (527025.0, 191945.0)  POINT (527025.000 191945.000)    0.0   \n",
      "516178  516178  (547445.0, 171775.0)  POINT (547445.000 171775.000)    0.0   \n",
      "487358  487358  (510135.0, 178115.0)  POINT (510135.000 178115.000)    0.0   \n",
      "593849  593849  (531175.0, 170985.0)  POINT (531175.000 170985.000)    0.0   \n",
      "336265  336265  (541345.0, 167315.0)  POINT (541345.000 167315.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "75813   527025.0  191945.0  2386.0   989.0          0  0.221069 -0.001902   \n",
      "516178  547445.0  171775.0  4428.0  3006.0          0  0.229511 -0.001558   \n",
      "487358  510135.0  178115.0   697.0  2372.0          0  0.035989 -0.003441   \n",
      "593849  531175.0  170985.0  2801.0  3085.0          0  0.091845 -0.004717   \n",
      "336265  541345.0  167315.0  3818.0  3452.0          0  0.274794  0.000446   \n",
      "\n",
      "        outlier  outlier_score  \n",
      "75813       1.0       0.952446  \n",
      "516178      1.0       0.973930  \n",
      "487358      1.0       0.963054  \n",
      "593849      1.0       0.957974  \n",
      "336265      1.0       0.979546  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18498/734998215.py:319: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
      "2024-07-27 17:13:18,344 - INFO - Test scores for Greater_Manchester using XGBoost: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 27, 17, 13, 18, 318271), 'accuracy_balanced': 0.358, 'accuracy': 0.358, 'precision_macro': 0.3545556518442582, 'recall_macro': 0.358, 'f1_score_macro': 0.34639416133279344, 'precision_weighted': 0.35455565184425825, 'recall_weighted': 0.358, 'f1_score_weighted': 0.34639416133279355}\n",
      "2024-07-27 17:13:30,540 - INFO - Processed and saved results for Greater_Manchester using XGBoost\n",
      "2024-07-27 17:14:01,104 - INFO - Test scores for West_Midlands using XGBoost: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 27, 17, 14, 1, 93555), 'accuracy_balanced': 0.3892, 'accuracy': 0.3892, 'precision_macro': 0.39052670965814473, 'recall_macro': 0.3892, 'f1_score_macro': 0.3861322554153458, 'precision_weighted': 0.3905267096581448, 'recall_weighted': 0.3892, 'f1_score_weighted': 0.38613225541534574}\n",
      "2024-07-27 17:14:03,774 - INFO - Test scores for Greater_London using XGBoost: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 27, 17, 14, 3, 762419), 'accuracy_balanced': 0.35359999999999997, 'accuracy': 0.3536, 'precision_macro': 0.3561474511746622, 'recall_macro': 0.35359999999999997, 'f1_score_macro': 0.3536374949455441, 'precision_weighted': 0.3561474511746622, 'recall_weighted': 0.3536, 'f1_score_weighted': 0.3536374949455441}\n",
      "2024-07-27 17:14:03,963 - INFO - Processed and saved results for West_Midlands using XGBoost\n",
      "2024-07-27 17:14:06,453 - INFO - Processed and saved results for Greater_London using XGBoost\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 1 leaked folder objects to clean up at shutdown\n",
      "  warnings.warn(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 1 leaked folder objects to clean up at shutdown\n",
      "  warnings.warn(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 1 leaked folder objects to clean up at shutdown\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 481\u001b[0m\n\u001b[1;32m    477\u001b[0m                 logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 481\u001b[0m     main()\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll jobs done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 472\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m future_to_city \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    469\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(process_city, city, tif_names) \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m cities\n\u001b[1;32m    470\u001b[0m }\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(future_to_city):\n\u001b[0;32m--> 472\u001b[0m     city \u001b[38;5;241m=\u001b[39m future_to_city[future]\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m         future\u001b[38;5;241m.\u001b[39mresult()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "bands=['ndti','ndre','ndvi','ndwi','mndwi','glcm','B2','B3','B4','B8']\n",
    "veges = pd.read_excel(r'osm_vegetation_classes.xlsx')\n",
    "foi = ['ndti','ndre','ndvi','ndwi','mndwi','glcm']\n",
    "ind_selected = [bands.index(x) for x in foi]\n",
    "bands = foi\n",
    "classification_comparison = r'../Processing/classification_comparison.csv'\n",
    "\n",
    "\n",
    "def sample_pnts(row):\n",
    "    geometry = row['geometry']\n",
    "    bounds = geometry.bounds\n",
    "    xmin, ymin, xmax, ymax = bounds[0], bounds[1], bounds[2], bounds[3]\n",
    "    x, y = np.mgrid[xmin:xmax+10:10, ymin:ymax+10:10]\n",
    "    x, y = np.vstack([x.ravel(), y.ravel()])\n",
    "    p = pd.DataFrame(list(zip(x, y)))\n",
    "    p[0] = np.floor((p[0] - row['xmin']) / row['xres']) * row['xres'] + row['xmin'] + row['xres'] / 2\n",
    "    p[1] = np.floor((p[1] - row['ymin']) / row['yres']) * row['yres'] + row['ymin'] + row['yres'] / 2\n",
    "    p['pnt'] = list(set(zip(p[0], p[1])))\n",
    "    p['pnt'] = p['pnt'].apply(Point)\n",
    "    p = gpd.GeoDataFrame(p['pnt'], geometry='pnt', crs=CRS.from_epsg(27700))\n",
    "    p = p[p.within(geometry)]\n",
    "    return p['pnt'].apply(lambda x: [x.x, x.y]).values\n",
    "\n",
    "\n",
    "def sample_raster(row, img_array):\n",
    "    y = int(row['y_n'])\n",
    "    x = int(row['x_n'])\n",
    "    if 0 <= y < img_array.shape[1] and 0 <= x < img_array.shape[2]:\n",
    "        res = img_array[:, y, x]\n",
    "    else:\n",
    "        res = np.nan\n",
    "    if np.isnan(res).any():\n",
    "        res = np.nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def extract_OSM_polygons(OSM, city):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile = shapefile.to_crs({'init': 'epsg:27700'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:, 'area_length'] = (building.area / building.length).values\n",
    "    # building.loc[:, 'general'] = 'bldg'\n",
    "    building.loc[:, 'general'] = 5\n",
    "    shapefile = shapefile[shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0, len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    # for i in veges.index:\n",
    "    sub = pd.DataFrame()\n",
    "    sub['geometry'] = shapefile.loc[shapefile['general'] == 'vegetation', 'geometry']\n",
    "    sub['FID'] = shapefile.loc[shapefile['general'] == 'vegetation', 'FID']\n",
    "    sub['key'] = 'general'\n",
    "    sub['value'] = 'vegetation'\n",
    "    sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "    sub['tag'] = shapefile.loc[shapefile['general'] == 'vegetation', 'tag']\n",
    "    if len(sub.index) > 0:\n",
    "        one_city = pd.concat([one_city, sub])\n",
    "    one_city['general'] = 'vegetation'\n",
    "        \n",
    "    one_city['general'] = one_city.apply(\n",
    "        lambda x: x['tag'] if x['general'] == 'vegetation' else 5, axis=1)\n",
    "\n",
    "    if len(one_city.index) > 0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city, geometry='geometry', crs=CRS.from_epsg(27700))\n",
    "        one_city_gdf.loc[:, 'shape_index'] = (one_city_gdf.length / (4 * np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[\n",
    "            (one_city_gdf.area <= one_city_gdf.area.quantile(0.975)) &\n",
    "            (one_city_gdf.area >= one_city_gdf.area.quantile(0.025)) &\n",
    "            (one_city_gdf['shape_index'] < one_city_gdf['shape_index'].quantile(0.9))\n",
    "        ]\n",
    "\n",
    "        background = shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])), ['geometry', 'FID']]\n",
    "        background['general'] = 5\n",
    "        background.loc[:, 'shape_index'] = (background.length / (4 * np.sqrt(background.area))).values\n",
    "        background = background.loc[\n",
    "            (background.area <= background.area.quantile(0.975)) &\n",
    "            (background.area >= background.area.quantile(0.025)) &\n",
    "            (background['shape_index'] < background['shape_index'].quantile(0.9))\n",
    "        ]\n",
    "        one_city_gdf = pd.concat([one_city_gdf, building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf, background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general', 'geometry', 'shape_index']], geometry='geometry',\n",
    "                                        crs=CRS.from_epsg(27700))\n",
    "    \n",
    "    one_city_gdf.to_file(driver='ESRI Shapefile', filename=r\"../Processing/polygon_%s.shp\" % city)\n",
    "    return one_city_gdf\n",
    "\n",
    "#To debug XGB classifier's strict integer rule\n",
    "custom_mapping = {0.0: 0, 1.0: 1, 2.0: 2, 4.0: 3}\n",
    "\n",
    "def preprocess_labels(y, mapping):\n",
    "    y_mapped = y.map(mapping)\n",
    "    return y_mapped.astype(int)\n",
    "\n",
    "def grid_search_wrapper(refit_score, clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params, city):\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=False,\n",
    "                               n_jobs=4, verbose=0)\n",
    "\n",
    "    #To debug XGB classifier's strict integer rule\n",
    "    if isinstance(clf, XGBClassifier):\n",
    "        y_train = preprocess_labels(y_train, custom_mapping)\n",
    "        y_test = preprocess_labels(y_test, custom_mapping)\n",
    "    \n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city': city, 'datetime': datetime.datetime.now(),\n",
    "                         'accuracy_balanced': balanced_accuracy_score(y_test, y_pred),\n",
    "                         'accuracy': accuracy_score(y_test, y_pred),\n",
    "                         'precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
    "                         'recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
    "                         'f1_score_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "                         'precision_weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "                         'recall_weighted': recall_score(y_test, y_pred, average='weighted'),\n",
    "                         'f1_score_weighted': f1_score(y_test, y_pred, average='weighted')}\n",
    "\n",
    "\n",
    "def generate_sample(one_city_gdf, img, city):\n",
    "    logging.info(\"Starting sample generation...\")\n",
    "    global bands\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts'] = one_city_gdf.apply(sample_pnts, axis=1)\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        logging.debug(f\"Processing class: {i}\")\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general'] == i, 'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0], sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0], sample_df[1]), crs=\"epsg:27700\")\n",
    "        sample_gdf['class'] = i\n",
    "        logging.debug(f\"Generated {len(sample_gdf)} points for class {i}.\")\n",
    "        all_sample = pd.concat([all_sample, sample_gdf])\n",
    "\n",
    "    print(\"all_sample values before filtering\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "\n",
    "    value_counts = all_sample['class'].value_counts()    \n",
    "    classes_to_exclude = value_counts[value_counts < 500].index\n",
    "    all_sample = all_sample[~all_sample['class'].isin(classes_to_exclude)]\n",
    "\n",
    "    print(\"all_sample values after filtering\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    all_sample['x'] = all_sample.geometry.x\n",
    "    all_sample['y'] = all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0]) / geoTransform[1] - 0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3]) / geoTransform[5] - 0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    print(\"all_sample values second\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "\n",
    "    land_sample = all_sample[all_sample['class'] != 5].copy()\n",
    "    print(\"land_sample\")\n",
    "    print(len(land_sample))\n",
    "    print(len(land_sample.drop_duplicates(subset=['x', 'y'], keep=False, inplace=False)))\n",
    "    land_sample.drop_duplicates(subset=['x', 'y'], keep='first', inplace=True)\n",
    "    clean_sample = pd.concat([land_sample, all_sample[all_sample['class'] != 5].copy()])\n",
    "    clean_sample['bldg_drop'] = 0\n",
    "    clean_sample.loc[clean_sample['class'] == 5, 'bldg_drop'] = 1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    # including green roof or other urban green infra - may need to fix\n",
    "    clean_sample.drop_duplicates(subset=['x', 'y'], keep='first', inplace=True)\n",
    "\n",
    "    img_array = np.array(raster.ReadAsArray())\n",
    "    img_array = img_array[ind_selected, :, :]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d = img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d, x_d * y_d)\n",
    "    img_array[:, np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d, x_d, y_d)\n",
    "\n",
    "    print(\"clean_sample number before ndvi filtering\")\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x, img_array), axis=1)\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x: x[bands.index('ndvi')])\n",
    "\n",
    "    clean_sample.loc[(clean_sample['class'] != 5)\n",
    "                     & (clean_sample['mean_ndvi'] <= 0.05), 'sample_value'] = np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "\n",
    "    print(\"clean_sample number after ndvi filtering\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    v_median = clean_sample.loc[(clean_sample['class'] != 5), 'mean_ndvi'].median()\n",
    "    print(\"v_median\")\n",
    "    print(v_median)\n",
    "    clean_sample.loc[(clean_sample['class'] == 5)\n",
    "                     & (clean_sample['mean_ndvi'] >= v_median), 'sample_value'] = np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi', 'sample_value'], axis=1)\n",
    "\n",
    "    print(\"clean_sample number before pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    for i in range(0, img_array.shape[0]):\n",
    "        v = img_array[i, :, :]\n",
    "        img_array[i, :, :] = (v - np.nanmin(v)) / (np.nanmax(v) - np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1] * img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var = np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3) * 100)\n",
    "    n_pc = sum(var <= 90) + 1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce, pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc, :, :])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0], img_reduce.shape[1] * img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0], img_reduce.shape[1], img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca, img_array_pca_valid, img_reduce_re, img_reduce, pca_reduce\n",
    "\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x, img_array), axis=1)\n",
    "    PCAs = list(range(0, n_pc))\n",
    "    for PC in PCAs:\n",
    "        i = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x: x[i])\n",
    "    clean_sample = clean_sample.drop('sample_value', axis=1)\n",
    "\n",
    "    print(\"clean_sample number after pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class'] == i, PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class'] == i, 'outlier'] = y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class'] == i, 'outlier_score'] = (outlier_score - outlier_score.min()) / (\n",
    "                    outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier'] != -1]\n",
    "\n",
    "    print(\"clean_sample number after second pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # Count the number of samples for each class\n",
    "    class_counts = clean_sample['class'].value_counts()\n",
    "    print(\"Class counts:\", class_counts)\n",
    "    \n",
    "    # Determine the sample size\n",
    "    n_sample = int(min(class_counts))\n",
    "    if n_sample >= 2500:\n",
    "        n_sample = 2500\n",
    "    \n",
    "    sampled_dfs = []\n",
    "    for class_label in range(6):\n",
    "        if class_label in class_counts:\n",
    "            sampled_dfs.append(clean_sample[clean_sample['class'] == class_label].sample(n=min(n_sample, class_counts[class_label]), random_state=0))\n",
    "    \n",
    "    sub_clean_sample = pd.concat(sampled_dfs)\n",
    "    \n",
    "    ext = sub_clean_sample[['class', 'geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:27700'})\n",
    "    ext.to_file(r\"../Processing/all_sample_%s.shp\" % city)\n",
    "    raster = None\n",
    "    return sub_clean_sample, img_array, PCAs\n",
    "\n",
    "\n",
    "def split_sample(sub_clean_sample, PCAs, city):\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "    targets = sub_clean_sample['class']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets, random_state=0)\n",
    "\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "\n",
    "    train_exp = X_train.merge(sub_clean_sample[['class', 'geometry']], left_index=True, right_index=True)\n",
    "    test_exp = X_test.merge(sub_clean_sample[['class', 'geometry']], left_index=True, right_index=True)\n",
    "\n",
    "    gpd.GeoDataFrame(train_exp[['class', 'geometry']], geometry='geometry').to_file(driver='ESRI Shapefile',\n",
    "                                                                                     filename=r\"../Processing/train_sample_%s.shp\" % city)\n",
    "    gpd.GeoDataFrame(test_exp[['class', 'geometry']], geometry='geometry').to_file(driver='ESRI Shapefile',\n",
    "                                                                                   filename=r\"../Processing/test_sample_%s.shp\" % city)\n",
    "    return X_train, X_test, y_train, y_test, X_train_weight\n",
    "\n",
    "\n",
    "def process_city(city, tif_names):\n",
    "    try:\n",
    "        logging.info(f'Starting processing for {city}')\n",
    "        OSM = r'../Code/%s_OSM_combined.shp' % city\n",
    "        img = r'../Sample_image/LA_%s_2023.tif' % tif_names[city]\n",
    "\n",
    "        if not os.path.exists(OSM):\n",
    "            logging.error(f'OSM file does not exist: {OSM}')\n",
    "            return\n",
    "        if not os.path.exists(img):\n",
    "            logging.error(f'Image file does not exist: {img}')\n",
    "            return\n",
    "\n",
    "        one_city_gdf = extract_OSM_polygons(OSM, city)  # get OSM polygons\n",
    "        print(\"one_city_gdf after extract\")\n",
    "        print(len(one_city_gdf))\n",
    "        sub_clean_sample, img_array, PCAs = generate_sample(one_city_gdf, img, city)  # generate random samples\n",
    "        print(\"sub_clean_sample\")\n",
    "        print(len(sub_clean_sample))\n",
    "        print(sub_clean_sample.head())\n",
    "\n",
    "        X_train, X_test, y_train, y_test, X_train_weight = split_sample(sub_clean_sample, PCAs, city)  # sample values, train test split\n",
    "        scorers = {\n",
    "            'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "            'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_score_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
    "            'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "            'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "            'f1_score_weighted': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        fit_params = {'sample_weight': X_train_weight}\n",
    "\n",
    "        classifiers = {\n",
    "            # 'RandomForest': RandomForestClassifier(),\n",
    "            # 'LogisticRegression': LogisticRegression(),\n",
    "            # 'KNeighbors': KNeighborsClassifier(),\n",
    "            # 'GradientBoosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier(),\n",
    "            # 'SVM': SVC()\n",
    "        }\n",
    "        \n",
    "        param_grids = {\n",
    "            # 'RandomForest': {'n_estimators': [100, 200, 300],\n",
    "            #                  'max_features': [None, 'sqrt', 'log2'],\n",
    "            #                  'min_samples_split': [2, 5, 10],\n",
    "            #                  'random_state': [0]},\n",
    "            # 'LogisticRegression': {'C': [0.1, 1, 10, 100],\n",
    "            #                        'solver': ['lbfgs'],\n",
    "            #                        'max_iter': [100, 200, 300],\n",
    "            #                        'class_weight': ['balanced']},\n",
    "            # 'KNeighbors': {'n_neighbors': [3, 5, 7, 9],\n",
    "            #                'weights': ['uniform', 'distance'],\n",
    "            #                'algorithm': ['ball_tree', 'kd_tree', 'brute']},\n",
    "            # 'GradientBoosting': {'n_estimators': [100, 200, 300],\n",
    "            #                      'learning_rate': [0.01, 0.1, 0.2],\n",
    "            #                      'max_depth': [3, 4, 5],\n",
    "            #                      'random_state': [0]},\n",
    "            'XGBoost': {'n_estimators': [100, 200, 300],\n",
    "                                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                                'max_depth': [3, 4, 5],\n",
    "                                'random_state': [0]},\n",
    "            # 'SVM': {'C': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "            #         'gamma': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "            #         'class_weight': ['balanced']}\n",
    "        }\n",
    "\n",
    "        for clf_name in classifiers:\n",
    "            clf = classifiers[clf_name]\n",
    "            param_grid = param_grids[clf_name]\n",
    "            grid_search_clf, test_scores = grid_search_wrapper('f1_score_weighted', clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params, city)\n",
    "\n",
    "            # save testing accuracy\n",
    "            logging.info(f'Test scores for {city} using {clf_name}: {test_scores}')\n",
    "\n",
    "            lock = FileLock(f\"{classification_comparison}.lock\")\n",
    "            with lock:\n",
    "                with open(classification_comparison, 'a') as csv_file:\n",
    "                    writer = csv.writer(csv_file, delimiter=',', lineterminator='\\n')\n",
    "                    writer.writerow([f\"{city}\"] + [clf_name] + list(test_scores.values()))\n",
    "\n",
    "            # save original img\n",
    "            img_array2 = np.copy(img_array)\n",
    "            img_re = img_array2.reshape((img_array2.shape[0], img_array2.shape[1] * img_array2.shape[2])).transpose()\n",
    "            img_pre = np.copy(img_re[~np.isnan(img_re).any(axis=1)])\n",
    "            img_pre = grid_search_clf.predict(img_pre)\n",
    "            img_pre = img_pre.astype(np.int16)\n",
    "            res = img_re[:, 0]\n",
    "            res[~np.isnan(res)] = img_pre\n",
    "            res[np.isnan(res)] = -32768\n",
    "            res = res.reshape(img_array[0, :, :].shape)\n",
    "            res = res.astype(np.int16)\n",
    "\n",
    "            org_img = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "            meta = {\n",
    "                'driver': 'GTiff',\n",
    "                'dtype': 'int16',\n",
    "                'nodata': -32768,\n",
    "                'width': res.shape[1],\n",
    "                'height': res.shape[0],\n",
    "                'count': 1,\n",
    "                'crs': CRS(\"EPSG:27700\"),\n",
    "                'transform': Affine(10, 0.0, org_img.GetGeoTransform()[0], 0, -10, org_img.GetGeoTransform()[-3]),\n",
    "                'compress': 'lzw',\n",
    "                'interleave': 'pixel'\n",
    "            }\n",
    "\n",
    "            result_path = r'../Results/{}_{}_{}.tif'.format(city, clf_name, datetime.datetime.now().strftime('%m%d_%H'))\n",
    "            with rio.open(result_path, 'w', **meta) as dst:\n",
    "                dst.write(res, 1)\n",
    "\n",
    "            logging.info(f'Processed and saved results for {city} using {clf_name}')\n",
    "\n",
    "        del img_array\n",
    "        org_img = None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    tif_names = {\"Greater_Manchester\": 'Manchester', \"Greater_London\": 'London', \"West_Midlands\": 'Westmidlands'}\n",
    "    cities = [\"Greater_Manchester\", \"West_Midlands\", \"Greater_London\"]\n",
    "\n",
    "    max_workers = 3\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # future_to_city = {\n",
    "        #     executor.submit(process_city, city, tif_names) for city in cities\n",
    "        # }\n",
    "        future_to_city = {executor.submit(process_city, city, tif_names): city for city in cities}\n",
    "        for future in concurrent.futures.as_completed(future_to_city):\n",
    "            city = future_to_city[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                logging.info(f'Completed processing for {city}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "print('All jobs done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49924b2e-6e0c-476b-a3ed-a2344ddd1a9d",
   "metadata": {},
   "source": [
    "## GI classification to 3 levels (diminished from 5)\n",
    "- Including Green space, agriculture, Urban green infra\n",
    "- clean_sample.drop_duplicates(subset=['x', 'y'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289e6000-d371-4e75-b66d-bdf320e956dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 14:39:55,927 - INFO - Starting processing for Greater_Manchester\n",
      "2024-07-28 14:39:55,928 - INFO - Starting processing for West_Midlands\n",
      "2024-07-28 14:39:55,928 - INFO - Starting processing for Greater_London\n",
      "2024-07-28 14:40:17,619 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "27436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 14:40:20,537 - INFO - Starting sample generation...\n",
      "2024-07-28 14:40:42,279 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "27602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 14:40:45,228 - INFO - Starting sample generation...\n",
      "2024-07-28 14:41:54,411 - WARNING - Normalized/laundered field name: 'shape_index' to 'shape_inde'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_city_gdf after extract\n",
      "12794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 14:41:55,804 - INFO - Starting sample generation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sample values before filtering\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    241863\n",
      "4.0    114473\n",
      "5.0     58012\n",
      "3.0       419\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    356336\n",
      "5.0     58012\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "1.0    990831\n",
      "0.0    857661\n",
      "2.0    356336\n",
      "5.0     58012\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "2204828\n",
      "2041779\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "1.0    986287\n",
      "0.0    842416\n",
      "2.0    213076\n",
      "5.0     57413\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after ndvi filtering\n",
      "1429731\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1377001  519340  (383035.0, 411895.0)  POINT (383035.000 411895.000)    1.0   \n",
      "1377016  519355  (382995.0, 411835.0)  POINT (382995.000 411835.000)    1.0   \n",
      "1377015  519354  (383065.0, 411845.0)  POINT (383065.000 411845.000)    1.0   \n",
      "1377014  519353  (383035.0, 411835.0)  POINT (383035.000 411835.000)    1.0   \n",
      "1377013  519352  (383025.0, 411895.0)  POINT (383025.000 411895.000)    1.0   \n",
      "\n",
      "                x         y     x_n    y_n  bldg_drop  \\\n",
      "1377001  383035.0  411895.0  3150.0  943.0          0   \n",
      "1377016  382995.0  411835.0  3146.0  949.0          0   \n",
      "1377015  383065.0  411845.0  3153.0  948.0          0   \n",
      "1377014  383035.0  411835.0  3150.0  949.0          0   \n",
      "1377013  383025.0  411895.0  3149.0  943.0          0   \n",
      "\n",
      "                                              sample_value  mean_ndvi  \n",
      "1377001  [0.10345907, 0.019747948, 0.071993224, -0.0750...   0.071993  \n",
      "1377016  [0.11201397, 0.020777155, 0.06466227, -0.07166...   0.064662  \n",
      "1377015  [0.09872934, 0.016671954, 0.06038201, -0.06970...   0.060382  \n",
      "1377014  [0.102840774, 0.01808876, 0.06288075, -0.06955...   0.062881  \n",
      "1377013  [0.10004026, 0.021084225, 0.0677078, -0.075059...   0.067708  \n",
      "class\n",
      "1.0    795195\n",
      "0.0    483204\n",
      "2.0     96165\n",
      "5.0     55167\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.06455253\n",
      "clean_sample number before pca\n",
      "1427597\n",
      "          index           coordinates                       geometry  class  \\\n",
      "1377001  519340  (383035.0, 411895.0)  POINT (383035.000 411895.000)    1.0   \n",
      "1377016  519355  (382995.0, 411835.0)  POINT (382995.000 411835.000)    1.0   \n",
      "1377015  519354  (383065.0, 411845.0)  POINT (383065.000 411845.000)    1.0   \n",
      "1377014  519353  (383035.0, 411835.0)  POINT (383035.000 411835.000)    1.0   \n",
      "1377013  519352  (383025.0, 411895.0)  POINT (383025.000 411895.000)    1.0   \n",
      "\n",
      "                x         y     x_n    y_n  bldg_drop  \n",
      "1377001  383035.0  411895.0  3150.0  943.0          0  \n",
      "1377016  382995.0  411835.0  3146.0  949.0          0  \n",
      "1377015  383065.0  411845.0  3153.0  948.0          0  \n",
      "1377014  383035.0  411835.0  3150.0  949.0          0  \n",
      "1377013  383025.0  411895.0  3149.0  943.0          0  \n",
      "class\n",
      "1.0    795195\n",
      "0.0    483204\n",
      "2.0     96165\n",
      "5.0     53033\n",
      "Name: count, dtype: int64\n",
      "all_sample values before filtering\n",
      "class\n",
      "2.0    683450\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "4.0    186044\n",
      "5.0     35668\n",
      "3.0      7497\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "2.0    876991\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "5.0     35668\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "2.0    876991\n",
      "0.0    613484\n",
      "1.0    462429\n",
      "5.0     35668\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "1952904\n",
      "1589121\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "2.0    580907\n",
      "0.0    549956\n",
      "1.0    458258\n",
      "5.0     35047\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after pca\n",
      "1427597\n",
      "class\n",
      "1.0    795195\n",
      "0.0    483204\n",
      "2.0     96165\n",
      "5.0     53033\n",
      "Name: count, dtype: int64\n",
      "all_sample values before filtering\n",
      "class\n",
      "2.0    418052\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "4.0    143958\n",
      "5.0     80459\n",
      "3.0         6\n",
      "Name: count, dtype: int64\n",
      "all_sample values after filtering\n",
      "class\n",
      "2.0    562010\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "5.0     80459\n",
      "Name: count, dtype: int64\n",
      "all_sample values second\n",
      "class\n",
      "2.0    562010\n",
      "0.0    401196\n",
      "1.0    298221\n",
      "5.0     80459\n",
      "Name: count, dtype: int64\n",
      "land_sample\n",
      "1261427\n",
      "946360\n",
      "clean_sample number after ndvi filtering\n",
      "1448889\n",
      "         index           coordinates                       geometry  class  \\\n",
      "1143482  67569  (518915.0, 166055.0)  POINT (518915.000 166055.000)    2.0   \n",
      "1143494  67581  (518935.0, 166065.0)  POINT (518935.000 166065.000)    2.0   \n",
      "1143493  67580  (518905.0, 166055.0)  POINT (518905.000 166055.000)    2.0   \n",
      "1143492  67579  (518895.0, 166075.0)  POINT (518895.000 166075.000)    2.0   \n",
      "1143491  67578  (518945.0, 166085.0)  POINT (518945.000 166085.000)    2.0   \n",
      "\n",
      "                x         y     x_n     y_n  bldg_drop  \\\n",
      "1143482  518915.0  166055.0  1575.0  3578.0          0   \n",
      "1143494  518935.0  166065.0  1577.0  3577.0          0   \n",
      "1143493  518905.0  166055.0  1574.0  3578.0          0   \n",
      "1143492  518895.0  166075.0  1573.0  3576.0          0   \n",
      "1143491  518945.0  166085.0  1578.0  3575.0          0   \n",
      "\n",
      "                                              sample_value  mean_ndvi  \n",
      "1143482  [0.10042319, 0.020196121, 0.08635303, -0.09302...   0.086353  \n",
      "1143494  [0.10133004, 0.019543206, 0.08420622, -0.09532...   0.084206  \n",
      "1143493  [0.10042319, 0.024033764, 0.09849037, -0.09332...   0.098490  \n",
      "1143492  [0.09830637, 0.018963972, 0.082311735, -0.0878...   0.082312  \n",
      "1143491  [0.09680435, 0.018370705, 0.07191105, -0.08699...   0.071911  \n",
      "class\n",
      "0.0    497515\n",
      "2.0    487069\n",
      "1.0    429603\n",
      "5.0     34702\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.07388607\n",
      "clean_sample number before pca\n",
      "1447260\n",
      "         index           coordinates                       geometry  class  \\\n",
      "1143482  67569  (518915.0, 166055.0)  POINT (518915.000 166055.000)    2.0   \n",
      "1143494  67581  (518935.0, 166065.0)  POINT (518935.000 166065.000)    2.0   \n",
      "1143493  67580  (518905.0, 166055.0)  POINT (518905.000 166055.000)    2.0   \n",
      "1143492  67579  (518895.0, 166075.0)  POINT (518895.000 166075.000)    2.0   \n",
      "1143491  67578  (518945.0, 166085.0)  POINT (518945.000 166085.000)    2.0   \n",
      "\n",
      "                x         y     x_n     y_n  bldg_drop  \n",
      "1143482  518915.0  166055.0  1575.0  3578.0          0  \n",
      "1143494  518935.0  166065.0  1577.0  3577.0          0  \n",
      "1143493  518905.0  166055.0  1574.0  3578.0          0  \n",
      "1143492  518895.0  166075.0  1573.0  3576.0          0  \n",
      "1143491  518945.0  166085.0  1578.0  3575.0          0  \n",
      "class\n",
      "0.0    497515\n",
      "2.0    487069\n",
      "1.0    429603\n",
      "5.0     33073\n",
      "Name: count, dtype: int64\n",
      "clean_sample number before ndvi filtering\n",
      "class\n",
      "2.0    359020\n",
      "1.0    296401\n",
      "0.0    290939\n",
      "5.0     79739\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after ndvi filtering\n",
      "815799\n",
      "        index           coordinates                       geometry  class  \\\n",
      "754386  54969  (415945.0, 292585.0)  POINT (415945.000 292585.000)    2.0   \n",
      "754372  54955  (415965.0, 292675.0)  POINT (415965.000 292675.000)    2.0   \n",
      "754373  54956  (415995.0, 292765.0)  POINT (415995.000 292765.000)    2.0   \n",
      "754374  54957  (415915.0, 292665.0)  POINT (415915.000 292665.000)    2.0   \n",
      "754375  54958  (415975.0, 292725.0)  POINT (415975.000 292725.000)    2.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop  \\\n",
      "754386  415945.0  292585.0  2984.0  1483.0          0   \n",
      "754372  415965.0  292675.0  2986.0  1474.0          0   \n",
      "754373  415995.0  292765.0  2989.0  1465.0          0   \n",
      "754374  415915.0  292665.0  2981.0  1475.0          0   \n",
      "754375  415975.0  292725.0  2987.0  1469.0          0   \n",
      "\n",
      "                                             sample_value  mean_ndvi  \n",
      "754386  [0.09777664, 0.020584524, 0.081493944, -0.0872...   0.081494  \n",
      "754372  [0.09890735, 0.023410272, 0.07729128, -0.08282...   0.077291  \n",
      "754373  [0.095520064, 0.02257116, 0.06546754, -0.07988...   0.065468  \n",
      "754374  [0.10172716, 0.023503624, 0.07617234, -0.08187...   0.076172  \n",
      "754375  [0.10672732, 0.022869477, 0.06881464, -0.08632...   0.068815  \n",
      "class\n",
      "2.0    273177\n",
      "1.0    236861\n",
      "0.0    229290\n",
      "5.0     76471\n",
      "Name: count, dtype: int64\n",
      "v_median\n",
      "0.07234748\n",
      "clean_sample number after second pca\n",
      "1425725\n",
      "class\n",
      "1.0    794383\n",
      "0.0    482687\n",
      "2.0     95904\n",
      "5.0     52751\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "1.0    794383\n",
      "0.0    482687\n",
      "2.0     95904\n",
      "5.0     52751\n",
      "Name: count, dtype: int64\n",
      "clean_sample number before pca\n",
      "812423\n",
      "        index           coordinates                       geometry  class  \\\n",
      "754386  54969  (415945.0, 292585.0)  POINT (415945.000 292585.000)    2.0   \n",
      "754372  54955  (415965.0, 292675.0)  POINT (415965.000 292675.000)    2.0   \n",
      "754373  54956  (415995.0, 292765.0)  POINT (415995.000 292765.000)    2.0   \n",
      "754374  54957  (415915.0, 292665.0)  POINT (415915.000 292665.000)    2.0   \n",
      "754375  54958  (415975.0, 292725.0)  POINT (415975.000 292725.000)    2.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop  \n",
      "754386  415945.0  292585.0  2984.0  1483.0          0  \n",
      "754372  415965.0  292675.0  2986.0  1474.0          0  \n",
      "754373  415995.0  292765.0  2989.0  1465.0          0  \n",
      "754374  415915.0  292665.0  2981.0  1475.0          0  \n",
      "754375  415975.0  292725.0  2987.0  1469.0          0  \n",
      "class\n",
      "2.0    273177\n",
      "1.0    236861\n",
      "0.0    229290\n",
      "5.0     73095\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "588897  588897  (363825.0, 409505.0)  POINT (363825.000 409505.000)    0.0   \n",
      "442985  442985  (393815.0, 413655.0)  POINT (393815.000 413655.000)    0.0   \n",
      "836673  836673  (359095.0, 403385.0)  POINT (359095.000 403385.000)    0.0   \n",
      "350388  350388  (398935.0, 389335.0)  POINT (398935.000 389335.000)    0.0   \n",
      "362794  362794  (396885.0, 386685.0)  POINT (396885.000 386685.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "588897  363825.0  409505.0  1229.0  1182.0          0  0.115350 -0.008304   \n",
      "442985  393815.0  413655.0  4228.0   767.0          0  0.153841  0.019071   \n",
      "836673  359095.0  403385.0   756.0  1794.0          0  0.106831 -0.010287   \n",
      "350388  398935.0  389335.0  4740.0  3199.0          0  0.053799  0.008425   \n",
      "362794  396885.0  386685.0  4535.0  3464.0          0  0.129359  0.017352   \n",
      "\n",
      "               2  outlier  outlier_score  \n",
      "588897  0.007659      1.0       0.967990  \n",
      "442985 -0.000270      1.0       0.958491  \n",
      "836673  0.014413      1.0       0.966865  \n",
      "350388 -0.005314      1.0       0.966146  \n",
      "362794  0.002899      1.0       0.959995  \n",
      "clean_sample number after pca\n",
      "1447260\n",
      "class\n",
      "0.0    497515\n",
      "2.0    487069\n",
      "1.0    429603\n",
      "5.0     33073\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after pca\n",
      "812423\n",
      "class\n",
      "2.0    273177\n",
      "1.0    236861\n",
      "0.0    229290\n",
      "5.0     73095\n",
      "Name: count, dtype: int64\n",
      "clean_sample number after second pca\n",
      "810386\n",
      "class\n",
      "2.0    272648\n",
      "1.0    236251\n",
      "0.0    228804\n",
      "5.0     72683\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "2.0    272648\n",
      "1.0    236251\n",
      "0.0    228804\n",
      "5.0     72683\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "152889  152889  (418295.0, 282625.0)  POINT (418295.000 282625.000)    0.0   \n",
      "395523  395523  (398285.0, 300735.0)  POINT (398285.000 300735.000)    0.0   \n",
      "218278  218278  (403275.0, 291225.0)  POINT (403275.000 291225.000)    0.0   \n",
      "247099  247099  (410135.0, 279725.0)  POINT (410135.000 279725.000)    0.0   \n",
      "10406    10406  (414525.0, 298375.0)  POINT (414525.000 298375.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "152889  418295.0  282625.0  3219.0  2479.0          0  0.193165  0.001663   \n",
      "395523  398285.0  300735.0  1218.0   668.0          0  0.085344  0.008292   \n",
      "218278  403275.0  291225.0  1717.0  1619.0          0  0.196379  0.006648   \n",
      "247099  410135.0  279725.0  2403.0  2769.0          0  0.104426  0.001179   \n",
      "10406   414525.0  298375.0  2842.0   904.0          0  0.344259 -0.000649   \n",
      "\n",
      "        outlier  outlier_score  \n",
      "152889      1.0       0.971790  \n",
      "395523      1.0       0.948561  \n",
      "218278      1.0       0.964732  \n",
      "247099      1.0       0.937946  \n",
      "10406       1.0       0.952867  \n",
      "clean_sample number after second pca\n",
      "1444784\n",
      "class\n",
      "0.0    496889\n",
      "2.0    486468\n",
      "1.0    428742\n",
      "5.0     32685\n",
      "Name: count, dtype: int64\n",
      "Class counts: class\n",
      "0.0    496889\n",
      "2.0    486468\n",
      "1.0    428742\n",
      "5.0     32685\n",
      "Name: count, dtype: int64\n",
      "sub_clean_sample\n",
      "10000\n",
      "         index           coordinates                       geometry  class  \\\n",
      "329140  329140  (551975.0, 193845.0)  POINT (551975.000 193845.000)    0.0   \n",
      "22158    22158  (559815.0, 186835.0)  POINT (559815.000 186835.000)    0.0   \n",
      "246637  246637  (522845.0, 173045.0)  POINT (522845.000 173045.000)    0.0   \n",
      "544599  544599  (548535.0, 192675.0)  POINT (548535.000 192675.000)    0.0   \n",
      "415110  415110  (545285.0, 176795.0)  POINT (545285.000 176795.000)    0.0   \n",
      "\n",
      "               x         y     x_n     y_n  bldg_drop         0         1  \\\n",
      "329140  551975.0  193845.0  4881.0   799.0          0  0.379399  0.004430   \n",
      "22158   559815.0  186835.0  5665.0  1500.0          0  0.254436  0.000485   \n",
      "246637  522845.0  173045.0  1968.0  2879.0          0  0.136107 -0.005674   \n",
      "544599  548535.0  192675.0  4537.0   916.0          0  0.409393 -0.000345   \n",
      "415110  545285.0  176795.0  4212.0  2504.0          0  0.155467 -0.002103   \n",
      "\n",
      "        outlier  outlier_score  \n",
      "329140      1.0       0.974146  \n",
      "22158       1.0       0.968657  \n",
      "246637      1.0       0.967316  \n",
      "544599      1.0       0.956464  \n",
      "415110      1.0       0.969092  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:20:13,227 - INFO - Test scores for Greater_Manchester using SVM: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 15, 20, 13, 217159), 'accuracy_balanced': 0.5740000000000001, 'accuracy': 0.574, 'precision_macro': 0.5950866671163494, 'recall_macro': 0.5740000000000001, 'f1_score_macro': 0.5798548243976104, 'precision_weighted': 0.5950866671163494, 'recall_weighted': 0.574, 'f1_score_weighted': 0.5798548243976104}\n",
      "2024-07-28 15:38:25,871 - INFO - Test scores for Greater_London using SVM: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 15, 38, 25, 860838), 'accuracy_balanced': 0.5472, 'accuracy': 0.5472, 'precision_macro': 0.5670141323933057, 'recall_macro': 0.5472, 'f1_score_macro': 0.5548575582056354, 'precision_weighted': 0.5670141323933056, 'recall_weighted': 0.5472, 'f1_score_weighted': 0.5548575582056354}\n",
      "2024-07-28 15:48:55,684 - INFO - Test scores for West_Midlands using SVM: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 15, 48, 55, 673263), 'accuracy_balanced': 0.562, 'accuracy': 0.562, 'precision_macro': 0.5825095577532196, 'recall_macro': 0.562, 'f1_score_macro': 0.5662902863049627, 'precision_weighted': 0.5825095577532197, 'recall_weighted': 0.562, 'f1_score_weighted': 0.5662902863049627}\n",
      "2024-07-28 16:05:42,822 - INFO - Processed and saved results for Greater_Manchester using SVM\n",
      "2024-07-28 16:08:19,995 - INFO - Test scores for Greater_Manchester using RandomForest: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 16, 8, 19, 984869), 'accuracy_balanced': 0.5572, 'accuracy': 0.5572, 'precision_macro': 0.5713088081927956, 'recall_macro': 0.5572, 'f1_score_macro': 0.5628875324472464, 'precision_weighted': 0.5713088081927956, 'recall_weighted': 0.5572, 'f1_score_weighted': 0.5628875324472463}\n",
      "2024-07-28 16:11:11,391 - INFO - Processed and saved results for Greater_Manchester using RandomForest\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2024-07-28 16:11:14,517 - INFO - Test scores for Greater_Manchester using LogisticRegression: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 16, 11, 14, 507927), 'accuracy_balanced': 0.5496000000000001, 'accuracy': 0.5496, 'precision_macro': 0.5495687238048084, 'recall_macro': 0.5496000000000001, 'f1_score_macro': 0.5487785893324268, 'precision_weighted': 0.5495687238048084, 'recall_weighted': 0.5496, 'f1_score_weighted': 0.5487785893324267}\n",
      "2024-07-28 16:11:15,250 - INFO - Processed and saved results for Greater_Manchester using LogisticRegression\n",
      "2024-07-28 16:11:16,314 - INFO - Test scores for Greater_Manchester using KNeighbors: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 16, 11, 16, 301173), 'accuracy_balanced': 0.5404, 'accuracy': 0.5404, 'precision_macro': 0.5583388169257375, 'recall_macro': 0.5404, 'f1_score_macro': 0.5469570212862219, 'precision_weighted': 0.5583388169257374, 'recall_weighted': 0.5404, 'f1_score_weighted': 0.5469570212862219}\n",
      "2024-07-28 16:16:12,302 - INFO - Processed and saved results for Greater_Manchester using KNeighbors\n",
      "2024-07-28 16:20:45,788 - INFO - Processed and saved results for West_Midlands using SVM\n",
      "2024-07-28 16:23:04,523 - INFO - Test scores for West_Midlands using RandomForest: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 16, 23, 4, 512701), 'accuracy_balanced': 0.5428000000000001, 'accuracy': 0.5428, 'precision_macro': 0.5555730591935666, 'recall_macro': 0.5428000000000001, 'f1_score_macro': 0.547921290844815, 'precision_weighted': 0.5555730591935666, 'recall_weighted': 0.5428, 'f1_score_weighted': 0.5479212908448151}\n",
      "2024-07-28 16:25:06,926 - INFO - Test scores for Greater_Manchester using GradientBoosting: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 16, 25, 6, 915867), 'accuracy_balanced': 0.5680000000000001, 'accuracy': 0.568, 'precision_macro': 0.5866169580353262, 'recall_macro': 0.5680000000000001, 'f1_score_macro': 0.5739798762314848, 'precision_weighted': 0.5866169580353262, 'recall_weighted': 0.568, 'f1_score_weighted': 0.5739798762314847}\n",
      "2024-07-28 16:25:16,631 - INFO - Processed and saved results for West_Midlands using RandomForest\n",
      "2024-07-28 16:25:18,083 - INFO - Test scores for West_Midlands using LogisticRegression: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 16, 25, 18, 72402), 'accuracy_balanced': 0.5456, 'accuracy': 0.5456, 'precision_macro': 0.5318201970520973, 'recall_macro': 0.5456, 'f1_score_macro': 0.5223888170249704, 'precision_weighted': 0.5318201970520973, 'recall_weighted': 0.5456, 'f1_score_weighted': 0.5223888170249704}\n",
      "2024-07-28 16:25:18,640 - INFO - Processed and saved results for West_Midlands using LogisticRegression\n",
      "2024-07-28 16:25:19,565 - INFO - Test scores for West_Midlands using KNeighbors: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 16, 25, 19, 556113), 'accuracy_balanced': 0.5307999999999999, 'accuracy': 0.5308, 'precision_macro': 0.552713599295725, 'recall_macro': 0.5307999999999999, 'f1_score_macro': 0.5393715955834508, 'precision_weighted': 0.552713599295725, 'recall_weighted': 0.5308, 'f1_score_weighted': 0.5393715955834508}\n",
      "2024-07-28 16:26:56,246 - INFO - Processed and saved results for Greater_Manchester using GradientBoosting\n",
      "2024-07-28 16:27:27,642 - INFO - Test scores for Greater_Manchester using XGBoost: {'city': 'Greater_Manchester', 'datetime': datetime.datetime(2024, 7, 28, 16, 27, 27, 632589), 'accuracy_balanced': 0.5696, 'accuracy': 0.5696, 'precision_macro': 0.5837754550299751, 'recall_macro': 0.5696, 'f1_score_macro': 0.5733270052540708, 'precision_weighted': 0.583775455029975, 'recall_weighted': 0.5696, 'f1_score_weighted': 0.5733270052540708}\n",
      "2024-07-28 16:27:29,848 - INFO - Processed and saved results for Greater_Manchester using XGBoost\n",
      "2024-07-28 16:27:30,375 - INFO - Completed processing for Greater_Manchester\n",
      "2024-07-28 16:27:57,105 - INFO - Processed and saved results for West_Midlands using KNeighbors\n",
      "2024-07-28 16:34:14,872 - INFO - Processed and saved results for Greater_London using SVM\n",
      "2024-07-28 16:34:15,082 - INFO - Test scores for West_Midlands using GradientBoosting: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 16, 34, 15, 73176), 'accuracy_balanced': 0.562, 'accuracy': 0.562, 'precision_macro': 0.5781805836343445, 'recall_macro': 0.562, 'f1_score_macro': 0.5660237324205748, 'precision_weighted': 0.5781805836343444, 'recall_weighted': 0.562, 'f1_score_weighted': 0.5660237324205748}\n",
      "2024-07-28 16:35:32,179 - INFO - Processed and saved results for West_Midlands using GradientBoosting\n",
      "2024-07-28 16:36:09,217 - INFO - Test scores for West_Midlands using XGBoost: {'city': 'West_Midlands', 'datetime': datetime.datetime(2024, 7, 28, 16, 36, 9, 188460), 'accuracy_balanced': 0.5671999999999999, 'accuracy': 0.5672, 'precision_macro': 0.5859163210716618, 'recall_macro': 0.5671999999999999, 'f1_score_macro': 0.5716417765251585, 'precision_weighted': 0.5859163210716617, 'recall_weighted': 0.5672, 'f1_score_weighted': 0.5716417765251585}\n",
      "2024-07-28 16:36:11,199 - INFO - Processed and saved results for West_Midlands using XGBoost\n",
      "2024-07-28 16:36:11,576 - INFO - Completed processing for West_Midlands\n",
      "2024-07-28 16:36:31,281 - INFO - Test scores for Greater_London using RandomForest: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 16, 36, 31, 270391), 'accuracy_balanced': 0.5296, 'accuracy': 0.5296, 'precision_macro': 0.539474958503548, 'recall_macro': 0.5296, 'f1_score_macro': 0.5336524971440499, 'precision_weighted': 0.5394749585035479, 'recall_weighted': 0.5296, 'f1_score_weighted': 0.5336524971440499}\n",
      "2024-07-28 16:40:10,683 - INFO - Processed and saved results for Greater_London using RandomForest\n",
      "2024-07-28 16:40:11,976 - INFO - Test scores for Greater_London using LogisticRegression: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 16, 40, 11, 966074), 'accuracy_balanced': 0.536, 'accuracy': 0.536, 'precision_macro': 0.5321130192442018, 'recall_macro': 0.536, 'f1_score_macro': 0.5208456624711993, 'precision_weighted': 0.5321130192442021, 'recall_weighted': 0.536, 'f1_score_weighted': 0.5208456624711992}\n",
      "2024-07-28 16:40:12,787 - INFO - Processed and saved results for Greater_London using LogisticRegression\n",
      "2024-07-28 16:40:13,724 - INFO - Test scores for Greater_London using KNeighbors: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 16, 40, 13, 715107), 'accuracy_balanced': 0.522, 'accuracy': 0.522, 'precision_macro': 0.5352845015991741, 'recall_macro': 0.522, 'f1_score_macro': 0.5275810773867917, 'precision_weighted': 0.5352845015991741, 'recall_weighted': 0.522, 'f1_score_weighted': 0.5275810773867916}\n",
      "2024-07-28 16:44:44,562 - INFO - Processed and saved results for Greater_London using KNeighbors\n",
      "2024-07-28 16:50:54,873 - INFO - Test scores for Greater_London using GradientBoosting: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 16, 50, 54, 864152), 'accuracy_balanced': 0.5616, 'accuracy': 0.5616, 'precision_macro': 0.5852467065375642, 'recall_macro': 0.5616, 'f1_score_macro': 0.5634977192197717, 'precision_weighted': 0.5852467065375642, 'recall_weighted': 0.5616, 'f1_score_weighted': 0.5634977192197717}\n",
      "2024-07-28 16:51:45,849 - INFO - Processed and saved results for Greater_London using GradientBoosting\n",
      "2024-07-28 16:52:05,208 - INFO - Test scores for Greater_London using XGBoost: {'city': 'Greater_London', 'datetime': datetime.datetime(2024, 7, 28, 16, 52, 5, 197859), 'accuracy_balanced': 0.5584, 'accuracy': 0.5584, 'precision_macro': 0.5748253162108934, 'recall_macro': 0.5584, 'f1_score_macro': 0.5596821640200502, 'precision_weighted': 0.5748253162108935, 'recall_weighted': 0.5584, 'f1_score_weighted': 0.5596821640200502}\n",
      "2024-07-28 16:52:07,599 - INFO - Processed and saved results for Greater_London using XGBoost\n",
      "2024-07-28 16:52:08,415 - INFO - Completed processing for Greater_London\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 6 leaked folder objects to clean up at shutdown\n",
      "  warnings.warn(\n",
      "/users/songs16/.conda/envs/lse/lib/python3.12/site-packages/joblib/externals/loky/backend/resource_tracker.py:314: UserWarning: resource_tracker: There appear to be 6 leaked folder objects to clean up at shutdown\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All jobs done\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "bands=['ndti','ndre','ndvi','ndwi','mndwi','glcm','B2','B3','B4','B8']\n",
    "veges = pd.read_excel(r'osm_vegetation_classes.xlsx')\n",
    "foi = ['ndti','ndre','ndvi','ndwi','mndwi','glcm']\n",
    "ind_selected = [bands.index(x) for x in foi]\n",
    "bands = foi\n",
    "classification_comparison = r'../Processing/classification_comparison.csv'\n",
    "\n",
    "\n",
    "def sample_pnts(row):\n",
    "    geometry = row['geometry']\n",
    "    bounds = geometry.bounds\n",
    "    xmin, ymin, xmax, ymax = bounds[0], bounds[1], bounds[2], bounds[3]\n",
    "    x, y = np.mgrid[xmin:xmax+10:10, ymin:ymax+10:10]\n",
    "    x, y = np.vstack([x.ravel(), y.ravel()])\n",
    "    p = pd.DataFrame(list(zip(x, y)))\n",
    "    p[0] = np.floor((p[0] - row['xmin']) / row['xres']) * row['xres'] + row['xmin'] + row['xres'] / 2\n",
    "    p[1] = np.floor((p[1] - row['ymin']) / row['yres']) * row['yres'] + row['ymin'] + row['yres'] / 2\n",
    "    p['pnt'] = list(set(zip(p[0], p[1])))\n",
    "    p['pnt'] = p['pnt'].apply(Point)\n",
    "    p = gpd.GeoDataFrame(p['pnt'], geometry='pnt', crs=CRS.from_epsg(27700))\n",
    "    p = p[p.within(geometry)]\n",
    "    return p['pnt'].apply(lambda x: [x.x, x.y]).values\n",
    "\n",
    "\n",
    "def sample_raster(row, img_array):\n",
    "    y = int(row['y_n'])\n",
    "    x = int(row['x_n'])\n",
    "    if 0 <= y < img_array.shape[1] and 0 <= x < img_array.shape[2]:\n",
    "        res = img_array[:, y, x]\n",
    "    else:\n",
    "        res = np.nan\n",
    "    if np.isnan(res).any():\n",
    "        res = np.nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def extract_OSM_polygons(OSM, city):\n",
    "    shapefile = gpd.read_file(OSM)\n",
    "    shapefile = shapefile.to_crs({'init': 'epsg:27700'})\n",
    "    shapefile['geometry'] = shapefile.geometry.buffer(-10)\n",
    "    shapefile = shapefile[~shapefile.is_empty]\n",
    "    building = shapefile[~shapefile['building'].isnull()]\n",
    "    building.loc[:, 'area_length'] = (building.area / building.length).values\n",
    "    # building.loc[:, 'general'] = 'bldg'\n",
    "    building.loc[:, 'general'] = 5\n",
    "    shapefile = shapefile[shapefile['building'].isnull()]\n",
    "    shapefile['FID'] = list(range(0, len(shapefile.index)))\n",
    "    one_city = pd.DataFrame()\n",
    "    # for i in veges.index:\n",
    "    sub = pd.DataFrame()\n",
    "    sub['geometry'] = shapefile.loc[shapefile['general'] == 'vegetation', 'geometry']\n",
    "    sub['FID'] = shapefile.loc[shapefile['general'] == 'vegetation', 'FID']\n",
    "    sub['key'] = 'general'\n",
    "    sub['value'] = 'vegetation'\n",
    "    sub['SALID1'] = OSM.split('\\\\')[-1].split('.')[0]\n",
    "    sub['tag'] = shapefile.loc[shapefile['general'] == 'vegetation', 'tag']\n",
    "    if len(sub.index) > 0:\n",
    "        one_city = pd.concat([one_city, sub])\n",
    "    one_city['general'] = 'vegetation'\n",
    "        \n",
    "    one_city['general'] = one_city.apply(\n",
    "        lambda x: x['tag'] if x['general'] == 'vegetation' else 5, axis=1)\n",
    "\n",
    "    if len(one_city.index) > 0:\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city, geometry='geometry', crs=CRS.from_epsg(27700))\n",
    "        one_city_gdf.loc[:, 'shape_index'] = (one_city_gdf.length / (4 * np.sqrt(one_city_gdf.area))).values\n",
    "        one_city_gdf = one_city_gdf.loc[\n",
    "            (one_city_gdf.area <= one_city_gdf.area.quantile(0.975)) &\n",
    "            (one_city_gdf.area >= one_city_gdf.area.quantile(0.025)) &\n",
    "            (one_city_gdf['shape_index'] < one_city_gdf['shape_index'].quantile(0.9))\n",
    "        ]\n",
    "\n",
    "        background = shapefile.loc[~shapefile['FID'].isin(set(one_city['FID'])), ['geometry', 'FID']]\n",
    "        background['general'] = 5\n",
    "        background.loc[:, 'shape_index'] = (background.length / (4 * np.sqrt(background.area))).values\n",
    "        background = background.loc[\n",
    "            (background.area <= background.area.quantile(0.975)) &\n",
    "            (background.area >= background.area.quantile(0.025)) &\n",
    "            (background['shape_index'] < background['shape_index'].quantile(0.9))\n",
    "        ]\n",
    "        one_city_gdf = pd.concat([one_city_gdf, building])\n",
    "        one_city_gdf = pd.concat([one_city_gdf, background])\n",
    "        one_city_gdf = gpd.GeoDataFrame(one_city_gdf[['general', 'geometry', 'shape_index']], geometry='geometry',\n",
    "                                        crs=CRS.from_epsg(27700))\n",
    "    \n",
    "    one_city_gdf.to_file(driver='ESRI Shapefile', filename=r\"../Processing/polygon_%s.shp\" % city)\n",
    "    return one_city_gdf\n",
    "\n",
    "#To debug XGB classifier's strict integer rule\n",
    "custom_mapping = {0.0: 0, 1.0: 1, 2.0: 2, 5.0: 3}\n",
    "\n",
    "def preprocess_labels(y, mapping):\n",
    "    y_mapped = y.map(mapping)\n",
    "    return y_mapped.astype(int)\n",
    "\n",
    "def grid_search_wrapper(refit_score, clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params, city):\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score, cv=skf, return_train_score=False,\n",
    "                               n_jobs=4, verbose=0)\n",
    "\n",
    "    #To debug XGB classifier's strict integer rule\n",
    "    if isinstance(clf, XGBClassifier):\n",
    "        y_train = preprocess_labels(y_train, custom_mapping)\n",
    "        y_test = preprocess_labels(y_test, custom_mapping)\n",
    "    \n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    return grid_search, {'city': city, 'datetime': datetime.datetime.now(),\n",
    "                         'accuracy_balanced': balanced_accuracy_score(y_test, y_pred),\n",
    "                         'accuracy': accuracy_score(y_test, y_pred),\n",
    "                         'precision_macro': precision_score(y_test, y_pred, average='macro'),\n",
    "                         'recall_macro': recall_score(y_test, y_pred, average='macro'),\n",
    "                         'f1_score_macro': f1_score(y_test, y_pred, average='macro'),\n",
    "                         'precision_weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "                         'recall_weighted': recall_score(y_test, y_pred, average='weighted'),\n",
    "                         'f1_score_weighted': f1_score(y_test, y_pred, average='weighted')}\n",
    "\n",
    "\n",
    "def generate_sample(one_city_gdf, img, city):\n",
    "    logging.info(\"Starting sample generation...\")\n",
    "    global bands\n",
    "    raster = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "    geoTransform = raster.GetGeoTransform()\n",
    "    one_city_gdf['xmin'] = geoTransform[0]\n",
    "    one_city_gdf['ymin'] = geoTransform[3]\n",
    "    one_city_gdf['xres'] = geoTransform[1]\n",
    "    one_city_gdf['yres'] = geoTransform[5]\n",
    "    one_city_gdf['pnts'] = one_city_gdf.apply(sample_pnts, axis=1)\n",
    "    all_sample = gpd.GeoDataFrame()\n",
    "    for i in set(one_city_gdf['general']):\n",
    "        logging.debug(f\"Processing class: {i}\")\n",
    "        xys = one_city_gdf.loc[one_city_gdf['general'] == i, 'pnts'].values\n",
    "        xys_flat = [item for sublist in xys for item in sublist]\n",
    "        sample_df = pd.DataFrame(xys_flat)\n",
    "        sample_df['coordinates'] = list(zip(sample_df[0], sample_df[1]))\n",
    "        sample_gdf = gpd.GeoDataFrame(sample_df['coordinates'],\n",
    "                                      geometry=gpd.points_from_xy(sample_df[0], sample_df[1]), crs=\"epsg:27700\")\n",
    "        sample_gdf['class'] = i\n",
    "        logging.debug(f\"Generated {len(sample_gdf)} points for class {i}.\")\n",
    "        all_sample = pd.concat([all_sample, sample_gdf])\n",
    "\n",
    "    print(\"all_sample values before filtering\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "\n",
    "    value_counts = all_sample['class'].value_counts()    \n",
    "    classes_to_exclude = value_counts[value_counts < 500].index\n",
    "    all_sample = all_sample[~all_sample['class'].isin(classes_to_exclude)]\n",
    "\n",
    "    all_sample['class'] = all_sample['class'].apply(lambda x: 2 if x in [2.0, 3.0, 4.0] else x)\n",
    "\n",
    "\n",
    "    print(\"all_sample values after filtering\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "    \n",
    "    all_sample['x'] = all_sample.geometry.x\n",
    "    all_sample['y'] = all_sample.geometry.y\n",
    "    all_sample['x_n'] = (all_sample['x'] - geoTransform[0]) / geoTransform[1] - 0.5\n",
    "    all_sample['y_n'] = (all_sample['y'] - geoTransform[3]) / geoTransform[5] - 0.5\n",
    "    all_sample = all_sample.reset_index()\n",
    "\n",
    "    print(\"all_sample values second\")\n",
    "    print(all_sample['class'].value_counts())\n",
    "\n",
    "    land_sample = all_sample[all_sample['class'] != 5].copy()\n",
    "    print(\"land_sample\")\n",
    "    print(len(land_sample))\n",
    "    print(len(land_sample.drop_duplicates(subset=['x', 'y'], keep=False, inplace=False)))\n",
    "    land_sample.drop_duplicates(subset=['x', 'y'], keep=False, inplace=True)\n",
    "    clean_sample = pd.concat([land_sample, all_sample[all_sample['class'] == 5].copy()])\n",
    "    clean_sample['bldg_drop'] = 0\n",
    "    clean_sample.loc[clean_sample['class'] == 5, 'bldg_drop'] = 1\n",
    "    clean_sample = clean_sample.sort_values('bldg_drop', ascending=True)\n",
    "    # including green roof or other urban green infra - may need to fix\n",
    "    clean_sample.drop_duplicates(subset=['x', 'y'], keep='first', inplace=True)\n",
    "\n",
    "    img_array = np.array(raster.ReadAsArray())\n",
    "    img_array = img_array[ind_selected, :, :]\n",
    "    x_d = img_array.shape[1]\n",
    "    y_d = img_array.shape[2]\n",
    "    n_d = img_array.shape[0]\n",
    "    img_array = img_array.reshape(n_d, x_d * y_d)\n",
    "    img_array[:, np.isnan(img_array).any(axis=0)] = np.nan\n",
    "    img_array = img_array.reshape(n_d, x_d, y_d)\n",
    "\n",
    "    print(\"clean_sample number before ndvi filtering\")\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x, img_array), axis=1)\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample['mean_ndvi'] = clean_sample['sample_value'].apply(lambda x: x[bands.index('ndvi')])\n",
    "\n",
    "    clean_sample.loc[(clean_sample['class'] != 5)\n",
    "                     & (clean_sample['mean_ndvi'] <= 0.05), 'sample_value'] = np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "\n",
    "    print(\"clean_sample number after ndvi filtering\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    v_median = clean_sample.loc[(clean_sample['class'] != 5), 'mean_ndvi'].median()\n",
    "    print(\"v_median\")\n",
    "    print(v_median)\n",
    "    clean_sample.loc[(clean_sample['class'] == 5)\n",
    "                     & (clean_sample['mean_ndvi'] >= v_median), 'sample_value'] = np.nan\n",
    "    clean_sample = clean_sample.loc[~clean_sample['sample_value'].isnull()]\n",
    "    clean_sample = clean_sample.drop(columns=['mean_ndvi', 'sample_value'], axis=1)\n",
    "\n",
    "    print(\"clean_sample number before pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample.head())\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    for i in range(0, img_array.shape[0]):\n",
    "        v = img_array[i, :, :]\n",
    "        img_array[i, :, :] = (v - np.nanmin(v)) / (np.nanmax(v) - np.nanmin(v))\n",
    "    img_array_pca = np.copy(img_array)\n",
    "    img_array_pca = img_array_pca.reshape((img_array_pca.shape[0],\n",
    "                                           img_array_pca.shape[1] * img_array_pca.shape[2])).transpose()\n",
    "    img_array_pca_valid = img_array_pca[~np.isnan(img_array_pca).any(axis=1)]\n",
    "    pca = PCA(n_components=img_array_pca_valid.shape[1])\n",
    "    pca_res = pca.fit(img_array_pca_valid)\n",
    "    var = np.cumsum(np.round(pca_res.explained_variance_ratio_, decimals=3) * 100)\n",
    "    n_pc = sum(var <= 90) + 1\n",
    "    pca = PCA(n_components=n_pc)\n",
    "    pca_reduce = pca.fit_transform(img_array_pca_valid)\n",
    "    pca_reduce = np.multiply(pca_reduce, pca_res.explained_variance_ratio_[:n_pc])\n",
    "\n",
    "    img_reduce = np.copy(img_array[:n_pc, :, :])\n",
    "    img_reduce_re = img_reduce.reshape((img_reduce.shape[0], img_reduce.shape[1] * img_reduce.shape[2])).transpose()\n",
    "    img_reduce_re[~np.isnan(img_reduce_re).any(axis=1)] = pca_reduce\n",
    "    img_reduce_re = img_reduce_re.transpose()\n",
    "    img_reduce = img_reduce_re.reshape((img_reduce.shape[0], img_reduce.shape[1], img_reduce.shape[2]))\n",
    "\n",
    "    img_array = np.copy(img_reduce)\n",
    "    del img_array_pca, img_array_pca_valid, img_reduce_re, img_reduce, pca_reduce\n",
    "\n",
    "    clean_sample['sample_value'] = clean_sample.apply(lambda x: sample_raster(x, img_array), axis=1)\n",
    "    PCAs = list(range(0, n_pc))\n",
    "    for PC in PCAs:\n",
    "        i = PC\n",
    "        clean_sample[PC] = clean_sample['sample_value'].apply(lambda x: x[i])\n",
    "    clean_sample = clean_sample.drop('sample_value', axis=1)\n",
    "\n",
    "    print(\"clean_sample number after pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "\n",
    "    for i in set(clean_sample['class']):\n",
    "        X = clean_sample.loc[clean_sample['class'] == i, PCAs].values\n",
    "        X = np.array(X.tolist())\n",
    "        clf = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        clean_sample.loc[clean_sample['class'] == i, 'outlier'] = y_pred\n",
    "        outlier_score = clf.negative_outlier_factor_\n",
    "        clean_sample.loc[clean_sample['class'] == i, 'outlier_score'] = (outlier_score - outlier_score.min()) / (\n",
    "                    outlier_score.max() - outlier_score.min())\n",
    "    clean_sample[PCAs] = clean_sample[PCAs].astype(np.float32)\n",
    "    clean_sample = clean_sample.dropna()\n",
    "    clean_sample = clean_sample.loc[clean_sample['outlier'] != -1]\n",
    "\n",
    "    print(\"clean_sample number after second pca\")\n",
    "    print(len(clean_sample))\n",
    "    print(clean_sample['class'].value_counts())\n",
    "    \n",
    "    # Count the number of samples for each class\n",
    "    class_counts = clean_sample['class'].value_counts()\n",
    "    print(\"Class counts:\", class_counts)\n",
    "    \n",
    "    # Determine the sample size\n",
    "    n_sample = int(min(class_counts))\n",
    "    if n_sample >= 2500:\n",
    "        n_sample = 2500\n",
    "    \n",
    "    sampled_dfs = []\n",
    "    for class_label in range(6):\n",
    "        if class_label in class_counts:\n",
    "            sampled_dfs.append(clean_sample[clean_sample['class'] == class_label].sample(n=min(n_sample, class_counts[class_label]), random_state=0))\n",
    "    \n",
    "    sub_clean_sample = pd.concat(sampled_dfs)\n",
    "    \n",
    "    ext = sub_clean_sample[['class', 'geometry']]\n",
    "    ext = ext.to_crs({'init': 'epsg:27700'})\n",
    "    ext.to_file(r\"../Processing/all_sample_%s.shp\" % city)\n",
    "    raster = None\n",
    "    return sub_clean_sample, img_array, PCAs\n",
    "\n",
    "\n",
    "def split_sample(sub_clean_sample, PCAs, city):\n",
    "    ft = sub_clean_sample[PCAs]\n",
    "    ft['outlier_score'] = sub_clean_sample['outlier_score']\n",
    "    targets = sub_clean_sample['class']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ft, targets, stratify=targets, random_state=0)\n",
    "\n",
    "    X_test_weight = X_test['outlier_score']\n",
    "    X_test = X_test[PCAs]\n",
    "    X_train_weight = X_train['outlier_score']\n",
    "    X_train = X_train[PCAs]\n",
    "\n",
    "    train_exp = X_train.merge(sub_clean_sample[['class', 'geometry']], left_index=True, right_index=True)\n",
    "    test_exp = X_test.merge(sub_clean_sample[['class', 'geometry']], left_index=True, right_index=True)\n",
    "\n",
    "    gpd.GeoDataFrame(train_exp[['class', 'geometry']], geometry='geometry').to_file(driver='ESRI Shapefile',\n",
    "                                                                                     filename=r\"../Processing/train_sample_%s.shp\" % city)\n",
    "    gpd.GeoDataFrame(test_exp[['class', 'geometry']], geometry='geometry').to_file(driver='ESRI Shapefile',\n",
    "                                                                                   filename=r\"../Processing/test_sample_%s.shp\" % city)\n",
    "    return X_train, X_test, y_train, y_test, X_train_weight\n",
    "\n",
    "\n",
    "def process_city(city, tif_names):\n",
    "    try:\n",
    "        logging.info(f'Starting processing for {city}')\n",
    "        OSM = r'../Code/%s_OSM_combined.shp' % city\n",
    "        img = r'../Sample_image/LA_%s_2023.tif' % tif_names[city]\n",
    "\n",
    "        if not os.path.exists(OSM):\n",
    "            logging.error(f'OSM file does not exist: {OSM}')\n",
    "            return\n",
    "        if not os.path.exists(img):\n",
    "            logging.error(f'Image file does not exist: {img}')\n",
    "            return\n",
    "\n",
    "        one_city_gdf = extract_OSM_polygons(OSM, city)  # get OSM polygons\n",
    "        print(\"one_city_gdf after extract\")\n",
    "        print(len(one_city_gdf))\n",
    "        sub_clean_sample, img_array, PCAs = generate_sample(one_city_gdf, img, city)  # generate random samples\n",
    "        print(\"sub_clean_sample\")\n",
    "        print(len(sub_clean_sample))\n",
    "        print(sub_clean_sample.head())\n",
    "\n",
    "        X_train, X_test, y_train, y_test, X_train_weight = split_sample(sub_clean_sample, PCAs, city)  # sample values, train test split\n",
    "        scorers = {\n",
    "            'precision_macro': make_scorer(precision_score, average='macro', zero_division=0),\n",
    "            'recall_macro': make_scorer(recall_score, average='macro', zero_division=0),\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_score_macro': make_scorer(f1_score, average='macro', zero_division=0),\n",
    "            'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "            'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "            'f1_score_weighted': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        fit_params = {'sample_weight': X_train_weight}\n",
    "\n",
    "        classifiers = {\n",
    "            'SVM': SVC(),\n",
    "            'RandomForest': RandomForestClassifier(),\n",
    "            'LogisticRegression': LogisticRegression(),\n",
    "            'KNeighbors': KNeighborsClassifier(),\n",
    "            'GradientBoosting': GradientBoostingClassifier(),\n",
    "            'XGBoost': XGBClassifier()\n",
    "        }\n",
    "        \n",
    "        param_grids = {\n",
    "            'SVM': {'C': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                    'gamma': [2 ** x for x in np.arange(-3, 13, dtype=float)],\n",
    "                    'class_weight': ['balanced']},\n",
    "            'RandomForest': {'n_estimators': [100, 200, 300],\n",
    "                             'max_features': [None, 'sqrt', 'log2'],\n",
    "                             'min_samples_split': [2, 5, 10],\n",
    "                             'random_state': [0]},\n",
    "            'LogisticRegression': {'C': [0.1, 1, 10, 100],\n",
    "                                   'solver': ['lbfgs'],\n",
    "                                   'max_iter': [100, 200, 300],\n",
    "                                   'class_weight': ['balanced']},\n",
    "            'KNeighbors': {'n_neighbors': [3, 5, 7, 9],\n",
    "                           'weights': ['uniform', 'distance'],\n",
    "                           'algorithm': ['ball_tree', 'kd_tree', 'brute']},\n",
    "            'GradientBoosting': {'n_estimators': [100, 200, 300],\n",
    "                                 'learning_rate': [0.01, 0.1, 0.2],\n",
    "                                 'max_depth': [3, 4, 5],\n",
    "                                 'random_state': [0]},\n",
    "            'XGBoost': {'n_estimators': [100, 200, 300],\n",
    "                                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                                'max_depth': [3, 4, 5],\n",
    "                                'random_state': [0]}\n",
    "        }\n",
    "\n",
    "        for clf_name in classifiers:\n",
    "            clf = classifiers[clf_name]\n",
    "            param_grid = param_grids[clf_name]\n",
    "            grid_search_clf, test_scores = grid_search_wrapper('f1_score_weighted', clf, param_grid, scorers, X_train, X_test, y_train, y_test, fit_params, city)\n",
    "\n",
    "            # save testing accuracy\n",
    "            logging.info(f'Test scores for {city} using {clf_name}: {test_scores}')\n",
    "\n",
    "            lock = FileLock(f\"{classification_comparison}.lock\")\n",
    "            with lock:\n",
    "                with open(classification_comparison, 'a') as csv_file:\n",
    "                    writer = csv.writer(csv_file, delimiter=',', lineterminator='\\n')\n",
    "                    writer.writerow([f\"{city}\"] + [clf_name] + list(test_scores.values()))\n",
    "\n",
    "            # save original img\n",
    "            img_array2 = np.copy(img_array)\n",
    "            img_re = img_array2.reshape((img_array2.shape[0], img_array2.shape[1] * img_array2.shape[2])).transpose()\n",
    "            img_pre = np.copy(img_re[~np.isnan(img_re).any(axis=1)])\n",
    "            img_pre = grid_search_clf.predict(img_pre)\n",
    "            img_pre = img_pre.astype(np.int16)\n",
    "            res = img_re[:, 0]\n",
    "            res[~np.isnan(res)] = img_pre\n",
    "            res[np.isnan(res)] = -32768\n",
    "            res = res.reshape(img_array[0, :, :].shape)\n",
    "            res = res.astype(np.int16)\n",
    "\n",
    "            org_img = gdal.Open(img, gdal.GA_ReadOnly)\n",
    "            meta = {\n",
    "                'driver': 'GTiff',\n",
    "                'dtype': 'int16',\n",
    "                'nodata': -32768,\n",
    "                'width': res.shape[1],\n",
    "                'height': res.shape[0],\n",
    "                'count': 1,\n",
    "                'crs': CRS(\"EPSG:27700\"),\n",
    "                'transform': Affine(10, 0.0, org_img.GetGeoTransform()[0], 0, -10, org_img.GetGeoTransform()[-3]),\n",
    "                'compress': 'lzw',\n",
    "                'interleave': 'pixel'\n",
    "            }\n",
    "\n",
    "            result_path = r'../Results/{}_{}_{}.tif'.format(city, clf_name, datetime.datetime.now().strftime('%m%d_%H'))\n",
    "            with rio.open(result_path, 'w', **meta) as dst:\n",
    "                dst.write(res, 1)\n",
    "\n",
    "            logging.info(f'Processed and saved results for {city} using {clf_name}')\n",
    "\n",
    "        del img_array\n",
    "        org_img = None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    tif_names = {\"Greater_Manchester\": 'Manchester', \"Greater_London\": 'London', \"West_Midlands\": 'Westmidlands'}\n",
    "    cities = [\"Greater_Manchester\", \"West_Midlands\", \"Greater_London\"]\n",
    "\n",
    "    max_workers = 3\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # future_to_city = {\n",
    "        #     executor.submit(process_city, city, tif_names) for city in cities\n",
    "        # }\n",
    "        future_to_city = {executor.submit(process_city, city, tif_names): city for city in cities}\n",
    "        for future in concurrent.futures.as_completed(future_to_city):\n",
    "            city = future_to_city[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                logging.info(f'Completed processing for {city}')\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error processing {city}: {e}', exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "print('All jobs done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lse",
   "language": "python",
   "name": "lse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
