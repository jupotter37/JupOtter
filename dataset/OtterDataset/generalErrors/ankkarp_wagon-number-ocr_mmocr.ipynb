{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akarpova/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmocr/textrecog/aster/aster_resnet45_6e_st_mj/aster_resnet45_6e_st_mj-cc56eca4.pth\n",
      "[{'type': 'InferencerLoader'}, {'type': 'Resize', 'scale': (256, 64)}, {'type': 'PackTextRecogInputs', 'meta_keys': ('img_path', 'ori_shape', 'img_shape', 'valid_ratio')}]\n",
      "10/14 19:40:11 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmocr\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmocr\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akarpova/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    }
   ],
   "source": [
    "from mmocr.apis import MMOCRInferencer\n",
    "ocr = MMOCRInferencer(det=None, rec='Aster')  # 'DBNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmocr.registry import MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_config = {'type': 'STN', 'in_channels': 3, 'resized_image_size': (32, 64), 'output_image_size': (32, 100), 'num_control_points': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn = MODELS.build(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MMOCRInferencer' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ocr\u001b[39m.\u001b[39;49mlayers()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MMOCRInferencer' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "ocr.layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextRecogDataPreprocessor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr.textrec_inferencer.model.data_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ocr.textrec_inferencer.model._modules.keys():\n",
    "    if 'preprocessor' not in k:\n",
    "        ocr.textrec_inferencer.model._modules[k] = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('data_preprocessor', TextRecogDataPreprocessor()),\n",
       "             ('preprocessor',\n",
       "              STN(\n",
       "                (tps): TPStransform()\n",
       "                (stn_convnet): Sequential(\n",
       "                  (0): ConvModule(\n",
       "                    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                  (2): ConvModule(\n",
       "                    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                  (4): ConvModule(\n",
       "                    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                  (6): ConvModule(\n",
       "                    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                  (8): ConvModule(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                  (10): ConvModule(\n",
       "                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (activate): ReLU(inplace=True)\n",
       "                  )\n",
       "                )\n",
       "                (stn_fc1): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "                (stn_fc2): Linear(in_features=512, out_features=40, bias=True)\n",
       "              )\n",
       "              init_cfg=[{'type': 'Xavier', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': 'BatchNorm2d'}]),\n",
       "             ('backbone', Sequential()),\n",
       "             ('encoder', Sequential()),\n",
       "             ('decoder', Sequential())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr.textrec_inferencer.model._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from mmengine.dataset import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pipeline.pkl', 'rb') as file: \n",
    "    # Call load method to deserialze \n",
    "    pipeline = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputs, batch_size: int = 1, **kwargs):\n",
    "    \"\"\"Process the inputs into a model-feedable format.\n",
    "\n",
    "    Args:\n",
    "        inputs (InputsType): Inputs given by user.\n",
    "        batch_size (int): batch size. Defaults to 1.\n",
    "\n",
    "    Yields:\n",
    "        Any: Data processed by the ``pipeline`` and ``collate_fn``.\n",
    "    \"\"\"\n",
    "    chunked_data = _get_chunk_data(inputs, batch_size)\n",
    "    yield from map(ocr.textrec_inferencer.collate_fn, chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('EasyOCR/trainer/all_data/train/42030510.jpg',\n",
       "   {'inputs': tensor([[[26, 26, 25,  ...,  7,  6,  6],\n",
       "             [27, 27, 27,  ...,  7,  6,  6],\n",
       "             [25, 25, 26,  ...,  7,  7,  7],\n",
       "             ...,\n",
       "             [17, 17, 17,  ..., 36, 34, 33],\n",
       "             [15, 15, 16,  ..., 35, 34, 33],\n",
       "             [14, 14, 14,  ..., 28, 27, 27]],\n",
       "    \n",
       "            [[28, 28, 27,  ...,  7,  6,  6],\n",
       "             [29, 29, 29,  ...,  7,  6,  6],\n",
       "             [27, 27, 28,  ...,  7,  7,  7],\n",
       "             ...,\n",
       "             [17, 17, 17,  ..., 38, 36, 35],\n",
       "             [15, 15, 16,  ..., 37, 35, 35],\n",
       "             [14, 14, 14,  ..., 30, 29, 29]],\n",
       "    \n",
       "            [[22, 22, 21,  ...,  7,  6,  6],\n",
       "             [23, 23, 23,  ...,  7,  6,  6],\n",
       "             [21, 21, 22,  ...,  7,  7,  7],\n",
       "             ...,\n",
       "             [17, 17, 17,  ..., 32, 30, 29],\n",
       "             [15, 15, 16,  ..., 31, 29, 29],\n",
       "             [14, 14, 14,  ..., 24, 23, 23]]], dtype=torch.uint8),\n",
       "    'data_samples': <TextRecogDataSample(\n",
       "    \n",
       "        META INFORMATION\n",
       "        img_path: 'EasyOCR/trainer/all_data/train/42030510.jpg'\n",
       "        img_shape: (64, 256)\n",
       "        valid_ratio: 1\n",
       "        ori_shape: (68, 128)\n",
       "    \n",
       "        DATA FIELDS\n",
       "        gt_text: <LabelData(\n",
       "            \n",
       "                META INFORMATION\n",
       "            \n",
       "                DATA FIELDS\n",
       "            ) at 0x7f54125a6e90>\n",
       "    ) at 0x7f5413b1e380>})]]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = list(_get_chunk_data(['EasyOCR/trainer/all_data/train/42030510.jpg']))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline('EasyOCR/trainer/all_data/train/42030510.jpg')['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/akarpova/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/akarpova/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ocr(\u001b[39m'\u001b[39;49m\u001b[39mEasyOCR/trainer/all_data/train/42030510.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m, show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, print_result\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/apis/inferencers/mmocr_inferencer.py:317\u001b[0m, in \u001b[0;36mMMOCRInferencer.__call__\u001b[0;34m(self, inputs, batch_size, det_batch_size, rec_batch_size, kie_batch_size, out_dir, return_vis, save_vis, save_pred, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m results \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mvisualization\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[1;32m    316\u001b[0m \u001b[39mfor\u001b[39;00m ori_input \u001b[39min\u001b[39;00m track(chunked_inputs, description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInference\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 317\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m    318\u001b[0m         ori_input,\n\u001b[1;32m    319\u001b[0m         det_batch_size\u001b[39m=\u001b[39;49mdet_batch_size,\n\u001b[1;32m    320\u001b[0m         rec_batch_size\u001b[39m=\u001b[39;49mrec_batch_size,\n\u001b[1;32m    321\u001b[0m         kie_batch_size\u001b[39m=\u001b[39;49mkie_batch_size,\n\u001b[1;32m    322\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_kwargs)\n\u001b[1;32m    323\u001b[0m     visualization \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualize(\n\u001b[1;32m    324\u001b[0m         ori_input, preds, img_out_dir\u001b[39m=\u001b[39mimg_out_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvisualize_kwargs)\n\u001b[1;32m    325\u001b[0m     batch_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(\n\u001b[1;32m    326\u001b[0m         preds,\n\u001b[1;32m    327\u001b[0m         visualization,\n\u001b[1;32m    328\u001b[0m         pred_out_dir\u001b[39m=\u001b[39mpred_out_dir,\n\u001b[1;32m    329\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_kwargs)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/apis/inferencers/mmocr_inferencer.py:147\u001b[0m, in \u001b[0;36mMMOCRInferencer.forward\u001b[0;34m(self, inputs, batch_size, det_batch_size, rec_batch_size, kie_batch_size, **forward_kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrec\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[39m# The extra list wrapper here is for the ease of postprocessing\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_inputs \u001b[39m=\u001b[39m inputs\n\u001b[0;32m--> 147\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtextrec_inferencer(\n\u001b[1;32m    148\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrec_inputs,\n\u001b[1;32m    149\u001b[0m         return_datasamples\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    150\u001b[0m         batch_size\u001b[39m=\u001b[39;49mrec_batch_size,\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_kwargs)[\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    152\u001b[0m     result[\u001b[39m'\u001b[39m\u001b[39mrec\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [[p] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m predictions]\n\u001b[1;32m    153\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mdet\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# 'det'/'det_rec'/'det_rec_kie'\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/apis/inferencers/base_mmocr_inferencer.py:194\u001b[0m, in \u001b[0;36mBaseMMOCRInferencer.__call__\u001b[0;34m(self, inputs, return_datasamples, batch_size, progress_bar, return_vis, show, wait_time, draw_pred, pred_score_thr, out_dir, save_vis, save_pred, print_result, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m results \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mvisualization\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[1;32m    192\u001b[0m \u001b[39mfor\u001b[39;00m ori_inputs, data \u001b[39min\u001b[39;00m track(\n\u001b[1;32m    193\u001b[0m         inputs, description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInference\u001b[39m\u001b[39m'\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m progress_bar):\n\u001b[0;32m--> 194\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_kwargs)\n\u001b[1;32m    195\u001b[0m     visualization \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualize(\n\u001b[1;32m    196\u001b[0m         ori_inputs, preds, img_out_dir\u001b[39m=\u001b[39mimg_out_dir, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvisualize_kwargs)\n\u001b[1;32m    197\u001b[0m     batch_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(\n\u001b[1;32m    198\u001b[0m         preds,\n\u001b[1;32m    199\u001b[0m         visualization,\n\u001b[1;32m    200\u001b[0m         return_datasamples,\n\u001b[1;32m    201\u001b[0m         pred_out_dir\u001b[39m=\u001b[39mpred_out_dir,\n\u001b[1;32m    202\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_kwargs)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/infer/infer.py:299\u001b[0m, in \u001b[0;36mBaseInferencer.forward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs: Union[\u001b[39mdict\u001b[39m, \u001b[39mtuple\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    298\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Feed the inputs to the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(inputs)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:145\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m    list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_preprocessor(data, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_forward(data, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:346\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 346\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    347\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    348\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39mdata, mode\u001b[39m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/models/textrecog/recognizers/base.py:90\u001b[0m, in \u001b[0;36mBaseRecognizer.forward\u001b[0;34m(self, inputs, data_samples, mode, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(inputs, data_samples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(inputs, data_samples, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensor\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(inputs, data_samples, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/models/textrecog/recognizers/encoder_decoder_recognizer.py:111\u001b[0m, in \u001b[0;36mEncoderDecoderRecognizer.predict\u001b[0;34m(self, inputs, data_samples, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m out_enc \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_encoder:\n\u001b[0;32m--> 111\u001b[0m     out_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(feat, data_samples)\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mpredict(feat, out_enc, data_samples)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Sequential.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ocr('EasyOCR/trainer/all_data/train/42030510.jpg', show=True, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'STN' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stn\u001b[39m.\u001b[39;49mpredict(pipeline(\u001b[39m'\u001b[39m\u001b[39mEasyOCR/trainer/all_data/train/42030510.jpg\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'STN' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "stn.predict(pipeline('EasyOCR/trainer/all_data/train/42030510.jpg')['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = preprocess(['EasyOCR/trainer/all_data/train/42030510.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = list(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlist\u001b[39;49m(pre)[\u001b[39m1\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list(pre)[1]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for ri_inputs, data in pre:\n",
    "    res.append(stn(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_p = _get_chunk_data('EasyOCR/trainer/all_data/train/42030510.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (2416207021.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[95], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    h = yield from map(ocr.textrec_inferencer.collate_fn, inputs_p)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "h = yield from map(ocr.textrec_inferencer.collate_fn, inputs_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stn(inputs_p)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/models/textrecog/preprocessors/tps_preprocessor.py:262\u001b[0m, in \u001b[0;36mSTN.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    254\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward function of STN.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m        Tensor: The rectified image tensor.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     resize_img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(\n\u001b[1;32m    263\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresized_image_size, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    264\u001b[0m     points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstn_convnet(resize_img)\n\u001b[1;32m    265\u001b[0m     batch_size, _, _, _ \u001b[39m=\u001b[39m points\u001b[39m.\u001b[39msize()\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3841\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3838\u001b[0m     \u001b[39mif\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m         align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 3841\u001b[0m dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdim() \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Number of spatial dimensions.\u001b[39;00m\n\u001b[1;32m   3843\u001b[0m \u001b[39m# Process size and scale_factor.  Validate that exactly one is set.\u001b[39;00m\n\u001b[1;32m   3844\u001b[0m \u001b[39m# Validate its length if it is a list, or expand it if it is a scalar.\u001b[39;00m\n\u001b[1;32m   3845\u001b[0m \u001b[39m# After this block, exactly one of output_size and scale_factors will\u001b[39;00m\n\u001b[1;32m   3846\u001b[0m \u001b[39m# be non-None, and it will be a list (or tuple).\u001b[39;00m\n\u001b[1;32m   3847\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m scale_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "stn(inputs_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pipeline('EasyOCR/trainer/all_data/train/42030510.jpg')['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[26, 26, 25,  ...,  7,  6,  6],\n",
       "         [27, 27, 27,  ...,  7,  6,  6],\n",
       "         [25, 25, 26,  ...,  7,  7,  7],\n",
       "         ...,\n",
       "         [17, 17, 17,  ..., 36, 34, 33],\n",
       "         [15, 15, 16,  ..., 35, 34, 33],\n",
       "         [14, 14, 14,  ..., 28, 27, 27]],\n",
       "\n",
       "        [[28, 28, 27,  ...,  7,  6,  6],\n",
       "         [29, 29, 29,  ...,  7,  6,  6],\n",
       "         [27, 27, 28,  ...,  7,  7,  7],\n",
       "         ...,\n",
       "         [17, 17, 17,  ..., 38, 36, 35],\n",
       "         [15, 15, 16,  ..., 37, 35, 35],\n",
       "         [14, 14, 14,  ..., 30, 29, 29]],\n",
       "\n",
       "        [[22, 22, 21,  ...,  7,  6,  6],\n",
       "         [23, 23, 23,  ...,  7,  6,  6],\n",
       "         [21, 21, 22,  ...,  7,  7,  7],\n",
       "         ...,\n",
       "         [17, 17, 17,  ..., 32, 30, 29],\n",
       "         [15, 15, 16,  ..., 31, 29, 29],\n",
       "         [14, 14, 14,  ..., 24, 23, 23]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STN(\n",
       "  (tps): TPStransform()\n",
       "  (stn_convnet): Sequential(\n",
       "    (0): ConvModule(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ConvModule(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): ConvModule(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ConvModule(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (stn_fc1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (stn_fc2): Linear(in_features=512, out_features=40, bias=True)\n",
       ")\n",
       "init_cfg=[{'type': 'Xavier', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': 'BatchNorm2d'}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'InferencerLoader'}, {'type': 'Resize', 'scale': [256, 64]}, {'type': 'PackTextRecogInputs', 'meta_keys': ['img_path', 'ori_shape', 'img_shape', 'valid_ratio']}]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'InferencerLoader is not in the mmengine::transform registry. Please check whether the value of `InferencerLoader` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf \u001b[39m=\u001b[39m Compose([{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mInferencerLoader\u001b[39;49m\u001b[39m\"\u001b[39;49m}, {\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mResize\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mscale\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m256\u001b[39;49m, \u001b[39m64\u001b[39;49m]}, {\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mPackTextRecogInputs\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmeta_keys\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m\"\u001b[39;49m\u001b[39mimg_path\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mori_shape\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mimg_shape\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mvalid_ratio\u001b[39;49m\u001b[39m\"\u001b[39;49m]}])\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/dataset/base_dataset.py:40\u001b[0m, in \u001b[0;36mCompose.__init__\u001b[0;34m(self, transforms)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m transform \u001b[39min\u001b[39;00m transforms:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# `Compose` can be built with config dict with type and\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39m# corresponding arguments.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(transform, \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m         transform \u001b[39m=\u001b[39m TRANSFORMS\u001b[39m.\u001b[39;49mbuild(transform)\n\u001b[1;32m     41\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(transform):\n\u001b[1;32m     42\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtransform should be a callable object, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     43\u001b[0m                             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(transform)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, cfg: \u001b[39mdict\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[39m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_func(cfg, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, registry\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmengine/registry/build_functions.py:100\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     obj_cls \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mget(obj_type)\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m obj_cls \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mobj_type\u001b[39m}\u001b[39;00m\u001b[39m is not in the \u001b[39m\u001b[39m{\u001b[39;00mregistry\u001b[39m.\u001b[39mscope\u001b[39m}\u001b[39;00m\u001b[39m::\u001b[39m\u001b[39m{\u001b[39;00mregistry\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m registry. \u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlease check whether the value of `\u001b[39m\u001b[39m{\u001b[39;00mobj_type\u001b[39m}\u001b[39;00m\u001b[39m` is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    103\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcorrect or it was registered as expected. More details \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcan be found at \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mhttps://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    106\u001b[0m         )\n\u001b[1;32m    107\u001b[0m \u001b[39m# this will include classes, functions, partial functions and more\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mcallable\u001b[39m(obj_type):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'InferencerLoader is not in the mmengine::transform registry. Please check whether the value of `InferencerLoader` is correct or it was registered as expected. More details can be found at https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#import-the-custom-module'"
     ]
    }
   ],
   "source": [
    "tf = Compose([{\"type\": \"InferencerLoader\"}, {\"type\": \"Resize\", \"scale\": [256, 64]}, {\"type\": \"PackTextRecogInputs\", \"meta_keys\": [\"img_path\", \"ori_shape\", \"img_shape\", \"valid_ratio\"]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = tf('EasyOCR/trainer/all_data/train/42030510.jpg')['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'resize_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akarpova/Projects/wagon-number-ocr/mmocr.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stn(img_tensor)\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/wagon-number-ocr/.venv/lib/python3.10/site-packages/mmocr/models/textrecog/preprocessors/tps_preprocessor.py:264\u001b[0m, in \u001b[0;36mSTN.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward function of STN.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[1;32m    256\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m    Tensor: The rectified image tensor.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m resize_img \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(\n\u001b[1;32m    263\u001b[0m     img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresized_image_size, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 264\u001b[0m points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstn_convnet(resize_img)\n\u001b[1;32m    265\u001b[0m batch_size, _, _, _ \u001b[39m=\u001b[39m points\u001b[39m.\u001b[39msize()\n\u001b[1;32m    266\u001b[0m points \u001b[39m=\u001b[39m points\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resize_img' is not defined"
     ]
    }
   ],
   "source": [
    "stn(img_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
