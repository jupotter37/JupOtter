{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bdeac4-d42a-46c9-9740-42a9ae74a059",
   "metadata": {},
   "source": [
    "# Deep Analysis of PISA 2022 Data: Interrelations of Academic Achievement with Sociocultural Factors\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project undertakes an in-depth analysis of the Program for International Student Assessment (PISA), an initiative orchestrated by the Organisation for Economic Co-operation and Development (OECD). PISA is a worldwide study designed to evaluate the educational performance of 15-year-old students and their preparedness for real-world challenges beyond the formal school curriculum. This assessment, which takes place every three years, measures the abilities of students in critical cognitive domains including reading literacy, mathematical literacy, and scientific literacy.\n",
    "\n",
    "The significance of PISA lies in its role as a global benchmark for evaluating education systems worldwide by comparing the skills and knowledge of students across different countries. This comparison helps in identifying effective educational practices and policies. By focusing on how well young adults can apply their knowledge to real-life situations, PISA provides valuable insights into the effectiveness of schooling in different regions and aids in policymaking to enhance educational outcomes.\n",
    "\n",
    "Through rigorous and standardized testing methodologies, PISA evaluates not just rote memorization, but the ability of students to think critically, solve complex problems, and make reasoned decisions. It thus provides a comprehensive picture of students' capabilities in handling the demands of future academic and occupational settings. The insights drawn from PISA data are instrumental for educators, policymakers, and stakeholders in crafting strategies that improve educational standards and foster an environment that nurtures the full potential of every learner.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "For this project, we harness the most recent 2022 data set obtained from the \"Student Questionnaire Data File\" available on the official OECD website. This file is a comprehensive repository of data, meticulously compiled from the responses of students and their parents across various countries. It serves as a foundational element for our analysis, providing both performance scores and a diverse range of background variables. These variables facilitate a deep dive into several critical aspects of the educational landscape:\n",
    "\n",
    "- **Demographic and Socio-economic Profiles**: This category includes detailed demographic information about students and their familial backgrounds, capturing data points such as age, gender, immigration status, as well as the educational levels and socio-economic statuses of their parents. These variables are crucial for understanding the diverse contexts from which students hail and how these factors might influence their educational achievements.\n",
    "\n",
    "- **Educational Performance**: The data file provides scores across fundamental academic subjects—mathematics, reading, and science. These scores are offered as plausible values, which are multiple imputed scores reflecting students’ abilities derived from their test performances. This approach allows for more nuanced statistical analyses and helps in understanding the variability in student performance across different educational systems.\n",
    "\n",
    "- **Learning Environment and Behaviors**: This section sheds light on the non-academic aspects of students' school life, encompassing their study habits, self-reported motivational attitudes, their sense of belonging at their school, and their experiences with bullying. Such data points are invaluable for assessing the psycho-social dimensions that influence educational outcomes.\n",
    "\n",
    "- **Digital Literacy and Resources**: In today’s digital age, access to technology is a significant factor in educational success. This variable assesses the availability and usage of information and communication technologies (ICT) at students’ homes. It examines how these tools are integrated into the learning process and their impact on students' educational performance.\n",
    "\n",
    "By analyzing these detailed and multi-faceted data, this project aims to uncover patterns and trends that offer insights into the factors that most significantly impact student learning outcomes. These insights can help policymakers, educators, and communities to design targeted interventions that enhance educational equity and effectiveness.\n",
    "\n",
    "## Core Analytical and Advanced Programming Methods\n",
    "\n",
    "The project harnesses a diverse array of data analysis techniques tailored specifically to dissect the complex interactions within educational data provided by the PISA 2022 dataset. Here’s a rundown of the primary methods applied:\n",
    "\n",
    "- **Statistical Analysis**:\n",
    "  - **Descriptive Statistics**: Summarize data features like central tendency, variability, and distribution shapes.\n",
    "  - **Shapiro-Wilk Test**: Assess the normality of data distributions, crucial for the validity of many other statistical tests.\n",
    "  - **Correlation Analysis**: Determine the strength and direction of relationships between variables using Pearson’s correlation coefficient.\n",
    "  - **Regression Analysis**: Linear regression to predict educational outcomes and polynomial regression to capture non-linear relationships.\n",
    "\n",
    "- **Machine Learning**:\n",
    "  - **Random Forest Regression**: Utilized to predict outcomes based on multiple input variables and to assess feature importance, providing insights into which factors most significantly impact student performance.\n",
    "  - **Cross-Validation**: Enhance model validation through k-fold cross-validation, ensuring the model’s robustness and generalizability.\n",
    "\n",
    "- **Deep Learning**:\n",
    "  - **Neural Networks**: Deploy neural networks to model complex, non-linear interactions between variables. The multi-layer perceptron architecture facilitates the exploration of deeper patterns in the data, instrumental in uncovering hidden insights.\n",
    "\n",
    "- **Data Visualization**:\n",
    "  - **Plotly and Seaborn**: Generate interactive graphs and static plots to visualize data distributions, correlations, and regression outcomes effectively. This includes creating heatmaps for correlation matrices, scatter plots for regression analysis, and treemaps for hierarchical data exploration.\n",
    "\n",
    "- **Advanced Data Processing**:\n",
    "  - **Handling Missing Data**: Techniques such as imputation and dropping rows/columns to clean the dataset, ensuring the integrity of the analyses.\n",
    "  - **Feature Engineering**: Includes generating polynomial features for regression models and encoding categorical variables to prepare the dataset for machine learning.\n",
    "\n",
    "These methods collectively facilitate a thorough exploration of the intricate dynamics influencing educational achievements across various demographics and socio-economic backgrounds. Through the intelligent application of these techniques, the project aims to provide actionable insights that could inform policy-making and educational strategies.\n",
    "\n",
    "## Project Aim\n",
    "\n",
    "The overarching goal of this project is to dissect and understand the multitude of factors that influence educational outcomes for students assessed in the PISA 2022 survey. By leveraging a comprehensive dataset provided by the OECD, this analysis seeks to uncover the nuanced interplay between students' academic performances and their demographic, socio-economic, and environmental contexts.\n",
    "\n",
    "### Objectives:\n",
    "1. **Identify Key Factors**: Determine the primary demographic, socio-economic, and educational variables that significantly impact students' scores in mathematics, reading, and science.\n",
    "2. **Model Educational Outcomes**: Utilize advanced statistical methods and machine learning algorithms to predict educational outcomes and interpret the relative importance of each predictor. This will include assessing the impact of factors such as access to technology, parental education levels, and school environments on student performance.\n",
    "3. **Evaluate Policy Implications**: Analyze the data to provide evidence-based recommendations to educational authorities and policymakers. The aim is to identify potential areas for intervention that could lead to improvements in educational equity and effectiveness.\n",
    "4. **Promote Educational Equity**: Explore how differences in educational access and quality affect performance across various groups, aiming to highlight disparities and recommend strategies for promoting inclusivity and fairness in education.\n",
    "\n",
    "### Impact:\n",
    "The insights derived from this project are intended to inform and enhance educational policies and practices worldwide. By understanding the factors that drive educational success, stakeholders can implement targeted interventions to support underperforming groups, optimize educational resources, and ultimately raise the standard of education provided to all students. Furthermore, this project aims to stimulate ongoing dialogue among educators, policymakers, and the academic community about how best to harness data-driven insights for educational planning and reform.\n",
    "\n",
    "In summary, this project not only aims to analyze the data from the PISA 2022 survey comprehensively but also seeks to translate these analyses into practical strategies that can lead to real and sustainable improvements in educational systems globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae37d5f-e3c9-4f30-a8ae-15e1cd19a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cdbc8-3bcb-479f-b36a-9f575888e50e",
   "metadata": {},
   "source": [
    "## Foundational Libraries\n",
    "\n",
    "- **warnings**: Utilized to manage warnings during runtime, particularly for suppressing specific warning categories that might clutter the output, ensuring a clean presentation of results. This is especially useful in a data science context to ignore routine warnings generated by third-party libraries without affecting the interpretation of code execution.\n",
    "\n",
    "- **numpy (np)**: A cornerstone for numerical computing in Python, numpy offers comprehensive support for arrays and matrices alongside a vast library of mathematical functions to operate on these data structures. In our project, it is indispensable for handling numerical operations on arrays efficiently, which underpins various data manipulation tasks.\n",
    "\n",
    "- **pandas (pd)**: Essential for data manipulation and analysis, pandas provides data structures and operations for manipulating numerical tables and time series. This library is crucial in our project for reading, writing, and processing data from various file formats. It enables sophisticated data manipulation capabilities such as merging, reshaping, selecting, as well as robust handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92c2e22-be95-4415-8a6f-86450c3d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import shapiro, randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.regressionplots import add_lowess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3468-e5b0-41f9-b93b-cb4af502bed7",
   "metadata": {},
   "source": [
    "## Statistical and Machine Learning Libraries\n",
    "\n",
    "- **scipy.stats, shapiro, sp_randint**: This suite of tools from the SciPy library supports the generation of random variables, conducting statistical tests, and exploratory data analysis. `shapiro` tests for normality, essential for validating assumptions in many statistical models, while `sp_randint` is used for generating discrete random numbers for hyperparameter tuning.\n",
    "\n",
    "- **sklearn.model_selection**: Includes submodules like `train_test_split` for dividing data into training and test sets, `RandomizedSearchCV` for optimizing model parameters through random search, and `cross_val_score` for evaluating a model's performance using cross-validation.\n",
    "\n",
    "- **sklearn.preprocessing**: Contains `PolynomialFeatures` for generating polynomial and interaction features and `StandardScaler` for feature scaling by standardizing variables. These preprocessing tools are vital for enhancing model performance and accuracy.\n",
    "\n",
    "- **sklearn.linear_model**: Provides models like `LinearRegression` for standard linear regression analysis and `SGDRegressor` for linear models fitted by stochastic gradient descent, a practical approach for large datasets.\n",
    "\n",
    "- **sklearn.ensemble**: Features `RandomForestRegressor`, an ensemble method based on randomized decision trees, known for its high accuracy and robustness against overfitting, particularly useful for regression tasks in complex datasets.\n",
    "\n",
    "- **sklearn.metrics**: Includes performance metrics such as `mean_squared_error` for quantifying the accuracy of regression models and `r2_score` for assessing the proportion of variance captured by the model.\n",
    "\n",
    "- **statsmodels.api**: Offers classes and functions for the estimation of different statistical models, as well as for conducting statistical tests and data exploration. An essential tool for in-depth statistical analysis, often used for regression diagnostics, time-series analysis, and hypothesis testing.\n",
    "\n",
    "- **statsmodels' diagnostic tools**: `het_breuschpagan` tests for heteroscedasticity, `durbin_watson` assesses autocorrelation in residuals from a regression, `variance_inflation_factor` evaluates multicollinearity, and `add_lowess` (locally weighted scatterplot smoothing) is useful for trend fitting in regression diagnostics. These tools provide deeper insights into the model's assumptions and performance, ensuring robust statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015bb99c-a8b6-4404-ae8c-bccba480c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c6cd5-26e0-4742-ac89-4460ada1630e",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks: TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow**: An open-source library developed by Google for numerical computation and machine learning. TensorFlow provides a broad toolkit for developing and training machine learning models, including powerful features for deep learning. It is used in this project to harness complex patterns in the data that simpler models might miss, especially beneficial for large datasets with intricate features.\n",
    "\n",
    "- **tensorflow.keras.models**: Contains `Sequential`, which is a linear stack of layers used for creating models. The Sequential model is straightforward to understand and use, which is particularly useful for standard deep learning applications where layers are added in sequence.\n",
    "\n",
    "- **tensorflow.keras.layers**: Provides various layers, including `Dense`, which is a regular densely-connected neural network layer. Dense layers are fundamental in neural networks for learning high-level patterns in large data sets and are used extensively in our models to process and learn from educational data. Each neuron in a Dense layer receives input from all neurons of the previous layer, thus being well-suited for pattern recognition tasks found in complex datasets like PISA.\n",
    "\n",
    "These tools are integral to building neural network architectures, facilitating the exploration and implementation of deep learning models that can potentially reveal nonlinear relationships and interactions not detectable by traditional statistical methods. This capability is particularly valuable in educational research, where interactions between variables are complex and multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a53dc3f-ccf4-4c29-ac1a-1f5ddb44b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import squarify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576052-a06e-4620-946d-2ee6ed74f824",
   "metadata": {},
   "source": [
    "## Data Visualization Libraries\n",
    "\n",
    "- **Matplotlib (plt)**: A powerful plotting library for Python, Matplotlib is fundamental for creating static, interactive, and animated visualizations in Python. In this project, Matplotlib is utilized primarily for generating histograms, scatter plots, and more complex visualizations like residual plots, providing a traditional and detailed approach to data visualization.\n",
    "\n",
    "- **Seaborn (sns)**: Built on top of Matplotlib, Seaborn extends its functionality, making it easier to generate complex visualizations with more attractive and informative statistical graphics. This library is used for creating enhanced visualizations such as heatmaps for correlation matrices, which are crucial for identifying relationships between variables in the dataset.\n",
    "\n",
    "- **Plotly (go, px, ff)**: A modern platform for creating interactive plots and dashboards. Plotly's Python graphing libraries `plotly.graph_objects` and `plotly.express` offer an extensive range of interactive plotting options that enhance user engagement with the data. Plotly is used for creating dynamic visualizations like choropleth maps, interactive scatter plots, and detailed histograms that allow stakeholders to delve deeper into the data insights through zooming, panning, and hovering to display additional data details.\n",
    "\n",
    "- **Squarify**: This library visualizes hierarchical data with adjustable-sized rectangular tree maps, allowing for effective space-filling representations of proportions amongst categories. In the project, Squarify is used to produce treemaps that visually represent the distribution of observations across various categories, such as different countries or educational variables, making it easier to understand complex hierarchical relationships.\n",
    "\n",
    "The combined capabilities of these libraries enable a comprehensive suite of visual tools that support both exploratory data analysis and the presentation of findings in a format conducive to stakeholder understanding and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d26db4a-4967-4d63-859f-66502f48f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "import pyreadstat\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bebfa-68c4-4670-a7ae-31b0c95405a3",
   "metadata": {},
   "source": [
    "## Data Import and Country Code Handling Libraries\n",
    "\n",
    "- **sas7bdat (SAS7BDAT)**: This library is specifically designed for reading SAS data files in the `.sas7bdat` format, which are often used in large-scale data analysis contexts. In our project, `sas7bdat` enables the direct importation of the PISA dataset stored in this format, ensuring that data from such specialized formats is accessible for analysis in Python without the need for conversion.\n",
    "\n",
    "- **pyreadstat**: A library that facilitates the reading and writing of SAS data files along with associated metadata. It provides a convenient bridge to work with `.sas7bdat` files in Python, similarly to how `sas7bdat` is used, but with additional support for reading the metadata, which can be crucial for understanding data structure and contents. This feature is essential for preprocessing steps in the project, as it allows a deeper understanding and manipulation of the data based on its inherent properties.\n",
    "\n",
    "- **pycountry**: Utilized for converting country names and codes between different standards (e.g., ISO, FIPS). In the project, `pycountry` is crucial for mapping country names from the dataset to their corresponding ISO alpha-3 codes. This capability supports the integration of the data with other global datasets and aids in the creation of geographically accurate visualizations such as choropleth maps, enhancing the geographic data analysis aspect of the project.\n",
    "\n",
    "These libraries collectively streamline the data import process from specialized formats and enhance the geographical mapping of data, key aspects that underpin the robust analysis and visualization capabilities of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27003a-78eb-4863-a469-d9785f940506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55a906-63cc-49cd-8eed-4b99e6f2e037",
   "metadata": {},
   "source": [
    "## Enhanced Display Handling with IPython\n",
    "\n",
    "- **IPython.display.HTML**: Part of the IPython ecosystem, this library is essential for embedding rich HTML content in Jupyter notebooks. IPython's `HTML` module enables the creation and rendering of HTML content dynamically within notebook cells. This functionality is particularly useful in our project for presenting information in a more visually appealing and interactive format than is possible with plain text output.\n",
    "\n",
    "In this project, `HTML` is used to enhance the presentation of data and analytics results, allowing for the custom formatting of outputs, which can include styling with CSS, embedding images or interactive elements, and more. This capability significantly improves the readability and user interaction with the project's outputs, making it easier for stakeholders to engage with and understand complex data insights directly within the Jupyter notebook environment.\n",
    "\n",
    "The use of `HTML` in IPython effectively bridges the gap between standard data output and a more polished, professional presentation format, catering to both technical and non-technical audiences. This ensures that our data visualizations and results are not only informative but also compelling and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a71c276-f99d-45b6-a14c-009c2d00bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747b2f2-5b2a-4b0b-87c9-3c2a3d0dcb3b",
   "metadata": {},
   "source": [
    "## Managing Warnings in Python\n",
    "\n",
    "- **warnings**: This module is a powerful tool for handling warnings in Python programs. In data science projects, particularly those involving multiple libraries that may not always align perfectly in terms of dependencies or deprecated features, managing warnings effectively is crucial to maintain a clean and readable output. Using `warnings.filterwarnings('ignore', category=FutureWarning)` specifically instructs Python to ignore warnings about future changes to the libraries' APIs or deprecated features that have not yet been removed but will be in future versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a26891-9542-42b2-8f06-803ab8aa0c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m html \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</ul>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Отображаем HTML\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m display(\u001b[43mHTML\u001b[49m(html))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = 'cy08msp_stu_qqq.sas7bdat'\n",
    "\n",
    "columns_to_load = [\n",
    "    \"CNT\", \"OECD\", \"ST004D01T\", \"ST001D01T\", \"ST126Q01TA\", \"ST125Q01NA\",\n",
    "    \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\",\n",
    "    \"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\",\n",
    "    \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\",\n",
    "    \"ESCS\", \"IMMIG\", \"BELONG\", \"BULLIED\", \"FEELSAFE\",\n",
    "    \"STUDYHMW\", \"DISCLIM\", \"ADMINMODE\", \"ST019CQ01T\",\n",
    "    \"ICTHOME\", \"MATHMOT\"\n",
    "]\n",
    "\n",
    "df, metadata = pyreadstat.read_sas7bdat(file_path, usecols=columns_to_load)\n",
    "\n",
    "html = '<ul>'\n",
    "html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "html += '</ul>'\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f215a2-70ba-4686-9996-7a2298a0faa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
