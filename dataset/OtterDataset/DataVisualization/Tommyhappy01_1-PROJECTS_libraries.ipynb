{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg_ZAS0B2slE"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EjVhtzq2slH"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqV3cXW-2slL"
   },
   "source": [
    "Welcome to \"***Employee Churn Analysis Project***\". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. \n",
    "\n",
    "Also you will learn what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** package. \n",
    "\n",
    "You will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with the Random Forest, Gradient Descent Boosting , KNN algorithms.\n",
    "\n",
    "At the end of the project, you will have the opportunity to deploy your model using *Streamlit*.\n",
    "\n",
    "Before diving into the project, please take a look at the determines and project structure.\n",
    "\n",
    "- NOTE: This project assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind K-Means, Gradient Boosting , KNN, Random Forest, and Confusion Matrices. You can try more models and methods beside these to improve your model metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oRnVXpS2slN"
   },
   "source": [
    "# #Determines\n",
    "In this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.\n",
    "\n",
    "The HR dataset has 14,999 samples. In the given dataset, you have two types of employee one who stayed and another who left the company.\n",
    "\n",
    "You can describe 10 attributes in detail as:\n",
    "- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.\n",
    "- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.\n",
    "- ***number_projects:*** How many of projects assigned to an employee?\n",
    "- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n",
    "- **time_spent_company:** time_spent_company means employee experience. The number of years spent by an employee in the company.\n",
    "- ***work_accident:*** Whether an employee has had a work accident or not.\n",
    "- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n",
    "- ***Departments:*** Employee's working department/division.\n",
    "- ***Salary:*** Salary level of the employee such as low, medium and high.\n",
    "- ***left:*** Whether the employee has left the company or not.\n",
    "\n",
    "First of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. \n",
    "\n",
    "Then, you must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n",
    "\n",
    "The purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n",
    "\n",
    "Once the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. \n",
    "\n",
    "Try to make your predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***. You can use the related modules of the ***scikit-learn*** library. You can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.\n",
    "\n",
    "In the final step, you will deploy your model using Streamlit tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97xzRLNj2slO"
   },
   "source": [
    "# #Tasks\n",
    "\n",
    "#### 1. Exploratory Data Analysis\n",
    "\n",
    "#### 2. Data Visualization\n",
    "\n",
    "#### 3. Data Pre-Processing\n",
    "\n",
    "#### 4. Cluster Analysis\n",
    "\n",
    "#### 5. Model Building\n",
    "\n",
    "#### 6. Model Deployement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLTGi7q02slP"
   },
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "- Importing Modules\n",
    "- Loading Dataset\n",
    "- Data Insigts\n",
    "----------\n",
    "Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyrWBiyM2sld"
   },
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TI19sGjE2slf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install pyforest\n",
    "\n",
    "# 1-Import Libraies\n",
    "\n",
    "import pandas_profiling\n",
    "import pyforest\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mticker\n",
    "import squarify as sq\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "import plotly.express as px\n",
    "import cufflinks as cf\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "# !pip install termcolor\n",
    "import colorama\n",
    "from colorama import Fore, Style  # maakes strings colored\n",
    "from termcolor import colored\n",
    "from termcolor import cprint\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import missingno as msno \n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor \n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, PowerTransformer, LabelEncoder \n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import plot_tree, DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier, plot_importance\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# Figure&Display options\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Useful Functions\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def missing_values(df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending = False)\n",
    "    missing_percent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending = False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_Number', 'Missing_Percent'])\n",
    "    return missing_values[missing_values['Missing_Number'] > 0]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def first_looking(df):\n",
    "    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']),\n",
    "          colored(\"\\nInfo:\\n\", attrs = ['bold']), sep = '')\n",
    "    print(df.info(), '\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Number of Uniques:\\n\", attrs = ['bold']), df.nunique(),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"All Columns:\", attrs = ['bold']), list(df.columns),'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "\n",
    "    df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n",
    "    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')  \n",
    "    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Descriptive Statistics \\n\", attrs = ['bold']), df.describe().round(2),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n",
    "    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs = ['bold']), df.describe(include = object).T,'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n",
    "    \n",
    "def multicolinearity_control(df):\n",
    "    feature = []\n",
    "    collinear = []\n",
    "    for col in df.corr().columns:\n",
    "        for i in df.corr().index:\n",
    "            if (abs(df.corr()[col][i]) > .9 and abs(df.corr()[col][i]) < 1):\n",
    "                    feature.append(col)\n",
    "                    collinear.append(i)\n",
    "                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n",
    "                                  \"red\", attrs = ['bold']), df.shape,'\\n',\n",
    "                                  colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(colored(\"Duplicate check...\", attrs = ['bold']), sep = '')\n",
    "    print(\"There are\", df.duplicated(subset = None, keep = 'first').sum(), \"duplicated observations in the dataset.\")\n",
    "    duplicate_values = df.duplicated(subset = None, keep = 'first').sum()\n",
    "    if duplicate_values > 0:\n",
    "        df.drop_duplicates(keep = 'first', inplace = True)\n",
    "        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n",
    "              colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "#     else:\n",
    "#         print(colored(\"There are no duplicates\"),'\\n',\n",
    "#               colored('*'*100, 'red', attrs = ['bold']), sep = '')     \n",
    "        \n",
    "# def drop_columns(df, drop_columns):\n",
    "#     if drop_columns != []:\n",
    "#         df.drop(drop_columns, axis = 1, inplace = True)\n",
    "#         print(drop_columns, 'were dropped')\n",
    "#     else:\n",
    "#         print(colored('We will now check the missing values and if necessary, the related columns will be dropped!', attrs = ['bold']),'\\n',\n",
    "#               colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "        \n",
    "def drop_null(df, limit):\n",
    "    print('Shape:', df.shape)\n",
    "    for i in df.isnull().sum().index:\n",
    "        if (df.isnull().sum()[i] / df.shape[0]*100) > limit:\n",
    "            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n",
    "            df.drop(i, axis = 1, inplace = True)\n",
    "            print('new shape:', df.shape)       \n",
    "    print('New shape after missing value control:', df.shape)\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "# To view summary information about the columns\n",
    "\n",
    "def first_look(col):\n",
    "    print(\"column name    : \", col)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Per_of_Nulls   : \", \"%\", round(df[col].isnull().sum() / df.shape[0]*100, 2))\n",
    "    print(\"Num_of_Nulls   : \", df[col].isnull().sum())\n",
    "    print(\"Num_of_Uniques : \", df[col].nunique())\n",
    "    print(\"Duplicates     : \", df.duplicated(subset = None, keep = 'first').sum())\n",
    "    print(df[col].value_counts(dropna = False))\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def fill_most(df, group_col, col_name):\n",
    "    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n",
    "    for group in list(df[group_col].unique()):\n",
    "        cond = df[group_col] == group\n",
    "        mode = list(df[cond][col_name].mode())\n",
    "        if mode != []:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n",
    "        else:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n",
    "    print(\"Number of NaN : \",df[col_name].isnull().sum())\n",
    "    print(\"------------------\")\n",
    "    print(df[col_name].value_counts(dropna = False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Churn Prediction_Student_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
