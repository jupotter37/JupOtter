{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# data visualization\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import Bar, Layout\n",
    "from plotly import offline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号\n",
    "\n",
    "# change text color\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# IPython\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<center><b>【第六阶段】项目日志</b></center>\n",
    "    \n",
    "    \n",
    "第五阶段：【数据建模全解析】\n",
    "   \n",
    "第五节阶段核心目的：全方面掌握数据建模的实用手段和适用场景（线性和树型）\n",
    "    \n",
    "难度（最高5星）：⭐⭐⭐⭐\n",
    "\n",
    "Good Luck!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于不平衡数据，我们不能用传统的accuracy来衡量模型的好坏，本项目给出了一个归一基尼系数，作为本项目的唯一指标。\n",
    "\n",
    "第一阶段已经讲过了！\n",
    "\n",
    "\n",
    "参考：\n",
    "- https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation\n",
    "- https://stats.stackexchange.com/questions/306287/why-use-normalized-gini-score-instead-of-auc-as-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 1</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "1. normalized gini coefficent 的应用场景\n",
    "2. 请写出该指标的算法实现（写个函数`eval_gini`）\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gini(y_true, y_prob):\n",
    "    '''\n",
    "    你的代码\n",
    "    '''\n",
    "    return gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = -eval_gini(labels, preds)\n",
    "    return [('gini', gini_score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合和欠拟合\n",
    "\n",
    "对于机器学习算法来说，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以对未知数据集（测试集）有很好的拟合结果（泛化能力），所产生的测试误差被称为泛化误差。\n",
    "\n",
    "> 利用模型对数据进行拟合，学习的目的并非是对有限训练集进行正确预测，而是对未曾在训练集合出现的样本能够正确预测。\n",
    "\n",
    "那么如何衡量泛化能力的好坏呢？\n",
    "\n",
    "也就是模型的过拟合（overfitting）和欠拟合（underfitting）。\n",
    "\n",
    "![](https://miro.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png)\n",
    "\n",
    "建议观看：https://www.youtube.com/watch?v=EuBBz3bI-aA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 2</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "1. 过拟合欠拟合出现的原因\n",
    "2. 怎么处理过拟合和欠拟合？\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"你的答案\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证（Cross validation)，用于防止模型过于复杂而引起的**过拟合**。\n",
    "\n",
    "交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是**重复的使用数据**，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。　\n",
    "\n",
    "那么什么时候才需要交叉验证呢？\n",
    "\n",
    "交叉验证用在**数据不是很充足的时候**。比如在我日常项目里面，对于普通适中问题，如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。如果样本大于一万条的话，我们一般随机的把数据分成三份，一份为训练集（Training Set），一份为验证集（Validation Set），最后一份为测试集（Test Set）。用训练集来训练模型，用验证集来评估模型预测的好坏和选择模型及其对应的参数。把最终得到的模型再用于测试集，最终决定使用哪个模型以及对应参数。\n",
    "\n",
    "![d9812a29f03c28c3b87694aa0cad613.jpg](https://img.wang.232232.xyz/img/2022/06/18/d9812a29f03c28c3b87694aa0cad613.jpg)\n",
    "\n",
    "交叉验证：\n",
    "https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 3</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "1. 交叉验证的用途有哪些？\n",
    "2. 交叉验证的方法有哪些?\n",
    "3. 时间序列的交叉验证方法和普通数据的区别\n",
    "4. 验证集和测试集的区别\n",
    "5. 完善下列代码\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"你的答案\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 10\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性算法\n",
    "\n",
    "\n",
    "![](https://saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "逻辑回归是什么？为什么要引入？\n",
    "\n",
    "> 一句话概括：逻辑回归假设数据服从伯努利分布，通过极大似然化函数的方法，运用梯度下降来求解参数，从而达到二分类的目的。\n",
    "\n",
    "首先逻辑回归虽名为回归，但并不是一个**回归方法**，它主要用来解决二分类问题，面对分类问题，线性回归显得无能为力，从下图可以看到，线性回归在对作为离散变量存在的y时，只能用直线进行拟合，并且所预测的y可能会超过0到1的范围，并且对异常值非常敏感。\n",
    "\n",
    "那么我们想引入非线性元素，来解决以上两个问题，我们希望输出的值$p(y=1|X;\\theta)$保证在 0 到 1 之间，并且对异常值不再敏感。\n",
    "\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zg104.github.io/Logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 4</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "1. Why we use logistic regression, not linear regression? What are the disadvantages of linear regression for classification?\n",
    "\n",
    "2. What type of datasets is most suited for logistic regression?\n",
    "\n",
    "3. Can you explain or interpret the hypothesis output of logistic regression?\n",
    "\n",
    "4. Why we define the sigmoid function, create a new version of cost function, and applied MLE to derive logistic regression?\n",
    "\n",
    "5. What are the disadvantage of logistic regression?\n",
    "\n",
    "6. How to deal with overfitting?\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"你的答案\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.read_csv('final_train.csv',index_col=0)\n",
    "final_test = pd.read_csv('final_test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"5\"></span>In order to get results between 0 and 1, a function, which is called **sigmoid**, is used to transform our hypothesis function. It is defined as\n",
    "$$ $$\n",
    "$$h_{\\theta}(x) = g(\\theta^{T} x)$$ \n",
    "$$ $$\n",
    "where $h_{\\theta}(x)$ is the hypothesis function, $x$ is a single record and \n",
    "$$ $$\n",
    "$$g(z)=\\dfrac{1}{1+e^{-z}}$$\n",
    "$$ $$\n",
    "By using $g(\\theta^{T} x)$, we obtain the probablity and if $h_{\\theta}(x) \\geq 0.5$, we get $y=1$; if $h_{\\theta}(x) < 0.5$, we get $y=0$. Further, when $z \\geq 0$, $g(z) \\geq 0.5$ is another detail. Thus, if the $\\theta^{T} x \\geq 0$, then $y=1$.\n",
    " \n",
    "By the definition, I defined the below ***sigmoid*** function.<span id=\"5\"></span>\n",
    "\n",
    "We can't use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. That's why we need to define a different cost function for logistic regression. It is simply defined as\n",
    "$$ $$\n",
    "$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$$ \n",
    "$$ $$\n",
    "where \n",
    "$$ $$\n",
    "$$Cost(h_{\\theta}(x^{(i)}), y^{(i)})=-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))$$\n",
    "$$ $$\n",
    "As the sanity check, $J(\\theta)$ can be plotted or printed as a function of the number of iterations to be sure that $J(\\theta)$ is **decreasing on every iteration**, which shows that it is converging correctly. At this point, choice of $\\alpha$ is important. If we select a high or small $\\alpha$ value, we might have problem about the converging.<span id=\"6\"></span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "# 1. 写出Sigmoid function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return \"你的代码\"\n",
    "    \n",
    "# 2. loss function \n",
    "def loss(h, y):\n",
    "    return \"你的代码\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "1. 完善上方代码\n",
    "2. 你能推导一下cost function么？这可是面试题\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树型算法\n",
    "\n",
    "XGB视频：https://www.youtube.com/watch?v=GrJP9FLV3FE\n",
    "\n",
    "什么是XGB?\n",
    "\n",
    "XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，堪称表格型数据大杀器！\n",
    "\n",
    "那么说到XGB不得不说一下gbdt。\n",
    "\n",
    "GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是Boosting的思想。\n",
    "\n",
    "那么啥是boosting思想？那就得先说说bagging（集成）\n",
    "\n",
    "如果你对随机森林有点了解，你应该知道它是个集成算法，也就是【人多力量大】的思想，也叫bagging。\n",
    "\n",
    "Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。\n",
    "\n",
    "大部分情况下，经过 bagging 得到的结果方差（variance）更小。\n",
    "\n",
    "具体过程：\n",
    "\n",
    "1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n",
    "2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n",
    "3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n",
    "\n",
    "ok所以你应该了解什么是bagging了，那么boosting的思想是什么？\n",
    "\n",
    "那就是【精英筛选】\n",
    "\n",
    "Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。\n",
    "\n",
    "大部分情况下，经过 boosting 得到的结果偏差（bias）更小。\n",
    "\n",
    "具体过程：\n",
    "\n",
    "1. 通过加法模型将基础模型进行线性的组合。\n",
    "2. 每一轮训练都提升那些错误率小的基础模型权重，同时减小错误率高的模型权重。\n",
    "3. 在每一轮改变训练数据的权值或概率分布，通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。\n",
    "\n",
    "---\n",
    "\n",
    "那么再回到gbdt来，原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差。\n",
    "\n",
    "举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？\n",
    "\n",
    "1. 它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；\n",
    "2. 接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；\n",
    "3. 接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；\n",
    "4. 最后在第四课树中用1岁拟合剩下的残差，完美。\n",
    "5. 最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。\n",
    "\n",
    "> 学习并不断改正错误，直至残差小于阈值。\n",
    "\n",
    "那么XGB其实是gbdt的优化版本，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。\n",
    "\n",
    "1. GBDT是机器学习算法，XGBoost是该算法的工程实现。\n",
    "2. XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。\n",
    "3. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。\n",
    "4. XGBoost支持多种类型的基分类器\n",
    "5. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样（相当于dropout）避免过拟合\n",
    "6. 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。\n",
    "7. xgboost工具支持特征颗粒度下的并行\n",
    "\n",
    "> 决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。\n",
    ">\n",
    ">这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。\n",
    ">\n",
    ">当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://dzone.com/storage/temp/13069535-xgboost-features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数调节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 5</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "描述下面这些参数的使用方法和意义\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_estimators: \n",
    "\n",
    "- max_depth:\n",
    "\n",
    "- objective: \n",
    "\n",
    "- learning_rate: \n",
    "\n",
    "- subsample: \n",
    "\n",
    "- min_child_weight: \n",
    "\n",
    "- colsample_bytree: \n",
    "\n",
    "- scale_pos_weight: \n",
    "\n",
    "- gamma: \n",
    "\n",
    "- reg_alpha:\n",
    "\n",
    "- reg_lambda: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的参数有很多，如何选择合适的参数，是个很头疼的问题，所以建模的玄学就在这里，一些超参数必须主动调节，可以固定筛选范围，一个一个试，看哪个表现更好，但是这么做成本巨大，下面这一堆代码，运行完，大概要16min。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5, 10],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5]\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier(learning_rate=0.06, n_estimators=300, objective='binary:logistic',nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# def timer(start_time=None):\n",
    "#     if not start_time:\n",
    "#         start_time = datetime.now()\n",
    "#         return start_time\n",
    "#     elif start_time:\n",
    "#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "#         tmin, tsec = divmod(temp_sec, 60)\n",
    "#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "        \n",
    "        \n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "# folds = 3\n",
    "# param_comb = 5\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n",
    "\n",
    "# # Here we go\n",
    "# start_time = timer(None)\n",
    "# random_search.fit(X, y)\n",
    "# timer(start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n All results:')\n",
    "# print(random_search.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(random_search.best_estimator_)\n",
    "# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "# print(random_search.best_score_ * 2 - 1)\n",
    "# print('\\n Best hyperparameters:')\n",
    "# print(random_search.best_params_)\n",
    "# results = pd.DataFrame(random_search.cv_results_)\n",
    "# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 调参之后，较优的参数组合\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "MAX_ROUNDS = 400\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50  \n",
    "\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=6,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=8,\n",
    "                        reg_lambda=1.3,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Step 5</b>: \n",
    "    \n",
    "完成下面题目：\n",
    "\n",
    "1. 尝试写出`from target_encoding import target_encode`的py文件（根据phase5）\n",
    "2. 完善代码\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_gini(df_train,tar_enc = True,pca = False):\n",
    "    \n",
    "    '''\n",
    "    df_train: 已处理的训练集数据\n",
    "    tar_enc: 是否对类别型变量使用target encoding\n",
    "    pca: 是否使用pca\n",
    "    '''    \n",
    "    \n",
    "    y = df_train.target\n",
    "    X = df_train.drop('target',axis=1)\n",
    "    \n",
    "    \n",
    "    y_valid_pred = 0*y\n",
    "    y_test_pred = 0\n",
    "    \n",
    "    \n",
    "    from target_encoding import target_encode\n",
    "    \n",
    "    train = pd.concat([X,y],axis=1)\n",
    "    \n",
    "    # k折交叉验证\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "\n",
    "        # 分成训练集、验证集、测试集\n",
    "\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()        \n",
    "        X_test = final_test.copy()\n",
    "        \n",
    "        # 如果想使用pca，降维至20维（超参数）\n",
    "        if pca == True:\n",
    "            n_comp = 20\n",
    "            print('\\nPCA执行中...')\n",
    "            pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n",
    "            X_train = pd.DataFrame(pca.fit_transform(X_train))\n",
    "            X_valid = \"你的代码\"\n",
    "            X_test = \"你的代码\"\n",
    "        print( f\"\\n{i}折交叉验证： \")\n",
    "        \n",
    "        if pca == False:\n",
    "            \n",
    "            # 如果tar_enc为true，对所有的cat变量进行target encoding\n",
    "            if tar_enc == True:\n",
    "                f_cat = [f for f in X.columns if '_cat' in f and 'tar_enc' not in  f]\n",
    "                for f in f_cat:\n",
    "                    X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                                    trn_series=X_train[f],\n",
    "                                                                    val_series=X_valid[f],\n",
    "                                                                    tst_series=X_test[f],\n",
    "                                                                    target=y_train,\n",
    "                                                                    min_samples_leaf=100,\n",
    "                                                                    smoothing=10,\n",
    "                                                                    noise_level=0\n",
    "                                                                    )\n",
    "\n",
    "    #     from category_encoders.target_encoder import TargetEncoder\n",
    "    #     tar_enc = TargetEncoder(cols = f_cat).fit(X_train,y_train)\n",
    "    #     X_train = tar_enc.transform(X_train) # 转换训练集\n",
    "    #     X_test = tar_enc.transform(X_test) # 转换测试集\n",
    "\n",
    "\n",
    "            X_train.drop(f_cat,axis=1,inplace=True)\n",
    "            X_valid.drop(f_cat,axis=1,inplace=True)\n",
    "            X_test.drop(f_cat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "        # 对于当前折，跑XGB\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit(\"你的代码\")\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "        # 生成验证集的预测结果\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  normalized gini coefficent = \", \"你的代码\")\n",
    "        y_valid_pred.iloc[test_index] = pred\n",
    "\n",
    "        # 累积计算测试集预测结果\n",
    "        y_test_pred += \"你的代码\"\n",
    "\n",
    "        del X_test, X_train, X_valid, y_train\n",
    "\n",
    "    y_test_pred /= K  # 取各fold结果均值\n",
    "\n",
    "    print( \"\\n整个训练集（合并）的normalized gini coefficent:\" )\n",
    "    print( \"  final normalized gini coefficent = \", eval_gini(y, y_valid_pred) )\n",
    "    \n",
    "    return y_test_pred,eval_gini(y, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0折交叉验证： \n",
      "  normalized gini coefficent =  0.2538658383235931\n",
      "\n",
      "1折交叉验证： \n",
      "  normalized gini coefficent =  0.30728575577060424\n",
      "\n",
      "2折交叉验证： \n",
      "  normalized gini coefficent =  0.28508943739615844\n",
      "\n",
      "3折交叉验证： \n",
      "  normalized gini coefficent =  0.2829810430876034\n",
      "\n",
      "4折交叉验证： \n",
      "  normalized gini coefficent =  0.29113416245470913\n",
      "\n",
      "5折交叉验证： \n",
      "  normalized gini coefficent =  0.2940775662258974\n",
      "\n",
      "6折交叉验证： \n",
      "  normalized gini coefficent =  0.2818130027819927\n",
      "\n",
      "7折交叉验证： \n",
      "  normalized gini coefficent =  0.27666764206685135\n",
      "\n",
      "8折交叉验证： \n",
      "  normalized gini coefficent =  0.2723987713495525\n",
      "\n",
      "9折交叉验证： \n",
      "  normalized gini coefficent =  0.3070892136897676\n",
      "\n",
      "整个训练集（合并）的normalized gini coefficent:\n",
      "  final normalized gini coefficent =  0.28507826718380913\n",
      "Wall time: 18min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred, gini_score = XGB_gini(df_train=final_train,tar_enc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = final_test.index.values\n",
    "submission['target'] = y_test_pred\n",
    "submission.to_csv('xgb_submit.csv', float_format='%.6f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
