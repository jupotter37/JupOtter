{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55b75170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65437d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =open('intents.json', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0e1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.loads(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0d389e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['hello',\n",
       "    'hi',\n",
       "    'hey',\n",
       "    \"what's up\",\n",
       "    'anybody here',\n",
       "    'good day',\n",
       "    'Evening'],\n",
       "   'responses': ['Hello!',\n",
       "    'Hi there, how can I help you today?',\n",
       "    'Feel free to ask me anything',\n",
       "    'Ask me about Data Science!']},\n",
       "  {'tag': 'Target User',\n",
       "   'patterns': ['Who should use this chatbot?',\n",
       "    'Is this chatbot suitable for advanced users?',\n",
       "    'for who they create you',\n",
       "    'Who benefits the most from using this chatbot?'],\n",
       "   'responses': ['This chatbot is designed for beginners and intermediate users looking to explore data science concepts.',\n",
       "    'Ideal users include first-year students, entry-level professionals, and anyone starting their journey in the field.']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'thanks for help',\n",
       "    \"that's helpful\",\n",
       "    'you are a greet help',\n",
       "    'awesome',\n",
       "    'very helpful'],\n",
       "   'responses': ['Happy to help!',\n",
       "    \"You're welcome!\",\n",
       "    'Feel free to ask me again',\n",
       "    'Anytime!',\n",
       "    'My pleasure']},\n",
       "  {'tag': 'data Science',\n",
       "   'patterns': ['How is data science applied in real-world scenarios?',\n",
       "    'Give me examples of data science applications.',\n",
       "    'In what industries is data science most commonly used?',\n",
       "    'Tell me about the role of data scientists.'],\n",
       "   'Responses': ['Data science finds applications in various industries, such as finance, healthcare, marketing, and technology.',\n",
       "    'Common examples of data science include predictive analytics, machine learning, and data-driven decision-making.',\n",
       "    'Data scientists play a crucial role in analyzing large datasets, extracting insights, and making data-driven recommendations for business improvement.']},\n",
       "  {'tag': 'nothing',\n",
       "   'patterns': [],\n",
       "   'responses': ['Please provide more context',\n",
       "    'Not sure I understand',\n",
       "    \"Sorry, I can't understand you\"]},\n",
       "  {'tag': 'options',\n",
       "   'patterns': ['How you could help me?',\n",
       "    'What you can do?',\n",
       "    'What help you provide?',\n",
       "    'How you can be helpful?',\n",
       "    'What support is offered'],\n",
       "   'responses': ['I am a data science chatbot. My capabilities are : I can chat with you and teach you about data science.']},\n",
       "  {'tag': 'jokes',\n",
       "   'patterns': ['Tell me a joke',\n",
       "    'Joke',\n",
       "    'Make me laugh',\n",
       "    'tell me something funny'],\n",
       "   'responses': [\"A perfectionist walked into a bar...apparently, the bar wasn't set high enough\",\n",
       "    'I ate a clock yesterday, it was very time-consuming',\n",
       "    \"Never criticize someone until you've walked a mile in their shoes. That way, when you criticize them, they won't be able to hear you from that far away. Plus, you'll have their shoes.\",\n",
       "    \"The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\",\n",
       "    \"I own the world's worst thesaurus. Not only is it awful, it's awful.\",\n",
       "    'What did the traffic light say to the car? \"Don\\'t look now, I\\'m changing.\"',\n",
       "    'What do you call a snowman with a suntan? A puddle.',\n",
       "    'How does a penguin build a house? Igloos it together',\n",
       "    'I went to see the doctor about my short-term memory problems – the first thing he did was make me pay in advance',\n",
       "    'As I get older and I remember all the people I’ve lost along the way, I think to myself, maybe a career as a tour guide wasn’t for me.',\n",
       "    \"o what if I don't know what 'Armageddon' means? It's not the end of the world.\"]},\n",
       "  {'tag': 'Identity',\n",
       "   'patterns': ['Who are you', 'what are you'],\n",
       "   'responses': ['I am a chatbot that was made to provide you with info about data science',\n",
       "    \"I am a data science chatbot, I was made to give you info about data science and it's related fields\"]},\n",
       "  {'tag': 'creator',\n",
       "   'patterns': ['Who made you', 'who designed you', 'who programmed you'],\n",
       "   'responses': ['I was made by Zaid and Layan.',\n",
       "    'My creators are Layan and Zaid, they are Data Science students from BAU']},\n",
       "  {'tag': 'activity',\n",
       "   'patterns': ['what are you doing', 'what are you upto'],\n",
       "   'responses': ['Talking to you, of course!', 'Just chatting with you!']},\n",
       "  {'tag': 'contact',\n",
       "   'patterns': ['contact developer',\n",
       "    'contact layan',\n",
       "    'contact programmer',\n",
       "    'contact creator',\n",
       "    'contact zaid',\n",
       "    'speak to developers',\n",
       "    'talk to programmers'],\n",
       "   'responses': ['You can contact my creators at theirs Linkedin profiles Zaid : https://www.linkedin.com/in/zaid-allwansah-a09412227/ \\n Layan : https://www.linkedin.com/in/layan-bilbeisi/']},\n",
       "  {'tag': 'data',\n",
       "   'patterns': ['what is data',\n",
       "    'where can I get data',\n",
       "    'how is data used',\n",
       "    'data'],\n",
       "   'responses': ['Data is facts and statistics collected together for reference or analysis, data is collected by a lot of different sources, like the web or sensory devices']},\n",
       "  {'tag': 'data types',\n",
       "   'patterns': ['tell me more about data',\n",
       "    'what are data types',\n",
       "    'different types of data'],\n",
       "   'responses': ['Data has many types like structured, unstrutcured and semi-structured, one of the most used data types is sturcured data, structured data refers to data that resides in a fixed field within a file or record. Like spreadsheets or excel files!']},\n",
       "  {'tag': 'Data Science',\n",
       "   'patterns': ['what is data science?',\n",
       "    'tell me about data science',\n",
       "    'data science',\n",
       "    'explain data science'],\n",
       "   'responses': ['Data science is a multidisciplinary field that involves the use of scientific methods, processes, algorithms, and systems to extract meaningful insights and knowledge from structured and unstructured data.']},\n",
       "  {'tag': 'data engineering',\n",
       "   'patterns': ['whta is data engineering',\n",
       "    'tell me about data engineering',\n",
       "    'defention of data engineering',\n",
       "    'data engineering'],\n",
       "   'responses': ['Data engineering is a field within data science that focuses on the practical application of data collection and processing']},\n",
       "  {'tag': 'data analysis',\n",
       "   'patterns': ['what is data analysis',\n",
       "    'defention of data analysis',\n",
       "    'explain data analysis',\n",
       "    'tell me about data analysis',\n",
       "    'data analysis'],\n",
       "   'responses': ['Data analysis is the process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, drawing conclusions, and supporting decision-making.']},\n",
       "  {'tag': 'salary',\n",
       "   'patterns': ['salary for data science', 'range of salary for data science'],\n",
       "   'responses': ['in USA Entry-Level: \\n$50,000 - $70,000 \\nMid-Level: $70,000 - $90,000 \\nSenior Level: $90,000 - $120,000+']},\n",
       "  {'tag': 'machine learning',\n",
       "   'patterns': ['what is machine learning',\n",
       "    'defention of machine learning',\n",
       "    'explain machine learning',\n",
       "    'tell me more about machine learning',\n",
       "    'machine learning'],\n",
       "   'responses': ['Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computer systems to perform tasks without explicit programming.']},\n",
       "  {'tag': 'data cleaning',\n",
       "   'patterns': ['what is data cleaning',\n",
       "    'how to do data cleaning',\n",
       "    'data cleaning',\n",
       "    'Why is data cleaning important in data science?',\n",
       "    'Can you explain the significance of data cleaning?'],\n",
       "   'responses': ['Data cleaning is a crucial step in the data analysis and data science process. It involves identifying and correcting errors or inconsistencies in datasets to improve their quality and reliability']},\n",
       "  {'tag': 'machine learning algorthim',\n",
       "   'patterns': ['what is the machine learning algorthim',\n",
       "    'algorthims to machine learning'],\n",
       "   'responses': ['these are some popular algorthims: Linear Regression,Random forest,KNN,Decision tree']},\n",
       "  {'tag': 'data preporcessing',\n",
       "   'patterns': ['what is data preprocessing', 'how to do it'],\n",
       "   'responses': [' is a crucial step in the data analysis and machine learning pipeline. It involves cleaning and transforming raw data into a format that is suitable for analysis or for training machine learning models.']},\n",
       "  {'tag': 'Feature engineering',\n",
       "   'patterns': ['what is Feature engineering',\n",
       "    'tell me about Feature engineering',\n",
       "    'Feature engineering',\n",
       "    'Feature engineering explained'],\n",
       "   'responses': ['Feature engineering is the process of creating new features or modifying existing ones to enhance the performance of machine learning models. It involves selecting, transforming, and creating features that provide meaningful and relevant information for the task at hand']},\n",
       "  {'tag': 'Model Evaluation Metrics',\n",
       "   'patterns': ['what is Model Evaluation Metrics',\n",
       "    'tell me about Model Evaluation Metrics',\n",
       "    'explain Model Evaluation Metrics'],\n",
       "   'responses': ['Model evaluation metrics are used to assess the performance of machine learning models such as accuracy.']},\n",
       "  {'tag': 'Supervised',\n",
       "   'patterns': ['what is supervised',\n",
       "    'is supervised type of machine learning',\n",
       "    'explain supervised'],\n",
       "   'responses': ['Supervised refers to a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data used for training is paired with corresponding output labels.']},\n",
       "  {'tag': 'Unsupervised',\n",
       "   'patterns': ['what is unssupervised',\n",
       "    'it is a part of unsupervised',\n",
       "    'explain unsupervised',\n",
       "    'is unsupervised type of machine learning'],\n",
       "   'responses': ['Unsupervised learning is a type of machine learning where the algorithm is trained on data without explicit labels or output categories. In other words, the algorithm explores the inherent structure in the input data without the guidance of predefined output labels.']},\n",
       "  {'tag': 'Reinforcement',\n",
       "   'patterns': ['what is Reinforcement',\n",
       "    'it is a part of Reinforcement',\n",
       "    'explain Reinforcement',\n",
       "    'is Reinforcement type of machine learning'],\n",
       "   'responses': ['Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment.']},\n",
       "  {'tag': 'cross validation',\n",
       "   'patterns': ['what is cross validation', 'explain cross validation'],\n",
       "   'responses': ['Cross-validation is a statistical technique used in machine learning to assess the performance and generalizability of a predictive model.']},\n",
       "  {'tag': 'ensemble learning',\n",
       "   'patterns': ['what is ensemble learning',\n",
       "    'tell me more about ensemble learning'],\n",
       "   'responses': ['Ensemble learning is a machine learning technique that combines the predictions of multiple individual models (learners) to improve overall performance and generalization']},\n",
       "  {'tag': 'hyperparameter tuning',\n",
       "   'patterns': ['what is hyperparameter tuning',\n",
       "    'tell me about hyperparameter tuning',\n",
       "    'explain hyperparameter tuning',\n",
       "    'hyperparameter tuning'],\n",
       "   'responses': ['Hyperparameter tuning, also known as hyperparameter optimization or model selection, is the process of finding the best set of hyperparameters for a machine learning model. ']},\n",
       "  {'tag': 'data visualization',\n",
       "   'patterns': ['what is data visualization',\n",
       "    'tell me more about data visualization',\n",
       "    'explain data visualization',\n",
       "    'data visualization'],\n",
       "   'responses': ['Data visualization is the representation of data in graphical or pictorial formats to help people understand, interpret, and derive insights from complex datasets.']},\n",
       "  {'tag': 'Programming languages',\n",
       "   'patterns': ['what is Programming languages use in data science',\n",
       "    'Programming languages in data science',\n",
       "    'what popluar Programming languages in data science'],\n",
       "   'responses': ['python and R']},\n",
       "  {'tag': 'big data technologies',\n",
       "   'patterns': ['what is the big data technologies', 'big data technologies'],\n",
       "   'responses': ['KNime', 'Hadoop', 'rapid Miner', 'Apache Spark']},\n",
       "  {'tag': 'popular libraries and Framework',\n",
       "   'patterns': ['what is popular libraries and Framework',\n",
       "    'most popular libraries in data science',\n",
       "    'what library uses for data science'],\n",
       "   'responses': ['python (numpy, pandas, matplotlib,seaborn,scikit-learn)',\n",
       "    'TensorFlow']},\n",
       "  {'tag': 'data Science tools',\n",
       "   'patterns': ['the most important tools for data science',\n",
       "    'what tools for data science',\n",
       "    'data Science tools'],\n",
       "   'responses': ['Jupyter notebook',\n",
       "    'SQl (oracle sql, microsoft sql server, postragl sql)\\nknime \\nPower bi\\n Tableau']},\n",
       "  {'tag': 'blog for data science',\n",
       "   'patterns': ['what is the best blog in data science',\n",
       "    'how to know the new in data science',\n",
       "    'blog for data science'],\n",
       "   'responses': ['https://www.smartdatacollective.com/',\n",
       "    'https://whatsthebigdata.com/',\n",
       "    'https://medium.com/kaggle-blog']},\n",
       "  {'tag': 'Natural Language processing',\n",
       "   'patterns': ['what is Natural Language processing',\n",
       "    'explain Natural Language processing',\n",
       "    'what is NLP',\n",
       "    'Natural Language processing',\n",
       "    'NLP'],\n",
       "   'responses': ['Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. The goal of NLP is to enable machines to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.']},\n",
       "  {'tag': 'Time Series Analysis',\n",
       "   'patterns': ['what is Time Series Analysis',\n",
       "    'explain Time Series Analysis',\n",
       "    'defention of Time Series Analysis',\n",
       "    'Time Series'],\n",
       "   'responses': ['Time series analysis is a statistical technique that deals with time-ordered data points. It involves studying the patterns, trends, and behaviors in sequential data to make predictions or gain insights into the underlying processes.']},\n",
       "  {'tag': 'Clustring Algorthims',\n",
       "   'patterns': ['what is Clustring Algorthims',\n",
       "    'explain Clustring Algorthims',\n",
       "    'tell me about Clustring Algorthims',\n",
       "    'Clustring Algorthims'],\n",
       "   'responses': ['Clustering algorithms are used in unsupervised machine learning to group similar data points together based on certain criteria. The goal of clustering is to identify patterns or structures in the data without explicit labels.']},\n",
       "  {'tag': 'Dimensionality Reduction',\n",
       "   'patterns': ['what is Dimensionality Reduction',\n",
       "    'tell me about Dimensionality Reduction',\n",
       "    'explain Dimensionality Reduction'],\n",
       "   'responses': ['Dimensionality reduction is a technique used in machine learning and data analysis to reduce the number of input features or variables in a dataset. The goal is to simplify the dataset while retaining its important characteristics and reducing computational complexity.']},\n",
       "  {'tag': 'Data Science project Lifecycle',\n",
       "   'patterns': ['what is a lifecycle for data science project',\n",
       "    'project in data science',\n",
       "    'what is the steps for data science project'],\n",
       "   'responses': ['1.Define Objectives and Scope 2.Data Collection  3.Data Cleaning and Preprocessing  4.Exploratory Data Analysis (EDA)  5.Feature Engineering  6.Model Development 7.Model Evaluation']},\n",
       "  {'tag': 'kaggle for data science',\n",
       "   'patterns': ['what is kaggle',\n",
       "    'platform for data science',\n",
       "    'where to get data',\n",
       "    'kaggle compition'],\n",
       "   'responses': ['Kaggle is a popular platform for data science competitions, collaborative projects, and learning https://www.kaggle.com/']},\n",
       "  {'tag': 'Data Warehousing',\n",
       "   'patterns': ['what is Data Warehousing',\n",
       "    'Data Warehousing',\n",
       "    'explain Data Warehousing',\n",
       "    'tell me about Data Warehousing'],\n",
       "   'responses': ['Data warehousing is a process of collecting, storing, and managing large volumes of data from various sources to support business intelligence (BI) and analytical reporting.']},\n",
       "  {'tag': 'IDE for data science',\n",
       "   'patterns': ['what is the most popular IDE for data science',\n",
       "    'how to donlowd IDE for data science'],\n",
       "   'responses': ['Jupyter Notebooks: https://www.anaconda.com/',\n",
       "    'colab: https://colab.research.google.com/']},\n",
       "  {'tag': 'EDA',\n",
       "   'patterns': ['what is EDA',\n",
       "    'what is Explorty data analysis',\n",
       "    'EDA',\n",
       "    'tell me about EDA'],\n",
       "   'responses': ['EDA stands for Exploratory Data Analysis, which is a crucial step in the data analysis process. EDA involves the initial exploration and understanding of a dataset to uncover patterns, trends, relationships, and anomalies. ']},\n",
       "  {'tag': 'statistical Description',\n",
       "   'patterns': ['what is statistical Description',\n",
       "    'tell me about statistical Description',\n",
       "    'statistical Description'],\n",
       "   'responses': ['Statistical description refers to the use of statistical measures and techniques to summarize, describe, and interpret the main features of a dataset.']},\n",
       "  {'tag': 'Feature Scaling',\n",
       "   'patterns': ['what is Feature Scaling',\n",
       "    'tell me about Feature Scaling',\n",
       "    'explain Feature Scaling',\n",
       "    'Feature Scaling'],\n",
       "   'responses': ['Feature scaling is a preprocessing step in machine learning that involves transforming the features of a dataset to a standardized range.']},\n",
       "  {'tag': 'Handling missing data',\n",
       "   'patterns': ['how to handle missing data', 'explain Handling missing data'],\n",
       "   'responses': ['1. Dropping Missing Values  2.Imputation  3.Forward Fill and Backward Fill  4.Interpolation  5. Creating a Missing Indicator']},\n",
       "  {'tag': 'Deep learning',\n",
       "   'patterns': ['what is Deep learning',\n",
       "    'tell me more about Deep learning',\n",
       "    'Deep learning',\n",
       "    'defention of deep learning'],\n",
       "   'responses': ['Deep learning is a subfield of machine learning that focuses on artificial neural networks and deep neural networks. It involves training complex models, often with multiple layers, to learn hierarchical representations of data']},\n",
       "  {'tag': 'skills for data science',\n",
       "   'patterns': ['what skills need for data science',\n",
       "    'skills for data science'],\n",
       "   'responses': ['1.Programming Languages \\n2.Statistical Knowledge \\n3.Data Manipulation and Analysis  \\n4.Data Visualization \\n5. Machine Learning  ]n6.SQL Database Knowledge  \\n7. Big Data Technologies  \\n8. Data Cleaning and Preprocessing  \\n9. Feature Engineering']},\n",
       "  {'tag': 'problem solving skills',\n",
       "   'patterns': ['problem solving skills for data science', 'problem solving'],\n",
       "   'responses': ['Problem-solving skills are crucial for data scientists, as they play a key role in formulating and addressing complex challenges using data-driven approaches.']},\n",
       "  {'tag': 'SQL',\n",
       "   'patterns': ['SQL in data science', 'it is sql important for data science'],\n",
       "   'responses': ['In data science, SQL is commonly used for querying, filtering, aggregating, and transforming data stored in relational databases.']},\n",
       "  {'tag': 'statistics',\n",
       "   'patterns': ['statistics in data science',\n",
       "    'what is statistics in data science'],\n",
       "   'responses': ['Statistics is a branch of mathematics that involves the collection, analysis, interpretation, presentation, and organization of data. ']},\n",
       "  {'tag': 'Data Mining',\n",
       "   'patterns': ['what is Data Mining',\n",
       "    'tell me more about Data Mining',\n",
       "    'data mining',\n",
       "    'defention of Data Mining'],\n",
       "   'responses': ['Data mining is the process of discovering patterns, trends, correlations, or valuable information from large datasets using various methods, including machine learning, statistical analysis, and database systems.']},\n",
       "  {'tag': 'Web Scraping',\n",
       "   'patterns': ['what is Web Scraping', 'Web Scraping in data science'],\n",
       "   'responses': ['Web scraping is the process of extracting data from websites. It involves fetching the HTML of a web page, parsing it, and extracting the desired information.']},\n",
       "  {'tag': 'Data Engineering',\n",
       "   'patterns': ['what is Data Engineering',\n",
       "    'tell me about Data Engineering',\n",
       "    'explain Data Engineering'],\n",
       "   'responses': ['Data engineering is a field of study and practice that focuses on designing, developing, and managing the architecture, tools, and systems for collecting, storing, processing, and analyzing data.']},\n",
       "  {'tag': 'company in data science',\n",
       "   'patterns': ['what is the company work in data science in jordan'],\n",
       "   'responses': ['all banks in jordan',\n",
       "    'Zain',\n",
       "    'Orange',\n",
       "    'Umniah',\n",
       "    'Shai for Ai',\n",
       "    'big 4 organization (pwc, E&Y, KMPG, Deloitte) ']},\n",
       "  {'tag': 'Data Analysis',\n",
       "   'patterns': ['what is Data Analysis',\n",
       "    'tell me about Data Analysis',\n",
       "    'explain Data Analysis'],\n",
       "   'responses': ['Data analysis is the process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making.']},\n",
       "  {'tag': 'Youtube Channels Data analysis',\n",
       "   'patterns': ['what is the Youtube Channels in data analysis',\n",
       "    'best youtube channels in data analysis '],\n",
       "   'responses': ['https://www.youtube.com/@AlexTheAnalyst , https://www.youtube.com/@codebasics']},\n",
       "  {'tag': 'Youtube Channels Python',\n",
       "   'patterns': ['what is the Youtube Channels to learn python',\n",
       "    'best youtube channels in python '],\n",
       "   'responses': ['https://www.youtube.com/watch?v=_uQrJ0TkZlc&t=173s']},\n",
       "  {'tag': 'Youtube Channels Machine learning',\n",
       "   'patterns': ['what is the Youtube Channels to learn Machine learning',\n",
       "    'best youtube channels in machine learning '],\n",
       "   'responses': ['https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU']},\n",
       "  {'tag': 'chatbots vs LLM',\n",
       "   'patterns': ['what is the difference between you and chatGPT',\n",
       "    'how you and Bard different',\n",
       "    'is chatGPT better than you',\n",
       "    'is Bard a better chatbot',\n",
       "    'Large Language Model',\n",
       "    'LMM',\n",
       "    'you vs llm',\n",
       "    'chatbots vs LLMs',\n",
       "    'chatGPT',\n",
       "    'bard'],\n",
       "   'responses': [\"If you are comparing me to a Large Language Model (LLM) then the (LLM) wins by a long shot, I'm just a chatbot with a static data and limited AI capabilities\",\n",
       "    'Large Language Models (LLMs) are much better than me in providing information, while my data is limited to data science, LLMs dataset has a wider range',\n",
       "    \"I am not a Large Language Model (LLM) I'm just a regular chatbot, I'm sorry if you got disappointed\"]},\n",
       "  {'tag': 'chatbots',\n",
       "   'patterns': ['chatbot', 'chatbots', 'what are chatbots', 'why chatbots'],\n",
       "   'responses': ['A chatbot is a software application or web interface that is designed to mimic human conversation through text or voice interactions. but you can only interact with me through text, sorry if you got diappointed',\n",
       "    'a chatbot is a computer program that simulates and processes human conversation (either written or spoken), allowing humans to interact with digital devices as if they were communicating with a real person',\n",
       "    'Chatbots can be as simple as rudimentary programs that answer a simple query with a single-line response, or as sophisticated as digital assistants that learn and evolve to deliver increasing levels of personalization as they gather and process information.']},\n",
       "  {'tag': 'LLM',\n",
       "   'patterns': ['what is a large language model',\n",
       "    'llm',\n",
       "    'what is chatgpt',\n",
       "    'what is bard',\n",
       "    'large language models',\n",
       "    'what are llms',\n",
       "    'what is an llm'],\n",
       "   'responses': ['A large language model (LLM) is a deep learning algorithm that can perform a variety of natural language processing (NLP) tasks. Large language models use transformer models and are trained using massive datasets — hence, large. This enables them to recognize, translate, predict, or generate text or other content.',\n",
       "    'Large language models are also referred to as neural networks (NNs), which are computing systems inspired by the human brain. These neural networks work using a network of nodes that are layered, much like neurons.',\n",
       "    'Large language models (LLMs) are recent advances in deep learning models to work on human languages. Some great use case of LLMs has been demonstrated. A large language model is a trained deep-learning model that understands and generates text in a human-like fashion. Behind the scene, it is a large transformer model that does all the magic.']},\n",
       "  {'tag': 'limitations',\n",
       "   'patterns': ['what is your limits',\n",
       "    'do you have limits',\n",
       "    'how far can I go in asking you',\n",
       "    'how scalable are you',\n",
       "    'how big are you',\n",
       "    'are you limited'],\n",
       "   'responses': ['I am limited by the data my devolopers have provided',\n",
       "    \"I'm not that big of a chatbot, but I'll try my best to answer your questions!\",\n",
       "    \"Please ask me anything related to data science and I'll do my best to answer you!\",\n",
       "    'Sorry if you got disappointed, but my data is limited to data science topics']},\n",
       "  {'tag': 'BAU',\n",
       "   'patterns': ['what is bau',\n",
       "    'bau',\n",
       "    'what university did your creators go to',\n",
       "    'where did your creators study',\n",
       "    'what is your developers university',\n",
       "    'balqaa applied university',\n",
       "    'AlBalqa Applied Universit',\n",
       "    'Al-Balqaʼ Applied University'],\n",
       "   'responses': ['Al-Balqa Applied University (BAU) is the place where my developers studied, BAU is considred to be one of the biggest Universitis in Jordan since it is divided to 18 different branches. The main branch is located in Salt, Jordan',\n",
       "    \"BAU is is a government-supported university located in Salt, Jordan, was founded in 1997, a distinctive state university in the field of Bachelor and associate degree Applied Education, at the capacity of more than 21,000 student distributed into 10,000 at the bachelor's degree program and 11,000 at the associate degree program.\"]},\n",
       "  {'tag': 'AI',\n",
       "   'patterns': ['what is ai',\n",
       "    'ai',\n",
       "    'Artificial intelligence',\n",
       "    'what is artficial intelligence'],\n",
       "   'responses': ['Artificial intelligence (AI), the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. The term is frequently applied to the project of developing systems endowed with the intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience. Still, despite continuing advances in computer processing speed and memory capacity, there are as yet no programs that can match full human flexibility over wider domains or in tasks requiring much everyday knowledge. On the other hand, some programs have attained the performance levels of human experts and professionals in performing certain specific tasks, so that artificial intelligence in this limited sense is found in applications as diverse as medical diagnosis, computer search engines, voice or handwriting recognition, and chatbots.',\n",
       "    'Artificial intelligence (AI) is a wide-ranging branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence. While AI is an interdisciplinary science with multiple approaches, advancements in machine learning and deep learning, in particular, are creating a paradigm shift in virtually every sector of the tech industry.',\n",
       "    'Artificial intelligence allows machines to model, or even improve upon, the capabilities of the human mind. And from the development of self-driving cars to the proliferation of generative AI tools like ChatGPT and Google’s Bard, AI is increasingly becoming part of everyday life — and an area companies across every industry are investing in.',\n",
       "    'At its simplest form, artificial intelligence is a field, which combines computer science and robust datasets, to enable problem-solving. It also encompasses sub-fields of machine learning and deep learning, which are frequently mentioned in conjunction with artificial intelligence. These disciplines are comprised of AI algorithms which seek to create expert systems which make predictions or classifications based on input data.',\n",
       "    'Artificial intelligence, or AI, refers to the simulation of human intelligence by software-coded heuristics. Nowadays this code is prevalent in everything from cloud-based, enterprise applications to consumer apps and even embedded firmware.']},\n",
       "  {'tag': 'BI',\n",
       "   'patterns': ['what is bi',\n",
       "    'bi',\n",
       "    'business intelligence',\n",
       "    'what is business intelligence'],\n",
       "   'responses': ['Business intelligence (BI) is software that ingests business data and presents it in user-friendly views such as reports, dashboards, charts and graphs.',\n",
       "    'Business intelligence combines business analytics, data mining, data visualization, data tools and infrastructure, and best practices to help organizations make more data-driven decisions. In practice, you know you’ve got modern business intelligence when you have a comprehensive view of your organization’s data and use that data to drive change, eliminate inefficiencies, and quickly adapt to market or supply changes. Modern BI solutions prioritize flexible self-service analysis, governed data on trusted platforms, empowered business users, and speed to insight.',\n",
       "    'Business intelligencecomprises the strategies and technologies used by enterprises for the data analysis and management of business information.[1] Common functions of business intelligence technologies include reporting, online analytical processing, analytics, dashboard development, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics, and prescriptive analytics.',\n",
       "    'Business intelligence (BI) is a set of strategies and technologies enterprises use to analyze business information and transform it into actionable insights that inform strategic and tactical business decisions. BI tools access and analyze data sets and present analytical findings in reports, summaries, dashboards, graphs, charts, and maps to provide users with detailed intelligence about the state of the business.']},\n",
       "  {'tag': 'personal projects',\n",
       "   'patterns': ['should i do personal projects',\n",
       "    'what is the importance of personal projects',\n",
       "    'how important is it to do a personal project',\n",
       "    'self made projects',\n",
       "    'focusing on personal projects'],\n",
       "   'responses': [\"Personal projects in data science are crucial for skill development, portfolio building, and career advancement. They offer hands-on experience, allowing practitioners to apply and reinforce their skills in real-world scenarios. These projects showcase problem-solving abilities, foster creativity and innovation, and demonstrate domain expertise. By engaging in personal projects, data scientists stay current with industry trends, contribute to open source, and build a network within the community. These initiatives reflect self-initiative, passion, and a willingness to experiment with new approaches, enhancing one's resume and setting them apart in a competitive job market.\",\n",
       "    'Data scientists benefit from personal projects as they provide hands-on experience, showcase skills, and build portfolios. These initiatives foster problem-solving, creativity, and domain expertise, demonstrating self-initiative and a passion for the field. Engagement in personal projects enhances resumes, setting data scientists apart in a competitive job market.',\n",
       "    'It is highly recommended to practice skills to become an efficient professional for this field. After grabbing some very good theoretical knowledge on Data Science, if you are really looking ahead to explore what it seems like to be a professional, then now is the time to do some practical projects.',\n",
       "    \"Personal projects are highly recommended because the knowledge you'll obtain from them will surpass any course\"]},\n",
       "  {'tag': 'people in data science',\n",
       "   'patterns': ['give some data science influencers',\n",
       "    'people in data science',\n",
       "    'people who are big in the data science world',\n",
       "    'people who influenced the data science landscape'],\n",
       "   'responses': [\"There are a lot of great people who have done a lot to data science, some of them are: 'Andrew NG', 'Mike West', 'Ben Rogojan' and 'Luke Barousse'\",\n",
       "    \"There is also: 'Bill Inmon', 'Mike Kimbell', 'Ken Jee', 'Joma Tech' and 'Joe Ries'\"]},\n",
       "  {'tag': 'databases',\n",
       "   'patterns': ['what is a database',\n",
       "    'importance of a database in data science',\n",
       "    'why use a database',\n",
       "    'database'],\n",
       "   'responses': ['A database is a structured collection of data that is organized in a way that allows for efficient storage, retrieval, and management of information. It serves as a centralized repository for storing and managing data, making it easier to organize, update, and access information as needed. Databases are used in various applications and industries, ranging from simple systems like address books to complex systems like enterprise-level data warehouses.',\n",
       "    'A database is an organized collection of data, structured into tables with rows (records) and columns (fields). It facilitates efficient storage, retrieval, and management of information. Key components include tables, rows, columns, and keys. Relational databases use SQL to interact with data, while NoSQL databases offer flexibility for handling unstructured data. Databases are fundamental in software applications, websites, and information systems, providing a centralized and structured approach to data storage and retrieval.',\n",
       "    'Databases are essential for organized data storage, efficient retrieval, and maintaining data integrity. They support scalability, security, and relationships between data sets. Integral to applications, databases enable analysis, backup, and recovery, serving as a foundation for diverse technologies.']},\n",
       "  {'tag': 'data modeling',\n",
       "   'patterns': ['what can you tell me about data modeling',\n",
       "    'what is data modeling',\n",
       "    'data modeling'],\n",
       "   'responses': ['Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures.',\n",
       "    'Data Modeling in software engineering is the process of simplifying the diagram or data model of a software system by applying certain formal techniques. It involves expressing data and information through text and symbols. The data model provides the blueprint for building a new database or reengineering legacy applications.',\n",
       "    'Data Modeling thus helps to increase consistency in naming, rules, semantics, and security. This, in turn, improves data analytics. The emphasis is on the need for availability and organization of data, independent of the manner of its application.',\n",
       "    'Data modeling is a process of creating a conceptual representation of data objects and their relationships to one another. The process of data modeling typically involves several steps, including requirements gathering, conceptual design, logical design, physical design, and implementation. During each step of the process, data modelers work with stakeholders to understand the data requirements, define the entities and attributes, establish the relationships between the data objects, and create a model that accurately represents the data in a way that can be used by application developers, database administrators, and other stakeholders.']},\n",
       "  {'tag': 'getting a degree',\n",
       "   'patterns': ['should i get a degree',\n",
       "    'are degrees a better choice',\n",
       "    'should i pursue a degree or should i self learn',\n",
       "    'self learning vs degrees'],\n",
       "   'responses': ['Getting a degree will get you ahead in your career by a long shot compared to learning by yourself, a degree might help you a lot but will be fruitless if you did not do anything else, you should learn more and keep going forwards even after getting a degree, because the job market in software changes everyday',\n",
       "    'Getting a degree is a great option if you can afford it, but Courses and coding websites like The Odin Project and Leet code will help you a lot, the project-based curriculum will give you the practical skills that you need to get a job.']},\n",
       "  {'tag': 'discord',\n",
       "   'patterns': ['what is discord',\n",
       "    'discord',\n",
       "    'is discord just for gaming',\n",
       "    'will discord help me'],\n",
       "   'responses': ['Discord is a communication platform that combines text, voice, and video chat. Initially developed for gamers, it has evolved into a versatile platform used by various communities, businesses, and educational groups. Users can create or join servers, which are essentially virtual spaces where people can communicate. Within servers, there are channels for different topics, and users can exchange messages, share multimedia, and participate in voice or video calls. Discord has gained popularity for its ease of use, low-latency voice chat, and the ability to create customized bots and integrations. It is widely used for socializing, collaboration, and online communities.',\n",
       "    \"Discord is a platform to commuincate with people using text, voice and audio forms. Discord has a lot of various communities so you'll definitely find something to your liking. Discord is also used by tech people for it's simplicity, and fimilarity, since a lot of IT people already have discord installed for gaming purposes\"]},\n",
       "  {'tag': 'lablabme',\n",
       "   'patterns': ['what is lablab.me',\n",
       "    'what were you made for',\n",
       "    'hackathon',\n",
       "    'chatathon',\n",
       "    'lab lab',\n",
       "    'lablab'],\n",
       "   'responses': ['LabLab is one of the biggest AI commuinties that is specialized in developing AI applications, they host many hackathon-like competetions.',\n",
       "    'LabLab is platform that hosts hackathons for anyone to join, these hackathons brings a lot of talented people to develop a variaty of impressive projects']},\n",
       "  {'tag': 'bye',\n",
       "   'patterns': ['good bye',\n",
       "    'ciao',\n",
       "    'adios',\n",
       "    'sayonara',\n",
       "    'bye bye',\n",
       "    'see you later'],\n",
       "   'responses': ['Please come again!', 'Good bye!']},\n",
       "  {'tag': 'books',\n",
       "   'patterns': ['can you recommend some books',\n",
       "    'i need some books about data science',\n",
       "    'what are some helpful books for data science',\n",
       "    'book recommendations',\n",
       "    'books',\n",
       "    'I need a book',\n",
       "    'reccomend me a book'],\n",
       "   'responses': ['Practical Statistics for Data Scientists',\n",
       "    'Introduction to Probability',\n",
       "    'Introduction to Machine Learning with Python: A Guide for Data Scientists',\n",
       "    'Python for Data Analysis',\n",
       "    'Python Data Science Handbook',\n",
       "    'R for Data Science',\n",
       "    'Understanding Machine Learning: From Theory to Algorithms',\n",
       "    'Deep Learning',\n",
       "    'Mining of Massive Datasets',\n",
       "    'The Elements of Statistical Learning — Data Mining, Inference, and Prediction',\n",
       "    'The Art of Statistics — How to Learn from Data',\n",
       "    'Data Science for Beginners',\n",
       "    'Data Science for Business — What You Need to Know about Data Mining and Data Analytic-Thinking',\n",
       "    'Build a Career in Data Science',\n",
       "    'Clean Code — A Handbook of Agile Software Craftsmanship',\n",
       "    'The Art of Data Science — A Guide for Anyone Who Works With Data',\n",
       "    'A Common-Sense Guide to Data Structures and Algorithms: Level Up Your Core Programming Skills (2nd Edition)',\n",
       "    'Deep Learning with Python',\n",
       "    'Foundations of Deep Reinforcement learning python',\n",
       "    'Big Data',\n",
       "    'Fundamentals of Data Visualization ',\n",
       "    'Storytelling with Data ',\n",
       "    'Hands-On Machine Learning with Scikit-Learn and TensorFlow',\n",
       "    'The Data Science Handbook',\n",
       "    'Thinking with Data']},\n",
       "  {'tag': 'nosql',\n",
       "   'patterns': ['what is nosql', 'nosql database', 'nosql vs sql', 'nosql'],\n",
       "   'responses': ['NoSQL is an approach to database management that can accommodate a wide variety of data models, including key-value, document, columnar and graph formats. A NoSQL database generally means that it is non-relational, distributed, flexible and scalable.',\n",
       "    'NoSQL databases (aka not only SQL) are non-tabular databases and store data differently than relational tables. NoSQL databases come in a variety of types based on their data model. The main types are document, key-value, wide-column, and graph. They provide flexible schemas and scale easily with large amounts of data and high user loads.',\n",
       "    'NoSQL, also referred to as “not only SQL”, “non-SQL”, is an approach to database design that enables the storage and querying of data outside the traditional structures found in\\u202frelational databases. While it can still store data found within relational database management systems (RDBMS), it just stores it differently compared to an RDBMS. The decision to use a relational database versus a non-relational database is largely contextual, and it varies depending on the use case.']},\n",
       "  {'tag': 'sql vs excel',\n",
       "   'patterns': [\"why can't I just use an excel spreadsheet\",\n",
       "    'spreadsheets vs databases',\n",
       "    'sql vs excel',\n",
       "    'is excel a better choice than sql',\n",
       "    'why use sql',\n",
       "    'using a spreadsheet to store data'],\n",
       "   'responses': [\"Excel is a spreadsheet, while SQL data storage is a relational database management system (RDBMS). SQL uses multiple related tables that give it a multi dimensional feel. Excel can link multiple worksheets, but that's not its strength. Excel is a great program for simplicity and flexibility.\",\n",
       "    \"The main differences between Excel and SQL revolve around accessibility and power: Excel is known for its ease-of-use. The application presents a visual notebook that makes it easy to format and visualize data. SQL is known for speed and volume. It lets users work with enormous amounts of raw data without sacrificing speed—Excel starts to lag with larger volumes of data. Both Excel and SQL let data analysts work with data, but the two tools have different use-cases depending on a business's data needs and expectations. Understanding the differences in terms of advantages and disadvantages with Excel and SQL will help you make the most of your data.\",\n",
       "    'SQL is much faster than Excel. It can take minutes in SQL to do what it takes nearly an hour to do in Excel. Excel can technically handle one million rows, but that’s before the pivot tables, multiple tabs, and functions you’re probably using. SQL also separates analysis from data. When using SQL, your data is stored separately from your analysis. Instead of emailing a massive Excel file, you can send tiny plain text files containing the instructions for your analysis. Teammates each have access to the same data, so they can run your analysis on their own. They don’t have to manage file versions or risk corrupting the data, and they can re-run it on any other data.']},\n",
       "  {'tag': 'DS vs SE',\n",
       "   'patterns': ['data science vs software engineering',\n",
       "    'software engineering as a career choice',\n",
       "    'using software engineering in data science',\n",
       "    'DS vs SE',\n",
       "    'is software engineering better than data science'],\n",
       "   'responses': ['A data scientist is a data-centered position that uses data to create an impact. This position works with data to generate valuable business insights and solve real-world problems. A software engineer, on the other hand, works closely developing systems and software for businesses and organizations and applies engineering concepts to software development.',\n",
       "    'The skills required for both positions do overlap, specifically when it comes to knowledge of mathematics and statistics, programming and soft skills such as good communication and the ability to effectively problem solve. Other skills commonly deployed by data scientists include machine learning, predictive modeling, data visualization, text mining, programming (including Python, R, SQL, Spark, Hadoop, Julia), and many more. Data scientists also need soft skills, especially oral and written communication, to present often complex concepts to stakeholders.',\n",
       "    'Data science is related to gathering and processing data, whereas software engineering focuses on the development of applications and features for users. A career in either data science or software engineering requires you to have programming skills. While data science includes statistics and machine learning, software engineering focuses more on coding languages.',\n",
       "    'Both career choices are in demand and highly rewarding. Ultimately, it depends on your area of interest. It’s a big decision, so make sure you’re informed: read up about both software engineering and data science, as well as a range of other tech careers.']},\n",
       "  {'tag': 'laptop',\n",
       "   'patterns': ['laptop to study data science',\n",
       "    'What are the specifications of a good laptop?',\n",
       "    'laptop specs to study',\n",
       "    'specifications for a good laptop to study and work in data science',\n",
       "    'laptop',\n",
       "    'personal computer',\n",
       "    'pc',\n",
       "    'how strong should my machine be to be a data scientist'],\n",
       "   'responses': ['For individuals working in data science, a robust CPU is indispensable for enhancing the efficiency and effectiveness of various computational tasks. The processing speed of a CPU is a critical factor, especially when dealing with large datasets and performing complex calculations inherent to data science.',\n",
       "    'Machine learning model training, a core aspect of data science, benefits significantly from a powerful CPU, as it accelerates the iterative process of experimenting with different model architectures and parameters.',\n",
       "    'Data manipulation tasks, such as preprocessing and cleaning, are foundational to the data science workflow and often demand substantial computational resources. A strong CPU facilitates the quick and efficient handling of these tasks, expediting the data preparation phase. Parallel processing capabilities are essential for tasks like parallel computing in machine learning algorithms, and modern CPUs with multiple cores excel in these scenarios, contributing to improved efficiency.',\n",
       "    'Data scientists frequently engage in simulation and optimization activities, requiring substantial computational power. A robust CPU ensures the swift execution of these computationally demanding tasks. In the realm of big data analytics, a powerful CPU becomes a linchpin for handling vast datasets effectively. Real-time analysis, crucial in certain data science applications, benefits from a strong CPU that enables quick computations for timely insights and decision-making.',\n",
       "    'As a data science student, selecting the right laptop is crucial for handling the computational demands of your coursework. Prioritize performance with a multi-core processor like Intel Core i7 or AMD Ryzen 7, ensuring efficient execution of complex computations. Aim for a minimum of 16GB RAM to handle large datasets and opt for a fast and spacious SSD for quick data access. If your work involves machine learning, consider a laptop with a dedicated GPU, such as NVIDIA or AMD, to accelerate model training.',\n",
       "    'Portability is important for students, so balance performance with a reasonably lightweight design and good battery life. Choose a laptop with a high-resolution display and consider IPS panels for better data visualization. Ensure the laptop has the necessary ports like USB Type-C, HDMI, and an SD card slot for connectivity with external devices.',\n",
       "    'Look for a durable build with a comfortable keyboard and trackpad, as you may spend extended periods working. Choose an operating system based on your preferences and the tools you plan to use—both Windows and macOS are popular in the data science community. Consider laptops that allow for RAM and storage upgrades to adapt to future requirements.',\n",
       "    'Verify compatibility with the data science tools and software you plan to use, ensuring smooth operation. Set a budget that balances your requirements, and consider networking capabilities for efficient data transfer. Additional security features like fingerprint sensors or facial recognition can enhance data protection. By considering these factors, you can make an informed decision and choose a laptop that supports your data science studies effectively.']},\n",
       "  {'tag': 'ETL',\n",
       "   'patterns': ['what is ETL',\n",
       "    'extract transform load',\n",
       "    'ETL',\n",
       "    'building etl'],\n",
       "   'responses': ['ETL, or Extract, Transform, Load, is a crucial process in data management and analytics. It involves three main stages. First, in the Extract phase, data is gathered from various sources such as databases, files, or APIs to ensure the collection of relevant information for analysis. Following that, in the Transform phase, the extracted data undergoes a series of operations to ensure quality, consistency, and compatibility with the target system. These operations include cleaning, aggregating, restructuring, and handling errors and missing values. Finally, in the Load phase, the transformed data is loaded into a target database or data warehouse. During this stage, the data is organized in a way that facilitates efficient querying and reporting. Loading can occur either in batches or in real-time, depending on specific requirements.',\n",
       "    'The ETL process plays a pivotal role in various domains, including business intelligence and decision-making, as it consolidates data from multiple sources, providing a unified view for analysis. Key considerations in ETL include maintaining data integrity, scalability to handle growing data volumes, efficient performance for timely data delivery, and the automation of processes for scheduled runs to ensure data freshness while reducing manual intervention.',\n",
       "    'ETL tools, such as Apache NiFi, Talend, Informatica, and Microsoft SSIS, are widely used to streamline and automate the ETL process. These tools offer graphical interfaces, simplifying the design, management, and monitoring of ETL workflows. In summary, ETL is fundamental for maintaining data quality, supporting analysis, and facilitating informed decision-making in data-driven environments.',\n",
       "    'The ETL approach uses a set of business rules to process data from several sources before centralized integration. The ELT approach loads data as it is and transforms it at a later stage, depending on the use case and analytics requirements.',\n",
       "    'Extract, transform, and load (ETL) is the process of combining data from multiple sources into a large, central repository called a data warehouse. ETL uses a set of business rules to clean and organize raw data and prepare it for storage, data analytics, and machine learning (ML).']},\n",
       "  {'tag': 'data pipelines',\n",
       "   'patterns': ['what is data pipelines',\n",
       "    'what is an ETL pipeline',\n",
       "    'designing an ETL pipeline',\n",
       "    'building a pipeline',\n",
       "    'data pipelines'],\n",
       "   'responses': ['A data pipeline is a series of data processing steps. If the data is not currently loaded into the data platform, then it is ingested at the beginning of the pipeline. Then there are a series of steps in which each step delivers an output that is the input to the next step.',\n",
       "    'A data pipeline is a method in which raw data is ingested from various data sources and then ported to data store, like a data lake or data warehouse, for analysis. Before data flows into a data repository, it usually undergoes some data processing. This is inclusive of data transformations, such as filtering, masking, and aggregations, which ensure appropriate data integration and standardization. This is particularly important when the destination for the dataset is a relational database. This type of data repository has a defined schema which requires alignment—i.e. matching data columns and types—to update existing data with new data.',\n",
       "    'A data pipeline is a series of processing steps to prepare enterprise data for analysis. Organizations have a large volume of data from various sources like applications, Internet of Things (IoT) devices, and other digital channels. However, raw data is useless; it must be moved, sorted, filtered, reformatted, and analyzed for business intelligence. A data pipeline includes various technologies to verify, summarize, and find patterns in data to inform business decisions. Well-organized data pipelines support various big data projects, such as data visualizations, exploratory data analyses, and machine learning tasks.',\n",
       "    'A data pipeline is a means of moving data from one place to a destination (such as a data warehouse) while simultaneously optimizing and transforming the data. As a result, the data arrives in a state that can be analyzed and used to develop business insights. A data pipeline essentially is the steps involved in aggregating, organizing, and moving data. Modern data pipelines automate many of the manual steps involved in transforming and optimizing continuous data loads. Typically, this includes loading raw data into a staging table for interim storage and then changing it before ultimately inserting it into the destination reporting tables.']},\n",
       "  {'tag': 'trends',\n",
       "   'patterns': ['tell me some of the data science trends',\n",
       "    \"what's going on in the data science world\",\n",
       "    'how is the data world',\n",
       "    'any news',\n",
       "    'any new trends'],\n",
       "   'responses': ['The trend of outrageously large models is nowhere near an end. One year ago the release of OpenAI’s GPT-3 got the AI community flabbergasted with 175 Billion parameters. This month was the turn of Wu Dao 2.0 to break the record, showing how China’s not dragging behind at all when it comes to pouring resources in AI research. Wu Dao is a multimodal (text and images) massive model with 1.75 Trillion parameters, based on a Mixture of Experts architecture (more on that later!). While the official press release only touched the surface of the model and not much is public about it, the paper outlining the system for training the model: FastMoE: A Fast Mixture-of-Expert Training System is on arXiv and the code open sourced on GitHub. Wish OpenAI would do more of that.',\n",
       "    'Github Copilot just released a few days ago: a plugin that brings next generation code synthesis, based on Codex, a GPT-like model from OpenAI trained on a massive dataset of public Github code. But the announcement leads to a dashing landing page with cherry picked examples, and a public demo is still not available. Many questions are still in the air: how big and how fast can this model do inference? What are the details of the training dataset used? Should we be concerned about copyright protected data being accidentally surfaced by the model as it has been shown previously⁵? This twitter thread sheds some light on the topic, and we’re impatient to try it ourselves… It has the potential to make programming 10x more productive, and to democratize writing code, but then it has to work really, really well. And we know that bugfree code does not exist. Would it be easier than bringing self-driving cars on the road?',\n",
       "    \"Python pandas creator Wes McKinney has joined Posit as a principal architect, a move that signals the company's increasing seriousness about being a key player in the Python universe as well as the R ecosystem. 'I will advocate for the needs of the PyData ecosystem in Posit’s work as well as continue advancing critical open-source initiatives,' McKinney wrote in a blog post Monday. Along with being known for the pandas data analysis library, McKinney has worked on other open-source projects including Apache Arrow, Apache Parquet, and Ibis. He is also a co-founder of Voltron Data, which focuses on composable enterprise data systems. McKinney's move highlights a continued expansion of mission for Posit, a company once known as RStudio with a primary focus on the R programming language and its popular RStudio IDE. The company changed its name last year, pledging not to forsake R but to help data science practitioners and teams who use both R and Python.\",\n",
       "    'check out more articles at: https://www.datasciencecentral.com/',\n",
       "    'check out more articles at: https://towardsdatascience.com/']},\n",
       "  {'tag': 'data sources',\n",
       "   'patterns': ['where can i get data',\n",
       "    'data sources',\n",
       "    'where to find data',\n",
       "    'how can i start with data',\n",
       "    'where is the data'],\n",
       "   'responses': ['Data could be located in many places on the web, one of the best data sources out there is https://kaggle.com this website is used by a lot of people in their personal projects',\n",
       "    'You can find data anywhere, here is some of the biggest data sources online: https://kaggle.com, https://data.world, https://data.gov, https://datasetsearch.research.google.com/']},\n",
       "  {'tag': 'where to stuyd data science',\n",
       "   'patterns': ['where can I start with data science',\n",
       "    'platforms to start with data science',\n",
       "    'how can I study data science',\n",
       "    'data science learning resources',\n",
       "    'learning data science'],\n",
       "   'responses': ['A good place to start learning data science is Kaggles free courses: https://kaggle.com',\n",
       "    'A good resource to learn programming and related fields is FreeCodeCamp you can find them here: https://www.freecodecamp.org or on thier youtube channel https://www.youtube.com/@freecodecamp']},\n",
       "  {'tag': 'youtube channels',\n",
       "   'patterns': [],\n",
       "   'responses': ['Here are some youtube channels that will teach you data science: https://www.youtube.com/@KeithGalli, https://www.youtube.com/@freecodecamp, https://www.youtube.com/@365DataScience, https://www.youtube.com/@edurekaIN, https://www.youtube.com/@SimplilearnOfficial']},\n",
       "  {'tag': 'sharing',\n",
       "   'patterns': ['how can i share my progress',\n",
       "    'where can i share my projects',\n",
       "    'places where I can make people see my projects and progress',\n",
       "    'i want people to see my code',\n",
       "    'where to share my code',\n",
       "    'how can i make a network of work connections'],\n",
       "   'responses': ['You can share your progress and make tons of connections using LinkeIn: https://linkedin.com, you can also share your code using GitHub: https://github.com']},\n",
       "  {'tag': 'certifications',\n",
       "   'patterns': ['where can I find certifications',\n",
       "    'are certificates good',\n",
       "    'certifications in data scinece',\n",
       "    'getting a certificate',\n",
       "    'getting a certification',\n",
       "    'best certifications and certificates available'],\n",
       "   'responses': ['Data science certifications give you an opportunity not only to develop skills that are hard to find in your desired industry but also to validate your data science know-how so that recruiters and hiring managers know what they’re getting if they hire you. Here are some of the top certificates available: Certified Analytics Professional (CAP) \\n Cloudera Data Platform Generalist Certification \\n Data Science Council of America (DASCA) Senior Data Scientist (SDS) \\n Data Science Council of America (DASCA) Principal Data Scientist (PDS) \\n IBM Data Science Professional Certificate \\n Microsoft Certified: Azure AI Fundamentals \\n Microsoft Certified: Azure Data Scientist Associate \\n Open Certified Data Scientist (Open CDS) \\n SAS Certified AI and Machine Learning Professional \\n SAS Certified Advanced Analytics Professional using SAS 9 \\n SAS Certified Data Scientist \\n Tensorflow Developer Certificate']},\n",
       "  {'tag': 'python vs r',\n",
       "   'patterns': ['python vs r', 'r', 'the best data science language'],\n",
       "   'responses': ['Python and R are the preferred languages in Data Science, Data Analysis, Machine Learning, etc. Although they are used for similar purposes they differ from each other. R mainly focuses on the statistical part of a project while Python is flexible in its usage and data analysis tasks.']},\n",
       "  {'tag': 'overfitting vs underfitting',\n",
       "   'patterns': ['overfitting',\n",
       "    'overfitting vs underfitting',\n",
       "    'difference between overfitting and underfitting',\n",
       "    'underfitting'],\n",
       "   'responses': ['Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize. \\n Underfitting refers to a model that can neither model the training data nor generalize to new data. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.']},\n",
       "  {'tag': 'why data scinece',\n",
       "   'patterns': ['why should i learn data science',\n",
       "    'data science importance',\n",
       "    'should i pursue a career in data science',\n",
       "    'why data scinece',\n",
       "    'reasons to study data science'],\n",
       "   'responses': [\"Data science has the potential to improve the way we live and work, and it can empower others to make better decisions, solve problems, discover new advancements, and address some of the world's most pressing issues. With a data science career, you can be a part of this transformation.\",\n",
       "    'Without data science, companies of all sizes, especially large organizations, would have difficulty making informed decisions. Data scientists extrapolate the data they collect to uncover trends in every area of the business. They help leaders and C-suite executives make decisions backed by data to continue growing their company and make the best decisions for their consumers. Businesses are all about analytics and metrics. Without those, you don’t have a clear picture of the health of your business. Data science is essential for every business because of the value and insight it provides.',\n",
       "    \"Whether you're an experienced information technology worker or completely new to the field, studying data science is an opportunity for you to become a lifelong learner. Data science is a developing industry that follows the trends of technological advancements. Therefore, if you pursue a career in data science, you can expect to continue learning about the latest developments in the field. Your position, its responsibilities, certifications and any learning you do to advance your career can keep you challenged with its technical and specialized information.\"]},\n",
       "  {'tag': 'common ML algorithms',\n",
       "   'patterns': ['what are some common machine learning algorithms',\n",
       "    'machine learning algorithms',\n",
       "    'most used machine learning algorithms',\n",
       "    'common and frequently used machine learning algorithms'],\n",
       "   'responses': ['Machine learning algorithms can be broadly categorized into: **Supervised Learning:** \\n - Linear Regression \\n - Logistic Regression \\n - Decision Trees \\n - Support Vector Machines (SVM) \\n **Unsupervised Learning:** \\n - K-Means Clustering \\n - Hierarchical Clustering \\n - Principal Component Analysis (PCA) \\n - Association Rule Learning \\n **Reinforcement Learning:** \\n - Q-Learning \\n - Deep Q Network (DQN) \\n - Policy Gradient Methods \\n **Ensemble Learning:** \\n - Random Forest \\n - Gradient Boosting (XGBoost, LightGBM) \\n **Neural Networks:** \\n - Multilayer Perceptron (MLP) \\n - Convolutional Neural Network (CNN) \\n - Recurrent Neural Network (RNN) \\n - Transformer \\n **Instance-Based Learning:** \\n - k-Nearest Neighbors (k-NN) \\n These algorithms serve various purposes, from predicting outcomes and grouping data to learning optimal strategies and modeling complex patterns.']},\n",
       "  {'tag': 'visualization libraries',\n",
       "   'patterns': ['python visualization libraries',\n",
       "    'give me some visualization tools in python',\n",
       "    'how can i visualize my data',\n",
       "    'visualizing my data in python',\n",
       "    'library to visualize my data',\n",
       "    'showcase my data in python',\n",
       "    'draw the data in python'],\n",
       "   'responses': ['Here are some of the most common vizualiztion libraries in python: \\n matplotlib \\n seaborn \\n plotly \\n bokeh']},\n",
       "  {'tag': 'power bi vs tableau',\n",
       "   'patterns': ['compare power bi to tableau',\n",
       "    'power bi vs tableau',\n",
       "    'which one is better power bi or tableau'],\n",
       "   'responses': [\"Power BI and Tableau are prominent players in the field of business intelligence and data visualization, each with its strengths and considerations. \\n Power BI: \\n Developed by Microsoft, Power BI offers seamless integration with the broader Microsoft ecosystem, making it an attractive choice for organizations already using products like Excel, Azure, and SQL Server. It is known for its cost-effectiveness, especially for businesses aligned with Microsoft technologies. Power BI has an intuitive interface, particularly beneficial for users familiar with Microsoft products, and features robust Extract, Transform, Load (ETL) capabilities for data preparation. Additionally, it supports natural language querying, enhancing the user experience. Power BI's integration with Microsoft Teams facilitates collaborative work within the organization. \\n Tableau: \\n Acquired by Salesforce, Tableau is renowned for its user-friendly drag-and-drop interface, making it accessible for users without extensive technical skills. It boasts broad integration capabilities with various data sources, providing flexibility in data connectivity. Tableau places a strong emphasis on data storytelling, enabling users to create compelling narratives through visualizations. It has a large and active user community, offering ample resources and support. While Tableau's pricing may be considered higher than Power BI, it caters to organizations seeking advanced visualization capabilities and a focus on storytelling in their analytics. \\n Considerations: \\n When deciding between Power BI and Tableau, organizations should weigh factors such as cost, integration requirements, user familiarity, and scalability. Power BI is often a more cost-effective choice for Microsoft-centric environments, whereas Tableau's strengths lie in its user-friendly interface and emphasis on storytelling, making it suitable for organizations valuing advanced visualization capabilities and a strong user community. Ultimately, the choice depends on the specific needs, budget, and existing technology landscape of the organization.\"]},\n",
       "  {'tag': 'salaries',\n",
       "   'patterns': ['data science salary',\n",
       "    'salaries',\n",
       "    'how much does a data scientist earn',\n",
       "    'how much does a data scientist get paid'],\n",
       "   'responses': [\"Data scientist earn a higher-than-average salary. According to the U.S. Bureau of Labor Statistics (BLS), the median salary for data scientists was $103,500 per year as of 2022. That's more than double the median annual wage for all workers, which the BLS pinned at $46,310 during the same period.\",\n",
       "    'The average data science job salary is around $156,395 per year in the United States.']},\n",
       "  {'tag': 'job titles',\n",
       "   'patterns': ['what are some of the jobs related to data science',\n",
       "    'data related fields and jobs',\n",
       "    'jobs in data',\n",
       "    'data science jobs',\n",
       "    'job titles'],\n",
       "   'responses': ['Data science is a very diverse field, some of the job titles that are related to data scinece are: Data Engineer, Machnie Learning Engineer, Data Analyst, Business Intelligence Aanlyst and Data Architecht']},\n",
       "  {'tag': 'data analysis',\n",
       "   'patterns': ['what is data analysis',\n",
       "    'data analysis',\n",
       "    'data analyst',\n",
       "    'analyzing data',\n",
       "    'data analytics'],\n",
       "   'responses': ['Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data.',\n",
       "    'Data analytics converts raw data into actionable insights. It includes a range of tools, technologies, and processes used to find trends and solve problems by using data. Data analytics can shape business processes, improve decision-making, and foster business growth.',\n",
       "    'Data analytics is the science of analyzing raw data to make conclusions about information. Many of the techniques and processes of data analytics have been automated into mechanical processes and algorithms that work over raw data for human consumption.']},\n",
       "  {'tag': 'outliers',\n",
       "   'patterns': ['tell me about outliers',\n",
       "    'explain outliers',\n",
       "    'what is outliers',\n",
       "    'defention of outliers'],\n",
       "   'responses': [' In data analytics, outliers are values within a dataset that vary greatly from the others']},\n",
       "  {'tag': 'Heatmao',\n",
       "   'patterns': ['What is Heat map'],\n",
       "   'responses': ['Heat map is a graphical representation of data where values are depicted by color']},\n",
       "  {'tag': 'Noisy data',\n",
       "   'patterns': ['what is Noisy data'],\n",
       "   'responses': ['Noisy data is a meaningless data that cannot be interpreted by machines']},\n",
       "  {'tag': 'variance',\n",
       "   'patterns': ['What a variance means',\n",
       "    'Define variance',\n",
       "    'Defintion of variance'],\n",
       "   'responses': ['The term variance refers to a statistical measurement of the spread between numbers in a data set']},\n",
       "  {'tag': 'standard deviation',\n",
       "   'patterns': ['what is the standard deviation',\n",
       "    'Define standard deviation',\n",
       "    'Defintion of standard deviation'],\n",
       "   'responses': ['In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values']},\n",
       "  {'tag': 'Boosting',\n",
       "   'patterns': ['Why is Boosting so effective'],\n",
       "   'response': ['Ensemble methods reduce the bias and variance of our Machine Learning models.Ensemble methods help increase the stability and performance of machine learning models by eliminating the dependency of a single estimator.']}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944099c7",
   "metadata": {},
   "source": [
    "# First made an empty list \n",
    "## doing for loops for intents in json file then insted loops for patterns in json file\n",
    "## Tokenize the pattern\n",
    "## extend the pattern to put them in the empty list \n",
    "## apped the patterns in data_x to use in the training data and append tag in data_y to use it in the training\n",
    "## using Lemmatizer to return the word to the root \n",
    "## then applies the lemmatizer to the word and convert it to lower and remove punctuatuion\n",
    "## then put them in the set to avoid any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95d53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "data_x = [] \n",
    "data_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        data_x.append(pattern)\n",
    "        data_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001cca4",
   "metadata": {},
   "source": [
    "# Initializing empty list the out_empty create a list with all zero the zeros of this list well equal the to the number of classes\n",
    "## make for loops to itearte the data_x \n",
    "## if the words it's in text and it's lower append zero otherwise append zero\n",
    "## then shuffling the data and put it in array beacude tensrflow just expect an array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cad813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Your previous code\n",
    "out_empty = [0] * len(classes)\n",
    "training = []\n",
    "\n",
    "for idx, doc in enumerate(data_x):\n",
    "    bow = [] \n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            bow.append(1)\n",
    "        else:\n",
    "            bow.append(0)\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(data_y[idx])] = 1\n",
    "    \n",
    "    training.append((bow, output_row))\n",
    "\n",
    "random.shuffle(training)\n",
    "\n",
    "train_x = [item[0] for item in training]\n",
    "train_y = [item[1] for item in training]\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58c26f",
   "metadata": {},
   "source": [
    "# the input_shape define the feature in the data set \n",
    "## dropout helps to avoid overfitting\n",
    "## avtivation make the nuearal network learn the cmoplex patterns and realtionship without the activation function is gonna be like linear model\n",
    "## the optimizer adam don't need to put the wieght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb52a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 128)               43136     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 102)               6630      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58022 (226.65 KB)\n",
      "Trainable params: 58022 (226.65 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "           #layer 1\n",
    "          (Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\")),\n",
    "          (Dropout(0.5)),\n",
    "    #layer 2\n",
    "          (Dense(64, activation=\"relu\")),\n",
    "          (Dropout(0.5)),\n",
    "    #layer 3\n",
    "          (Dense(len(train_y[0]), activation=\"softmax\"))\n",
    "                 ])  \n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb589d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.dense.Dense at 0x22d9a878a90>,\n",
       " <keras.src.layers.regularization.dropout.Dropout at 0x22d95afd410>,\n",
       " <keras.src.layers.core.dense.Dense at 0x22d9d3579d0>,\n",
       " <keras.src.layers.regularization.dropout.Dropout at 0x22d9d349990>,\n",
       " <keras.src.layers.core.dense.Dense at 0x22d9d33a1d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "954fc9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9419\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 0.9293\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 0.9495\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0061 - accuracy: 0.9419\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9419\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9470\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0067 - accuracy: 0.9394\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9419\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9192\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 0.9268\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 0.9419\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9444\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9444\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9470\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9394\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 0.9192\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 0.9217\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9495\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9419\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 0.9369\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0067 - accuracy: 0.9192\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9293\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9293\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0061 - accuracy: 0.9444\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9571\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9520\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 0.9242\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9470\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 0.9217\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 0.9268\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9545\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9343\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 0.9192\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0060 - accuracy: 0.9343\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9773\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 0.9268\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0059 - accuracy: 0.9470\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 0.9369\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9545\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9318\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9520\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0066 - accuracy: 0.9470\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9318\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9369\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0066 - accuracy: 0.9217\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9495\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9470\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 0.9293\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9495\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9470\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9444\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0067 - accuracy: 0.9293\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 0.9217\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9419\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 0.9242\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9697\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9495\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 0.9242\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9369\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9369\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 0.9293\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0049 - accuracy: 0.9520\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.9444\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9470\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9520\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9495\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 0.9217\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0051 - accuracy: 0.9571\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9369\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9444\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 0.9394\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0060 - accuracy: 0.9444\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9470\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9444\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9217\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9419\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9343\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9293\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9192\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9495\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9444\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9495\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9394\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9394\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0059 - accuracy: 0.9419\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9318\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9520\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.9394\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9596\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 0.9293\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9444\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9293\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9268\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9268\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9394\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 0.9470\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9520\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9444\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9571\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9419\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9470\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.9495\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9470\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9318\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9318\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0050 - accuracy: 0.9596\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9470\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9419\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9444\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 0.9444\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9495\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9520\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9394\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9520\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9419\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9470\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9369\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 0.9470\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9520\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9520\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9419\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9343\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9444\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 0.9520\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9419\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 0.9419\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9419\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9343\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9343\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0048 - accuracy: 0.9495\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9419\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9369\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9495\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 0.9470\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9394\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9495\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 0.9394\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9470\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 0.9343\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 0.9268\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9444\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0063 - accuracy: 0.9318\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9343\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9268\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9369\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9394\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9495\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9470\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0045 - accuracy: 0.9646\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0067 - accuracy: 0.9293\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9394\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9495\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9495\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9470\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9520\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9470\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 0.9293\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 0.9369\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 0.9217\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9394\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9444\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 0.9621\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0061 - accuracy: 0.9369\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9470\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0048 - accuracy: 0.9444\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 0.9343\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9470\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9268\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 0.9318\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0053 - accuracy: 0.9520\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 0.9268\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0052 - accuracy: 0.9520\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 0.9369\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0052 - accuracy: 0.9394\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9495\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9520\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0053 - accuracy: 0.9444\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9495\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9520\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0053 - accuracy: 0.9520\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 0.9520\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9444\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9495\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0055 - accuracy: 0.9470\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0055 - accuracy: 0.9419\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9444\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9722\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9495\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 0.9596\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 0.9343\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0049 - accuracy: 0.9596\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0057 - accuracy: 0.9444\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0059 - accuracy: 0.9394\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0050 - accuracy: 0.9747\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0053 - accuracy: 0.9369\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0052 - accuracy: 0.9419\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0051 - accuracy: 0.9520\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0061 - accuracy: 0.9268\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9343\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e6d5d80590>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_x, y=train_y,epochs=200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f41e8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the Input\n",
    "def clean_text(text):\n",
    "    tokens =nltk.word_tokenize(text)\n",
    "    tokens=[lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
    "    return tokens\n",
    "def bag_of_words(text, vocab):\n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens:\n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word ==w:\n",
    "                bow[idx] =1\n",
    "    return np.array(bow)\n",
    "def pred_class(text, vocab, labels):\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.5\n",
    "    y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    if len(intents_list)==0:\n",
    "        result=\"Soory! I don't unerstand\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"]==tag:\n",
    "                result=random.choice(i[\"responses\"])\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "decc7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 if you don't want to chat with chatbot\n",
      "hi\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "hello!\n",
      "DaTa ScienCe\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "data science is a multidisciplinary field that involves the use of scientific methods, processes, algorithms, and systems to extract meaningful insights and knowledge from structured and unstructured data.\n",
      "DATA SCIENCE\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "data science is a multidisciplinary field that involves the use of scientific methods, processes, algorithms, and systems to extract meaningful insights and knowledge from structured and unstructured data.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Press 0 if you don't want to chat with chatbot\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == \"0\":\n",
    "        break\n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, data)\n",
    "    print(result.lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59049a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
