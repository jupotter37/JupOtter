{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOVuY8vET8o5"
   },
   "source": [
    "# CMPS 4010\n",
    "## Milestone 5: Unrealized Volatility Prediction\n",
    "### By Alex Olteanu and Shira Rozenthal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCqMjbuQ1yhB"
   },
   "source": [
    "### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5oTL5_y11GZ"
   },
   "source": [
    "**IN PROGRESS**\n",
    "- Polish code, include more descriptive markdown cells, emphasize readability \n",
    "- Implement optuna parameter optimizer for each model\n",
    "- Refine LSTM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (7.16.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (3.1.3)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (5.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (23.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (2.17.2)\n",
      "Requirement already satisfied: tinycss2 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbconvert) (5.14.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert) (4.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert) (4.21.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from beautifulsoup4->nbconvert) (2.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nbconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6gZlJILpStt"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages/IPython/core/magics/osm.py:428: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDK1YOIGnWDN",
    "outputId": "a3410984-14ff-4d15-96b5-f6d2d51047a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Tulane/S24/CSC/optiver-realized-volatility-prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/alex/Desktop/Tulane/S24/CSC/optiver-realized-volatility-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /Users/shirarozenthal/Documents/CS/Git/optiver-realized-volatility-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alex/Desktop/Tulane/S24/CSC/optiver-realized-volatility-prediction'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gLeqVlfgpK1C"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oBkUzxmtzAl"
   },
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kU_S9kcjGc-H"
   },
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Initialize stock_id\n",
    "stock_id = 0\n",
    "\n",
    "# Load book data\n",
    "book_example = pd.read_parquet('book_train.parquet/stock_id=0')\n",
    "\n",
    "# Load trade data\n",
    "trade_example = pd.read_parquet('trade_train.parquet/stock_id=0')\n",
    "\n",
    "# Reassign book_example to only include stock_id 0\n",
    "book_example = book_example[book_example['time_id']==5]\n",
    "\n",
    "# Create stock_id column in book_example\n",
    "book_example.loc[:,'stock_id'] = stock_id\n",
    "\n",
    "# Reassign trade_example to only include stock_id 0\n",
    "trade_example = trade_example[trade_example['time_id']==5]\n",
    "\n",
    "# Create stock_id column in trade_example\n",
    "trade_example.loc[:,'stock_id'] = stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtQ6mroFuHKJ",
    "outputId": "47187cbc-912c-4a4f-e20e-1684ec0bf052"
   },
   "outputs": [],
   "source": [
    "# Generate a WAP feature for example book df\n",
    "# WAP = (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1)\n",
    "book_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n",
    "                                book_example['ask_price1'] * book_example['bid_size1']) / (\n",
    "                                       book_example['bid_size1']+ book_example['ask_size1'])\n",
    "\n",
    "# Define function for computing log returns\n",
    "# Log return = log(current price / previous price)\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "# Generate a log return column for book_example df\n",
    "# Takes the log return of the current row and previous row \n",
    "# Row zero omitted because it cannot be compared to a previous time entry using ~ operator\n",
    "book_example.loc[:,'log_return'] = log_return(book_example['wap'])\n",
    "book_example = book_example[~book_example['log_return'].isnull()]\n",
    "\n",
    "# Define a function to compute realized volatility using the log returns in a time bucket\n",
    "# Computed by taking the square root of the sum of squared log returns\n",
    "# # Realized volatility = sqrt(sum(log returns^2))\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "realized_vol = realized_volatility(book_example['log_return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_DM1-Xuuf-Q",
    "outputId": "454a8390-fdf1-4ae0-a877-8e275dc4b690"
   },
   "outputs": [],
   "source": [
    "# Create a list of all file paths within book training parquet file\n",
    "list_order_book_file_train = glob.glob('book_train.parquet/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0nxgei2ruiey"
   },
   "outputs": [],
   "source": [
    "# Define a function for computing the realitized volatiltiy for each time bucket for a specific stock\n",
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    # Load the parquet file for a specific stock into a DataFrame\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Compute the Weighted Average Price (WAP) using bid and ask prices and sizes\n",
    "    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n",
    "                                      df_book_data['bid_size1']+ df_book_data['ask_size1'])\n",
    "\n",
    "    # Calculate the log returns of WAP for each 'time_id'\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
    "\n",
    "    # Remove rows with NaN values in the 'log_return' column\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n",
    "\n",
    "    # Compute the realized volatility for each 'time_id' based on the log returns\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n",
    "\n",
    "    # Rename the 'log_return' column to the provided prediction_column_name\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "\n",
    "    # Extract the stock_id from the file_path\n",
    "    stock_id = file_path.split('=')[1]\n",
    "\n",
    "    # Create a 'row_id' column combining the stock_id and time_id\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "\n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgKxuJqfungc",
    "outputId": "54ff55fb-bb6a-442d-ff94-fc1d979c13da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/2202599754.py:11: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return)\n"
     ]
    }
   ],
   "source": [
    "def past_realized_volatility_per_stock(list_file,prediction_column_name):\n",
    "    # Initialize an empty DataFrame to store the results for all stocks\n",
    "    df_past_realized = pd.DataFrame()\n",
    "\n",
    "    # Loop through each file in the provided list (each file corresponds to a stock's data)\n",
    "    for file in list_file:\n",
    "        # Compute the realized volatility for the current stock using the function 'realized_volatility_per_time_id'\n",
    "        # This function returns the realized volatility for each 'time_id' of the current stock\n",
    "        df_single_stock_realized_vol = realized_volatility_per_time_id(file, prediction_column_name)\n",
    "\n",
    "        # Concatenate the results for the current stock with the aggregated results\n",
    "        df_past_realized = pd.concat([df_past_realized, df_single_stock_realized_vol])\n",
    "\n",
    "    # Return the aggregated results for all stocks\n",
    "    return df_past_realized\n",
    "\n",
    "# Calculate the realized volatility for all stocks in the training data\n",
    "# The list 'list_order_book_file_train' contains file paths for all stocks in the training set\n",
    "df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train, prediction_column_name='pvol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W-UeQgpaupAW"
   },
   "outputs": [],
   "source": [
    "# Create a new column 'row_id' in the 'train' dataframe. This column is a combination of the 'stock_id' and 'time_id' columns.\n",
    "# The two values are separated by a '-' and both are converted to string type to facilitate concatenation.\n",
    "train_mod = pd.read_csv('train.csv')\n",
    "\n",
    "train_mod['row_id'] = train['stock_id'].astype(str) + '-' + train_mod['time_id'].astype(str)\n",
    "\n",
    "# Update the 'train' dataframe to keep only the 'row_id' and 'target' columns.\n",
    "train_mod = train_mod[['row_id','target']]\n",
    "\n",
    "# Merge the 'train' dataframe with the 'df_past_realized_train' dataframe.\n",
    "# The merging is based on the 'row_id' column, which is common between the two dataframes.\n",
    "# This is a left merge, which means all the rows from the 'train' dataframe will be retained and corresponding\n",
    "# values from 'df_past_realized_train' will be added wherever there's a match based on 'row_id'.\n",
    "# If there's no match for a particular 'row_id' in 'df_past_realized_train', NaN values will be filled for 'pred' column.\n",
    "df_joined = train_mod.merge(df_past_realized_train[['row_id','pvol']], on = ['row_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vEjSs4lDwf9L",
    "outputId": "af34bd80-c77f-4676-981b-e5c47c2d8bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of the naive prediction: R2 score: 0.628, RMSPE: 0.341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define a function to calculate the Root Mean Squared Percentage Error (RMSPE)\n",
    "def rmspe(y_true, y_pred):\n",
    "    # The formula for RMSPE is the square root of the average of squared percentage errors.\n",
    "    # The percentage error is calculated as (actual - predicted) / actual, squared to penalize larger errors.\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "# Calculate the R^2 score using the true target values and the predicted values from the 'df_joined' dataframe.\n",
    "# The R^2 score measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "# It provides a measure of how well the observed outcomes are replicated by the model.\n",
    "R2 = round(r2_score(y_true = df_joined['target'], y_pred = df_joined['pvol']),3)\n",
    "\n",
    "# Calculate the RMSPE using the true target values and the predicted values from the 'df_joined' dataframe.\n",
    "RMSPE = round(rmspe(y_true = df_joined['target'], y_pred = df_joined['pvol']),3)\n",
    "\n",
    "# Print out the calculated R^2 score and RMSPE.\n",
    "print(f'Performance of the naive prediction: R2 score: {R2}, RMSPE: {RMSPE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLeoQqC8mJa1"
   },
   "source": [
    "### Reversing Time-ID order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "399MsEMwnBQa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:33: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:33: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/3112154199.py:33: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob('book_train.parquet/**/*.parquet')}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "\n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiVyA70iob3P",
    "outputId": "3b4dfa09-862c-4e31-f67b-65bbbea491f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load files] 0.004sec\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:1: SyntaxWarning: invalid escape sequence '\\d'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:    1.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done  35 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  37 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done  39 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Tulane/S24/Repos/CMPS-4020-Capstone/Capstone/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done  51 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  54 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done  55 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done  57 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=4)]: Done  59 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=4)]: Done  61 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  63 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done  65 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done  66 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done  67 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done  70 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done  71 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=4)]: Done  73 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=4)]: Done  74 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done  75 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done  78 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done  79 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done  80 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  81 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  82 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  83 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done  85 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  86 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  87 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=4)]: Done  89 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=4)]: Done  91 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=4)]: Done  94 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=4)]: Done  95 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=4)]: Done  96 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done  97 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done  98 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done  99 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=4)]: Done 108 out of 112 | elapsed:    7.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:    7.7s finished\n",
      "[calc prices] 7.738sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dq/mxm2qk4s4_34j_8qtpxrhk_h0000gn/T/ipykernel_37598/3112154199.py:37: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE(400) -> 50] 143.087sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ordered = reconstruct_time_id_order()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvuDDcP307Xo"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighboring Volatility Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all file paths within book training parquet file\n",
    "list_order_book_file_train = glob.glob('book_train.parquet/*')\n",
    "\n",
    "# Create a list of all file paths within trade training parquet file\n",
    "list_order_trade_file_train = glob.glob('trade_train.parquet/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "master = df_joined\n",
    "\n",
    "# \n",
    "master = master.rename(columns={'pvol': 'realized_vol'})\n",
    "\n",
    "# Set the 'order' column of df_ordered to be its index values\n",
    "df_ordered['order'] = df_ordered.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'train' DataFrame with 'df_ordered' based on the 'time_id' column\n",
    "# This is a left join, which means all rows from 'train' will be kept, \n",
    "# along with matched rows from 'df_ordered'\n",
    "merged_df = pd.merge(train, df_ordered, on='time_id', how='left')\n",
    "\n",
    "# Sort the merged DataFrame first by 'stock_id' and then by 'order'\n",
    "merged_df.sort_values(by=['stock_id', 'order'], inplace=True)\n",
    "\n",
    "# Create a new 'row_id' column by concatenating 'stock_id' and 'time_id' as strings\n",
    "merged_df['row_id'] = merged_df['stock_id'].astype(str) + '-' + merged_df['time_id'].astype(str)\n",
    "\n",
    "# Reorder columns so that 'row_id' is the first column, and retain all other columns\n",
    "cols = ['row_id'] + [col for col in merged_df.columns if col != 'row_id']\n",
    "df_merged = merged_df[cols]\n",
    "\n",
    "# Drop 'stock_id' and 'time_id' columns from the DataFrame\n",
    "df_merged = df_merged.drop(['stock_id', 'time_id'], axis=1)\n",
    "\n",
    "# Swap the positions of the second and third columns in the DataFrame\n",
    "cols = df_merged.columns.tolist()\n",
    "cols[1], cols[2] = cols[2], cols[1]\n",
    "df_merged = df_merged[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'master' DataFrame with 'df_merged' on 'row_id', using a right join\n",
    "# This keeps all rows from 'df_merged' and the matched rows from 'master'\n",
    "master = master.merge(df_merged, on='row_id', how='right')\n",
    "\n",
    "# Rename the column 'target_x' to 'target' in the 'master' DataFrame\n",
    "master = master.rename(columns={'target_x': 'target'})\n",
    "\n",
    "# Drop the column 'target_y' from the 'master' DataFrame\n",
    "master = master.drop(['target_y'], axis=1)\n",
    "\n",
    "# Select and reorder specific columns in the 'master' DataFrame\n",
    "master = master[['row_id', 'order', 'target', 'realized_vol']]\n",
    "\n",
    "# Extract the stock identifier from 'row_id' and create a new column 'stock'\n",
    "master['stock'] = master['row_id'].str.split('-').str[0]\n",
    "\n",
    "# Finalize the DataFrame by selecting specific columns\n",
    "master = master[['stock', 'row_id', 'order', 'target', 'realized_vol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted_volatilities(df, n_before, n_after):\n",
    "    # Ensure the DataFrame is sorted by 'stock' and then by 'order'\n",
    "    df = df.sort_values(by=['stock', 'order'])\n",
    "    \n",
    "    # Generate shifted columns for prior volatilities\n",
    "    for i in range(1, n_before + 1):\n",
    "        shifted_col_name = f'previous_vol_{i}'\n",
    "        df[shifted_col_name] = df.groupby('stock')['realized_vol'].shift(i)\n",
    "        # Fill NaN values for the first 'n_before' rows within each stock group with the first available 'realized_vol' in that group\n",
    "        df[shifted_col_name] = df.groupby('stock')[shifted_col_name].transform(lambda x: x.fillna(method='bfill'))\n",
    "    \n",
    "    # Generate shifted columns for following volatilities\n",
    "    for i in range(1, n_after + 1):\n",
    "        shifted_col_name = f'next_vol_{i}'\n",
    "        df[shifted_col_name] = df.groupby('stock')['realized_vol'].shift(-i)\n",
    "        # Fill NaN values for the last 'n_after' rows within each stock group with the last available 'realized_vol' in that group\n",
    "        df[shifted_col_name] = df.groupby('stock')[shifted_col_name].transform(lambda x: x.fillna(method='ffill'))\n",
    "\n",
    "    return df\n",
    "\n",
    "n_before = 3  # Number of previous volatilities to include\n",
    "n_after = 3   # Number of following volatilities to include\n",
    "master = add_shifted_volatilities(master, n_before, n_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated Time Bucket Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "S9Sp2xdtxDu6"
   },
   "outputs": [],
   "source": [
    "def calculate_wap(df):\n",
    "    return ((df['bid_price1'] * df['ask_size1']) + (df['ask_price1'] * df['bid_size1'])) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def calculate_price_spread(df):\n",
    "    return (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "\n",
    "def calculate_bid_ask_spread(df, bid_col, ask_col):\n",
    "    return df[ask_col] - df[bid_col]\n",
    "\n",
    "def calculate_total_volume(df):\n",
    "    return df[['ask_size1', 'ask_size2', 'bid_size1', 'bid_size2']].sum(axis=1)\n",
    "\n",
    "def calculate_volume_imbalance(df):\n",
    "    return abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "\n",
    "def book_aggregate_features_per_time_id(file_path, feature_aggregations):\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    stock_id = int(file_path.split('=')[1])\n",
    "\n",
    "    # Precompute complex features\n",
    "    df_book_data['wap'] = calculate_wap(df_book_data)  \n",
    "    df_book_data['price_spread'] = calculate_price_spread(df_book_data)\n",
    "    df_book_data['bid_spread'] = calculate_bid_ask_spread(df_book_data, 'bid_price1', 'bid_price2')\n",
    "    df_book_data['ask_spread'] = calculate_bid_ask_spread(df_book_data, 'ask_price1', 'ask_price2')\n",
    "    df_book_data['total_volume'] = calculate_total_volume(df_book_data)\n",
    "    df_book_data['volume_imbalance'] = calculate_volume_imbalance(df_book_data)\n",
    "\n",
    "    # Prepare aggregation\n",
    "    aggregation_dict = {}\n",
    "    for new_name, (original, agg_func) in feature_aggregations.items():\n",
    "        aggregation_dict.setdefault(original, []).append((new_name, agg_func))\n",
    "\n",
    "    # Aggregate features\n",
    "    df_aggregated = df_book_data.groupby('time_id').agg(\n",
    "        {key: [func for _, func in value] for key, value in aggregation_dict.items()}\n",
    "    )\n",
    "\n",
    "    # Simplify MultiIndex in columns\n",
    "    df_aggregated.columns = [\n",
    "        f\"{original}_{agg_func.__name__ if callable(agg_func) else agg_func}\"\n",
    "        for original, funcs in aggregation_dict.items() for _, agg_func in funcs\n",
    "    ]\n",
    "\n",
    "    # Efficient row_id creation\n",
    "    df_aggregated['row_id'] = f\"{stock_id}-\" + df_aggregated.index.astype(str)\n",
    "    df_aggregated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = ['row_id'] + [col for col in df_aggregated.columns if col != 'row_id']\n",
    "    df_aggregated = df_aggregated[cols]\n",
    "\n",
    "    return df_aggregated\n",
    "\n",
    "def book_aggregate_features_for_all_stocks(list_file, feature_aggregations):\n",
    "    aggregated_data = [book_aggregate_features_per_time_id(file, feature_aggregations) for file in list_file]\n",
    "    return pd.concat(aggregated_data, ignore_index=True)\n",
    "\n",
    "def append_book_flattened_feature(list_file, flattened_feature, master):\n",
    "    df_aggregated_book = book_aggregate_features_for_all_stocks(list_file, flattened_feature)\n",
    "    master = pd.merge(master, df_aggregated_book, on='row_id', how='left')\n",
    "    return master\n",
    "\n",
    "def max_div_avg(series):\n",
    "    \"\"\"Returns the ratio of the maximum to the average of a series.\"\"\"\n",
    "    if series.mean() != 0:\n",
    "        return series.max() / series.mean()\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def abs_price_change_first_last(series):\n",
    "    \"\"\"Returns the absolute difference between the first and last values of a series.\"\"\"\n",
    "    return abs(series.iloc[-1] - series.iloc[0])\n",
    "\n",
    "def price_spread(series_ask, series_bid):\n",
    "    \"\"\"Calculate price spread; requires preprocessing to apply.\"\"\"\n",
    "    return (series_ask - series_bid) / ((series_ask + series_bid) / 2)\n",
    "\n",
    "def bid_ask_spread(series_ask_price, series_bid_price):\n",
    "    \"\"\"Calculate bid and ask spread; requires preprocessing to apply.\"\"\"\n",
    "    return series_ask_price - series_bid_price\n",
    "\n",
    "def total_volume(series_ask_size, series_bid_size):\n",
    "    \"\"\"Calculate total volume; requires preprocessing to apply.\"\"\"\n",
    "    return series_ask_size.sum() + series_bid_size.sum()\n",
    "\n",
    "def volume_imbalance(series_ask_size, series_bid_size):\n",
    "    \"\"\"Calculate volume imbalance; requires preprocessing to apply.\"\"\"\n",
    "    return abs(series_ask_size.sum() - series_bid_size.sum())\n",
    "\n",
    "def calculate_wap(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_aggregations = {\n",
    "    'max_bid_size_div_avg_bid_size': ('bid_size1', max_div_avg),\n",
    "    'max_ask_size_div_avg_ask_size': ('ask_size1', max_div_avg),\n",
    "    'ask_price2_ptp': ('ask_price2', np.ptp),\n",
    "    'bid_price2_ptp': ('bid_price2', np.ptp),\n",
    "    # 'ask_price2_calculate_percent_change_from_extremes': ('ask_price2', calculate_percent_change_from_extremes),\n",
    "    # 'bid_price2_calculate_percent_change_froxm_extremes': ('bid_price2', calculate_percent_change_from_extremes),\n",
    "    'ask_price1_ptp': ('ask_price1', np.ptp),\n",
    "    'bid_price1_ptp': ('bid_price1', np.ptp),\n",
    "    # 'ask_price1_calculate_percent_change_from_extremes': ('ask_price1', calculate_percent_change_from_extremes),\n",
    "    # 'bid_price1_calculate_percent_change_from_extremes': ('bid_price1', calculate_percent_change_from_extremes),\n",
    "    'ask_price2_max': ('ask_price2', 'max'),\n",
    "    'ask_price1_max': ('ask_price1', 'max'),\n",
    "    'abs_bid_price_change': ('bid_price1', abs_price_change_first_last),\n",
    "    'abs_ask_price_change': ('ask_price1', abs_price_change_first_last),\n",
    "    'wap_mean': ('wap', 'mean'),\n",
    "    'price_spread_mean': ('price_spread', 'mean'),  # Assuming 'price_spread' is pre-calculated\n",
    "    'bid_spread_mean': ('bid_spread', 'mean'),  # Assuming 'bid_spread' is pre-calculated\n",
    "    'ask_spread_mean': ('ask_spread', 'mean'),  # Assuming 'ask_spread' is pre-calculated\n",
    "    'volume_imbalance_mean': ('volume_imbalance', 'mean'),\n",
    "    'total_volume_mean': ('total_volume', 'mean'),\n",
    "}\n",
    "\n",
    "master = append_book_flattened_feature(list_order_book_file_train, book_aggregations, master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WXyKWI2pxaOb"
   },
   "outputs": [],
   "source": [
    "def trade_aggregate_features_per_time_id(file_path, feature_aggregations):\n",
    "    df_trade_data = pd.read_parquet(file_path)\n",
    "    stock_id = int(file_path.split('=')[1])\n",
    "\n",
    "    # Prepare the aggregation dictionary\n",
    "    aggregation_dict = {}\n",
    "    for new_name, (original, agg_func) in feature_aggregations.items():\n",
    "        aggregation_dict.setdefault(original, []).append((new_name, agg_func))\n",
    "\n",
    "    # Aggregate features\n",
    "    df_aggregated = df_trade_data.groupby('time_id').agg(\n",
    "        {key: [func for _, func in value] for key, value in aggregation_dict.items()}\n",
    "    )\n",
    "\n",
    "    # Simplify MultiIndex in columns\n",
    "    df_aggregated.columns = [\n",
    "        f\"{original}_{agg_func.__name__ if callable(agg_func) else agg_func}\"\n",
    "        for original, funcs in aggregation_dict.items() for _, agg_func in funcs\n",
    "    ]\n",
    "\n",
    "    # Efficient row_id creation\n",
    "    df_aggregated['row_id'] = f\"{stock_id}-\" + df_aggregated.index.astype(str)\n",
    "    df_aggregated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = ['row_id'] + [col for col in df_aggregated.columns if col != 'row_id']\n",
    "    df_aggregated = df_aggregated[cols]\n",
    "\n",
    "    return df_aggregated\n",
    "\n",
    "def trade_aggregate_features_for_all_stocks(list_file, feature_aggregations):\n",
    "    aggregated_data = [trade_aggregate_features_per_time_id(file, feature_aggregations) for file in list_file]\n",
    "    return pd.concat(aggregated_data, ignore_index=True)\n",
    "\n",
    "def append_trade_flattened_feature(list_file, flattened_feature, master):\n",
    "    df_aggregated_trade = trade_aggregate_features_for_all_stocks(list_file, flattened_feature)\n",
    "    master = pd.merge(master, df_aggregated_trade, on='row_id', how='left')\n",
    "    return master\n",
    "\n",
    "def calculate_percent_change(series):\n",
    "    return (series.iloc[-1] - series.iloc[0]) / series.iloc[0] if series.iloc[0] != 0 else np.nan\n",
    "\n",
    "def calculate_percent_change_from_extremes(series):\n",
    "    return (series.max() - series.min()) / series.min() if series.min() != 0 else np.nan\n",
    "\n",
    "def abs_price_change(series):\n",
    "    \"\"\"Returns the absolute price change within the series.\"\"\"\n",
    "    return abs(series.iloc[-1] - series.iloc[0])\n",
    "\n",
    "def max_div_avg_size(series):\n",
    "    \"\"\"Returns the ratio of the max size to the average size.\"\"\"\n",
    "    if series.mean() != 0:\n",
    "        return series.max() / series.mean()\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def max_div_avg_order_count(series):\n",
    "    \"\"\"Returns the ratio of the max order count to the average order count.\"\"\"\n",
    "    if series.mean() != 0:\n",
    "        return series.max() / series.mean()\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_aggregations = {\n",
    "    'price_ptp': ('price', np.ptp),\n",
    "    'price_calculate_percent_change_from_extremes': ('price', calculate_percent_change_from_extremes),\n",
    "    'abs_price_last_first': ('price', abs_price_change),\n",
    "    'max_size_div_avg_size': ('size', max_div_avg_size),\n",
    "    'max_order_count_div_avg_order_count': ('order_count', max_div_avg_order_count),\n",
    "    'avg_size': ('size', 'mean'),\n",
    "    'avg_order_count': ('order_count', 'mean'),\n",
    "    'trade_seconds_in_bucket_count_unique': ('seconds_in_bucket', 'nunique'),\n",
    "    'trade_size_sum': ('size', 'sum'),\n",
    "    'trade_order_count_mean': ('order_count', 'mean'),\n",
    "    'price_ptp': ('price', np.ptp),\n",
    "    'price_calculate_percent_change_from_extremes': ('price', calculate_percent_change_from_extremes),\n",
    "}\n",
    "\n",
    "master = append_trade_flattened_feature(list_order_trade_file_train, trade_aggregations, master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Derived Volatility Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Select features for KNN\n",
    "X = master.drop(['row_id', 'target', 'realized_vol'], axis=1)\n",
    "\n",
    "# Step 2: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Apply KNN to find the nearest 3 neighbors (excluding the row itself, so k=4)\n",
    "knn = NearestNeighbors(n_neighbors=4, algorithm='auto').fit(X_scaled)\n",
    "distances, indices = knn.kneighbors(X_scaled)\n",
    "\n",
    "# Step 4: Extract 'realized_vol' from the three nearest neighbors\n",
    "master.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Initialize columns for neighbor's realized_vol\n",
    "master['neighbor_vol_1'] = np.nan\n",
    "master['neighbor_vol_2'] = np.nan\n",
    "master['neighbor_vol_3'] = np.nan\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    # Get the indices of the three nearest neighbors (excluding the row itself)\n",
    "    neighbors_indices = indices[i, 1:]  # Exclude the first index since it's the row itself\n",
    "    \n",
    "    # Use the DataFrame's index to correctly reference rows for 'realized_vol'\n",
    "    neighbor_vols = master.iloc[neighbors_indices]['realized_vol'].values\n",
    "    \n",
    "    # Assign 'realized_vol' values from neighbors\n",
    "    master.at[i, 'neighbor_vol_1'] = neighbor_vols[0]\n",
    "    master.at[i, 'neighbor_vol_2'] = neighbor_vols[1]\n",
    "    master.at[i, 'neighbor_vol_3'] = neighbor_vols[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Model Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "neVlY6pD5Y8o"
   },
   "outputs": [],
   "source": [
    "master = master.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positively Correlated Features with 'target':\n",
      "realized_vol                                    0.873782\n",
      "neighbor_vol_1                                  0.835488\n",
      "neighbor_vol_2                                  0.830088\n",
      "neighbor_vol_3                                  0.826825\n",
      "price_spread_mean                               0.752907\n",
      "ask_price2_ptp                                  0.723822\n",
      "bid_price2_ptp                                  0.722555\n",
      "ask_price1_ptp                                  0.717661\n",
      "bid_price1_ptp                                  0.717583\n",
      "price_ptp                                       0.702566\n",
      "price_calculate_percent_change_from_extremes    0.700976\n",
      "next_vol_1                                      0.668897\n",
      "previous_vol_1                                  0.661782\n",
      "next_vol_2                                      0.619351\n",
      "previous_vol_2                                  0.615192\n",
      "next_vol_3                                      0.589616\n",
      "previous_vol_3                                  0.579523\n",
      "bid_price1_abs_price_change_first_last          0.532092\n",
      "ask_price1_abs_price_change_first_last          0.532028\n",
      "price_abs_price_change                          0.530796\n",
      "ask_price2_max                                  0.435032\n",
      "ask_spread_mean                                 0.428855\n",
      "ask_price1_max                                  0.411688\n",
      "bid_size1_max_div_avg                           0.151730\n",
      "ask_size1_max_div_avg                           0.132252\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_mean                                0.087330\n",
      "order_count_max_div_avg_order_count             0.069163\n",
      "size_max_div_avg_size                           0.067118\n",
      "seconds_in_bucket_nunique                       0.043729\n",
      "size_sum                                        0.036423\n",
      "wap_mean                                       -0.022303\n",
      "size_mean                                      -0.041525\n",
      "volume_imbalance_mean                          -0.048862\n",
      "total_volume_mean                              -0.061919\n",
      "order                                          -0.109195\n",
      "bid_spread_mean                                -0.419546\n",
      "dtype: float64\n",
      "\n",
      "Most Negatively Correlated Features with 'target':\n",
      "size_mean               -0.041525\n",
      "volume_imbalance_mean   -0.048862\n",
      "total_volume_mean       -0.061919\n",
      "order                   -0.109195\n",
      "bid_spread_mean         -0.419546\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter out non-numeric columns (excluding 'target' for correlation calculation)\n",
    "numeric_cols = master.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols.remove('target')  # Remove 'target' from the list to avoid self-correlation\n",
    "\n",
    "# Calculate correlations with the 'target' feature directly\n",
    "target_correlations = master[numeric_cols].apply(lambda x: x.corr(master['target']))\n",
    "\n",
    "# Sort the correlations\n",
    "sorted_correlations = target_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Print most positively and negatively correlated features\n",
    "print(\"Most Positively Correlated Features with 'target':\")\n",
    "print(sorted_correlations)  # Adjust the number as needed\n",
    "\n",
    "print(\"\\nMost Negatively Correlated Features with 'target':\")\n",
    "print(sorted_correlations.tail())  # Adjust the number as needed\n",
    "\n",
    "# Convert the sorted correlations series to a DataFrame\n",
    "sorted_correlations_df = sorted_correlations.reset_index()\n",
    "sorted_correlations_df.columns = ['Feature', 'Correlation_with_Target']\n",
    "\n",
    "# Filter for features with a correlation greater than 0.2\n",
    "high_correlation_features = sorted_correlations_df[sorted_correlations_df['Correlation_with_Target'] > 0]['Feature']\n",
    "\n",
    "# Select the filtered features from the master DataFrame\n",
    "filtered_master = master[['row_id', 'target'] + high_correlation_features.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "      <th>realized_vol</th>\n",
       "      <th>neighbor_vol_1</th>\n",
       "      <th>neighbor_vol_2</th>\n",
       "      <th>neighbor_vol_3</th>\n",
       "      <th>price_spread_mean</th>\n",
       "      <th>ask_price2_ptp</th>\n",
       "      <th>bid_price2_ptp</th>\n",
       "      <th>ask_price1_ptp</th>\n",
       "      <th>bid_price1_ptp</th>\n",
       "      <th>price_ptp</th>\n",
       "      <th>price_calculate_percent_change_from_extremes</th>\n",
       "      <th>next_vol_1</th>\n",
       "      <th>previous_vol_1</th>\n",
       "      <th>next_vol_2</th>\n",
       "      <th>previous_vol_2</th>\n",
       "      <th>next_vol_3</th>\n",
       "      <th>previous_vol_3</th>\n",
       "      <th>bid_price1_abs_price_change_first_last</th>\n",
       "      <th>ask_price1_abs_price_change_first_last</th>\n",
       "      <th>price_abs_price_change</th>\n",
       "      <th>ask_price2_max</th>\n",
       "      <th>ask_spread_mean</th>\n",
       "      <th>ask_price1_max</th>\n",
       "      <th>bid_size1_max_div_avg</th>\n",
       "      <th>ask_size1_max_div_avg</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_max_div_avg_order_count</th>\n",
       "      <th>size_max_div_avg_size</th>\n",
       "      <th>seconds_in_bucket_nunique</th>\n",
       "      <th>size_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4294</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>1.007990</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>1.007732</td>\n",
       "      <td>2.806304</td>\n",
       "      <td>6.434691</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.498525</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2034.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-24033</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.004334</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>1.000128</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>2.440895</td>\n",
       "      <td>3.456275</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>2.212766</td>\n",
       "      <td>3.214815</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-5666</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>3.053678</td>\n",
       "      <td>4.802214</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>4.578947</td>\n",
       "      <td>4.770754</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-29740</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>1.001563</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>1.001461</td>\n",
       "      <td>2.944738</td>\n",
       "      <td>4.280906</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>3.319149</td>\n",
       "      <td>4.967666</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-22178</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>1.000205</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>1.000051</td>\n",
       "      <td>3.586559</td>\n",
       "      <td>3.590304</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>5.150943</td>\n",
       "      <td>7.735746</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428908</th>\n",
       "      <td>99-24913</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.000982</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.000831</td>\n",
       "      <td>2.247635</td>\n",
       "      <td>2.343238</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.613497</td>\n",
       "      <td>7.736945</td>\n",
       "      <td>61.0</td>\n",
       "      <td>21524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428909</th>\n",
       "      <td>99-32195</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.000075</td>\n",
       "      <td>3.261741</td>\n",
       "      <td>6.643945</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>14.251719</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26474.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428910</th>\n",
       "      <td>99-15365</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>1.001584</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.001433</td>\n",
       "      <td>3.384056</td>\n",
       "      <td>10.750909</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>4.155738</td>\n",
       "      <td>3.794150</td>\n",
       "      <td>78.0</td>\n",
       "      <td>16138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428911</th>\n",
       "      <td>99-10890</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>1.000678</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.000527</td>\n",
       "      <td>3.441576</td>\n",
       "      <td>3.883602</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>5.197452</td>\n",
       "      <td>6.327757</td>\n",
       "      <td>96.0</td>\n",
       "      <td>51825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428912</th>\n",
       "      <td>99-29316</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.002338</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>1.002187</td>\n",
       "      <td>3.974785</td>\n",
       "      <td>5.009612</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>5.003344</td>\n",
       "      <td>6.472060</td>\n",
       "      <td>68.0</td>\n",
       "      <td>20383.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428913 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          row_id    target  realized_vol  neighbor_vol_1  neighbor_vol_2  \\\n",
       "0         0-4294  0.003267      0.007026        0.004937        0.006760   \n",
       "1        0-24033  0.002580      0.004136        0.002395        0.002581   \n",
       "2         0-5666  0.002051      0.002395        0.003481        0.001790   \n",
       "3        0-29740  0.002364      0.001790        0.001958        0.003481   \n",
       "4        0-22178  0.001439      0.002601        0.001628        0.001867   \n",
       "...          ...       ...           ...             ...             ...   \n",
       "428908  99-24913  0.001040      0.001153        0.001007        0.000905   \n",
       "428909  99-32195  0.001248      0.001502        0.001670        0.001905   \n",
       "428910  99-15365  0.001257      0.001379        0.001569        0.001595   \n",
       "428911  99-10890  0.002815      0.001468        0.001466        0.001355   \n",
       "428912  99-29316  0.001351      0.001443        0.001616        0.001784   \n",
       "\n",
       "        neighbor_vol_3  price_spread_mean  ask_price2_ptp  bid_price2_ptp  \\\n",
       "0             0.009059           0.002081        0.003196        0.009381   \n",
       "1             0.004334           0.000786        0.002452        0.002759   \n",
       "2             0.003443           0.000456        0.001332        0.002048   \n",
       "3             0.002056           0.000576        0.001384        0.001692   \n",
       "4             0.002264           0.000617        0.002514        0.002719   \n",
       "...                ...                ...             ...             ...   \n",
       "428908        0.000850           0.000158        0.001208        0.001208   \n",
       "428909        0.001790           0.000164        0.001509        0.001661   \n",
       "428910        0.001367           0.000172        0.002262        0.002262   \n",
       "428911        0.001246           0.000154        0.002259        0.002259   \n",
       "428912        0.001367           0.000181        0.001961        0.001810   \n",
       "\n",
       "        ask_price1_ptp  bid_price1_ptp  price_ptp  \\\n",
       "0             0.003505        0.004691   0.003957   \n",
       "1             0.002503        0.002810   0.001908   \n",
       "2             0.001280        0.002048   0.001078   \n",
       "3             0.001486        0.001281   0.001076   \n",
       "4             0.002616        0.002719   0.002411   \n",
       "...                ...             ...        ...   \n",
       "428908        0.001208        0.001208   0.001195   \n",
       "428909        0.001510        0.001661   0.001510   \n",
       "428910        0.002262        0.002262   0.002111   \n",
       "428911        0.002259        0.002259   0.002108   \n",
       "428912        0.001961        0.001810   0.001810   \n",
       "\n",
       "        price_calculate_percent_change_from_extremes  next_vol_1  \\\n",
       "0                                           0.003943    0.004136   \n",
       "1                                           0.001913    0.002395   \n",
       "2                                           0.001080    0.001790   \n",
       "3                                           0.001076    0.002601   \n",
       "4                                           0.002417    0.001474   \n",
       "...                                              ...         ...   \n",
       "428908                                      0.001196    0.001502   \n",
       "428909                                      0.001512    0.001379   \n",
       "428910                                      0.002113    0.001468   \n",
       "428911                                      0.002112    0.001443   \n",
       "428912                                      0.001810    0.001443   \n",
       "\n",
       "        previous_vol_1  next_vol_2  previous_vol_2  next_vol_3  \\\n",
       "0             0.007026    0.002395        0.007026    0.001790   \n",
       "1             0.007026    0.001790        0.007026    0.002601   \n",
       "2             0.004136    0.002601        0.007026    0.001474   \n",
       "3             0.002395    0.001474        0.004136    0.001465   \n",
       "4             0.001790    0.001465        0.002395    0.000891   \n",
       "...                ...         ...             ...         ...   \n",
       "428908        0.001301    0.001379        0.001368    0.001468   \n",
       "428909        0.001153    0.001468        0.001301    0.001443   \n",
       "428910        0.001502    0.001443        0.001153    0.001443   \n",
       "428911        0.001379    0.001443        0.001502    0.001443   \n",
       "428912        0.001468    0.001443        0.001379    0.001443   \n",
       "\n",
       "        previous_vol_3  bid_price1_abs_price_change_first_last  \\\n",
       "0             0.007026                                0.003608   \n",
       "1             0.007026                                0.002299   \n",
       "2             0.007026                                0.000717   \n",
       "3             0.007026                                0.000308   \n",
       "4             0.004136                                0.001642   \n",
       "...                ...                                     ...   \n",
       "428908        0.001394                                0.000302   \n",
       "428909        0.001368                                0.000906   \n",
       "428910        0.001301                                0.001056   \n",
       "428911        0.001153                                0.000301   \n",
       "428912        0.001502                                0.000302   \n",
       "\n",
       "        ask_price1_abs_price_change_first_last  price_abs_price_change  \\\n",
       "0                                     0.001598                0.001756   \n",
       "1                                     0.002197                0.001890   \n",
       "2                                     0.001024                0.000559   \n",
       "3                                     0.000410                0.000308   \n",
       "4                                     0.001744                0.001949   \n",
       "...                                        ...                     ...   \n",
       "428908                                0.000302                0.000151   \n",
       "428909                                0.000755                0.000604   \n",
       "428910                                0.001056                0.001056   \n",
       "428911                                0.000301                0.000455   \n",
       "428912                                0.000151                0.000151   \n",
       "\n",
       "        ask_price2_max  ask_spread_mean  ask_price1_max  \\\n",
       "0             1.007990         0.000423        1.007732   \n",
       "1             1.000128         0.000199        0.999923   \n",
       "2             1.000000         0.000176        0.999898   \n",
       "3             1.001563         0.000185        1.001461   \n",
       "4             1.000205         0.000165        1.000051   \n",
       "...                ...              ...             ...   \n",
       "428908        1.000982         0.000151        1.000831   \n",
       "428909        1.000226         0.000151        1.000075   \n",
       "428910        1.001584         0.000151        1.001433   \n",
       "428911        1.000678         0.000151        1.000527   \n",
       "428912        1.002338         0.000151        1.002187   \n",
       "\n",
       "        bid_size1_max_div_avg  ask_size1_max_div_avg  order_count_mean  \\\n",
       "0                    2.806304               6.434691          3.857143   \n",
       "1                    2.440895               3.456275          1.807692   \n",
       "2                    3.053678               4.802214          1.965517   \n",
       "3                    2.944738               4.280906          1.807692   \n",
       "4                    3.586559               3.590304          2.523810   \n",
       "...                       ...                    ...               ...   \n",
       "428908               2.247635               2.343238          5.344262   \n",
       "428909               3.261741               6.643945          3.600000   \n",
       "428910               3.384056              10.750909          3.128205   \n",
       "428911               3.441576               3.883602          6.541667   \n",
       "428912               3.974785               5.009612          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        order_count_max_div_avg_order_count  size_max_div_avg_size  \\\n",
       "0                                  2.333333               2.498525   \n",
       "1                                  2.212766               3.214815   \n",
       "2                                  4.578947               4.770754   \n",
       "3                                  3.319149               4.967666   \n",
       "4                                  5.150943               7.735746   \n",
       "...                                     ...                    ...   \n",
       "428908                             5.613497               7.736945   \n",
       "428909                            11.666667              14.251719   \n",
       "428910                             4.155738               3.794150   \n",
       "428911                             5.197452               6.327757   \n",
       "428912                             5.003344               6.472060   \n",
       "\n",
       "        seconds_in_bucket_nunique  size_sum  \n",
       "0                            14.0    2034.0  \n",
       "1                            26.0    1755.0  \n",
       "2                            29.0    1313.0  \n",
       "3                            26.0    1701.0  \n",
       "4                            21.0    2017.0  \n",
       "...                           ...       ...  \n",
       "428908                       61.0   21524.0  \n",
       "428909                      100.0   26474.0  \n",
       "428910                       78.0   16138.0  \n",
       "428911                       96.0   51825.0  \n",
       "428912                       68.0   20383.0  \n",
       "\n",
       "[428913 rows x 95 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>row_id</th>\n",
       "      <th>order</th>\n",
       "      <th>target</th>\n",
       "      <th>realized_vol</th>\n",
       "      <th>previous_vol_1</th>\n",
       "      <th>previous_vol_2</th>\n",
       "      <th>previous_vol_3</th>\n",
       "      <th>next_vol_1</th>\n",
       "      <th>next_vol_2</th>\n",
       "      <th>next_vol_3</th>\n",
       "      <th>bid_size1_max_div_avg</th>\n",
       "      <th>ask_size1_max_div_avg</th>\n",
       "      <th>ask_price2_ptp</th>\n",
       "      <th>ask_price2_max</th>\n",
       "      <th>bid_price2_ptp</th>\n",
       "      <th>ask_price1_ptp</th>\n",
       "      <th>ask_price1_max</th>\n",
       "      <th>ask_price1_abs_price_change_first_last</th>\n",
       "      <th>bid_price1_ptp</th>\n",
       "      <th>bid_price1_abs_price_change_first_last</th>\n",
       "      <th>wap_mean</th>\n",
       "      <th>price_spread_mean</th>\n",
       "      <th>bid_spread_mean</th>\n",
       "      <th>ask_spread_mean</th>\n",
       "      <th>volume_imbalance_mean</th>\n",
       "      <th>total_volume_mean</th>\n",
       "      <th>price_ptp</th>\n",
       "      <th>price_calculate_percent_change_from_extremes</th>\n",
       "      <th>price_abs_price_change</th>\n",
       "      <th>size_max_div_avg_size</th>\n",
       "      <th>size_mean</th>\n",
       "      <th>size_sum</th>\n",
       "      <th>order_count_max_div_avg_order_count</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>order_count_mean</th>\n",
       "      <th>seconds_in_bucket_nunique</th>\n",
       "      <th>neighbor_vol_1</th>\n",
       "      <th>neighbor_vol_2</th>\n",
       "      <th>neighbor_vol_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0-4294</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>2.806304</td>\n",
       "      <td>6.434691</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>1.007990</td>\n",
       "      <td>0.009381</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>1.007732</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.004691</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>1.005457</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>205.157609</td>\n",
       "      <td>498.081522</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>2.498525</td>\n",
       "      <td>145.285714</td>\n",
       "      <td>2034.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>0.009059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0-24033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>2.440895</td>\n",
       "      <td>3.456275</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>1.000128</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.998583</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>116.313783</td>\n",
       "      <td>382.923754</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>3.214815</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>2.212766</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>0.004334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0-5666</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>3.053678</td>\n",
       "      <td>4.802214</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>115.274576</td>\n",
       "      <td>319.016949</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>4.770754</td>\n",
       "      <td>45.275862</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>4.578947</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.003443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0-29740</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>2.944738</td>\n",
       "      <td>4.280906</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>1.001563</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.001461</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>1.000602</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>174.898058</td>\n",
       "      <td>496.927184</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>4.967666</td>\n",
       "      <td>65.423077</td>\n",
       "      <td>1701.0</td>\n",
       "      <td>3.319149</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.002056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0-22178</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>3.586559</td>\n",
       "      <td>3.590304</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>1.000205</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>1.000051</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.998349</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>148.091667</td>\n",
       "      <td>482.441667</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>7.735746</td>\n",
       "      <td>96.047619</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.150943</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.002264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428908</th>\n",
       "      <td>99</td>\n",
       "      <td>99-24913</td>\n",
       "      <td>3825</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>2.247635</td>\n",
       "      <td>2.343238</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>1.000982</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>1.000831</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>1.000192</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>357.053830</td>\n",
       "      <td>2268.246377</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>7.736945</td>\n",
       "      <td>352.852459</td>\n",
       "      <td>21524.0</td>\n",
       "      <td>5.613497</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>5.344262</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428909</th>\n",
       "      <td>99</td>\n",
       "      <td>99-32195</td>\n",
       "      <td>3826</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>3.261741</td>\n",
       "      <td>6.643945</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>1.000226</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>1.000075</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.999309</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>631.287109</td>\n",
       "      <td>2172.791016</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>14.251719</td>\n",
       "      <td>264.740000</td>\n",
       "      <td>26474.0</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.001790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428910</th>\n",
       "      <td>99</td>\n",
       "      <td>99-15365</td>\n",
       "      <td>3827</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>3.384056</td>\n",
       "      <td>10.750909</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>1.001584</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>1.001433</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>1.000229</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>386.595238</td>\n",
       "      <td>1867.503968</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>3.794150</td>\n",
       "      <td>206.897436</td>\n",
       "      <td>16138.0</td>\n",
       "      <td>4.155738</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>3.128205</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428911</th>\n",
       "      <td>99</td>\n",
       "      <td>99-10890</td>\n",
       "      <td>3828</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>3.441576</td>\n",
       "      <td>3.883602</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>1.000678</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>1.000527</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.999461</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>994.640777</td>\n",
       "      <td>3326.633010</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>6.327757</td>\n",
       "      <td>539.843750</td>\n",
       "      <td>51825.0</td>\n",
       "      <td>5.197452</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>6.541667</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428912</th>\n",
       "      <td>99</td>\n",
       "      <td>99-29316</td>\n",
       "      <td>3829</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>3.974785</td>\n",
       "      <td>5.009612</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>1.002338</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>1.002187</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>1.001094</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>445.455969</td>\n",
       "      <td>2089.162427</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>6.472060</td>\n",
       "      <td>299.750000</td>\n",
       "      <td>20383.0</td>\n",
       "      <td>5.003344</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>4.397059</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428913 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock    row_id  order    target  realized_vol  previous_vol_1  \\\n",
       "0          0    0-4294      0  0.003267      0.007026        0.007026   \n",
       "1          0   0-24033      1  0.002580      0.004136        0.007026   \n",
       "2          0    0-5666      2  0.002051      0.002395        0.004136   \n",
       "3          0   0-29740      3  0.002364      0.001790        0.002395   \n",
       "4          0   0-22178      4  0.001439      0.002601        0.001790   \n",
       "...      ...       ...    ...       ...           ...             ...   \n",
       "428908    99  99-24913   3825  0.001040      0.001153        0.001301   \n",
       "428909    99  99-32195   3826  0.001248      0.001502        0.001153   \n",
       "428910    99  99-15365   3827  0.001257      0.001379        0.001502   \n",
       "428911    99  99-10890   3828  0.002815      0.001468        0.001379   \n",
       "428912    99  99-29316   3829  0.001351      0.001443        0.001468   \n",
       "\n",
       "        previous_vol_2  previous_vol_3  next_vol_1  next_vol_2  next_vol_3  \\\n",
       "0             0.007026        0.007026    0.004136    0.002395    0.001790   \n",
       "1             0.007026        0.007026    0.002395    0.001790    0.002601   \n",
       "2             0.007026        0.007026    0.001790    0.002601    0.001474   \n",
       "3             0.004136        0.007026    0.002601    0.001474    0.001465   \n",
       "4             0.002395        0.004136    0.001474    0.001465    0.000891   \n",
       "...                ...             ...         ...         ...         ...   \n",
       "428908        0.001368        0.001394    0.001502    0.001379    0.001468   \n",
       "428909        0.001301        0.001368    0.001379    0.001468    0.001443   \n",
       "428910        0.001153        0.001301    0.001468    0.001443    0.001443   \n",
       "428911        0.001502        0.001153    0.001443    0.001443    0.001443   \n",
       "428912        0.001379        0.001502    0.001443    0.001443    0.001443   \n",
       "\n",
       "        bid_size1_max_div_avg  ask_size1_max_div_avg  ask_price2_ptp  \\\n",
       "0                    2.806304               6.434691        0.003196   \n",
       "1                    2.440895               3.456275        0.002452   \n",
       "2                    3.053678               4.802214        0.001332   \n",
       "3                    2.944738               4.280906        0.001384   \n",
       "4                    3.586559               3.590304        0.002514   \n",
       "...                       ...                    ...             ...   \n",
       "428908               2.247635               2.343238        0.001208   \n",
       "428909               3.261741               6.643945        0.001509   \n",
       "428910               3.384056              10.750909        0.002262   \n",
       "428911               3.441576               3.883602        0.002259   \n",
       "428912               3.974785               5.009612        0.001961   \n",
       "\n",
       "        ask_price2_max  bid_price2_ptp  ask_price1_ptp  ask_price1_max  \\\n",
       "0             1.007990        0.009381        0.003505        1.007732   \n",
       "1             1.000128        0.002759        0.002503        0.999923   \n",
       "2             1.000000        0.002048        0.001280        0.999898   \n",
       "3             1.001563        0.001692        0.001486        1.001461   \n",
       "4             1.000205        0.002719        0.002616        1.000051   \n",
       "...                ...             ...             ...             ...   \n",
       "428908        1.000982        0.001208        0.001208        1.000831   \n",
       "428909        1.000226        0.001661        0.001510        1.000075   \n",
       "428910        1.001584        0.002262        0.002262        1.001433   \n",
       "428911        1.000678        0.002259        0.002259        1.000527   \n",
       "428912        1.002338        0.001810        0.001961        1.002187   \n",
       "\n",
       "        ask_price1_abs_price_change_first_last  bid_price1_ptp  \\\n",
       "0                                     0.001598        0.004691   \n",
       "1                                     0.002197        0.002810   \n",
       "2                                     0.001024        0.002048   \n",
       "3                                     0.000410        0.001281   \n",
       "4                                     0.001744        0.002719   \n",
       "...                                        ...             ...   \n",
       "428908                                0.000302        0.001208   \n",
       "428909                                0.000755        0.001661   \n",
       "428910                                0.001056        0.002262   \n",
       "428911                                0.000301        0.002259   \n",
       "428912                                0.000151        0.001810   \n",
       "\n",
       "        bid_price1_abs_price_change_first_last  wap_mean  price_spread_mean  \\\n",
       "0                                     0.003608  1.005457           0.002081   \n",
       "1                                     0.002299  0.998583           0.000786   \n",
       "2                                     0.000717  0.998998           0.000456   \n",
       "3                                     0.000308  1.000602           0.000576   \n",
       "4                                     0.001642  0.998349           0.000617   \n",
       "...                                        ...       ...                ...   \n",
       "428908                                0.000302  1.000192           0.000158   \n",
       "428909                                0.000906  0.999309           0.000164   \n",
       "428910                                0.001056  1.000229           0.000172   \n",
       "428911                                0.000301  0.999461           0.000154   \n",
       "428912                                0.000302  1.001094           0.000181   \n",
       "\n",
       "        bid_spread_mean  ask_spread_mean  volume_imbalance_mean  \\\n",
       "0             -0.000231         0.000423             205.157609   \n",
       "1             -0.000088         0.000199             116.313783   \n",
       "2             -0.000095         0.000176             115.274576   \n",
       "3             -0.000101         0.000185             174.898058   \n",
       "4             -0.000087         0.000165             148.091667   \n",
       "...                 ...              ...                    ...   \n",
       "428908        -0.000151         0.000151             357.053830   \n",
       "428909        -0.000151         0.000151             631.287109   \n",
       "428910        -0.000151         0.000151             386.595238   \n",
       "428911        -0.000151         0.000151             994.640777   \n",
       "428912        -0.000151         0.000151             445.455969   \n",
       "\n",
       "        total_volume_mean  price_ptp  \\\n",
       "0              498.081522   0.003957   \n",
       "1              382.923754   0.001908   \n",
       "2              319.016949   0.001078   \n",
       "3              496.927184   0.001076   \n",
       "4              482.441667   0.002411   \n",
       "...                   ...        ...   \n",
       "428908        2268.246377   0.001195   \n",
       "428909        2172.791016   0.001510   \n",
       "428910        1867.503968   0.002111   \n",
       "428911        3326.633010   0.002108   \n",
       "428912        2089.162427   0.001810   \n",
       "\n",
       "        price_calculate_percent_change_from_extremes  price_abs_price_change  \\\n",
       "0                                           0.003943                0.001756   \n",
       "1                                           0.001913                0.001890   \n",
       "2                                           0.001080                0.000559   \n",
       "3                                           0.001076                0.000308   \n",
       "4                                           0.002417                0.001949   \n",
       "...                                              ...                     ...   \n",
       "428908                                      0.001196                0.000151   \n",
       "428909                                      0.001512                0.000604   \n",
       "428910                                      0.002113                0.001056   \n",
       "428911                                      0.002112                0.000455   \n",
       "428912                                      0.001810                0.000151   \n",
       "\n",
       "        size_max_div_avg_size   size_mean  size_sum  \\\n",
       "0                    2.498525  145.285714    2034.0   \n",
       "1                    3.214815   67.500000    1755.0   \n",
       "2                    4.770754   45.275862    1313.0   \n",
       "3                    4.967666   65.423077    1701.0   \n",
       "4                    7.735746   96.047619    2017.0   \n",
       "...                       ...         ...       ...   \n",
       "428908               7.736945  352.852459   21524.0   \n",
       "428909              14.251719  264.740000   26474.0   \n",
       "428910               3.794150  206.897436   16138.0   \n",
       "428911               6.327757  539.843750   51825.0   \n",
       "428912               6.472060  299.750000   20383.0   \n",
       "\n",
       "        order_count_max_div_avg_order_count  order_count_mean  \\\n",
       "0                                  2.333333          3.857143   \n",
       "1                                  2.212766          1.807692   \n",
       "2                                  4.578947          1.965517   \n",
       "3                                  3.319149          1.807692   \n",
       "4                                  5.150943          2.523810   \n",
       "...                                     ...               ...   \n",
       "428908                             5.613497          5.344262   \n",
       "428909                            11.666667          3.600000   \n",
       "428910                             4.155738          3.128205   \n",
       "428911                             5.197452          6.541667   \n",
       "428912                             5.003344          4.397059   \n",
       "\n",
       "        order_count_mean  order_count_mean  order_count_mean  \\\n",
       "0               3.857143          3.857143          3.857143   \n",
       "1               1.807692          1.807692          1.807692   \n",
       "2               1.965517          1.965517          1.965517   \n",
       "3               1.807692          1.807692          1.807692   \n",
       "4               2.523810          2.523810          2.523810   \n",
       "...                  ...               ...               ...   \n",
       "428908          5.344262          5.344262          5.344262   \n",
       "428909          3.600000          3.600000          3.600000   \n",
       "428910          3.128205          3.128205          3.128205   \n",
       "428911          6.541667          6.541667          6.541667   \n",
       "428912          4.397059          4.397059          4.397059   \n",
       "\n",
       "        seconds_in_bucket_nunique  neighbor_vol_1  neighbor_vol_2  \\\n",
       "0                            14.0        0.004937        0.006760   \n",
       "1                            26.0        0.002395        0.002581   \n",
       "2                            29.0        0.003481        0.001790   \n",
       "3                            26.0        0.001958        0.003481   \n",
       "4                            21.0        0.001628        0.001867   \n",
       "...                           ...             ...             ...   \n",
       "428908                       61.0        0.001007        0.000905   \n",
       "428909                      100.0        0.001670        0.001905   \n",
       "428910                       78.0        0.001569        0.001595   \n",
       "428911                       96.0        0.001466        0.001355   \n",
       "428912                       68.0        0.001616        0.001784   \n",
       "\n",
       "        neighbor_vol_3  \n",
       "0             0.009059  \n",
       "1             0.004334  \n",
       "2             0.003443  \n",
       "3             0.002056  \n",
       "4             0.002264  \n",
       "...                ...  \n",
       "428908        0.000850  \n",
       "428909        0.001790  \n",
       "428910        0.001367  \n",
       "428911        0.001246  \n",
       "428912        0.001367  \n",
       "\n",
       "[428913 rows x 42 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuJ6YpphCGdq"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "    return rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_master.drop(columns=['row_id', 'target'])\n",
    "y = filtered_master['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=25)\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_predictions = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Percentage Error: 0.2983427307371117\n"
     ]
    }
   ],
   "source": [
    "knn_rmspe = rmspe(y_test, knn_predictions)\n",
    "print(f'Root Mean Squared Percentage Error: {knn_rmspe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_master.drop(columns=['row_id', 'target'])\n",
    "y = filtered_master['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "linear_predictions = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSPE: 0.27519916645782005\n"
     ]
    }
   ],
   "source": [
    "linear_rmspe = rmspe(y_test, linear_predictions)\n",
    "print(f'Linear Regression RMSPE: {linear_rmspe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_master.drop(columns=['row_id', 'target'])\n",
    "y = filtered_master['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "rf_predictions = random_forest_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSPE: 0.26714806943406016\n"
     ]
    }
   ],
   "source": [
    "rf_rmspe = rmspe(y_test, rf_predictions)\n",
    "print(f'Random Forest RMSPE: {rf_rmspe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_master.drop(columns=['row_id', 'target'])\n",
    "y = filtered_master['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "gb_predictions = gradient_boosting_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting RMSPE: 0.26577042121482414\n"
     ]
    }
   ],
   "source": [
    "gb_rmspe = rmspe(y_test, gb_predictions)\n",
    "print(f'Gradient Boosting RMSPE: {gb_rmspe}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    mask = y_true != 0\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true[mask] - y_pred[mask]) / y_true[mask])))\n",
    "    return rmspe\n",
    "\n",
    "def lgb_rmspe(y_pred, data):\n",
    "    y_true = data.get_label()\n",
    "    rmspe_val = rmspe(y_true, y_pred)\n",
    "    return 'RMSPE', rmspe_val, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',  \n",
    "    'metric': 'None',  \n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = master.drop(columns=['row_id', 'target'])\n",
    "y = master['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_test, label=y_test, reference=d_train)\n",
    "\n",
    "clf = lgb.train(\n",
    "    params,\n",
    "    d_train,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[d_train, d_valid],\n",
    "    valid_names=['train', 'valid'],\n",
    "    feval=lgb_rmspe,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSPE: 0.2522851559480745\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "print(\"RMSPE:\", rmspe(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-14 11:28:04,009] A new study created in memory with name: no-name-65208294-609e-418d-a222-1c1d606d5962\n",
      "[I 2024-04-14 11:28:12,391] Trial 0 finished with value: 0.24958733291752547 and parameters: {'num_leaves': 159, 'learning_rate': 0.12562975188621053, 'n_estimators': 345, 'min_child_samples': 77}. Best is trial 0 with value: 0.24958733291752547.\n",
      "[I 2024-04-14 11:28:27,326] Trial 1 finished with value: 0.25037711589011136 and parameters: {'num_leaves': 32, 'learning_rate': 0.2571059716832975, 'n_estimators': 2347, 'min_child_samples': 68}. Best is trial 0 with value: 0.24958733291752547.\n",
      "[I 2024-04-14 11:28:38,860] Trial 2 finished with value: 0.2533388920195773 and parameters: {'num_leaves': 117, 'learning_rate': 0.29230720046192415, 'n_estimators': 671, 'min_child_samples': 59}. Best is trial 0 with value: 0.24958733291752547.\n",
      "[I 2024-04-14 11:29:21,718] Trial 3 finished with value: 0.24606739560678317 and parameters: {'num_leaves': 278, 'learning_rate': 0.06592380203092278, 'n_estimators': 1133, 'min_child_samples': 92}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:30:35,702] Trial 4 finished with value: 0.2467703983082413 and parameters: {'num_leaves': 147, 'learning_rate': 0.0921709040224219, 'n_estimators': 3559, 'min_child_samples': 100}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:32:18,011] Trial 5 finished with value: 0.24769212162637697 and parameters: {'num_leaves': 95, 'learning_rate': 0.08683100039320234, 'n_estimators': 7058, 'min_child_samples': 82}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:34:34,602] Trial 6 finished with value: 0.2600697488456216 and parameters: {'num_leaves': 119, 'learning_rate': 0.22494955347112364, 'n_estimators': 8135, 'min_child_samples': 29}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:35:12,144] Trial 7 finished with value: 0.2698368436527429 and parameters: {'num_leaves': 133, 'learning_rate': 0.29689251775149594, 'n_estimators': 2047, 'min_child_samples': 46}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:36:23,468] Trial 8 finished with value: 0.26815586924483525 and parameters: {'num_leaves': 58, 'learning_rate': 0.2998469170221017, 'n_estimators': 7401, 'min_child_samples': 95}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:36:30,050] Trial 9 finished with value: 0.2516897203474242 and parameters: {'num_leaves': 144, 'learning_rate': 0.10500698335506196, 'n_estimators': 286, 'min_child_samples': 87}. Best is trial 3 with value: 0.24606739560678317.\n",
      "[I 2024-04-14 11:39:49,793] Trial 10 finished with value: 0.24454924717021356 and parameters: {'num_leaves': 298, 'learning_rate': 0.014757601750275094, 'n_estimators': 4926, 'min_child_samples': 15}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:43:20,572] Trial 11 finished with value: 0.24522882185459088 and parameters: {'num_leaves': 289, 'learning_rate': 0.011298511893500028, 'n_estimators': 5231, 'min_child_samples': 10}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:46:58,975] Trial 12 finished with value: 0.24462310028919285 and parameters: {'num_leaves': 299, 'learning_rate': 0.012914564316783003, 'n_estimators': 5265, 'min_child_samples': 8}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:49:52,108] Trial 13 finished with value: 0.24781639496368185 and parameters: {'num_leaves': 230, 'learning_rate': 0.010906241016388422, 'n_estimators': 5073, 'min_child_samples': 5}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:54:32,824] Trial 14 finished with value: 0.2556921328106546 and parameters: {'num_leaves': 221, 'learning_rate': 0.18073655627206764, 'n_estimators': 9363, 'min_child_samples': 23}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:56:32,701] Trial 15 finished with value: 0.24492495651977664 and parameters: {'num_leaves': 228, 'learning_rate': 0.04613439813211073, 'n_estimators': 3963, 'min_child_samples': 30}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 11:59:47,416] Trial 16 finished with value: 0.2532685954302337 and parameters: {'num_leaves': 259, 'learning_rate': 0.15910672008570406, 'n_estimators': 5772, 'min_child_samples': 17}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 12:02:28,408] Trial 17 finished with value: 0.24513172866455263 and parameters: {'num_leaves': 185, 'learning_rate': 0.04242370808787474, 'n_estimators': 6256, 'min_child_samples': 38}. Best is trial 10 with value: 0.24454924717021356.\n",
      "[I 2024-04-14 12:05:04,840] Trial 18 finished with value: 0.24412369627910407 and parameters: {'num_leaves': 298, 'learning_rate': 0.044602997457432514, 'n_estimators': 3979, 'min_child_samples': 15}. Best is trial 18 with value: 0.24412369627910407.\n",
      "[I 2024-04-14 12:06:37,932] Trial 19 finished with value: 0.25202476779240424 and parameters: {'num_leaves': 200, 'learning_rate': 0.1272318859205708, 'n_estimators': 3517, 'min_child_samples': 50}. Best is trial 18 with value: 0.24412369627910407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial: {'num_leaves': 298, 'learning_rate': 0.044602997457432514, 'n_estimators': 3979, 'min_child_samples': 15}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmspe',  \n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    d_train = lgb.Dataset(X_train, label=y_train)\n",
    "    gbm = lgb.train(param, d_train)\n",
    "    preds = gbm.predict(X_test)\n",
    "    return rmspe(y_test, preds) \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DCqMjbuQ1yhB",
    "U6gZlJILpStt",
    "3oBkUzxmtzAl",
    "ObRgMeHASM-B",
    "xLeoQqC8mJa1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
