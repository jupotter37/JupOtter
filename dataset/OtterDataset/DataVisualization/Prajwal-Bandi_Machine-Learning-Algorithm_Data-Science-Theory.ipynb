{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eaefad9-4492-42bb-81be-0b7322553977",
   "metadata": {},
   "source": [
    "Statistics : It is a branch of mathematics which deals with collecting analyzing and organizing data for business purpose or decision making process.It involves a range of methonds and techiques used to understand and draw conclusions from data\n",
    "\n",
    "Type's of Statistics :\n",
    "1. Descriptive Statistics : It is a method of organizing and summarizing data from featured dataset.EX : Measure of tendencu and measure of dispersion.\n",
    "2. Inferential Statistics : This involves making predictions or inrefrences about a population based on sample data. Ex: Probability, Hypothesis testing,Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dedb22-a332-4a96-97c8-eccf475d4826",
   "metadata": {},
   "source": [
    "Population (N): Population refers to the entire group of individual or observations that you are interested in studying.Ex; All students in a class \n",
    "\n",
    "Sample(n): A sample is a subset of population that we used to make prediction or conclusion on it.Ex; 50 students are randomly choosen from a class of 500 students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788d565-d9aa-40b8-8812-14eaa7c494bb",
   "metadata": {},
   "source": [
    "Data : It feres to the information collected for analysis\n",
    "\n",
    "Data collection : Methods for gathering data,including surveys,experiments,observational studies and administrative records.\n",
    "\n",
    "Types of Data Based on Measurement Levels\n",
    "\n",
    "1. Quantitative Data: This type of data represents quantities and is numerical value. It can be measured and counted.\n",
    "\n",
    "a.Discrete data:Consists of distinct, separate values. It is countable and often represents whole numbers.\n",
    "ex:Number of students in a class, number of cars in a parking lot.\n",
    "\n",
    "b.Continuous data: Can take any value within a given range. It is measurable and can be represented by fractions and decimals.\n",
    "ex; heigh,weight\n",
    "\n",
    "2. Qualitative Data: This type of data represents qualities or characteristics. It is non-numerical and is used to categorize or describe attributes of a population.\n",
    "\n",
    "a.Nominal Data: Categorical data with no inherent order or ranking. It consists of names, labels, or categories.\n",
    "ex:Gender (male, female), eye color (blue, green, brown)\n",
    "\n",
    "b.Ordinal Data: Categorical data with a meaningful order or ranking, but the intervals between values are not necessarily equal.\n",
    "ex:Educational level (high school, bachelor‚Äôs, master‚Äôs), satisfaction rating (satisfied, neutral, dissatisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a317f-07a9-49f4-b92e-31ab230cc760",
   "metadata": {},
   "source": [
    "DESCRIPTIVE STATISTICS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51d39a-5441-4107-9cbd-677ddcdceff3",
   "metadata": {},
   "source": [
    "Measure of central tendency:\n",
    "Measures of central tendency are statistical metrics that describe the center or typical value of a dataset. These measures help to summarize a large amount of data with a single value representing the middle or average of the dataset. The three most common measures of central tendency are the mean, median, and mode.\n",
    "\n",
    "1. Mean:(Average):The mean, also known as the average, is calculated by summing all the values in a dataset and dividing by the number of values. mean in sample data and average in population data.\n",
    "Mean=‚àëx/i\n",
    "where: ‚àëùë•ùëñ is the sum of all data points\n",
    "       ùëõ is the number of data points\n",
    "\n",
    "2. Median:The median is the middle value in a dataset when it is ordered from least to greatest. If there is an even number of observations, the median is the average of the two middle numbers.\n",
    "\n",
    "3. Mode:The mode is the value that appears most frequently in a dataset. A dataset may have one mode, more than one mode, or no mode at all.\n",
    "\n",
    "\n",
    "Measure of dispersion:\n",
    "\n",
    "Measures of dispersion, also known as measures of variability or spread, describe the extent to which data points in a dataset differ from the central tendency (mean, median, or mode). These measures provide insights into the distribution and variability of the data. Common measures of dispersion include the range, variance, standard deviation, interquartile range (IQR), and mean absolute deviation (MAD).\n",
    "\n",
    "1. Range:The range is the simplest measure of dispersion and represents the difference between the highest and lowest values in a dataset.\n",
    "Formula:  Range=Maximum¬†Value‚àíMinimum¬†Value\n",
    "\n",
    "2. Variance: Variance measures the average squared deviation of each data point from the mean. It provides insight into the spread of the data around the mean.\n",
    "Variance(œÉ2)=‚àë(xi‚àíŒº)2/n for sample data sample mean \n",
    "\n",
    "3. Standard Deviation: The standard deviation is the square root of the variance and provides a measure of the average distance of each data point from the mean.\n",
    "\n",
    "4. Interquartile :It's a statistical measure used to describe the spread or dispersion of a dataset by focusing on the range within which the middle 50% of the values lie. and is the difference between the first quartile (Q1) and the third quartile (Q3).\n",
    "IQR=Q3-Q1\n",
    "\n",
    "5. Mean Absolute Deviation (MAD): The mean absolute deviation is the average of the absolute deviations of each data point from the mean.\n",
    "\n",
    "NOTE : Why we divide sample variance by n-1: The sample variance is divided by n-1 so tht we can create an unbaised estimator of the population variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a139fa-f7ec-45d5-aa08-cd4c51170af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 196.42857142857142\n",
      "Median: 200.0\n",
      "Mode: ModeResult(mode=180, count=2)\n",
      "Variance: 1578.5714285714287\n",
      "Standard Deviation: 39.73123995763823\n",
      "count     14.000000\n",
      "mean     196.428571\n",
      "std       39.731240\n",
      "min      100.000000\n",
      "25%      180.000000\n",
      "50%      200.000000\n",
      "75%      227.500000\n",
      "max      250.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Example data\n",
    "sales_data = pd.Series([100, 150, 230, 180, 200, 190, 210, 180, 220, 250, 170, 200, 230, 240])\n",
    "\n",
    "# Calculate mean\n",
    "mean_sales = np.mean(sales_data)\n",
    "print(f\"Mean: {mean_sales}\")\n",
    "\n",
    "# Calculate median\n",
    "median_sales = np.median(sales_data)\n",
    "print(f\"Median: {median_sales}\")\n",
    "\n",
    "# Calculate mode\n",
    "mode_sales = stats.mode(sales_data)\n",
    "print(f\"Mode: {mode_sales}\")\n",
    "\n",
    "# Calculate variance (Sample variance)\n",
    "variance_sales = np.var(sales_data, ddof=1)\n",
    "print(f\"Variance: {variance_sales}\")\n",
    "\n",
    "# Calculate standard deviation (Sample standard deviation)\n",
    "std_dev_sales = np.std(sales_data, ddof=1)\n",
    "print(f\"Standard Deviation: {std_dev_sales}\")\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(sales_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d133edf-557e-4833-922a-738e1f03e3ad",
   "metadata": {},
   "source": [
    "Percentile : A percentile is a measure used in statistics to indicate the value below which a given percentage of observations in a dataset falls. Percentiles are useful for understanding the distribution of data and identifying the relative standing of a particular value within a dataset.It is a value below which a certain percentage of observation lies..\n",
    "\n",
    "Percentile(number) ranking : number of values below x /n * 100\n",
    "Ranking Value = percentile/100 *(n+1)=which gives index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9ad35-5fe4-4775-a87d-d67717d8ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers: Outliers are data points that are significantly different from the rest of the dataset\n",
    "\n",
    "Handling Outlier : Using Interquartile range method\n",
    "\n",
    "IQR=Q3-Q1\n",
    "Lower bound = Q1-1.5*IQR\n",
    "Uppper bound =Q3+1.5*IQR\n",
    "\n",
    "the values between below the lower bound and above the upper bound are considered as outlier.\n",
    "\n",
    "NOTE:Quantile and percentile both are same.\n",
    "\n",
    "Using Boxplot outlier can be detected.\n",
    "Five number summary\n",
    "Minimum=0\n",
    "Q1=25%\n",
    "Median=50%\n",
    "Q3=75%\n",
    "Maximum=100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb52ed-ea25-4bbe-b58a-b0d22d3373d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sample data\n",
    "data = [15, 20, 35, 40, 50, 55, 60, 70, 80, 90, 100, 150, 200]\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = np.quantile(data, 0.25)\n",
    "Q3 = np.quantile(data, 0.75)\n",
    "IQR = Q3 - Q1\n",
    "# Calculate lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# Identify outliers\n",
    "outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n",
    "print(\"Outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d85510-b9e7-42d6-a871-19744444acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For csv dataset\n",
    "num_cols=data.select_dtypes(include=[float,int]).columns\n",
    "num_cols=data.select_dtypes(include=[object,'category']).columnd\n",
    "\n",
    "def detect_outlier(data):\n",
    "    Q1=data[num_cols].quantile(0.25)\n",
    "    Q3=data[num_cols].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    outliers=(data[num_cols]< Q1-1.5*IQR) | (data[num_cols]> Q3+1.5*IQR)\n",
    "    return outliers\n",
    "\n",
    "outlier=detect_outlier(data)\n",
    "print(outlier.sum()/len(data)*100)\n",
    "\n",
    "data=data[~outliers.any(axis=1)]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8849525-3dac-4441-b149-bb92c888b591",
   "metadata": {},
   "source": [
    "Skewness : Skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean. In simpler terms, it indicates whether the data is skewed to the left (negative skew) or to the right (positive skew) relative to the normal distribution.\n",
    "\n",
    "Negative Skewness: The left tail is longer, and the mass of the distribution is concentrated on the right side. The mean is less than the median.\n",
    "\n",
    "Positive Skewness: The right tail is longer, and the mass of the distribution is concentrated on the left side. The mean is greater than the median.\n",
    "\n",
    "Zero Skewness: The distribution is symmetric around the mean (bell-shaped like the normal distribution).\n",
    "\n",
    "Kurtosis : Kurtosis is a statistical measure that describes the shape of s distribution's tails in relation to its overall shape.It indicates whether data points are more or less concentrated in the tails compared to a normal distribution.High kurtosis signifies heavy tails and possibly outliers,while low kurtosis indicates light tails and a lack of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315429bf-fbca-4b76-94d2-58fba3d09631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Example data\n",
    "data = np.array([15, 20, 35, 40, 50, 55, 60, 70, 80, 90])\n",
    "\n",
    "# Calculate skewness\n",
    "data_skew = skew(data)\n",
    "print(f\"Skewness: {data_skew}\")\n",
    "\n",
    "# Calculate kurtosis\n",
    "data_kurtosis = kurtosis(data)\n",
    "print(f\"Kurtosis: {data_kurtosis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ef595-8016-4cc6-a780-ef8774ea7891",
   "metadata": {},
   "source": [
    "Central Limit Theorem:\n",
    "It says that the sampling distribution of the mean will always be normally distributed as long as the sample size is large enough.Regardless of whether the population has a normal,possion binomial r any distribution , the sampling distribution of the mean will be always normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fb94b-950b-4faf-aed6-63e28b46a4d1",
   "metadata": {},
   "source": [
    "Z_score : It is a statistical measurement that describes to value position relative to the mean of a group of value. also known as the standard score, is a statistical measure that indicates how many standard deviations a data point is from the mean of the dataset. It helps in understanding where a particular data point stands in relation to the mean and how typical or atypical it is compared to the rest of the data.\n",
    "\n",
    "Z=(X-mean)/std\n",
    "\n",
    "Use of Z-score\n",
    "1. Outlier Detection: Z-score helps identify outliers in a dataset. Typically, data points with a Z-score greater than 3 or less than -3 are considered outliers.\n",
    "2. Normalization: Z-score normalization (standardization) transforms data so that it has a mean of 0 and a standard deviation of 1. This is useful in comparing data from different distributions.\n",
    "3. Hypothesis Testing: Z-scores are used in hypothesis testing to determine whether a sample mean is significantly different from a population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614431ae-5509-40bc-9a7b-39d8f910938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([15, 20, 35, 40, 50, 55, 60, 70, 80, 90])\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)\n",
    "\n",
    "# Example data point\n",
    "X = 70\n",
    "\n",
    "# Calculate Z-score\n",
    "Z = (X - mean) / std_dev\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Z-score for {X}: {Z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31458c3a-cdf0-44e1-8821-f1e9f33eab40",
   "metadata": {},
   "source": [
    "INFERENTIAL STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e48bd3-96b6-416e-8223-855f0444089a",
   "metadata": {},
   "source": [
    "P_value: The p value is a number calculated from a statistical test,that describes how likely you are to have found a particuler set of observation if the null hypothesis were true.It is used in hypothesis testing to decide whether to reject r accept the null hypothesis.\n",
    "It is evidence against a null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25781de0-c0c7-4dd3-a4a2-8aa876533df6",
   "metadata": {},
   "source": [
    "Confidence interval :A range of values that are likely contain a true population parameter.\n",
    "\n",
    "p_value=1-confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9d6c0-30db-43fb-a131-2c2923e7bc44",
   "metadata": {},
   "source": [
    "Parametric test : Parametric test are statistical methods that make assumption that the data follows normal distribution.\n",
    "\n",
    "Z_test & T_test : Both are statistical tests used to determine whether their is a significant difference between sample mean  and population mean in hypothesis testing.\n",
    "\n",
    "z_test : used for large sample with a known population standard deviation.It is based on the standard normal distribution (Z-distribution)\n",
    "1. one sample Z-test : compares the mean of single sample to known population mean.\n",
    "2. two sample Z-test : compares the mean of two independent samples to determine if they are significantly different.\n",
    "\n",
    "T_test : used for small sample or when the population standard deviation is unknown\n",
    "1. one sample t-test : compare the mean of a single sample to the mean of the population.\n",
    "2. two sample t-test : compare the means of two independent samples.\n",
    "\n",
    "Annova : Analysis of variance is a statistical method used to compare the mean of 2 or more groups to determine if there are statistically significance difference among them.\n",
    "H0 : Assumes the mean of all groups are equall\n",
    "H1 : Assumes that at least one group mean is different from the others.\n",
    "\n",
    "1. One-way-Anova : Tests the effect of a single factor (independent variable) on a dependent variable.\n",
    "2. Two-way-Anova : Tests the effect of two factors (independent variables) on a dependent variable, and also examines the interaction between the factors.\n",
    "\n",
    "Regression Analysis:\n",
    "1. Simple Linear Regression: Examines the relationship between two variables.\n",
    "2. Multiple Regression: Examines the relationship between one dependent variable and multiple independent variables.\n",
    "\n",
    "Non-Parametric Test : Non-parametric test does not make assumptions about the distribution of the data.Thy are useful when data does not meet the assumptions required for parametric test.\n",
    "\n",
    "chi-square test:Test is a statistical test used to determine if there is a significant association between categorical variables. .It is non parametric test that is performed on categorical(Ordinal,Nomial) data.\n",
    "\n",
    "Proportion Tests : Z-Test for Proportions: Tests whether the proportion of successes in a sample is significantly different from a known proportion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249f00c-8763-4059-b96c-745aff75bde0",
   "metadata": {},
   "source": [
    "Hypothesis testing : Hypothesis Testing is a statistical method used to make inferences or draw conclusions about a population based on sample data. It involves formulating and testing a hypothesis to determine if there is enough evidence to support a particular claim or assumption.\n",
    "        \n",
    "1. Null Hypothesis (H0):\n",
    "->The default assumption or status quo that there is no effect, difference, or relationship.\n",
    "Example: \"There is no difference in test scores between two teaching methods.\"\n",
    "\n",
    "2. Alternative Hypothesis (H1 or Ha):\n",
    "->The hypothesis that contradicts the null hypothesis, indicating the presence of an effect, difference, or relationship.\n",
    "Example: \"There is a difference in test scores between two teaching methods.\"\n",
    "\n",
    "3. Test Statistic:\n",
    "->A numerical value calculated from the sample data that is used to decide whether to reject the null hypothesis.\n",
    "->Common test statistics include z-scores, t-scores, F-scores, and chi-square statistics, depending on the test used.\n",
    "\n",
    "4. P-Value:\n",
    "->The probability of observing the test statistic or something more extreme, assuming the null hypothesis is true.\n",
    "->A smaller p-value indicates stronger evidence against the null hypothesis.\n",
    "\n",
    "5. Significance Level (Œ±):\n",
    "A threshold for the p-value used to decide whether to reject the null hypothesis. Common values are 0.05, 0.01, and 0.10.\n",
    "\n",
    "6. Make a Decision:\n",
    "->Compare the p-value to the significance level (Œ±).\n",
    "->If the p-value ‚â§ Œ±, reject the null hypothesis.\n",
    "->If the p-value > Œ±, fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a701e-dcf9-49b6-ac05-59dc77247bdd",
   "metadata": {},
   "source": [
    "Type 1 and Type 2 error\n",
    "\n",
    "Reality : Null hypothesis is true r false\n",
    "conclusion : Null hypothesis is true r false\n",
    "\n",
    "Outcome 1 : We reject the null hypothesis in reality it is False (Good)\n",
    "Outcome 2 : We reject the null hypothesis in reality it is True  (Type 1 -Error)\n",
    "Outcome 3 : we accept the null hypothesis in reality it is False (Type 2 -Error)\n",
    "Outcome 4 : We accept the null hypothesis in reality it is True  (Good)\n",
    "\n",
    "Type 1 Error is False Positive\n",
    "Type 2 Error is False Negative \n",
    "\n",
    "A Type I error occurs when the null hypothesis(Ho) is true, but we mistakenly reject it. In other words, it's the incorrect rejection of a true null hypothesis.\n",
    "A Type II error occurs when the null hypothesis(Ho) is false, but we fail to reject it. In other words, it's the failure to reject a false null hypothesis.\n",
    "\n",
    "Ex :\n",
    "1. Test indicates that patient has diseas but in reality they do not \n",
    "2. Test indicates that patient do not have disease but in reality thy do have.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d549a66-e518-448a-9e67-8c110a5ec8e2",
   "metadata": {},
   "source": [
    "Correlation : Correlation is the statistical measure that describes the strength and direction of linear relationship between two random variables.Correlation is a standardized form of covariance.It is standardized measure and its value ranges from (-1 to +1)\n",
    "+1 indicates a perfect positive linear relationship.\n",
    "‚àí1 indicates a perfect negative linear relationship.\n",
    "0 indicates no linear relationship.\n",
    "\n",
    "Covariance : Covariance measures the direction of the linear relationship between two variables. It indicates whether an increase in one variable corresponds to an increase (positive covariance) or a decrease (negative covariance) in the other variable.Not standardized value depends on unit of variables.\n",
    "->Positive covariance indicates that the two variables tend to move in the same direction.\n",
    "->Negative covariance indicates that the two variables tend to move in opposite directions.\n",
    "->Zero covariance indicates no linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee28a07-a766-4fb4-9471-3798d9fe6db8",
   "metadata": {},
   "source": [
    "PROBABILITY: An action or process that leads 1 or more possible outcomes\n",
    "\n",
    "Random variable : It is a variable whose possible values are numerical outcomes of random phenomenon\n",
    "\n",
    "Discrete random variable: which has countable numeric of distinct value \n",
    "Ex: number of students in class \n",
    "\n",
    "Continous random variable : which has infinite number of possible values within given range \n",
    "Ex: height,weight \n",
    "\n",
    "Probability : An action r process that lead 1 or more possible outcomes\n",
    "Ex; Rolling die P(A)=Number of favorable outcome/Total number of outcomes\n",
    "\n",
    "Addition rule ( 'or')\n",
    "Multiple rule ('and')\n",
    "\n",
    "Probability Distribution : It is a function that provides the probability of occurance of different possible outcomes in an experiment.\n",
    "1. Discrete probability distribution : Burnoulli,Binomail,Poission\n",
    "2. Continuous probability distribution : Normal,uniform ,chisquare\n",
    "\n",
    "1. Bernoulli Distribution : An experiement which takes only 2 possible outcome(success,failure)\n",
    "2. Binomial Distribution : Represent the number of success in a fixed number of independent bernoulli trails.\n",
    "3. Normal(Gaussian)Distribution : It i s a continous random variable with symmetric bell shaped distribution (mean=median=mode).\n",
    "4. Uniform Distribution : Represents a continuous random variables that has equal probability over a given range.\n",
    "\n",
    "Permutations : Permutations are arrangements of items where the order matters. The number of permutations of a set of n distinct items taken ùëü at a time is given by:\n",
    "P(n,r)=n!/(n-r)!\n",
    "\n",
    "Combination:Combinations are selections of items where the order does not matter. The number of combinations of a set of n distinct items taken r at a time is given by:\n",
    "C(n,r)=n!/r!(n-r)!\n",
    "\n",
    "Bayes theorem:Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence or information.which describes the probability of an event based on prior knowledge of conditions related to the event.\n",
    "\n",
    "P(A/B)=P(B/A)*P(A)/P(B).\n",
    "\n",
    "P(A/B)=Probability of event A given B is true\n",
    "P(A)=Probability of event A\n",
    "P(B)=Probability of event B\n",
    "P(B/A)=Probability of event B given A is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2733ad-8e89-4839-810b-be246dcf9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Number of permutations of 5 items taken 3 at a time\n",
    "n = 5\n",
    "r = 3\n",
    "permutations = math.perm(n, r)\n",
    "print(f\"Permutations P({n}, {r}): {permutations}\")\n",
    "\n",
    "# Number of combinations of 5 items taken 3 at a time\n",
    "combinations = math.comb(n, r)\n",
    "print(f\"Combinations C({n}, {r}): {combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5ce51-0b2c-45ed-9b17-7b357b8e5153",
   "metadata": {},
   "source": [
    "Data Science Project Workflow or Life Cycle  : \n",
    "1. Problem definition : Clearly define the problem or question you aim to answer with data science.\n",
    "2. Data Collection : Gather the data required to address the problem.\n",
    "3. Data Cleaning :  Prepare the data for analysis by handling missing values, duplicates, and errors.\n",
    "4. Exploratory Data Analysis (EDA) :Gain insights into the data through visualization and summary statistics.\n",
    "5. Feature Engineering : Create new features or modify existing ones to improve model performance.\n",
    "6. Model Selection : Choose the appropriate machine learning algorithms for the task.\n",
    "7. Model Building : Train machine learning models on the training data.\n",
    "8. Model Evaluation : Assess the performance of the models using appropriate metrics.\n",
    "9. Model Deployment : Deploy the selected model to a production environment.\n",
    "10. Model Monitoring and Maintenance : Continuously monitor and maintain the model to ensure it remains effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a0a51-2814-4fe9-93d5-d6515c40f1b9",
   "metadata": {},
   "source": [
    "EDA : Exploratory Data Analysis (EDA) is a crucial step in data analysis that involves exploring and summarizing key characteristics of a dataset. It helps analysts understand the data, discover patterns, spot anomalies, and formulate hypotheses for further investigation.\n",
    "\n",
    "1. Understand the Data Structure: EDA helps to understand the dataset's dimensions (number of rows and columns), data types, and basic information about each variable.\n",
    "2. Detect Anomalies: Identify missing values, outliers, or inconsistencies in the dataset that could affect analysis or modeling.\n",
    "3. Explore Relationships: Investigate relationships and dependencies between variables to inform subsequent analysis or modeling decisions.\n",
    "4. Test Assumptions: Validate assumptions about the data distribution, correlations, or other statistical properties.\n",
    "5. Extract Insights: Use visual methods and statistical summaries to extract meaningful insights and patterns from the data.\n",
    "\n",
    "Steps in Performing EDA:\n",
    "1. Data Collection: Gather the dataset from various sources, such as databases, files, or APIs.\n",
    "2. Data Cleaning: Handle missing values, remove duplicates, and standardize data formats to ensure data quality.\n",
    "3. Exploratory Analysis: Explore each variable individually (univariate analysis) and relationships between variables (bivariate/multivariate analysis).\n",
    "4. Visualization: Create visual representations of data using plots and charts to gain insights quickly and effectively.\n",
    "5. Statistical Testing: Perform statistical tests (e.g., hypothesis testing, correlation analysis) to validate findings or test hypotheses.\n",
    "6. Interpretation and Reporting: Summarize findings, draw conclusions, and present insights to stakeholders or decision-makers.\n",
    "\n",
    "Techniques Used in EDA :  EDA involves a variety of techniques and tools to achieve its goals:\n",
    "1. Summary Statistics: Compute descriptive statistics such as mean, median, mode, variance, and standard deviation to summarize the central tendency and dispersion of data.\n",
    "2. Data Visualization: Use plots and charts (e.g., histograms, box plots, scatter plots, heatmaps) to visually explore distributions, trends, and relationships within the dataset.\n",
    "3. Correlation Analysis: Calculate correlation coefficients (e.g., Pearson, Spearman) to quantify the strength and direction of relationships between pairs of variables.\n",
    "4. Dimensionality Reduction: Apply techniques like Principal Component Analysis (PCA) or t-SNE to reduce the number of variables while preserving essential information.\n",
    "5. Feature Engineering: Create new features or transform existing features to enhance model performance or improve interpretability.\n",
    "\n",
    "Automated EDA with Pandas-Profiling : Pandas-Profiling is a powerful library that provides an easy way to generate comprehensive reports on data analysis.These records includes descriptive statistics,correlation matrix and missing value analysis among other useful insights.\n",
    "\n",
    "Automated EDA with Autoviz Library : It is a python library designed for automated visualization of datasets.It is perficularly useful for quickly generating a variety of visualization during EDA\n",
    "\n",
    "Automated EDA with Sweetviz library : Sweetviz is another python librry designed for automated EDA,similer to autoviz.It generates detailed and informative visualization and statistical summeries of your dataset with minimal code\n",
    "\n",
    "Automated EDA with Dtale library : D-Tale is a powerful library in Python that provides a visual interface for exploring and analyzing Pandas DataFrames. It combines the capabilities of a DataFrame viewer and an analysis tool, allowing for an interactive and intuitive exploration of datasets. Here‚Äôs a guide on how to use D-Tale for automated exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd6239f-c993-4e47-bb22-1b073c008116",
   "metadata": {},
   "source": [
    "Data Visualization : Data visualization in data science is the graphical representation of information and data. It involves creating visual elements such as charts, graphs, maps, and plots to help understand and interpret complex data sets more easily.\n",
    "\n",
    "1. Simplifies Complex Data: Visualizations can simplify large and complex data sets, making it easier to grasp patterns, trends, and outliers.\n",
    "2. Enhances Insight: Effective visualizations can reveal insights that might be missed in raw data, helping to make data-driven decisions.\n",
    "3. Facilitates Communication: Visualizations make it easier to communicate findings to stakeholders who might not have a technical background.\n",
    "4. Supports Exploration: Interactive visualizations allow users to explore data dynamically, adjusting parameters and drilling down into details.\n",
    "\n",
    "Common types of data visualizations include:\n",
    "1. Bar Charts: Useful for comparing quantities across different categories.\n",
    "2. Line Charts: Ideal for showing trends over time.\n",
    "3. Pie Charts: Used to illustrate proportions of a whole.\n",
    "4. Histograms: Useful for displaying the distribution of a data set.\n",
    "5. Scatter Plots: Help to identify relationships and correlations between variables.\n",
    "6. Heatmaps: Show the intensity of values across a matrix.\n",
    "7. KDE : kernel density estimate used to check the distribution of data \n",
    "8. Box Plots: Useful for summarizing the distribution and identifying outliers.\n",
    "Tools for data visualization include libraries and platforms such as Matplotlib, Seaborn, Plotly, Tableau, and Power BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c25a6-65c7-459a-a29a-0575c2c32fcb",
   "metadata": {},
   "source": [
    "Data Science : Data science is the study of data to extract meaning full insights for business purpose \n",
    "Data science is an interdisciplinary field that combines statistical analysis,machine learning & domain knowledge to extract insights and knowledge from structured and unstructured data unabling informed decision making & predictive analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52294b-653f-4713-bce1-a864b8584504",
   "metadata": {},
   "source": [
    "AI : Artificical Intelligence is a smart application which performs tasks without human interaction.\n",
    "Ex; Robots,Alexa,Recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55c995-48fd-4fc9-91de-33ff5e0cc08f",
   "metadata": {},
   "source": [
    "ML : Machine learning is a subset of AI that involves training algorithm to recognize patterns and make predictions based on data.It provides statistical tool to analyze,visualize,predict model and forecasting \n",
    "Ex;Amazon recommendation system .House price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f105148-ca11-4ed7-836d-2f36462e6f39",
   "metadata": {},
   "source": [
    "DL : Deep Learning is a subset of ML that artificial neural networks with many layers to model and understand complex pattern and relationship in large dataset.\n",
    "Ex;Image recognition,Face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecf561-3e7d-4c99-83b9-6ba2ef431411",
   "metadata": {},
   "source": [
    "Type's of Machine Learning :\n",
    "\n",
    "1. supervised machine learning :It is an algorithm which involves training model based on labeled data input and output feature extraction to predict new output or new value \n",
    "Ex ; House price prediction , Student result analysis\n",
    "\n",
    "2. Unsupervised machine learning : It is an algorithm which involves training model based on unlabeled data to group similar data to make clusters \n",
    "Ex; customer segmentation, Salary expectations\n",
    "\n",
    "3. Reinforcement Learning : It is a machine learning algorithm paradigm where an agent learns to make decisions by interacting with environment.\n",
    "Ex; Teaching Robot to navigate ,Mobile games "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0e63e-4367-47a5-9be2-534f9d707966",
   "metadata": {},
   "source": [
    "Type's of Supervised machine learning :\n",
    "\n",
    "1. Classification algorithm  : It is a supervised machine learning algorithm the goal is to predict categorical value for given input features or output features which involves categorical value i.e Binary form (0,1) \n",
    "Ex : Pass or Fail,success or Fail ,0 or 1 \n",
    "\n",
    "2. Regression algorithm : It is a supervised machine learning algorithm the goal is to predict numerical value for given input features or output features which involves numerical value.\n",
    "Ex : House price prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e35790-a11f-4ee1-b25c-e535f59ee864",
   "metadata": {},
   "source": [
    "Feature Engineering : Feature Engineering is the process of transforming raw data into features (input variables) that can be used by machine learning algorithms to improve model performance. It involves creating new features, modifying existing ones, and selecting the most relevant features to enhance the predictive power of a model.\n",
    "\n",
    "Importance of Feature Engineering:\n",
    "1. Improve model performance\n",
    "2. handle complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb49192-f34f-4835-b171-13af0628367b",
   "metadata": {},
   "source": [
    "Feature Encoding : Data Encoding coverting categorical data to numerical data\n",
    "Different type of feature encoding \n",
    "1. One-Hot-Encoding : Creating binary columns for each category(0,1). # get dummies # imputer\n",
    "2. Label Encoding : Assigning a unique integer to each category # LabelEncoder\n",
    "3. Ordinal Encoding :  Assigning integers to categories with a natural order. (Education level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b58940-444c-4875-afe5-9ca18bf5b744",
   "metadata": {},
   "source": [
    "Feature Transformation/ Scaling :  Scaling features to have a specific range or distribution, which can help algorithms converge faster.\n",
    "\n",
    "Standardization : It is a feature scaling  data preprocesing technique used in ml and statistics,it is used when the feature of the dataset are on different scale to bring them to a common scale.It transforms the data to have mean of 0 and std of 1.here the values are ranges from -1 to +1 using z_score technique.The algorithm assumes that the data is normally distributed(Linear,logistic,SVM)\n",
    "\n",
    "z_score=x-mean/std\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use standardization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms: Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Normalization : It is a feature scaling  data preprocessing technique used in ml and statistics.It rescales the values of feature to range of (0,1) as min-max-scaling.Normalization is used when you want to ensure that the data fits within a specific range,particulerly in algorithm that do not make assumption about the distribution of data.It is especially useful when features in a dataset have different units or scales. Normalization ensures that each feature contributes equally to the model and can improve the performance of algorithms that are sensitive to feature scaling.\n",
    "\n",
    "x=(x-min(x))/(max(x)-min(x))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data=scaler.fit_transform(data)\n",
    "\n",
    "When to use Normalization :\n",
    "1. Gradient Descent-Based Algorithms: Algorithms like linear regression, logistic regression, and neural networks.\n",
    "2. Distance-Based Algorithms:Algorithms like k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "3. Algorithms Sensitive to Scale: Algorithms like k-nearest neighbors (KNN), support vector machines (SVM), and gradient descent-based methods benefit from normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822a725-e222-43eb-8413-f447422aec77",
   "metadata": {},
   "source": [
    "Feature Extraction : Dimensionality Reduction technique: Reducing the number of features while retaining the essential information.\n",
    "PCA : Principle Component Analysis is a statistical technique used for dimensionality reduction in data analysis and machine learning.which is used to transform a high-dimentional dataset to lower dimentional without retailing its original data.\n",
    "\n",
    "Application of PCA :\n",
    "1. Dimensionality Reduction: Reduces the number of variables in high-dimensional datasets while retaining as much variance as possible.\n",
    "2. Data Visualization: Visualizes high-dimensional data in lower-dimensional space (often 2D or 3D) to explore patterns and relationships.\n",
    "3. Noise Filtering: PCA can filter out noise by retaining only the principal components with significant eigenvalues.\n",
    "4. Feature Extraction: PCA can be used as a feature extraction technique for machine learning algorithms by transforming high-dimensional features into a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e202699-fde2-413c-8e36-4312867fde7a",
   "metadata": {},
   "source": [
    "Spliting Data : spliting data r dividing data dependent and independent variable\n",
    "x=data[] # independent variable , feature input ,predictor \n",
    "y=data[] # dependent variable ,target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8f8ac-218b-4fe0-8891-0eeb2a2878f8",
   "metadata": {},
   "source": [
    "Dataset is divided into 2 types:\n",
    "1. Training data # minimum 70%-80%\n",
    "2. Testing data # 20%-30%\n",
    "\n",
    "In training data its 2 type's:\n",
    "1. Training data \n",
    "2. Validation data (Cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb3a8b-ae71-450d-90ea-41b4b32de94d",
   "metadata": {},
   "source": [
    "Training data : It is used by ml algorithm to build the model and learn relationship between input and output features.\n",
    "training data size should be minimum 70% of data.\n",
    "\n",
    "Testing data : It is separate portion of data which is used to evaluate the performance of ml model which is trained on training data.\n",
    "testing data size should be minimum 20% of data.\n",
    "\n",
    "Validation data : It is subset of training data which is used to evaluate the performance of ml model during the training process.\n",
    "\n",
    "Cross validation : It is statistical technique used to evaluate the performance of machine learning model by partitioning the dataset into subsets,training model on some subsets and testing model on other subsets.\n",
    "\n",
    "Advantages : to get better accuracy and build model.\n",
    "\n",
    "Disadvantages : \n",
    "->Time complexity is huge for training big dataset\n",
    "->Model overfit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead553f9-d1b5-4d68-9964-5f4d0cb9a01f",
   "metadata": {},
   "source": [
    "1. Variance :It refers to the change in model when using different portion on training r testing data.Good accuracy low variance and bad accuracy high variance.It can lead to overfitting.\n",
    "2. Bias : Bias refers to the error introduced by approximating a real-world problem with a simplified model that doesn't fully capture the complexity of the data.if the low accuracy it has high bias and high accuracy low bias.it can lead to underfitting\n",
    "3. low bias algorithm : Decision tree,knn,and SVM\n",
    "4. high bias algorithm : Linear regression,logistic regression.\n",
    "\n",
    "Variance-Bias Tradeoff: Balancing model complexity to achieve a good tradeoff between bias and variance. The goal is to find a model that generalizes well to new data by minimizing both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd2b31-70a0-4b83-b689-ba7ed2d81d5e",
   "metadata": {},
   "source": [
    "Overfitting : When the model performs on training data but fails to perform on test data than overfitting occurs.trainig accuracy high and testing accuracy low \n",
    "1. The difference between actual and predicted value is less.\n",
    "2. Low bias\n",
    "3. High accuracy/variance\n",
    "4. training accuracy 90% and testing accuracy 70%\n",
    "Due to high number of datapoints and feature's\n",
    "\n",
    "Underfitting : When the model not able to perform on both training and testing data than underfitting occurs. both training and testing accuracy will b low.\n",
    "1. The difference between actual and predicted value is more.\n",
    "2. High bias\n",
    "3. Low accuracy/variance\n",
    "4. training accuracy 70% and testing accuracy 50%\n",
    "Due to less number of datapoints and feature's\n",
    "\n",
    "Normalized model : when the model performs on both trainig and testing data.\n",
    "low variance and low bias,training and testing accuracy willl be similer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbe448-39b7-47f2-8706-885fc2332d34",
   "metadata": {},
   "source": [
    "Ridge Regression : regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients.adds a penalty equal to the sum of the squared values of the coefficients to the ordinary least squares (OLS) cost function.L2 regularization. ridge = Ridge(alpha=1.0)\n",
    "\n",
    "Lasso Regression :regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients.regularization techniques used to prevent overfitting in linear regression models by adding a penalty to the regression coefficients. lasso = Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8dc0e1-7d06-41fe-ae15-e2a7d64bca82",
   "metadata": {},
   "source": [
    "Hyper Parameter Tunning : It is variable of ML algorithm that evaluate the performance of ML model is good r not while training dataset.  \n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to maximize its performance. Hyperparameters are different from model parameters; they are set before the training process and are not learned from the data.\n",
    "\n",
    "Techniques for Hyperparameter Tuning\n",
    "\n",
    "1. Grid Search:Grid search + Cross validation : it is a technique used in ml to find the optimal hyper parameter for a model\n",
    "->Exhaustively searches over a specified parameter grid.\n",
    "->Evaluates all possible combinations of hyperparameters.\n",
    "->Can be computationally expensive, especially with many hyperparameters and large datasets.\n",
    "\n",
    "2. Random Search:\n",
    "->Samples a fixed number of hyperparameter combinations from the specified parameter space randomly.\n",
    "->Often more efficient than grid search since it explores a larger area of the parameter space with fewer iterations.\n",
    "\n",
    "3. Bayesian Optimization:\n",
    "->Uses probabilistic models to predict the performance of hyperparameters.\n",
    "->Balances exploration of new hyperparameter values with exploitation of known good values.\n",
    "->Efficient for expensive and high-dimensional hyperparameter tuning.\n",
    "\n",
    "4. Gradient-based Optimization:\n",
    "->Uses gradient information to iteratively improve hyperparameters.\n",
    "->Suitable for differentiable hyperparameters.\n",
    "\n",
    "5. Evolutionary Algorithms:\n",
    "->Uses principles from evolutionary biology, such as mutation and selection, to evolve hyperparameters over successive generations.\n",
    "->Useful for complex and high-dimensional hyperparameter spaces.\n",
    "\n",
    "6. Automated Machine Learning (AutoML):\n",
    "->Tools and libraries that automate the hyperparameter tuning process (e.g., Google AutoML, AutoKeras, H2O.ai).\n",
    "->Often use a combination of the above techniques.\n",
    "\n",
    "Advantage's : \n",
    "1. Improved Model Performance:\n",
    "->Optimal Parameters: Finding the best hyperparameters can significantly enhance model accuracy, precision, recall, or other performance metrics.\n",
    "->Generalization: Proper tuning can improve a model's ability to generalize to unseen data, reducing overfitting or underfitting.\n",
    "->To reduce overfitting and get better accuracy.\n",
    "\n",
    "2. Better Feature Utilization:\n",
    "->Feature Selection: Techniques like Lasso regression can automatically select important features during hyperparameter tuning, simplifying the model.\n",
    "->Dimensionality Reduction: Some hyperparameters can control the complexity of the model, effectively reducing dimensionality.\n",
    "\n",
    "Disadvantage's :\n",
    "1. Complexity:\n",
    "->Hyperparameter Space: The space of possible hyperparameters can be vast and complex, making the search for optimal values challenging.\n",
    "->Time-Consuming: The process can be lengthy, especially if the dataset is large or the model is complex.\n",
    "->Time complexity.\n",
    "\n",
    "2. Risk of Overfitting:\n",
    "->Validation Set Overfitting: Overly aggressive tuning can lead to models that are too finely tuned to the validation set, reducing their ability to generalize to truly unseen data.\n",
    "->Bias: Repeatedly evaluating hyperparameters on the same validation set can introduce bias.\n",
    "\n",
    "3. Data Dependency:\n",
    "->Dataset Specificity: Hyperparameters optimized for one dataset may not perform well on a different dataset, limiting the transferability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2801583-2f42-4881-91f3-e992c6b2b973",
   "metadata": {},
   "source": [
    "Ensemble Technique : \n",
    "Ensemble models are techniques in machine learning where multiple models (often referred to as \"weak learners\" or \"base models\") are combined to produce a single predictive model. The idea is that by combining the predictions of several models, the ensemble can achieve better performance and robustness compared to any individual model.\n",
    "\n",
    "Type's of Ensemble technique:\n",
    "\n",
    "1. Bagging technique : It combines multiple models to create a stronger overall model.Bagging involves training multiple versions of the same model on different subsets of the training data (created using bootstrap sampling), and then averaging their predictions (for regression) or using a majority vote (for classification).\n",
    "Ex : Example: Random Forest, which is an ensemble of decision trees.\n",
    "\n",
    "2. Boosting technique : Boosting is a machine learning technique designed to improve the performance of models by combining the predictions of multiple weak learners to create a strong learner. The primary goal of boosting is to improve the predictive accuracy of model by sequentially adding models that corrects the errors made by previous errors.\n",
    "Weak learners: Haven't learnt much from the training dataset.\n",
    "Ex : AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f5885-3a48-4fff-acc2-316c26ab9305",
   "metadata": {},
   "source": [
    "For Regression metrics:\n",
    "1. Mean_squared_Error(MSE)\n",
    "2. Root mean squared error(RMSE)\n",
    "3. Mean absolute error\n",
    "4. R squared\n",
    "5. Adjusted R squared\n",
    "\n",
    "For Classification metrics:\n",
    "1. Accuracy score\n",
    "2. Confusion matrix\n",
    "3. Classification_report\n",
    "4. Precision\n",
    "5. Recall\n",
    "6. F1-score\n",
    "7. support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8eb8f5-db56-4893-b7de-1b8834196960",
   "metadata": {},
   "source": [
    "1. Mean Squared Error (MSE): MSE measures the average of the squares of the errors, which are the differences between the actual and predicted values.Lower MSE values indicate better model performance. MSE gives a higher penalty for large errors.\n",
    "\n",
    "2.  Root mean squared error(RMSE):  RMSE is the square root of MSE, providing a measure of the average magnitude of the error.Like MSE, lower RMSE values indicate better model performance. RMSE is in the same units as the target variable, making it easier to interpret.\n",
    "\n",
    "3.  Mean Absolute Error :MAE measures the average of the absolute differences between actual and predicted values.Lower MAE values indicate better model performance. MAE is less sensitive to outliers compared to MSE and RMSE.\n",
    "   \n",
    "4.  R-Squared : R¬≤ measures the proportion of the variance in the dependent variable that is predictable from the independent variables.R¬≤ values range from 0 to 1, with higher values indicating a better fit. An R¬≤ value of 1 means the model explains all the variability in the response data.1-(sum of squared residual (error)/sum of square total)\n",
    "  \n",
    "5.  Adjusted R-Squared: Adjusted R¬≤ adjusts the R¬≤ value based on the number of predictors in the model, penalizing the addition of irrelevant predictors.Adjusted R¬≤ provides a more accurate measure of model performance, especially when comparing models with a different number of predictors.\n",
    "\n",
    "6.  Accuracy score : Accuracy is the ratio of correctly predicted instances to the total instances.Higher accuracy indicates better model performance. However, it can be misleading in imbalanced datasets.\n",
    "   accuracy score= number of correct predictions/Total number of predictions\n",
    "\n",
    "7. Confusion matrix :  A confusion matrix is a table that is often used to describe the performance of a classification model by showing the true positives, true negatives, false positives, and false negatives.Provides detailed insights into the model's performance, showing where the model is making errors.\n",
    "   confusion matrix=   +ve  -ve\n",
    "                   +ve  TP   FP\n",
    "                   -ve  FN   TN\n",
    "   \n",
    "9. Classification Report : The classification report provides a summary of the precision, recall, F1-score, and support for each class.\n",
    "It is a tool used to evaluate the performance of classification model.It provides the summary of various metrics that help in accesing how well the model is performing in terms of classification.\n",
    "\n",
    "10. Precision : It is the ratio of correctly predicted positive prediction to the total number of positive prediction.Measures the accuracy of positive predictions.\n",
    "   Precision=TP/TP+FP\n",
    "\n",
    "11. Recall :  It is the ratio of correctly predicted positive observations to the all observations in the actual class..Measures the ability to correctly identify positive instances.\n",
    "    Recall=TP/TP+FN\n",
    "\n",
    "12. F1 score:The F1 Score is the harmonic mean of Precision and Recall. It combines both metrics into a single score that balances the two.\n",
    "    F1-Score=2√ó (Precision*Recall/Precision+Recall)\n",
    "‚Äã\n",
    "13. Support: The number of actual occurrences of each class in the dataset.\n",
    "\n",
    "14. ROC AUC Curve : The ROC AUC (Receiver Operating Characteristic Area Under the Curve) curve is a fundamental tool in evaluating the performance of binary classification models. It provides insight into the model's ability to distinguish between two classes\n",
    "\n",
    "15. ROC : Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It is created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings\n",
    "           TPR=TP/TP+FN\n",
    "           FPR=FP/FP+TN\n",
    "16. AUC :  AUC (Area Under the Curve): The AUC represents the area under the ROC curve. It quantifies the overall ability of the model to discriminate between the positive and negative classes.\n",
    "\n",
    "17. Specificity : Specificity is a performance metric used in binary classification to measure the proportion of actual negatives that are correctly identified by the model. It is particularly useful in evaluating the effectiveness of a classifier in distinguishing between the negative class and other outcomes.\n",
    "       Specificity = TN/TN+FP\n",
    "\n",
    "18. Log-Loss : Log Loss, also known as Logarithmic Loss or Logistic Loss, is a performance metric used to evaluate the performance of classification models, particularly for binary classification. It measures the accuracy of a classifier by penalizing incorrect predictions with a higher weight as the prediction probability deviates from the actual class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74ee20-7f40-473c-8f1f-7f08a808bc37",
   "metadata": {},
   "source": [
    "MACHINE LEARNING ALGORIHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd0a67-c1dc-4d62-b426-74dff527b646",
   "metadata": {},
   "source": [
    "Classification Algorithm : output as categorical value\n",
    "1. Logistic Regression \n",
    "2. Decision tree   \n",
    "3. Random Forest \n",
    "4. Support vector classifier\n",
    "5. KNN\n",
    "6. Naive Bayes \n",
    "7. Neural networks\n",
    "8. Gradient Boosting machines\n",
    "9. Adaboost\n",
    "10. XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c92c9-d924-41c0-88ed-46786d7ad1e1",
   "metadata": {},
   "source": [
    "Regression Algorithm : output as numerical value \n",
    "1. Linear Regression \n",
    "2. Ridge Regression\n",
    "3. Lasso Regression\n",
    "4. Elastic Net \n",
    "5. Decision tree\n",
    "6. Random Forest\n",
    "7. Support vector Regression\n",
    "8. KNN\n",
    "9. Neural networks\n",
    "10. Gradient Boostinf machines\n",
    "11. Adaboost\n",
    "12. XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb19d8-15a9-4524-b29f-584ccc60c16a",
   "metadata": {},
   "source": [
    "SUPERVISED MACHINE LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f8798-1a7b-4aaa-96e3-0c2b8b8ebefc",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION : It is a supervised machine learning regression algorithm which defines the relationship between dependent and independent variable.It is data analysis technique used to predict value of unknown data using known data.\n",
    "\n",
    "1. simple linear gression : It is a model that describes the relationship between one dependent and independent variable .\n",
    "2. Multilinear regression : It is a model that describes the relationship between one dependent and multiple independent variable.\n",
    "3. Polynomial regression : It is the form of linear regression where the relationship between independent variable X and dependent variable Y is modeled as nth degree polynomial.\n",
    "\n",
    "y=mx+c ( simple linear regression)\n",
    "y=mx+m1x1+m2x2+...C ( multilinear regression)\n",
    "\n",
    "y=dependent variable \n",
    "x=independent variable \n",
    "m=slope of the model (coefficient)\n",
    "c=Intercept\n",
    "\n",
    "The main aim of linear regression is to find the best fit line in such a way that the actual and predicted value should be minimal(less).\n",
    "\n",
    "Multicollinearity : Multicollinearity in linear regression refers to a situation where two or more predictor variables (independent variables) are highly correlated, leading to redundancy in the predictor variables. This can cause problems in the regression analysis, making it difficult to determine the individual effect of each predictor on the dependent variable.\n",
    "\n",
    "\n",
    "Application : \n",
    "1. Health care analysis\n",
    "2. house price prediction \n",
    "3. marketing\n",
    "4. stock prediction \n",
    "\n",
    "Advantage's : \n",
    "1. Simplicity:\n",
    "->Easy to understand and implement.\n",
    "->Computationally efficient, even for large datasets.\n",
    "->scales to large dataset\n",
    "2. Interpretability:\n",
    "->Coefficients provide insights into the relationship between variables.\n",
    "->Interpretable model outputs.\n",
    "3. Quick to Train:\n",
    "->Training linear regression models is fast.\n",
    "4. Baseline Model:\n",
    "->Serves as a good starting point for more complex models.\n",
    "''''''''''''''''''\n",
    "Disadvantage's: \n",
    "1. Linearity Assumption:\n",
    "->Only models linear relationships. Not suitable for non-linear data.\n",
    "->Needs more feature Engineering \n",
    "->If the true relationship is not linear then it not provides the accurate predictions.\n",
    "2. Sensitivity to Outliers:\n",
    "->Outliers can significantly affect the model.\n",
    "->Overfitting with too many variable.\n",
    "3. Multicollinearity:\n",
    "->High correlation between independent variables can lead to unreliable coefficient estimates.\n",
    "4. Homoscedasticity:\n",
    "->Assumes constant variance of errors, which may not hold in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e6b72-ddf0-4ac6-b755-7c8a96cfa892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e00b8b5a-38be-4707-b037-cdf78ac7de61",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION : It is a supervised machine learning classification algorithm and it is a statistical technique used for binary classification task.The goal is to predict probability of a  categorical value that have two possible value or outcomes(0,1)(True,False)\n",
    "\n",
    "->Logistic regression uses the sigmoid function to map predicted values to probabilities between 0 and 1.(1/1+e to the power of -z)\n",
    "\n",
    "Application :\n",
    "1. Disease Prediction: Predicting the presence or absence of diseases based on patient characteristics, medical history, and diagnostic test results.\n",
    "2. Medical Diagnostics: Classifying medical conditions such as diabetes, hypertension, or heart disease based on symptoms and medical tests.\n",
    "3. Fraud Detection: Identifying fraudulent transactions or activities based on transactional data, user behavior, and historical patterns.\n",
    "4. Risk Management: Predicting the risk associated with investments, insurance claims, or financial decisions based on market data and economic indicators.\n",
    "5. Student Success Prediction: Predicting academic performance and success factors for students based on demographic information, educational background, and previous academic achievements.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity and Interpretability:\n",
    "->Easy to understand and interpret the relationship between features and the probability of the outcome.\n",
    "2. Efficiency:\n",
    "->Computationally efficient and fast to train even with large datasets.\n",
    "3. Probabilistic Output:\n",
    "->Provides probabilities for class membership, which can be useful for decision-making processes.\n",
    "4. Feature Importance:\n",
    "->Coefficients indicate the importance and direction of influence of each feature.\n",
    "5. Handles Binary Classification Well:\n",
    "->Well-suited for binary classification problems and can be extended to multi-class classification using techniques like One-vs-Rest (OvR).\n",
    "\n",
    "Disadvantage's:\n",
    "1. Requires Large Sample Size:\n",
    "->Requires a relatively large sample size for stable and reliable estimates.\n",
    "2. Not Suitable for Non-linear Problems:\n",
    "->Struggles with non-linear relationships unless features are transformed.It may not perform well if the relationship is not linear\n",
    "3. Sensitive to Outliers:\n",
    "->Can be sensitive to outliers, which may affect the coefficients.\n",
    "4. Overfitting with high dimentionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632ea98-0c76-43d4-8180-7d9358fc8e85",
   "metadata": {},
   "source": [
    "Logistic regression is classification problem which gives categorical  value as outcome were Linear regression is regression problem which gives continuous value as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6808b-c0e9-40f1-bf50-47a7b3715153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "093227f9-acd3-4084-a681-be1a37022cb9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f356c-1584-4bff-b3f1-2fcccd1e8875",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE : It is a supervised machine learning algorithm used for both classification and regression tasks.SVM works by finding the hyperplane that best separates the classes in the feature space. One of the key features of SVM is the use of kernel functions, which enable SVM to perform efficiently in high-dimensional spaces and to handle non-linear decision boundaries..separates best fit line with 2 marginal plane.\n",
    "\n",
    "kernel function : a kernel function that enables the algorithm to operate in a high-dimentional implicit features space without having to compute the co-ordinates of the data in that space explicity.\n",
    "1. Linear kernel : The simplest kernel function, used when the data is linearly separable r when we have large number of features.\n",
    "2. Polynomial function : Allows learning of non-linear models by mapping the original features into polynomial feature space.suitable for dataset where iteration between features are significant.\n",
    "3. RBF kernel(Radial Basis Function): Where we have no prior knowleddge about data,when the data is not linearly separable, and you want to capture the non-linear relationships. This is the default and often the most effective kernel.\n",
    "4. Sigmoid kernel : Can be used as a proxy for neural networks.It is less common and typically used in specific scenarios related to neural network models.\n",
    "\n",
    "Application : \n",
    "1. Stock Market Prediction: Classifying stock price movements based on historical data and other financial indicators.\n",
    "2. Spam Detection: Classifying emails as spam or non-spam.\n",
    "3. Sentiment Analysis: Determining the sentiment of a text as positive, negative, or neutral.\n",
    "4. Document Classification: Categorizing documents into predefined classes (e.g., news articles, scientific papers).\n",
    "5. Disease Diagnosis: Predicting the presence or absence of diseases based on patient data \n",
    "6. Face Detection: Identifying and locating human faces in images.\n",
    "\n",
    "Advantage's:\n",
    "1. Effective in High-Dimensional Spaces:\n",
    "->SVMs perform well in high-dimensional spaces and are effective even when the number of dimensions is greater than the number of samples.\n",
    "2. Versatility with Different Kernels:\n",
    "->SVMs support various kernel functions (linear, polynomial, RBF, sigmoid), enabling them to handle both linear and non-linear relationships.\n",
    "3. Robustness to Overfitting:\n",
    "->With the appropriate choice of kernel and regularization parameters, SVMs are less prone to overfitting, especially in high-dimensional space.\n",
    "4. Clear Margin of Separation:\n",
    "->SVMs maximize the margin between the classes, which can lead to better generalization and performance on unseen data.\n",
    "5. Good Performance with Small and Medium-Sized Datasets:\n",
    "->SVMs can be effective when the dataset is small to medium-sized, where other algorithms might not perform as well.\n",
    "\n",
    "Disadvantage's :\n",
    "1. Computational Complexity:\n",
    "->Training an SVM can be computationally intensive, especially with large datasets. The time complexity can be significant, particularly with non-linear kernels.\n",
    "2. Memory Usage:\n",
    "->SVMs can require large amounts of memory for storing the support vectors and other parameters, especially in high-dimensional spaces.\n",
    "3. Choice of Kernel and Parameters:\n",
    "->The performance of SVMs heavily depends on the choice of kernel function and its parameters (e.g., the regularization parameter C and kernel-specific parameters). Finding the optimal parameters can be challenging and time-consuming.\n",
    "4. Not Suitable for Large Datasets:\n",
    "->SVMs are not well-suited for very large datasets due to their computational and memory requirements.\n",
    "5. Interpretability:\n",
    "->SVM models can be less interpretable compared to other models like decision trees or linear regression, particularly when using complex kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1125ac9-87bc-478f-8cb2-374d4f66659e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89538407-2e41-4900-80f8-5771cb2d77c2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bae74-0e2d-449c-94f7-fb79babbd289",
   "metadata": {},
   "source": [
    "KNN: K-Nearest Neighbors : It is a instance based supervised machine learning algorithm used for both classification as well as regression tasks.It operates on the principle that similar instances exist in close proximity to each other.\n",
    "\n",
    "Instance-Based Learning: KNN is a type of instance-based learning, meaning it memorizes the training dataset rather than learning a set of parameters. Predictions are made based on the stored instances from the training set.\n",
    "\n",
    "It is called Lazy-model bcz algorithms that involve a distinct training phase where the model is constructed and parameters are learned, KNN does not build an explicit model beforehand. Instead, it stores the entire training dataset and performs computations only when predictions are required.which works under only testing phase not in training phase\n",
    "\n",
    "How KNN works\n",
    "1. Store the Dataset: KNN requires the entire training dataset to be stored. Unlike many algorithms that build a model, KNN does not involve a training phase in the conventional sense. Instead, it keeps the entire dataset in memory.\n",
    "2. Choose the Number of Neighbors (k): Before making predictions, you need to decide on the number of neighbors to consider, denoted as \n",
    "ùëò.This parameter determines how many of the closest data points will influence the prediction for a new data point.\n",
    "3. Compute Distances: To make a prediction for a new data point, the algorithm first calculates the distance between this new data point and all points in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance, or Minkowski distance.\n",
    "->Euclidean distance : The straight line distance bw two points.\n",
    "->Manhattan distance : The sum of absolute difference between the co-ordinates of the points.\n",
    "->Minkowski Distance : Generalization of both Euclidean and Manhattan distances.\n",
    "4. Find Nearest Neighbors: After calculating the distances, KNN identifies the \n",
    "ùëò training examples that are closest to the new data point. These are the nearest neighbors.\n",
    "5. Make a Prediction:\n",
    "For Classification: KNN classifies the new data point by a majority vote among the k nearest neighbors. The class that appears most frequently among these neighbors is assigned to the new data point.\n",
    "For Regression: KNN predicts the value for the new data point by averaging the values of the \n",
    "ùëò nearest neighbors.\n",
    "6. Output the Result: The algorithm outputs the predicted class label (for classification) or the predicted value (for regression) based on the K nearest neighbors.\n",
    "\n",
    "Application : \n",
    "1. Recommendation Systems: Used in collaborative filtering to recommend items based on the preferences of similar users.\n",
    "2. Image Recognition: Classifying images based on the similarity to other images in the dataset.\n",
    "3. Anomaly Detection: Identifying outliers in data by comparing the distance to neighboring points.\n",
    "4. Pattern Recognition: Handwriting recognition, speech recognition, and other pattern classification tasks.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity: Easy to understand and implement.\n",
    "2. No Training Phase: k-NN is a lazy learner, meaning it doesn‚Äôt require a training phase. All computation is deferred until classification.\n",
    "3. Adaptability: Works well with multi-class classification problems.\n",
    "4. Versatility: Can be used for both classification and regression tasks.\n",
    "\n",
    "Disdvantage's :\n",
    "1. Computationally Intensive: The algorithm needs to calculate the distance between the sample and all other samples in the training set, making it slow for large datasets.\n",
    "2. Memory Intensive: Requires storing all training data.\n",
    "3. Sensitive to Irrelevant Features: All features contribute equally to the distance metric, so irrelevant features can negatively impact performance.\n",
    "4. Curse of Dimensionality: Performance can degrade with high-dimensional data due to the sparsity of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd033a0-d5a2-498b-bf3a-2b8dc8889db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4108cc3-c13b-4d2d-ab8e-75ee2705d60b",
   "metadata": {},
   "source": [
    "DECISION TREE : It is a supervised machine learning algorithm  used for both classification and regression tasks.A decision tree is a graphical representation used for decision-making and classification. It consists of nodes, branches, and leaves that collectively form a tree-like structure. It operates by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\n",
    "\n",
    "->pre-prunning(smaller dataset) and post prunning(Large dataset)are used to reduce overfitting by hyper parameter (max_depth,max_feature).\n",
    "\n",
    "Tree Structure: A decision tree consists of nodes. There are three types of nodes:\n",
    "->Root Node: Represents the entire dataset and is the starting point of the tree.\n",
    "->Internal Nodes: Represent the features and make decisions based on feature values.\n",
    "->Leaf Nodes: Represent the outcome (class label or continuous value).\n",
    "\n",
    "Splitting Criteria: At each node, the algorithm selects the feature that best splits the data. Common criteria for splitting include:\n",
    "->Gini Impurity: Measures the frequency at which any element of the dataset would be misclassified if it was randomly labeled.\n",
    "-> Gain (Entropy): Measures the reduction in entropy or uncertainty after the dataset is split.\n",
    "->Mean Squared Error (for regression): Measures the average squared difference between observed actual outcomes and predicted outcomes.\n",
    "\n",
    "what features you need to select to start the split -Information Gain\n",
    "->When the dataset is small : Entropy have to select \n",
    "->when the dataset is large : Gini impurity have to select\n",
    "\n",
    "Application :\n",
    "1. Medical Diagnosis: Predicting diseases based on patient symptoms and medical history.\n",
    "2. Customer Segmentation: Segmenting customers based on purchase behavior for targeted marketing.\n",
    "3. Credit Scoring: Assessing the risk of lending money to potential borrowers.\n",
    "4. Fraud Detection: Identifying fraudulent transactions in banking and finance.\n",
    "5. Manufacturing: Quality control and predictive maintenance.\n",
    "\n",
    "Advantages:\n",
    "1. Interpretability: Decision trees are easy to understand and interpret. They can be visualized, making the decision-making process clear.\n",
    "2. No Feature Scaling Required: Decision trees do not require normalization or scaling of features.\n",
    "3. Handle Both Numerical and Categorical Data: Decision trees can handle both types of data without any transformation.\n",
    "4. Non-Parametric: They do not assume any underlying distribution of the data.\n",
    "                            \n",
    "Disadvantage's :\n",
    "1. Overfitting: Decision trees can create overly complex trees that do not generalize well to unseen data. Pruning and setting parameters like maximum depth can mitigate this.\n",
    "2. Instability: Small changes in the data can result in a completely different tree structure.\n",
    "3. Bias Toward Features with More Levels: Decision trees can be biased toward features with many levels, leading to splits that are not necessarily the best.\n",
    "4. Not Optimal for Linear Relationships: Decision trees are not well-suited for capturing linear relationships compared to other algorithms like linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489600a1-739c-4690-a42a-5c1522e63d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef5416ed-2cef-4a3c-87c0-8463ade467c4",
   "metadata": {},
   "source": [
    "NAIVE BAYES CLASSIFIER : Naive Bayes classifier is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, which describes the probability of an event based on prior knowledge of conditions related to the event. \n",
    " It's called \"naive\" because it assumes that the presence (or absence) of a particular feature in a class is unrelated to the presence (or absence) of any other feature.Bayes' Theorem calculates the probability of a hypothesis given prior knowledge and observed data. \n",
    " P(A/B)=P(A).P(B/A)/P(B)\n",
    " P(A‚à£B) ; Probability of event A given B true.\n",
    " P(A) ; Probability of event A\n",
    " P(B) ; Probability of event B\n",
    " P(B/A) ; Probability of event B given A true.\n",
    "\n",
    "->Types of Naive Bayes Classifiers\n",
    "1. Gaussian Naive Bayes: Assumes that the continuous values associated with each feature are distributed according to a Gaussian (normal) distribution.\n",
    "2. Multinomial Naive Bayes: Typically used for discrete counts (e.g., word frequencies in text classification).\n",
    "3. Bernoulli Naive Bayes: Used when features are binary (e.g., presence or absence of a feature).\n",
    "\n",
    "Application : \n",
    "1. Text Classification: Naive Bayes is widely used for spam filtering, sentiment analysis, and document classification.\n",
    "2. Medical Diagnosis: Predicts the probability of diseases based on symptoms.\n",
    "3. Email Spam Detection: Classifies emails as spam or not spam based on word frequency.\n",
    "4. Sentiment Analysis: Analyzes customer feedback or social media posts to determine sentiment.\n",
    "5. Recommender Systems: Provides recommendations based on user preferences and behavior.\n",
    "6. News Categorization: Classifies news articles into categories like sports, politics, technology, etc.\n",
    "\n",
    "Advantage's :\n",
    "1. Simplicity: Easy to understand and implement.\n",
    "2. Speed: Fast to train and predict, even with large datasets.\n",
    "3. Efficiency: Requires a small amount of training data to estimate the parameters.\n",
    "4. Scalability: Handles large numbers of features efficiently.\n",
    "5. Performance with High Dimensional Data: Performs well in high-dimensional settings such as text classification.\n",
    "6. Works Well with Small Datasets: Effective even with relatively small datasets.\n",
    "7. No Need for Complex Feature Engineering: Simple probability-based approach doesn't require complex transformations of features.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Independence Assumption: Assumes that features are independent, which is rarely true in real-world scenarios. This can affect accuracy.\n",
    "2. Zero Probability Issue: If a category in the dataset is not present during training, it assigns zero probability to it. Smoothing techniques like Laplace smoothing can mitigate this.\n",
    "3. Limited Expressiveness: The model's simplicity means it may not capture complex relationships in the data.\n",
    "4. Bias: Can be biased if the training data is not representative of the actual data distribution.\n",
    "5. Not Suitable for All Problems: Doesn't perform well on datasets with highly correlated features or where the independence assumption is significantly violated.\n",
    "6. Output Probability Interpretation: The predicted probabilities are not always reliable, especially if the independence assumption is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d6d0c-ff4c-43ac-a300-47d2e2f6bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "803b4f37-60bb-4653-baad-19b0d63b525d",
   "metadata": {},
   "source": [
    "RANDOM FOREST CLASSIFIER: It is supervised machine learning ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "->A random forest classifier is an ensemble learning method that combines multiple decision trees to improve the classification performance\n",
    "->It is perticularly effective for a variety of classification and regression tasks due to its ability to handle large dataset with higher dimensionality,deals with missing value and overfitting.\n",
    "\n",
    "How Random Forest Works\n",
    "1. Bootstrap Sampling: Randomly selects subsets of the training data (with replacement) to build multiple decision trees.\n",
    "2. Random Feature Selection: At each split in a tree, a random subset of features is chosen, and the best feature from this subset is -selected to split the node. This introduces diversity among the trees.\n",
    "3. Voting: For classification tasks, each tree in the forest votes, and the class with the majority vote is the final prediction. For regression tasks, the average prediction of all the trees is taken.\n",
    "\n",
    "Why should we use Random Forest Instead of Decision Tree\n",
    "1. To improve accuracy\n",
    "2. Reduce overfitting \n",
    "3. Better generalization\n",
    "4. Handling high dimensionality\n",
    "5. Generalized model\n",
    "\n",
    "OOB-Score: out of bag data score : The data which is missed to train during training that is oob_score.(validation data)\n",
    "\n",
    "Application :\n",
    "1. Financial Forecasting: Predicts stock prices, credit scoring, and other financial metrics.\n",
    "2. Medical Diagnosis: Assists in predicting disease presence based on patient data.\n",
    "3. Image Classification: Used in computer vision for tasks like object detection and recognition.\n",
    "4. Recommendation Systems: Recommends products based on user behavior and preferences.\n",
    "5. Biological Data Analysis: Analyzes genomic data and other biological datasets for research purposes.\n",
    "6. Fraud Detection: Identifies fraudulent transactions and activities in finance and other industries.\n",
    "7. customer Segmentation: Segments customers based on their purchasing behavior and other characteristics.\n",
    "\n",
    "Advantage's :\n",
    "1. High Accuracy: Tends to achieve high accuracy by combining the predictions of multiple trees, reducing overfitting.\n",
    "2. Robustness to Overfitting: The averaging of multiple trees helps prevent overfitting, especially when there are many trees.\n",
    "3. Handles Large Datasets: Can efficiently handle large datasets with higher dimensionality.\n",
    "4. Versatility: Applicable to both classification and regression problems.\n",
    "5. Feature Importance: Can provide estimates of feature importance, which helps in understanding the data.\n",
    "6. Handles Missing Values: Can handle missing data to some extent by using the median value for regression and the most common value for classification.\n",
    "7. Works Well with Non-linear Data: Capable of capturing non-linear patterns in the data.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Complexity: More complex and computationally intensive compared to simpler models like decision trees or linear regression.\n",
    "2. Slower Predictions: Making predictions can be slower as it involves aggregating results from multiple trees.\n",
    "3. Resource Intensive: Requires more memory and computational resources, especially with large forests and datasets.\n",
    "4. Interpretability: Harder to interpret compared to a single decision tree due to the aggregation of many trees.\n",
    "5. Overfitting on Noisy Data: While it reduces overfitting, if there is a lot of noise in the data, it can still overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46ab66-e705-495a-ad08-aa1e08547253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159a2dde-4643-4222-8fc9-9a54509f03ed",
   "metadata": {},
   "source": [
    "GRADIENT BOOSTING: Gradient Boosting is an ensemble machine learning technique for regression and classification problems. It builds models sequentially, with each new model attempting to correct the errors made by the previous models. The key idea is to combine the outputs of many weak learners to create a strong learner.\n",
    "\n",
    "How Gradient Boosting Works\n",
    "1. Initialize the Model: Start with an initial prediction, often the mean of the target values for regression tasks or a simple classification rule for classification tasks.\n",
    "2. Calculate Residuals: Compute the difference between the actual values and the predicted values (residuals).\n",
    "3. Train Weak Learner: Fit a weak learner (usually a decision tree with limited depth) to the residuals. The goal is to capture the pattern of the residuals.\n",
    "4. Update Model: Add the predictions of the weak learner to the previous predictions to improve accuracy.\n",
    "5. Iterate: Repeat steps 2-4 for a predefined number of iterations or until the residuals are minimized.\n",
    "\n",
    "Applications :\n",
    "1. Finance: Credit scoring, risk management, and algorithmic trading.\n",
    "2. Marketing: Customer segmentation, churn prediction, and campaign optimization.\n",
    "3. Healthcare: Disease prediction, patient risk assessment, and personalized medicine.\n",
    "4. E-commerce: Recommendation systems, fraud detection, and inventory management.\n",
    "5. Manufacturing: Predictive maintenance and quality control.\n",
    "\n",
    "Advantages :\n",
    "1. High Accuracy: Often achieves high predictive accuracy.\n",
    "2. Flexibility: Can be used for both regression and classification tasks.\n",
    "3. Handles Various Data Types: Works well with structured and unstructured data.\n",
    "4. Feature Importance: Provides estimates of feature importance.\n",
    "5. Regularization: Includes techniques like shrinkage (learning rate) and subsampling to prevent overfitting.\n",
    "6. Customizable: Allows use of different loss functions, making it versatile for various applications.\n",
    "\n",
    "Disadvantages :\n",
    "1. Computationally Intensive: Training can be slow, especially with large datasets.\n",
    "2. Memory Usage: Requires more memory compared to simpler models.\n",
    "3. Parameter Tuning: Involves multiple hyperparameters that need tuning (e.g., number of trees, learning rate, tree depth).\n",
    "4. Interpretability: Harder to interpret compared to single decision trees.\n",
    "5. Overfitting: Can overfit if the number of boosting rounds is too high or if the model is too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4dd290-c903-4ba9-a080-37106d714aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18674fea-a607-42b5-85b1-b103346daaec",
   "metadata": {},
   "source": [
    "ADABOOST : AdaBoost, short for Adaptive Boosting, is an ensemble learning technique that combines multiple weak learners to form a strong learner. It is particularly known for improving the performance of decision trees and other simple models by focusing on the errors made by previous models.\n",
    "\n",
    "How AdaBoost Works\n",
    "1. Initialize Weights: Each training instance is assigned an equal weight at the start.\n",
    "2. Train Weak Learner: A weak learner (e.g., a decision stump) is trained on the weighted dataset.\n",
    "3. Compute Error: The weighted error of the weak learner is calculated.\n",
    "4. Update Weights: The weights of incorrectly classified instances are increased, while the weights of correctly classified instances are decreased. This process ensures that subsequent learners focus more on the difficult instances.\n",
    "5. Combine Learners: The final model is a weighted sum of all the weak learners, where each learner's weight is determined by its accuracy.\n",
    "\n",
    "Applications :\n",
    "1. Image Recognition: Used in facial recognition and object detection.\n",
    "2. Text Classification: Classifies documents, emails, and other text data.\n",
    "3. Customer Churn Prediction: Predicts whether customers will leave a service.\n",
    "4. Medical Diagnosis: Assists in diagnosing diseases based on patient data.\n",
    "5. Fraud Detection: Identifies fraudulent transactions and activities.\n",
    "\n",
    "Advantages of AdaBoost\n",
    "1. Boosts Accuracy: Can significantly improve the accuracy of weak learners.\n",
    "2. Simple and Versatile: Easy to implement and can be combined with various weak learners.\n",
    "3. Feature Importance: Provides insights into feature importance based on the weak learners.\n",
    "4. Adaptive: Focuses on difficult instances, improving model robustness.\n",
    "5. Less Overfitting: Tends to have lower overfitting compared to other boosting methods, especially when combined with simple base learners.\n",
    "\n",
    "Disadvantages:\n",
    "1. Sensitive to Noisy Data: Can be sensitive to noisy data and outliers, as it focuses heavily on difficult instances.\n",
    "2. Computational Cost: Training can be slow, especially with large datasets.\n",
    "3. Requires Weak Learners: Relies on having weak learners that perform slightly better than random guessing.\n",
    "4. Parameter Tuning: Requires careful tuning of hyperparameters (e.g., number of learners, learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f1c05-4665-4901-a6e3-742f6464250a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da244c5-2201-4f6b-b441-8dc4ff30d6c0",
   "metadata": {},
   "source": [
    "XGBOOST : XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting that solves many data science problems in a fast and accurate way.\n",
    "\n",
    "How XGBoost Works\n",
    "1. Gradient Boosting Framework: Like traditional gradient boosting, XGBoost builds trees sequentially, where each new tree tries to correct errors made by the previous trees.\n",
    "2. Regularization: XGBoost includes regularization terms in the objective function to control the complexity of the model and prevent overfitting.\n",
    "3. Sparsity Aware: Efficiently handles sparse data and missing values.\n",
    "4. Weighted Quantile Sketch: Uses a distributed algorithm to handle weighted data, allowing efficient construction of trees.\n",
    "5. Block Structure: Optimized for parallel computation on both single machines and distributed environments.\n",
    "\n",
    "Application : \n",
    "1. Kaggle Competitions: Frequently used in data science competitions for its high performance.\n",
    "2. Finance: Credit scoring, risk assessment, and algorithmic trading.\n",
    "3. Marketing: Customer segmentation, churn prediction, and recommendation systems.\n",
    "4. Healthcare: Disease prediction, patient risk assessment, and personalized medicine.\n",
    "5. Sales Forecasting: Predicting future sales based on historical data.\n",
    "6. Fraud Detection: Identifying fraudulent transactions and activities.\n",
    "\n",
    "Advantage's:\n",
    "1. High Performance: Known for its high accuracy and performance in machine learning competitions.\n",
    "2. Regularization: Built-in L1 and L2 regularization to prevent overfitting.\n",
    "3. Parallel Processing: Supports parallel computation, speeding up training time.\n",
    "4. Handling Missing Values: Efficiently handles missing values.\n",
    "5. Scalability: Can handle large datasets and distributed computing environments.\n",
    "6. Customizable: Allows for custom loss functions and evaluation metrics.\n",
    "7. Feature Importance: Provides feature importance scores.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Complexity: More complex than simpler models like logistic regression or decision trees.\n",
    "2. Parameter Tuning: Requires careful tuning of multiple hyperparameters to achieve optimal performance.\n",
    "3. Computational Cost: Can be computationally expensive, especially with large datasets.\n",
    "4. Memory Usage: Requires significant memory, particularly with large datasets and deep trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77981a2-24b4-4674-b8fd-5f11d199ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc0ca09-592d-468e-a0d7-a375b31bf779",
   "metadata": {},
   "source": [
    "UNSUPERVISED MACHINE LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350d0c6-7159-4293-a7f9-877a1c36908a",
   "metadata": {},
   "source": [
    "clustering : Grouping data points into clusters based on their similarity\n",
    "\n",
    "Clusters : A group of data points that are more similer to each other than to other points in the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c95f2f-0d97-4c8c-b381-a4f8b13a7dca",
   "metadata": {},
   "source": [
    "KMEAN-CLUSTERING : It is a popular unsupervised machine-learning algorithm used for partitioning a dataset into a set number of clusters K \n",
    "->The algorithm aims to partition the data into K clusters in which each data points belongs to the cluster with the nearest mean(centroid).\n",
    "\n",
    "Centroid : The center of a cluster often calculated as the mean of all points in the cluster.\n",
    "\n",
    "Process:\n",
    "1. initialise the k value : centroids ;\n",
    "2. Assign the nearest data points to clusters to make group\n",
    "3. update centroids by calculating mean \n",
    "4. Repeate the step 2 and 3 until we get pairs single clusters.\n",
    "5. Result \n",
    "\n",
    "\n",
    "How do we select the K value \n",
    "WCSS: within the cluster sum of squares\n",
    "\n",
    "using Euclidean distance and square of distance bw points to the nearest centroids\n",
    "which form elbow method \n",
    "\n",
    "The Elbow Method is a technique used to determine the optimal number of clusters in a dataset for clustering algorithms, such as K-means clustering. The goal is to identify the number of clusters that best represents the structure of the data without overfitting.\n",
    "\n",
    "Limitations:\n",
    "The number of clusters k\n",
    "1. k must be specified beforehand.\n",
    "2. Sensitive to the initial placement of centroids.\n",
    "3. Can converge to a local minimum.\n",
    "4. Not suitable for clusters with non-spherical shapes or varying sizes and densities.\n",
    "\n",
    "Application : \n",
    "1. Customer Segmentation: Segmenting customers based on purchasing behavior, demographics, and preferences.\n",
    "2. Image Compression: Reducing the number of colors in an image while maintaining its overall appearance.\n",
    "3. Document Clustering: Grouping similar documents for topic modeling, organizing large datasets, or improving search relevance.\n",
    "4. Biological Data Analysis: Grouping genes or proteins with similar expression patterns or functional characteristics.\n",
    "\n",
    "Advantage's:\n",
    "1. Simplicity and Ease of Implementation:\n",
    "->Easy to understand and implement.\n",
    "->Suitable for beginners in machine learning and data analysis.\n",
    "2. Efficiency:\n",
    "->Computationally efficient for small to medium-sized datasets.\n",
    "->Scales linearly with the number of data points.\n",
    "3. Speed:\n",
    "->Typically converges quickly compared to other clustering algorithms.\n",
    "4. Versatility:\n",
    "->Applicable to various types of data and numerous fields such as marketing, biology, and image processing.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Need for Prior Specification of k:\n",
    "->The number of clusters k:\n",
    "->k must be specified beforehand, which may not be known in advance.\n",
    "2. Sensitivity to Initial Centroid Placement:\n",
    "->Different initial placements of centroids can lead to different clustering results.\n",
    "->The algorithm may converge to a local minimum.\n",
    "3. Assumption of Spherical Clusters:\n",
    "->Assumes clusters are spherical and equally sized, which may not be true for real-world data.\n",
    "->Not suitable for clusters of arbitrary shapes and sizes.\n",
    "4. Scaling Issues:\n",
    "->Performance degrades with high-dimensional data due to the curse of dimensionality.\n",
    "5. Outliers and Noise Sensitivity:\n",
    "->Highly sensitive to outliers and noise, which can skew the results significantly.\n",
    "6. Fixed Number of Clusters:\n",
    "->Not adaptive; does not change the number of clusters dynamically based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30d78d-ef6f-4357-9094-333817653811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23a5bba0-6d81-45a1-95c0-7014e1999bb0",
   "metadata": {},
   "source": [
    "HIERARCHICAL CLUSTRING :Hierarchical clustering is a type of unsupervised machine learning algorithm used to group data points into a hierarchy or tree of clusters. Unlike K-means clustering, hierarchical clustering does not require specifying the number of clusters in advance.\n",
    "There are two main types of hierarchical clustering:\n",
    "1. Agglomerative (Bottom-Up):\n",
    "->Starts with each data point as its own cluster.\n",
    "->Iteratively merges the closest pairs of clusters until only one cluster remains or the desired number of clusters is reached.\n",
    "2. Divisive (Top-Down):\n",
    "->Starts with all data points in one cluster.\n",
    "->Iteratively splits the cluster into smaller clusters until each data point is its own cluster or the desired number of clusters is reached.\n",
    "\n",
    "Steps in Agglomerative Hierarchical Clustering\n",
    "1. Calculate the Distance Matrix: Compute the distance between every pair of data points using a distance metric such as Euclidean distance.\n",
    "2. Merge Closest Clusters: Find the pair of clusters with the smallest distance between them and merge them into a single cluster.\n",
    "3. Update the Distance Matrix: Recalculate the distances between the new cluster and all other clusters.\n",
    "4. Repeat: Continue merging the closest pairs of clusters and updating the distance matrix until all data points are in one cluster or the desired number of clusters is achieved.\n",
    "\n",
    "Dendogram : which helps to select number of clusters by selecting the largest vertical line such a way that no horizontal line passes through it.\n",
    "\n",
    "Application :\n",
    "1. Document and Text Clustering:\n",
    "->Application: Organizing documents or articles into a hierarchical structure based on content similarity.\n",
    "2. Market Segmentation:\n",
    "->Application: Segmenting customers based on purchasing behavior, demographics, or preferences.\n",
    "3. Image Segmentation:\n",
    "->Application: Dividing an image into segments based on pixel similarity.\n",
    "4. Customer Support:\n",
    "->Application: Grouping customer support tickets or feedback based on the nature of issues.\n",
    "\n",
    "Advantages :\n",
    "1. No Need to Specify Number of Clusters:\n",
    "->Unlike K-means, there is no need to pre-specify the number of clusters.\n",
    "2. Dendrogram:\n",
    "->The dendrogram provides a visual representation of the data‚Äôs hierarchical structure, which can help in choosing the number of clusters.\n",
    "3. Flexibility:\n",
    "->Can use different distance metrics and linkage criteria based on the problem requirements.\n",
    "4. Reproducibility:\n",
    "->Deterministic in nature (especially agglomerative clustering) which means the result is reproducible.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Computational Complexity:\n",
    "->More computationally intensive compared to K-means, especially for large datasets (time complexity more).\n",
    "2. No Reassignment:\n",
    "->Once a merge or split is done, it cannot be undone, potentially leading to suboptimal clustering.\n",
    "3. Scalability:\n",
    "->Not suitable for very large datasets due to high memory and time requirements.\n",
    "4. Sensitive to Noise and Outliers:\n",
    "->Can be significantly affected by the presence of noisy data and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6b343-f94b-4d7b-ab16-83b784bd9748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a7a15a-162b-46e4-946d-6cba8f6d941a",
   "metadata": {},
   "source": [
    "DBSCAN CLUSTERING :DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful clustering algorithm that identifies clusters based on the density of data points. Unlike K-means or hierarchical clustering, DBSCAN does not require the number of clusters to be specified in advance and can handle noise and outliers effective.\n",
    "\n",
    "How DBSCAN Works: DBSCAN groups points into clusters based on their density, defined by two parameters: eps (the maximum distance between two points to be considered neighbors) and minPts (the minimum number of points required to form a dense region). Here‚Äôs a step-by-step \n",
    "\n",
    "explanation:\n",
    "1. Core Points: A point is a core point if it has at least minPts neighbors within eps distance.\n",
    "2. Border Points: A point is a border point if it is within eps distance of a core point but has fewer than minPts neighbors.\n",
    "3. Noise Points: A point is a noise point if it is neither a core point nor a border point.\n",
    "4. Cluster Formation: Start with an arbitrary point. If it is a core point, create a new cluster with this point and all its density-reachable points. Repeat for all points in the dataset.\n",
    "\n",
    "Application :\n",
    "1. Social Network Analysis: Application: Detecting communities or groups within social networks based on interaction patterns.\n",
    "2. Astronomy: Grouping stars, galaxies, or other celestial objects based on their spatial distribution.\n",
    "3. Market Basket Analysis: Grouping items frequently bought together in retail datasets.\n",
    "4. Anomaly Detection: Identifying outliers or anomalies in data, such as fraud detection or network security.\n",
    "\n",
    "Advantages :\n",
    "\n",
    "1. No Need to Specify Number of Clusters:\n",
    "->DBSCAN can automatically detect the number of clusters based on the data‚Äôs density.\n",
    "2. Identifies Arbitrarily Shaped Clusters:\n",
    "->Unlike K-means, DBSCAN can find clusters of various shapes and sizes.\n",
    "3. Handles Noise and Outliers:\n",
    "->DBSCAN effectively identifies and labels noise points, making it robust to outliers.\n",
    "4. Scalability:\n",
    "->Efficient for large datasets, especially with indexing structures like k-d trees or ball trees.\n",
    "\n",
    "Disadvantage's:\n",
    "\n",
    "1. Parameter Sensitivity:\n",
    "->The performance of DBSCAN heavily depends on the choice of eps and minPts. Choosing the right values can be challenging.\n",
    "2. Not Suitable for Varying Density Clusters:\n",
    "->DBSCAN may struggle with datasets containing clusters with varying densities, as a single eps value may not be suitable for all clusters.\n",
    "3. Computational Complexity:\n",
    "->Although more scalable than hierarchical clustering, it can still be computationally expensive for very large datasets, especially in high-dimensional spaces.\n",
    "4. Distance Metric Dependency:\n",
    "->The results can be significantly affected by the choice of distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0a117-a58d-418a-b071-7757a5e6e77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8ef8bc-4f86-41f4-b4fa-e2b8cd1cdabc",
   "metadata": {},
   "source": [
    "ASSOCIATION RULE: It is a unsupervised machine learning algorithm used in data mining technique for association rule in transactional database.Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large datasets. It is commonly used in market basket analysis, where the goal is to find associations between items that frequently co-occur in transactions. The most popular algorithm for generating association rules is the Apriori algorithm.\n",
    "\n",
    "The APRIORI algorithm is a classic algorithm used in data mining for learning association rules. It is primarily used for market basket analysis to identify frequent itemsets and generate association rules. The algorithm operates on the principle that any subset of a frequent itemset must also be frequent.\n",
    "\n",
    "Steps of the Apriori Algorithm\n",
    "\n",
    "1. Generate Candidate Itemsets: Begin with single-item itemsets and generate larger itemsets iteratively.\n",
    "\n",
    "2. Prune Infrequent Itemsets: Remove itemsets that do not meet the minimum support threshold.\n",
    "\n",
    "3. Generate Frequent Itemsets: Identify itemsets that meet the minimum support threshold.\n",
    "\n",
    "4. Generate Association Rules: From the frequent itemsets, generate rules that meet the minimum confidence threshold.\n",
    "\n",
    "Support:Support of an itemset is the proportion of transactions in the dataset in which the itemset appears.\n",
    "Support(A)=number of transaction contain A/total number of transaction.\n",
    "\n",
    "Confidence:Confidence of a rule is the proportion of transactions containing the antecedent in which the consequent also appears.\n",
    "Confidence(A->B)=support(AUB)/Support(A)\n",
    "\n",
    "Lift: Lift of a rule is the ratio of the observed support to that expected if A and B were independent.\n",
    "lift(A->B)=support(AUB/support(A)xsupport(B)\n",
    "\n",
    "Application :\n",
    "1. Market Basket Analysis: Identifying items frequently bought together in retail stores.\n",
    "2. Recommendation Systems: Recommending products or content based on user behavior.\n",
    "3. Web Usage Mining: Discovering patterns in web navigation data to optimize website structure and content\n",
    "4. Fraud Detection: Identifying unusual patterns in transactions that may indicate fraudulent activity.\n",
    "5. Healthcare: Finding patterns in patient records to improve diagnosis and treatment.\n",
    "\n",
    "Advantages :\n",
    "1. Interpretability: The generated rules are easy to understand and interpret, making it accessible for non-experts.\n",
    "2. Actionable Insights: Provides direct actionable insights that can be used for decision-making in various domains.\n",
    "3. Scalability: Can handle large datasets efficiently, especially with optimized algorithms like Apriori and FP-Growth.\n",
    "4. Simplicity to implement\n",
    "5. Support for large dataset\n",
    "6. clear output\n",
    "7. Flexibility\n",
    "\n",
    "Disdvantage's:\n",
    "\n",
    "1. Parameter Sensitivity: The performance and relevance of the rules depend heavily on the chosen support and confidence thresholds.\n",
    "2. Complexity with Large Itemsets: Can generate a large number of rules, making it challenging to identify the most useful ones.\n",
    "3. Redundancy: Many rules can be redundant or irrelevant, especially in datasets with many items.\n",
    "4. Requires Discrete Data: Works best with categorical data and may require preprocessing for continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c25db3-8d62-41b5-baff-d43a224c7a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5af20f2-cd87-4b64-a502-f4e1a36f7694",
   "metadata": {},
   "source": [
    "DEEP LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f884eca-cf29-4da1-8c4d-00ba281d9189",
   "metadata": {},
   "source": [
    "Deep  learning is a subset of machine learning that involves neural network with many layers (hence 'deep') to model and understand complex pattern in data.It is particulerly effective for tasks where traditional algorithm struggle,such as image and speech recognition & NLP(Natural Language Processing) & Game playing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbe6f6-edf5-4fdc-9401-161314f5a985",
   "metadata": {},
   "source": [
    " These deep learning's are used for handling large dataset.The AIM of deep learning is to mimic human brain.Neural networks are class of machine learning algorithm inspired by the strucuture and function of human brain.They are designed to recognize patterns,make decisions and solve complex problems by learning from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bbb60-81ed-40a2-853e-3db7d57bbde7",
   "metadata": {},
   "source": [
    "ANN : AN Artificial Neural networks is a computational model inspired by the way biological neural network in the human brain process information.It consists of interconnected nodes organized in layers, which can learn to recognize pattern classify data and make predictions\n",
    "ANN is the key technology in machine learning and deep learning.\n",
    "\n",
    "Perceptron : Type of neural network in Machine Learning(ANN).\n",
    "\n",
    "How ANN work's:\n",
    "\n",
    "1. Input layers: Recieves the initial data.\n",
    "2. Hidden layers : Multiple layers where data is processed through neurons with activation function,these layers captures heirarchical pattern in the data.\n",
    "3. Output layers : Produces the final prediction or classification,\n",
    "\n",
    "weights : Parameters that adjust the input signals.Each connection between neurons has a weight,which determines the importance of input data.\n",
    "\n",
    "loss function : measures the difference between the predicted and actual output\n",
    "mean squared error for regression task.cross entropy loss for classification task.\n",
    "\n",
    "optimization algorithm : Used to minimize the loss function by adjusting the networks weights.\n",
    "\n",
    "Gradient Descent : Updates the weights based on the gradient of the loss function\n",
    "\n",
    "Forward Propagation : Forward propagation is the process in neural network where input data is passed through the layer of network to produce an output.This process involves calculating the weighted sums of inputs at each neuron,applying activation functions and moving data from the input layers to output layers.\n",
    "\n",
    "Backward Propagation : Backward propagation is the process of training neural networks,where the network  weigths are adjusted based on error of the output compared to expected result.It involves calculating the gradient of loss function with respect to each weights by chain rule which helps in minimizing the loss by updating weights using optimization algorithm \n",
    "\n",
    "Activation function : A function applied to the weighted sum of inputs to introduce non-linearity into the networks.\n",
    "some of common activation function sigmoid,Tanh,ReLu,softmax(used in the output layer to produce probability)\n",
    "\n",
    "Epoch : It is an one full iteration over the entire training dataset(1 complete forward and backward propagation).After an epoch every example in the training dataset has been seen once by the model\n",
    "\n",
    "Batches : The dataset is split into several batches,each batch contains a subset of the training data.The model processes one batch at a time the parameters are updated based on the average loss of example in that batch.\n",
    "\n",
    "Conclusions :\n",
    "Forward Propagation : input layers,weights,hidden layers,bias,activation function\n",
    "Backward Propagation : Loss function optimization update the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f125f-ba57-4858-a9a0-05c383782efb",
   "metadata": {},
   "source": [
    "Advantage's:\n",
    "\n",
    "1. Ability to Learn Complex Patterns: ANNs can learn and model complex non-linear relationships in data, making them suitable for tasks where traditional algorithms may struggle.\n",
    "\n",
    "2. Adaptability: ANNs can adapt and learn from new data, making them robust in dynamic environments or where data patterns change over time.\n",
    "\n",
    "3. Parallel Processing: ANNs can perform computations in parallel, which can lead to faster processing times for certain tasks compared to sequential algorithms.\n",
    "\n",
    "4. Generalization: ANNs can generalize from training data to make predictions on unseen data, provided they are properly trained and validated.\n",
    "\n",
    "5. Feature Learning: Deep learning architectures (a type of ANN) can automatically learn relevant features from raw data, reducing the need for manual feature engineering.\n",
    " \n",
    "6. Scalability: ANNs can scale with the size of data, leveraging advancements in hardware (like GPUs) to handle large datasets efficiently.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Requires Large Amounts of Data: ANNs, especially deep learning models, often require large amounts of labeled data for training, which can be expensive and time-consuming to collect and annotate.\n",
    "\n",
    "2. Computational Complexity: Training complex ANNs can require significant computational resources, including high-performance GPUs or even specialized hardware, which can be costly.\n",
    "\n",
    "3. Black Box Nature: ANNs, particularly deep models, can be difficult to interpret and understand how decisions are made (i.e., they are often seen as \"black box\" models).\n",
    "\n",
    "4. Overfitting: ANNs, if not properly regularized or validated, can overfit to the training data, meaning they perform well on training data but poorly on unseen test data.\n",
    "\n",
    "5. Hyperparameter Sensitivity: ANNs have many hyperparameters (e.g., number of layers, learning rate) that need to be tuned, which can require expertise and experimentation.\n",
    "\n",
    "6. Lack of Transparency: Due to their complex nature, ANNs may lack transparency and accountability in certain critical applications where understanding the decision-making process is crucial (e.g., healthcare, legal).\n",
    "\n",
    "Application :\n",
    "1. Image and Speech Recognition:\n",
    "2. Natural Language Processing (NLP):\n",
    "3. Autonomous Vehicles:\n",
    "4. Healthcare and Medicine:\n",
    "5. Finance and Business:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c1991-e84d-4d10-a38a-3b76fbe762b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "583acb88-8842-4a8b-836f-af8ccbed6041",
   "metadata": {},
   "source": [
    "CNN : Convolutional Neural Network: It is a class of deep learning primarily used for processing and analyzing visual data.CNN are particularly effective for tasks like image classification,object detection and image segmentation because they can automatically and adaptively learn spatial heirarchies of features from input images.\n",
    "      \n",
    "CNN are widely used in image recognition,computer vision and various other application.\n",
    "\n",
    "Convolutional layer : The convolutional layer is the core building block of a CNN,it applies a set of learnable filters(kernel) to the input data to produce feature map.\n",
    "\n",
    "FilterKernel :In a Convolutional Neural Network (CNN), a filter kernel (or simply a kernel) is a small matrix used to scan over the input data (such as an image) and extract features. Each kernel performs a convolution operation, which involves element-wise multiplication with a portion of the input, followed by summing the results to produce a single value. This operation is repeated across the entire input to generate a feature map.\n",
    "\n",
    "Size: The size of a filter kernel is typically much smaller than the size of the input image. Common sizes include 3x3, 5x5, or 7x7 pixels.\n",
    "        \n",
    "Activation function : Introduce non linearity into the model allowing it to learn more complex functions\n",
    "->Apply non-linear transformations to the output of each layer.\n",
    "->Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
    "->ReLU is the most widely used activation function in CNNs.\n",
    "\n",
    "CNN Architecture:\n",
    "\n",
    "1. Input Layer:\n",
    "->Takes the raw image data as input.\n",
    "->Dimensions are typically height x width x channels (e.g., 32x32x3 for a color image).\n",
    "   \n",
    "2. Convolutional Layers + Activation Functions:\n",
    "->Extract features from the input image.\n",
    "->Multiple convolutional layers are stacked to learn complex patterns.\n",
    "             \n",
    "3. Pooling Layers: Reduce the spatial dimensions of the feature maps while retaining important information.\n",
    "\n",
    "4. Flatten layer : Converts the 2D feature map into 1D feature vector to be fed into fully connected layer.\n",
    "\n",
    "5. Fully Connected Layers: Integrate the features extracted by the convolutional layers to make final predictions.dense layer that performs the final prediction for classification or regression.\n",
    "\n",
    "6. Output Layer: For classification tasks, a softmax activation function is used to output the probabilities for each class.\n",
    "\n",
    "Application :\n",
    "1. Image Classification\n",
    "->Object Recognition: Identifying objects within an image, such as distinguishing between different animals, vehicles, or everyday items.\n",
    "->Scene Classification: Categorizing an entire scene, such as recognizing different types of landscapes or environments (e.g., beaches, forests, cities).\n",
    "        \n",
    "2. Object Detection\n",
    "->Bounding Box Prediction: Identifying and localizing objects within an image by drawing bounding boxes around them.\n",
    "->Face Detection: Recognizing and locating human faces in images or videos.\n",
    "\n",
    "3. Medical Imaging\n",
    "->Disease Diagnosis: Analyzing medical images such as X-rays, MRIs, and CT scans to detect abnormalities, tumors, or other medical conditions.\n",
    "->Histopathology: Examining tissue samples to identify diseases at a cellular level\n",
    "       \n",
    "4. Facial Recognition\n",
    "->Authentication Systems: Verifying identities in security systems, smartphones, and other devices.\n",
    "->Emotion Detection: Analyzing facial expressions to determine emotional states.\n",
    "             \n",
    "5. Natural Language Processing (NLP)\n",
    "->Image Captioning: Generating descriptive captions for images by combining CNNs with Recurrent Neural Networks (RNNs).\n",
    "->Visual Question Answering: Answering questions about the content of an image.\n",
    "      \n",
    "Advantages :\n",
    "1. Automatic Feature Extraction:\n",
    "CNNs automatically learn and extract features from raw images, eliminating the need for manual feature engineering.\n",
    "2. Spatial Hierarchies of Features:\n",
    "Convolutional layers capture spatial hierarchies in images, recognizing patterns and structures at different levels (e.g., edges, textures, objects).\n",
    "3. Translation Invariance:\n",
    "CNNs can recognize objects regardless of their position in the image, making them robust to translations and slight variations.\n",
    "   \n",
    "Disadvantages :\n",
    "1. Computationally Intensive:\n",
    "Training CNNs requires significant computational power and memory, especially for deep networks and large datasets.\n",
    "2. Data Requirements:\n",
    "CNNs typically require large amounts of labeled data to train effectively, which can be a limitation in scenarios with limited data availability\n",
    "3. Lack of Interpretability:\n",
    "CNNs are often considered \"black boxes\" because it can be challenging to understand how they make decisions and which features they prioritize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcf429-fa43-454c-ac39-b48ed4858529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "117adce9-9a66-4040-a240-0d95451d0788",
   "metadata": {},
   "source": [
    "RNN : RECURRENT NEURAL NETWORK These are the type of artificial neural network designed for sequential data.Unlike traditional neural network.RNN have connection that form directed cycles,allowing information to perist.\n",
    "\n",
    "->Recurrent Connections: Unlike traditional neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a memory of previous inputs in the sequence.\n",
    "\n",
    "Type of RNN :\n",
    "1. one to one :Passing one input and getting one output.\n",
    "ex: Image classification\n",
    "2. one to many : Passing one input and getting many output.\n",
    "ex: Music generation,Google translation\n",
    "3. Many to one : Passing many input and getting one output.\n",
    "ex: Sentiment analysis,Sale Prediction\n",
    "4. Many to many : Passing many output and getting many output\n",
    "ex: chatbot,Language traslation.\n",
    "\n",
    "Forward Propagation : It refers to the process of passing input data through the network to generate an output.RNN maintain a hidden state that is passed along time steps allowing the network to retain information from previous input.\n",
    "\n",
    "Backward propagation : In RNN it is a process of computing gradient for the networks parameters by propagating errors backward through the network.This is essential for updating the networks weights during training to minimize the loss function.\n",
    "\n",
    "RNN Architecture :\n",
    "1. Initialization : The input layer takes the sequential input data.\n",
    "2. Hidden layer : Processes each element in the sequence,updating its state based on the current input and previous hidden state. RNN have hidden states that get updated at each time step. The hidden state at time t is a function of the input at time t and the hidden state at time t‚àí1.RNN maintain a hidden state that captures information about previous input in the sequence.\n",
    "3. Output layer : Produces the final output for each time step or the entire sequence\n",
    "4. Weight Sharing: The weights of the network are shared across all time steps, making RNNs parameter-efficient for sequential data.\n",
    "5. parameter sharing : some weights are used across all time steps which helps in learning patterns in the data.\n",
    "6. Loss Function: The loss is typically computed over the entire sequence, and the network is trained using backpropagation through time (BPTT).\n",
    "\n",
    "Variants of RNN: \n",
    "\n",
    "1. LSTM : LONG-SHORT-TERM-MEMORY : It is a type of recurrent neural network architecture that is particularly well-suited for sequence prediction problems.LSTM have connections that loop back on themselves,allowing them to maintain a memory of previous inputs.This makes them especially usefull for tasks where the order of the input data matter such as time series prediction,natural language processing,speech recognition.\n",
    "\n",
    "How LSTM works:\n",
    "1. Input Gate: Decides which values from the input should be updated in the memory cell.It uses a sigmoid function to determine hoe much of each component should be updated.\n",
    "2. Forget Gate : Decides what new information to store in the cell state.It also uses a sigmoid function to decide which ports of the cell state should be forgetten.\n",
    "3. Output Gate : Decides what part of the cell state to output as the hidden state.Determines the output of the LSTM cell.It uses a sigmoid function to decide which ports of the cell state to output.\n",
    "\n",
    "2. GRU : Gated Recurrent Unit (GRU) is a variant of the Recurrent Neural Network (RNN) that is designed to solve the vanishing gradient problem and to be more efficient than the Long Short-Term Memory (LSTM) networks. GRUs have a simpler structure than LSTMs but can often achieve similar performance.\n",
    "\n",
    "->Gates: GRUs use two gates to control the flow of information:->\n",
    "->Reset Gate: Determines how much of the previous hidden state to forget.\n",
    "->Update Gate: Determines how much of the previous hidden state to retain and how much of the new input to incorporate.\n",
    "->Hidden State: GRUs maintain and update a single hidden state, which makes them computationally more efficient than LSTMs that have both a cell state and a hidden state.\n",
    "\n",
    "How GRU works:\n",
    "1. Reset Gate:The reset gate decides how much of the previous hidden state to forget.\n",
    "2. Update Gate: The update gate determines how much of the previous hidden state to keep and how much of the new candidate hidden state to use.\n",
    "3. New Hidden State:The new hidden state is calculated using the reset gate.\n",
    "4. Final Hidden State: The final hidden state is a combination of the previous hidden state and the new hidden state, controlled by the update gate.\n",
    "\n",
    "Advantage's:\n",
    "1. Sequential Data Handling:RNNs are specifically designed to handle sequential data, making them ideal for tasks like time series prediction, natural language processing, and speech recognition.\n",
    "2. Parameter Sharing: RNNs use the same parameters (weights) across all time steps, which makes them parameter-efficient and helps in learning temporal patterns.\n",
    "3. Temporal Dependencies: RNNs can model temporal dependencies and relationships between elements of a sequence, capturing context over time.\n",
    "4. Flexibility: They can handle input sequences of varying lengths, making them flexible for different applications.\n",
    "\n",
    "Disadvantage's:\n",
    "1. Long-Term Dependencies:RNNs struggle to learn long-term dependencies due to the vanishing gradient problem, making it hard to capture patterns that span long sequences.\n",
    "2. Training Time: RNNs can be slow to train because each time step requires the previous hidden state, leading to sequential processing that cannot be parallelized easily.\n",
    "3. Complexity: Designing and tuning RNNs can be complex, requiring careful consideration of the architecture and hyperparameters to achieve good performance.\n",
    "4. Resource Intensive: RNNs can be computationally intensive, requiring significant memory and processing power, especially for long sequences and large datasets.\n",
    "\n",
    "Application : \n",
    "1. Natural Language Processing (NLP):\n",
    "->Language modeling\n",
    "->Machine translation\n",
    "->Text generation\n",
    "->Sentiment analysis\n",
    "2. Time Series Analysis:\n",
    "->Stock price prediction\n",
    "->Weather forecasting\n",
    "3. Speech Recognition:\n",
    "->Transcribing audio to text\n",
    "4. Video Analysis:\n",
    "->Action recognition\n",
    "->Video captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb05eb2-accc-46ce-8f51-437251d25a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ecf6ef2-cc32-4319-a468-ced0843c7c51",
   "metadata": {},
   "source": [
    "RECOMMENDATION SYSTEM :It is a type of machine learning system that suggest items to users based on various factors.These items could be products,movies,boooks,articles or any other content that user might be interested in.Recommendation Engine's are widely used in E-commerce streaming services,social media and other platforms to persomalize user experience and increase engagements.\n",
    "\n",
    "Type's of Recommendation system.\n",
    "\n",
    "1. Content-Based Filtering:\n",
    "->Recommends items similar to those the user has liked in the past based on item features.recommends items by analyzing the content of items and user preference.\n",
    "2. Collaborative Filtering:\n",
    "->Recommends items based on the preferences of similar users or the similarity between items. It can be user-based or item-based.\n",
    "->User-Based Collaborative Filtering: Finds users similar to the target user and recommends items they liked.\n",
    "->Item-Based Collaborative Filtering: Finds items similar to those the target user liked and recommends these similar items.\n",
    "->Recommends items by finding similer users.If A & B have similer tastes,item liked by B can be recommends to A.\n",
    "3. Hybrid Systems:\n",
    "Combine content-based and collaborative filtering techniques to leverage the strengths of both methods and provide more accurate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17580471-721b-4f02-aadc-ce9af03241db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b942445-ca39-4aa5-b638-96e94e8bdac5",
   "metadata": {},
   "source": [
    "TIME SERIES ANALYSIS :Time series analysis involves methods for analyzing time series data to extract meaningful statistics and other characteristics of the data. It is used for forecasting, anomaly detection, and understanding the underlying structure of the data.\n",
    "Time series in machine learning refers to the sequence of data points collected or recorded at successfull points in time,typically at uniform  intervals.\n",
    "->This type of data is crucial in various domain including finance,economics,weather forecasting,stock market analysis..\n",
    "\n",
    "1. Time Series Data: A sequence of data points collected or recorded at specific time intervals. Examples include stock prices, weather data, and sales figures.\n",
    "2. Stationarity: A stationary time series has statistical properties (mean, variance, autocorrelation) that do not change over time. Stationarity is a crucial assumption for many time series models.\n",
    "3. Trend: The long term progression or direction in the data.It can be downward or upward.\n",
    "4. Seasonality: Regular repeating patterns or cycles in data,often influenced by seasonal factors(e.g., hourly, daily, monthly).\n",
    "5. Level : Average of variable over a fixed period of time.\n",
    "6. Noise : It refers to the variations which occurs due to unpredictable factors and also do not repeate in particuler pattersns.\n",
    "\n",
    "Time Series Forecasting Models\n",
    "1. Autoregressive Integrated Moving Average (ARIMA):\n",
    "Combines autoregression (AR), differencing (I), and moving average (MA) components. Suitable for univariate time series data.\n",
    "2. Seasonal ARIMA (SARIMA):\n",
    "Extends ARIMA to account for seasonality by incorporating seasonal terms.\n",
    "3. Exponential Smoothing (ETS):\n",
    "Methods like Simple Exponential Smoothing, Holt‚Äôs Linear Trend Model, and Holt-Winters Seasonal Model.\n",
    "4. Prophet:\n",
    "A forecasting tool developed by Facebook, designed for handling time series data with strong seasonal effects and missing data.\n",
    "5. Long Short-Term Memory (LSTM):\n",
    "A type of recurrent neural network (RNN) capable of learning long-term dependencies, suitable for complex and large time series datasets.\n",
    "\n",
    "Forecasting model building strategy.\n",
    "1. Define goal\n",
    "2. Data collection\n",
    "3. Explore and visualize series\n",
    "4. preprocess data\n",
    "5. partition series\n",
    "6. Apply forecasting method\n",
    "7. Evaluate and compare performance\n",
    "8. Implement forecasting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc6e74-8bd6-46e0-b185-67c6b82c0525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee79122d-78e5-4dc1-ade4-7c6466c6e0ef",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. Its goal is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP combines computational linguistics, computer science, and statistics to process and analyze large amounts of natural language data.\n",
    "\n",
    "Application of NLP:\n",
    "1. Machine Translation: Automatically translating text from one language to another (e.g., Google Translate).\n",
    "2. Speech Recognition: Converting spoken language into text (e.g., Siri, Google Assistant).\n",
    "3. Chatbots and Virtual Assistants: Providing automated customer service or personal assistance (e.g., chatbots on websites).\n",
    "4. Text Summarization: Creating concise summaries of long documents.\n",
    "5. Information Retrieval: Finding relevant information from large datasets (e.g., search engines).\n",
    "6. Sentiment Analysis: Analyzing customer reviews to gauge sentiment towards products or services.\n",
    "7. Language Generation: Creating human-like text based on input data (e.g., generating news articles).\n",
    "\n",
    "\n",
    "Text-Preprocessing Techniques : \n",
    "\n",
    "1. Tokenization: Tokenization is a fundamental preprocessing step in Natural Language Processing (NLP) that involves breaking down a text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the level of granularity needed for a specific NLP task. Example: \"ChatGPT is amazing!\" becomes (\"ChatGPT\", \"is\", \"amazing\", \"!\").\n",
    "2. Stemming: Stemming is a process in Natural Language Processing (NLP) that involves reducing words to their root or base form, typically by removing suffixes. The primary goal of stemming is to group words with similar meanings under a single term, thus simplifying the data and reducing dimensionality. Ex : 'Running': Run\n",
    "3. Lemmatization: Lemmatization is a text preprocessing technique in Natural Language Processing (NLP) that involves reducing a word to its base or dictionary form, known as the lemma. Unlike stemming, which simply truncates words to remove prefixes or suffixes, lemmatization considers the context and the morphological analysis of the words, ensuring that the root form is a meaningful word.\n",
    "   \n",
    "5. BOW (Bag of words):Bow model is a fundamental technique in NLP for txt representation.It simplifies txt data into a format that can be used for various machine learning algorithm by transforming the text into a set of words frequencies.\n",
    "   ->if the word is present than its 1.]                       \n",
    "   ->if the word is not present than its 0.\n",
    "6. TF: Term Frequency : Measures how frequently a term appears in a document.\n",
    "    TF=(Number of times term appears/Total number of term in document)\n",
    "7. IDF : Inverse Document Frequency : Measures how important a term is. While computing TF,all terms are equal imp\n",
    "   IDF=(Number of sentances/No of sentances containing word)\n",
    "\n",
    "Why TFIDF : Filtering common wordds,it reduces the weight of terms that occur very frequently in the corpus and are hence empirically less informative and highlighting rare words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4a82a-8222-4232-80ef-6b746d7f3046",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field of technology that enables computers to understand and interact with human language in a way that is meaningful and useful. By leveraging various techniques, NLP allows machines to process and analyze large amounts of natural language data, making it possible to apply this understanding in machine learning algorithms. Techniques such as tokenization, stop words removal, and lemmatization help preprocess text data, while methods like named entity recognition and dependency parsing enable the extraction of meaningful information and relationships. Advanced models, including those based on transformers, enhance the ability of machines to perform complex tasks such as text classification, sentiment analysis, and machine translation, thereby bridging the gap between human language and computational analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5971ef-03ce-423e-bd62-254dc49a09e5",
   "metadata": {},
   "source": [
    "TF-IDF is a statistical measure used in text processing and information retrieval to evaluate the importance of a word within a document relative to a collection of documents, known as a corpus. The TF-IDF score is computed as the product of two components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "Term Frequency (TF): This measures how often a word appears in a specific document. The assumption here is that words appearing more frequently in a document are more important for understanding that document's content. TF is typically calculated as the number of times a word appears in the document divided by the total number of words in the document.\n",
    "\n",
    "Inverse Document Frequency (IDF): This measures the importance of the word across the entire corpus. Words that appear in many documents are less informative about any single document because they are common. IDF is calculated as the logarithm of the total number of documents divided by the number of documents containing the word.\n",
    "\n",
    "Combining these two metrics, TF-IDF gives a high score to words that are frequent in a specific document but not common across all documents, thus highlighting words that are particularly relevant to that document. This technique is widely used for text classification, information retrieval, and as a feature extraction method in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fef2ef-8abc-4649-80b3-39423a24ec72",
   "metadata": {},
   "source": [
    "Text-Preprocessing :  Text preprocessing is a crucial step in Natural Language Processing (NLP) that prepares raw text for analysis and modeling.Proper preprocessing improves the quality and performance of NLP model by formatting text into vector as machine understanable language  Here are some common text preprocessing techniques:\n",
    "1. Text Cleaning: Remove any unnecessary characters or symbols from the text. For example, get rid of extra spaces or special symbols.\n",
    "2. Tokenization: Break the text into smaller pieces, like words or sentences, so it‚Äôs easier to work with.\n",
    "3. Normalization(lemmatization r stemming ): Make all text lowercase and reduce words to their root forms (e.g., \"running\" to \"run\") to make sure similar words are treated the same.\n",
    "4. Stopword Removal: Remove common, unimportant words like \"and\" or \"the\" that don‚Äôt add much meaning.\n",
    "5. Text Normalization: Fix things like spelling errors and handle special characters or numbers if they‚Äôre not needed.\n",
    "6. N-gram Generation: Group words into sets (like pairs or triples) to understand the context better.\n",
    "7. Named Entity Recognition (NER): Identify and label important names, dates, and places in the text.\n",
    "8. Part-of-Speech Tagging: Tag each word with its grammatical role, like noun or verb, to understand its function in the sentence.\n",
    "9. Vectorization: Convert text into numbers so a computer can understand it. For example, turning words into a list of numbers or vectors.\n",
    "BOW (Bag of words) and TfIdf(Term Frequency-Inverse Document Frequency)\n",
    "TF(t,d)= Total¬†number¬†of¬†terms¬†in¬†document¬†d/Number¬†of¬†times¬†term¬†t¬†appears¬†in¬†document\n",
    "IDF(t,D)=log( Number¬†of¬†documents¬†containing¬†term¬†t/ Total¬†number¬†of¬†documents¬†D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b090e-0abd-4714-a5c3-011ba2c8b7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a9e9d1f-0b2a-4f96-ad1b-b48c60896252",
   "metadata": {},
   "source": [
    "Tell me About ur self \n",
    "Hello, my name is Prajwal Bandi. I recently graduated with a Bachelor of Engineering in Computer Science and Engineering from Sharnsbava University, Kalaburgi, where I achieved a CGPA of 8.7. I am a certified data scientist from ExcelR Solutions Institute, Bangalore. I'm proficiency in data science, with hands-on experience in machine learning model building, data preprocessing techniques, and natural language processing.\n",
    "\n",
    "Presently working as Data Science intern at AI Variant.During my internship at AI Variant, I worked on a sentiment analysis project where I gained real-time problem experience and applied various NLP techniques. This role allowed me to further hone my skills in Python, SQL, and statistics, and deepen my understanding of data analysis methods.\n",
    "\n",
    "Outside of work, I enjoy playing games, watching movies, and thinking deeply about relevant topics. I am a team player and am actively seeking an entry-level data science position where I can contribute effectively and showcase my abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748023cb-dde4-4c82-9b12-91406149f957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cef0884-7d85-4207-ae6b-78b6a6475e94",
   "metadata": {},
   "source": [
    "Explain about ur project : \n",
    "\n",
    "During my internship, I developed a sentiment analysis model to classify user sentiments into positive, negative, and neutral categories based on Amazon product review data. The project involved several key steps:\n",
    "\n",
    "First, I preprocessed the data using various NLP techniques. This included tokenization to break the text into words and sentences, removing stopwords to eliminate common but uninformative words, and stemming to reduce words to their root forms. I also vectorized the text data using TF-IDF (Term Frequency-Inverse Document Frequency) to convert the text into numerical features suitable for analysis.and also performed textblod sentiment polarity for reviewbody data and classified positive negative and neutral based on sentiment values..\n",
    "\n",
    "Next, I applied and compared several classification algorithms to determine the best approach for sentiment classification. These algorithms included Support Vector Machine (SVM), Logistic Regression, Random Forest Classifier, Naive Bayes Classifier, and Decision Trees. By using an ensemble model, which combines multiple algorithms, I was able to enhance the overall performance and achieve higher accuracy.\n",
    "\n",
    "Finally, I deployed the model using Streamlit, creating a web application that enables users to input text and receive real-time sentiment analysis results. This deployment allowed for practical application and user interaction with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62895c61-b7de-43e2-b1ac-fd54ef2e4388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc19ce8-4c74-4b04-a876-bb3249ba3328",
   "metadata": {},
   "source": [
    "The \"Lung Cancer Detection Using CNN\" project focuses on developing a Convolutional Neural Network (CNN) model to accurately identify lung cancer from medical imaging data, specifically chest X-ray images. The project leverages the power of deep learning to enhance diagnostic accuracy and support early detection of lung cancer, which is crucial for improving patient outcomes. The CNN model is designed to process and analyze a dataset of labeled chest X-ray images, where each image is categorized as either cancerous or non-cancerous. The architecture of the CNN includes multiple convolutional layers to extract hierarchical features from the images, followed by pooling layers to reduce spatial dimensions and retain essential information. These features are then fed into fully connected layers to perform classification. The model is trained using a combination of data augmentation techniques to handle variability and improve generalization. Performance is evaluated through metrics such as accuracy, precision, recall, and F1-score, ensuring the model's reliability in distinguishing between affected and unaffected images. The end result is a robust diagnostic tool that can assist radiologists in making more accurate and timely diagnoses, ultimately contributing to better patient management and treatment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d6874-75e5-4391-9f1b-ebb3ff682ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f42a30e-380e-40d6-93c4-a7f6586f6043",
   "metadata": {},
   "source": [
    "Explain about ur Project : \n",
    "In my diabetes prediction project, I developed a model to assess the risk of diabetes based on various health indicators. The process involved several key steps:\n",
    "\n",
    "First, I collected and preprocessed the data, which included features such as age, BMI (Body Mass Index), blood pressure, insulin levels, family history of diabetes, and glucose levels. During preprocessing, I handled missing values, normalized the data to ensure consistency, and performed feature scaling to improve model performance.\n",
    "\n",
    "Next, I built and evaluated several machine learning models to predict diabetes risk. The models included Logistic Regression, Decision Trees, and Random Forests. I also applied techniques like cross-validation and hyperparameter tuning to optimize model performance. By comparing the models‚Äô accuracy, precision, recall, and F1-score, I selected the best-performing model.\n",
    "\n",
    "Finally, I integrated the predictive model into a user-friendly interface for practical use, allowing users to input their health metrics and receive a risk assessment for diabetes. This project aimed to support early detection and intervention, improving patient outcomes by providing actionable insights based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7388d-9d26-45a0-8799-5882b92da482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e00f3557-0836-46e6-8819-5b1bb3bae177",
   "metadata": {},
   "source": [
    "Experience of Data Science Intern : \n",
    "\n",
    "As a Data Science Intern at AI Variant, I worked on a sentiment analysis project where I gained hands-on experience with real-world data problems. My role involved preprocessing data collected from Amazon product reviews in CSV format. I began by performing exploratory data analysis (EDA), which included cleaning the data, handling null values and duplicates, dealing with outliers, and converting categorical variables into numerical ones.\n",
    "\n",
    "I used various NLP techniques to preprocess the data, such as removing stopwords, tokenizing words, stemming (reducing words to their root form), and removing special symbols. I also applied TF-IDF vectorization to convert text into a format that machine learning models can understand. For the modeling phase, I employed classification algorithms within an ensemble method, which resulted in high accuracy. Finally, I deployed the model using a Streamlit application, allowing for interactive user access and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9fd5b-6bf8-4be3-8293-0a48cf3a2b70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdb73c-c4e7-4c64-b789-a957e4a31e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c8453c-69ac-4039-853a-eeab36669951",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
