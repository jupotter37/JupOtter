{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274b05aa",
   "metadata": {
    "papermill": {
     "duration": 0.007221,
     "end_time": "2023-10-14T04:37:35.303133",
     "exception": false,
     "start_time": "2023-10-14T04:37:35.295912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explained Baseline Solution ğŸ’¨\n",
    "\n",
    "## Introduction ğŸŒŸ\n",
    "Welcome to this Jupyter notebook developed for the Google - Fast or Slow? Predict AI Model Runtime! This notebook is designed to help you participate in the competition and to Detect sleep onset and wake from wrist-worn accelerometer data.\n",
    "\n",
    "### Inspiration and Credits ğŸ™Œ\n",
    "This notebook is inspired by the work of Bhukya Satheesh\n",
    ", available at [this Kaggle project](https://www.kaggle.com/code/satheeshbhukya1/google-fast-or-slow/notebook). I extend my gratitude to Bhukya Satheesh\n",
    " for sharing their insights and code.\n",
    "\n",
    "ğŸŒŸ Explore my profile and other public projects, and don't forget to share your feedback! \n",
    "ğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ\n",
    "\n",
    "ğŸ™ Thank you for taking the time to review my work, and please give it a thumbs-up if you found it valuable! ğŸ‘\n",
    "\n",
    "## Purpose ğŸ¯\n",
    "The primary purpose of this notebook is to:\n",
    "- Load and preprocess the competition data ğŸ“\n",
    "- Engineer relevant features for model training ğŸ‹ï¸â€â™‚ï¸\n",
    "- Train predictive models to make target variable predictions ğŸ§ \n",
    "- Submit predictions to the competition environment ğŸ“¤\n",
    "\n",
    "## Notebook Structure ğŸ“š\n",
    "This notebook is structured as follows:\n",
    "1. **Data Preparation**: In this section, we load and preprocess the competition data.\n",
    "2. **Feature Engineering**: We generate and select relevant features for model training.\n",
    "3. **Model Training**: We train machine learning models on the prepared data.\n",
    "4. **Prediction and Submission**: We make predictions on the test data and submit them for evaluation.\n",
    "\n",
    "\n",
    "## How to Use ğŸ› ï¸\n",
    "To use this notebook effectively, please follow these steps:\n",
    "1. Ensure you have the competition data and environment set up.\n",
    "2. Execute each cell sequentially to perform data preparation, feature engineering, model training, and prediction submission.\n",
    "3. Customize and adapt the code as needed to improve model performance or experiment with different approaches.\n",
    "\n",
    "**Note**: Make sure to replace any placeholder paths or configurations with your specific information.\n",
    "\n",
    "## Acknowledgments ğŸ™\n",
    "We acknowledge theChild Mind Institute organizers for providing the dataset and the competition platform.\n",
    "\n",
    "Let's get started! Feel free to reach out if you have any questions or need assistance along the way.\n",
    "ğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e07731",
   "metadata": {
    "papermill": {
     "duration": 0.006666,
     "end_time": "2023-10-14T04:37:35.317099",
     "exception": false,
     "start_time": "2023-10-14T04:37:35.310433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“š Importing necessary libraries ğŸ“Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b18066b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:35.335660Z",
     "iopub.status.busy": "2023-10-14T04:37:35.334428Z",
     "iopub.status.idle": "2023-10-14T04:37:42.448740Z",
     "shell.execute_reply": "2023-10-14T04:37:42.447297Z"
    },
    "papermill": {
     "duration": 7.126601,
     "end_time": "2023-10-14T04:37:42.451843",
     "exception": false,
     "start_time": "2023-10-14T04:37:35.325242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ğŸ“š Importing necessary libraries ğŸ“Š\n",
    "import numpy as np              # NumPy for numerical operations\n",
    "import pandas as pd             # Pandas for data manipulation\n",
    "import plotly.express as px     # Plotly Express for interactive plotting\n",
    "import matplotlib.pyplot as plt # Matplotlib for basic plotting\n",
    "import seaborn as sns           # Seaborn for statistical data visualization\n",
    "import random                   # Random for generating random numbers\n",
    "import os                       # OS for interacting with the operating system\n",
    "import gc                       # Garbage collector for memory management\n",
    "from copy import deepcopy      # Deepcopy for creating deep copies of objects\n",
    "from functools import partial  # Partial function application for function manipulation\n",
    "from itertools import combinations  # Combinations for creating combinations of elements\n",
    "from itertools import groupby  # Groupby for grouping elements in an iterable\n",
    "from tqdm import tqdm          # tqdm for progress bars\n",
    "import polars as pl            # Polars for data manipulation\n",
    "import datetime                # Datetime for date and time operations\n",
    "\n",
    "# ğŸ§± Importing specific functions and classes ğŸ§±\n",
    "from sklearn.model_selection import train_test_split  # Splitting data into training and testing sets\n",
    "from sklearn.model_selection import StratifiedKFold, KFold  # Cross-validation techniques\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, f1_score  # Evaluation metrics\n",
    "from sklearn.model_selection import cross_validate  # Cross-validation scoring\n",
    "from sklearn.metrics import RocCurveDisplay, confusion_matrix, ConfusionMatrixDisplay, precision_score, average_precision_score  # Metrics and displays\n",
    "import optuna  # Library for hyperparameter tuning\n",
    "import xgboost as xgb  # XGBoost for gradient boosting\n",
    "import lightgbm as lgb  # LightGBM for gradient boosting\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Random Forest and Gradient Boosting\n",
    "from sklearn.pipeline import Pipeline  # Pipeline for building a sequence of data transformations\n",
    "from catboost import Pool  # CatBoost for gradient boosting\n",
    "\n",
    "# âš™ï¸ Importing a custom metric function âš™ï¸\n",
    "from metric import score  # Importing a custom event detection AP score function\n",
    "\n",
    "# ğŸ“‹ Define column names and tolerances for the score function ğŸ“‹\n",
    "column_names = {\n",
    "    'series_id_column_name': 'series_id',\n",
    "    'time_column_name': 'step',\n",
    "    'event_column_name': 'event',\n",
    "    'score_column_name': 'score',\n",
    "}\n",
    "\n",
    "tolerances = {\n",
    "    'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360], \n",
    "    'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]\n",
    "}\n",
    "\n",
    "# ğŸ“Š Setting display options for Pandas DataFrames ğŸ“Š\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "# ğŸš« Suppressing warnings ğŸš«\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186312fd",
   "metadata": {
    "papermill": {
     "duration": 0.007113,
     "end_time": "2023-10-14T04:37:42.466623",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.459510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“‚ Importing and transforming data ğŸ”„\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fce2c",
   "metadata": {
    "papermill": {
     "duration": 0.006852,
     "end_time": "2023-10-14T04:37:42.481108",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.474256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "This code will do data transformations and loading data from input directory.\n",
    "\n",
    "1. `dt_transforms = [...]`: This defines a list called `dt_transforms` that will contain a series of transformations to be applied to a timestamp column in the data.\n",
    "\n",
    "2. `pl.col('timestamp').str.to_datetime()`: This line converts a column named 'timestamp' to datetime format using the `str.to_datetime()` method.\n",
    "\n",
    "3. `(pl.col('timestamp').str.to_datetime().dt.year() - 2000).cast(pl.UInt8).alias('year')`: This line extracts the year from the 'timestamp' column, subtracts 2000 from it, casts the result as an 8-bit unsigned integer, and assigns it an alias 'year'.\n",
    "\n",
    "4. `pl.col('timestamp').str.to_datetime().dt.month().cast(pl.UInt8).alias('month')`: Similar to line 3, this line extracts the month from the 'timestamp' column, casts it as an 8-bit unsigned integer, and assigns it an alias 'month'.\n",
    "\n",
    "5. `pl.col('timestamp').str.to_datetime().dt.day().cast(pl.UInt8).alias('day')`: This line extracts the day from the 'timestamp' column, casts it as an 8-bit unsigned integer, and assigns it an alias 'day'.\n",
    "\n",
    "6. `pl.col('timestamp').str.to_datetime().dt.hour().cast(pl.UInt8).alias('hour')`: Similar to lines 3 and 4, this line extracts the hour from the 'timestamp' column, casts it as an 8-bit unsigned integer, and assigns it an alias 'hour'.\n",
    "\n",
    "7. `data_transforms = [...]`: This defines another list called `data_transforms` that will contain transformations for columns other than the timestamp.\n",
    "\n",
    "8. `pl.col('anglez').cast(pl.Int16)`: This line casts the column 'anglez' to a 16-bit signed integer.\n",
    "\n",
    "9. `(pl.col('enmo') * 1000).cast(pl.UInt16)`: This line multiplies the 'enmo' column by 1000 and casts the result as a 16-bit unsigned integer.\n",
    "\n",
    "10. `train_series = pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/train_series.parquet').with_columns(dt_transforms + data_transforms)`: This line reads a parquet file ('train_series.parquet'), applies the transformations defined in `dt_transforms` and `data_transforms`, and assigns the result to the `train_series` variable.\n",
    "\n",
    "11. `train_events = pl.read_csv('/kaggle/input/child-mind-institute-detect-sleep-states/train_events.csv').with_columns(dt_transforms)`: This line reads a CSV file ('train_events.csv'), applies the transformations defined in `dt_transforms`, and assigns the result to the `train_events` variable.\n",
    "\n",
    "12. `test_series = pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet').with_columns(dt_transforms + data_transforms)`: Similar to line 10, this line reads a parquet file ('test_series.parquet'), applies the transformations defined in `dt_transforms` and `data_transforms`, and assigns the result to the `test_series` variable.\n",
    "\n",
    "13. `series_ids = train_events['series_id'].unique(maintain_order=True).to_list()`: This line extracts unique 'series_id' values from the `train_events` DataFrame while maintaining the original order and converts them to a Python list.\n",
    "\n",
    "14. `onset_counts = ...`: These lines calculate the counts of 'onset' events and 'wakeup' events for each 'series_id' and store them in `onset_counts` and `wakeup_counts` DataFrames.\n",
    "\n",
    "15. `counts = pl.DataFrame(...)`: This line creates a DataFrame called `counts` containing 'series_id', 'onset_counts', and 'wakeup_counts' columns by combining the results of the previous step.\n",
    "\n",
    "16. `count_mismatches = counts.filter(...)`: This line filters `counts` to find series where the 'onset_counts' do not match 'wakeup_counts'.\n",
    "\n",
    "17. `train_series = train_series.filter(...)` and `train_events = train_events.filter(...)`: These lines filter the `train_series` and `train_events` DataFrames to exclude series with count mismatches.\n",
    "\n",
    "18. `series_ids = train_events.drop_nulls()['series_id'].unique(maintain_order=True).to_list()`: This line updates the `series_ids` list by removing rows with null values in 'series_id' and extracting unique values while maintaining the original order.\n",
    "\n",
    "This code loads and transforms data from different sources, applies various data transformations, and filters out series with count mismatches in 'onset' and 'wakeup' events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bb6241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.497463Z",
     "iopub.status.busy": "2023-10-14T04:37:42.497103Z",
     "iopub.status.idle": "2023-10-14T04:37:42.772552Z",
     "shell.execute_reply": "2023-10-14T04:37:42.771015Z"
    },
    "papermill": {
     "duration": 0.287156,
     "end_time": "2023-10-14T04:37:42.775503",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.488347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Column transformations for timestamp\n",
    "dt_transforms = [\n",
    "    pl.col('timestamp').str.to_datetime(),  # Convert timestamp to datetime\n",
    "    (pl.col('timestamp').str.to_datetime().dt.year() - 2000).cast(pl.UInt8).alias('year'),  # Extract and cast year\n",
    "    pl.col('timestamp').str.to_datetime().dt.month().cast(pl.UInt8).alias('month'),  # Extract and cast month\n",
    "    pl.col('timestamp').str.to_datetime().dt.day().cast(pl.UInt8).alias('day'),  # Extract and cast day\n",
    "    pl.col('timestamp').str.to_datetime().dt.hour().cast(pl.UInt8).alias('hour')  # Extract and cast hour\n",
    "]\n",
    "\n",
    "# Column transformations for data\n",
    "data_transforms = [\n",
    "    pl.col('anglez').cast(pl.Int16),  # Casting 'anglez' to 16-bit integer\n",
    "    (pl.col('enmo') * 1000).cast(pl.UInt16)  # Convert 'enmo' to 16-bit unsigned integer\n",
    "]\n",
    "\n",
    "# Loading and transforming training series data\n",
    "train_series = pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/train_series.parquet').with_columns(\n",
    "    dt_transforms + data_transforms\n",
    ")\n",
    "\n",
    "\n",
    "# Loading and transforming training events data\n",
    "train_events = pl.read_csv('/kaggle/input/child-mind-institute-detect-sleep-states/train_events.csv').with_columns(\n",
    "    dt_transforms\n",
    ")\n",
    "\n",
    "# Loading and transforming test series data\n",
    "test_series = pl.scan_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet').with_columns(\n",
    "    dt_transforms + data_transforms\n",
    ")\n",
    "\n",
    "# Getting unique series IDs for convenience\n",
    "series_ids = train_events['series_id'].unique(maintain_order=True).to_list()\n",
    "\n",
    "# Removing series with mismatched event counts (onset vs. wakeup)\n",
    "onset_counts = train_events.filter(pl.col('event') == 'onset').group_by('series_id').count().sort('series_id')['count']\n",
    "wakeup_counts = train_events.filter(pl.col('event') == 'wakeup').group_by('series_id').count().sort('series_id')['count']\n",
    "\n",
    "counts = pl.DataFrame({'series_id': sorted(series_ids), 'onset_counts': onset_counts, 'wakeup_counts': wakeup_counts})\n",
    "count_mismatches = counts.filter(counts['onset_counts'] != counts['wakeup_counts'])\n",
    "\n",
    "# Filtering out series with count mismatches\n",
    "train_series = train_series.filter(~pl.col('series_id').is_in(count_mismatches['series_id']))\n",
    "train_events = train_events.filter(~pl.col('series_id').is_in(count_mismatches['series_id']))\n",
    "\n",
    "# Updating the list of series IDs, excluding series with no non-null values\n",
    "series_ids = train_events.drop_nulls()['series_id'].unique(maintain_order=True).to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442f619",
   "metadata": {
    "papermill": {
     "duration": 0.00696,
     "end_time": "2023-10-14T04:37:42.790035",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.783075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ§® Creating Features ğŸ“ˆ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6134322",
   "metadata": {
    "papermill": {
     "duration": 0.006629,
     "end_time": "2023-10-14T04:37:42.803811",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.797182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "This code is focused on generating and adding various features to two DataFrames (`train_series` and `test_series`) based on different time windows and data transformations.\n",
    "\n",
    "1. `features, feature_cols = [pl.col('hour')], ['hour']`: This line initializes two lists, `features` and `feature_cols`. It starts with one feature, which is the 'hour' column, and one feature column name 'hour'.\n",
    "\n",
    "2. `for mins in [5, 30, 60*2, 60*8]:`: This is the beginning of a loop that iterates over different time window durations specified in minutes (5, 30, 120, and 480).\n",
    "\n",
    "3. `features += [...]`: This line appends new features to the `features` list. For each time window duration (`mins`), it calculates the following features for the 'enmo' column:\n",
    "   - Rolling mean with a window of 12 times the specified minutes (`12 * mins`) using `pl.col('enmo').rolling_mean(...)`.\n",
    "   - Rolling maximum with the same window using `pl.col('enmo').rolling_max(...)`.\n",
    "   It then casts these features to 16-bit unsigned integers and assigns aliases with the format 'enmo_Xm_mean' and 'enmo_Xm_max', where X is the time window duration in minutes.\n",
    "\n",
    "4. `feature_cols += [...]`: This line appends the corresponding feature column names to the `feature_cols` list, which are 'enmo_Xm_mean' and 'enmo_Xm_max' for each time window duration.\n",
    "\n",
    "5. The same steps are repeated for the 'anglez' column, resulting in features named 'anglez_Xm_mean' and 'anglez_Xm_max' for each time window duration.\n",
    "\n",
    "6. `id_cols = ['series_id', 'step', 'timestamp']`: This line defines a list of columns to keep in the final DataFrame. These columns are 'series_id', 'step', and 'timestamp'.\n",
    "\n",
    "7. `train_series = train_series.with_columns(features).select(id_cols + feature_cols)`: This line adds the calculated features to the `train_series` DataFrame using the `with_columns` method and then selects only the columns specified in `id_cols` and `feature_cols`. This effectively updates `train_series` with the newly generated features.\n",
    "\n",
    "8. `test_series = test_series.with_columns(features).select(id_cols + feature_cols)`: Similar to the previous line, this line adds the same calculated features to the `test_series` DataFrame and selects the specified columns.\n",
    "\n",
    "This code generates features based on rolling statistics (mean and max) for both the 'enmo' and 'anglez' columns, with different time window durations. It then updates the `train_series` and `test_series` DataFrames with these calculated features while keeping the specified identifier and feature columns. This process is common in feature engineering for machine learning tasks, where additional features are created from existing data to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d5991f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.819517Z",
     "iopub.status.busy": "2023-10-14T04:37:42.819083Z",
     "iopub.status.idle": "2023-10-14T04:37:42.832464Z",
     "shell.execute_reply": "2023-10-14T04:37:42.831392Z"
    },
    "papermill": {
     "duration": 0.023936,
     "end_time": "2023-10-14T04:37:42.834597",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.810661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing features and feature column names\n",
    "features, feature_cols = [pl.col('hour')], ['hour']\n",
    "\n",
    "# Generating features for different time windows\n",
    "for mins in [5, 30, 60*2, 60*8]:\n",
    "    # Enmo rolling mean and max\n",
    "    features += [\n",
    "        pl.col('enmo').rolling_mean(12 * mins, center=True, min_periods=1).abs().cast(pl.UInt16).alias(f'enmo_{mins}m_mean'),\n",
    "        pl.col('enmo').rolling_max(12 * mins, center=True, min_periods=1).abs().cast(pl.UInt16).alias(f'enmo_{mins}m_max')\n",
    "    ]\n",
    "\n",
    "    feature_cols += [\n",
    "        f'enmo_{mins}m_mean', f'enmo_{mins}m_max'\n",
    "    ]\n",
    "\n",
    "    # Anglez and Enmo first variations\n",
    "    for var in ['enmo', 'anglez']:\n",
    "        features += [\n",
    "            (pl.col(var).diff().abs().rolling_mean(12 * mins, center=True, min_periods=1) * 10).abs().cast(pl.UInt32).alias(f'{var}_1v_{mins}m_mean'),\n",
    "            (pl.col(var).diff().abs().rolling_max(12 * mins, center=True, min_periods=1) * 10).abs().cast(pl.UInt32).alias(f'{var}_1v_{mins}m_max')\n",
    "        ]\n",
    "\n",
    "        feature_cols += [\n",
    "            f'{var}_1v_{mins}m_mean', f'{var}_1v_{mins}m_max'\n",
    "        ]\n",
    "\n",
    "# Defining columns to keep\n",
    "id_cols = ['series_id', 'step', 'timestamp']\n",
    "\n",
    "# Adding calculated features to the training and testing series\n",
    "train_series = train_series.with_columns(\n",
    "    features\n",
    ").select(id_cols + feature_cols)\n",
    "\n",
    "test_series = test_series.with_columns(\n",
    "    features\n",
    ").select(id_cols + feature_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c2259",
   "metadata": {
    "papermill": {
     "duration": 0.006442,
     "end_time": "2023-10-14T04:37:42.848185",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.841743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“Š Creating Training Dataset Function ğŸ“‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f7df7",
   "metadata": {
    "papermill": {
     "duration": 0.006656,
     "end_time": "2023-10-14T04:37:42.861544",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.854888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination** :\n",
    "\n",
    "This Python function, `make_train_dataset`, is designed to create a training dataset from raw data, which includes both feature data (`train_data`) and event information (`train_events`). It returns feature data `X` and target labels `y` for training our machine learning model.:\n",
    "\n",
    "1. `series_ids = train_data['series_id'].unique(maintain_order=True).to_list()`: This line extracts unique 'series_id' values from the `train_data` DataFrame while maintaining the original order and converts them to a Python list. These unique series identifiers will be used to process data for each individual series.\n",
    "\n",
    "2. `X, y = pl.DataFrame(), pl.DataFrame()`: This line initializes two empty DataFrames, `X` for features and `y` for target labels.\n",
    "\n",
    "3. `for idx in tqdm(series_ids):`: This loop iterates through each unique 'series_id' in the `series_ids` list, using `tqdm` to display a progress bar.\n",
    "\n",
    "4. `sample = train_data.filter(...)`: Within the loop, this line filters the `train_data` DataFrame to obtain the data for the current 'series_id'. It then normalizes the selected columns (except 'hour') by dividing each column by its standard deviation and casting the result as a 32-bit floating-point number (Float32).\n",
    "\n",
    "5. `events = train_events.filter(...)`: This line filters the `train_events` DataFrame to obtain the events data for the current 'series_id'.\n",
    "\n",
    "6. `if drop_nulls: ...`: If the `drop_nulls` parameter is set to `True`, this section removes data points in the `sample` DataFrame that correspond to dates where no events were recorded. It does this by filtering the `sample` DataFrame based on matching dates between `sample` and `events` using the 'timestamp' column.\n",
    "\n",
    "7. `X = X.vstack(...)`: This line vertically stacks the current 'sample' DataFrame onto the existing 'X' DataFrame, combining the features for multiple series.\n",
    "\n",
    "8. `onsets = events.filter(...)`: This line extracts the 'step' values corresponding to 'onset' events from the 'events' DataFrame and stores them in the `onsets` list.\n",
    "\n",
    "9. `wakeups = events.filter(...)`: Similar to the previous line, this extracts the 'step' values corresponding to 'wakeup' events and stores them in the `wakeups` list.\n",
    "\n",
    "10. `y = y.vstack(...)`: This line constructs target labels ('asleep') based on the 'onsets' and 'wakeups' values. It checks for each data point in 'sample' if it falls within any sleep interval defined by 'onset' and 'wakeup' events, and casts the result as a Boolean value. The resulting labels are stacked vertically onto the 'y' DataFrame.\n",
    "\n",
    "11. `y = y.to_numpy().ravel()`: After processing all series, this line converts the 'y' DataFrame to a NumPy array and flattens it to a 1D array. This is typically done to match the format expected by many machine learning models.\n",
    "\n",
    "12. Finally, the function returns `X` (the feature matrix) and `y` (the target labels) as the output.\n",
    "\n",
    "This function takes raw feature data and event information, processes them for each series, and creates a dataset suitable for training machine learning models. It provides a way to drop null data points if desired and generates target labels based on sleep event information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96ff1ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.877334Z",
     "iopub.status.busy": "2023-10-14T04:37:42.876454Z",
     "iopub.status.idle": "2023-10-14T04:37:42.888840Z",
     "shell.execute_reply": "2023-10-14T04:37:42.887368Z"
    },
    "papermill": {
     "duration": 0.023338,
     "end_time": "2023-10-14T04:37:42.891442",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.868104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_dataset(train_data, train_events, drop_nulls=False):\n",
    "    \"\"\"\n",
    "    Create a training dataset from raw data.\n",
    "\n",
    "    Args:\n",
    "        train_data (pl.DataFrame): Raw training data with features.\n",
    "        train_events (pl.DataFrame): Training events data with event information.\n",
    "        drop_nulls (bool): Whether to drop null data points where no events were recorded.\n",
    "\n",
    "    Returns:\n",
    "        X (pl.DataFrame): Features for training.\n",
    "        y (numpy.ndarray): Target labels for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    series_ids = train_data['series_id'].unique(maintain_order=True).to_list()\n",
    "    X, y = pl.DataFrame(), pl.DataFrame()\n",
    "    \n",
    "    for idx in tqdm(series_ids):\n",
    "        # Normalizing sample features\n",
    "        sample = train_data.filter(pl.col('series_id') == idx).with_columns(\n",
    "            [(pl.col(col) / pl.col(col).std()).cast(pl.Float32) for col in feature_cols if col != 'hour']\n",
    "        )\n",
    "        \n",
    "        events = train_events.filter(pl.col('series_id') == idx)\n",
    "        \n",
    "        if drop_nulls:\n",
    "            # Removing datapoints on dates where no data was recorded\n",
    "            sample = sample.filter(\n",
    "                pl.col('timestamp').dt.date().is_in(events['timestamp'].dt.date())\n",
    "            )\n",
    "        \n",
    "        X = X.vstack(sample[id_cols + feature_cols])\n",
    "\n",
    "        onsets = events.filter((pl.col('event') == 'onset') & (pl.col('step') != None))['step'].to_list()\n",
    "        wakeups = events.filter((pl.col('event') == 'wakeup') & (pl.col('step') != None))['step'].to_list()\n",
    "\n",
    "        # NOTE: This will break if there are event series without any recorded onsets or wakeups\n",
    "        y = y.vstack(sample.with_columns(\n",
    "            sum([(onset <= pl.col('step')) & (pl.col('step') <= wakeup) for onset, wakeup in zip(onsets, wakeups)]).cast(pl.Boolean).alias('asleep')\n",
    "            ).select('asleep')\n",
    "        )\n",
    "    \n",
    "    y = y.to_numpy().ravel()\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac99d6",
   "metadata": {
    "papermill": {
     "duration": 0.006708,
     "end_time": "2023-10-14T04:37:42.905116",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.898408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ•’ Get Events Function ğŸŒ™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d8c48",
   "metadata": {
    "papermill": {
     "duration": 0.006834,
     "end_time": "2023-10-14T04:37:42.918790",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.911956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "This Python function, `get_events`, is designed to generate sleep event predictions and format them as a submission DataFrame. It takes a time series dataset (`series`) with features and a trained classifier (`classifier`) for predicting sleep events. \n",
    "\n",
    "1. `series_ids = series['series_id'].unique(maintain_order=True).to_list()`: This line extracts unique 'series_id' values from the `series` DataFrame while maintaining the original order and converts them to a Python list. These unique series identifiers will be used to process data for each individual series.\n",
    "\n",
    "2. `events = pl.DataFrame(...)`: This line initializes an empty DataFrame called `events` with a specific schema containing columns: 'series_id' (string), 'step' (integer), 'event' (string), and 'score' (float). This DataFrame will store sleep event predictions.\n",
    "\n",
    "3. `for idx in tqdm(series_ids):`: This loop iterates through each unique 'series_id' in the `series_ids` list, using `tqdm` to display a progress bar.\n",
    "\n",
    "4. `scale_cols = [...]`: This line identifies the columns in the `feature_cols` list (excluding 'hour') that have a standard deviation not equal to zero. These columns will be used for feature scaling later.\n",
    "\n",
    "5. `X = series.filter(...)`: Within the loop, this line filters the `series` DataFrame to obtain data for the current 'series_id'. It selects the columns specified in `id_cols` and `feature_cols`. The selected columns are then normalized by dividing them by their standard deviation and casting the result as a 32-bit floating-point number (Float32).\n",
    "\n",
    "6. `preds, probs = ...`: This line applies the trained classifier (`classifier`) to predict sleep events and obtain corresponding probabilities for each data point in the 'X' DataFrame.\n",
    "\n",
    "7. `X = X.with_columns(...)`: This code adds two new columns to the 'X' DataFrame:\n",
    "   - 'prediction': It contains the integer predictions (0 or 1) for sleep events.\n",
    "   - 'probability': It contains the predicted probabilities of being in a sleep state.\n",
    "\n",
    "8. `pred_onsets = X.filter(...)`, `pred_wakeups = X.filter(...)`: These lines extract predicted 'onset' and 'wakeup' time steps based on changes in the 'prediction' column. These steps are converted to Python lists.\n",
    "\n",
    "9. `if len(pred_onsets) > 0: ...`: This conditional block checks if there are any predicted sleep events ('onset' and 'wakeup' pairs). If there are, it proceeds to process and filter these events.\n",
    "\n",
    "10. Inside the conditional block, it ensures that predicted sleep periods have a minimum duration of 30 minutes (12 * 30 time steps).\n",
    "\n",
    "11. It calculates a 'score' for each sleep period by taking the mean probability over that period.\n",
    "\n",
    "12. It then adds sleep events to the `events` DataFrame, including 'onset' and 'wakeup' events, along with their corresponding 'score'.\n",
    "\n",
    "13. The loop continues to process each series, accumulating sleep events in the `events` DataFrame.\n",
    "\n",
    "14. After processing all series, it converts the `events` DataFrame to a Pandas DataFrame, resets the index, and renames the index column to 'row_id'.\n",
    "\n",
    "15. Finally, the function returns the formatted sleep events as a Pandas DataFrame.\n",
    "\n",
    "This function takes a time series dataset and a trained classifier, predicts sleep events, filters and processes them, and returns the results as a Pandas DataFrame suitable for submission or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3715d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.935190Z",
     "iopub.status.busy": "2023-10-14T04:37:42.934729Z",
     "iopub.status.idle": "2023-10-14T04:37:42.949147Z",
     "shell.execute_reply": "2023-10-14T04:37:42.947547Z"
    },
    "papermill": {
     "duration": 0.026168,
     "end_time": "2023-10-14T04:37:42.951773",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.925605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_events(series, classifier):\n",
    "    \"\"\"\n",
    "    Takes a time series and a classifier and returns a formatted submission dataframe.\n",
    "\n",
    "    Args:\n",
    "        series (pl.DataFrame): Time series data with features.\n",
    "        classifier: A trained classifier for predicting sleep events.\n",
    "\n",
    "    Returns:\n",
    "        events (pd.DataFrame): Formatted submission dataframe with sleep events.\n",
    "    \"\"\"\n",
    "    \n",
    "    series_ids = series['series_id'].unique(maintain_order=True).to_list()\n",
    "    events = pl.DataFrame(schema={'series_id': str, 'step': int, 'event': str, 'score': float})\n",
    "\n",
    "    for idx in tqdm(series_ids):\n",
    "        # Collecting sample and normalizing features\n",
    "        scale_cols = [col for col in feature_cols if (col != 'hour') & (series[col].std() != 0)]\n",
    "        X = series.filter(pl.col('series_id') == idx).select(id_cols + feature_cols).with_columns(\n",
    "            [(pl.col(col) / series[col].std()).cast(pl.Float32) for col in scale_cols]\n",
    "        )\n",
    "\n",
    "        # Applying classifier to get predictions and scores\n",
    "        preds, probs = classifier.predict(X[feature_cols]), classifier.predict_proba(X[feature_cols])[:, 1]\n",
    "\n",
    "        # NOTE: Considered using rolling max to get sleep periods excluding <30 min interruptions,\n",
    "        # but ended up decreasing performance\n",
    "        X = X.with_columns(\n",
    "            pl.lit(preds).cast(pl.Int8).alias('prediction'),\n",
    "            pl.lit(probs).alias('probability')\n",
    "        )\n",
    "        \n",
    "        # Getting predicted onset and wakeup time steps\n",
    "        pred_onsets = X.filter(X['prediction'].diff() > 0)['step'].to_list()\n",
    "        pred_wakeups = X.filter(X['prediction'].diff() < 0)['step'].to_list()\n",
    "        \n",
    "        if len(pred_onsets) > 0:\n",
    "            # Ensuring all predicted sleep periods begin and end\n",
    "            if min(pred_wakeups) < min(pred_onsets):\n",
    "                pred_wakeups = pred_wakeups[1:]\n",
    "\n",
    "            if max(pred_onsets) > max(pred_wakeups):\n",
    "                pred_onsets = pred_onsets[:-1]\n",
    "\n",
    "            # Keeping sleep periods longer than 30 minutes\n",
    "            sleep_periods = [(onset, wakeup) for onset, wakeup in zip(pred_onsets, pred_wakeups) if wakeup - onset >= 12 * 30]\n",
    "\n",
    "            for onset, wakeup in sleep_periods:\n",
    "                # Scoring using mean probability over period\n",
    "                score = X.filter((pl.col('step') >= onset) & (pl.col('step') <= wakeup))['probability'].mean()\n",
    "\n",
    "                # Adding sleep event to dataframe\n",
    "                events = events.vstack(pl.DataFrame().with_columns(\n",
    "                    pl.Series([idx, idx]).alias('series_id'),\n",
    "                    pl.Series([onset, wakeup]).alias('step'),\n",
    "                    pl.Series(['onset', 'wakeup']).alias('event'),\n",
    "                    pl.Series([score, score]).alias('score')\n",
    "                ))\n",
    "\n",
    "    # Adding row id column and converting to a pandas DataFrame\n",
    "    events = events.to_pandas().reset_index().rename(columns={'index': 'row_id'})\n",
    "\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8542e64d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.968370Z",
     "iopub.status.busy": "2023-10-14T04:37:42.967614Z",
     "iopub.status.idle": "2023-10-14T04:37:42.977972Z",
     "shell.execute_reply": "2023-10-14T04:37:42.977085Z"
    },
    "papermill": {
     "duration": 0.021629,
     "end_time": "2023-10-14T04:37:42.980192",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.958563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \n",
    "    \"\"\" \n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and not is_datetime64_ns_dtype(df[col]):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        \n",
    "\n",
    "    df['series_id'] = df['series_id'].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f'Decreased by {decrease:.2f}%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962939fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:42.996716Z",
     "iopub.status.busy": "2023-10-14T04:37:42.995894Z",
     "iopub.status.idle": "2023-10-14T04:37:43.000757Z",
     "shell.execute_reply": "2023-10-14T04:37:42.999559Z"
    },
    "papermill": {
     "duration": 0.016252,
     "end_time": "2023-10-14T04:37:43.003583",
     "exception": false,
     "start_time": "2023-10-14T04:37:42.987331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduce_mem_usage(train_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469f964",
   "metadata": {
    "papermill": {
     "duration": 0.006427,
     "end_time": "2023-10-14T04:37:43.022173",
     "exception": false,
     "start_time": "2023-10-14T04:37:43.015746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“Š Collecting and Sampling Data ğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b56e1f",
   "metadata": {
    "papermill": {
     "duration": 0.006245,
     "end_time": "2023-10-14T04:37:43.035171",
     "exception": false,
     "start_time": "2023-10-14T04:37:43.028926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "This line of code is selecting and collecting a subset of data from the `train_series` DataFrame for further processing.\n",
    "\n",
    "1. `train_data = ...`: This part of the code initializes a variable named `train_data` to store the selected data points.\n",
    "\n",
    "2. `train_series.filter(...)`: Here, the `train_series` DataFrame is filtered using the `filter` method. The condition being applied is `pl.col('series_id').is_in(series_ids)`. This condition filters the rows in `train_series` to include only those where the 'series_id' column matches any of the values in the `series_ids` list.\n",
    "\n",
    "3. `.collect()`: After filtering, the `collect` method is used to retrieve all the rows that satisfy the condition and create a new DataFrame containing these rows. This step essentially materializes the filtered DataFrame.\n",
    "\n",
    "4. `.sample(int(1e6))`: Finally, the `sample` method is applied to the collected DataFrame. It randomly selects a specified number of samples from the DataFrame. In this case, it's selecting 1 million samples (`int(1e6)` represents 1 million). The purpose of taking a random sample is often to reduce the size of the data for quicker experimentation or analysis when working with a large dataset.\n",
    "\n",
    "So, the overall purpose of this line of code is to filter the `train_series` DataFrame to include only rows where the 'series_id' matches any of the values in the `series_ids` list, and then randomly sample 1 million rows from this filtered data. This can be useful when you want to work with a manageable subset of data for model training or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dc44de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:37:43.050746Z",
     "iopub.status.busy": "2023-10-14T04:37:43.050244Z",
     "iopub.status.idle": "2023-10-14T04:44:15.177015Z",
     "shell.execute_reply": "2023-10-14T04:44:15.175671Z"
    },
    "papermill": {
     "duration": 392.14793,
     "end_time": "2023-10-14T04:44:15.189775",
     "exception": false,
     "start_time": "2023-10-14T04:37:43.041845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# We will collect datapoints and take 1 million samples\\ntrain_data = train_series.filter(pl.col('series_id').is_in(series_ids)).collect().sample(int(1e6))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids, val_ids = train_test_split(series_ids, train_size=0.7, random_state=42, shuffle = False)\n",
    "\n",
    "# We will collect datapoints at 5 minute intervals for training for validating\n",
    "# train_data = train_series.filter(pl.col('series_id').is_in(train_ids)).take_every(12 * 5).collect()\n",
    "\n",
    "# train_data = train_series.filter(pl.col('series_id').is_in(train_ids)).collect()\n",
    "\n",
    "train_data = train_series.filter(pl.col('series_id').is_in(series_ids)).collect().sample(int(1e7))\n",
    "\n",
    "val_data = train_series.filter(pl.col('series_id').is_in(val_ids)).collect()\n",
    "val_solution = train_events.filter(pl.col('series_id').is_in(val_ids)).select(['series_id', 'event', 'step']).to_pandas()\n",
    "'''\n",
    "# We will collect datapoints and take 1 million samples\n",
    "train_data = train_series.filter(pl.col('series_id').is_in(series_ids)).collect().sample(int(1e6))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c10b97",
   "metadata": {
    "papermill": {
     "duration": 0.006838,
     "end_time": "2023-10-14T04:44:15.204068",
     "exception": false,
     "start_time": "2023-10-14T04:44:15.197230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸš‚ Creating Training Dataset ğŸš‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d54044",
   "metadata": {
    "papermill": {
     "duration": 0.006875,
     "end_time": "2023-10-14T04:44:15.218299",
     "exception": false,
     "start_time": "2023-10-14T04:44:15.211424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "This code is using a custom function called `make_train_dataset` to generate a training dataset from two data sources: `train_data` and `train_events`. It then assigns the resulting features and labels to `X_train` and `y_train`, respectively.\n",
    "\n",
    "1. `X_train, y_train = ...`: This line initializes two variables, `X_train` and `y_train`, which will store the features and target labels for the training dataset, respectively.\n",
    "\n",
    "2. `make_train_dataset(train_data, train_events)`: This is a function call to `make_train_dataset` with two arguments: `train_data` and `train_events`. It's invoking the function and passing these two data sources to it.\n",
    "\n",
    "3. `make_train_dataset` is presumably a custom function defined elsewhere in the code (not shown here). This function is designed to create a training dataset from raw data. Based on the function signature and comments provided earlier, it takes the following arguments:\n",
    "   - `train_data`: A DataFrame containing raw training data with features.\n",
    "   - `train_events`: A DataFrame containing training events data with event information.\n",
    "   - `drop_nulls`: An optional boolean parameter that determines whether to drop null data points where no events were recorded (this parameter is not explicitly passed in the code snippet you provided).\n",
    "\n",
    "4. The function returns two values: `X` and `y`, where:\n",
    "   - `X` is a DataFrame containing features for training.\n",
    "   - `y` is a NumPy array containing the target labels for training.\n",
    "\n",
    "5. The line `X_train, y_train = make_train_dataset(train_data, train_events)` assigns the returned values from the `make_train_dataset` function to `X_train` and `y_train`, respectively. This essentially populates `X_train` with the features and `y_train` with the corresponding target labels for training.\n",
    "\n",
    "In summary, this line of code is invoking a custom function, `make_train_dataset`, to generate a training dataset from `train_data` and `train_events`, and then assigns the resulting features and labels to `X_train` and `y_train`. This is a common step in preparing data for machine learning, where the data is split into features (X) and labels (y) for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35af1783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:44:15.237278Z",
     "iopub.status.busy": "2023-10-14T04:44:15.236317Z",
     "iopub.status.idle": "2023-10-14T04:44:50.234800Z",
     "shell.execute_reply": "2023-10-14T04:44:50.233512Z"
    },
    "papermill": {
     "duration": 35.0114,
     "end_time": "2023-10-14T04:44:50.237099",
     "exception": false,
     "start_time": "2023-10-14T04:44:15.225699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269/269 [00:34<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating training dataset using the 'make_train_dataset' function\n",
    "X_train, y_train = make_train_dataset(train_data, train_events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e0d97",
   "metadata": {
    "papermill": {
     "duration": 0.022019,
     "end_time": "2023-10-14T04:44:50.388104",
     "exception": false,
     "start_time": "2023-10-14T04:44:50.366085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ§¹ Recovering Memory ğŸ§¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bfb09",
   "metadata": {
    "papermill": {
     "duration": 0.022155,
     "end_time": "2023-10-14T04:44:50.434319",
     "exception": false,
     "start_time": "2023-10-14T04:44:50.412164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "This code is responsible for removing the `train_data` variable from memory and then explicitly triggering garbage collection to free up any associated memory resources.:\n",
    "\n",
    "1. `del train_data`: The `del` statement is used in Python to delete a reference to an object. In this case, it is deleting the reference to the `train_data` variable. This means that the variable `train_data` will no longer exist in the current scope, and any memory associated with it will become eligible for garbage collection.\n",
    "\n",
    "2. `gc.collect()`: The `gc` module in Python provides an interface to the garbage collection mechanism. Calling `gc.collect()` explicitly triggers garbage collection. Garbage collection is the process by which Python automatically reclaims memory that is no longer being used by the program, such as objects that have no remaining references. By invoking `gc.collect()`, the program is requesting that the Python garbage collector run immediately to reclaim any memory that can be freed.\n",
    "\n",
    "In summary, this code snippet is cleaning up memory by removing the reference to the `train_data` variable and then explicitly requesting garbage collection to release any memory resources associated with it. This can be useful when working with large datasets to ensure that memory is managed efficiently and that resources are released when they are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0823942e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:44:50.482391Z",
     "iopub.status.busy": "2023-10-14T04:44:50.481382Z",
     "iopub.status.idle": "2023-10-14T04:44:50.923270Z",
     "shell.execute_reply": "2023-10-14T04:44:50.922020Z"
    },
    "papermill": {
     "duration": 0.46948,
     "end_time": "2023-10-14T04:44:50.926083",
     "exception": false,
     "start_time": "2023-10-14T04:44:50.456603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the 'train_data' variable and triggering garbage collection to free up memory\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939f4cb",
   "metadata": {
    "papermill": {
     "duration": 0.02486,
     "end_time": "2023-10-14T04:44:50.973995",
     "exception": false,
     "start_time": "2023-10-14T04:44:50.949135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and validating random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e53078a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:44:51.034702Z",
     "iopub.status.busy": "2023-10-14T04:44:51.034257Z",
     "iopub.status.idle": "2023-10-14T04:44:51.039936Z",
     "shell.execute_reply": "2023-10-14T04:44:51.038727Z"
    },
    "papermill": {
     "duration": 0.043589,
     "end_time": "2023-10-14T04:44:51.042112",
     "exception": false,
     "start_time": "2023-10-14T04:44:50.998523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # parameter searching by Optuna\n",
    "# def objective(trial):\n",
    "#     # Define the hyperparameters to search over\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 100, 1000, step=100)\n",
    "#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 100, 500)\n",
    "#     max_depth = trial.suggest_int('max_depth', 5, 20)  # Add max_depth as a hyperparameter\n",
    "\n",
    "#     # Create and train the random forest classifier\n",
    "#     rf_classifier = RandomForestClassifier(\n",
    "#         n_estimators=n_estimators,\n",
    "#         min_samples_leaf=min_samples_leaf,\n",
    "#         max_depth=max_depth,  # Use the suggested max_depth value\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     rf_classifier.fit(X_train[feature_cols], y_train)\n",
    "\n",
    "#     # Make predictions on the validation set\n",
    "#     y_pred = rf_classifier.predict(X_val[feature_cols])\n",
    "\n",
    "#     # Calculate the F1 score as the objective to minimize\n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#     return 1 - f1  # Optuna minimizes, so we return 1 - f1 to minimize F1 score\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')  # We want to minimize the F1 score\n",
    "# study.optimize(objective, n_trials=100)  # You can adjust the number of trials as needed\n",
    "\n",
    "# best_params = study.best_params\n",
    "# best_score = 1 - study.best_value  # Convert back to F1 score\n",
    "\n",
    "# print(\"Best Hyperparameters:\", best_params)\n",
    "# print(\"Best F1 Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e507c61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:44:51.089104Z",
     "iopub.status.busy": "2023-10-14T04:44:51.088170Z",
     "iopub.status.idle": "2023-10-14T04:44:51.093174Z",
     "shell.execute_reply": "2023-10-14T04:44:51.092337Z"
    },
    "papermill": {
     "duration": 0.031136,
     "end_time": "2023-10-14T04:44:51.095504",
     "exception": false,
     "start_time": "2023-10-14T04:44:51.064368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plotting feature importances\n",
    "# px.bar(x=feature_cols, \n",
    "#        y=rf_classifier.feature_importances_,\n",
    "#        title='Random forest feature importances'\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29869825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T04:44:51.144345Z",
     "iopub.status.busy": "2023-10-14T04:44:51.143692Z",
     "iopub.status.idle": "2023-10-14T05:43:45.508156Z",
     "shell.execute_reply": "2023-10-14T05:43:45.506445Z"
    },
    "papermill": {
     "duration": 3534.418394,
     "end_time": "2023-10-14T05:43:45.537404",
     "exception": false,
     "start_time": "2023-10-14T04:44:51.119010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=19, min_samples_leaf=122, n_jobs=-1,\n",
       "                       random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=19, min_samples_leaf=122, n_jobs=-1,\n",
       "                       random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=19, min_samples_leaf=122, n_jobs=-1,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for 1mln and 10 minutes:\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=700,\n",
    "#                                     min_samples_leaf=101,\n",
    "#                                     random_state=42,\n",
    "#                                     n_jobs=-1)\n",
    "\n",
    "#RF with hyperparameters from Optuna\n",
    "\n",
    "# Random Forest with hyperparameters from Optuna\n",
    "# rf_classifier = RandomForestClassifier(best_params)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,\n",
    "                                    min_samples_leaf=122,\n",
    "                                    max_depth = 19,\n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "rf_classifier.fit(X_train[feature_cols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522b7a83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T05:43:45.584678Z",
     "iopub.status.busy": "2023-10-14T05:43:45.584157Z",
     "iopub.status.idle": "2023-10-14T06:17:45.209511Z",
     "shell.execute_reply": "2023-10-14T06:17:45.207449Z"
    },
    "papermill": {
     "duration": 2039.680097,
     "end_time": "2023-10-14T06:17:45.240130",
     "exception": false,
     "start_time": "2023-10-14T05:43:45.560033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [33:34<00:00, 24.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score: 0.2587968694553282\n"
     ]
    }
   ],
   "source": [
    "# Checking performance on validation set\n",
    "rf_submission = get_events(val_data, rf_classifier)\n",
    "\n",
    "print(f\"Random forest score: {score(val_solution, rf_submission, tolerances, **column_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c7389",
   "metadata": {
    "papermill": {
     "duration": 0.02751,
     "end_time": "2023-10-14T06:17:45.296168",
     "exception": false,
     "start_time": "2023-10-14T06:17:45.268658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ğŸ“ Getting Event Predictions and Saving Submission ğŸ“„\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e71d8",
   "metadata": {
    "papermill": {
     "duration": 0.027205,
     "end_time": "2023-10-14T06:17:45.350005",
     "exception": false,
     "start_time": "2023-10-14T06:17:45.322800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "This code snippet is responsible for generating sleep event predictions for the test set using a trained LightGBM classifier (`my_classifier`) and saving the predictions to a CSV file.\n",
    "\n",
    "1. `submission = get_events(test_series.collect(), my_classifier)`: This line calls a function called `get_events` to obtain sleep event predictions for the test set. Here's what each part of the code does:\n",
    "   - `test_series.collect()`: The `collect` method is used to gather all the data points from the `test_series` DataFrame. This step effectively retrieves all the test data for processing.\n",
    "   - `my_classifier`: This is the trained LightGBM classifier that will be used to predict sleep events based on the test data.\n",
    "\n",
    "   The `get_events` function presumably takes this collected test data and the classifier as input and returns a formatted DataFrame containing sleep event predictions.\n",
    "\n",
    "2. `submission.to_csv('submission.csv', index=False)`: This line saves the DataFrame named `submission` to a CSV file named 'submission.csv'. Here's what each part of the code does:\n",
    "   - `submission`: This is the DataFrame containing the sleep event predictions generated using the test data.\n",
    "   - `.to_csv(...)`: This is a method call on the DataFrame that exports the data to a CSV file. The parameters are as follows:\n",
    "      - `'submission.csv'`: This specifies the name of the CSV file to which the DataFrame will be saved.\n",
    "      - `index=False`: This parameter indicates that the DataFrame's index (row numbers) should not be included in the CSV file. When `index` is set to `False`, the CSV file will not have an extra column for row numbers.\n",
    "\n",
    "In summary, this code segment applies a trained LightGBM classifier to the test data to predict sleep events, collects these predictions into a DataFrame named `submission`, and then saves this DataFrame to a CSV file named 'submission.csv'. The resulting CSV file can be used for submission in a machine learning competition or for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9584856e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T06:17:45.409913Z",
     "iopub.status.busy": "2023-10-14T06:17:45.409110Z",
     "iopub.status.idle": "2023-10-14T06:17:45.414214Z",
     "shell.execute_reply": "2023-10-14T06:17:45.412976Z"
    },
    "papermill": {
     "duration": 0.03713,
     "end_time": "2023-10-14T06:17:45.416503",
     "exception": false,
     "start_time": "2023-10-14T06:17:45.379373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recovering memory\n",
    "# del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c093845b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-14T06:17:45.475864Z",
     "iopub.status.busy": "2023-10-14T06:17:45.475121Z",
     "iopub.status.idle": "2023-10-14T06:17:45.782135Z",
     "shell.execute_reply": "2023-10-14T06:17:45.780899Z"
    },
    "papermill": {
     "duration": 0.33966,
     "end_time": "2023-10-14T06:17:45.784755",
     "exception": false,
     "start_time": "2023-10-14T06:17:45.445095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting event predictions for test set and saving submission\n",
    "submission = get_events(test_series.collect(), rf_classifier)\n",
    "\n",
    "# Saving the submission dataframe to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081ba81",
   "metadata": {
    "papermill": {
     "duration": 0.025618,
     "end_time": "2023-10-14T06:17:45.837080",
     "exception": false,
     "start_time": "2023-10-14T06:17:45.811462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Explore More! ğŸ‘€\n",
    "Thank you for exploring this notebook! If you found this notebook insightful or if it helped you in any way, I invite you to explore more of my work on my profile.\n",
    "\n",
    "ğŸ‘‰ [Visit my Profile](https://www.kaggle.com/zulqarnainali) ğŸ‘ˆ\n",
    "\n",
    "## Feedback and Gratitude ğŸ™\n",
    "We value your feedback! Your insights and suggestions are essential for our continuous improvement. If you have any comments, questions, or ideas to share, please don't hesitate to reach out.\n",
    "\n",
    "ğŸ“¬ Contact me via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n",
    "\n",
    "I would like to express our heartfelt gratitude for your time and engagement. Your support motivates us to create more valuable content.\n",
    "\n",
    "Happy coding and best of luck in your data science endeavors! ğŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6017.081246,
   "end_time": "2023-10-14T06:17:48.896290",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-14T04:37:31.815044",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
