{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-Conv1D Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myro4dtnQB-y"
   },
   "outputs": [],
   "source": [
    "# Reload modules\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJfk3P6IQB_g"
   },
   "outputs": [],
   "source": [
    "#supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#numpy and pandas for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import median\n",
    "from scipy.stats import norm\n",
    "import re\n",
    "import math\n",
    "\n",
    "#matplotlib and seaborn for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "import plotly\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#file system management\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import optimizers, callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Input, Embedding, LSTM, Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "J4CqhUXOQCAM",
    "outputId": "53c3b0d7-1e7f-44ca-a3cf-3dddbb93e441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1482535, 8)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "df = pd.read_table('/content/drive/My Drive/data/train.tsv')\n",
    "\n",
    "print('Training data shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XaOnbhQQCAq"
   },
   "outputs": [],
   "source": [
    "#stopwords without no, not, etc\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0iiRodBfQCBC"
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(input_data):\n",
    "    \"\"\"\n",
    "    Fills the nan/missing values with 'missing' for text columns\n",
    "    \"\"\"\n",
    "    input_data.fillna({'name': 'missing', 'item_description': 'missing', 'category_name': 'other'}, inplace=True)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9jWjdnfQCBa"
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(sentence):\n",
    "    \"\"\"\n",
    "    Remove emojis from the string\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return pattern.sub(r'', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2Qeun-jQCBw"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Remove all puntuations from the string\n",
    "    \"\"\"\n",
    "    import string\n",
    "    regular_punct = list(string.punctuation)\n",
    "    \n",
    "    for punc in regular_punct:\n",
    "        if punc in sentence:\n",
    "            sentence = sentence.replace(punc, ' ')\n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKnhwRyTQCCF"
   },
   "outputs": [],
   "source": [
    "# https://www.appliedaicourse.com/\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Expand and create common english contractions in the text\n",
    "    \"\"\"\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeWC3tK5QCCS"
   },
   "outputs": [],
   "source": [
    "# https://www.appliedaicourse.com/\n",
    "def process_text(input_data, cols):\n",
    "    \"\"\"\n",
    "    Take the text columns and process the data. Expand contractions, use regex to remove symbols/numbers, remove emojis, punctuations\n",
    "    and stopwords and convert text to lowercase\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        \n",
    "        processed_data = []\n",
    "        \n",
    "        for sent in input_data[col].values:\n",
    "            \n",
    "            sent = decontracted(sent)\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            sent = remove_emoji(sent)\n",
    "            sent = remove_punctuation(sent)\n",
    "            sent = ' '.join(e for e in sent.split() if e not in STOPWORDS)\n",
    "            processed_data.append(sent.lower().strip())\n",
    "            \n",
    "        input_data[col] = processed_data\n",
    "        \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnfBhZwgQCCl"
   },
   "outputs": [],
   "source": [
    "def process_category(input_data):\n",
    "    \"\"\"\n",
    "    Split the category_name into 3 parts as category_0, category_1 and category_2\n",
    "    \"\"\"\n",
    "    for i in range(3):\n",
    "        \n",
    "        def get_categories(ele):\n",
    "            \n",
    "            if type(ele) != str:\n",
    "                return np.nan\n",
    "        \n",
    "            cat = ele.split('/')\n",
    "            \n",
    "            if i >= len(cat):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return cat[i]\n",
    "\n",
    "        col_name = 'category_' + str(i)\n",
    "        \n",
    "        input_data[col_name] = input_data['category_name'].apply(get_categories)\n",
    "        \n",
    "        input_data.fillna({'category_name': 'Other'}, inplace = True)\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PRfxGubQCC4"
   },
   "outputs": [],
   "source": [
    "#nlp features\n",
    "def get_text_features(input_data):\n",
    "    \"\"\"\n",
    "    NLP features derived from the text columns\n",
    "    \"\"\"\n",
    "    input_data['has_brand_name'] = (input_data['brand_name'].isnull()).astype(np.int8) #if brand_name is present, 1 else 0\n",
    "    \n",
    "    input_data['has_price'] = np.where(input_data['item_description'].str.contains(' rm ', na = False), 1, 0) #if item_description has [rm] which is price string removed, 1 else 0\n",
    "\n",
    "    input_data['reversed_item_condition_id'] = 5 - input_data['item_condition_id']\n",
    "\n",
    "    luxury_brands = [\"MCM\", \"MCM Worldwide\", \"Louis Vuitton\", \"Burberry\", \"Burberry London\", \"Burberry Brit\", \"HERMES\", \"Tieks\",\n",
    "                     \"Rolex\", \"Apple\", \"Gucci\", \"Valentino\", \"Valentino Garavani\", \"RED Valentino\", \"Cartier\", \"Christian Louboutin\",\n",
    "                     \"Yves Saint Laurent\", \"Saint Laurent\", \"YSL Yves Saint Laurent\", \"Georgio Armani\", \"Armani Collezioni\", \"Emporio Armani\"]\n",
    "    \n",
    "    input_data['is_luxurious'] = (input_data['brand_name'].isin(luxury_brands)).astype(np.int8)\n",
    "\n",
    "    expensive_brands = [\"Michael Kors\", \"Louis Vuitton\", \"Lululemon\", \"LuLaRoe\", \"Kendra Scott\", \"Tory Burch\", \"Apple\", \"Kate Spade\",\n",
    "                  \"UGG Australia\", \"Coach\", \"Gucci\", \"Rae Dunn\", \"Tiffany & Co.\", \"Rock Revival\", \"Adidas\", \"Beats\", \"Burberry\",\n",
    "                  \"Christian Louboutin\", \"David Yurman\", \"Ray-Ban\", \"Chanel\"]\n",
    "\n",
    "    input_data['is_expensive'] = (input_data['brand_name'].isin(expensive_brands)).astype(np.int8)\n",
    "\n",
    "    cheap_brands = [\"FOREVER 21\", \"Old Navy\", \"Carter's\", \"Elmers\", \"NYX\", \"Maybelline\", \"Disney\", \"American Eagle\", \"PopSockets\", \"Wet n Wild\", \"Hollister\", \"Pokemon\", \"Hot Topic\", \"Konami\",\n",
    "                      \"Charlotte Russe\", \"H&M\", \"e.l.f.\", \"Bath & Body Works\", \"Gap\"]\n",
    "\n",
    "    input_data['is_cheap'] = (input_data['brand_name'].isin(cheap_brands)).astype(np.int8)\n",
    "\n",
    "    input_data['len_name'] = input_data['name'].str.len()\n",
    "    input_data['len_item_description'] = input_data['item_description'].str.len()\n",
    "    input_data['len'] = input_data['len_name'] + input_data['len_item_description']\n",
    "\n",
    "    input_data['token_count_name'] = input_data['name'].apply(lambda x: len(x.split(' ')))\n",
    "    input_data['token_count_item_description'] = input_data['item_description'].apply(lambda x: len(x.split(' ')))\n",
    "    input_data['token_count'] = input_data['token_count_name'] + input_data['token_count_item_description']\n",
    "    input_data['token_count_ratio'] = input_data['token_count_name']/input_data['token_count_item_description']\n",
    "\n",
    "    input_data[\"name_words\"] = input_data[\"name\"].str.count(\"(\\s|^)[a-z]+(\\s|$)\")\n",
    "    input_data[\"item_description_words\"] = input_data[\"item_description\"].str.count(\"(\\s|^)[a-z]+(\\s|$)\")\n",
    "    input_data[\"words\"] = input_data[\"name_words\"] + input_data[\"item_description_words\"]\n",
    "\n",
    "    input_data[\"name_numbers\"] = input_data[\"name\"].str.count(\"(\\s|^)[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?(\\s|$)\")\n",
    "    input_data[\"item_description_numbers\"] = input_data[\"item_description\"].str.count(\"(\\s|^)[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?(\\s|$)\")\n",
    "    input_data[\"numbers\"] = input_data[\"name_numbers\"] + input_data[\"item_description_numbers\"]\n",
    "\n",
    "    input_data[\"name_letters\"] = input_data[\"name\"].str.count(\"[a-zA-Z]\")\n",
    "    input_data[\"item_description_letters\"] = input_data[\"item_description\"].str.count(\"[a-zA-Z]\")\n",
    "    input_data[\"letters\"] = input_data[\"name_letters\"] + input_data[\"item_description_letters\"]\n",
    "\n",
    "    input_data[\"name_digits\"] = input_data[\"name\"].str.count(\"[0-9]\")\n",
    "    input_data[\"item_description_digits\"] = input_data[\"item_description\"].str.count(\"[0-9]\")\n",
    "    input_data[\"digits\"] = input_data[\"name_digits\"] + input_data[\"item_description_digits\"]\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usevN_PsQCDZ"
   },
   "outputs": [],
   "source": [
    "def preprocess(input_data):\n",
    "    \"\"\"\n",
    "    Process the data by handling missing values, process category_name, process text\n",
    "    \"\"\"\n",
    "    input_data = input_data[(input_data['price'] >= 3) & (input_data['price'] <= 2000)]\n",
    "    \n",
    "    input_data['price'] = np.log1p(input_data['price'])\n",
    "\n",
    "    input_data = handle_missing_values(input_data)\n",
    "    \n",
    "    #input_data = process_category(input_data)\n",
    "    \n",
    "    input_data = process_text(input_data, ['name', 'item_description', 'category_name'])\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CSOYTlBQCDy"
   },
   "outputs": [],
   "source": [
    "data = preprocess(df)\n",
    "\n",
    "#data.fillna({'category_0': 'other', 'category_1': 'other', 'category_2': 'other'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsO_tAKfQCEI"
   },
   "outputs": [],
   "source": [
    "#NLP features\n",
    "data = get_text_features(data)\n",
    "\n",
    "data.fillna({'brand_name': ' '}, inplace = True)\n",
    "\n",
    "#concatenate text features\n",
    "data['name'] = data['name'] + ' ' + data['brand_name'] + ' ' + data['category_name']\n",
    "data['text'] = data['name'] + ' ' + data['item_description']\n",
    "\n",
    "data = data.drop(columns = ['brand_name', 'item_description', 'category_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxtmL1G-QCEb"
   },
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dvOzZQ6QCEf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taking necessary features for modeling\n",
    "\"\"\"\n",
    "data = data[['price', 'name', 'shipping', 'item_condition_id', 'is_expensive', 'is_luxurious', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDuvkRO3QCE0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into train and test\n",
    "\"\"\"\n",
    "y = data['price']\n",
    "\n",
    "X = data.drop('price', axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuIFHBEKQCFD"
   },
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Get RMSLE score by passing actual values and the predictions from models\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4ouUb6iD3MO"
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQj8YaZCQCFS"
   },
   "source": [
    "#### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vnvi1zHNQCFW"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer(num_words = 1000)\n",
    "t.fit_on_texts(X_train['name'].tolist())\n",
    "\n",
    "seq_tr = t.texts_to_sequences(X_train['name'])\n",
    "seq_te = t.texts_to_sequences(X_test['name'])\n",
    "\n",
    "wordIdx = t.word_index\n",
    "\n",
    "max_length = 350\n",
    "encoded_tr = pad_sequences(seq_tr, maxlen=max_length, padding='post')\n",
    "encoded_te = pad_sequences(seq_te, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TFYl1-gQCFq"
   },
   "outputs": [],
   "source": [
    "# Loading Embedding File\n",
    "f = open('/content/drive/My Drive/data/glove_vectors', 'rb')\n",
    "glove_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vQhUm7fQCF8"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "num_words = min(MAX_VOCAB_SIZE, len(wordIdx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in wordIdx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = glove_words.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all zeros.\n",
    "          embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZqr6FFVQCGO"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 350\n",
    "embedding_layer = Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "input_text = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_text)\n",
    "x = LSTM(4, return_sequences=True)(x)\n",
    "flatten_1 = Flatten()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KduWUnpfQCGk"
   },
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0QB9fHpQCGn"
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer(num_words = 1000)\n",
    "t.fit_on_texts(X_train['text'].tolist())\n",
    "\n",
    "seq_tr = t.texts_to_sequences(X_train['text'])\n",
    "seq_te = t.texts_to_sequences(X_test['text'])\n",
    "\n",
    "wordIdx = t.word_index\n",
    "\n",
    "max_length = 350\n",
    "encoded_text_tr = pad_sequences(seq_tr, maxlen=max_length, padding='post')\n",
    "encoded_text_te = pad_sequences(seq_te, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELTdUiycQCGv"
   },
   "outputs": [],
   "source": [
    "# Loading Embedding File\n",
    "f = open('/content/drive/My Drive/data/glove_vectors', 'rb')\n",
    "glove_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtK-dRKmQCG_"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "num_words = min(MAX_VOCAB_SIZE, len(wordIdx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in wordIdx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = glove_words.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irEeD8wbQCHS"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 350\n",
    "embedding_layer = Embedding(num_words, 300, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "input_text_tr = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(input_text)\n",
    "x = LSTM(16, return_sequences=True)(x)\n",
    "flatten_2 = Flatten()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlOoACZvQCHh"
   },
   "source": [
    "#### shipping, item_condition_id, is_expensive, is_luxurious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llKCDnOfQCHm"
   },
   "outputs": [],
   "source": [
    "# Now we will prepare numerical features for our model\n",
    "num_tr_ship = X_train['shipping'].values.reshape(-1, 1)\n",
    "num_tr_cond = X_train['item_condition_id'].values.reshape(-1, 1)\n",
    "num_tr_exp = X_train['is_expensive'].values.reshape(-1, 1)\n",
    "num_tr_lux = X_train['is_luxurious'].values.reshape(-1, 1)\n",
    "\n",
    "num_te_ship = X_test['shipping'].values.reshape(-1, 1)\n",
    "num_te_cond = X_test['item_condition_id'].values.reshape(-1, 1)\n",
    "num_te_exp = X_test['is_expensive'].values.reshape(-1, 1)\n",
    "num_te_lux = X_test['is_luxurious'].values.reshape(-1, 1)\n",
    "\n",
    "num_train = np.concatenate((num_tr_ship, num_tr_cond, num_tr_exp, num_tr_lux), axis=1)\n",
    "num_test = np.concatenate((num_te_ship, num_te_cond, num_te_exp, num_te_lux), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6CacpnGQCHy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "norm_train = scaler.fit_transform(num_train)\n",
    "norm_test = scaler.transform(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVGJRWRXQCH-"
   },
   "outputs": [],
   "source": [
    "expand_tr = np.expand_dims(norm_train, 2)\n",
    "expand_te = np.expand_dims(norm_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZCYJA1BQCIr"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Early Stopping is to stop training when a monitored metric has stopped improving\n",
    "\n",
    "monitor -  Quantity to be monitored\n",
    "patience - Number of epochs with no improvement\n",
    "\"\"\"\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWI3IL6TQCI2"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/layers/convolutional/\n",
    "\n",
    "inp_conv = Input(shape=(4, 1))\n",
    "\n",
    "x1 = Conv1D(filters=128, kernel_size=2, activation='relu')(inp_conv)\n",
    "x1 = Conv1D(filters=128, kernel_size=2, activation='relu')(x1)\n",
    "x2 = Flatten()(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUcNWEbPQCI_"
   },
   "outputs": [],
   "source": [
    "x_concatenate = concatenate([flatten_1, flatten_2, x2])\n",
    "\n",
    "x = Dense(32, activation=\"relu\")(x_concatenate)\n",
    "\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "output = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[input_text, input_text_tr, inp_conv], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3KfzQm8QCJP"
   },
   "outputs": [],
   "source": [
    "train_data = [encoded_tr, encoded_text_tr, expand_tr]\n",
    "test_data = [encoded_te, encoded_text_te, expand_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "colab_type": "code",
    "id": "o2xNsV7VQCJZ",
    "outputId": "14b10030-6b34-42eb-d652-6b41bddde140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 350)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 350, 300)     300000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 350, 300)     300000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 3, 128)       384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 350, 4)       4880        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 350, 16)      20288       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2, 128)       32896       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1400)         0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5600)         0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7256)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           232224      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           528         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 350)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            17          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 892,273\n",
      "Trainable params: 292,273\n",
      "Non-trainable params: 600,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4jX3XfUQCJr"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "4hMfl4JIQCJ5",
    "outputId": "ddedcaef-7ce2-476f-bf7a-826fdaa7ddb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18521/18521 [==============================] - 745s 40ms/step - loss: 0.3122 - val_loss: 0.2866\n",
      "Epoch 2/20\n",
      "18521/18521 [==============================] - 748s 40ms/step - loss: 0.2800 - val_loss: 0.2740\n",
      "Epoch 3/20\n",
      "18521/18521 [==============================] - 741s 40ms/step - loss: 0.2730 - val_loss: 0.2723\n",
      "Epoch 4/20\n",
      "18521/18521 [==============================] - 735s 40ms/step - loss: 0.2686 - val_loss: 0.2687\n",
      "Epoch 5/20\n",
      "18521/18521 [==============================] - 745s 40ms/step - loss: 0.2659 - val_loss: 0.2668\n",
      "Epoch 6/20\n",
      "18521/18521 [==============================] - 721s 39ms/step - loss: 0.2637 - val_loss: 0.2654\n",
      "Epoch 7/20\n",
      "18521/18521 [==============================] - 725s 39ms/step - loss: 0.2620 - val_loss: 0.2652\n",
      "Epoch 8/20\n",
      "18521/18521 [==============================] - 730s 39ms/step - loss: 0.2605 - val_loss: 0.2627\n",
      "Epoch 9/20\n",
      "18521/18521 [==============================] - 735s 40ms/step - loss: 0.2593 - val_loss: 0.2626\n",
      "Epoch 10/20\n",
      "18521/18521 [==============================] - 733s 40ms/step - loss: 0.2580 - val_loss: 0.2621\n",
      "Epoch 11/20\n",
      "18521/18521 [==============================] - 723s 39ms/step - loss: 0.2570 - val_loss: 0.2612\n",
      "Epoch 12/20\n",
      "18521/18521 [==============================] - 732s 40ms/step - loss: 0.2561 - val_loss: 0.2601\n",
      "Epoch 13/20\n",
      "18521/18521 [==============================] - 715s 39ms/step - loss: 0.2553 - val_loss: 0.2613\n",
      "Epoch 14/20\n",
      "18521/18521 [==============================] - 713s 39ms/step - loss: 0.2545 - val_loss: 0.2617\n",
      "Epoch 15/20\n",
      "18521/18521 [==============================] - 710s 38ms/step - loss: 0.2538 - val_loss: 0.2635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83c0225a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, y_train, batch_size=64, epochs=20, validation_data=(test_data, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WPyYQrWvQCKJ",
    "outputId": "76f937a6-172a-4b7b-bd84-1ee44216fa0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5133484696910033\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "print(get_rmsle(np.expm1(y_test), np.expm1(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNEoYwPcK2nh"
   },
   "source": [
    "### Observation/Conclusions\n",
    "- Test RMSLE with LSTM-Conv1D and Word2Vec word embeddings came out to be 0.513\n",
    "- This model does not overfit on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
