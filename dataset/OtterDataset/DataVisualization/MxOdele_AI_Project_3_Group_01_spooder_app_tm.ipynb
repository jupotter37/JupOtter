{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SpooderAppâ„¢**\n",
    "#### *Leveraging business reviews to gain insights for potential improvements.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis;\n",
    "\n",
    "\n",
    "Consumer reviews are critical to the success of any business, yet many lack the resources to effectively analyze and act on this feedback. We hypothesize that by leveraging advanced Natural Language Processing (NLP) models, specifically through HuggingFace Transformers and OpenAI LangChain, we can accurately classify customer sentiment and generate actionable recommendations. Our goal is to empower businesses with detailed sentiment analysis and dynamic feedback, enabling them to enhance consumer satisfaction and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initialization**\n",
    "\n",
    "When executing the following code, it is recommended to uncomment any necessary packages not yet installed in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries \n",
    "# NOTE: Uncomment any libraries not currently present in your environment for\n",
    "#       initial execution of this notebook\n",
    "\n",
    "# General utilities\n",
    "# %pip install pandas --quiet                  # Data manipulation and analysis\n",
    "# %pip install numpy --quiet                   # Numerical computations\n",
    "# %pip install scipy --quiet                   # Scientific computing\n",
    "# %pip install matplotlib --quiet              # Plotting and visualization\n",
    "# %pip install seaborn --quiet                 # Statistical data visualization\n",
    "# %pip install tqdm --quiet                    # Progress bar for loops\n",
    "# %pip install gdown --quiet                   # Downloading files from Google Drive\n",
    "# %pip install zipfile --quiet                 # Working with zip files\n",
    "# %pip install json --quiet                    # JSON handling\n",
    "\n",
    "# Machine Learning & NLP\n",
    "# %pip install torch --quiet                   # PyTorch for deep learning\n",
    "# %pip install transformers --quiet            # HuggingFace Transformers\n",
    "# %pip install datasets --quiet                # HuggingFace Datasets\n",
    "# %pip install scikit-learn --quiet            # Machine learning tools\n",
    "# %pip install nltk --quiet                    # Natural Language Toolkit for text processing\n",
    "# %pip install accelerate --quiet              # Accelerate training\n",
    "# %pip install evaluate --quiet                # Metric evaluation\n",
    "\n",
    "# Web scraping\n",
    "# %pip install selenium --quiet                # Browser automation\n",
    "# %pip install webdriver-manager --quiet       # Manage WebDriver binaries\n",
    "# %pip install beautifulsoup4 --quiet          # Parsing HTML and XML\n",
    "\n",
    "# Environment & API\n",
    "# %pip install python-dotenv --quiet           # Load environment variables\n",
    "# %pip install langchain --quiet               # OpenAI LangChain for AI models\n",
    "\n",
    "# Dash (Web App Framework)\n",
    "# %pip install dash --quiet                       # Dash core components\n",
    "# %pip install dash-bootstrap-components --quiet  # Dash Bootstrap components\n",
    "\n",
    "# Plotting & Visualization\n",
    "# %pip install plotly --quiet                  # Interactive graphing library\n",
    "\n",
    "# Image Handling\n",
    "# %pip install opencv-python-headless --quiet  # OpenCV for image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities\n",
    "import pandas as pd               # Data manipulation and analysis\n",
    "import os                         # Operating system interfaces\n",
    "import re                         # Regular expressions\n",
    "import json                       # JSON handling\n",
    "import time                       # Time management\n",
    "import zipfile                    # Working with zip files\n",
    "import unicodedata                # Unicode character handling\n",
    "import numpy as np                # Numerical computations\n",
    "import scipy as sp                # Scientific computing\n",
    "import gdown                      # Google Drive file download\n",
    "from tqdm import tqdm             # Progress bar for loops\n",
    "import base64                     # Encoding and decoding binary data\n",
    "from io import BytesIO            # Handling binary data in memory\n",
    "\n",
    "# Image Handling\n",
    "import cv2                        # OpenCV for image processing\n",
    "from PIL import Image             # Image processing via PIL (for handling image conversion)\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt   # Plotting and visualization\n",
    "import matplotlib.ticker as mtick # Setting ticks to larger numbers\n",
    "import seaborn as sns             # Statistical data visualization\n",
    "import plotly.express as px       # Simple interactive plots\n",
    "import plotly.graph_objects as go # Detailed interactive plots\n",
    "\n",
    "# Machine Learning & NLP\n",
    "import torch                                          # PyTorch for deep learning\n",
    "from sklearn.model_selection import train_test_split  # Data splitting for training and testing\n",
    "from datasets import load_metric                      # Compute metrics for NLP models\n",
    "import nltk                                           # Natural Language Toolkit for text processing\n",
    "from nltk.corpus import stopwords                     # Stop words for text preprocessing\n",
    "from nltk.tokenize import word_tokenize               # Tokenization of text\n",
    "import transformers                                   # HuggingFace Transformers\n",
    "\n",
    "# Pretrained Model and Tokenization\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer  # DistilBERT model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification         # Auto-tokenizer and model for sequence classification\n",
    "from transformers import DataCollatorWithPadding                                   # Dynamic padding for batched data\n",
    "from transformers import TrainingArguments, Trainer                                # Training arguments and trainer\n",
    "from transformers import pipeline                                                  # Inference pipeline\n",
    "\n",
    "# Dataset Formatting\n",
    "import accelerate                           # Accelerate training\n",
    "from datasets import Dataset                # Dataset handling\n",
    "from evaluate import load                   # Metric evaluation\n",
    "\n",
    "# Web Scraping\n",
    "from selenium import webdriver                                          # Browser automation\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService  # WebDriver service for Chrome\n",
    "from selenium.webdriver.support.ui import WebDriverWait                 # WebDriver wait\n",
    "from selenium.webdriver.common.by import By                             # Locating elements by attributes\n",
    "from selenium.webdriver.support import expected_conditions as EC        # Expected conditions for WebDriver waits\n",
    "from selenium.common.exceptions import NoSuchElementException           # Exception handler when elements are not found\n",
    "from webdriver_manager.chrome import ChromeDriverManager                # Manage WebDriver binaries\n",
    "from bs4 import BeautifulSoup                                           # Parsing HTML and XML\n",
    "\n",
    "# Environment & API\n",
    "from dotenv import load_dotenv              # Load environment variables\n",
    "from langchain_openai import ChatOpenAI      # OpenAI API for LangChain\n",
    "\n",
    "# Prompt Template and LLM Chain\n",
    "from langchain import PromptTemplate        # Prompt template for LangChain\n",
    "from langchain.chains import LLMChain       # LLM Chain for linking models\n",
    "\n",
    "# Dash (Web App Framework)\n",
    "from dash import Dash, dcc, html, callback, callback_context  # Dash core components and callbacks\n",
    "from dash.dependencies import Input, Output, State            # Dash dependencies for callbacks\n",
    "from dash.exceptions import PreventUpdate                     # Prevent updates in callbacks\n",
    "import dash_bootstrap_components as dbc                       # Dash Bootstrap components\n",
    "\n",
    "# Other\n",
    "import math                       # Mathematical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To prepare our sentiment analysis model, we leveraged the __[Yelp Open Dataset](https://www.yelp.com/dataset)__ to harness existing reviews and ratings. We also explored other available metrics during our EDA, before proceeding to preprocessing and model training.\n",
    "\n",
    "The [Yelp Open Dataset](#Yelp-Open-Dataset) provided `.json` files with businesses, reviews, and user data from their businesses. Given the lack of image classification, we opted to forego the photo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "\n",
    "Due to the large size of the provided files, direct pushes to GitHub were not a viable option. Instead, the files were converted to `.csv` formates (outlined in `Resources/json_convesion_for_gdown.ipynb`) and uploaded to a Google Drive for retrieval through `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to access datasets through `gdown`\n",
    "def fetch_data(set):\n",
    "    '''\n",
    "    Fetches a specific dataset from Google Drive using gdown and loads it into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        set (str):      A string representing the dataset to be retrieved. Must be one of the following;\n",
    "                        'buesiness', 'checkin', 'reviews', 'tip', or 'user'\n",
    "    \n",
    "    Returns:\n",
    "        df (DataFrame): A DataFrame with the retrieved Yelp dataset.\n",
    "\n",
    "    Raises:\n",
    "        ValueError:     If an invalid dataset identifier is provided.\n",
    "        OSError:        If there is an issue with downloading the file or reading the CSV file.\n",
    "        Exception:      If any other unexpected error occurs during the download or file reading process.\n",
    "    '''\n",
    "    # Declaring `url` and `output` for dataset\n",
    "    match set:\n",
    "        case 'business':\n",
    "            url = 'https://drive.google.com/file/d/1t-_rOjZ8oMqPcMJunVaMgY3OEbhnuSCv/view?usp=sharing'\n",
    "            output = 'Resources/business_dataset.csv'\n",
    "        case 'checkin':\n",
    "            url = 'https://drive.google.com/file/d/1_AVWp31ymfvf4QgTiMN_WLAeapfr0omf/view?usp=sharing'\n",
    "            output = 'Resources/checkin_dataset.csv'\n",
    "        case 'reviews':\n",
    "            url = 'https://drive.google.com/file/d/1L8rFjhOQyU90Ycr9t_OLA70vCYM0e7ck/view?usp=sharing'\n",
    "            output = 'Resources/reviews_dataset.csv'\n",
    "        case 'tip':\n",
    "            url = 'https://drive.google.com/file/d/1LMkCi5AFC_58_m7ELmn1hR8YDykuXwqq/view?usp=sharing'\n",
    "            output = 'Resources/tip_dataset.csv'\n",
    "        case 'user':\n",
    "            url = 'https://drive.google.com/file/d/1kQ522qcod7AjD5DO9vj8qFcSKxwJCDrO/view?usp=sharing'\n",
    "            output = 'Resources/user_dataset.csv'\n",
    "        case _:\n",
    "            # Raises\n",
    "            raise ValueError('Invalid dataset selected, please try again')\n",
    "    \n",
    "    # Attempting to fetch dataset\n",
    "    try:\n",
    "        # Downloading dataset\n",
    "        gdown.download(url, output, fuzzy=True, quiet=True)\n",
    "\n",
    "        # Reading in the dataset\n",
    "        df = pd.read_csv(output, low_memory=False)\n",
    "\n",
    "    # Raises\n",
    "    except ImportError as e:\n",
    "        raise ImportError(f\"Required module not found: {e}\")\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error occurred during file operation: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    # Returning the dataset\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching and Reading In\n",
    "\n",
    "*Note: Once the* `fetch_data()` *function has been run for all five (5) datasets, you may comment out those lines of code (annotaed in cell, as well). For any additional executions, use the* `pd.read_csv()` *lines instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all datasets (uncomment for first run of code)\n",
    "# business_df = fetch_data('business')\n",
    "# checkin_df = fetch_data('checkin')\n",
    "# reviews_df = fetch_data('reviews')\n",
    "# tips_df = fetch_data('tip')\n",
    "# user_df = fetch_data('user')\n",
    "\n",
    "# Reading in all datasets (comment out if data is not already fetched)\n",
    "business_df = pd.read_csv('./Resources/business_dataset.csv')\n",
    "checkin_df = pd.read_csv('./Resources/checkin_dataset.csv')\n",
    "reviews_df = pd.read_csv('./Resources/reviews_dataset.csv')\n",
    "tips_df = pd.read_csv('./Resources/tip_dataset.csv')\n",
    "user_df = pd.read_csv('./Resources/user_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Each dataset was explored individually before final feature selection and concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business dataset\n",
    "\n",
    "Contains business data including location data, attributes, and categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "business_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin dataset\n",
    "\n",
    "Contains checkins on a business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "checkin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "checkin_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We determined this dataset would not add any value to our training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews dataset\n",
    "\n",
    "Contains full review text data including the user_id that wrote the review and the business_id the review is written for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records\n",
    "reviews_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping features:\n",
    "- **review_id**\n",
    "- **useful**\n",
    "- **funny**\n",
    "- **cool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features\n",
    "reviews_df.drop(columns = ['review_id','useful','funny','cool'],\n",
    "                inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming features:\n",
    "- **text** to **review**\n",
    "\n",
    "*Note: This change was reverted before model training.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming features\n",
    "reviews_df.rename(columns = {'text':'review'},inplace = True)\n",
    "\n",
    "#Confirming columns renamed\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dropped:*\n",
    "- **review_id:** *Eliminated due to low informational value.*\n",
    "- **useful:** *Eliminated due to low relevance.*\n",
    "- **funny:** *Eliminated due to low relevance.*\n",
    "- **cool:** *Eliminated due to low relevance.*\n",
    "\n",
    "*Essential:*\n",
    "- **business_id:** *Used as an identifier for concatenation.*\n",
    "- **stars:** *Used as eventual model target.*\n",
    "- **review:** *Used as feature for multiple models.*\n",
    "\n",
    "*Retained:*\n",
    "- **date:** *Retained for potential time series analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips dataset\n",
    "\n",
    "Contains tips written by a user on a business. Tips are shorter than reviews and tend to convey quick suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping features:\n",
    "- **compliment_count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features\n",
    "tips_df.drop(columns = ['compliment_count'],\n",
    "             inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming features:\n",
    "- **text** to **recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "tips_df.rename(columns = {'text':'recommendations'},inplace = True)\n",
    "\n",
    "#Confirming columns renamed\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dropped:*\n",
    "- **compliment_count:** *Eliminated due to low informational value.*\n",
    "\n",
    "*Retained:*\n",
    "- **recommendations:** *Retained for potential use as a target variable, since dataset has similar poitential for insights to improve the customer experience.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User dataset\n",
    "\n",
    "Contains user data including the user's friend mapping and all the metadata associated with the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "user_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We decided this dataset was not to be included in the training data to preserve user anonimity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "Merging the `reviews` and `business` data sets to create a single DataFrame to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `data_df` as the merge of `reviews_df` and `business_df`\n",
    "data_df = reviews_df.merge(business_df,how='left',on = 'business_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records as percentages for specific features\n",
    "# Calculating percentages\n",
    "na_prcnt = data_df[['attributes','categories','hours']].isna().sum()/data_df.shape[0]*100\n",
    "\n",
    "# Converting to a DataFrame\n",
    "nas_df = pd.DataFrame(na_prcnt, columns=['percentage'])\n",
    "\n",
    "# Transposing values\n",
    "nas_df = nas_df.transpose()\n",
    "\n",
    "# Rounding values\n",
    "nas_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na Count Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a bar plot of NA records\n",
    "sns.barplot(data = nas_df).set_title('NA percentage')\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/NA_Percentage_Plot.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Ultimately, we decided to drop all three columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping features with NA records:\n",
    "- **attributes**\n",
    "- **categories**\n",
    "- **hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features\n",
    "data_df.drop(columns = ['attributes','categories','hours'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing similar features\n",
    "\n",
    "Exploring the features `stars_x` and `stars_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing data with different values in both features\n",
    "data_df.loc[data_df['stars_x'] != data_df['stars_y']][['stars_x','stars_y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing values for the same business\n",
    "data_df.loc[data_df['business_id']=='XQfwVwDr-v0ZS3_CbbE5Xw'][['stars_x','stars_y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the average value for `star_x` for the same business\n",
    "round(data_df.loc[data_df['business_id']=='XQfwVwDr-v0ZS3_CbbE5Xw']['stars_x'].mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`star_y` seems to represent a business' average rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming features\n",
    "- **stars_y** to **stars_avg**\n",
    "- **stars_x** to **stars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming features\n",
    "data_df.rename(columns={'stars_y':'stars_avg','stars_x':'stars'},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring plots\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plotting figure\n",
    "sns.countplot(data_df,\n",
    "             x='is_open',\n",
    "             hue = 'is_open',\n",
    "             ax = ax).set_title('`is_open` Feature Count')\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/is_open_Feature_Count.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping features:\n",
    "- **is_open**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features\n",
    "data_df.drop(columns = ['is_open'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We decided to drop this feature due low informational value and feature imbalance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging with tips dataset\n",
    "\n",
    "Exploring to potential gain from merging the tips dataset, since it, too, contains customer recommendations to improve experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "tips_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the quantity of unique values in `business_id` in `tips_df`\n",
    "display(tips_df['business_id'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the quantity of unique values in `business_id` in `data_df`\n",
    "data_df['business_id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset of **business_id** in `data_df` not found in `tips_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `no_tips_df` as a subset of data not found in `tips_df`\n",
    "no_tips_df = data_df[~data_df['business_id'].isin(tips_df['business_id'])]\n",
    "\n",
    "# Previewing the data\n",
    "no_tips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantity of **business_id** in `data_df` not found in `tips_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the unique `business_id`s in `no_tips_df`\n",
    "not_found = no_tips_df['business_id'].unique().shape[0]\n",
    "\n",
    "# Printing the result\n",
    "print(f'Number of business_ids in tips_df not found in data_df: {not_found}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating a speciting `business_id` in `tips_df`\n",
    "tips_df.loc[tips_df['business_id'] == no_tips_df['business_id'].iloc[33]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging `tips_df` and `data_df` as `test_df`\n",
    "test_df = pd.merge(tips_df,data_df,\n",
    "                   on = ['business_id','user_id'],\n",
    "                   how = 'inner')\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "Comparing **review** agains **recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data for the two selected features\n",
    "test_df[['review','recommendations']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The* `data_df` *DataFrame has approximately* ***7 million*** *entries, and the* `tips_df` *DataFrame has about* ***1 million***. *After merging them we end up the a little under* ***500 thousand***. *The comparison above shows little difference between a* ***review*** *from the reviews dataset and a* ***recommendation*** *from the tips dataset. As shown above, we stand to loose a significant amount of data if a merge is performed, we we decided against it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping featires:\n",
    "- **user_id** *(to preserve user anonymity)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features\n",
    "data_df.drop(columns = ['user_id'],inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the data\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming additional data details\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "\n",
    "Preparing some visualizations of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ratings distribution\n",
    "\n",
    "# Building a bar graph of the distribution of star ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "data_df['stars'].value_counts().sort_index().plot(kind='bar', color='brown')\n",
    "\n",
    "# Setting plot features\n",
    "plt.title('Distribution of Star Ratings')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Formating the y-axis to show full numbers\n",
    "plt.gca().yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Disribution_of_Star_Ratings.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review counts per business\n",
    "\n",
    "# Grouping data by business name and counting the number of reviews per business\n",
    "review_counts = data_df.groupby('name')['review'].count().reset_index()\n",
    "review_counts.columns = ['business', 'review_count']\n",
    "\n",
    "# Sorting the data by review count and selecting the top 10 businesses\n",
    "top_10_review_counts = review_counts.sort_values(by='review_count', ascending=False).head(10)\n",
    "\n",
    "# Building a bar graph of review counts per business\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=top_10_review_counts, x='business', y='review_count', palette='viridis')\n",
    "\n",
    "plt.title('Top 10 Businesses by Review Count')\n",
    "plt.xlabel('Business')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Disribution_of_Star_Ratings_Top_10_Business.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business category distribution\n",
    "\n",
    "# Extracting categories\n",
    "all_categories = business_df['categories'].str.split(',').explode().str.strip()\n",
    "\n",
    "# Counting the occurrences of each category\n",
    "category_counts = all_categories.value_counts().head(20)\n",
    "\n",
    "# Building the bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values, palette='summer')\n",
    "\n",
    "# Setting plot features\n",
    "plt.title('Distribution of Top 20 Business Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Businesses')\n",
    "plt.xticks(rotation=67)\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Distribution_of_Top_20_Business_Categories.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions (Pt 1)\n",
    "\n",
    "The following user-defined functions will be used throughout the sampling, preproccing, an modeling of our data, each developed with their annotated purposes in mind:\n",
    "\n",
    "| **Function** | **Notes** |\n",
    "| :--- | :--- |\n",
    "| `sample_stars()` | Selects subsets of a DataFrame based on user rating value thresholds |\n",
    "| `remove_accented_chars()` | Removes accented characters from text |\n",
    "| `clean_text()` | Removes web formatting from text |\n",
    "| `pre_process_reviews()` | Removes stop words from text |\n",
    "| `tokenizer_function()` | Tokenizes text |\n",
    "| `compute_metrics()` | Computes metrics to assist with evaluating model performance |\n",
    "\n",
    "Functions outlined in more detail below require a DataFrame with the following features:\n",
    "\n",
    "| **Feature** | **Notes** |\n",
    "| :--- | :--- |\n",
    "| `rating` | An int or float column with submitted user review scores |\n",
    "| `rev_col` | A text column with available reviews |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select various subsets of data\n",
    "def sample_stars(df, val):\n",
    "    '''\n",
    "    Samples a specific subset of a DataFrame based on the value of the 'stars' column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Any DataFrame with sufficient data.\n",
    "        val (int):      An integer representing the specific star rating to filter and sample from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        df (DataFrame): A subset of the input DataFrame filtered by the specified star rating.\n",
    "\n",
    "    Raises:\n",
    "        KeyError:       If 'stars' is not a valid column name in the DataFrame.\n",
    "        TypeError:      If `df` is not a DataFrame or if `val` is not an integer.\n",
    "        ValueError:     If `val` is not within the acceptable range (1 to 5).\n",
    "        ValueError:     If there are not enough records in the DataFrame to sample the requested amount.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    if 'stars' not in df.columns:\n",
    "        raise KeyError(\"Column 'stars' not found in DataFrame.\")\n",
    "    if not isinstance(val, int):\n",
    "        raise TypeError('The `val` parameter must be passed as an integer.')\n",
    "    if val < 1 or val > 5:\n",
    "        raise ValueError('The `val` parameter must be an integer between 1 and 5.')\n",
    "    \n",
    "    # Filtering and sampling the DataFrame\n",
    "    df = df[df['stars'] == val].copy()\n",
    "\n",
    "    # Sampling the DataFrame based on specified star rating to balance end dataset\n",
    "    if val >= 4:\n",
    "        df = df.sample(1000)\n",
    "    elif val <= 2:\n",
    "        df = df.sample(1000)\n",
    "    else:\n",
    "        df = df.sample(2000)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Retuning the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove accented characters\n",
    "def remove_accented_chars(text):\n",
    "    '''\n",
    "    Removes accecnted characters.\n",
    "\n",
    "    Args:\n",
    "        text (str):     A corpus of text.\n",
    "\n",
    "    Returns:\n",
    "        text (str):     A processed corpus of text.\n",
    "    '''\n",
    "    # Removing accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    # Returning `text`\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to remove web formatting\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Removes Urls',mentions, hashtags, and multiple spaces from text.\n",
    "\n",
    "    Args:\n",
    "        text (str):     A corpus of text.\n",
    "    \n",
    "    Returns:\n",
    "        text (str):     A processed corpus of text.\n",
    "    '''\n",
    "    # Removing URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Removing mentions\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    # Removing hashtags\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "    # Removing multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Retuning `text`\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to remove stop words from a corups\n",
    "def pre_process_reviews(reviews):\n",
    "    '''\n",
    "    Removes stop words from corpus.\n",
    "\n",
    "    Args:\n",
    "        reviews (str):      A corpus of text.\n",
    "    \n",
    "    Returns:\n",
    "        reviews (str):     A processed corpus of text.\n",
    "    '''\n",
    "    # Setting stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Declaring an empty list for processed reviews\n",
    "    norm_reviews = []\n",
    "\n",
    "    # Looping\n",
    "    for review in tqdm(reviews):\n",
    "        # Clean text\n",
    "        review = clean_text(review)\n",
    "        # remove extra newlines and convert them to spaces\n",
    "        review = review.translate(review.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "        # lower case\n",
    "        review = review.lower()\n",
    "        # remove accents\n",
    "        review = remove_accented_chars(review)\n",
    "        # remove special characters\n",
    "        review = re.sub(r'[^a-zA-Z0-9\\s]', '', review, flags=re.I|re.A)\n",
    "        # remove extra whitespaces\n",
    "        review = re.sub(' +', ' ', review)\n",
    "        # remove leading and training whitespaces\n",
    "        review = review.strip()\n",
    "\n",
    "        review_tokens = word_tokenize(review)\n",
    "        review = [w for w in review_tokens if not w in stop_words]\n",
    "        review = ' '.join(review)\n",
    "\n",
    "        # Appending to `norm_reviews`\n",
    "        norm_reviews.append(review)\n",
    "    \n",
    "    # Returning `norm_reviews`\n",
    "    return norm_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a corpus\n",
    "def tokenizer_function(review):\n",
    "    '''\n",
    "    Tokenizes corpus\n",
    "    \n",
    "    Args:\n",
    "        reviews (str):              A corpus of text.\n",
    "    \n",
    "    Returns:\n",
    "        return_tensors (pythorch):  A tokenized corpus of text.\n",
    "    '''\n",
    "    # Extracting text\n",
    "    text = review['text']\n",
    "\n",
    "    # Tokenize text with truncation and padding\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        # Truncate to max_length from the right by default\n",
    "        truncation=True,\n",
    "        # Pad to the maximum length\n",
    "        padding=\"max_length\",\n",
    "        # Maximum sequence length for BERT models\n",
    "        max_length=512,\n",
    "        # Assuming you are using PyTorch; change to 'np' if necessary\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Returning `tokenized_inputs`\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple metrics\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "precision_metric = load(\"precision\")\n",
    "recall_metric = load(\"recall\")\n",
    "f1_metric = load(\"f1\")\n",
    "\n",
    "# Function to compute the metrics of our model\n",
    "def compute_metrics(pred):\n",
    "    '''\n",
    "    Computes metrics to assist in evaluating model perfomance.\n",
    "\n",
    "    Args:\n",
    "        pred (tuple):   A series representing modeled predictions and a boolian indicator.\n",
    "    \n",
    "    Returns:\n",
    "        return (dict):  A dictionary containing model performance metrics.\n",
    "    \n",
    "    Raises:\n",
    "        TypeError:      If `pred` is not a tuple or if the elements of `pred` are not numpy arrays.\n",
    "        ValueError:     If the shape of the predictions does not match the shape of the labels.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(pred, tuple):\n",
    "        raise TypeError(\"The `pred` argument must be a tuple.\")\n",
    "    if not isinstance(pred[0], np.ndarray) or not isinstance(pred[1], np.ndarray):\n",
    "        raise TypeError(\"The elements of `pred` must be numpy arrays.\")\n",
    "    if pred[0].shape[0] != pred[1].shape[0]:\n",
    "        raise ValueError(\"The number of predictions must match the number of labels.\")\n",
    "    \n",
    "    # Unpacked tuple\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Metrics\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # Precision\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    # Recall\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    # F1 score\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    # Returning a dictionary containing all metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a subset of data\n",
    "\n",
    "Due to the imbalance of submitted user ratings, we decided to narrow our training data to reflect a balanced spread of \"positive\", \"neutral\", and \"negative\" reviews. To accomplish this, we selected an equal number of records with 5 or 4 star ratings, 3 star ratings, and 2 or 1 star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring sample subsets of data\n",
    "sample_5 = sample_stars(data_df,5)\n",
    "sample_4 = sample_stars(data_df,4)\n",
    "sample_3 = sample_stars(data_df,3)\n",
    "sample_2 = sample_stars(data_df,2)\n",
    "sample_1 = sample_stars(data_df,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating sample sets into a single DataFrame\n",
    "sample_data_df = pd.concat(\n",
    "    [\n",
    "        sample_1,\n",
    "        sample_2,\n",
    "        sample_3,\n",
    "        sample_4,\n",
    "        sample_5\n",
    "    ], axis=0, ignore_index=True\n",
    ")\n",
    "\n",
    "# Confirming total records in `sample_data_df`\n",
    "sample_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming appropriate quantities of each star values were pulled\n",
    "sample_data_df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual encoding\n",
    "\n",
    "Our goal was to break down reviews into \"positive\", \"neutral\", or \"negative\" sentiments. As such, we manually encoded the value of `stars` as follows;\n",
    "\n",
    "| **Stars** | **Label** | **Sentiment Meaning** |\n",
    "| :--- | :--- | :--- |\n",
    "| **5** | **2** | \"Positive\" |\n",
    "| **4** | **2** | \"Positive\" |\n",
    "| **3** | **1** | \"Neutral\" |\n",
    "| **2** | **0** | \"Negative\" |\n",
    "| **1** | **0** | \"Negative\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually encoding `stars` value to create sentiment labels\n",
    "sample_data_df['stars']= sample_data_df['stars'].replace(to_replace=[1,2], value=0)\n",
    "sample_data_df['stars']= sample_data_df['stars'].replace(to_replace=3, value=1)\n",
    "sample_data_df['stars']= sample_data_df['stars'].replace(to_replace=[4,5], value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming equal label class counts\n",
    "sample_data_df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing records\n",
    "sample_data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Since this dataset is for training the sentiment analysis model, only, NA values in fields other than* `stars` *and* `review` *are ultimately irrelevent to the model training. As such, no records were in need of being dropped.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming featires:\n",
    "- **review** to **text**\n",
    "- **stars** to **label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming features\n",
    "sample_data_df.rename(columns={'review':'text','stars':'label'},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Our sample set was prepared for training with `train_test_split()` set to a `random_state=` of **42**, because life, the universe, and everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring `X` as features\n",
    "X = sample_data_df['text']\n",
    "# Declaring `y` as target\n",
    "y = sample_data_df['label']\n",
    "\n",
    "# Splitting the data into training and testing sets with a `test_size` of 0.3\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline\n",
    "\n",
    "Our approach to sentiment analysis centered around utilizing a BERT-based Transormer. As such, steps were taken to establish, then tune the hyperparameters of, our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Once split, each dataset was passed through our established preprocessing functions to prepare for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing training reviews\n",
    "norm_train_reviews = pre_process_reviews(X_train)\n",
    "\n",
    "# Preprocessing testing reviews\n",
    "norm_test_reviews = pre_process_reviews(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting training target data as a Dataset\n",
    "train_dataset = Dataset.from_dict({'label':y_train.to_list(),'text':norm_train_reviews})\n",
    "# Formatting testing target data as a Dataset\n",
    "test_dataset = Dataset.from_dict({'label':y_test.to_list(),'text':norm_test_reviews})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the model\n",
    "\n",
    "Each step below brought us one step closer to building our model, now lovingly referred to as `roberto`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model\n",
    "\n",
    "Here we set `roberto`'s initial definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pretrained model\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Defining label classes\n",
    "id_to_label = {0:'Negative', 1:'Neutral', 2:'Positive'}\n",
    "label_to_id = {'Negative':0, 'Neutral': 1,'Positive':2}\n",
    "\n",
    "#Model definiftion\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 3,\n",
    "    id2label = id_to_label,\n",
    "    label2id =label_to_id \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "And here we prepared the necessary steps for `roberto`'s tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_checkpoint)\n",
    "#Padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "## Mapping Preprocessed data set to tokinezed data\n",
    "tokenized_train_dataset = train_dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_test_dataset = train_dataset.map(tokenizer_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collator\n",
    "\n",
    "`roberto` needed a collator, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training arguments\n",
    "\n",
    "While not argumentative by nature, `roberto` needed controllable hyperparameters to tune through the training process. Here we did that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trainig arguments hypter-parameters,which will be used to train the model\n",
    "\n",
    "#Output directory\n",
    "output_dir = 'model_sentiment'\n",
    "#Learning Rate\n",
    "lr = 2e-5\n",
    "#Batch size\n",
    "batch_size = 32\n",
    "#Epochs\n",
    "EPOCHS = 3\n",
    "\n",
    "#Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size*2,\n",
    "    num_train_epochs = EPOCHS,\n",
    "    weight_decay = 0.01,\n",
    "    save_strategy = 'epoch',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_steps = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    # Enable mixed precision\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "As we were trained to do, we etablished a proper trainer for `roberto` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer instatiation\n",
    "trainer = Trainer(\n",
    "                  model = model,\n",
    "                  args = training_args,\n",
    "                  train_dataset = tokenized_train_dataset,\n",
    "                  eval_dataset = tokenized_test_dataset,\n",
    "                  tokenizer = tokenizer,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "It all came down to this. `roberto`'s moment t make us proud!\n",
    "\n",
    "*Note: We commented this next cell out seeing as it took six (6) hours to train* `roebrto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model with sample set of balanced data\n",
    "# (commented out to prevent re-training)\n",
    "\n",
    "# trained_model_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`roberto`... did not make us proud at first...\n",
    "\n",
    "#### Training and Hypertuning;\n",
    "\n",
    "Here are the initial and final accuracy values during `roberto`'s development;\\\n",
    "Initial fine-tuning of pretrained model yielded accuracy values of ~45%.\\\n",
    "Final training run yielded accruacy values are around ~82%.\n",
    "\n",
    "Steps taken to improve accuracy (in something close to resembling order of application);\n",
    "* Changed pre-trained model from `distilbert-base-uncased` to `MarieAngeA13/Sentiment-Analysis-BERT`\n",
    "* Adjusted sample sizes of data <br> (from ~100 records total to a balanced sample set of 1000 with equal representation for all ratings) <br> (it would be another iteration before that sample set would be a balanced represntaion of the *labels*, though)\n",
    "* Updates to text cleaning to include more web-present syntax <br> (eg; mentions, multiple spaces, hashtags, and web address elements) <br> (because reviews aren't literary works, typically)\n",
    "* Adjustted syntax and arguments of tokenizer function and the application of it\n",
    "* Adjusted training arguments to better align with our BERT-based model <br> (*spoiler: this gets undone pretty soon after*)\n",
    "* Added additional metrics for better understanding of neccessary optimization\n",
    "* Increased sample data size, again, and removed subset step entirely <br> (started at 10,000 only to then decrease that sample size to 600 because of time, but it was still a larger sample than where it started)\n",
    "* Adjusted batch size and epochs <br> (twice)\n",
    "* Moved back to `distilbert-base-uncased` and adjusted learning tokenizers, learning rate, logging steps, and such hyperparameters accordingly <br> (because sometimes less Bert is better Bert)\n",
    "* Bargained with Eldritch beings in the hopes of a single soul buying even just a 10% boost to accuracy <br> (which is to say the sample size was changed to 3,000) <br> (also added and evaluation step to get a better idea of performance)\n",
    "* Exchanged soul because the deal was pretty tempting <br> (3,000 records had an accuracy of ~78%, so set the model to train overnight with 6,000 records in the hopes of an above 80% result)\n",
    "\n",
    "In the end, though? Yeah. `roberto` made us ***VERY*** proud!\n",
    "\n",
    "Copied output from final evaluation;\n",
    "\n",
    "> {'eval_loss': 0.4815390408039093, 'eval_accuracy': 0.8169047619047619, 'eval_precision': 0.8175331785953032, 'eval_recall': 0.8169047619047619, 'eval_f1': 0.8171466024398222, 'eval_runtime': 1743.5234, 'eval_samples_per_second': 2.409, 'eval_steps_per_second': 0.038, 'epoch': 3.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "How we tested `roberto`'s performance. Don't worry, we saved the best results just above here.\n",
    "\n",
    "*Note: We commented this out because there are no model metrics to review if the training cell is not run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model (commented out due to trainer already being trained)\n",
    "# evaluation_metrics = trainer.evaluate()\n",
    "\n",
    "# Print the final score (commented out due to trainer already being trained)\n",
    "# print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model & Tokenizer\n",
    "\n",
    "Once we had an adequately trained `roberto`, we saved out model off for later fetching.\n",
    "\n",
    "*Note: We commented this out because there is no model to save if the training cell is not run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and toekenizer\n",
    "# (commented out to prevent overwriting, # fetching handled through `gdown` and `zipfile`)\n",
    "# trainer.save_model(model_path)\n",
    "\n",
    "# tokenizer.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching and unzipping model\n",
    "\n",
    "So... Long story short, training a model on six-thousand (**6,000**) records makes for a sizable directory. Much as we circumnavicated Git's file size push restrictions with the datasets, we utlized `gdown` to fetch our model from being stored on Google Drive.\n",
    "\n",
    "*Note: The cell below is only necessary to be run during this notebook's first execution. It may be commented out for any subsequent execution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching model through `gdown` (uncomment for first run of code)\n",
    "# url = 'https://drive.google.com/file/d/1tzYRkjv3wWpfg21pJ02SEYNXEcj-TVH3/view?usp=sharing'\n",
    "# output = 'Resources/Sentiment_Analysis.zip'\n",
    "# Download model (uncomment for first run of code)\n",
    "# gdown.download(url, output, fuzzy=True, quiet=False)\n",
    "\n",
    "# Extracting model (uncomment for first run of code)\n",
    "# with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./Sentiment_Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths\n",
    "\n",
    "Once saved or fetched, pathing to `roberto` is necessary for later use. Here we set our paths.\n",
    "\n",
    "*Note: If you unzip a file, it puts itself in a folder named for its zip, so... Yeah, the redundant filepath is a byproduct of default zip behavior.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model path\n",
    "model_path = 'Sentiment_Analysis/Sentiment_Analysis/model'\n",
    "\n",
    "# Declaring the tokenizer path\n",
    "tokenizer_path =  'Sentiment_Analysis/Sentiment_Analysis/tokenizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end result\n",
    "\n",
    "Finally complete, `roberto` comes together with all the necessary pieces of its pipeline in place. **Behold!** `roberto`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "#Loading Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Because of his dedication, and our exhaustion, which led us (Vanessa)\n",
    "# to mispronunciate his namem in honor of our TA Alberto Aigner we have named\n",
    "# our Pipeline 'roberto'\n",
    "roberto = pipeline('sentiment-analysis',model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions (Pt 2)\n",
    "\n",
    "While trained on Yelp! data, and developed for Google Reviews, the goal of the application is to be as univerally applicable to business reviews as possible - regardless of the source. The following functions were developed with their annotated purposes in mind:\n",
    "\n",
    "| **Function** | **Notes** |\n",
    "| :--- | :---|\n",
    "| `apply_roberto()` | Generates sentiment analysis for reviews in a given dataset, and a confidence in that sentiment |\n",
    "| `business_names_list()` | Generates a list of unique business names from a given dataset |\n",
    "| `reviews_list()` | Generates a list of all reviews submitted to a business for all its locations |\n",
    "| `general_sentiment()` | Classifies the general sentiment for a business' reviews and provides a mean confidence in that sentiment <br> *Note: To be run after a DataFrame has been passed through* `apply_roberto()` |\n",
    "| `get_business_overview()` | Extracts business overview details from a webpage using BeaugtifulSoup and Selenium. |\n",
    "| `read_csv_with_error_handling()` | Reads a CSV file into a pandas DataFrame with error handling for common file-related issues. |\n",
    "| `get_review_summary()` | Gathers and summarizes review data from a set of review elements parsed from HTML. |\n",
    "| `web_Scraper()` | Scrapes Google Maps via web driver and gather business information and reviews for the list of businesses in the imported file. |\n",
    "\n",
    "Functions outlined in more detail below require a DataFrame with the following features:\n",
    "\n",
    "| **Feature** | **Notes** |\n",
    "| :--- | :--- |\n",
    "| `bus_name_col` | A text column with the name of a business |\n",
    "| `bus_add` | A text column with the street address of a business' location |\n",
    "| `rev_col` | A text column with available reviews |\n",
    "| `sent_lbl` | A text column with the generated sentiment classification <br> *Note: Generated through* `apply_roberto()` |\n",
    "| `sent_scr` | A text column with the generated sentiment classification <br> *Note: Generated through* `apply_roberto()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply `roberto` to any DF\n",
    "def apply_roberto(df,rev_col):\n",
    "    '''\n",
    "    Applies the `roberto` model to generate sentiment analysis for the reviews in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        rev_col (str):      A string with the feature name that contains the review text.\n",
    "    \n",
    "    Returns:\n",
    "        df (DataFrame):     The same DataFrame with the appended sentiments and confidence scores.\n",
    "    \n",
    "    Raises:\n",
    "        KeyError:  If `rev_col` is not a valid column name in the DataFrame.\n",
    "        TypeError: If `df` is not a DataFrame or if `review_col` is not a string.\n",
    "    '''\n",
    "    #Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    if not isinstance(rev_col, str):\n",
    "        raise TypeError('The `rev_col` parameter must be passed as a string.')\n",
    "    if rev_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{rev_col}' not found in DataFrame.\")\n",
    "    \n",
    "    # Initializing features for results\n",
    "    df['sent_label'] = ''\n",
    "    df['sent_score'] = 0.0\n",
    "\n",
    "    # Iterating through `df`\n",
    "    for index,row in df.iterrows():\n",
    "        # Setting review text as `text`\n",
    "        text = row[rev_col]\n",
    "        # Generating results for a given review\n",
    "        result = roberto(text, truncation=True)[0]\n",
    "        # Appending the sentiment label\n",
    "        df.at[index, 'sent_label'] = result['label']\n",
    "        #Appending the sentiment score\n",
    "        df.at[index, 'sent_score'] = result['score']\n",
    "    \n",
    "    # Returning `df`\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve unique business names\n",
    "def business_names_list(df, bus_name_col):\n",
    "    '''\n",
    "    Places unique names from a list of businesses into a list.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        bus_name_col (str): A string with the feature name that contains the business name.\n",
    "\n",
    "    Returns:\n",
    "        names (list):       A list of strings with only unique values.\n",
    "\n",
    "    Raises:\n",
    "        KeyError:           If `bus_name` is not a valid column name in the DataFrame.\n",
    "        TypeError:          If `df` is not a DataFrame or if `bus_name` is not a string.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    if not isinstance(bus_name_col, str):\n",
    "        raise TypeError('The `bus_name` parameter must be passed as a string.')\n",
    "    if bus_name_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{bus_name_col}' not found in DataFrame.\")\n",
    "\n",
    "    # Generating a list of business names\n",
    "    names = df[bus_name_col].unique().tolist()\n",
    "    \n",
    "    # Returning the list\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve all reviews\n",
    "def reviews_list(df, bus_name_col, bus_name, bus_add, rev_col):\n",
    "    '''\n",
    "    Places all reviews for a given business into a list, attributing each review to its specific location.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        bus_name_col (str): A string with the feature name that contains the business name.\n",
    "        bus_name (str):     A string of a specific business' name for which to map the locations.\n",
    "        bus_add (str):      A string with the feature name that contains the business street address.\n",
    "        rev_col (str):      A string with the feature name that contains the review text.\n",
    "\n",
    "    Returns:\n",
    "        reviews (list):     A list of strings with all the reviews for a given business.\n",
    "\n",
    "    Raises:\n",
    "        KeyError:           If any passed str is not a valid column name in the DataFrame, or if `bus_name` is not a value in `bus_name_col`.\n",
    "        TypeError:          If `df` is not a DataFrame  if and feature is not a string, or if `bus_name` is not a string.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    for param, name in zip(\n",
    "        [bus_name_col, bus_add, rev_col],\n",
    "        ['bus_name_col', 'bus_add', 'rev_col']\n",
    "    ):\n",
    "        if not isinstance(param, str):\n",
    "            raise TypeError(f\"The '{name}' parameter must be passed as a string.\")\n",
    "        if param not in df.columns:\n",
    "            raise KeyError(f\"Column '{param}' not found in DataFrame.\")\n",
    "    if not isinstance(bus_name, str):\n",
    "        raise TypeError('The `bus_name` parameter must be passed as a string.')\n",
    "    if bus_name not in df[bus_name_col].values:\n",
    "        raise KeyError(f\"Value '{bus_name}' not found in column '{bus_name_col}'.\")\n",
    "    \n",
    "    # Filtering `df`\n",
    "    filtered_df = df[[bus_add, rev_col]][df[bus_name_col] == bus_name].copy()\n",
    "\n",
    "    # Handling missing or empty reviews\n",
    "    filtered_df[rev_col] = filtered_df[rev_col].fillna('No review provided.')\n",
    "\n",
    "    # Creating a list of all reviews\n",
    "    reviews = filtered_df[bus_add] + ':\\n' + filtered_df[rev_col] + '\\n\\n'\n",
    "\n",
    "    # Converting to a list\n",
    "    reviews = reviews.to_list()\n",
    "\n",
    "    # Returning reviews\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generalize the overall sentiment\n",
    "def general_sentiment(df, bus_name_col, bus_name, sent_lbl, sent_scr):\n",
    "    '''\n",
    "    Compares the total positive, negative, and neutral reviews to classify an overall sentiment.\n",
    "\n",
    "    Note:\n",
    "        To be run after passing a DataFrame through `apply_roberto()`.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        bus_name_col (str): A string with the feature name that contains the business name.\n",
    "        bus_name (str):     A string of a specific business' name for which to map the locations.\n",
    "        sent_lbl (str):     A string with the feature name that contains the modeled sentiment label.\n",
    "        sent_scr (str):     A string with the feature name that contains the modeled sentiment confidence.\n",
    "\n",
    "    Returns:\n",
    "        gen_sent (str):     A string with the overall sentiment, and the model's mean confidence in that classification.\n",
    "    \n",
    "    Raises:\n",
    "        KeyError:           If any passed str is not a valid column name in the DataFrame, or if `bus_name` is not a value in `bus_name_col`.\n",
    "        TypeError:          If `df` is not a DataFrame  if and feature is not a string, or if `bus_name` is not a string.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    for param, name in zip(\n",
    "        [bus_name_col, sent_lbl, sent_scr],\n",
    "        ['bus_name_col', 'sent_lbl', 'sent_scr']\n",
    "    ):\n",
    "        if not isinstance(param, str):\n",
    "            raise TypeError(f\"The '{name}' parameter must be passed as a string.\")\n",
    "        if param not in df.columns:\n",
    "            raise KeyError(f\"Column '{param}' not found in DataFrame.\")\n",
    "    if not isinstance(bus_name, str):\n",
    "        raise TypeError('The `bus_name` parameter must be passed as a string.')\n",
    "    if bus_name not in df[bus_name_col].values:\n",
    "        raise KeyError(f\"Value '{bus_name}' not found in column '{bus_name_col}'.\")\n",
    "    \n",
    "    # Filtering `df`\n",
    "    filtered_df = df[[sent_lbl, sent_scr]][df[bus_name_col] == bus_name].copy()\n",
    "\n",
    "    # Converting `sentiment` to lower case\n",
    "    filtered_df[sent_lbl] = filtered_df[sent_lbl].str.lower()\n",
    "\n",
    "    # Calculating total `positive` sentiment\n",
    "    pos = filtered_df.loc[filtered_df[sent_lbl] == 'positive'].shape[0]\n",
    "    # Calculating total `neutral` sentiment\n",
    "    ntrl = filtered_df.loc[filtered_df[sent_lbl] == 'neutral'].shape[0]\n",
    "    # Calculating total `negative` sentiment\n",
    "    neg = filtered_df.loc[filtered_df[sent_lbl] == 'negative'].shape[0]\n",
    "\n",
    "    # Match case to generate general sentiment\n",
    "    match (pos, ntrl, neg):\n",
    "        case (p, n, ng) if p > n > ng:\n",
    "            sent = 'highly positive'\n",
    "        case (p, n, ng) if p > n + ng:\n",
    "            sent = 'strongly positive'\n",
    "        case (p, n, ng) if p + n > ng:\n",
    "            sent = 'moderately positive'\n",
    "        case (p, n, ng) if p < n > ng:\n",
    "            sent = 'generally neutral'\n",
    "        case (p, n, ng) if p < n < ng:\n",
    "            sent = 'moderately negative'\n",
    "        case (p, n, ng) if p + n < ng:\n",
    "            sent = 'strongly negative'\n",
    "        case (p, n, ng) if ng > n > p:\n",
    "            sent = 'highly negative'\n",
    "        case (p, n, ng) if p == n == ng:\n",
    "            sent = 'perfectly neutral'\n",
    "        case _:\n",
    "            sent = 'undetermined'\n",
    "    \n",
    "    # Calculate the mean confidence\n",
    "    conf = filtered_df[sent_scr].mean() * 100\n",
    "\n",
    "    # Generating the final sentiment\n",
    "    if pos + ntrl + neg != 0:\n",
    "        # Concatenating sentiment and confidence\n",
    "        gen_sent = f'The general sentiment is {sent}, with an average confidence of {conf:.1f}%.'\n",
    "    else:\n",
    "        # When no sentment available due to no reviews\n",
    "        gen_sent = 'Cannot confirm sentiment due to a lack of reviews.'\n",
    "\n",
    "    # Returning sentiment\n",
    "    return gen_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to scrape each business info and reviews\n",
    "def get_business_overview(driver,lat,long,i):\n",
    "    \"\"\"\n",
    "    Extracts business overview details from a webpage using BeaugtifulSoup and Selenium.\n",
    "\n",
    "    Args:\n",
    "        driver (selenium.webdriver): The Selenium WebDriver instance.\n",
    "        lat (list): List of latitude values.\n",
    "        long (list): List of longitude values.\n",
    "        i (int): Index to access latitude and longitude.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: A dataframe containing business overview details or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get page source and parse it with BeautifulSoup\n",
    "        response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract business details\n",
    "        business_name = response.find('h1', class_='DUwDvf lfPIob')\n",
    "        avg_rating = response.find('div', class_='fontDisplayLarge')\n",
    "        address = response.find('div', class_='rogA2c')\n",
    "        \n",
    "\n",
    "        # Check if elements are found and extract text\n",
    "        business_name_text = business_name.text if business_name else \"Not available\"\n",
    "        avg_rating_text = avg_rating.text if avg_rating else \"Not available\"\n",
    "        address_text = address.text if address else \"Not available\"\n",
    "        \n",
    "        # Get latitude and longitude\n",
    "        lat_value = lat[i] if i < len(lat) else \"Not available\"\n",
    "        long_value = long[i] if i < len(long) else \"Not available\"\n",
    "        \n",
    "        # # parse out buss_add and bus_city from address_text\n",
    "        # address_list = address_text.split(',')\n",
    "        # bus_add = address_list[0]\n",
    "        # bus_city = address_list[1]\n",
    "        \n",
    "        #create dictionary with business attributes\n",
    "        business_dic = {\n",
    "                'bus_id': business_name_text,\n",
    "                'avg_rating': avg_rating_text,\n",
    "                'bus_add': address_text,\n",
    "                'lat': lat_value,\n",
    "                'lon': long_value\n",
    "                }\n",
    "        \n",
    "        # generate dataframe\n",
    "        business_df = pd.DataFrame([business_dic])\n",
    "        return business_df\n",
    "\n",
    "    except IndexError as e:\n",
    "            print(f\"Error: Index out of range - {e}\")\n",
    "            return {'error': 'Index out of range'}\n",
    "    except NoSuchElementException as e:\n",
    "        print(f\"Error: Element not found - {e}\")\n",
    "        return {'error': 'Element not found'}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return {'error': 'Unexpected error'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error handling file_path function \n",
    "def read_csv_with_error_handling(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a pandas DataFrame with error handling for common file-related issues.\n",
    "\n",
    "    This function attempts to read a CSV file from the specified `file_path` into a pandas DataFrame.\n",
    "    It includes error handling for common issues such as file not found, empty files, parsing errors,\n",
    "    and permission errors. If an error occurs, it prints an appropriate error message.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file to be read. It should be a valid file path string.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame created from the CSV file, or `None` if an error occurs.\n",
    "            - If the file is successfully read, the DataFrame is returned.\n",
    "            - If an error occurs, the function prints an error message and returns `None`.\n",
    "\n",
    "    Raises:\n",
    "        None: The function handles exceptions internally and does not raise them further.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to read the CSV file\n",
    "        url_df = pd.read_csv(file_path)\n",
    "        return url_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The file could not be parsed.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to read '{file_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to gather relevant data from the reviews result set obtained by parsing through HTML\n",
    "def get_review_summary(result_set):\n",
    "\n",
    "    \"\"\"\n",
    "    Gathers and summarizes review data from a set of review elements parsed from HTML.\n",
    "\n",
    "    This function extracts review text and ratings from a parsed HTML result set.\n",
    "    It compiles this information into a pandas DataFrame. If any issues are encountered during\n",
    "    extraction, such as missing elements, appropriate error handling is performed.\n",
    "\n",
    "    Args:\n",
    "        result_set (iterable): An iterable of BeautifulSoup elements containing review data.\n",
    "            Each element should represent an individual review with a `span` for review text\n",
    "            and an element with a class of 'kvMYJc' for the rating.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with two columns:\n",
    "            - 'review': A list of review texts.\n",
    "            - 'rating': A list of review ratings (typically a single character representing the rating).\n",
    "        \n",
    "        dict: If an exception is caught during processing, a dictionary with an error message is returned.\n",
    "            - For missing elements: {'error': 'Element not found'}\n",
    "            - For unexpected errors: {'error': 'Unexpected error'}\n",
    "\n",
    "    Raises:\n",
    "        NoSuchElementException: Raised by BeautifulSoup if the expected HTML elements are not found.\n",
    "        Exception: Catches any other unexpected errors that may occur.\n",
    "    \"\"\"\n",
    "        \n",
    "    # create empty review dictionary to add each review\n",
    "    rev_dict = {\n",
    "        'review' : [],\n",
    "        'rating' : []\n",
    "    }\n",
    "\n",
    "    # scrape the necessary review elements such as review text and rating\n",
    "    for result in result_set:\n",
    "        try:\n",
    "            review_text = result.find('span',class_='wiI7pd').text\n",
    "            review_rating = result.find(class_='kvMYJc')['aria-label']\n",
    "            review_rating = review_rating[0]\n",
    "            rev_dict['review'].append(review_text)\n",
    "            rev_dict['rating'].append(review_rating)\n",
    "        \n",
    "        # exception handling\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Error: Element not found - {e}\")\n",
    "            return {'error': 'Element not found'}\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return {'error': 'Unexpected error'}\n",
    "\n",
    "    reviews_summary = pd.DataFrame(rev_dict)\n",
    "    return reviews_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define web scrapper function\n",
    "def web_Scraper(file_path):\n",
    "\n",
    "    '''\n",
    "    Scrapes Google Maps via web driver and gather business information and reviews for the list of businesses in the imported file.\n",
    "\n",
    "    Note:\n",
    "        the following function must be defined before this function as they're nested:\n",
    "        read_csv_with_error_handling\n",
    "        get_business_overview \n",
    "        get_review_summary\n",
    "\n",
    "    Args:\n",
    "\n",
    "        filepath (str):     A string of the filepath that points the business url csv.\n",
    "\n",
    "    Returns:\n",
    "        df_list (list):     A list of dataframes each containing the business info and reviews data. Each dataframe represents a location.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #run csv file with error handling function\n",
    "    url_df = read_csv_with_error_handling(file_path)\n",
    "\n",
    "    # keep only the first 11 businesses from the business_urls file for performance purposes\n",
    "    url_df = url_df.head(11)\n",
    "\n",
    "    # append the Google maps business urls and their corresponding lat and long coordinates to their individual lists\n",
    "    url = url_df['url'].tolist()\n",
    "    lat = url_df['lat'].astype(str).tolist()\n",
    "    long = url_df['long'].astype(str).tolist()\n",
    "\n",
    "    # initiate driver\n",
    "    driver = webdriver.Chrome(service = ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "    # create for loop to parse through the each business location in the url list above\n",
    "    c = 0\n",
    "    df_list = []\n",
    "\n",
    "    # loop for the length of the url list\n",
    "    for i in range(0,len(url)):\n",
    "        c += 1\n",
    "        driver.get(url[i])\n",
    "        time.sleep(5)\n",
    "    \n",
    "        try:\n",
    "            # execute get_business_overview function to get main business info\n",
    "            business_df = get_business_overview(driver, lat, long, i)\n",
    "            \n",
    "            # try:\n",
    "            # navigate to Reviews tab\n",
    "            driver.find_element(By.CLASS_NAME, \"RWPxGd\").click()\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Find the total number of reviews\n",
    "            total_number_of_reviews = driver.find_element('xpath','//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[2]/div/div[2]/div[3]').text.split(\" \")[0]\n",
    "            total_number_of_reviews = int(total_number_of_reviews.replace(',','')) if ',' in total_number_of_reviews else int(total_number_of_reviews)\n",
    "\n",
    "            # scrape the first 50 reviews for efficiency - this variable can be automatically using function above if you want extract ALL reviews\n",
    "            total_number_of_reviews = 50\n",
    "\n",
    "            #Find scroll layout\n",
    "            scrollable_div = driver.find_element('xpath','//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "            #Scroll as many times as necessary to load all reviews - 10 reviews shown at a time\n",
    "            for i in range(0,(round(total_number_of_reviews/10 - 1))):\n",
    "                driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "                time.sleep(1)\n",
    "\n",
    "            ## parse HTML and Data Extraction\n",
    "            # loop over the number of reviews \n",
    "            next_item = driver.find_elements('xpath','//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[9]/div[1]/div/div')\n",
    "            time.sleep(3)\n",
    "\n",
    "            #expand review by click on 'more' button\n",
    "            for i in next_item:\n",
    "                button = i.find_elements(By.TAG_NAME,'button')\n",
    "                for m in button:\n",
    "                    if m.text == \"More\":\n",
    "                        m.click()\n",
    "                time.sleep(5)\n",
    "\n",
    "            # parse through the HTML \n",
    "            response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            reviews = response.find_all('div',class_ = 'jftiEf')\n",
    "\n",
    "            reviews_summary_df = get_review_summary(reviews)\n",
    "\n",
    "            #repeat business_df rows to then be concatenated with reviews summary df\n",
    "            bus_df_repeated = pd.concat([business_df] * len(reviews_summary_df), ignore_index=True)\n",
    "\n",
    "            #combine business summary and reviews summary df\n",
    "            business_summary_df = pd.concat([bus_df_repeated,reviews_summary_df],axis=1, ignore_index=True)\n",
    "            \n",
    "            # set column names\n",
    "            business_summary_df.columns = ['bus_id','avg_rating','bus_add','lat','lon','review','rating']\n",
    "            \n",
    "            #append each business_summary_df to df_list\n",
    "            df_list.append(business_summary_df)\n",
    "\n",
    "            #concat list of dfs to create master df with all locations\n",
    "            spooder_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Error: Element not found - {e}\")\n",
    "            return {'error': 'Element not found'}\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return {'error': 'Unexpected error'}\n",
    "\n",
    "        \n",
    "    return spooder_df    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sameple set trial\n",
    "\n",
    "After `roberto` was developed, we used the same dataset that it trained on to test its application. Knowing that the initial distribution of `stars` (or `label` in the case of this set) was controlled other elements of the sample set were explored to see what `roberto` trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the previously saved sample set\n",
    "# NOTE: This set was saved off following the finalization of `roberto`\n",
    "sample_set_df = pd.read_csv('Resources/sample_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Distribution\n",
    "\n",
    "# Building the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=sample_set_df, x='sent_label', palette='viridis')\n",
    "\n",
    "# Setting plot features\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Distribution_of_Sentiment_Labels.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this makes sense since an even distribution of ratings was selected to make for a balanced training dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review Count Distribution\n",
    "\n",
    "# Building the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=sample_set_df, x='review_count', bins=20, kde=True, color='skyblue')\n",
    "\n",
    "# Setting plot features\n",
    "plt.title('Distribution of Review Counts')\n",
    "plt.xlabel('Review Count')\n",
    "plt.ylabel('Number of Businesses')\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Distribution_of_Review_Counts.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: This illustrates that fewer businesses reveive the highest number of reviews, and emphasizes the importance of harnessing available reviews.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Score Distribution\n",
    "\n",
    "# Building the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=sample_set_df, x='sent_score', bins=20, kde=True, color='lightgreen')\n",
    "\n",
    "# Setting plot features\n",
    "plt.title('Distribution of Sentiment Scores\\n(Model Confidence)')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Saving the figure (commented out after initial save)\n",
    "# plt.savefig('Images/Distribution_of_Sentiment_Scores.png')\n",
    "\n",
    "# Displaying the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: This was to be expected. We knew* `roberto` *felt less confident with \"neutral\" and \"negative\" reviews, which comprised two-thirds (2/3) of our training dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "The web scraping process uses Selenium and BeautifulSoup to extract business and review data from Google Maps. Selenium navigates to business URLs from a CSV file, and BeautifulSoup parses details like names, ratings, and addresses after accessing the Reviews tab. The script also scrolls through and expands reviews to capture complete data, which is then compiled into Pandas DataFrames for each business and combined into a single structured dataset for frontend development.\n",
    "\n",
    "*Note: A chrome window will open to proceed through the scraping process. Do* ***NOT*** *close or click on any links within that window, as it will interrupt the web scraper!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the web_Scraper function with the business urls csv file as an input\n",
    "web_Scraper('Resources/business_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting spooder_df for later use (commented out because `.csv` in `Resources/`)\n",
    "# spooder_df.to_csv('./Resources/spooder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing `spooder_df`\n",
    "spooder_df = pd.read_csv('Resources/spooder.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Sentiment Analysis Model to the Web Scrapped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Roberto to web scrapping dataframe with Google Reviews\n",
    "# (commented out due to already applying to saved `.csv`)\n",
    "# roberto_df = apply_roberto(spooder_df,'review')\n",
    "# roberto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting roberto_df for later use (commented out because `.csv` in `Resources/`)\n",
    "# roberto_df.to_csv('./Resources/roberto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing `spooder_df`\n",
    "roberto_df = pd.read_csv('Resources/roberto.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function to get general sentiment per business to be used as input into chatgpt model \n",
    "general_sentiment_web_scrapping = general_sentiment(roberto_df, 'bus_id', 'Dulce de Leche Bakery', 'sent_label', 'sent_score')\n",
    "general_sentiment_web_scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function to add all reviews of a given business as a list\n",
    "review_list = reviews_list(roberto_df, 'bus_id', 'Dulce de Leche Bakery', 'bus_add', 'review')\n",
    "review_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Reviews from Selected Business to run ChatGPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Store the API key in a variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "llm=ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "# Define the format for the template.\n",
    "format = \"\"\"\n",
    "\n",
    "Provide a summary of the given reviews:{review_list} and three recommendations for ways in which to improve the business. The summary should capture the main points and key details of the text \n",
    "while conveying the author's intended meaning accurately. The recommendations should be actionable, clear and conscise, and there should always be three of them. Please ensure that the summary is well-organized and easy to read, \n",
    "with clear headings and subheadings to guide the reader through each section. The length of the summary should be appropriate to capture the main points and key details of the text, \n",
    "without including unnecessary information or becoming overly long. Please also ensure that there are always three recommendations for ways to improve the business.\n",
    "\n",
    "reviews = {review_list}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Construct the prompt template.\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"review_list\"],\n",
    "    template=format\n",
    ")\n",
    "\n",
    "# Construct a chain using this template.\n",
    "davidlingo = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input variable as a dictionary\n",
    "review_list = {\"review_list\": review_list}\n",
    "\n",
    "# Run the chain using the query as input and get the result.\n",
    "result = davidlingo.invoke(review_list)\n",
    "results = result[\"text\"]\n",
    "\n",
    "# split the results by new lines to extract review summary and business recommendations\n",
    "results_list = results.split('\\n')\n",
    "reviews_summary = results_list[1]\n",
    "recommendations = results_list[4]\n",
    "\n",
    "\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions (Pt 3)\n",
    "\n",
    "While trained on Yelp! data, and developed for Google Reviews, the goal of the application is to be as univerally applicable to business reviews as possible - regardless of the source. The following functions were developed with their annotated purposes in mind:\n",
    "\n",
    "| **Function** | **Notes** |\n",
    "| :--- | :---|\n",
    "| `unique_locs_df()` | Creates a DataFrame with all unique locations in a given dataset |\n",
    "| `location_details()` | Generates a dictionary with geographic coordinates for all locations of a given business <br> *Note: To be run on the DataFrame generated by* `unique_locs_df()` |\n",
    "| `build_map()` | Constructs a Scattermapbox based on the locations from `location_details()` |\n",
    "| `apply_davidlingo()` | Generates the final summary of a business' reviews, or recommendations for improvement based off the reviews and overall sentiment <br> *Note: To be used with the ouputs of* `reviews_list()` *and* `general_sentiment()` |\n",
    "\n",
    "Each function outlined in more detail below requires a DataFrame with the following features:\n",
    "\n",
    "| **Feature** | **Notes** |\n",
    "| :--- | :--- |\n",
    "| `bus_name_col` | A text column with the name of a business |\n",
    "| `bus_add` | A text column with the street address of a business' location |\n",
    "| `bus_lat` | A float column with the latitude coordinate for a business' location |\n",
    "| `bus_lon` | A float column with the longitude coordinate for a business' location |\n",
    "| `rev_col` | A text column with available reviews |\n",
    "| `sent_lbl` | A text column with the generated sentiment classification <br> *Note: Generated through* `apply_roberto()` |\n",
    "| `sent_scr` | A text column with the generated sentiment classification <br> *Note: Generated through* `apply_roberto()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve unique business locations\n",
    "def unique_locs_df(df, bus_name_col, bus_add, bus_lat, bus_lon):\n",
    "    '''\n",
    "    Gathers unique adresses and coordinates for unique locations into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        bus_name_col (str): A string with the feature name that contains the business name.\n",
    "        bus_add (str):      A string with the feature name that contains the business street address.\n",
    "        bus_lat (str):      A string with the feature name that contains the latitude of a location.\n",
    "        bus_lon (str):      A string with the feature name that contains the longitude of a location.\n",
    "    \n",
    "    Returns:\n",
    "        loc (DataFrame):    A DataFrame with only unique locations.\n",
    "\n",
    "    Raises:\n",
    "        KeyError:           If any passed str is not a valid column name in the DataFrame.\n",
    "        TypeError:          If `df` is not a DataFrame or if and feature is not a string.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    for param, name in zip(\n",
    "        [bus_name_col, bus_add, bus_lat, bus_lon],\n",
    "        ['bus_name_col', 'bus_add', 'bus_lat', 'bus_lon']\n",
    "    ):\n",
    "        if not isinstance(param, str):\n",
    "            raise TypeError(f\"The '{name}' parameter must be passed as a string.\")\n",
    "        if param not in df.columns:\n",
    "            raise KeyError(f\"Column '{param}' not found in DataFrame.\")\n",
    "    \n",
    "    # Generating a DataFrame of unique locations\n",
    "    locs = df[[bus_name_col, bus_add, bus_lat, bus_lon]].drop_duplicates()\n",
    "    \n",
    "    # Returning the DataFrame\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a `locations` list of dictionaries\n",
    "def location_details(df, bus_name_col, bus_name, bus_add, bus_lat, bus_lon):\n",
    "    '''\n",
    "    Transfers the latitude, longitude, and a concatenated identifier into a dictionary for later use in generating a Scattermapbox figure.\n",
    "\n",
    "    Note:\n",
    "        Advised to run `location_details()` on the DataFrame generated by `unique_locs_df()`\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame):     Any DataFrame with sufficient data.\n",
    "        bus_name_col (str): A string with the feature name that contains the business name.\n",
    "        bus_name (str):     A string of a specific business' name for which to map the locations.\n",
    "        bus_add (str):      A string with the feature name that contains the business street address.\n",
    "        bus_lat (str):      A string with the feature name that contains the latitude of a location.\n",
    "        bus_lon (str):      A string with the feature name that contains the longitude of a location.\n",
    "\n",
    "    Returns:\n",
    "        locs (dict):        A list of dictionaries with the necessary details for building a figure.\n",
    "    \n",
    "    Raises:\n",
    "        KeyError:           If any passed str is not a valid column name in the DataFrame.\n",
    "        TypeError:          If `df` is not a DataFrame or if and feature is not a string.\n",
    "        ValueError:         If `bus_name` is not a value in `bus_col_name`.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input `df` must be a pandas DataFrame.')\n",
    "    for param, name in zip(\n",
    "        [bus_name_col, bus_add, bus_lat, bus_lon],\n",
    "        ['bus_name_col', 'bus_add', 'bus_lat', 'bus_lon']\n",
    "    ):\n",
    "        if not isinstance(param, str):\n",
    "            raise TypeError(f\"The '{name}' parameter must be passed as a string.\")\n",
    "        if param not in df.columns:\n",
    "            raise KeyError(f\"Column '{param}' not found in DataFrame.\")\n",
    "    if bus_name not in df[bus_name_col].values:\n",
    "        raise ValueError(f\"'{bus_name}' not found in column '{bus_name_col}'.\")\n",
    "    \n",
    "    # Creating a list of features to retain\n",
    "    retain = [bus_name_col, bus_add, bus_lat, bus_lon]\n",
    "\n",
    "    # Filtering `df`\n",
    "    filtered_df = df[retain][df[bus_name_col] == bus_name].copy()\n",
    "\n",
    "    # Intializing a `name` feature\n",
    "\n",
    "    # Creating a concatenated `name` feature with a business' name and location address\n",
    "    filtered_df['loc_name'] = filtered_df[bus_name_col] + ' - ' + filtered_df[bus_add]\n",
    "\n",
    "    # Renaming features\n",
    "    filtered_df.rename(columns={bus_lat: 'lat', bus_lon: 'lon'}, inplace=True)\n",
    "\n",
    "    # Dropping features\n",
    "    filtered_df.drop([bus_name_col, bus_add], axis=1, inplace=True)\n",
    "\n",
    "    # Converting `filtered_df` to a dictionary\n",
    "    locs = filtered_df.to_dict('records')\n",
    "\n",
    "    # Returning list of dictionaries\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a map\n",
    "def build_map(locs):\n",
    "    '''\n",
    "    Generates and updates a Scattermapbox figure based on the location details previously generated.\n",
    "\n",
    "    Note:\n",
    "        To be run on the dictionary returned by `location_details()`.\n",
    "\n",
    "    Args:\n",
    "        locs (dict):    A dictionary containing the latitude and longitude coordinates, as well as the business name and street address, of all given locations for that business.\n",
    "    \n",
    "    Returns:\n",
    "        fig (fig):      A Scattermapbox formated to an appropriate zoom level and centered on all given locations for a business.\n",
    "    \n",
    "    Raises:\n",
    "        TypeError:      If `locs` is not a list of dictionaries, or if the dictionaries do not contain the expected keys.\n",
    "        KeyError:       If any of the expected keys are missing from the dictionaries.\n",
    "        ValueError:     If `locs` is empty, or if latitude and longitude values are not valid numbers.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(locs, list) or not all(isinstance(loc, dict) for loc in locs):\n",
    "        raise TypeError(\"`locs` must be a list of dictionaries.\")\n",
    "    required_keys = {'lat', 'lon', 'loc_name'}\n",
    "    for loc in locs:\n",
    "        if not required_keys.issubset(loc):\n",
    "            raise KeyError(f\"Each dictionary in `locs` must contain the keys: {required_keys}.\")\n",
    "    if not locs:\n",
    "        raise ValueError(\"`locs` cannot be an empty list.\")\n",
    "\n",
    "    # Generating location text\n",
    "    hover_text = [loc['loc_name'] for loc in locs]\n",
    "\n",
    "    # Generating location lat and lon\n",
    "    lat_loc = [loc['lat'] for loc in locs]\n",
    "    lon_loc = [loc['lon'] for loc in locs]\n",
    "\n",
    "    # Calculating middle point for lat and lon\n",
    "    lat_mean = sum(lat_loc)/len(lat_loc)\n",
    "    lon_mean = sum(lon_loc)/len(lon_loc)\n",
    "\n",
    "    # Calculating borders of locatoins\n",
    "    lat_min, lat_max = min(lat_loc), max(lat_loc)\n",
    "    lon_min, lon_max = min(lon_loc), max(lon_loc)\n",
    "\n",
    "    # Calculating size of borders\n",
    "    lat_diff = lat_max - lat_min\n",
    "    lon_diff = lon_max - lon_min\n",
    "\n",
    "    # Using `log()` to scale zoom based on distances at slower rates for larger geographic areas\n",
    "    zoom = min(7 - math.log(lat_diff + 0.1), 7 - math.log(lon_diff + 0.1))\n",
    "\n",
    "    # Creating the map figure\n",
    "    fig = go.Figure(go.Scattermapbox(\n",
    "        lat=lat_loc,\n",
    "        lon=lon_loc,\n",
    "        mode='markers',\n",
    "        hovertext=hover_text,\n",
    "        marker=dict(size=10)\n",
    "    ))\n",
    "\n",
    "    # Updating layout with map style and properties\n",
    "    fig.update_layout(\n",
    "        mapbox={\n",
    "            'style': 'open-street-map',\n",
    "            'center': {'lon': lon_mean, 'lat': lat_mean},\n",
    "            'zoom': zoom\n",
    "        },\n",
    "        margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    # Returning figure\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate OpenAI summary or recommendations\n",
    "def apply_davidlingo(reviews):\n",
    "    '''\n",
    "    Assesses the reviews for a given business to then outputs a summary of those reviews and generates actionable feedback.\n",
    "\n",
    "    Note:\n",
    "        To be run after `reviews_list()`.\n",
    "    \n",
    "    Args:\n",
    "        reviews (list):     A list of reviews for a given business, as generates by `reviews_list()`\n",
    "    \n",
    "    Returns:\n",
    "        summary (str):      A [??]] with a summary of the provided reviews.\n",
    "        feedback (list):    A [??]] with a the feedback based on the reviews.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError:         If `reviews` is not a list or is empty.\n",
    "        RuntimeError:       If the `davidlingo.invoke()` call fails or returns unexpected results.\n",
    "    '''\n",
    "    # Raises\n",
    "    if not isinstance(reviews, list) or len(reviews) == 0:\n",
    "        raise ValueError(\"The `reviews` argument must be a non-empty list.\")\n",
    "    \n",
    "    # Placing the list of reviews into a dictionary\n",
    "    review_list = {'review_list': reviews}\n",
    "\n",
    "    try:\n",
    "        # Running the `davidlingo` chain\n",
    "        result = davidlingo.invoke(review_list)\n",
    "    except Exception as e:\n",
    "        # Raises\n",
    "        raise RuntimeError(f\"Failed to invoke `davidlingo`: {str(e)}\")\n",
    "\n",
    "    # Check for expected result format\n",
    "    if 'text' not in result:\n",
    "        # Raises\n",
    "        raise RuntimeError(\"The `davidlingo` response did not contain the expected `text` key.\")\n",
    "    \n",
    "    # Storing the results\n",
    "    results = result['text']\n",
    "\n",
    "    # Initializing summary and feedback\n",
    "    reviews_summary = \"\"\n",
    "    feedback = []\n",
    "    \n",
    "    # Split the text into lines\n",
    "    results_list = results.split('\\n')\n",
    "\n",
    "    # Identify the starting point of recommendations\n",
    "    recommendations_title = \"Recommendations for Improvement:\"\n",
    "    for idx, line in enumerate(results_list):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"1.\") or line.startswith(\"2.\") or line.startswith(\"3.\"):\n",
    "            # All lines from here are feedback\n",
    "            feedback = results_list[idx:]  \n",
    "            break\n",
    "        if recommendations_title in line:\n",
    "            # Remove from summary line\n",
    "            line = line.replace(recommendations_title, \"\").strip()  \n",
    "        # Collect summary lines\n",
    "        reviews_summary += line + \" \"  \n",
    "\n",
    "    # Cleaning unwanted characters from summary and recommendations\n",
    "    reviews_summary = re.sub(r'[##*]', '', reviews_summary).strip()\n",
    "    feedback = [re.sub(r'[##*]', '', item).strip() for item in feedback]\n",
    "\n",
    "    # Adding \"Recommendations for Improvement:\" with a line break\n",
    "    feedback.insert(0, recommendations_title)\n",
    "\n",
    "    # Returning summary and feedback\n",
    "    return reviews_summary, feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SpooderAppâ„¢**\n",
    "\n",
    "With `roberto` and `davidlingo` at the ready, it was time to build them a home. It was time... for `SpooderAppâ„¢`!\n",
    "\n",
    "`SpooderAppâ„¢`, developed in __[Dash](https://dash.plotly.com/)__, is the interactive medium through which users will interact with our models directly. With data scraped from our curated list of businesses, we set out to build `SpooderAppâ„¢` as a demonstrative tool to prove our hypothesis correct. That yes, we *can* leverage reviews into accurate sentiment analysis and actionable feedback for businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Components;\n",
    "\n",
    "To prepare `SpooderAppâ„¢`'s loading state, a few components needed to be etablished prior to initializing the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "*Loading the map on the US map because, well... We're largely american students, so it made sense?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default map centered on the US\n",
    "fig_placeholder = go.Figure(go.Scattermapbox())\n",
    "fig_placeholder.update_layout(\n",
    "    mapbox={\n",
    "        'style': \"open-street-map\",\n",
    "        'center': {'lon': -98.583, 'lat': 39.833},\n",
    "        'zoom': 2.5\n",
    "    },\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0},\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame\n",
    "\n",
    "*Making the interchanging of datasets simple without needed to refactor the app, itself*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a DataFrame to be used for the app\n",
    "app_df = roberto_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: During testing, it was discovered that a difference of 'de' and 'De' in 'Dulce De Leche Bakery' records was causing the business to populate twice in our dropdown menu. With* `davidlingo` *having the current limitation of processessing only so many reviews per business, we made the decition to drop the lesser populated instance of the business (lowercase 'de') for a cleaner proof of concept.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row where 'bus_id' is 'Dulce de Leche Bakery'\n",
    "app_df = app_df[app_df['bus_id'] != 'Dulce de Leche Bakery']\n",
    "\n",
    "# Verify if the row was dropped\n",
    "app_df.loc[app_df['bus_id'] == 'Dulce de Leche Bakery']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of business names\n",
    "\n",
    "*For user input selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of business names\n",
    "drop_opts = business_names_list(app_df, 'bus_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location details\n",
    "\n",
    "*For making the rest of the code below easier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame of uniqur locations\n",
    "uniq_locs = unique_locs_df(app_df, 'bus_id', 'bus_add', 'lat', 'lon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown guide\n",
    "\n",
    "*For on-screen, in-app assistance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading markdown content for guide\n",
    "with open('Resources/SpooderApp_Guide.md', 'r') as file:\n",
    "    guide_content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logo\n",
    "\n",
    "`SpooderAppâ„¢` *is too important to NOT have a logo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring logo path\n",
    "logo_path = 'Images/SpooderApp_Logo_Inverted_Color.png'\n",
    "\n",
    "# Reading in logo using OpenCV (with unchanged flag to keep transparency)\n",
    "logo_read = cv2.imread(logo_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Converting to RGBA if not already\n",
    "# Checking if already BGRA, since OpenCV uses for 4 channels\n",
    "if logo_read.shape[2] == 4:  \n",
    "    rgba_logo = cv2.cvtColor(logo_read, cv2.COLOR_BGRA2RGBA)\n",
    "else:\n",
    "    # Converting to RGBA if not already\n",
    "    rgba_logo = cv2.cvtColor(logo_read, cv2.COLOR_BGR2RGBA) \n",
    "\n",
    "# Converting the image to a PIL Image to handle base64 conversion\n",
    "rgba_pil = Image.fromarray(rgba_logo)\n",
    "\n",
    "# Converting image to base64\n",
    "buffered = BytesIO()\n",
    "rgba_pil.save(buffered, format=\"PNG\")  # Save the PIL image to the buffer\n",
    "spooder_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Preparing logo source for `html.Img()` component of app\n",
    "spooder_logo = f\"data:image/png;base64,{spooder_str}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App Development;\n",
    "\n",
    "The stage was set, the players at the ready... It was time for `SpooderAppâ„¢` to truly shine.\n",
    "\n",
    "*Note: While a monstrous cell, all components of* `SpooderAppâ„¢` *were developed in once cell for simplicity and consistency's sake.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize app\n",
    "app = Dash(external_stylesheets=[dbc.themes.QUARTZ])\n",
    "\n",
    "# App layout\n",
    "app.layout = html.Div([\n",
    "    # Wrapping the whole GUI in a stack for uinform formatting\n",
    "    dbc.Stack(\n",
    "        [\n",
    "            # Blank col for spacing (1/12 of parent container)\n",
    "            dbc.Col('', width=1),\n",
    "            # Col with all of GUI\n",
    "            dbc.Col(\n",
    "                [\n",
    "                    # Row for header\n",
    "                    dbc.Row(\n",
    "                        dbc.Col(\n",
    "                            # Header (as is evident by the `H1` method)\n",
    "                            html.Img(\n",
    "                                # Does whatever a SpooderAppâ„¢ can\n",
    "                                src=spooder_logo, \n",
    "                                # Placing in the middle of the page\n",
    "                                style={'height': '185px', 'width': 'auto'}\n",
    "                            ),\n",
    "                            # Get in the middle\n",
    "                            className='text-center' \n",
    "                        ),\n",
    "                        # Give us some room, please\n",
    "                        style={'margin-top': '20px', 'margin-bottom': '20px'}\n",
    "                    ),\n",
    "                    # Row for subheader\n",
    "                    dbc.Row(\n",
    "                        # Subheader (as is less evident by the `H3` method)\n",
    "                        html.H3(\n",
    "                            # Taglines are important\n",
    "                            'Leveraging business reviews to gain insights for potential improvements.',\n",
    "                            # Placing in the middle of the page\n",
    "                            style={'textAlign':'center'}\n",
    "                        ),\n",
    "                        # Buffer space\n",
    "                        style={'margin-bottom': '20px'}\n",
    "                    ),\n",
    "                    # Row for business name and ratings\n",
    "                    dbc.Row(\n",
    "                        [\n",
    "                            # Col for user input\n",
    "                            dbc.Col(\n",
    "                                # InputGroup for user input\n",
    "                                dbc.InputGroup(\n",
    "                                    [\n",
    "                                        # Dropdown menu for user input\n",
    "                                        dbc.DropdownMenu(\n",
    "                                            # Instructions for user input\n",
    "                                            label = 'Select a business',\n",
    "                                            # To know it's user input\n",
    "                                            id = 'business_dropdown',\n",
    "                                            # Selections for user input\n",
    "                                            children = [\n",
    "                                                dbc.DropdownMenuItem(\n",
    "                                                    name,\n",
    "                                                    id=f\"menu_item_{i}\",\n",
    "                                                    style={'color': 'grey'}\n",
    "                                                ) for i, name in enumerate(drop_opts)\n",
    "                                            ],\n",
    "                                            # Making it pretty ([insert sparkles here])\n",
    "                                            class_name='btn-info'\n",
    "                                        ),\n",
    "                                        # Not actually user input, but reflects it\n",
    "                                        dbc.InputGroupText(\n",
    "                                            # Blank until user input selected\n",
    "                                            children='',\n",
    "                                            # To know where to put user input\n",
    "                                            id='chld_nm',\n",
    "                                            # Making it pretty, but not AS pretty\n",
    "                                            class_name='form-control'\n",
    "                                        )\n",
    "                                    ],\n",
    "                                    # Be tall, but only so tall, please\n",
    "                                    style={'width': '100%', 'height': '60px'}\n",
    "                                ),\n",
    "                                # 6/12 of parent container, because math\n",
    "                                width=6\n",
    "                            ),\n",
    "                            # Col for average rating information\n",
    "                            dbc.Col(\n",
    "                                # Card display for averate rating information\n",
    "                                dbc.Card(\n",
    "                                    # Blank until user input selected\n",
    "                                    children='',\n",
    "                                    # To know where to put average rating information\n",
    "                                    id='avg_rtng',\n",
    "                                    # Making it pretty-ish\n",
    "                                    body=True,\n",
    "                                    # Be no taller than the column to your left\n",
    "                                    style={\n",
    "                                        'width': '100%',\n",
    "                                        'height': '60px',\n",
    "                                        'display': 'flex',\n",
    "                                        'align-items': 'left',\n",
    "                                        'justify-content': 'center'\n",
    "                                    }\n",
    "                                ),\n",
    "                                # 3/12 of parent container, or 1/4 but HTML/CSS doesn't like quarters as much\n",
    "                                width=3\n",
    "                            ),\n",
    "                            # Column for total reviews information\n",
    "                            dbc.Col(\n",
    "                                # Card display for total reviews information\n",
    "                                dbc.Card(\n",
    "                                    # Blank until user input selected\n",
    "                                    children='',\n",
    "                                    # To know where to put total reviews information\n",
    "                                    id='tot_rvws',\n",
    "                                    # Making it pretty-ish like its sibling to the left\n",
    "                                    body=True,\n",
    "                                    # You must be this short to display\n",
    "                                    style={\n",
    "                                        'width': '100%',\n",
    "                                        'height': '60px',\n",
    "                                        'display': 'flex',\n",
    "                                        'align-items': 'left',\n",
    "                                        'justify-content': 'center'\n",
    "                                    }\n",
    "                                ),\n",
    "                                # 3/12 of parent container, beacuse 12 - 6 - 3 leaves 3\n",
    "                                width=3\n",
    "                            )\n",
    "                        ],\n",
    "                        # Usually a good place to begin - The beginning\n",
    "                        justify='start',\n",
    "                        # Matching buffer space for that glossy, uniform look ([more sparkles])\n",
    "                        style={'margin-bottom': '20px'},\n",
    "                    ),\n",
    "                    # \"Row\" for map and accordion\n",
    "                    dbc.Stack(\n",
    "                        [\n",
    "                            # Col for map\n",
    "                            dbc.Col(\n",
    "                                # Map\n",
    "                                dcc.Graph(figure=fig_placeholder, id='bus_map'),\n",
    "                                # 5/12 of parent container, because the map wanted to be special\n",
    "                                width=5\n",
    "                            ),\n",
    "                            # Col for accordion\n",
    "                            dbc.Col(\n",
    "                                # Unfortunately, an accodion menu, not a Weird Al cameo\n",
    "                                dbc.Accordion(\n",
    "                                    [\n",
    "                                        # Menu item for reviews\n",
    "                                        dbc.AccordionItem(\n",
    "                                            # Paragraph - in the loosest sense - for reviews\n",
    "                                            html.P(\n",
    "                                                # To know where to put the reviews\n",
    "                                                id='reviews',\n",
    "                                                # Blank until user input selected\n",
    "                                                children='',\n",
    "                                                # Only be so tall, and scroll if longer\n",
    "                                                style={'max-height': '295px', 'overflow-y': 'auto'}\n",
    "                                            ),\n",
    "                                            # So you know it's got the reviews in it\n",
    "                                            title='Reviews'\n",
    "                                        ),\n",
    "                                        # Menu item for sentiment analysis\n",
    "                                        dbc.AccordionItem(\n",
    "                                            html.P(\n",
    "                                                # To know where to put the sentiment analysis\n",
    "                                                id='sentiment',\n",
    "                                                # Blank until user input selected\n",
    "                                                children='',\n",
    "                                                # Overkill, since this will only ever be a single line of text\n",
    "                                                style={'max-height': '295px', 'overflow-y': 'auto'}\n",
    "                                            ),\n",
    "                                            # To identify it as the container for the sentiment analysis\n",
    "                                            title='Sentiment Analysis'\n",
    "                                        ),\n",
    "                                        # Menu item for recommendations\n",
    "                                        dbc.AccordionItem(\n",
    "                                            html.P(\n",
    "                                                # To know where to put the OpenAI feedback\n",
    "                                                id='feedback',\n",
    "                                                # Blank until user input selected\n",
    "                                                children='',\n",
    "                                                # Only be so tall, and scroll if longer\n",
    "                                                style={'max-height': '295px', 'overflow-y': 'auto'}\n",
    "                                            ),\n",
    "                                            # For the purposes of labeling it as the recepticle for feedback\n",
    "                                            title='Feedback'\n",
    "                                        )\n",
    "                                    ]\n",
    "                                ),\n",
    "                                # Again, a good palce to begin\n",
    "                                align='start',\n",
    "                                # 7/12 of parent container, because that's what was left and it looks good\n",
    "                                width=7\n",
    "                            )\n",
    "                        ],\n",
    "                        # That's what was meant by \"row\", earlier - go this way <-->\n",
    "                        direction='horizontal',\n",
    "                        # Little bit of breathing room in there, too, please\n",
    "                        gap=1\n",
    "                    ),\n",
    "                    # Row for markdown guide\n",
    "                    dbc.Row(\n",
    "                        # Markdown guide\n",
    "                        dcc.Markdown(\n",
    "                            # Content for the markdown guide\n",
    "                            guide_content,\n",
    "                            # Making the markdown guide pretty\n",
    "                            style={\n",
    "                                'margin-top': '50px',\n",
    "                                'padding': '20px',\n",
    "                                'background-color': 'rgba(255, 255, 255, 0.35)', # This one is super important!\n",
    "                                'border-radius': '10px'\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                ],\n",
    "                # 10/12 of parent container, because this really is the star of the show, right here\n",
    "                width=10\n",
    "            ),\n",
    "             # Blank col for spacing (1/12 of parent container)\n",
    "            dbc.Col('', width=1),\n",
    "        ],\n",
    "        # Another go this way <--> bit\n",
    "        direction='horizontal',\n",
    "        # We like negative space, let's have more of that between things\n",
    "        gap=1\n",
    "    )\n",
    "])\n",
    "\n",
    "# Callback to populate the `DropdownMenu`\n",
    "@callback(\n",
    "    Output('chld_nm', 'children'),\n",
    "    Output('avg_rtng', 'children'),\n",
    "    Output('tot_rvws', 'children'),\n",
    "    Output('bus_map', 'figure'),\n",
    "    Output('reviews', 'children'),\n",
    "    Output('sentiment', 'children'),\n",
    "    Output('feedback', 'children'),\n",
    "    [Input(f\"menu_item_{i}\", \"n_clicks\") for i in range(len(drop_opts))],\n",
    "    [State(f\"menu_item_{i}\", \"children\") for i in range(len(drop_opts))]\n",
    ")\n",
    "def update_content(*args):\n",
    "    # Default states for elements\n",
    "    load_input = 'Use the dropdown menu on the left'\n",
    "    load_avg_rtng = 'Average Rating: '\n",
    "    load_tot_rvws = 'Total Available Reviews: '\n",
    "    load_fig = fig_placeholder\n",
    "    load_revs = 'Select a business to see reviews.'\n",
    "    load_sent = 'Select a business to generate sentiment analysis.'\n",
    "    load_feedback = 'Select a business to generate dynamic feedback.'\n",
    "\n",
    "    # Confirming a dropdown selection has been made\n",
    "    ctx = callback_context\n",
    "    if ctx.triggered:\n",
    "        # Finding which business was clicked\n",
    "        selected_item_id = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "        # Finding the index of the clicked business\n",
    "        selected_index = int(selected_item_id.split('_')[-1])\n",
    "        # Getting the selected business name\n",
    "        selected_business = args[len(drop_opts) + selected_index]\n",
    "\n",
    "        # Getting the average rating for the selected business\n",
    "        # NOTE: Since `avg_rating` is stores on a per-business basis,\n",
    "        # not a per-record basis, the first record's value will suffice\n",
    "        rvw_avg = app_df.loc[app_df['bus_id'] == selected_business, 'avg_rating'].iloc[0]\n",
    "        # Returning the average rating value\n",
    "        avg_rtng = f'Average Rating: {rvw_avg:.1f}'\n",
    "\n",
    "        # Gathering the reviews for the selected business\n",
    "        reviews = reviews_list(app_df, 'bus_id', selected_business, 'bus_add', 'review')\n",
    "        # Calculating the total number of reviews\n",
    "        if len(reviews) >= 1:\n",
    "            # If 1 or more, returning a count of available reviews\n",
    "            rev_tot = f'Total Available Reviews: {len(reviews)}'\n",
    "            # Preparing an empty list\n",
    "            rev_list = []\n",
    "            # Appending each review into `rev_list` with HTML formatting\n",
    "            for rev in reviews:\n",
    "                rev_list.append(html.P([rev.split(':\\n')[0], ':', html.Br(), rev.split(':\\n')[1], html.Br()]))\n",
    "        else:\n",
    "            rev_tot = 'Total Available Reviews: 0'\n",
    "            rev_list = 'Too few reviews available to display.'\n",
    "\n",
    "        # Preparing location details for the selected business\n",
    "        locations = location_details(uniq_locs,'bus_id', selected_business, 'bus_add','lat', 'lon')\n",
    "        # Building map based on locations for the selected business\n",
    "        fig = build_map(locations)\n",
    "\n",
    "        # Gather the general sentiment for the selected business\n",
    "        gen_sent = general_sentiment(app_df, 'bus_id', selected_business, 'sent_label', 'sent_score')\n",
    "\n",
    "        # Generate OpenAI response\n",
    "        reviews_summary, feedback = apply_davidlingo(reviews)\n",
    "        \n",
    "        # Creating an empy list to hold the formatted content\n",
    "        formatted_content = []\n",
    "\n",
    "        # Appending content\n",
    "        # Appending summary of reviews\n",
    "        formatted_content.append(html.P(reviews_summary))\n",
    "        # Adding a line break for readability\n",
    "        formatted_content.append(html.Br())\n",
    "        # Adding each reommendation as a separate paragraph\n",
    "        for recommendation in feedback:\n",
    "            # Esnuring not an empty line\n",
    "            if recommendation.strip():\n",
    "                # Adding recommendation line\n",
    "                formatted_content.append(html.P(recommendation))\n",
    "                # Adding a line break for readability\n",
    "                formatted_content.append(html.Br())\n",
    "        # Combining list into a single Div\n",
    "        feedback_component = html.Div(formatted_content)\n",
    "\n",
    "        # Returning the label corresponding to the clicked item\n",
    "        return selected_business, avg_rtng, rev_tot, fig, rev_list, gen_sent, feedback_component\n",
    "    # Returning original placeholder text if none selected\n",
    "    return load_input, load_avg_rtng, load_tot_rvws, load_fig, load_revs, load_sent, load_feedback\n",
    "\n",
    "# Launch app (in browser tab) (comment out if running in notebook)\n",
    "app.run(jupyter_mode='tab')\n",
    "# Launch app (in notebook) (uncomment to run)\n",
    "# app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Thoughts**\n",
    "\n",
    "`SpooderAppâ„¢` was designed to provide businesses with a practical way to track and imporove their consumer experiences by focusing soley on customer feedback. With an effectively accurate sentiment analysis model in `roberto`, the versitilaty of our web scraping tools, and the power of our OpenAI LangChain in `davidlingo`, we feel that we exceeded - if not merely met - our goals. While there is still room for imporovements and further developments (such as handling multiple languages, allowing for ad hoc searches, and training `roberto` on a larger sample size), our app has more than laid the foundations for a marketable and applicable utility due to its simple and intuitive display, integration of multiple NLP technologies, and robust capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Citations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yelp Open Dataset\n",
    "\n",
    "Yelp Inc. (2021). *Yelp Open Dataset*. Retrieved from __[https://www.yelp.com/dataset](https://www.yelp.com/dataset)__.\n",
    "\n",
    "*Note: The findings and applications of this study are those of the authors and do not necessarily reflect the views or opinions of Yelp Inc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "ai_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
