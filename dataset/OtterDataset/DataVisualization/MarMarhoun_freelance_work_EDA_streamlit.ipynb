{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/RnaIwCTWO0xLdii3oTZi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/EDA_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA dashboard using streamlit\n",
        "\n"
      ],
      "metadata": {
        "id": "hBVJxy7pcHRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VanUSNUNjW3Q",
        "outputId": "e02a7a59-ced5-47b9-846c-282470195d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, importlib-metadata, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.41 importlib-metadata-6.11.0 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.29.0 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ],
      "source": [
        "# Installing Necessary Libraries:\n",
        "\n",
        "!pip install pandas numpy matplotlib seaborn nltk wordcloud streamlit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading and Reading the Custom Dataset:\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose a custom dataset...\", type=[\"csv\", \"txt\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # For csv file\n",
        "    if uploaded_file.name.endswith('.csv'):\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # For txt file\n",
        "    elif uploaded_file.name.endswith('.txt'):\n",
        "        df = pd.read_csv(uploaded_file, sep=\"\\t\")\n",
        "\n",
        "    # For other types of files\n",
        "    else:\n",
        "        st.write(\"Unsupported File Format. Please upload a csv or txt file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxuDPvIjmogp",
        "outputId": "be133639-8712-4561-ccdf-e56ecfebbbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-01-10 13:43:01.273 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Exploratory Data Analysis (EDA):\n",
        "\n",
        "if st.button(\"Perform Basic EDA\"):\n",
        "    st.write(\"Number of Rows and Columns: \", df.shape)\n",
        "\n",
        "    st.write(\"Column Names: \", df.columns)\n",
        "\n",
        "    st.write(\"First 5 Rows of the Dataset:\")\n",
        "    st.write(df.head())\n",
        "\n",
        "    st.write(\"Summary Statistics:\")\n",
        "    st.write(df.describe())\n",
        "\n",
        "    st.write(\"Number of Missing Values in Each Column:\")\n",
        "    st.write(df.isnull().sum())"
      ],
      "metadata": {
        "id": "oU31blI9m1Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Correlation Matrix:\n",
        "\n",
        "if st.button(\"Visualize Correlation Matrix\"):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "    st.pyplot(plt)"
      ],
      "metadata": {
        "id": "wf1lJNoym_T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Word Cloud for a Text Column:\n",
        "\n",
        "st.write(\"Word Cloud for a Text Column:\")\n",
        "text_column = st.selectbox(\"Select a text column for word cloud:\", df.columns)\n",
        "\n",
        "if text_column:\n",
        "    text = ' '.join(df[text_column].dropna().tolist())\n",
        "    stopwords = set(STOPWORDS)\n",
        "\n",
        "    wc = WordCloud(background_color='white', max_words=200, stopwords=stopwords)\n",
        "    wc.generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    st.pyplot(plt)"
      ],
      "metadata": {
        "id": "WVxFZSWAnEpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the Distribution of a Variable:\n",
        "\n",
        "st.write(\"Distribution of a Variable:\")\n",
        "variable = st.selectbox(\"Select a variable for distribution plot:\", df.columns)\n",
        "\n",
        "if variable:\n",
        "    st.write(df[variable].value_counts(normalize=True))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.countplot(x=variable, data=df)\n",
        "    st.pyplot(plt)"
      ],
      "metadata": {
        "id": "OkDAmQGsm56q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA dashboard using streamlit\n",
        "\n"
      ],
      "metadata": {
        "id": "EqvlP9fR-ybW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit plotly pandas numpy"
      ],
      "metadata": {
        "id": "ZXUFtrAS-xnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # to simulate a real time data, time loop\n",
        "import numpy as np  # np mean, np random\n",
        "import pandas as pd  # read csv, df manipulation\n",
        "import plotly.express as px  # interactive charts\n",
        "import streamlit as st  # ğŸˆ data web app development\n",
        "\n",
        "# read csv from a URL\n",
        "@st.experimental_memo\n",
        "def get_data() -> pd.DataFrame:\n",
        "    return pd.read_csv(\"https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv\")\n",
        "\n",
        "df = get_data()"
      ],
      "metadata": {
        "id": "zp8i8kQI-4S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.set_page_config(\n",
        "    page_title=\"Real-Time Data Science Dashboard\",\n",
        "    page_icon=\"âœ…\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# dashboard title\n",
        "st.title(\"Real-Time / Live Data Science Dashboard\")\n",
        "\n",
        "# top-level filter\n",
        "job_filter = st.selectbox(\"Select the Job\", pd.unique(df[\"job\"]))\n",
        "\n",
        "# dataframe filter\n",
        "df = df[df[\"job\"] == job_filter]"
      ],
      "metadata": {
        "id": "z-MYvFoH-7oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create three columns\n",
        "kpi1, kpi2, kpi3 = st.columns(3)\n",
        "\n",
        "# fill in those three columns with respective metrics or KPIs\n",
        "kpi1.metric(\n",
        "    label=\"Age â³\",\n",
        "    value=round(avg_age),\n",
        "    delta=round(avg_age) - 10,\n",
        ")\n",
        "\n",
        "kpi2.metric(\n",
        "    label=\"Married Count ğŸ’\",\n",
        "    value=int(count_married),\n",
        "    delta=-10 + count_married,\n",
        ")\n",
        "\n",
        "kpi3.metric(\n",
        "    label=\"A/C Balance ï¼„\",\n",
        "    value=f\"$ {round(balance,2)} \",\n",
        "    delta=-round(balance / count_married) * 100,\n",
        ")\n",
        "\n",
        "# create two columns for charts\n",
        "fig_col1, fig_col2 = st.columns(2)\n",
        "\n",
        "with fig_col1:\n",
        "    st.markdown(\"### First Chart\")\n",
        "    fig = px.density_heatmap(\n",
        "        data_frame=df, y=\"age_new\", x=\"marital\"\n",
        "    )\n",
        "    st.write(fig)\n",
        "\n",
        "with fig_col2:\n",
        "    st.markdown(\"### Second Chart\")\n",
        "    fig2 = px.histogram(data_frame=df, x=\"age_new\")\n",
        "    st.write(fig2)"
      ],
      "metadata": {
        "id": "6mGUi_Kj--YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.markdown(\"### Detailed Data View\")\n",
        "st.dataframe(df)\n",
        "\n",
        "# near real-time / live feed simulation\n",
        "for seconds in range(200):\n",
        "    df[\"age_new\"] = df[\"age\"] * np.random.choice(range(1, 5))\n",
        "    df[\"balance_new\"] = df[\"balance\"] * np.random.choice(range(1, 5))\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "b_LEW1Is_BQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "e8QyIWu1_DFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second example"
      ],
      "metadata": {
        "id": "H_zBN0XPEdgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Display the title and a brief description of the app\n",
        "st.title('Exploratory Data Analysis App')\n",
        "st.write('This app performs exploratory data analysis on the dataset.')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "st.subheader('First few rows of the dataset')\n",
        "st.write(data.head())\n",
        "\n",
        "# Display some basic statistics about the dataset\n",
        "st.subheader('Basic statistics')\n",
        "st.write(data.describe())\n",
        "\n",
        "# Create a histogram of a column\n",
        "st.subheader('Histogram of column_name')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['column_name'], kde=True)\n",
        "st.pyplot()\n",
        "\n",
        "# Create a scatterplot of two columns\n",
        "st.subheader('Scatterplot of column_1 and column_2')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='column_1', y='column_2', data=data)\n",
        "st.pyplot()\n",
        "\n",
        "# Create a correlation matrix\n",
        "st.subheader('Correlation matrix')\n",
        "corr = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "st.pyplot()\n",
        "\n",
        "# Perform some data cleaning\n",
        "st.subheader('Data cleaning')\n",
        "data = data.dropna()\n",
        "st.write('Number of rows dropped due to missing values:', data.shape[0] - data.shape[0])\n",
        "\n",
        "# Train a machine learning model\n",
        "# ...\n",
        "\n",
        "# Display the accuracy of the model\n",
        "st.subheader('Model accuracy')\n",
        "st.write('Model accuracy:', model_accuracy)\n",
        "\n",
        "# Add a sidebar to allow users to select a column to visualize\n",
        "st.sidebar.subheader('Select a column to visualize')\n",
        "column_name = st.sidebar.selectbox('', data.columns)\n",
        "\n",
        "# Display a histogram of the selected column\n",
        "st.sidebar.subheader('Histogram of selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data[column_name], kde=True)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a scatterplot of the selected column against another column\n",
        "st.sidebar.subheader('Scatterplot of selected column against another column')\n",
        "col_to_plot_against = st.sidebar.selectbox('', data.columns)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=column_name, y=col_to_plot_against, data=data)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a bar chart of the top 10 values in the selected column\n",
        "st.sidebar.subheader('Top 10 values in selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "top_10 = data[column_name].value_counts().head(10)\n",
        "sns.barplot(x=top_10.index, y=top_10.values, alpha=0.8)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a box plot of the selected column\n",
        "st.sidebar.subheader('Box plot of selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=column_name, data=data)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a correlation matrix with the selected column highlighted\n",
        "st.sidebar.subheader('Correlation matrix with selected column highlighted')\n",
        "corr = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='white')\n",
        "sns.scatterplot(x=column_name, y=column_name, s=10, color='red', alpha=0.5)\n",
        "st.sidebar.pyplot()"
      ],
      "metadata": {
        "id": "wwVIB24oEg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third example"
      ],
      "metadata": {
        "id": "1z0ZPSYLFQJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Display the title and a brief description of the app\n",
        "st.title('Exploratory Data Analysis App')\n",
        "st.write('This app performs exploratory data analysis on the dataset and trains machine learning models.')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "st.subheader('First few rows of the dataset')\n",
        "st.write(data.head())\n",
        "\n",
        "# Display some basic statistics about the dataset\n",
        "st.subheader('Basic statistics')\n",
        "st.write(data.describe())\n",
        "\n",
        "# Create a histogram of a column\n",
        "st.subheader('Histogram of column_name')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['column_name'], kde=True)\n",
        "st.pyplot()\n",
        "\n",
        "# Create a scatterplot of two columns\n",
        "st.subheader('Scatterplot of column_1 and column_2')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='column_1', y='column_2', data=data)\n",
        "st.pyplot()\n",
        "\n",
        "# Create a correlation matrix\n",
        "st.subheader('Correlation matrix')\n",
        "corr = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "st.pyplot()\n",
        "\n",
        "# Perform some data cleaning\n",
        "st.subheader('Data cleaning')\n",
        "data = data.dropna()\n",
        "st.write('Number of rows dropped due to missing values:', data.shape[0] - data.shape[0])\n",
        "\n",
        "# Prepare the data for machine learning\n",
        "X = data.drop('target_column', axis=1)\n",
        "y = data['target_column']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Display the accuracy of the model\n",
        "st.subheader('Model accuracy')\n",
        "st.write('Model accuracy:', accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Add a sidebar to allow users to select a column to visualize\n",
        "st.sidebar.subheader('Select a column to visualize')\n",
        "column_name = st.sidebar.selectbox('', data.columns)\n",
        "\n",
        "# Display a histogram of the selected column\n",
        "st.sidebar.subheader('Histogram of selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data[column_name], kde=True)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a scatterplot of the selected column against another column\n",
        "st.sidebar.subheader('Scatterplot of selected column against another column')\n",
        "col_to_plot_against = st.sidebar.selectbox('', data.columns)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=column_name, y=col_to_plot_against, data=data)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a bar chart of the top 10 values in the selected column\n",
        "st.sidebar.subheader('Top 10 values in selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "top_10 = data[column_name].value_counts().head(10)\n",
        "sns.barplot(x=top_10.index, y=top_10.values, alpha=0.8)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a box plot of the selected column\n",
        "st.sidebar.subheader('Box plot of selected column')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=column_name, data=data)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Display a correlation matrix with the selected column highlighted\n",
        "st.sidebar.subheader('Correlation matrix with selected column highlighted')\n",
        "corr = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='white')\n",
        "sns.scatterplot(x=column_name, y=column_name, s=10, color='red', alpha=0.5)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        "# Add a section for machine learning models\n",
        "st.subheader('Machine learning models')\n",
        "\n",
        "# Train a decision tree model\n",
        "tree_model = DecisionTreeClassifier()\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_y_pred = tree_model.predict(X_test)\n",
        "st.write('Decision tree model accuracy:', accuracy_score(y_test, tree_y_pred))\n",
        "\n",
        "# Train a random forest model\n",
        "forest_model = RandomForestClassifier()\n",
        "forest_model.fit(X_train, y_train)\n",
        "forest_y_pred = forest_model.predict(X_test)\n",
        "st.write('Random forest model accuracy:', accuracy_score(y_test, forest_y_pred))\n",
        "\n",
        "# Train a k-nearest neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_y_pred = knn_model.predict(X_test)\n",
        "st.write('K-nearest neighbors model accuracy:', accuracy_score(y_test, knn_y_pred))\n",
        "\n",
        "# Train a logistic regression model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "logistic_y_pred = logistic_model.predict(X_test)\n",
        "st.write('Logistic regression model accuracy:', accuracy_score(y_test, logistic_y_pred))\n",
        "\n",
        "# Train a support vector machine model\n",
        "svm_model = SVC(gamma='auto')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "st.write('Support vector machine model accuracy:', accuracy_score(y_test, svm_y_pred))\n",
        "\n",
        "# Add a section for visualization\n",
        "st.subheader('Visualization')\n",
        "\n",
        "# Visualize the data\n",
        "st.write(data.head())\n",
        "\n",
        "# Visualize the class distribution\n",
        "st.write('Class distribution:', data['Survived'].value_counts())\n",
        "\n",
        "# Display a histogram for the selected column\n",
        "st.write('Histogram for selected column:', column_name)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data[column_name], kde=True, bins=30)\n",
        "st.pyplot()\n",
        "\n",
        "# Display a correlation matrix with the selected column highlighted\n",
        "st.sidebar.subheader('Correlation matrix with selected column highlighted')\n",
        "corr = data.corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=0.5, linecolor='white')\n",
        "sns.scatterplot(x=column_name, y=column_name, s=10, color='red', alpha=0.5)\n",
        "st.sidebar.pyplot()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "xbF5_Z-4FUHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customer segmentation model and streamlit in python\n",
        "\n",
        "Here's the code to build a customer segmentation model using the K-means algorithm and deploy it using Streamlit.\n",
        "\n",
        "First, make sure you have installed all the necessary libraries by running:\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "K085UEy5dwqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas sklearn plotly streamlit"
      ],
      "metadata": {
        "id": "scNcVG3EeA4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then, create a Python file and import the necessary libraries:\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import streamlit as st\n",
        "\n",
        "# Next, load the customer data:\n",
        "\n",
        "data = pd.read_csv('customers.csv')\n",
        "\n",
        "# In this case, the CSV file contains columns such as \"CustomerId\", \"Age\", \"Annual Income\", etc.\n",
        "\n",
        "# Scale the features to be in the same range using the StandardScaler:\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(data[['Age', 'Annual Income', 'Spending Score']]])\n"
      ],
      "metadata": {
        "id": "d7sTK8RVeC1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the K-means model on the scaled features:\n",
        "kmeans = KMeans(n_clusters=5, random_state=0).fit(scaled_features)\n",
        "\n",
        "# Assign each customer to a cluster:\n",
        "\n",
        "data['Cluster'] = kmeans.labels_\n"
      ],
      "metadata": {
        "id": "lQ_YLD8TeSPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Streamlit app:\n",
        "\n",
        "st.title('Customer Segmentation')\n",
        "\n",
        "# Display the raw data\n",
        "st.subheader('Raw Data')\n",
        "st.write(data)\n",
        "\n",
        "# Display the clusters\n",
        "st.subheader('Clusters')\n",
        "st.write(data.groupby('Cluster').mean())\n",
        "\n"
      ],
      "metadata": {
        "id": "l-RmaiO8eaGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, run the Streamlit app:\n",
        "\n",
        "streamlit run your_python_file.py\n",
        "\n",
        "\n",
        "'''\n",
        "Replace your_python_file.py with the name of your Python file.\n",
        "\n",
        "Please note that you should have the 'customers.csv' file in the same directory as your Python file for this to work. Additionally, make sure to replace 'customers.csv' with the path to your actual data file.\n",
        "\n",
        "You can also customize the code to display different types of plots and add more interactive features to the app.\n",
        "'''"
      ],
      "metadata": {
        "id": "3cTO21rBee7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another code for this task\n",
        "\n",
        "\n",
        "In this updated code, the data file is uploaded through the streamlit interface instead of being read from a fixed path. Additionally, the user can select multiple columns for segmentation.\n",
        "\n",
        "To run the streamlit application, you can execute the following command in your terminal:"
      ],
      "metadata": {
        "id": "pPDwFVc3zv19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def plot_cluster(data):\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "    kmeans.fit(data_scaled)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=kmeans.labels_)\n",
        "    plt.title('Customer Segmentation')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_streamlit():\n",
        "    st.title('Customer Segmentation')\n",
        "\n",
        "    # Upload dataset\n",
        "    uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "    if uploaded_file is not None:\n",
        "        # load dataset\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "\n",
        "        # choose the columns for segmentation\n",
        "        selected_columns = st.multiselect(\"Select Columns for Segmentation\", df.columns)\n",
        "        if len(selected_columns) > 0:\n",
        "            # get the data\n",
        "            data = df[selected_columns].values\n",
        "\n",
        "            # visualize the cluster\n",
        "            plot_cluster(data)\n",
        "\n",
        "            # get the labels\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            # show the cluster distribution\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.hist(labels, bins=5)\n",
        "            ax.set_title('Cluster Distribution')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # show the details of the data points\n",
        "            st.write('Top 10 Data Points:')\n",
        "            st.write(df.head(10))\n",
        "\n",
        "\n",
        "run_streamlit()"
      ],
      "metadata": {
        "id": "wd7xwuG10Rx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another code for this task\n",
        "\n",
        "\n",
        "This code uses Streamlit to create an interactive application for building a customer segmentation model. It uses the KMeans algorithm from scikit-learn for clustering.\n",
        "\n",
        "The code loads a sample loan dataset and performs feature scaling using StandardScaler from scikit-learn.\n",
        "\n",
        "It takes the number of clusters as input from the user using a slider.\n",
        "\n",
        "The code performs KMeans clustering and attaches the cluster labels to the dataframe.\n",
        "\n",
        "The dataframe with the clustered data is displayed using Streamlit's st.write() function.\n",
        "\n",
        "The clustered data can be downloaded by the user in CSV format.\n",
        "\n",
        "To run the application, simply save the code in a Python file and execute it using a terminal or command prompt. Then, navigate to the URL provided by Streamlit in your browser to view and interact with the application."
      ],
      "metadata": {
        "id": "RhRM0qweHRBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import streamlit as st\n",
        "\n",
        "# Loading data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_train.csv')\n",
        "data.drop(columns=['Loan_ID'], inplace=True)\n",
        "\n",
        "# Normalizing data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Initializing streamlit\n",
        "st.title('Customer Segmentation App')\n",
        "\n",
        "# Getting user inputs\n",
        "st.subheader('Input Features')\n",
        "num_clusters = st.slider('Number of Clusters', 2, 20, 5)\n",
        "\n",
        "# Performing KMeans clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "kmeans.fit(data_scaled)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Attaching labels to dataframe\n",
        "data['Customer Segment'] = labels\n",
        "\n",
        "# Displaying the clustered data\n",
        "st.subheader('Clustered Data')\n",
        "st.write(data)\n",
        "\n",
        "# Downloading the clustered data\n",
        "csv = data.to_csv(index=False)\n",
        "st.download_button(\n",
        "    label=\"Download Data as CSV\",\n",
        "    data=csv,\n",
        "    file_name='clustered_customer_data.csv',\n",
        "    mime='text/csv',\n",
        ")"
      ],
      "metadata": {
        "id": "VuTJLuW6HVuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another code for this task\n",
        "\n",
        "\n",
        "1. Include elbow plot visualization to help users select the optimal number of clusters.\n",
        "2. Implement interactive plots using Plotly or Matplotlib to display customer segments.\n",
        "3. Use PCA to reduce dimensionality before performing KMeans clustering, to handle high-dimensional data and improve performance.\n",
        "Here is the updated code:\n",
        "\n",
        "\n",
        "In this updated code, we include an elbow plot to visualize the distortion and help users select the optimal number of clusters. We also reduce the dimensionality of the data using PCA before clustering it, to handle high-dimensional data and improve performance. Finally, we display an interactive scatter plot of the data, where each point is colored according to its assigned cluster.\n",
        "\n",
        "This updated code provides a more comprehensive and interactive user experience for building a customer segmentation model using Streamlit.</s"
      ],
      "metadata": {
        "id": "Xqe38C_pHbTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import streamlit as st\n",
        "\n",
        "# Loading data\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_train.csv')\n",
        "data.drop(columns=['Loan_ID'], inplace=True)\n",
        "\n",
        "# Normalizing data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Reducing dimensionality using PCA\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(data_scaled)\n",
        "\n",
        "# Plotting data with PCA\n",
        "fig = px.scatter(x=data_pca[:, 0], y=data_pca[:, 1], color=data['Customer Segment'])\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "# Elbow plot for determining optimal number of clusters\n",
        "distortions = []\n",
        "K = range(1, 20)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k)\n",
        "    kmeanModel.fit(data_scaled)\n",
        "    distortions.append(kmeanModel.inertia_)\n",
        "\n",
        "fig = px.line(x=K, y=distortions, labels={'x': 'Number of Clusters', 'y': 'Distortion'})\n",
        "st.plotly_chart(fig)\n",
        "\n",
        "# Getting user inputs\n",
        "st.subheader('Input Features')\n",
        "num_clusters = st.slider('Number of Clusters', 2, 20, 5)\n",
        "\n",
        "# Performing KMeans clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "kmeans.fit(data_scaled)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Attaching labels to dataframe\n",
        "data['Customer Segment'] = labels\n",
        "\n",
        "# Displaying the clustered data\n",
        "st.subheader('Clustered Data')\n",
        "st.write(data)\n",
        "\n",
        "# Downloading the clustered data\n",
        "csv = data.to_csv(index=False)\n",
        "st.download_button(\n",
        "    label=\"Download Data as CSV\",\n",
        "    data=csv,\n",
        "    file_name='clustered_customer_data.csv',\n",
        "    mime='text/csv',\n",
        ")"
      ],
      "metadata": {
        "id": "9UCgKmKoHkCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another code for this task\n",
        "\n",
        "\n",
        "Here is an enhanced and improved version of the for building a customeration model using Stream:\n",
        "\n",
        "This code includes the following improvements:\n",
        "\n",
        "Separation of concerns: The code is organized into functions, making it easier to understand and maintain.\n",
        "Error handling: The code includes error handling for missing or invalid data.\n",
        "Better use of Streamlit features: The code uses Streamlit's built-in form functionality to create a more user-friendly interface.\n",
        "Improved comments and documentation: The code includes comments and documentation to make it easier to understand.\n",
        "Note: This code assumes that you have a Snowflake account and have created a table called 'public.customer_database' with an 'INCOME' column. You will also need to create a 'pwd.json' file with your Snowflake connection parameters."
      ],
      "metadata": {
        "id": "_2re4lwUpcR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import snowflake.snowpark as snow\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import json\n",
        "import math\n",
        "\n",
        "# Configure page\n",
        "st.set_page_config(\n",
        "    page_title=\"Segmentation Tool\",\n",
        "    page_icon=\"ğŸª„\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# Create Session object\n",
        "def create_session_object():\n",
        "    connection_parameters = json.load(open('pwd.json'))\n",
        "    session = snow.Session.builder.configs(connection_parameters).create()\n",
        "    return session\n",
        "\n",
        "# Get minimum and maximum values for the INCOME column\n",
        "def get_aggregations(snow):\n",
        "    aggs = {}\n",
        "    aggs['income_min'] = int(snow.table('public.customer_database').agg(min(col('INCOME'))).collect()[0][0] - 1)\n",
        "    aggs['income_max'] = int(snow.table('public.customer_database').agg(max(col('INCOME'))).collect()[0][0] + 1)\n",
        "    return aggs\n",
        "\n",
        "# Get the number of customers in the selected segment\n",
        "def get_count(snow, income_min, income_max):\n",
        "    count_segment = snow.table('public.customer_database').filter(col('income').between(income_min, income_max)).count()\n",
        "    return count_segment\n",
        "\n",
        "# Store the selected segment in the public.sync_segment view\n",
        "def store_segment(snow, income_min, income_max):\n",
        "    sdf = snow.table('public.customer_database').filter(col('income').between(income_min, income_max))\n",
        "    sdf.createOrReplaceView('public.sync_segment')\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Create Snowpark session object\n",
        "    snow = create_session_object()\n",
        "\n",
        "    # Get minimum and maximum values for the INCOME column\n",
        "    aggs = get_aggregations(snow)\n",
        "\n",
        "    # Set the default values for the income slider\n",
        "    income_step = int(math.floor(math.log10((aggs['income_max'] - aggs['income_min']))))\n",
        "\n",
        "    # Set the title and header of the application\n",
        "    st.title('Segmentation tool')\n",
        "    st.header('Filters')\n",
        "\n",
        "    # Create a form with the income slider and submit button\n",
        "    with st.form(key='filter_form'):\n",
        "        filt_income_min, filt_income_max = st.slider(\n",
        "            label='Minimum income',\n",
        "            min_value=aggs['income_min'],\n",
        "            max_value=aggs['income_max'],\n",
        "            value=(aggs['income_min'], aggs['income_max']),\n",
        "            step=income_step\n",
        "        )\n",
        "        submitted = st.form_submit_button(label='Preview segment')\n",
        "\n",
        "    # Get the number of customers in the selected segment\n",
        "    if submitted:\n",
        "        count_segment = get_count(snow, filt_income_min, filt_income_max)\n",
        "        st.text(f'Customers in segment: {count_segment}.')\n",
        "\n",
        "        # Add a sync button to store the segment in Snowflake\n",
        "        if st.button(label='Sync segment'):\n",
        "            store_segment(snow, filt_income_min, filt_income_max)\n",
        "            st.write('Segment synced to Snowflake.')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "v7RAebX7pgNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another code for this task\n",
        "\n",
        "\n",
        "It seems like you're looking to build a custom deep-learning app using Streamlit and image processing models. Here's a step-by-step guide on how to do that:\n",
        "\n",
        "Train an image processing model: First, you need to train an image processing model for your specific task. You can use a pre-trained model like EfficientNet and fine-tune it on your dataset. Make sure to save the model as an h5 file, which will be used in the app.\n",
        "\n",
        "Create the Streamlit app: Start by setting up the Streamlit app with the necessary libraries and configurations. Then, create the user interface for uploading images and displaying the results.\n",
        "\n",
        "Load the model: Use the load_model() function from Keras to load the saved h5 file of your trained model.\n",
        "\n",
        "Process the uploaded image: Preprocess the uploaded image to match the input dimensions of your model. This includes resizing the image and normalizing the pixel values.\n",
        "\n",
        "Make predictions: Use the loaded model to make predictions on the processed image. You can use the predict() function from Keras to get the predicted class probabilities.\n",
        "\n",
        "Display the results: Display the predicted class and any additional information (like confidence scores) in the app. You can also include links to resources or suggested treatments based on the predicted class.\n",
        "\n",
        "Test the app: Test the app with sample images to ensure that it's working as expected.\n",
        "\n",
        "Deploy the app: Once you're satisfied with the app, you can deploy it using a cloud service like Heroku or Streamlit Sharing.\n",
        "\n",
        "Here's some sample code to get you started:"
      ],
      "metadata": {
        "id": "ZCxxU7ort7Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Define the image processing function\n",
        "def process_image(image):\n",
        "    # Preprocess the image here\n",
        "    img_array = np.array(image)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "    return img_array\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_class(image):\n",
        "    img_array = process_image(image)\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "    return predicted_class[0]\n",
        "\n",
        "# Define the app layout\n",
        "st.title('Custom Deep-Learning App')\n",
        "st.subheader('Upload an image to get started')\n",
        "\n",
        "# Add a file uploader\n",
        "uploaded_file = st.file_uploader('Choose an image...', type='jpg')\n",
        "\n",
        "# Add a button to trigger the prediction\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "    if st.button('Predict'):\n",
        "        predicted_class = predict_class(image)\n",
        "        st.write(f'Predicted Class: {predicted_class}')"
      ],
      "metadata": {
        "id": "ZkZh__uKuBEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depression prediction dashbord using streamlit\n",
        "\n",
        "Here is the code for a depression dashboard using Streamlit:\n",
        "\n"
      ],
      "metadata": {
        "id": "laZY0_YpY6bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # to simulate a real time data, time loop\n",
        "\n",
        "import numpy as np  # np mean, np random\n",
        "import pandas as pd  # read csv, df manipulation\n",
        "import plotly.express as px  # interactive charts\n",
        "import streamlit as st  # ğŸˆ data web app development\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Real-Time Data Science Dashboard\",\n",
        "    page_icon=\"âœ…\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# read csv from a github repo\n",
        "dataset_url = \"https://raw.githubusercontent.com/Lexie88rus/bank-marketing-analysis/master/bank.csv\"\n",
        "\n",
        "# read csv from a URL\n",
        "@st.experimental_memo\n",
        "def get_data() -> pd.DataFrame:\n",
        "    return pd.read_csv(dataset_url)\n",
        "\n",
        "df = get_data()\n",
        "\n",
        "# dashboard title\n",
        "st.title(\"Real-Time / Live Data Science Dashboard\")\n",
        "\n",
        "# top-level filters\n",
        "job_filter = st.selectbox(\"Select the Job\", pd.unique(df[\"job\"]))\n",
        "\n",
        "# creating a single-element container\n",
        "placeholder = st.empty()\n",
        "\n",
        "# dataframe filter\n",
        "df = df[df[\"job\"] == job_filter]\n",
        "\n",
        "# near real-time / live feed simulation\n",
        "for seconds in range(200):\n",
        "\n",
        "    df[\"age_new\"] = df[\"age\"] * np.random.choice(range(1, 5))\n",
        "    df[\"balance_new\"] = df[\"balance\"] * np.random.choice(range(1, 5))\n",
        "\n",
        "    # creating KPIs\n",
        "    avg_age = np.mean(df[\"age_new\"])\n",
        "\n",
        "    count_married = int(\n",
        "        df[(df[\"marital\"] == \"married\")][\"marital\"].count()\n",
        "        + np.random.choice(range(1, 30))\n",
        "    )\n",
        "\n",
        "    balance = np.mean(df[\"balance_new\"])\n",
        "\n",
        "    with placeholder.container():\n",
        "\n",
        "        # create three columns\n",
        "        kpi1, kpi2, kpi3 = st.columns(3)\n",
        "\n",
        "        # fill in those three columns with respective metrics or KPIs\n",
        "        kpi1.metric(\n",
        "            label=\"Age â³\",\n",
        "            value=round(avg_age),\n",
        "            delta=round(avg_age) - 10,\n",
        "        )\n",
        "\n",
        "        kpi2.metric(\n",
        "            label=\"Married Count ğŸ’\",\n",
        "            value=int(count_married),\n",
        "            delta=-10 + count_married,\n",
        "        )\n",
        "\n",
        "        kpi3.metric(\n",
        "            label=\"A/C Balance ï¼„\",\n",
        "            value=f\"$ {round(balance,2)} \",\n",
        "            delta=-round(balance / count_married) * 100,\n",
        "        )\n",
        "\n",
        "        # create two columns for charts\n",
        "        fig_col1, fig_col2 = st.columns(2)\n",
        "        with fig_col1:\n",
        "            st.markdown(\"### First Chart\")\n",
        "            fig = px.density_heatmap(\n",
        "                data_frame=df, y=\"age_new\", x=\"marital\"\n",
        "            )\n",
        "            st.write(fig)\n",
        "\n",
        "        with fig_col2:\n",
        "            st.markdown(\"### Second Chart\")\n",
        "            fig2 = px.histogram(data_frame=df, x=\"age_new\")\n",
        "            st.write(fig2)\n",
        "\n",
        "        st.markdown(\"### Detailed Data View\")\n",
        "        st.dataframe(df)\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "WxXAWZ7UZDAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second sample"
      ],
      "metadata": {
        "id": "ecfjw1vWOYjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit scikit-learn pandas"
      ],
      "metadata": {
        "id": "cgZf7BFRPNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = RandomForestClassifier()\n",
        "model.load_model(\"depression_model.pkl\")\n",
        "\n",
        "# Define a function to preprocess the input data\n",
        "def preprocess_input(input_data):\n",
        "    # Add your preprocessing code here\n",
        "    # For example, convert categorical variables to numerical, scale/normalize features, etc.\n",
        "    processed_data = pd.DataFrame(input_data, columns=[\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"])\n",
        "    return processed_data\n",
        "\n",
        "# Define a function to make predictions using the model\n",
        "def predict_depression(processed_data):\n",
        "    prediction = model.predict(processed_data)\n",
        "    return prediction\n",
        "\n",
        "# Define the Streamlit app\n",
        "def main():\n",
        "    # Set the page title\n",
        "    st.title(\"Depression Prediction Dashboard\")\n",
        "\n",
        "    # Add a description of the app\n",
        "    st.write(\"This dashboard allows you to predict the likelihood of depression based on various input features.\")\n",
        "\n",
        "    # Add input forms for the user to enter data\n",
        "    feature_1 = st.number_input(\"Enter feature 1 value:\")\n",
        "    feature_2 = st.number_input(\"Enter feature 2 value:\")\n",
        "    feature_3 = st.number_input(\"Enter feature 3 value:\")\n",
        "    feature_4 = st.number_input(\"Enter feature 4 value:\")\n",
        "\n",
        "    # Preprocess the input data\n",
        "    input_data = [feature_1, feature_2, feature_3, feature_4]\n",
        "    processed_data = preprocess_input(input_data)\n",
        "\n",
        "    # Make a prediction using the model\n",
        "    prediction = predict_depression(processed_data)\n",
        "\n",
        "    # Display the prediction result\n",
        "    if prediction == 0:\n",
        "        st.write(\"The predicted likelihood of depression is low.\")\n",
        "    else:\n",
        "        st.write(\"The predicted likelihood of depression is high.\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lVPHHyCqPbh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third sample"
      ],
      "metadata": {
        "id": "oMxCX-pxSNI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from PIL import Image\n",
        "\n",
        "st.title('Depression Prediction Dashboard')\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_data():\n",
        "    df = pd.read_csv('depression.csv')\n",
        "    df['Score'] = df['Score'].astype(int)\n",
        "    df['Diagnosis'] = df['Diagnosis'].astype(int)\n",
        "    return df\n",
        "\n",
        "def explore_data(df):\n",
        "    st.subheader('Data Overview')\n",
        "    st.write(df.head())\n",
        "    st.write(df.describe())\n",
        "\n",
        "def split_data(df):\n",
        "    features = df.columns[:-1]\n",
        "    target = df['Diagnosis']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df[features], target, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def build_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    return accuracy, confusion, report\n",
        "\n",
        "def main():\n",
        "    df = load_data()\n",
        "    explore_data(df)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    model = build_model(X_train, y_train)\n",
        "\n",
        "    accuracy, confusion, report = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    st.subheader('Model Accuracy')\n",
        "    st.write(f'Model accuracy is {accuracy:.2f}%')\n",
        "\n",
        "    st.subheader('Confusion Matrix')\n",
        "    st.write(pd.DataFrame(confusion, index=['Actual No', 'Actual Yes'], columns=['Predicted No', 'Predicted Yes']))\n",
        "\n",
        "    st.subheader('Classification Report')\n",
        "    st.write(report)\n",
        "\n",
        "    st.subheader('Predict Depression')\n",
        "    user_input = pd.DataFrame({\n",
        "        'Age': [st.slider('Age', 18, 100, 30)],\n",
        "        'Gender': [st.selectbox('Gender', ('Male', 'Female'))],\n",
        "        'Score': [st.slider('Score', 0, 100, 50)]\n",
        "    }, index=[0])\n",
        "\n",
        "    prediction = model.predict(user_input)\n",
        "\n",
        "    if prediction == 0:\n",
        "        st.write('Prediction: Non-Depressed')\n",
        "    else:\n",
        "        st.write('Prediction: Depressed')\n",
        "\n",
        "    # Add a section for uploading a custom image\n",
        "    uploaded_image = st.file_uploader('Upload an Image', type=['png', 'jpg', 'jpeg'])\n",
        "    if uploaded_image:\n",
        "        img = Image.open(uploaded_image)\n",
        "        st.image(img, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZuyyWk1FR0qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourth sample"
      ],
      "metadata": {
        "id": "9h4c2XbHStcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from PIL import Image\n",
        "\n",
        "st.title('Depression Prediction Dashboard')\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_data():\n",
        "    df = pd.read_csv('depression.csv')\n",
        "    df['Score'] = df['Score'].astype(int)\n",
        "    df['Diagnosis'] = df['Diagnosis'].astype(int)\n",
        "    return df\n",
        "\n",
        "def explore_data(df):\n",
        "    st.subheader('Data Overview')\n",
        "    st.write(df.head())\n",
        "    st.write(df.describe())\n",
        "\n",
        "def split_data(df):\n",
        "    features = df.columns[:-1]\n",
        "    target = df['Diagnosis']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df[features], target, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def build_model(X_train, y_train):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    return accuracy, confusion, report\n",
        "\n",
        "def main():\n",
        "    df = load_data()\n",
        "    explore_data(df)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    model = build_model(X_train, y_train)\n",
        "\n",
        "    accuracy, confusion, report = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    st.subheader('Model Accuracy')\n",
        "    st.write(f'Model accuracy is {accuracy:.2f}%')\n",
        "\n",
        "    st.subheader('Confusion Matrix')\n",
        "    st.write(pd.DataFrame(confusion, index=['Actual No', 'Actual Yes'], columns=['Predicted No', 'Predicted Yes']))\n",
        "\n",
        "    st.subheader('Classification Report')\n",
        "    st.write(report)\n",
        "\n",
        "    st.subheader('Predict Depression')\n",
        "    user_input = pd.DataFrame({\n",
        "        'Age': [st.slider('Age', 18, 100, 30)],\n",
        "        'Gender': [st.selectbox('Gender', ('Male', 'Female'))],\n",
        "        'Score': [st.slider('Score', 0, 100, 50)],\n",
        "        'Family_History': [st.selectbox('Family History', ('Yes', 'No'))],\n",
        "        'Friends_Support': [st.slider('Friends Support', 0, 10, 5)],\n",
        "        'Self_Esteem': [st.slider('Self-Esteem', 0, 20, 10)],\n",
        "        'Life_Satisfaction': [st.slider('Life Satisfaction', 0, 20, 10)],\n",
        "        'Negative_Thoughts': [st.slider('Negative Thoughts', 0, 20, 10)]\n",
        "    }, index=[0])\n",
        "\n",
        "    prediction = model.predict(user_input)\n",
        "\n",
        "    if prediction == 0:\n",
        "        st.write('Prediction: Non-Depressed')\n",
        "    else:\n",
        "        st.write('Prediction: Depressed')\n",
        "\n",
        "    # Add a section for uploading a custom image\n",
        "    uploaded_image = st.file_uploader('Upload an Image', type=['png', 'jpg', 'jpeg'])\n",
        "    if uploaded_image:\n",
        "        img = Image.open(uploaded_image)\n",
        "        st.image(img, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Add a section for displaying the top 10 most important features\n",
        "    st.subheader('Top 10 Most Important Features')\n",
        "    feature_importance = model.feature_importances_\n",
        "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "    top_features = df.columns[sorted_idx][:10]\n",
        "    top_importance = feature_importance[sorted_idx][:10]\n",
        "    st.write(pd.DataFrame({'Feature': top_features, 'Importance': top_importance}))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "xscPKF1wS1QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fifth sample"
      ],
      "metadata": {
        "id": "Zbj3FAcCTm95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/hads.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Preprocess the dataset\n",
        "data.dropna(inplace=True)\n",
        "data['Gender'] = data['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "data['Marital'] = data['Marital'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "X = data.drop('Depression', axis=1)\n",
        "y = data['Depression']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Depression Prediction Dashboard\")\n",
        "st.write(\"This dashboard predicts the likelihood of depression based on the Hospital Anxiety and Depression Scale (HADS) dataset.\")\n",
        "\n",
        "# User input form\n",
        "st.subheader(\"Enter your responses:\")\n",
        "age = st.number_input(\"Age\", min_value=16, max_value=99, value=16, step=1)\n",
        "gender = st.radio(\"Gender\", ('Male', 'Female'))\n",
        "marital = st.radio(\"Marital status\", ('Yes', 'No'))\n",
        "anxiety = st.slider(\"Anxiety score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "insomnia = st.slider(\"Insomnia score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "social_support = st.slider(\"Social support score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "\n",
        "# Preprocess user input\n",
        "user_input = np.array([age, gender == 'Female', marital == 'No', anxiety, insomnia, social_support]).reshape(1, -1)\n",
        "\n",
        "# Make prediction\n",
        "prediction = model.predict(user_input)\n",
        "\n",
        "# Display prediction\n",
        "if st.button(\"Predict\"):\n",
        "    if prediction[0] == 0:\n",
        "        st.write(\"The user is not likely to be depressed.\")\n",
        "    else:\n",
        "        st.write(\"The user is likely to be depressed. Please consult a mental health professional.\")\n",
        "\n",
        "# Display evaluation metrics\n",
        "st.subheader(\"Model Evaluation Metrics\")\n",
        "st.write(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "st.write(f\"Confusion Matrix:\\n{confusion}\")\n",
        "st.write(f\"Classification Report:\\n{report}\")\n",
        "\n",
        "# Display correlation matrix\n",
        "st.subheader(\"Correlation Matrix\")\n",
        "correlation = X.corr()\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(correlation, annot=True, cmap=\"coolwarm\", ax=ax)\n",
        "st.pyplot(fig)"
      ],
      "metadata": {
        "id": "iO1CaWi1ULCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "EXRg0_ZQUUDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sixth sample\n"
      ],
      "metadata": {
        "id": "HtSDlY3VZcRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/hads.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Preprocess the dataset\n",
        "data.dropna(inplace=True)\n",
        "data['Gender'] = data['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "data['Marital'] = data['Marital'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "X = data.drop('Depression', axis=1)\n",
        "y = data['Depression']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Depression Prediction Dashboard\")\n",
        "st.write(\"This dashboard predicts the likelihood of depression based on the Hospital Anxiety and Depression Scale (HADS) dataset.\")\n",
        "\n",
        "# User input form\n",
        "st.subheader(\"Enter your responses:\")\n",
        "age = st.number_input(\"Age\", min_value=16, max_value=99, value=16, step=1)\n",
        "gender = st.radio(\"Gender\", ('Male', 'Female'))\n",
        "marital = st.radio(\"Marital status\", ('Yes', 'No'))\n",
        "anxiety = st.slider(\"Anxiety score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "insomnia = st.slider(\"Insomnia score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "social_support = st.slider(\"Social support score (0-21)\", min_value=0, max_value=21, value=0, step=1)\n",
        "\n",
        "# Preprocess user input\n",
        "user_input = np.array([age, gender == 'Female', marital == 'No', anxiety, insomnia, social_support]).reshape(1, -1)\n",
        "\n",
        "# Make prediction\n",
        "prediction = model.predict(user_input)\n",
        "\n",
        "# Display prediction\n",
        "if st.button(\"Predict\"):\n",
        "    if prediction[0] == 0:\n",
        "        st.write(\"The user is not likely to be depressed.\")\n",
        "    else:\n",
        "        st.write(\"The user is likely to be depressed. Please consult a mental health professional.\")\n",
        "\n",
        "    # Depression threshold dropdown\n",
        "    threshold = st.slider(\"Depression Threshold\", min_value=0, max_value=21, value=8, step=1)\n",
        "    if prediction[0] > threshold:\n",
        "        st.write(f\"The user's depression score ({prediction[0]}) is above the threshold ({threshold}).\")\n",
        "    else:\n",
        "        st.write(f\"The user's depression score ({prediction[0]}) is below the threshold ({threshold}).\")\n",
        "\n",
        "# Display evaluation metrics\n",
        "st.subheader(\"Model Evaluation Metrics\")\n",
        "st.write(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "st.write(f\"Confusion Matrix:\\n{confusion}\")\n",
        "st.write(f\"Classification Report:\\n{report}\")\n",
        "\n",
        "# Display correlation matrix\n",
        "st.subheader(\"Correlation Matrix\")\n",
        "correlation = X.corr()\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(correlation, annot=True, cmap=\"coolwarm\", ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Display bar chart of depression score distribution\n",
        "st.subheader(\"Distribution of Depression Scores\")\n",
        "depression_scores = data['Depression']\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(depression_scores, bins=range(0, 22, 1))\n",
        "plt.xlabel(\"Depression Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "st.pyplot(plt.gcf())\n",
        "\n",
        "# Display scatter plot of age vs depression score\n",
        "st.subheader(\"Age vs Depression Score\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data['Age'], data['Depression'], s=10)\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Depression Score\")\n",
        "st.pyplot(plt.gcf())"
      ],
      "metadata": {
        "id": "s7poWwRuZhX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traffic sign classifier using deep learning and streamlit in python\n",
        "\n",
        "To improve the code for a traffic sign classifier using deep learning and Streamlit in Python, you can consider the following suggestions:\n",
        "\n",
        "Use a more powerful and accurate deep learning model:\n",
        "* You can use a more powerful and accurate deep learning model such as Yolo, EfficientDet, or Faster R-CNN for traffic sign detection and classification. These models are more accurate and faster than the traditional CNN models.\n",
        "\n",
        "* Use pre-trained weights: You can use pre-trained weights for the deep learning model to improve the accuracy and reduce the training time. You can use pre-trained weights from models such as ImageNet, COCO, or PASCAL VOC.\n",
        "\n",
        "* Use data augmentation: You can use data augmentation techniques such as rotation, scaling, flipping, and cropping to increase the size of the training dataset and improve the model's ability to generalize.\n",
        "\n",
        "* Use Streamlit's built-in functions for data loading and preprocessing: Streamlit provides built-in functions for data loading and preprocessing, such as st.cache and st.sidebar. You can use these functions to cache the data and preprocess it only once, reducing the loading time and improving the user experience.\n",
        "* Use Streamlit's interactive widgets: Streamlit provides interactive widgets such as sliders, checkboxes, and dropdown menus that you can use to allow users to interact with the model and adjust the parameters.\n",
        "* Use Streamlit's real-time prediction: Streamlit provides real-time prediction functionality that you can use to show the predicted class and confidence score in real-time as the user uploads an image.\n",
        "* Use Streamlit's custom components: Streamlit allows you to create custom components using React, Vue, or Angular. You can use custom components to create more complex and interactive user interfaces.\n",
        "* Use Streamlit's deployment options: Streamlit provides deployment options such as Streamlit sharing, Heroku, and AWS that you can use to deploy the app and share it with others.\n",
        "\n",
        "Here is an example of how you can improve the code for a traffic sign classifier using deep learning and Streamlit in Python:"
      ],
      "metadata": {
        "id": "aBhf4kgi_ywA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# Define a function for image preprocessing\n",
        "def preprocess_image(img):\n",
        "    img = load_img(img, target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "# Define a function for image classification\n",
        "def classify_image(img):\n",
        "    img = preprocess_image(img)\n",
        "    preds = model.predict(img)\n",
        "    preds = decode_predictions(preds, top=3)[0]\n",
        "    return preds\n",
        "\n",
        "# Define a function for real-time prediction\n",
        "def real_time_prediction(img):\n",
        "    st.write('Predicting...')\n",
        "    preds = classify_image(img)\n",
        "    st.write('Predicted class:', preds[0][1])\n",
        "    st.write('Confidence score:', preds[0][2])\n",
        "\n",
        "# Define a function for data loading and preprocessing\n",
        "@st.cache\n",
        "def load_data():\n",
        "    data = np.load('traffic_signs.npz')\n",
        "    X_train, y_train = data['X_train'], data['y_train']\n",
        "    X_test, y_test = data['X_test'], data['y_test']\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load the data\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "# Create the user interface\n",
        "st.title('Traffic Sign Classifier')\n",
        "st.write('Upload an image or take a picture to classify a traffic sign.')\n",
        "\n",
        "# Create a file uploader\n",
        "uploaded_file = st.file_uploader('Upload an image:', type='jpg')\n",
        "\n",
        "# Create a camera input\n",
        "if st.button('Take a picture'):\n",
        "    st.write('Taking picture...')\n",
        "    # Use OpenCV or another library to capture an image from the camera\n",
        "    # img = cv2.imread('traffic_sign.jpg')\n",
        "    # uploaded_file = ('traffic_sign.jpg', img, 'image/jpeg')\n",
        "\n",
        "# Display the uploaded image\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file)\n",
        "    st.image(img, caption='Uploaded image', use_column_width=True)\n",
        "    real_time_prediction(uploaded_file)\n",
        "\n",
        "# Create a dropdown menu for selecting a test image\n",
        "test_images = ['test_image_1.jpg', 'test_image_2.jpg', 'test_image_3.jpg']\n",
        "selected_image = st.selectbox('Select a test image:', test_images)\n",
        "\n",
        "# Display the selected test image\n",
        "img = Image.open(selected_image)\n",
        "st.image(img, caption='Selected test image', use_column_width=True)\n",
        "\n",
        "# Display the predicted class and confidence score\n",
        "preds = classify_image(selected_image)\n",
        "st.write('Predicted class:', preds[0][1])\n",
        "st.write('Confidence score:', preds[0][2])\n",
        "\n",
        "# Create a slider for adjusting the brightness\n",
        "brightness = st.slider('Brightness:', 0, 200, 100)\n",
        "\n",
        "# Apply the brightness adjustment to the selected test image\n",
        "img_bright = img.point(lambda i: i * brightness / 100)\n",
        "st.image(img_bright, caption='Brightness adjusted image', use_column_width=True)\n",
        "\n",
        "# Display the predicted class and confidence score for the brightness adjusted image\n",
        "preds_bright = classify_image(img_bright)\n",
        "st.write('Predicted class (brightness adjusted):', preds_bright[0][1])\n",
        "st.write('Confidence score (brightness adjusted):', preds_bright[0][2])\n",
        "\n",
        "# Create a checkbox for enabling data augmentation\n",
        "data_augmentation = st.checkbox('Enable data augmentation')\n",
        "\n",
        "# Apply data augmentation to the selected test image"
      ],
      "metadata": {
        "id": "tEMCS9VcAbT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second sample\n",
        "\n",
        "To create a custom deep-learning app using image processing models with Streamlit, you can follow the steps below:\n",
        "\n",
        "Train an image processing model using a dataset of your choice. In this example, we will use a mango leaf disease dataset. You can use the code provided in the previous answer to train the EfficientNet model on the dataset.\n",
        "Save the trained model as an h5 file using the model.save() function.\n",
        "Create a new Python file for the Streamlit app. Import the necessary libraries, including Streamlit, TensorFlow, and the trained model.\n",
        "Define a function to load the saved model. You can use the following code to load the model:"
      ],
      "metadata": {
        "id": "Mwc0ikqfAs8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model():\n",
        "    model = tf.keras.models.load_model('mango_model.h5')\n",
        "    return model\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = image.resize((224, 224))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def predict_class(model, image):\n",
        "    image = preprocess_image(image)\n",
        "    prediction = model.predict(image)\n",
        "    class_index = np.argmax(prediction)\n",
        "    class_name = class_names[class_index]\n",
        "    return class_name\n",
        "\n",
        "def display_prediction(class_name):\n",
        "    if class_name == 'Healthy':\n",
        "        st.balloons()\n",
        "        st.success(f\"The mango leaf is {class_name}!\")\n",
        "    else:\n",
        "        st.warning(f\"The mango leaf is {class_name}.\")\n",
        "        st.markdown(\"## Remedy\")\n",
        "        if class_name == 'Anthracnose':\n",
        "            st.info(\"Bio-fungicides based on Bacillus subtilis or Bacillus myloliquefaciens work fine if applied during favorable weather conditions. Hot water treatment of seeds or fruits (48Â°C for 20 minutes) can kill any fungal residue and prevent further spreading of the disease in the field or during transport.\")\n",
        "        elif class_name == 'Bacterial Canker':\n",
        "            st.info(\"Prune flowering trees during blooming when wounds heal fastest. Remove wilted or dead limbs well below infected areas. Avoid pruning in early spring and fall when bacteria are most active. If using string trimmers around the base of trees avoid damaging bark with breathable Tree Wrap to prevent infection.\")\n",
        "        # Add more conditions for other classes\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Mango Leaf Disease Detection\", page_icon=\":mango:\", initial_sidebar_state=\"auto\")\n",
        "    st.markdown(hide_streamlit_style, unsafe_allow_html=True)\n",
        "\n",
        "    model = load_model()\n",
        "\n",
        "    st.write(\"\"\"\n",
        "             # Mango Disease Detection with Remedy Suggestion\n",
        "             \"\"\")\n",
        "\n",
        "    file = st.file_uploader(\"\", type=[\"jpg\", \"png\"])\n",
        "\n",
        "    if file is None:\n",
        "        st.text(\"Please upload an image file\")\n",
        "    else:\n",
        "        image = Image.open(file)\n",
        "        st.image(image, use_column_width=True)\n",
        "        class_name = predict_class(model, image)\n",
        "        display_prediction(class_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "w0hb7BBGAlvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webbapp of a resume parsing using streamlit"
      ],
      "metadata": {
        "id": "-6XDcpGEZO5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide provides a detailed walkthrough of creating a web application using Streamlit to showcase a data scientist's resume and skills. The app includes an interactive chatbot powered by LlamaIndex and OpenAI to answer questions about the candidate's work experience. The project also incorporates CSS styles, Lottie animations, and various features to display the candidate's skills, timeline, and projects.\n",
        "\n",
        "To get started, clone the starter code from the GitHub repository:"
      ],
      "metadata": {
        "id": "dZ6Of29RZoPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/vicky-playground/portfolio-template/"
      ],
      "metadata": {
        "id": "ztZlrSXfZZSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, personalize the constant.py and images files with your own information. Customize the bio.txt file with a self-introduction for the chatbot.\n",
        "\n",
        "The app is divided into several pages, each with its own .py file:\n",
        "\n",
        "1_Home.py\n",
        "2_Resume.py\n",
        "3_Hobbies.py\n",
        "4_Projects.py\n",
        "5_Contact.py\n",
        "In this example, we will focus on the 1_Home.py file, which contains the code for the chatbot. The code is divided into several sections:\n",
        "\n",
        "A. Load the LlamaIndex and OpenAI libraries B. Define a function to ask the bot a question C. Define a function to get user input D. Display the user input form and chatbot response\n",
        "\n",
        "Here's the relevant code for the chatbot:"
      ],
      "metadata": {
        "id": "j3hC9FOxZ3Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import st from streamlit as st\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import openai\n",
        "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
        "\n",
        "# Set up OpenAI API key\n",
        "openai_api_key = st.sidebar.text_input('Enter your OpenAI API Key and hit Enter', type=\"password\")\n",
        "openai.api_key = (openai_api_key)\n",
        "\n",
        "# Load the bio.txt file\n",
        "documents = SimpleDirectoryReader(input_files=[\"bio.txt\"]).load_data()\n",
        "\n",
        "# Build a query engine\n",
        "def ask_bot(input_text):\n",
        "    # Define LLM\n",
        "    llm = ChatOpenAI(\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        openai_api_key=openai.api_key,\n",
        "    )\n",
        "    llm_predictor = LLMPredictor(llm=llm)\n",
        "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "\n",
        "    # Load index\n",
        "    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    # Query LlamaIndex and GPT-3.5 for the AI's response\n",
        "    PROMPT_QUESTION = \"\"\"You are an AI agent named Buddy helping answer questions about Vicky to recruiters. Introduce yourself when you are introducing who you are.\n",
        "    If you do not know the answer, politely admit it and let users know how to contact Vicky to get more information.\n",
        "    Human: {input}\n",
        "    \"\"\"\n",
        "    output = index.as_query_engine().query(PROMPT_QUESTION.format(input=input_text))\n",
        "    return output.response\n",
        "\n",
        "# Get user input\n",
        "def get_text():\n",
        "    input_text = st.text_input(\"You can send your questions and hit Enter to know more about me from my AI agent, Buddy!\", key=\"input\")\n",
        "    return input_text\n",
        "\n",
        "# Display user input form and chatbot response\n",
        "user_input = get_text()\n",
        "\n",
        "if user_input:\n",
        "    if not openai_api_key.startswith('sk-'):\n",
        "        st.warning('âš ï¸Please enter your OpenAI API key on the sidebar.', icon='âš ')\n",
        "    if openai_api_key.startswith('sk-'):\n",
        "        st.info(ask_bot(user_input))"
      ],
      "metadata": {
        "id": "s7tWlQY7Z6st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second sample\n",
        "\n",
        "Here's an enhanced version of the Resume Parser web app using Streamlit. This version includes better UI, improved error handling, and more detailed explanations of the parsing results.\n",
        "\n"
      ],
      "metadata": {
        "id": "O0ZyA_fGZ-Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# Load Spacy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a custom Spacy matcher to extract skills from the resume text\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "with open(\"skills.txt\", \"r\") as f:\n",
        "    skills = [line.strip() for line in f.readlines()]\n",
        "    matcher.add(\"SKILLS\", [nlp(skill) for skill in skills])\n",
        "\n",
        "# Title of the app\n",
        "st.title(\"Resume Parser\")\n",
        "\n",
        "# Upload a resume file\n",
        "uploaded_file = st.file_uploader(\"Upload a Resume (.pdf or .txt)\", type=[\"pdf\", \"txt\"])\n",
        "if uploaded_file:\n",
        "    # Read the resume text based on the file type\n",
        "    if uploaded_file.type == \"application/pdf\":\n",
        "        import PyPDF2\n",
        "\n",
        "        pdf_file = PyPDF2.PdfFileReader(uploaded_file)\n",
        "        resume_text = \"\"\n",
        "        for page_num in range(pdf_file.numPages):\n",
        "            page_obj = pdf_file.getPage(page_num)\n",
        "            resume_text += page_obj.extractText()\n",
        "    elif uploaded_file.type == \"text/plain\":\n",
        "        resume_text = uploaded_file.read().decode(\"utf-8\")\n",
        "\n",
        "    # Parse the resume text using Spacy\n",
        "    doc = nlp(resume_text)\n",
        "\n",
        "    # Display parsed entities\n",
        "    st.subheader(\"Entities\")\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"PERSON\", \"ORG\", \"EMAIL\", \"PHONE\"]:\n",
        "            st.write(f\"{ent.text} ({ent.label_})\")\n",
        "\n",
        "    # Extract skills using the custom matcher\n",
        "    st.subheader(\"Skills\")\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        skill = doc[start:end].text\n",
        "        st.write(f\"{skill} ({match_id})\")\n",
        "\n",
        "    # Display a message if no entities or skills are found\n",
        "    if not doc.ents and not matches:\n",
        "        st.write(\"No entities or skills were found in the resume.\")\n",
        "else:\n",
        "    st.write(\"Please upload a resume to parse.\")\n"
      ],
      "metadata": {
        "id": "cr0FsoK-aZYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the app, save the code in a Python file (e.g., app.py) and execute the following command in the terminal:"
      ],
      "metadata": {
        "id": "cFqljHURax7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "TyBrtvYtaxT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third sample (without streamlit)\n",
        "\n",
        "To create a web app for resume parsing, you can use a web framework like Flask or Django in Python. Here, I'll provide an example using Flask. You will need to install Flask if you haven't already:\n",
        "\n"
      ],
      "metadata": {
        "id": "_plTfcZ8cfMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Flask"
      ],
      "metadata": {
        "id": "B_gLSXiAcmew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new file called app.py and add the following code:\n"
      ],
      "metadata": {
        "id": "cnZ7wdLicv0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "app = Flask(__name__)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "@app.route('/parse', methods=['POST'])\n",
        "def parse_resume():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No file selected'}), 400\n",
        "\n",
        "    if file:\n",
        "        filename = file.filename\n",
        "        filepath = os.path.join(os.getcwd(), filename)\n",
        "        file.save(filepath)\n",
        "\n",
        "        if filename.endswith('.pdf'):\n",
        "            text = extract_text_from_pdf(filepath)\n",
        "        elif filename.endswith(('.doc', '.docx')):\n",
        "            text = extract_text_from_doc(filepath)\n",
        "        else:\n",
        "            return jsonify({'error': 'Unsupported file format'}), 400\n",
        "\n",
        "        os.remove(filepath)\n",
        "\n",
        "        name = extract_name(text)\n",
        "        contact_number = extract_contact_number_from_resume(text)\n",
        "        email = extract_email_from_resume(text)\n",
        "        skills = extract_skills_from_resume(text, ['Python', 'Data Analysis', 'Machine Learning', 'Communication'])\n",
        "        education = extract_education_from_resume(text)\n",
        "\n",
        "        return jsonify({\n",
        "            'name': name,\n",
        "            'contact_number': contact_number,\n",
        "            'email': email,\n",
        "            'skills': skills,\n",
        "            'education': education\n",
        "        }), 200\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    # Implement docx extraction here\n",
        "    pass\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    doc = nlp(resume_text)\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    patterns = [\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
        "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matcher.add('NAME', patterns=[pattern])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        return span.text\n",
        "\n",
        "    return None\n",
        "\n",
        "# Add other functions for extracting information here\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "uL6_JVlShNaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace # Implement docx extraction here with the appropriate code for extracting text from DOCX files.\n",
        "Run the application:\n"
      ],
      "metadata": {
        "id": "Q3UI3TDGhQ7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python app.py"
      ],
      "metadata": {
        "id": "EFfpWekWhUdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the web application at http://127.0.0.1:5000/ and upload a PDF or DOCX file to parse the resume.\n",
        "This example demonstrates a simple web application for a resume parser using Flask. You can further customize and enhance the application by adding more features, error handling, and user interfaces."
      ],
      "metadata": {
        "id": "grnZld-LhXAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST -H \"Content-Type: multipart/form-data\" -F \"file=@path/to/your/resume.pdf\" http://127.0.0.1:5000/parse"
      ],
      "metadata": {
        "id": "-F0ICDhguf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### with features\n",
        "\n",
        "Here is an updated version of the code that includes improved features and error handling:"
      ],
      "metadata": {
        "id": "Vjqc5cEjhaj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from pdfminer.high_level import extract_text\n",
        "from docx import Document\n",
        "\n",
        "app = Flask(__name__)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "@app.route('/parse', methods=['POST'])\n",
        "def parse_resume():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'No file selected'}), 400\n",
        "\n",
        "    if file:\n",
        "        filename = file.filename\n",
        "        filepath = os.path.join(os.getcwd(), filename)\n",
        "        file.save(filepath)\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.pdf'):\n",
        "                text = extract_text_from_pdf(filepath)\n",
        "            elif filename.endswith(('.doc', '.docx')):\n",
        "                text = extract_text_from_doc(filepath)\n",
        "            else:\n",
        "                return jsonify({'error': 'Unsupported file format'}), 400\n",
        "\n",
        "            os.remove(filepath)\n",
        "\n",
        "            name = extract_name(text)\n",
        "            contact_number = extract_contact_number_from_resume(text)\n",
        "            email = extract_email_from_resume(text)\n",
        "            skills = extract_skills_from_resume(text, ['Python', 'Data Analysis', 'Machine Learning', 'Communication'])\n",
        "            education = extract_education_from_resume(text)\n",
        "\n",
        "            return jsonify({\n",
        "                'name': name,\n",
        "                'contact_number': contact_number,\n",
        "                'email': email,\n",
        "                'skills': skills,\n",
        "                'education': education\n",
        "            }), 200\n",
        "        except Exception as e:\n",
        "            return jsonify({'error': str(e)}), 500\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    doc = Document(doc_path)\n",
        "    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return text\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    # Implement name extraction here\n",
        "    pass\n",
        "\n",
        "# Add other functions for extracting information here\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "cbS1DEF1hfWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourth sample (without streamlit)\n",
        "\n",
        "First, let's update the resume parsing module to support additional file formats:\n",
        "\n"
      ],
      "metadata": {
        "id": "QeyvLLjOvszl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfFileReader\n",
        "from docx import Document\n",
        "from rtfparse import RtfDocument\n",
        "\n",
        "def parse_resume(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf = PdfFileReader(file)\n",
        "            text = \"\"\n",
        "            for page_num in range(pdf.getNumPages()):\n",
        "                text += pdf.getPage(page_num).extractText()\n",
        "        return text\n",
        "    elif file_path.endswith('.doc') or file_path.endswith('.docx'):\n",
        "        doc = Document(file_path)\n",
        "        return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    elif file_path.endswith('.rtf'):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            rtf = RtfDocument(file)\n",
        "            return ' '.join(rtf.get_all_text())"
      ],
      "metadata": {
        "id": "DDKrmG8nv1ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Named Entity Recognition (NER)\n",
        "\n",
        "To add NER, we can use the nltk library to tokenize the resume text and extract named entities:"
      ],
      "metadata": {
        "id": "TDRD6-E6v5u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "    named_entities = nltk.ne_chunk(tags)\n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "l5ZVhxLav-R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add resume summarization\n",
        "\n",
        "We can use the bert-extractive-summarizer library to generate a summary of the resume:"
      ],
      "metadata": {
        "id": "DVE7oRaXwBP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer\n",
        "\n",
        "def summarize_resume(text):\n",
        "    model = Summarizer()\n",
        "    summary = model(text)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "zSsMmUvAwEJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integration with job boards and ATS\n",
        "\n",
        "Integrating with job boards and ATS can be done using their respective APIs. This may require creating an account and obtaining an API key from the respective platforms.\n",
        "\n",
        "User authentication and authorization\n",
        "\n",
        "To implement user authentication and authorization, we can use the flask-login library:"
      ],
      "metadata": {
        "id": "UMJfOUIWwIih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_login import LoginManager, UserMixin, login_user, logout_user\n",
        "\n",
        "app.secret_key = 'your_secret_key'\n",
        "login_manager = LoginManager()\n",
        "login_manager.init_app(app)\n",
        "\n",
        "class User(UserMixin):\n",
        "    pass\n",
        "\n",
        "@login_manager.user_loader\n",
        "def load_user(user_id):\n",
        "    return User(user_id)"
      ],
      "metadata": {
        "id": "U6LnXG6pwHga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization\n",
        "\n",
        "For data visualization, we can use libraries like matplotlib and seaborn. The exact implementation will depend on the specific visualization requirements.\n",
        "\n",
        "Machine learning algorithms\n",
        "\n",
        "To use machine learning algorithms, we can integrate the sklearn library and train models using the extracted named entities and resume summaries. This may require a dataset of labeled resumes for training and evaluation purposes.\n",
        "\n",
        "Apply the updated features to the resume parsing web application\n",
        "\n",
        "Once all the enhanced features have been implemented, we can apply them to the resume parsing web application. This may involve modifying the frontend to allow users to upload resumes in different formats and access the new features provided by the updated resume parsing module.\n",
        "\n",
        "Please note that implementing some of these features may require a significant amount of time and expertise in various domains such as natural language processing, machine learning, and web development. Additionally, obtaining an API key from job boards and ATS platforms may not be possible without prior arrangement or payment of fees."
      ],
      "metadata": {
        "id": "b6Gu26A9wRKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fifth sample (without streamlit)\n",
        "\n",
        "Data Extraction Accuracy Improvements:\n",
        "\n",
        "You can enhance the accuracy of your data extraction by using machine learning algorithms and Natural Language Processing (NLP) techniques. For instance, you can utilize libraries like SpaCy, NLTK, or even transformer-based models like BERT for better Named Entity Recognition (NER) and part-of-speech tagging.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxNmnXoE57mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"Sales experience, team player, excellent communication skills\")\n",
        "\n",
        "# Analyze syntax\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract entities and noun phrases\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "metadata": {
        "id": "38npszlM57CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Field Customization:\n",
        "\n",
        "Allow users to customize the data fields they want to extract from resumes. Provide a list of predefined common fields and enable users to add custom fields."
      ],
      "metadata": {
        "id": "eNkubdF76IPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_extracted_data(resume_text, selected_fields):\n",
        "    extracted_data = {}\n",
        "    for field in selected_fields:\n",
        "        # Implement data extraction logic for each field\n",
        "        extracted_data[field] = extract_field_from_resume(resume_text, field)\n",
        "    return extracted_data"
      ],
      "metadata": {
        "id": "a9zxNwph6M3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-time Analytics and Reporting:\n",
        "\n",
        "Display real-time analytics and reporting for parsed resumes, such as top skills, education levels, and experience.\n",
        "\n"
      ],
      "metadata": {
        "id": "RUbm3D3x6Qu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def analyze_resumes(resumes):\n",
        "    skill_counter = Counter()\n",
        "    education_counter = Counter()\n",
        "    experience_counter = Counter()\n",
        "\n",
        "    for resume in resumes:\n",
        "        # Extract relevant data from each resume\n",
        "        skills = resume[\"skills\"]\n",
        "        education = resume[\"education\"]\n",
        "        experience = resume[\"experience\"]\n",
        "\n",
        "        # Update counters\n",
        "        skill_counter.update(skills)\n",
        "        education_counter.update(education)\n",
        "        experience_counter.update(experience)\n",
        "\n",
        "    # Generate analytics report\n",
        "    report = {\n",
        "        \"top_skills\": skill_counter.most_common(10),\n",
        "        \"education_levels\": education_counter.most_common(10),\n",
        "        \"experience_levels\": experience_counter.most_common(10),\n",
        "    }\n",
        "    return report"
      ],
      "metadata": {
        "id": "dihHHQka6TJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By incorporating these features and enhancements into your web application, you can create a more robust and accurate resume parsing solution that provides users with a better experience and access to valuable insights.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lMxY31T_6VoR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6A9r4-Eu6TQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples"
      ],
      "metadata": {
        "id": "oKd80FRqkCjx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwr1QCA_mn6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA and machine learning using streamlit"
      ],
      "metadata": {
        "id": "9ZjeHwKyHkHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import streamlit as st\n",
        "\n",
        "# Loading the data\n",
        "data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Drop missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Select features for analysis\n",
        "feature_cols = ['pclass', 'sex', 'age', 'sibsp', 'parch']\n",
        "\n",
        "# One-hot encoding of categorical variables\n",
        "label = LabelEncoder()\n",
        "for col in feature_cols:\n",
        "    if data[col].dtype == type(object):\n",
        "        data[col] = label.fit_transform(data[col])\n",
        "\n",
        "# Data overview\n",
        "st.title('Titanic Dataset Exploratory Data Analysis')\n",
        "st.write(data.head())\n",
        "\n",
        "# Display data description\n",
        "st.subheader('Data Description')\n",
        "st.write(data.describe())\n",
        "\n",
        "# Display missing values\n",
        "st.subheader('Missing Values')\n",
        "st.write(data.isnull().sum())\n",
        "\n",
        "# Visualization: Distribution of Survived passengers\n",
        "st.subheader('Survival Rate')\n",
        "fig, ax = plt.subplots()\n",
        "sns.countplot(data['survived'], ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Visualization: Correlation Heatmap\n",
        "st.subheader('Correlation Heatmap')\n",
        "correlation_matrix = data[feature_cols].corr()\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Logistic Regression model for prediction\n",
        "st.subheader('Predicting Survival using Logistic Regression')\n",
        "\n",
        "X = data[feature_cols]\n",
        "y = data['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "predictions = logreg.predict(X_test)\n",
        "\n",
        "# Model performance metrics\n",
        "st.subheader('Model Performance Metrics')\n",
        "\n",
        "st.write('Accuracy Score:', accuracy_score(y_test, predictions))\n",
        "\n",
        "st.write('Confusion Matrix:')\n",
        "st.write(confusion_matrix(y_test, predictions))\n",
        "\n",
        "st.write('Classification Report:')\n",
        "st.write(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "5fUwDqk0kFCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "dTYOzfA9kbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display a word cloud for the names of passengers\n",
        "st.subheader('Word Cloud for Passenger Names')\n",
        "names = data['name'].dropna().tolist()\n",
        "name_text = ' '.join(names)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white').generate(name_text)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.imshow(wordcloud, interpolation='bilinear')\n",
        "ax.axis('off')\n",
        "\n",
        "st.pyplot(fig)"
      ],
      "metadata": {
        "id": "b5jqXLcHkYtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## customer segmentation model and online dataset using gradio"
      ],
      "metadata": {
        "id": "m_SRFnAAeJuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Dimensionality reduction\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {'n_clusters': [2, 3, 4, 5]}\n",
        "grid_search = GridSearchCV(KMeans(), param_grid, cv=5, scoring='silhouette_score')\n",
        "grid_search.fit(X_train_pca)\n",
        "model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "jqjsNQe6eLIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Cross-validation\n",
        "scores = cross_val_score(model, X_train_pca, y_train, cv=5, scoring='accuracy')\n",
        "print(f'Cross-validation scores: {scores}')\n",
        "\n",
        "# ROC curve\n",
        "y_prob = model.predict_proba(X_test_pca)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f'ROC AUC: {roc_auc}')\n",
        "\n",
        "# Precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob[:, 1])\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ],
      "metadata": {
        "id": "gO29C62geRLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input validation\n",
        "def validate_input(inputs):\n",
        "    if not isinstance(inputs, pd.DataFrame):\n",
        "        raise ValueError('Inputs must be a Pandas DataFrame.')\n",
        "    if not set(inputs.columns).issubset(set(model.feature_names_in_)):\n",
        "        raise ValueError('Inputs must contain only the following columns: {}.'.format(', '.join(model.feature_names_in_)))\n",
        "    return inputs\n",
        "\n",
        "iface = gr.Interface(fn=predict,\n",
        "                     inputs=gr.inputs.PandasDataFrame(type='open',\n",
        "                                                      columns=model.feature_names_in_,\n",
        "                                                      validate=validate_input),\n",
        "                     outputs='label')\n",
        "\n",
        "# Custom styles\n",
        "iface.style(css='.gradio-container { background-color: #f5f5f5; }')\n",
        "\n",
        "# Custom branding\n",
        "iface.set_page_title('Customer Segmentation Model')\n",
        "iface.set_html_title('Customer Segmentation Model')\n",
        "iface.set_html_element('h1', '<h1 style=\"color:#3F51B5;\">Customer Segmentation Model</h1>')\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "bACK7qzbeUbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer segmentation models and online dataset using gradio"
      ],
      "metadata": {
        "id": "X8OwGmXLXu-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import datasets\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load(\"model.pkl\")\n",
        "\n",
        "# Load the example dataset\n",
        "df = datasets.load_dataset(\"merve/supersoaker-failures\")\n",
        "df = df[\"train\"].to_pandas()\n",
        "\n",
        "def infer(input_dataframe):\n",
        "  return pd.DataFrame(model.predict(input_dataframe))\n",
        "\n",
        "# Create the Gradio interface\n",
        "inputs = gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(4,\"dynamic\"), label=\"Input Data\", interactive=1)\n",
        "outputs = gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(1, \"fixed\"), label=\"Predictions\", headers=[\"Failures\"])\n",
        "\n",
        "gr.Interface(fn = infer, inputs = inputs, outputs = outputs, examples = [[df.head(2)]]).launch()"
      ],
      "metadata": {
        "id": "ObIG8DnBX1_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = gr.Dataframe(row_count = (2, \"dynamic\"), col_count=(4,\"dynamic\"), label=\"Input Data\", interactive=1, style=\"font-size: 14px; font-family: Arial;\")"
      ],
      "metadata": {
        "id": "ivlhT4xlX5t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(fn=your_function, inputs=your_inputs, outputs=your_outputs).launch()"
      ],
      "metadata": {
        "id": "ijLouM2nX77Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced features"
      ],
      "metadata": {
        "id": "mgQbR78eY0s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import datasets\n",
        "\n",
        "# Load the segmentation models\n",
        "models = {\n",
        "    \"Model 1\": joblib.load(\"model1.pkl\"),\n",
        "    \"Model 2\": joblib.load(\"model2.pkl\"),\n",
        "    \"Model 3\": joblib.load(\"model3.pkl\"),\n",
        "}\n",
        "\n",
        "# Load the dataset\n",
        "df = datasets.load_dataset(\"merve/supersoaker-failures\")\n",
        "df = df[\"train\"].to_pandas()\n",
        "df.dropna(axis=0, inplace=True)\n",
        "\n",
        "# Define the inference function\n",
        "def infer(input_dataframe, model_name, num_clusters, regularization):\n",
        "    model = models[model_name]\n",
        "    # Preprocess the input data\n",
        "    X = preprocess(input_dataframe)\n",
        "    # Train the model\n",
        "    model.train(X, num_clusters=num_clusters, regularization=regularization)\n",
        "    # Predict the clusters\n",
        "    y_pred = model.predict(X)\n",
        "    # Postprocess the output data\n",
        "    output_dataframe = postprocess(y_pred)\n",
        "    return output_dataframe\n",
        "\n",
        "# Define the UI\n",
        "inputs = [\n",
        "    gr.Dataframe(label=\"Input Data\", interactive=1),\n",
        "    gr.Dropdown(choices=list(models.keys()), label=\"Segmentation Model\"),\n",
        "    gr.Number(label=\"Number of Clusters\"),\n",
        "    gr.Number(label=\"Regularization\"),\n",
        "]\n",
        "outputs = [gr.Dataframe(label=\"Predictions\", headers=[\"Clusters\"])]\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(fn=infer, inputs=inputs, outputs=outputs, examples=[df.head(2)])\n",
        "\n",
        "# Add real-time feedback\n",
        "progress = gr.Progress(min=0, max=100, label=\"Model Progress\")\n",
        "iface.add_component(progress)\n",
        "\n",
        "# Implement online dataset updates\n",
        "new_data = gr.Dataframe(label=\"New Data\", interactive=1)\n",
        "new_data_btn = gr.Button(label=\"Add New Data\")\n",
        "\n",
        "def update_dataset(new_data):\n",
        "    # Append the new data to the existing dataset\n",
        "    df = pd.concat([df, new_data], ignore_index=True)\n",
        "    # Save the updated dataset\n",
        "    datasets.save_dataset(\"merve/supersoaker-failures\", df)\n",
        "    # Retrain the models\n",
        "    for model_name in models:\n",
        "        model = models[model_name]\n",
        "        model.train(df, num_clusters=num_clusters, regularization=regularization)\n",
        "\n",
        "new_data_btn.change(update_dataset, new_data, _js=\"return {new_data: inputs.new_data.value}\")\n",
        "\n",
        "# Implement A/B testing\n",
        "test_btn = gr.Button(label=\"Perform A/B Testing\")\n",
        "\n",
        "def ab_test(model_name1, model_name2, num_clusters1, num_clusters2, regularization1, regularization2):\n",
        "    model1 = models[model_name1]\n",
        "    model2 = models[model_name2]\n",
        "    # Preprocess the input data\n",
        "    X = preprocess(input_dataframe)\n",
        "    # Train the models\n",
        "    model1.train(X, num_clusters=num_clusters1, regularization=regularization1)\n",
        "    model2.train(X, num_clusters=num_clusters2, regularization=regularization2)\n",
        "    # Predict the clusters\n",
        "    y_pred1 = model1.predict(X)\n",
        "    y_pred2 = model2.predict(X)\n",
        "    # Postprocess the output data\n",
        "    output_dataframe1 = postprocess(y_pred1)\n",
        "    output_dataframe2 = postprocess(y_pred2)\n",
        "    return output_dataframe1, output_dataframe2\n",
        "\n",
        "test_btn.click(ab_test, inputs=[model_name, model_name, num_clusters, num_clusters, regularization, regularization], outputs=[outputs[0], outputs[0]])\n",
        "\n",
        "# Implement model explainability\n",
        "explain_btn = gr.Button(label=\"Explain Model\")\n",
        "\n",
        "def explain_model(model_name):\n",
        "    model = models[model_name]\n",
        "    # Visualize the segmentation results\n",
        "    visualize(model.predict(X))\n",
        "    # Provide insights into the model's decision-making process\n",
        "    explain(model)\n",
        "\n",
        "\n",
        "def explain_model(model_name):\n",
        "    model = models[model_name]\n",
        "    # Preprocess the input data\n",
        "    X = preprocess(input_dataframe)\n",
        "    # Explain the model's predictions using LIME\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns, class_names=model.class_names, mode='classification')\n",
        "    for i in range(5): # Explain the predictions of the first 5 data points\n",
        "        exp = explainer.explain_instance(X.iloc[i], model.predict_proba)\n",
        "        exp.show_in_notebook()\n",
        "\n",
        "explain_btn.click(explain_model, inputs=[model_name])"
      ],
      "metadata": {
        "id": "O1qlKzqcY3ve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}