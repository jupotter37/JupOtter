{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this Kernel\nTI have tried to keep things as simple as possible in this kernel. I've also tried to write reusable codes as much as possible using custom functions so that we can avoid writing the same code again and again. *Most importantly, I didn't sacrifice interpretability at the cost of accuracy.* In this notebook, I extensively use *plotly* along with seaborn and matplotlib for data visualization so that you can use plotly by yourself easily. You may ask any question if you have, Let's get started:","metadata":{"_uuid":"febab8ead618d387a2c151983fd963c1a19ecef8"}},{"cell_type":"markdown","source":"# Outlines\n\n* [1.Problem Description and Objective](#1)\n* [2.Hypothesis Generation](#2)\n* [3.Importing Packages and Collecting Data](#3)\n* [4.Variable Description, Identification, and Correction](#4)\n* [6.Outliers treatment](#6)\n* [7.Imputing Missing Variables](#7)\n* [8.Transformation of Distributions](#8)\n* [9.Bivariate Analysis](#9)\n  * [9.1 Numerical-Numerical Variable](#9.1)\n  * [9.2 Categorical-Numerical Variable](#9.2)\n* [10.Feature Engineering](#10)\n  * [10.1 Creating New Features](#10.1)\n  * [10.2 Feature Scaling](#10.2)\n  * [10.3 Encoding Categorical Variables](#10.3)\n     * [10.3.1 Label Encoding](#10.3.1) [10.3.2 One Hot Encoding](#10.3.2)\n* [11.Model Building and Evaluation](#11)\n   * [11.1 Model Training](#11.1)\n   * [11.2 Model Evaluation](#11.2)\n      * [11.2.1 K-Fold Cross Validation](#11.2.1)\n      * [11.2.2 Optimizing Hyperparameters](#11.2.2)\n      * [11.2.3 Retrain and Predict Using Best Hyperparameters](#11.2.3)\n      * [11.2.4 Feature Importance](#11.2.4)\n      * [11.2.5 Learning Curves](#11.2.5)\n* [12.Introduction to Ensemble](#12)\n   * [12.1 Simple Ensemble Method](#12.1)\n   * [12.2 Advanced Ensemble Method](#12.2)\n* [13.End Note](#13)","metadata":{"_uuid":"1b13306ab36f17e3e4a65d15161141bb12abc260"}},{"cell_type":"markdown","source":"# 1.Problem Description and Objective <a id=\"1\"></a>\nKaggle describes this competition as:\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\nSo, the task is to predict house prices in Ames, Iowa using these 79 predictor variables using machine learning. So this is a regression problem. **For more, head over to [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)**","metadata":{"_uuid":"85be7b4a9c7cf441f720951d28736ec0c3a6f842"}},{"cell_type":"markdown","source":"# 2. Hypothesis Generation <a id=\"2\"></a>\nHypothesis generation, in general, is to create a set of features that should impact the target variable given a confidence interval (taken as 95% all the time). We should do this before looking at the data to avoid biased thoughts. This step often helps in creating new features. Defining a hypothesis has two parts: Null Hypothesis (H0) and Alternate Hypothesis(H1).\n* The H0 (Null Hypothesis): There is no relationship between our independent and dependent variables.\n* The H1 (Alternate Hypothesis): There is indeed a relationship between our independent and dependent variables.\n\nNow how do we can accept or reject any hypothesis? Usually, we look for probablity values(p value) between our predictor and target variables. That is if the p-value is significant (less than 0.05), we can reject the null hypothesis and claim that the findings support the alternate hypothesis. On the other hand, if p value is greater than 0.05, we fail to reject the null hypothesis and therefore conclude that there is statistically no relationship between our target and predictor variables. The features that I think should have an impact on the price of the house are:\n1. Area the house has been built on.\n2. Number of floors it has.\n3. Number of bedrooms it has.\n4. How old is the house.\n5. Materials used to build the house\n6. Location of the house. \n7. How close/far is the house off the market, and so on.","metadata":{"_uuid":"8e66ab6f9d6a0f7f29fe6c6aa984421a53c263cf"}},{"cell_type":"markdown","source":"# 3.Importing Packages and Collecting Data <a id=\"3\"></a>\nAfter importing required modules, read train and test data from csv files.","metadata":{"_uuid":"7bf80b6b68f966f0c410fa8280a70621bc8a8b8a"}},{"cell_type":"code","source":"'''Ignore deprecation and future, and user warnings.'''\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) ","metadata":{"_uuid":"0df5fb5da6d400501e5401331dc1adc1b20ee2e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Import basic modules.'''\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats","metadata":{"_uuid":"7bffc9cbc313bae9098243eb31858ec63651f571","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Customize visualization\nSeaborn and matplotlib visualization.'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('bmh')                    \nsns.set_style({'axes.grid':False}) \n\n'''Plotly visualization .'''\nimport plotly.offline as py\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected = True) # Required to use plotly offline in jupyter notebook\n\n'''Display markdown formatted output like bold, italic bold etc.'''\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So we're done with the required modules. Let's read in train and test data.**","metadata":{}},{"cell_type":"code","source":"'''Read in train and test data from csv files.'''\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","metadata":{"_uuid":"bef34f4ebaf6b0246c930d816f304f46beaabd6a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.Variable Description, Identification, and Correction <a id=\"4\"></a>\nDescribe what each of the variable indicates and identify our response and predictor variables. Then seperate the categorical variables from numerical variables (i.e., pandas object, float64 or int64 data types).","metadata":{"_uuid":"142da875044a7035755b9e17b9cea0f23ef4a3df"}},{"cell_type":"code","source":"'''Train and test data at a glance.'''\nbold('**Preview of Train Data:**')\ndisplay(train.head(3))\nbold('**Preview of Test Data:**')\ndisplay(test.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Dimensions of train and test data'''\nbold('**Shape of our train and test data:**')\ndisplay(train.shape, test.shape)","metadata":{"_uuid":"14ece907ebad313849159d77bd70f27ff8e103a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Have a look of our variable names'''\nbold('**Name of our variables (1st 20):**')\ndisplay(train.columns[:20].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So what can we see??\n**We can see total 81 variables for train and 80 variables for test data. And we don't have *SalePrice* variable for test set because this will be our task to infer *SalePrice* for test set by learning from train set. So *SalePrice* is our target variable and rest of the variables are our predictor variables.**\n#### Here comes the description of a few variables:\n* MSSubClass — The building class\n* MSZoning — The general zoning classification\n* LotFrontage — Linear feet of street connected to property\n* LotArea — Lot size in square feet\n* Street — Type of road access\n* Alley — Type of alley access\n* LotShape — General shape of property\n* LandContour — Flatness of the property\n* Utilities — Type of utilities available\n* LotConfig — Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n\n**For delailed variable description please check out [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)**\n\nInspecting variables one by one to separate categorical variables from numerical variables will take so much time. Hence we will extract nominal variables from numerical variables using pandas select_dtypes method and then visualize them using histogram (for continuous variables) or bar chart (for nominal and discrete variables) to make sure pandas select_dtypes method isn't misleading us.","metadata":{"_uuid":"946439784c371d2b5fd51432f4614b034ade6698"}},{"cell_type":"code","source":"\"\"\"Let's first inspect how many kinds of data types that we have to deal with.\"\"\"\nmerged = pd.concat([train, test], axis = 0, sort = True)\nbold('**Data types of our variables:**')\ndisplay(merged.dtypes.value_counts())","metadata":{"_uuid":"6cf5a898fc15199077c916d03eab395da492e566","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Extract numerical variables first.'''\nnum_merged = merged.select_dtypes(include = ['int64', 'float64'])\nbold('**Numerical variables:**')\ndisplay(num_merged.head(3))\nbold('**Name of numerical variables:**')\ndisplay(num_merged.columns.values)","metadata":{"_uuid":"90619bc82fdd5296b2e873a0408c458f3bbdb45f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Plot histogram of numerical variables to validate pandas intuition.'''\nfig, axes = plt.subplots(nrows = 19, ncols = 2, figsize = (40, 200))\nfor ax, column in zip(axes.flatten(), num_merged.columns):\n    sns.distplot(num_merged[column].dropna(), ax = ax, color = 'darkred')\n    ax.set_title(column, fontsize = 43)\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 35)\n    ax.tick_params(axis = 'both', which = 'minor', labelsize = 35)\n    ax.set_xlabel('')\nfig.tight_layout(rect = [0, 0.03, 1, 0.95])","metadata":{"_uuid":"e86089e656c4a3eeea598bacadf4cab9d77bb25a","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well we can clearly see some distributions are continuous (like LotFrontage, LotArea, and  YearBuilt) and some are discrete (like MSSubClass, OverallQual, OverallCond, BsmtFullBath, and HalfBath etc.). Let's correct the data types accordingly. Reading data description we see some variables are actually categorical (like like MSSubClass, OverallQual, and OverallCond). **Hence we would explicitly cast them into categorical variables. **For detailed data documentation see [here ](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt)","metadata":{"_uuid":"3000ce907fed88057adba549d700eb2fac16572c"}},{"cell_type":"code","source":"'''Convert MSSubClass, OverallQual, OverallCond, MoSold, YrSold into categorical variables.'''\nmerged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']] = merged.loc[:,['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']].astype('object')","metadata":{"_uuid":"1818a7b2d96a780ec7366d80bf195a0d19354310","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Check out the data types after correction'''\nbold('**Data types after correction:**')\ndisplay(merged.dtypes.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Extract train and test data from the combined data set.'''\ndf_train = merged.iloc[:1460, :].drop(columns = ['Id'], axis = 1)\ndf_test = merged.iloc[1460:, :].drop(columns = ['Id', 'SalePrice'], axis = 1) # SalePrice due to concatenation","metadata":{"_uuid":"8786c799ac175acf906382d79df3988b124ecd96","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now I would like to write 3 functions for different Plotly plots.**","metadata":{}},{"cell_type":"code","source":"'''Function to plot bar chart'''\ndef bar_plot(x, y, title, yaxis, c_scale):\n    trace = go.Bar(\n    x = x,\n    y = y,\n    marker = dict(color = y, colorscale = c_scale))\n    layout = go.Layout(hovermode= 'closest', title = title, yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)\n\n'''Function to plot scatter plot'''\ndef scatter_plot(x, y, title, xaxis, yaxis, size, c_scale):\n    trace = go.Scatter(\n    x = x,\n    y = y,\n    mode = 'markers',\n    marker = dict(color = y, size = size, showscale = True, colorscale = c_scale))\n    layout = go.Layout(hovermode= 'closest', title = title, xaxis = dict(title = xaxis), yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)    \n    \n'''Function to plot histogram'''\ndef plot_histogram(x, title, yaxis, color):\n    trace = go.Histogram(x = x,\n                        marker = dict(color = color))\n    layout = go.Layout(hovermode= 'closest', title = title, yaxis = dict(title = yaxis))\n    fig = go.Figure(data = [trace], layout = layout)\n    return iplot(fig)","metadata":{"_uuid":"83b15bdab875cb8aef5ad47bfa94dd1dec45a9d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.Outliers Treatment <a id=\"6\"></a>\nThere is a **Special Notes** in data documentation that says, **\"There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students\"**. ***Let's plot a scatter plot of GrLivArea vs SalePrice first to treat the outliers mentioned. The data documentation can be found [here](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt) ***","metadata":{"_uuid":"87bfa2c3387ad21f931a98bcaba99046e61a279e"}},{"cell_type":"code","source":"'''Sactter plot of GrLivArea vs SalePrice.'''\nscatter_plot(df_train.GrLivArea, df_train.SalePrice, 'GrLivArea vs SalePrice', 'GrLivArea', 'SalePrice', 10, 'Rainbow')","metadata":{"_uuid":"13098d0f7e5fc4ffaac63b456429bfc4c26fa589","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Drop observations where GrLivArea is greater than 4000 sq.ft'''\ndf_train.drop(df_train[df_train.GrLivArea>4000].index, inplace = True)\ndf_train.reset_index(drop = True, inplace = True)","metadata":{"_uuid":"ea388a3499ebfc9983d510a2cc9124576c452555","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Plot the sactter plot again of GrLivArea vs SalePrice to see if outliers are gone.'''\nscatter_plot(df_train.GrLivArea, df_train.SalePrice, 'GrLivArea vs SalePrice', 'GrLivArea', 'SalePrice', 10, 'Rainbow')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7.Imputing Missing Variables <a id=\"7\"></a>\nThe simpliest way to impute missing values of a variable is to impute its missing values with its mean, median or mode depending on its distribution and variable type(categorical or numerical). By now, we should have a idea about the distribution of the variables and the presence of outliers in those variables. For categorical variables mode-imputation is performed and for numerical variable mean-impuation is performed if its distribution is symmetric(or almost symmetric or normal like Age). On the other hand, for a variable with skewed distribution and outliers, meadian-imputation is recommended as median is more immune to outliers than mean.\n\nHowever, one clear disadvantage of using mean, median or mode to impute missing values is the addition of bias if the amount of missing values is significant. So simply replacing missing values with the mean or the median might not be the best solution since missing values may differ by groups and categories. To solve this, we can group our data by some variables that have no missing values and for each subset compute the median to impute the missing values of a variable. \n\n**For this purpose we would merge train and test data together. This will eliminate the hassle of handling train and test data separately thought it might induce some data leakage problem. But in practice, we would not have any test data during model training, so data leakage problem in real world could be avoided. Moreover, all the preprocessings done combinedly lead to better a leaderboard score (may be due to data leakage).**","metadata":{"_uuid":"a86ab02e57b90507d51d3c7d4245ce3ca1ba5b30"}},{"cell_type":"code","source":"'''Separate our target variable first.'''\ny_train = df_train.SalePrice\n\n'''Drop SalePrice from train data.'''\ndf_train.drop('SalePrice', axis = 1, inplace = True)\n\n'''Now combine train and test data frame together.'''\ndf_merged = pd.concat([df_train, df_test], axis = 0)","metadata":{"_uuid":"890393ca4238f23e5304dc8e43296ed58ba7dd01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Columns with missing observations.'''\nmissing_columns = df_merged.columns[df_merged.isnull().any()].values\n'''Number of columns with missing observations.'''\ntotal_columns = np.count_nonzero(df_merged.isna().sum())\nprint('We have ' , total_columns ,  'features with missing values and those features (with missing values) are: \\n\\n' , missing_columns)","metadata":{"_uuid":"05f4f3d752b3f7d4d0aae57f0842ff6d3b2c79f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Get and plot only the features (with missing values) and their corresponding missing values.'''\nmissing_columns = len(df_merged) - df_merged.loc[:, np.sum(df_merged.isnull())>0].count()\nx = missing_columns.index\ny = missing_columns\ntitle = 'Variables with Missing Values'\nscatter_plot(x, y, title, 'Features Having Missing Observations','Missing Values', 30, 'Picnic')","metadata":{"_uuid":"222e4e0868da915019064c385edccbd96b630bbe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Usually we drop a variable if at least 40% of its values are missing. Hence, one might tempt to drop variables like PoolQC, MiscFeature, Alley, Fence, and FirePlaceQu. Deleting these variables would be a blunder because data description tells these 'NaN' has some purpose for those variables. Like 'NaN' in PoolQC refers to  'No Pool', 'NaN' in MiscFeature refers to 'None', and 'NaN' in Alley means 'No alley access' etc. More generally NaN means the absent of that variable. Hence we gonna replace NaN with 'None' in those variable. Please do read data [documentation](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt) carefully ","metadata":{"_uuid":"23ccaf99b9084bd4d5fe9f9f6a3666582d079875"}},{"cell_type":"code","source":"'''Impute by None where NaN means something.'''\nto_impute_by_none = df_merged.loc[:, ['PoolQC','MiscFeature','Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageCond','GarageFinish','GarageQual','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType']]\nfor i in to_impute_by_none.columns:\n    df_merged[i].fillna('None', inplace = True)","metadata":{"_uuid":"7af8ad313e0e441bf6c6e39f2ebbbb63a383debf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''These are categorical variables and will be imputed by mode.'''\nto_impute_by_mode = df_merged.loc[:, ['Electrical', 'MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional', 'SaleType']]\nfor i in to_impute_by_mode.columns:\n    df_merged[i].fillna(df_merged[i].mode()[0], inplace = True)","metadata":{"_uuid":"e07dbee26ea54d73ff91dcfa13e89a00d7da5e8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''The following variables are either discrete numerical or continuous numerical variables.So the will be imputed by median.'''\nto_impute_by_median = df_merged.loc[:, ['BsmtFullBath','BsmtHalfBath', 'GarageCars', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']]\nfor i in to_impute_by_median.columns:\n    df_merged[i].fillna(df_merged[i].median(), inplace = True)","metadata":{"_uuid":"7d04fbcff3339a415044ed91a10296dc0afa5667","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Columns remaining to be imputed'''\ndisplay(df_merged.columns[df_merged.isna().any()].values)","metadata":{"_uuid":"d1c5c5e680b77954def6d3ae240279eff1bdd227","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Almost 17% observations of LotFrontage are missing in. Hence, simply imputing LotFrontage by mean or median might introduce bias since the amount of missing values is significant. Again LotFrontage may differ by different categories of house. To solve this, we can group our data by some variables that have no missing values and for each subset compute the median LotFrontage to impute the missing values of it. This method may result in better accuracy without high bias, unless a missing value is expected to have a very high variance.**","metadata":{"_uuid":"2fdf020068bd71d99aa5f41740401192412179ca"}},{"cell_type":"code","source":"\"\"\"Let's create a function to find the variables correlated with LotFrontage\"\"\"\ndef corr(correlation, variable):\n    from sklearn.preprocessing import LabelEncoder # Convert categorical variables into numerical\n    correlation = correlation.agg(LabelEncoder().fit_transform)\n    correlation['LotFrontage'] = variable\n    corr = correlation.corr()\n    display(corr['LotFrontage'].sort_values(ascending = False)[:5])\n    display(corr['LotFrontage'].sort_values(ascending = False)[-5:])","metadata":{"_uuid":"8a530f10d913a55f703ad5496eda7c798be16e49","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Correlation of LotFrontage with categorical data.'''\nbold(\"**LotFrontage's correlation with categorical data:**\")\ncorr(df_merged.select_dtypes(include = ['object']), df_merged.LotFrontage)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**BldgType has the highest correlation with LotFrontage. So the tactic is to impute missing values of LotFrontage with the median LotFrontage of similar rows according BldgType.**","metadata":{"_uuid":"58fda5b5b71ee0d26ee5860c3cd88b75c3d99fc2"}},{"cell_type":"code","source":"'''Impute LotFrontage with the median of highest correlated column(i.e., BldgType)'''\ndf_merged.LotFrontage = df_merged.groupby(['BldgType'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","metadata":{"_uuid":"06b396969400933c2b3db9e3b8786389a50afc18","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Is there any missing values left untreated??'''\nprint('Missing variables left untreated: ', df_merged.columns[df_merged.isna().any()].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Okay, we,re also done with imputation. Now let's head over to transformation distribution.**","metadata":{}},{"cell_type":"markdown","source":"# 8.Transformation of Distributions <a id=\"8\"></a>\nNormal distribution (bell-shaped) of variables is not only one of the assumptions of regression problems but also a assumption of parametric test (like one-way-anova, t-test etc) and pearson correlation. But in practice, this can not be met perfectly and hence some deviation off this assumption is acceptable. In this section, we would try to make the skewed distribution as normal as possible. Since most of the variables are positively skewed, we would apply log transformation on them. **Let's observe our target variable separately:**","metadata":{"_uuid":"b5e8a4dfa322eae045fdadf98e3bb2a5335de855"}},{"cell_type":"code","source":"'''Plot the distribution of SalePrice with skewness.'''\ntitle = 'SalePrice without Transformation (skewness: {:0.4f})'.format(y_train.skew())\nplot_histogram(y_train, title, 'Abs Frequency', 'darkred')","metadata":{"_uuid":"fbfce39e0aed0f907e94d182590bfed527a0d9d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Plot the distribution of SalePrice removing skewness.'''\ny_train = np.log1p(y_train)\ntitle = 'SalePrice after Transformation (skewness: {:0.4f})'.format(y_train.skew())\nplot_histogram(y_train, title, 'Abs Frequency', 'green')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Now calculate and plot the skewness for rest of the explanetory variables.'''\nskew_merged = pd.DataFrame(data = df_merged.select_dtypes(include = ['int64', 'float64']).skew(), columns = ['Skewness'])\nskew_merged_sorted = skew_merged.sort_values(ascending = False, by = 'Skewness')\nbar_plot(skew_merged_sorted.index, skew_merged_sorted.Skewness, 'Skewness in Explanetory Variables', 'Skewness', 'Bluered')","metadata":{"_uuid":"bb89b4300f1220df7f37778077bea2025cb0ccfb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see variables which are skewed are mostly right skewed (except YearBuilt, GarageYrBuilt, GarageCars etc which are slightly skewed) . We would apply log transformation (infact adding 1 before taking log) taking 0.75 as threshold value for transformation. That means we would transform variables that have skewness greater than 0.75.**","metadata":{"_uuid":"4c2b011d7d4a3bef97c44f2924afc09d22717bed"}},{"cell_type":"code","source":"'''Extract numeric variables merged data.'''\ndf_merged_num = df_merged.select_dtypes(include = ['int64', 'float64'])","metadata":{"_uuid":"e75621351396b041c9c441a3214f57a72c19c3f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Get the index of the data to be transformed'''\nbold('**Features to be transformed (skewness>0.75):**')\ndisplay(df_merged_num.skew()[df_merged_num.skew()>0.75].index.values) \n# So these are the variables to be transformed.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Make the transformation.'''\ndf_merged_skewed = np.log1p(df_merged_num[df_merged_num.skew()[df_merged_num.skew()>0.75].index])\ndf_merged_normal = df_merged_num[df_merged_num.skew()[df_merged_num.skew()< 0.75].index] # Normal variables\ndf_merged_num_all = pd.concat([df_merged_skewed, df_merged_normal], axis = 1)","metadata":{"_uuid":"7a07cdb53983ef6a3ba807441f1a2ef00cf3982f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Update numerical variables with transformed variables.'''\ndf_merged_num.update(df_merged_num_all)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9.Bivariate Analysis <a id=\"9\"></a>\nBeing the most important part, bivariate analysis tries to find the relationship between two variables. We will look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. For numeric-numeric combination, we would use scatter plot to analyse how strong the correlation is. And for numeric-categorical combinbation, we would use boxplot to see if there is any association between our target and explanatory variables. Since we're dealing with 79 predictor variables, it would not be a good idea to analyse all the variables one by one to find any correlation with SalePrice. We would anylyse only 20 variables that are highly correlated (either positive or negative) with SalePrice. As there are some categorical variables, we need to fit label encoder to have their correlation with target variable and based on this correlation value, we would only choose 20 variables that have the highest correlation (either positive or negative) for further anlysis.","metadata":{"_uuid":"3c602ff4c1ffd0bd5561b02121da4443af183b24"}},{"cell_type":"code","source":"'''Fit label encoder to find the correlated variables.'''\nfrom sklearn.preprocessing import LabelEncoder\ndf_corr = pd.concat([df_merged.iloc[0:1456, :], y_train], axis = 1)\ndf_corr = df_corr.agg(LabelEncoder().fit_transform)\ndf_corr = df_corr.corr()\nbold('**Best 10 Positively Correlated Variables:**')\ndisplay(df_corr['SalePrice'].sort_values(ascending = False)[:11]) # Best 10 positively correlated\nbold('**Best 10 Negatively Correlated Variables:**')\ndisplay(df_corr['SalePrice'].sort_values(ascending = False)[-10:]) # Best 10 negatively correlated","metadata":{"_uuid":"b7e6be3c1f9a2f3cedd2d44d4c29b66a148efb75","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''We can also create a scatter plot matrix of positively correlated variables (top 5) with SalePrice.'''\ncorr_positive = df_corr.loc[:, ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF']]\ng = sns.pairplot(corr_positive, kind = 'reg', diag_kind = 'kde', plot_kws = {'line_kws':{'color':'green', 'alpha': 0.7}}) \ng = g.map_diag(sns.distplot, color = 'darkred')\ng = g.map_offdiag(plt.scatter, alpha = 0.5, color = 'darkred')\ng.fig.suptitle('Most Positively Correlated Variables with Sale Price', fontsize = 20)\nplt.subplots_adjust(top = 0.95)","metadata":{"_uuid":"3192e8346a3759b39aca7c6e894b371c549ce9b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" In the same way, let's also create a scatter plot matrix of negatively correlated variables (top 5) with SalePrice.\"\"\"\ncorr_neg = df_corr.loc[:, ['SalePrice', 'ExterQual', 'BsmtQual', 'KitchenQual', 'GarageType', 'GarageFinish']]\ng = sns.pairplot(corr_neg, kind = 'reg', diag_kind = 'kde', plot_kws = {'line_kws':{'color':'red'}})\ng = g.map_diag(sns.distplot, color = 'darkred')\ng = g.map_offdiag(plt.scatter, alpha = 0.5, color = 'darkred')\ng.fig.suptitle('Most Negatively Correlated Variables with Sale Price', fontsize = 20)\nplt.subplots_adjust(top = 0.95)","metadata":{"_uuid":"5a3b0996485e02f31b55b4f9b0b030fee23500e4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.1 Numerical and Numerical Variable <a id=\"9.1\"></a>\nAmong these 20 variables, GrLivArea, YearBuilt, GarageArea, TotalBsmtSF, 1stFlrSF, YearRemodAdd, GarageYrBlt are continuous numerical variables. Hence we would investigate how these numerical continuous variables are correlated with our dependent variable SalePrice with the help of regression plot.","metadata":{"_uuid":"24d41dd756d457a7a75f592f7e76d64e13f25d8a"}},{"cell_type":"code","source":"'''Plot regression plot to see how SalePrice is correlated with numerical variables.'''\ncorr_num = df_merged.loc[:, ['GrLivArea', 'YearBuilt', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'YearRemodAdd', 'GarageYrBlt']]\ncorr_num = corr_num.iloc[0:1456, :]\nfor i in corr_num.columns:\n    x = corr_num[i]\n    y = y_train\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n    line = slope*x + intercept\n\n    # Creating the dataset, and generating the plot\n    trace0 = go.Scatter(\n                  x = x,\n                  y = y,\n                  mode = 'markers',\n                  marker = dict(color = 'red'),\n                  name ='Data'\n                  )\n    \n    # Creating regression line\n    trace1 = go.Scatter(\n                  x = x,\n                  y = line,\n                  mode='lines',\n                  marker = dict(color = 'green'),\n                  name='Fit'\n                  )\n\n    # Layout for regression plot\n    title = '{} vs SalePrice (r: {:0.4f}, p: {})'.format(corr_num[i].name, r_value, p_value)\n    layout = go.Layout(\n            title = title, yaxis = dict(title = 'SalePrice'))\n\n    data = [trace0, trace1]\n    fig = go.Figure(data = data, layout = layout)\n    iplot(fig)","metadata":{"_uuid":"4e91bce07769c363ff642c853a0206b61fc307bf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.2 Categorical  andNumerical Variable <a id=\"9.2\"></a>\nA boxplot between our numerical and categorical variables helps to reveal if the distribution of numerical variable is distinct in different classes of nominal variables. More specifically, how SalePrice (strongly or weakly) is associated with these categorical variables. Among the 20 variables, OverallQual, GarageCars, FullBath, ExterQual, BsmtQual, KitchenQual, GarageType, GarageFinish, HeatingQC, BsmtExposure, LotShape, MSZoning, KitchenAbvGr are categorical variables that are highly correlated (either positively or negatively) with SalePrice.","metadata":{"_uuid":"0c994bc8aee0ea260944562866dde5641e1c16e2"}},{"cell_type":"code","source":"'''Create boxplots to see the association between categorical and target variables.'''\ncorr_cat = df_merged.loc[:, ['OverallQual', 'GarageCars', 'FullBath', 'ExterQual', 'BsmtQual', 'KitchenQual', 'GarageType', 'GarageFinish', 'HeatingQC', 'BsmtExposure', 'LotShape', 'MSZoning', 'KitchenAbvGr']]\ncorr_cat = corr_cat.iloc[0:1456, :]\nfor i in corr_cat.columns:\n    trace = go.Box(x = corr_cat[i], y = y_train, marker = dict (color = 'magenta'))\n    data = [trace]\n    layout = go.Layout(title = '{} vs SalePrice'.format(i), yaxis = dict(title = 'SalePrice'))\n    fig = go.Figure(data = data, layout = layout)\n    iplot(fig)","metadata":{"_uuid":"7b962c84a1d8d246b62edb41dd8eb2b75dfd22ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since thse variables are highly associated with SalePrice, mean SalePrice should be different across the classes (groups) of these categorical variables. We can visualize this pattern using pivot table. ","metadata":{"_uuid":"933b12d683b4409691fd2a7a4b56b3da3cfb5be1"}},{"cell_type":"code","source":"'''Create pivot table to see if mean SalePrice varries across groups of categorical variables.'''\npivot_df = pd.concat([corr_cat, y_train], axis = 1)\nfor i in corr_cat.columns:\n    pivot_table = pivot_df.pivot_table(index = i, values = 'SalePrice', aggfunc = np.mean)\n    bar_plot(pivot_table.index, pivot_table.SalePrice, '{} vs Mean SalePrice'.format(i), 'Mean SalePrice', 'Rainbow')","metadata":{"_uuid":"5c91081a469141cfd5a8f476dbdeb0b4c2d467e9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mean SalePrice is distinct across the groups of our different categorical variables and hence they have high correlation with SalePrice.**","metadata":{"_uuid":"41e12a0ffc64314adf2832f87769af96260ca836"}},{"cell_type":"markdown","source":"# 10.Feature Engineering <a id=\"10\"></a>","metadata":{"_uuid":"bbf7c00228d7642c594a6c957f0df11f3690a224"}},{"cell_type":"markdown","source":"## 10.1 Creating New Features <a id=\"10.1\"></a>\nWe would create a new feature named TotalSF combining TotalBsmtSF, 1stFlrSF, and 2ndFlrSF.","metadata":{"_uuid":"56fdeff28a556b9f55bc755b43d883c380aa758a"}},{"cell_type":"code","source":"'''Create Feature TotalSF'''\ndf_merged_num['TotalSF'] = df_merged_num['TotalBsmtSF'] + df_merged_num['1stFlrSF'] + df_merged_num['2ndFlrSF']","metadata":{"_uuid":"402df88921f1c604f936d8652eac27cb0ce595fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.2 Feature Scaling <a id=\"10.2\"></a>\nTwo methods are usually well known for rescaling data, i.e., normalization and standarization. Normalization scales all numeric variables in the range [0,1]. So outliers might be lost. On the other hand, standarization transforms data to have zero mean and unit variance. Feature scaling helps gradient descent converge faster, thus reducing training time. Its not necessary to standarize the target variable. However, due to the presence of outliers, we would use sklearn's RobustScaler since it is not affected by outliers. **For more see the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)**","metadata":{"_uuid":"c9891bb271ee417b5ec8774ec9f10834c9b5d16b"}},{"cell_type":"code","source":"'''Standarize numeric features with RobustScaler'''\nfrom sklearn.preprocessing import RobustScaler\n\n'''Initialize robust scaler object.'''\nrobust_scl = RobustScaler()\n\n'''Fit scaler object on train data.'''\nrobust_scl.fit(df_merged_num)\n\n'''Apply scaler object to both train and test data.'''\ndf_merged_num_scaled = robust_scl.transform(df_merged_num)","metadata":{"_uuid":"c30679cc0f5c4fda459f179d15aeb64aca093723","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Retrive column names'''\ndf_merged_num_scaled = pd.DataFrame(data = df_merged_num_scaled, columns = df_merged_num.columns, index = df_merged_num.index)\n# Pass the index of index df_merged_num, otherwise it will sum up the index.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.3 Encoding Categorical Variables  <a id=\"10.3\"></a>\nWe have to encode categorical variables for our machine learning algorithms to interpret them. We would use label encoding and then one hot encoding.\n\n### 10.3.1 Label Encoding <a id=\"10.3.1\"></a>\n**We would like to encode some categorical (ordinal) variables to preserve their ordinality. If we use sklearn's label encoder, it will randomly encode these ordinal variables and therefore ordinality would be lost. To overcome this, we will use pandas replace method to manually encode orninal variables. Variables like LotShape, LandContour, Utilities, LandSlope, OverallQual (already encoded), OverallCond (already encoded), ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, HeatingQC, BsmtFinType2, Electrical, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence have inherent orders. Let's encode them. Don't get bored if you fell exhausted in the process.**","metadata":{"_uuid":"910b22219b12495b13cb393fe238a147446a7376"}},{"cell_type":"code","source":"\"\"\"Let's extract categorical variables first and convert them into category.\"\"\"\ndf_merged_cat = df_merged.select_dtypes(include = ['object']).astype('category')\n\n\"\"\"let's begin the tedious process of label encoding.\"\"\"\ndf_merged_cat.LotShape.replace(to_replace = ['IR3', 'IR2', 'IR1', 'Reg'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.LandContour.replace(to_replace = ['Low', 'Bnk', 'HLS', 'Lvl'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Utilities.replace(to_replace = ['NoSeWa', 'AllPub'], value = [0, 1], inplace = True)\ndf_merged_cat.LandSlope.replace(to_replace = ['Sev', 'Mod', 'Gtl'], value = [0, 1, 2], inplace = True)\ndf_merged_cat.ExterQual.replace(to_replace = ['Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.ExterCond.replace(to_replace = ['Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtQual.replace(to_replace = ['None', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtCond.replace(to_replace = ['None', 'Po', 'Fa', 'TA', 'Gd'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtExposure.replace(to_replace = ['None', 'No', 'Mn', 'Av', 'Gd'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.BsmtFinType1.replace(to_replace = ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.BsmtFinType2.replace(to_replace = ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.HeatingQC.replace(to_replace = ['Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.Electrical.replace(to_replace = ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'], value = [0, 1, 2, 3, 4], inplace = True)\ndf_merged_cat.KitchenQual.replace(to_replace = ['Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Functional.replace(to_replace = ['Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'], value = [0, 1, 2, 3, 4, 5, 6], inplace = True)\ndf_merged_cat.FireplaceQu.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.GarageFinish.replace(to_replace =  ['None', 'Unf', 'RFn', 'Fin'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.GarageQual.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.GarageCond.replace(to_replace =  ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], value = [0, 1, 2, 3, 4, 5], inplace = True)\ndf_merged_cat.PavedDrive.replace(to_replace =  ['N', 'P', 'Y'], value = [0, 1, 2], inplace = True)\ndf_merged_cat.PoolQC.replace(to_replace =  ['None', 'Fa', 'Gd', 'Ex'], value = [0, 1, 2, 3], inplace = True)\ndf_merged_cat.Fence.replace(to_replace =  ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'], value = [0, 1, 2, 3, 4], inplace = True)","metadata":{"_uuid":"f2022ecc94afd3ec678280a2db75439aea5adbc8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''All the encodeded variables have int64 dtype except OverallQual and OverallCond. So convert them back into int64.'''\ndf_merged_cat.loc[:, ['OverallQual', 'OverallCond']] = df_merged_cat.loc[:, ['OverallQual', 'OverallCond']].astype('int64')\n\n'''Extract label encoded variables'''\ndf_merged_label_encoded = df_merged_cat.select_dtypes(include = ['int64'])","metadata":{"_uuid":"bd27e195323ebc6df6cd57a4fdb1200e1f9c3a50","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10.3.2 One Hot Encoding <a id=\"10.3.2\"></a>\nCategorical variables without any inherent order will be converted into numerical for our model using pandas get_dummies method. So basically variables in df_merged_cat data frame with category dtypes are nominal variables for one hot encoding.","metadata":{"_uuid":"20c4379642173f75562cc2bdb66653d2440a3910"}},{"cell_type":"code","source":"'''Now extract the nominal variables for one hot encoding.'''\ndf_merged_one_hot = df_merged_cat.select_dtypes(include = ['category'])\ndf_merged_one_hot = pd.get_dummies(df_merged_one_hot)","metadata":{"_uuid":"69dd49f0d2c431a53a6d020ed3fe2fe7041c13ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Let's concate one hot encoded and label encoded variables together.\"\"\"\ndf_merged_encoded = pd.concat([df_merged_one_hot, df_merged_label_encoded], axis = 1)\n\n'''Finally join processed categorical and numerical variables.'''\ndf_merged_processed = pd.concat([df_merged_num_scaled, df_merged_encoded], axis = 1)","metadata":{"_uuid":"140eb39c95a04d79e19e042431f39534aa763f08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Now retrive train and test data for modelling.'''\ndf_train_final = df_merged_processed.iloc[0:1456, :]\ndf_test_final = df_merged_processed.iloc[1456:, :]\n\n'''And we have our target variable as y_train.'''\ny_train = y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"Let's look at our final train and test data for modelling.\"\"\"\nbold('**Updated train data for modelling:**')\ndisplay(df_train_final.head(3))\nbold('**Updated test data for modelling:**')\ndisplay(df_test_final.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11.Model Building & Evaluation <a id=\"11\"></a>\nWith all the preprocessings done and dusted, we're ready to train our regression models with the processed data.","metadata":{"_uuid":"666210083694d2716854b6c3272e048fa7727b01"}},{"cell_type":"code","source":"\"\"\"Let's have a final look at our data\"\"\"\nbold('**Data Dimension for Model Building:**')\nprint('Input matrix dimension:', df_train_final.shape)\nprint('Output vector dimension:',y_train.shape)\nprint('Test data dimension:', df_test_final.shape)","metadata":{"_uuid":"b9d65e502d467225cf94917d26b92a53c97b2e9c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11.1 Model Training <a id=\"11.1\"></a>","metadata":{"_uuid":"bd76a93e7580e538bf10d5679b818773523c3e75"}},{"cell_type":"code","source":"'''Set a seed for reproducibility'''\nseed = 43\n\n'''Initialize all the regression models object we are interested in.'''\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n\n'''We are interested in the following 14 regression models.\nAll initialized with default parameters except random_state and n_jobs.'''\nlinear = LinearRegression(n_jobs = -1)\nlasso = Lasso(random_state = seed)\nridge = Ridge(random_state = seed)\nkr = KernelRidge()\nelnt = ElasticNet(random_state = seed)\ndt = DecisionTreeRegressor(random_state = seed)\nsvm = SVR()\nknn = KNeighborsRegressor(n_jobs = -1)\nrf =  RandomForestRegressor(n_jobs = -1, random_state = seed)\net = ExtraTreesRegressor(n_jobs = -1, random_state = seed)\nab = AdaBoostRegressor(random_state = seed)\ngb = GradientBoostingRegressor(random_state = seed)\nxgb = XGBRegressor(random_state = seed, n_jobs = -1)\nlgb = LGBMRegressor(random_state = seed, n_jobs = -1)","metadata":{"_uuid":"077b6fdb3313fca0376283d5f7c308c134b470a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Training accuracy of our regression models. By default score method returns coefficient of determination (r_squared).'''\ndef train_r2(model):\n    model.fit(df_train_final, y_train)\n    return model.score(df_train_final, y_train)\n\n'''Calculate and plot the training accuracy.'''\nmodels = [linear, lasso, ridge, kr, elnt, dt, svm, knn, rf, et, ab, gb, xgb, lgb]\ntraining_score = []\nfor model in models:\n    training_score.append(train_r2(model))\n    \n'''Plot dataframe of training accuracy.'''\ntrain_score = pd.DataFrame(data = training_score, columns = ['Training_R2'])\ntrain_score.index = ['LR', 'LSO', 'RIDGE', 'KR', 'ELNT', 'DT', 'SVM', 'KNN', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\ntrain_score = (train_score*100).round(4)\nscatter_plot(train_score.index, train_score['Training_R2'], 'Training Score (R_Squared)', 'Models','% Training Score', 30, 'Rainbow')","metadata":{"_uuid":"4145d2701243783a5d8fce5f999aa3a5b1334c7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Being a regression problem, score method returns r_squared(coefficients of determination) and hence bigger is better. Looks like DT and ET have exactly r2_score of 100%. Usually higher r2_score is better but r2_score very close to 1 might indicate overfitting. But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not. Because training data is something our model has been trained with, i.e., data our model has already seen it. We all know that, the purpose of building a machine learning model is to generalize the unseen data, i.e., data our model has not yet seen. Hence we can't use training accuracy for our model evaluation rather we must know how our model will perform on the data our model is yet to see.**","metadata":{"_uuid":"14ed43321b72b5e21b9eb7eb5633b9715e04870d"}},{"cell_type":"markdown","source":"## 11.2 Model Evaluation <a id=\"11.2\"></a>\nSo basically, to evaluate a model's performance, we need some data (input) for which we know the ground truth(label). For this problem, we don't know the ground truth for the test set but we do know for the train set. So the idea is to train and evaluate the model performance on different data. One thing we can do is to split the train set in two groups, usually in 80:20 ratio. That means we would train our model on 80% of the training data and we reserve the rest 20% for evaluating the model since we know the ground truth for this 20% data. Then we can compare our model prediction with this ground truth (for 20% data). That's how we can tell how our model would perform on unseen data. This is the first model evaluation technique. In sklearn we have a train_test_split method for that. Let's evaluate our model using train_test_split method. **Note: From now on, we will be using root mean squared error as the evaluation metric for this problem. So smaller is better.**","metadata":{"_uuid":"392445b0177f4149b6436032f819b7a5d79ad090"}},{"cell_type":"code","source":"'''Evaluate models on the holdout set(say on 30%).'''\ndef train_test_split_score(model):\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, Y_train, Y_test = train_test_split(df_train_final, y_train, test_size = 0.3, random_state = seed)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n    mse = mean_squared_error(prediction, Y_test)\n    rmse = np.sqrt(mse)\n    return rmse\n\n'''Calculate train_test_split score of differnt models and plot them.'''\nmodels = [lasso, ridge, kr, elnt, dt, svm, knn, rf, et, ab, gb, xgb, lgb]\ntrain_test_split_rmse = []\nfor model in models:\n    train_test_split_rmse.append(train_test_split_score(model))\n\n'''Plot data frame of train test rmse'''\ntrain_test_score = pd.DataFrame(data = train_test_split_rmse, columns = ['Train_Test_RMSE'])\ntrain_test_score.index = ['LSO', 'RIDGE', 'KR', 'ELNT', 'DT', 'SVM', 'KNN', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\ntrain_test_score = train_test_score.round(5)\nx = train_test_score.index\ny = train_test_score['Train_Test_RMSE']\ntitle = \"Models' Test Score (RMSE) on Holdout(30%) Set\"\nscatter_plot(x, y, title, 'Models','RMSE', 30, 'RdBu')","metadata":{"_uuid":"abe53fd9df9e4db4138feab9e0f29559cbaf2934","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Being root mean squared error, smaller is better. Looks like, SVM is the best regression model followed by Ridge, GB and XGB. Unfortunately, LR can't find any linear pattern, hence it performs worst and hence discarded.** \n\nHowever, train_test split has its drawbacks. Because this approach introduces bias as we are not using all of our observations for testing and also we're reducing the train data size. To overcome this we can use a technique called cross validation where all the data is used for training and testing periodically. Thus we may reduce the bias introduced by train_test_split. From different cross validation methods, we would use k-fold cross validation. In sklearn we have a method cross_val_score for calculating k-fold cross validation score.\n\nHowever, as the train set gets larger, train_test_split has its advantage over k-fold cross validation. Train_test_split is k-times faster than k-fold cross validation. If the training set is very large, both train_test_split and k-fold cross validation perform identically. So for a large training data, train_test_split is prefered over k-fold cross validation to accelerate the training process.\n\n### 11.2.1 K-Fold Cross Validation <a id=\"11.2.1\"></a>\nLet's say we will use 10-fold cross validation. So k = 10 and we have total 1456 observations. Each fold would have 1456/10 = 145.6 observations. So basically k-fold cross validation uses fold-1 (145.6 samples) as the testing set and k-1 (9 folds) as the training sets and calculates test accuracy.This procedure is repeated k times (if k = 10, then 10 times); each time, a different group of observations is treated as a validation or test set. This process results in k estimates of the test accuracy which are then averaged out.","metadata":{"_uuid":"5a63a05bf602ac76dd44a4a0e11584009c4c89ce"}},{"cell_type":"code","source":"'''Function to compute cross validation scores.'''\ndef cross_validate(model):\n    from sklearn.model_selection import cross_val_score\n    neg_x_val_score = cross_val_score(model, df_train_final, y_train, cv = 10, n_jobs = -1, scoring = 'neg_mean_squared_error')\n    x_val_score = np.round(np.sqrt(-1*neg_x_val_score), 5)\n    return x_val_score.mean()\n\n'''Calculate cross validation score of differnt models and plot them.'''\nmodels = [lasso, ridge, kr, elnt, dt, svm, knn, rf, et, ab, gb, xgb, lgb]\ncross_val_scores = []\nfor model in models:\n    cross_val_scores.append(cross_validate(model))\n\n'''Plot data frame of cross validation scores.'''\nx_val_score = pd.DataFrame(data = cross_val_scores, columns = ['Cross Validation Scores (RMSE)'])\nx_val_score.index = ['LSO', 'RIDGE', 'KR', 'ELNT', 'DT', 'SVM', 'KNN', 'RF', 'ET', 'AB', 'GB', 'XGB', 'LGB']\nx_val_score = x_val_score.round(5)\nx = x_val_score.index\ny = x_val_score['Cross Validation Scores (RMSE)']\ntitle = \"Models' 10-fold Cross Validation Scores (RMSE)\"\nscatter_plot(x, y, title, 'Models','RMSE', 30, 'Viridis')","metadata":{"_uuid":"a6031106257b50623102ea376d86c95e11ae602a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looks like Ridge has managed to beat SVM as the best regression model on 10-fold cross validation. And rmse of GB, XGB, and LGB have also dropped from previous holdout set's rmse.**","metadata":{"_uuid":"80e52bda2b7b6551aa772a1785d455bb3d5ad68c"}},{"cell_type":"markdown","source":"### 11.2.2 Optimizing Hyperparameters <a id=\"11.2.2\"></a>\nNow let's add *Grid Search* to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out.\n\nNote: Though optimizing hyperparameters is time consuming, hyperparameters should be tuned for all the models you try because only then you will be able to tell what is the best you can get out of that particular model.","metadata":{"_uuid":"42ca24e8acdfeada9df1bbb40483d157e1651822"}},{"cell_type":"code","source":"def grid_search_cv(model, params):\n    global best_params, best_score\n    from sklearn.model_selection import GridSearchCV\n    grid_search = GridSearchCV(estimator = model, param_grid = params, cv = 10, verbose = 1,\n                            scoring = 'neg_mean_squared_error', n_jobs = -1)\n    grid_search.fit(df_train_final, y_train)\n    best_params = grid_search.best_params_ \n    best_score = np.sqrt(-1*(np.round(grid_search.best_score_, 5)))\n    return best_params, best_score","metadata":{"_uuid":"08f16813fc98875091098f311e3da84f761f376b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.1 Optimize Lasso","metadata":{"_uuid":"5c1de473e738c32b85ae1403058e896ab5dcffee"}},{"cell_type":"code","source":"''''Define hyperparameters of lasso.'''\nalpha = [0.0001, 0.0002, 0.00025, 0.0003, 0.00031, 0.00032, 0.00033, 0.00034, 0.00035, 0.00036, 0.00037, 0.00038, \n         0.0004, 0.00045, 0.0005, 0.00055, 0.0006, 0.0008,  0.001, 0.002, 0.005, 0.007, 0.008, 0.01]\n\nlasso_params = {'alpha': alpha,\n               'random_state':[seed]}\n\ngrid_search_cv(lasso, lasso_params)\nlasso_best_params, lasso_best_score = best_params, best_score\nprint('Lasso best params:{} & best_score:{:0.5f}' .format(lasso_best_params, lasso_best_score))","metadata":{"_uuid":"b4f5fd4b82aa03a0f634550e128f5c87558644a7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.2 Optimize Ridge","metadata":{"_uuid":"b50f49ed98e5c0b2e3256ca8a244dbd137cace7d"}},{"cell_type":"code","source":"''''Define hyperparameters of ridge.'''\nridge_params = {'alpha':[ 9, 9.2, 9.4, 9.5, 9.52, 9.54, 9.56, 9.58, 9.6, 9.62, 9.64, 9.66, 9.68, 9.7,  9.8],\n               'random_state':[seed]}\n\ngrid_search_cv(ridge, ridge_params)\nridge_best_params, ridge_best_score = best_params, best_score\nprint('Ridge best params:{} & best_score:{:0.5f}' .format(ridge_best_params, ridge_best_score))","metadata":{"_uuid":"3d5f4e6277a340fb57b9f8baa56fb5c8bcf9d782"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.3 Optimize Kernel Ridge","metadata":{"_uuid":"3aa897a96d4761d00c2a7bf90597d32c688c4629"}},{"cell_type":"code","source":"'''Define hyperparameters of kernel ridge'''\nkernel_params = {'alpha':[0.27, 0.28, 0.29, 0.3],\n                'kernel':['polynomial', 'linear'], \n                'degree':[2, 3],\n                'coef0':[3.5, 4, 4.2]}\ngrid_search_cv(kr, kernel_params)\nkernel_best_params, kernel_best_score = best_params, best_score\nprint('Kernel Ridge best params:{} & best_score:{:0.5f}' .format(kernel_best_params, kernel_best_score))","metadata":{"_uuid":"cc8c58144297fcee3592126e0b221f1cfa4d7d90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.4 Optimize Elastic Net","metadata":{"_uuid":"e1c2702ed0e117907bf648b5da1509dc6e570824"}},{"cell_type":"code","source":"'''Define hyperparameters of Elastic net.'''\nelastic_params = {'alpha': [ 0.0003, 0.00035, 0.00045, 0.0005], \n                 'l1_ratio': [0.80, 0.85, 0.9, 0.95],\n                 'random_state':[seed]}\ngrid_search_cv(elnt, elastic_params)\nelastic_best_params, elastic_best_score = best_params, best_score\nprint('Elastic Net best params:{} & best_score:{:0.5f}' .format(elastic_best_params, elastic_best_score))","metadata":{"_uuid":"cec13eea1eacbc038a886dfa9c3adeaf0f98fd4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.5 Optimize Support Vector Machine","metadata":{"_uuid":"5dba49ccc5411deaaf573b543b72e2cc9f479db6"}},{"cell_type":"code","source":"'''Define hyperparameters of support vector machine'''\nsvm_params = {\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # precomputed is omitted from kernel to avoid error.\n    'C': [4, 5], \n    'gamma':[0.0001, 0.001]}\n\ngrid_search_cv(svm, svm_params)\nsvm_best_params, svm_best_score = best_params, best_score\nprint('SVM best params:{} & best_score:{:0.5f}' .format(svm_best_params, svm_best_score))\n# Don't bother it takes some time. Training is usually more slower in svm.","metadata":{"_uuid":"b5004d8acdb4c0cf8026588bc85ed7a958b2bb7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2.2.6 Optimize XGB, GB, and LGB\n**For computational restrictions and time limit, I won't optimize xgb, gb and lgb models. I also reckon some models like DT, KNN won't do any better after optimization since they have poor cross validation scores. The following hyperparameters of xgb, gb and lgb are found after a bit of experiments. If you have resources and time, I would encourage you to try to optimize these models yourself and see how they perform.**","metadata":{"_uuid":"8bae5e6da6e08a76b33811d082a665bd8523ca77"}},{"cell_type":"code","source":"'''Hyperparameters of xgb'''\nxgb_opt = XGBRegressor(colsample_bytree = 0.4603, gamma = 0.0468, \n                             learning_rate = 0.04, max_depth = 3, \n                             min_child_weight = 1.7817, n_estimators = 2500,\n                             reg_alpha = 0.4640, reg_lambda = 0.8571,\n                             subsample = 0.5213, silent = 1,\n                             nthread = -1, random_state = 7)\n\n'''Hyperparameters of gb'''\ngb_opt = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05,\n                                   max_depth = 4, max_features = 'sqrt',\n                                   min_samples_leaf = 15, min_samples_split = 10, \n                                   loss = 'huber', random_state = seed)\n'''Hyperparameters of lgb'''\nlgb_opt = LGBMRegressor(objective = 'regression', num_leaves = 5,\n                              learning_rate=0.05, n_estimators = 660,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed = 9, bagging_seed = 9,\n                              min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\n\n'''We can assume these 3 model best score is equal to cross validation scores.\nThought it might not be precise, but I will take it'''\nxgb_best_score = cross_validate(xgb_opt)\ngb_best_score = cross_validate(gb_opt)\nlgb_best_score = cross_validate(lgb_opt)","metadata":{"_uuid":"b4b797151b27dc3b4fa18fa7064b53a8a548bda1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Let's plot the models' rmse after optimization.\"\"\"\noptimized_scores = pd.DataFrame({'Optimized Scores':np.round([lasso_best_score, ridge_best_score, kernel_best_score, \n                  elastic_best_score, svm_best_score, xgb_best_score, gb_best_score, lgb_best_score], 5)})\noptimized_scores.index = ['Lasso', 'Ridge', 'Kernel_ridge', 'E_net', 'SVM', 'XGB', 'GB', 'LGB']\noptimized_scores.sort_values(by = 'Optimized Scores')\nscatter_plot(optimized_scores.index, optimized_scores['Optimized Scores'], \"Models' Scores after Optimization\", 'Models','Optimized Scores', 40, 'Rainbow')","metadata":{"_uuid":"b1f30cade30fdae0ad927e7533478cbe53009de4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looks like kernel ridge is the best model after optimization. Lasso and elastic net score exactly identical. LGB and GB scores the worst among five models after optimization.**","metadata":{"_uuid":"4c0c015757233c4b80fe5cbc64a442f060571d15"}},{"cell_type":"markdown","source":"### 11.2.3 Retrain and Predict Using Best Hyperparameters <a id=\"11.2.3\"></a>\nNow we would like to retrain our  models using the best parameters responsible for best rmse after optimization. Then we would predict on test data to see how different models perform on leaderboard.","metadata":{"_uuid":"cc19f191d54c6c89da4653eb3998a21906798b33"}},{"cell_type":"code","source":"'''Initialize 8 object models with best hyperparameters'''\nlasso_opt = Lasso(**lasso_best_params)\nridge_opt = Ridge(**ridge_best_params)\nkernel_ridge_opt = KernelRidge(**kernel_best_params)\nelastic_net_opt = ElasticNet(**elastic_best_params)\nsvm_opt = SVR(**svm_best_params)\nxgb_opt = xgb_opt\ngb_opt = gb_opt\nlgb_opt = lgb_opt","metadata":{"_uuid":"935e1a30defe84b171c88dd28ad9ba5604a4ae06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Now train and predict with optimized models'''\ndef predict_with_optimized_models(model):\n    model.fit(df_train_final, y_train)\n    y_pred = np.expm1(model.predict(df_test_final))\n    submission = pd.DataFrame()\n    submission['Id']= test.Id\n    submission['SalePrice'] = y_pred\n    return submission\n\n'''Make submission with optimized lasso, ridge, kernel_ridge, elastic_net and svm, xgb, gb, and lgb.'''\npredict_with_optimized_models(lasso_opt).to_csv('lasso_optimized.csv', index = False)\npredict_with_optimized_models(ridge_opt).to_csv('ridge_optimized.csv', index = False)\npredict_with_optimized_models(kernel_ridge_opt).to_csv('kernel_ridge_optimized.csv', index = False)\npredict_with_optimized_models(elastic_net_opt).to_csv('elastic_net_optimized.csv', index = False)\npredict_with_optimized_models(svm_opt).to_csv('svm_opt_optimized.csv', index = False)\npredict_with_optimized_models(xgb_opt).to_csv('xgb_optimized.csv', index = False)\npredict_with_optimized_models(gb_opt).to_csv('gb_optimized.csv', index = False)\npredict_with_optimized_models(lgb_opt).to_csv('lgb_optimized.csv', index = False)","metadata":{"_uuid":"a483628066c0dd51b704b577856d4788b35eb765"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Okay then 8 csv files are created to make submission. Let's see how much they can score on kaggle leaderboard.**","metadata":{"_uuid":"22ea4fbbdc8b7206e57d05a0df4cf592e9dd4bef"}},{"cell_type":"code","source":"'''Plot the leaderboard results.'''\nscores_on_submission = pd.DataFrame({'Leaderboard_score':[0.12193, 0.12005, 0.11786, 0.12190, 0.12057, 0.12764, 0.12234, 0.12188]})\nscores_on_submission.index = ['Opt_lasso', 'Opt_ridge', 'Opt_kernel_ridge', 'Opt_elastic_net', 'Opt_svm', 'Opt_xgb', 'Opt_gb', 'Opt_lgb']\nscores_on_submission.sort_values(by = 'Leaderboard_score')\nscatter_plot(scores_on_submission.index, scores_on_submission['Leaderboard_score'], 'Leaderboard Scores after Optimization','Models', 'Optimized Scores', 40, 'Greens')","metadata":{"_uuid":"499a13ad4bdab0548f7fa482344f06fc17acf904"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we go! Its kernel ridge that scores best (0.11786) on leaderboard after optimization, followed by ridge and svm. The xgb scores worst among the 8 models.**\n\n### 11.2.4 Feature Importance <a id=\"11.2.4\"></a>\nDo all these 8 models give equal importance to every features? Of course not! We would try to visualize feature importance given by every model except svm. Since svm is not using linear kernel, its feature importance can not be derived.","metadata":{"_uuid":"624833da408f456999dcd85f36032f17b7d41b78"}},{"cell_type":"code","source":"'''Function to plot scatter plot of feature importance of lasso, ridge and elastic net.'''\ndef plot_feature_importance(model, title):\n    model.fit(df_train_final, y_train)\n    coef = pd.DataFrame({'Feature':df_train_final.columns,'Importance':np.transpose(model.coef_)})\n    coef = coef.set_index('Feature')\n    trace = go.Scatter(x = coef.index, y = coef.Importance, mode = 'markers',\n            marker = dict(color = np.random.randn(500), size = 20, showscale = True, colorscale = 'Hot'))\n    layout = go.Layout(hovermode = 'closest', title = title, yaxis = dict(title = 'Importance'))\n    fig = go.Figure(data = [trace], layout = layout)\n    iplot(fig)\n\n'''Now plot feature importance of optimized lasso, ridge, and elastic net.'''\nplot_feature_importance(lasso_opt, 'Lasso Feature Importance')\nplot_feature_importance(ridge_opt, 'Ridge Feature Importance')\nplot_feature_importance(elastic_net_opt, 'ElasticNet Feature Importance')","metadata":{"_uuid":"f1b1d2f154aea2dad2d121aca2941d598f4d9907"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looks like GrLivArea is the the most positively correlated factor for SalePrice while MSZoning_C(all) is the most inversely correlated factors for SalePrice for all the three models.**","metadata":{"_uuid":"3eaf3e037d7cfe9600cf5f055f7dcf0e9ddd1e50"}},{"cell_type":"code","source":"'''Now plot feature importance given by xgb, gb, and lgb.'''\ndef plot_featute_importance(model, title):\n    importance = pd.DataFrame({'Features': df_train_final.columns, 'Importance': model.feature_importances_})\n    importance = importance.set_index('Features')\n    trace = go.Scatter(x = importance.index, y = importance.Importance, mode = 'markers',\n                      marker = dict(color = np.random.randn(500), size = 20, showscale = True, colorscale = 'Rainbow'))\n    layout = go.Layout(hovermode = 'closest', title = title, yaxis = dict(title = 'Importance'))\n    fig = go.Figure(data = [trace], layout = layout)\n    iplot(fig)\n    \nplot_featute_importance(xgb_opt, 'XGB Feature Importance')\nplot_featute_importance(gb_opt, 'GB Feature Importance')\nplot_featute_importance(lgb_opt, 'LGB Feature Importance')","metadata":{"_uuid":"3fd7ef5992cda5dcfdc49d4e1812170237ba77ba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GrLivArea and LotArea seem to be most importance features given by xgb, gb, and lgb.**","metadata":{"_uuid":"5c29c4463e32461f1dc2fe28cc1fa4d3502ea787"}},{"cell_type":"markdown","source":"### 11.2.5 Learning Curves <a id=\"11.2.5\"></a>\nPlot learning curves to see the bias-variance tradeoff of our models.","metadata":{"_uuid":"0817846e5f042fd435ee86c2f0ef6efc8b2e3df9"}},{"cell_type":"code","source":"'''Create a function to plot learning curves.'''\ndef plot_learning_curve(model):\n    from sklearn.model_selection import learning_curve\n    \n    # df_train_final is training matrix and y_train is target matrix.\n    # Create CV training and test scores for various training set sizes\n    train_sizes, train_scores, test_scores = learning_curve(model, df_train_final, y_train, \n                                            train_sizes = np.linspace(0.01, 1.0, 20), cv = 10, scoring = 'neg_mean_squared_error', \n                                            n_jobs = -1, random_state = seed)\n    \n    \n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis = 1)\n    train_std = np.std(train_scores, axis = 1)\n\n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis = 1)\n    test_std = np.std(test_scores, axis = 1)  \n    \n    # Draw lines\n    plt.plot(train_sizes, train_mean, 'o-', color = 'red',  label = 'Training score')\n    plt.plot(train_sizes, test_mean, 'o-', color = 'green', label = 'Cross-validation score')\n    \n    # Draw bands\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha = 0.1, color = 'r') # Alpha controls band transparency.\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha = 0.1, color = 'g')\n    \n    \n    # Create plot\n    font_size = 12\n    plt.xlabel('Training Set Size', fontsize = font_size)\n    plt.ylabel('Accuracy Score', fontsize = font_size)\n    plt.xticks(fontsize = font_size)\n    plt.yticks(fontsize = font_size)\n    plt.legend(loc = 'best')\n    plt.grid()","metadata":{"_uuid":"97893dc1d9302b80ed4f2f4e809d2a8b4e7aae77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Now plot learning curves of the optimized models in subplots.\nFor computational reason, I am omitting XGB, GB, and LGB.'''\nplt.figure(figsize = (16,14))\nlc_models = [lasso_opt, ridge_opt, kernel_ridge_opt, elastic_net_opt, svm_opt]\nlc_labels = ['Lasso', 'Ridge', 'Kernel Ridge', 'Elastic Net', 'SVM']\n\nfor ax, models, labels in zip (range(1,6), lc_models, lc_labels):\n    plt.subplot(3,2,ax)\n    plot_learning_curve(models)\n    plt.title(labels, fontsize = 15)\nplt.suptitle('Learning Curves of Optimized Models', fontsize = 20)\nplt.tight_layout(rect = [0, 0.03, 1, 0.97])","metadata":{"_uuid":"07622a03c939f51ce5d7022c6feeebf22d909afd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All of the model are doing okay in terms of bias-variance tradeoff except kernel ridge(just a bit of high bias or low variance and hence underfitting). Since training and validation curves haven't yet converged, adding more instances might help for lasso, ridge, elastic net and svm. And for kernel ridge, increasing model's complexity (perhaps adding more features) might help.**\n\n**Can we further improve our model? May be we can! In the following section we would introduce ensemble technique and try to improve our position on leaderboard.**\n\n# 12.Introduction to Ensemble <a id=\"12\"></a>\n**Ensembles combine predictions from different models to generate a final prediction, and the more models we include the better it performs. Better still, because ensembles combine baseline predictions, they perform at least as well as the best baseline model. Most of the errors from a model’s learning are from three main factors: variance, noise, and bias. By using ensemble methods, we’re able to increase the stability of the final model and reduce the errors caused by bias, variance, and noise. By combining many models, we’re able to (mostly) reduce the variance, even when they are individually not great, as we won’t suffer from random errors from a single source. The main principle behind ensemble modelling is to group weak learners together to form one strong learner.**\n\n**To implement an ensemble we need three basic things:**\n* A group of base learners that generate predictions.\n* A meta learner that learns how to best combine these predictions outputed by base learners.\n* And finally a method for splitting the training data between the base learners and the meta learner.\n\n**An ensemble works best if:**\n* There is a less correlation in the base models' predictions.\n* We increase the number of base learners though it might slow the process down.\n\n**Ensemble methods can be divided into two, i.e., **\n1. Simple Ensemble Method 2. Advanced Ensemble Method\n\n## 12.1 Simple Ensemble Methods <a id=\"12.1\"></a>\nThey're the simpliest yet so useful form of ensembles. They can be further categorised into\n* Voting,\n* Averaging, and\n* Weighted Average.\n\n**Being a regression problem, we would perform averaging only. Before performing averaging ensemble, we need to investigate the correlations among the base models' predictions so that we can choose the most diverse model that would perform better than correlated models. Let's do that:**","metadata":{"_uuid":"7701f1d10a2ed9bcd658ee59da8b3f4f43073ea9"}},{"cell_type":"code","source":"'''Data frame of our optimized base model predictions.'''\nbase_model_prediction = pd.DataFrame({'LGB':lgb_opt.predict(df_test_final), 'Lasso':lasso_opt.predict(df_test_final), 'Ridge':ridge_opt.predict(df_test_final), 'GB':gb_opt.predict(df_test_final), 'Kernel Ridge':kernel_ridge_opt.predict(df_test_final),'Elastic Net':elastic_net_opt.predict(df_test_final), 'XGB':xgb_opt.predict(df_test_final), 'SVM':svm_opt.predict(df_test_final)})\nbold('**All the Base Model Predictions:**')\ndisplay(base_model_prediction.head())","metadata":{"_uuid":"9f9545a37e909b1f87b4239a8ab5bd1f19d6c9ee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Let's visualize the correlations among the predictions of base models.\"\"\"\nfig, ax = plt.subplots(figsize = (22, 8))\nsns.heatmap(base_model_prediction.corr(), annot = True, cmap ='BrBG', ax = ax, fmt='.2f', linewidths = 0.05, annot_kws = {'size': 14})\nax.set_title('Prediction Correlation among the Base Models', fontsize = 22)\nplt.show()","metadata":{"_uuid":"a7582055395cbe71ad55df0a678b701fc7d31b29"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions look pretty similar for all the 8 models. We would like to take kernel ridge, svm, lgb, gb, and ridge as the base models for averaging ensemble method. Now you might wonder why I don't choose lasso and elastic net instead of gb and lgb since the former two are superior  in terms of rmse. As I said earlier, the more diverse our base models are, the more superior our ensemble is. We saw GrLivArea is the top priority for all the 6 models in feature importance section. But the second priority for lasso, ridge, and elastic net was YearBuilt while it was LotArea for xgb, gb, and lgb. That's the variation we need for our ensemble to get better at prediction. If we would choose lasso and elastic net, there would be similarity instead of diversity (well that's just one example, there are many more). So our ensemble would not perform according to our expectation. I encourage you to experiment in this part.**","metadata":{"_uuid":"6fafeddb4a75e104ed3cdf564fc058e8b7fb2196"}},{"cell_type":"code","source":"\"\"\"Now let's build a simple averaging ensemble and predict with it.\"\"\"\ny_kernel_ridge = np.expm1(kernel_ridge_opt.predict(df_test_final))\ny_svm = np.expm1(svm_opt.predict(df_test_final))\ny_lgb = np.expm1(lgb_opt.predict(df_test_final))\ny_gb = np.expm1(gb_opt.predict(df_test_final))\ny_ridge = np.expm1(ridge_opt.predict(df_test_final))\n\n'''Just average the 5 prediction to form final prediction.'''\navg_ensemble = (y_kernel_ridge + y_lgb + y_svm + y_gb + y_ridge)/5\nsubmission_avg = pd.DataFrame()\nsubmission_avg['Id'] = test.Id\nsubmission_avg['SalePrice'] = avg_ensemble\nsubmission_avg.to_csv('avrage_ensemble.csv', index = False)","metadata":{"_uuid":"1365efab821ba8af44e36f5b13dc2575d8e6df02"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Well what we have done is pretty simple! We have averaged 5 models predictions to make final prediction. One thing to remember is the more diverse our base models are, the more accurate our ensemble will be. The above submission scored *0.11628* on leader board that will take you to top 16% of the competition. Lets try more sophisticated ensemble method to further advance on the leaderboard.**","metadata":{"_uuid":"e972dc6d109282571afebb395c033a662c5406ed"}},{"cell_type":"markdown","source":"## 12.2 Advanced Ensemble Methods <a id=\"12.2\"></a>\n**Stacking:** Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. So basically in stacking, the individual models (or base models) are trained on the complete training set; then, the meta-learner is fitted on the outputs (predictions) of those base learners. We will use package *vecstack* to perform stacking that can save you from writing a lot of codes if you implement stacking from scratch.","metadata":{"_uuid":"c262ed4ff75a829244b158b70d74f3c59903f8e9"}},{"cell_type":"code","source":"'''Import stacking method from vecstack.'''\nfrom vecstack import stacking\nfrom sklearn.metrics import mean_squared_error\n\n'''Initialize base models. We will use the same base models as averaging ensemble.'''\nbase_models = [kernel_ridge_opt, svm_opt, lgb_opt, gb_opt, ridge_opt]\n\nbold('**Performing Stacking...**')\n'''Perform stacking.'''\nS_train, S_test = stacking(base_models,                # list of base models\n                           df_train_final, y_train, df_test_final,   # data\n                           regression = True,          # We need regression - set to True)\n                                                       \n                           mode = 'oof_pred_bag',      # mode: oof for train set, predict test \n                                                       # set in each fold and vote\n                           needs_proba = False,        # predict class labels (if you need \n                                                       # probabilities - set to True) \n                           save_dir = None,            # do not save result and log (to save \n                                                       # in current dir - set to '.')\n                           metric = mean_squared_error,# metric: callable\n                           n_folds = 10,               # number of folds\n                           stratified = False,         # stratified split for folds\n                           shuffle = True,             # shuffle the data\n                           random_state =  seed,       # ensure reproducibility\n                           verbose = 1)                # print progress\nbold('**Stacking Done...**')","metadata":{"_uuid":"5e68007e1a682dcae4d94bc2fbd7485575daecd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So now we have OOF from base (or 0 level models) models and we can build level 1 model(meta model). We have 5 base models (level 0 models), so we expect to get 5 columns in S_train and S_test. S_train will be our input feature to train our meta learner and then prediction will be made on S_test after we train our meta learner. And this prediction on S_test is actually the prediction for our test set (X_test). Before we train our meta learner we can investigate S_train and S_test.**","metadata":{"_uuid":"3545b1a382408be3af4f0c35c633bfd878c6b75d"}},{"cell_type":"code","source":"'''Input features for meta learner.'''\nbold('**Input Features for Meta Learner:**')\ndisplay(S_train[:5])\ndisplay(S_train.shape)","metadata":{"_uuid":"4f750ce1d83d723ff9817134f2aa850c4daf44b3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Test (prediction) set for meta learner.'''\nbold('**Test Set for Meta Learner:**')\ndisplay(S_test[:5].shape)\ndisplay(S_test.shape)","metadata":{"_uuid":"96c8b1fddc94645789f2cf01db356a763adb0184"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Initialize 1st level model that is our meta learner. We will use kernel ridge.\"\"\"\nsuper_learner = kernel_ridge_opt \n\n'''Fit meta learner on the output of base learners.'''\nprint('Fitting Stacking...')\nsuper_learner.fit(S_train, y_train)\nprint('Done.')","metadata":{"_uuid":"3cc7f26d5e43e82d6952ea94bdf6e19ab552ced0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''Finally predict using super learner.'''\ny_pred_super = np.expm1(super_learner.predict(S_test))\n\n'''Make submission with super learner'''\nsubmission_super_learner = pd.DataFrame()\nsubmission_super_learner['Id']= test.Id\nsubmission_super_learner['SalePrice'] = y_pred_super\nsubmission_super_learner.to_csv('Super Learner.csv', index = False) # 0.11607 (stacking) < 0.11628 (averaging)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This submission scored 0.11607 (slightly better than averaging) on submission that would take you top 13% on the leaderboard.**","metadata":{"_uuid":"1bbbd2167415f0f7204d99c75f6a33d041cbd042"}},{"cell_type":"markdown","source":"# 13.End Note <a id=\"13\"></a>\n**Of course, there is always room for improvement. I'm still learning. I've tried to explain everything I could possibly know. Throughtout this notebook, I have tried to keep things as simple as possible. I didn't sacrifice interpretability at the cost of accuracy. Any suggestion is cordially welcomed. May be trying out different base learners and meta learner to improve ensemble further. And if you find my kernel useful, some upvotes will be appreciated. I have also another kernel that might be useful to you as well. I provide some links that I've found useful in creating this notebook.**\n\n**Recommended Readings:**\n\n**1. Vecstack package for stacking ensemble: https://github.com/vecxoz/vecstack**","metadata":{"_uuid":"459e6aebbd347597cee199a4a8f978284e581ffb"}}]}