{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 274957,
          "sourceType": "datasetVersion",
          "datasetId": 49864
        },
        {
          "sourceId": 11357,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5305,
          "modelId": 3301
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Model_ML_LLM_Gemma_review",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manjusys/Andrio-apps-llm-gemma-review-/blob/main/Model_ML_LLM_Gemma_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'google-play-store-apps:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F49864%2F274957%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241005%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241005T075453Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9990b97f9dae87905048ed22e723a51eb412ac218c9d9670485470e9f28dc273e19c69278086ba0627c4b07fb332139166c45851b9dec94872805f982f5c05a3e5fea0c147b36f714b219107cf94d0c2fed9da666ad0c8eb96ae529ce7cc3310a4fac72a0b253b0b72d9446f9daf64da004d52fd907b4c42fa55eda8cf12276ec60fa7111f2268417e0d60d7314155523ee7971e08b030ca01a1ed2a46139802a2a480f0f945919d4efd5a080979446b01e72193c25d89d1f9c32bab4aa1b195371a06db7ee98be0fbb5700eafc7b623439345d6a71fb6dc8936c99061115d8b99e8079fc33698f70fdabe32d5c56fb5d48bcfb2ea02679d7d6b29d51f0cf701'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "QpICHaonVIeJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> LLM Gemma - Analysis Review Google Play Score</b></div>"
      ],
      "metadata": {
        "id": "jQqQazEFVIeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Business Problem: Sentiment Analysis for App Reviews\n",
        "\n",
        "**Context:**\n",
        "The mobile app industry is highly competitive, with millions of applications available across various platforms. User reviews provide valuable feedback on user experience, functionality, and satisfaction. Analyzing these reviews is crucial for app developers and companies to understand user sentiment, improve user engagement, and guide product development.\n",
        "\n",
        "**Problem Statement:**\n",
        "The goal is to build a sentiment classification model that uses app reviews to automatically categorize user sentiment into positive, negative, or neutral categories. This will enable companies to gain real-time insights into how their apps are being perceived and take proactive measures to enhance user satisfaction and retention.\n",
        "\n",
        "**Business Questions:**\n",
        "1. **How are users reacting to the app?**\n",
        "   - Analyze the proportion of positive, negative, and neutral reviews.\n",
        "   \n",
        "2. **What factors influence user sentiment the most?**\n",
        "   - Use the sentiment polarity and subjectivity scores to identify patterns and key factors driving user satisfaction or dissatisfaction.\n",
        "\n",
        "3. **How can the app's performance be improved based on user feedback?**\n",
        "   - By categorizing sentiment, identify common issues in negative reviews and highlight features appreciated in positive reviews.\n",
        "\n",
        "4. **How can the app development and marketing teams use sentiment analysis to guide decisions?**\n",
        "   - Generate insights to support data-driven decisions for feature improvements, bug fixes, or marketing strategies.\n",
        "\n",
        "**Solution Approach:**\n",
        "Develop a Large Language Model (LLM) using the Gemma platform to automatically classify app reviews based on user sentiment. This model will process the `Translated_Review` and provide a sentiment classification, along with sentiment polarity and subjectivity, to give a comprehensive view of the user experience."
      ],
      "metadata": {
        "id": "FP5CeCHbVIeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing packages\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install transformers accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install faiss-cpu\n",
        "!pip install torch\n",
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "!pip install watermark\n",
        "!pip install accelerate deepspeed\n",
        "!pip install transformers sentence_transformers faiss-cpu torch PyPDF2 nltk"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:28:28.182678Z",
          "iopub.execute_input": "2024-09-10T16:28:28.182968Z",
          "iopub.status.idle": "2024-09-10T16:31:18.645729Z",
          "shell.execute_reply.started": "2024-09-10T16:28:28.182934Z",
          "shell.execute_reply": "2024-09-10T16:31:18.644713Z"
        },
        "trusted": true,
        "id": "3VM_rYj4VIeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import of libraries\n",
        "\n",
        "# System libraries\n",
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "from datasets import Dataset\n",
        "\n",
        "# Library for file manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas\n",
        "\n",
        "# Data visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as pl\n",
        "import matplotlib as m\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Configuration for graph width and layout\n",
        "sns.set_theme(style='whitegrid')\n",
        "palette='viridis'\n",
        "\n",
        "## LLM\n",
        "# Importing necessary libraries from PyTorch and Hugging Face Transformers\n",
        "# PyTorch is a deep learning framework used for model training and inference\n",
        "import torch\n",
        "\n",
        "# AutoTokenizer: Automatically loads a pre-trained tokenizer for encoding text\n",
        "# AutoModelForCausalLM: Loads a pre-trained model for causal language modeling (e.g., for text generation)\n",
        "# pipeline: Provides an easy-to-use interface to perform tasks like text generation, sentiment analysis, etc.\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Warnings remove alerts\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Python version\n",
        "from platform import python_version\n",
        "print('Python version in this Jupyter Notebook:', python_version())\n",
        "\n",
        "# Load library versions\n",
        "import watermark\n",
        "\n",
        "# Library versions\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Library versions\" --iversions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:18.648148Z",
          "iopub.execute_input": "2024-09-10T16:31:18.649018Z",
          "iopub.status.idle": "2024-09-10T16:31:37.963925Z",
          "shell.execute_reply.started": "2024-09-10T16:31:18.648964Z",
          "shell.execute_reply": "2024-09-10T16:31:37.963033Z"
        },
        "trusted": true,
        "id": "uUG5FavzVIeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Database"
      ],
      "metadata": {
        "id": "jIYdQEx4VIeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Database\n",
        "df = pd.read_csv(\"/kaggle/input/google-play-store-apps/googleplaystore_user_reviews.csv\")\n",
        "\n",
        "# Viewing dataset\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:37.964982Z",
          "iopub.execute_input": "2024-09-10T16:31:37.965636Z",
          "iopub.status.idle": "2024-09-10T16:31:38.234009Z",
          "shell.execute_reply.started": "2024-09-10T16:31:37.9656Z",
          "shell.execute_reply": "2024-09-10T16:31:38.233041Z"
        },
        "trusted": true,
        "id": "GnfxjfxAVIeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing first 5 data\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.236177Z",
          "iopub.execute_input": "2024-09-10T16:31:38.236529Z",
          "iopub.status.idle": "2024-09-10T16:31:38.248345Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.236494Z",
          "shell.execute_reply": "2024-09-10T16:31:38.247567Z"
        },
        "trusted": true,
        "id": "-woPTp1xVIeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing 5 latest data\n",
        "df.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.24986Z",
          "iopub.execute_input": "2024-09-10T16:31:38.25028Z",
          "iopub.status.idle": "2024-09-10T16:31:38.261102Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.250236Z",
          "shell.execute_reply": "2024-09-10T16:31:38.260129Z"
        },
        "trusted": true,
        "id": "wgpks5_3VIeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Info data\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.262267Z",
          "iopub.execute_input": "2024-09-10T16:31:38.262582Z",
          "iopub.status.idle": "2024-09-10T16:31:38.300965Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.262548Z",
          "shell.execute_reply": "2024-09-10T16:31:38.300055Z"
        },
        "trusted": true,
        "id": "D25InHqAVIeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Type data\n",
        "df.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.30217Z",
          "iopub.execute_input": "2024-09-10T16:31:38.30256Z",
          "iopub.status.idle": "2024-09-10T16:31:38.31Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.302516Z",
          "shell.execute_reply": "2024-09-10T16:31:38.308982Z"
        },
        "trusted": true,
        "id": "StKEdP9SVIeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing rows and columns\n",
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.31107Z",
          "iopub.execute_input": "2024-09-10T16:31:38.311391Z",
          "iopub.status.idle": "2024-09-10T16:31:38.319181Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.311358Z",
          "shell.execute_reply": "2024-09-10T16:31:38.318202Z"
        },
        "trusted": true,
        "id": "FQ4YPrZfVIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Data cleaning"
      ],
      "metadata": {
        "id": "GzslkV9qVIeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking for missing values in each column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.320367Z",
          "iopub.execute_input": "2024-09-10T16:31:38.320695Z",
          "iopub.status.idle": "2024-09-10T16:31:38.343299Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.320664Z",
          "shell.execute_reply": "2024-09-10T16:31:38.342294Z"
        },
        "trusted": true,
        "id": "Xr2MwBKbVIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of rows and the number of missing values in the 'Translated_Review' column.\n",
        "# Then, print the percentage of missing values in the 'Translated_Review' column.\n",
        "total_rows = len(df)\n",
        "missing_translated_review = df['Translated_Review'].isnull().sum()\n",
        "print(f\"Percentage of missing Translated_Review: {(missing_translated_review/total_rows)*100:.2f}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.347403Z",
          "iopub.execute_input": "2024-09-10T16:31:38.347767Z",
          "iopub.status.idle": "2024-09-10T16:31:38.35803Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.347733Z",
          "shell.execute_reply": "2024-09-10T16:31:38.357219Z"
        },
        "trusted": true,
        "id": "zJkPOciiVIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows that have missing values in the 'Translated_Review' or 'Sentiment' columns\n",
        "df = df.dropna(subset=['Translated_Review', 'Sentiment'])\n",
        "\n",
        "# Print the count of missing values for each column after dropping the rows with missing data\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Display the DataFrame\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.359083Z",
          "iopub.execute_input": "2024-09-10T16:31:38.359395Z",
          "iopub.status.idle": "2024-09-10T16:31:38.403096Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.359353Z",
          "shell.execute_reply": "2024-09-10T16:31:38.40214Z"
        },
        "trusted": true,
        "id": "B-t7OOgVVIeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Text Preprocessing"
      ],
      "metadata": {
        "id": "yUzZIx20VIeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "### Download the necessary resources from nltk (tokenizers and stopwords corpus)\n",
        "# Punkt tokenizer for word tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Stopwords list in multiple languages\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the Porter stemmer and load English stopwords\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Create a set of English stopwords for efficient lookup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean the text by removing URLs, handles, and punctuation\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Convert text to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs (http, https, and www links)\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove markdown-style links [text](link)\n",
        "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
        "\n",
        "        # Remove handles (@username mentions)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        # Remove punctuation and special characters\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Function to tokenize the text into individual words\n",
        "def tokenize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return word_tokenize(text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Function to remove stopwords from the tokenized text\n",
        "def remove_stopwords(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "# Function to apply stemming to the tokens\n",
        "def stem_tokens(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return [stemmer.stem(token) for token in tokens]\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "### Apply the functions to the DataFrame\n",
        "# Clean the text\n",
        "df['Cleaned_Review'] = df['Translated_Review'].apply(clean_text)\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "df['Tokenized_Review'] = df['Cleaned_Review'].apply(tokenize_text)\n",
        "\n",
        "# Apply stemming to the tokenized words\n",
        "df['Stemmed_Review'] = df['Tokenized_Review'].apply(stem_tokens)\n",
        "\n",
        "# Remove stopwords from the tokenized text\n",
        "df['No_Stopwords_Review'] = df['Tokenized_Review'].apply(remove_stopwords)\n",
        "\n",
        "# Display the first few rows of the DataFrame to visualize the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:31:38.404525Z",
          "iopub.execute_input": "2024-09-10T16:31:38.404858Z",
          "iopub.status.idle": "2024-09-10T16:32:08.869229Z",
          "shell.execute_reply.started": "2024-09-10T16:31:38.404824Z",
          "shell.execute_reply": "2024-09-10T16:32:08.868211Z"
        },
        "trusted": true,
        "id": "9404DpFVVIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Exploratory data analysis"
      ],
      "metadata": {
        "id": "wnb7dd3zVIeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "# Calculate and display the distribution of sentiment labels in the dataset\n",
        "sentiment_distribution = df['Sentiment'].value_counts()\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(sentiment_distribution)\n",
        "print()\n",
        "\n",
        "# Descriptive statistics for sentiment polarity and subjectivity\n",
        "# Display basic statistics (mean, std, min, max, etc.) for the polarity and subjectivity columns\n",
        "sentiment_stats = df[['Sentiment_Polarity', 'Sentiment_Subjectivity']].describe()\n",
        "print(\"Sentiment Polarity and Subjectivity Statistics:\")\n",
        "print(sentiment_stats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:08.870332Z",
          "iopub.execute_input": "2024-09-10T16:32:08.870661Z",
          "iopub.status.idle": "2024-09-10T16:32:08.895303Z",
          "shell.execute_reply.started": "2024-09-10T16:32:08.870627Z",
          "shell.execute_reply": "2024-09-10T16:32:08.894324Z"
        },
        "trusted": true,
        "id": "8HghH4HtVIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfico da distribuição de sentimentos com melhorias\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Countplot com ordenação por contagem e cores personalizadas\n",
        "sns.countplot(data=df, x='Sentiment', palette='Set2', order=df['Sentiment'].value_counts().index)\n",
        "\n",
        "# Adicionar título mais descritivo\n",
        "plt.title('Distribution of Sentiments in Reviews', fontsize=16)\n",
        "\n",
        "# Adicionar rótulos aos eixos\n",
        "plt.xlabel('Sentiment', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "# Adicionar rótulos de dados (contagem) nas barras\n",
        "for p in plt.gca().patches:\n",
        "    plt.gca().annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                       ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "# Mostrar o gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:08.896775Z",
          "iopub.execute_input": "2024-09-10T16:32:08.897645Z",
          "iopub.status.idle": "2024-09-10T16:32:09.274302Z",
          "shell.execute_reply.started": "2024-09-10T16:32:08.897609Z",
          "shell.execute_reply": "2024-09-10T16:32:09.273381Z"
        },
        "trusted": true,
        "id": "A8v0ct4JVIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved Visualization of Sentiment Polarity vs Subjectivity\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot with improvements: transparency, marker size, and adjusted legend\n",
        "sns.scatterplot(x='Sentiment_Polarity',\n",
        "                y='Sentiment_Subjectivity',\n",
        "                data=df,\n",
        "                hue='Sentiment',\n",
        "                palette='Set1',\n",
        "                alpha=0.6,  # Transparency to reduce overlap\n",
        "                s=70)       # Adjust marker size for better clarity\n",
        "\n",
        "# Improve title and axis labels for better clarity\n",
        "plt.title('Sentiment Polarity vs Subjectivity', fontsize=16)\n",
        "plt.xlabel('Sentiment Polarity', fontsize=14)\n",
        "plt.ylabel('Sentiment Subjectivity', fontsize=14)\n",
        "\n",
        "# Move the legend to a better location outside the plot\n",
        "plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show the improved plot\n",
        "plt.tight_layout()  # Adjust layout to avoid cutting off labels or legend\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:09.275534Z",
          "iopub.execute_input": "2024-09-10T16:32:09.27582Z",
          "iopub.status.idle": "2024-09-10T16:32:10.753285Z",
          "shell.execute_reply.started": "2024-09-10T16:32:09.275789Z",
          "shell.execute_reply": "2024-09-10T16:32:10.752328Z"
        },
        "trusted": true,
        "id": "oz2yyLDbVIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Function to generate and plot a word cloud\n",
        "def plot_wordcloud(text, title):\n",
        "    # Join the list of reviews into a single string and generate the word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "\n",
        "    # Plot the word cloud\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')  # Turn off axis lines and labels\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "# Filter the reviews based on sentiment\n",
        "positive_reviews = df[df['Sentiment'] == 'Positive']['Cleaned_Review']\n",
        "negative_reviews = df[df['Sentiment'] == 'Negative']['Cleaned_Review']\n",
        "neutral_reviews = df[df['Sentiment'] == 'Neutral']['Cleaned_Review']\n",
        "\n",
        "# Generate word clouds for each sentiment\n",
        "plot_wordcloud(positive_reviews, 'Word Cloud for Positive Sentiment')\n",
        "plot_wordcloud(negative_reviews, 'Word Cloud for Negative Sentiment')\n",
        "plot_wordcloud(neutral_reviews, 'Word Cloud for Neutral Sentiment')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:10.754451Z",
          "iopub.execute_input": "2024-09-10T16:32:10.754753Z",
          "iopub.status.idle": "2024-09-10T16:32:18.597002Z",
          "shell.execute_reply.started": "2024-09-10T16:32:10.75472Z",
          "shell.execute_reply": "2024-09-10T16:32:18.596086Z"
        },
        "trusted": true,
        "id": "KtD0ifESVIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine all tokens into a single list\n",
        "all_tokens = [token for tokens in df['Tokenized_Review'] for token in tokens]\n",
        "\n",
        "# Count the frequency of tokens\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Get the top 20 most common tokens\n",
        "common_tokens = token_counts.most_common(20)  # Limiting to top 20\n",
        "\n",
        "# Separate tokens and their frequencies\n",
        "tokens, frequencies = zip(*common_tokens)\n",
        "\n",
        "# Create a bar plot for the most frequent tokens\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(frequencies), y=list(tokens), palette='husl')  # Changed palette\n",
        "\n",
        "# Improved title and axis labels\n",
        "plt.title('Top 20 Most Common Tokens in Reviews', fontsize=16)\n",
        "plt.xlabel('Frequency', fontsize=14)\n",
        "plt.ylabel('Tokens', fontsize=14)\n",
        "\n",
        "# Add gridlines for easier reading of bar heights\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()  # Ensure layout is clean and labels fit well\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:18.598223Z",
          "iopub.execute_input": "2024-09-10T16:32:18.5989Z",
          "iopub.status.idle": "2024-09-10T16:32:19.329374Z",
          "shell.execute_reply.started": "2024-09-10T16:32:18.598864Z",
          "shell.execute_reply": "2024-09-10T16:32:19.32842Z"
        },
        "trusted": true,
        "id": "kUaaCEH7VIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reviews App\n",
        "reviews_per_app = df['App'].value_counts()\n",
        "print(\"Reviews per App:\")\n",
        "print(reviews_per_app.head(10))\n",
        "\n",
        "# Improved version of the top 10 apps by number of reviews bar plot\n",
        "plt.figure(figsize=(12, 7))  # Increase the figure size for better spacing\n",
        "\n",
        "# Plot with improved palette and rotation\n",
        "sns.barplot(x=reviews_per_app.head(10).index, y=reviews_per_app.head(10).values, palette='Blues_d')\n",
        "\n",
        "# Rotate the x-axis labels to 90 degrees for easier reading\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add data labels on top of each bar\n",
        "for i, value in enumerate(reviews_per_app.head(10).values):\n",
        "    plt.text(i, value + 5, str(value), ha='center', fontsize=10)\n",
        "\n",
        "# Improved title and axis labels\n",
        "plt.title('Top 10 Apps by Number of Reviews', fontsize=16)\n",
        "plt.xlabel('App', fontsize=14)\n",
        "plt.ylabel('Number of Reviews', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:19.330621Z",
          "iopub.execute_input": "2024-09-10T16:32:19.330935Z",
          "iopub.status.idle": "2024-09-10T16:32:19.88262Z",
          "shell.execute_reply.started": "2024-09-10T16:32:19.330901Z",
          "shell.execute_reply": "2024-09-10T16:32:19.881715Z"
        },
        "trusted": true,
        "id": "gzNrNo_HVIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze sentiment distribution by app\n",
        "sentiment_by_app = df.groupby('App')['Sentiment'].value_counts(normalize=True).unstack()\n",
        "\n",
        "# Display the top 10 apps\n",
        "sentiment_by_app_top = sentiment_by_app.head(10)  # Limiting to top 10 apps\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot stacked bar chart with an improved color palette\n",
        "sentiment_by_app_top.plot(kind='bar', stacked=True, colormap='Set1', figsize=(12, 8))\n",
        "\n",
        "# Rotate x-axis labels to 90 degrees for better readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Improve title and axis labels\n",
        "plt.title('Sentiment Distribution for Top 10 Apps', fontsize=16)\n",
        "plt.xlabel('App', fontsize=14)\n",
        "plt.ylabel('Proportion', fontsize=14)\n",
        "\n",
        "# Move the legend outside the plot\n",
        "plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Add gridlines for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Adjust layout to ensure the plot fits well with the legend\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the improved plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:19.883888Z",
          "iopub.execute_input": "2024-09-10T16:32:19.884199Z",
          "iopub.status.idle": "2024-09-10T16:32:20.50826Z",
          "shell.execute_reply.started": "2024-09-10T16:32:19.884165Z",
          "shell.execute_reply": "2024-09-10T16:32:20.507254Z"
        },
        "trusted": true,
        "id": "jDy8hl9eVIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to count the most common words in reviews\n",
        "def get_most_common_words(reviews, num_words=10):\n",
        "    # Flatten the list of lists into a single list of words\n",
        "    words = [word for review in reviews for word in review]\n",
        "    return Counter(words).most_common(num_words)\n",
        "\n",
        "# Get the most common words for each sentiment type\n",
        "positive_words = get_most_common_words(df[df['Sentiment'] == 'Positive']['No_Stopwords_Review'], num_words=10)\n",
        "negative_words = get_most_common_words(df[df['Sentiment'] == 'Negative']['No_Stopwords_Review'], num_words=10)\n",
        "neutral_words = get_most_common_words(df[df['Sentiment'] == 'Neutral']['No_Stopwords_Review'], num_words=10)\n",
        "\n",
        "# Display results in a clear format\n",
        "print(\"Most Common Positive Words:\")\n",
        "for word, count in positive_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nMost Common Negative Words:\")\n",
        "for word, count in negative_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nMost Common Neutral Words:\")\n",
        "for word, count in neutral_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:20.509615Z",
          "iopub.execute_input": "2024-09-10T16:32:20.509932Z",
          "iopub.status.idle": "2024-09-10T16:32:20.649203Z",
          "shell.execute_reply.started": "2024-09-10T16:32:20.509898Z",
          "shell.execute_reply": "2024-09-10T16:32:20.648292Z"
        },
        "trusted": true,
        "id": "IHHFdjTMVIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the most common words for a given sentiment\n",
        "def plot_most_common_words(common_words, sentiment, color):\n",
        "    words, counts = zip(*common_words)\n",
        "\n",
        "    # Create the bar plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(words), palette=color)\n",
        "\n",
        "    # Set titles and labels\n",
        "    plt.title(f'Most Common Words in {sentiment} Reviews', fontsize=16)\n",
        "    plt.xlabel('Frequency', fontsize=14)\n",
        "    plt.ylabel('Words', fontsize=14)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Positive Sentiment\n",
        "plot_most_common_words(positive_words, 'Positive', 'Greens')\n",
        "\n",
        "# Plot for Negative Sentiment\n",
        "plot_most_common_words(negative_words, 'Negative', 'Reds')\n",
        "\n",
        "# Plot for Neutral Sentiment\n",
        "plot_most_common_words(neutral_words, 'Neutral', 'Blues')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:20.650268Z",
          "iopub.execute_input": "2024-09-10T16:32:20.650579Z",
          "iopub.status.idle": "2024-09-10T16:32:21.797128Z",
          "shell.execute_reply.started": "2024-09-10T16:32:20.650547Z",
          "shell.execute_reply": "2024-09-10T16:32:21.796178Z"
        },
        "trusted": true,
        "id": "6_sAGEYcVIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean polarity and subjectivity by sentiment\n",
        "mean_polarity_subjectivity = df.groupby('Sentiment')[['Sentiment_Polarity', 'Sentiment_Subjectivity']].mean()\n",
        "\n",
        "# Print the result for verification\n",
        "print(\"Mean Polarity and Subjectivity by Sentiment:\")\n",
        "print(mean_polarity_subjectivity)\n",
        "\n",
        "# Bar plot of average polarity and subjectivity by sentiment\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a bar plot with side-by-side bars instead of stacked bars\n",
        "mean_polarity_subjectivity.plot(kind='bar', figsize=(15, 6), colormap='viridis', width=0.8)\n",
        "\n",
        "# Improve title and axis labels\n",
        "plt.title('Average Sentiment Polarity and Subjectivity by Sentiment', fontsize=16)\n",
        "plt.xlabel('Sentiment', fontsize=14)\n",
        "plt.ylabel('Average Values', fontsize=14)\n",
        "\n",
        "# Add data labels on top of the bars\n",
        "for i in range(len(mean_polarity_subjectivity)):\n",
        "    for j in range(len(mean_polarity_subjectivity.columns)):\n",
        "        plt.text(i - 0.2 + j * 0.4,\n",
        "                 mean_polarity_subjectivity.iloc[i, j] + 0.02 * (-1 if mean_polarity_subjectivity.iloc[i, j] < 0 else 1),\n",
        "                 round(mean_polarity_subjectivity.iloc[i, j], 2),\n",
        "                 ha='center', fontsize=12)\n",
        "\n",
        "# Display the legend\n",
        "plt.legend(loc='upper left', fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:21.798307Z",
          "iopub.execute_input": "2024-09-10T16:32:21.798634Z",
          "iopub.status.idle": "2024-09-10T16:32:22.145283Z",
          "shell.execute_reply.started": "2024-09-10T16:32:21.7986Z",
          "shell.execute_reply": "2024-09-10T16:32:22.144321Z"
        },
        "trusted": true,
        "id": "-JJGpxm4VIeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section A) Machine learning models"
      ],
      "metadata": {
        "id": "-ch6mb9fVIeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6) Vectorizing text with Tfidf Vectorizer"
      ],
      "metadata": {
        "id": "E5syhIXvVIeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Convert text data into numerical features using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to top 5000 features\n",
        "X = tfidf_vectorizer.fit_transform(df['Cleaned_Review'])\n",
        "y = df['Sentiment']\n",
        "tfidf_vectorizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:22.146624Z",
          "iopub.execute_input": "2024-09-10T16:32:22.146951Z",
          "iopub.status.idle": "2024-09-10T16:32:23.039092Z",
          "shell.execute_reply.started": "2024-09-10T16:32:22.146916Z",
          "shell.execute_reply": "2024-09-10T16:32:23.038173Z"
        },
        "trusted": true,
        "id": "sbhRAEvuVIeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: If 'Sentiment' is not already numeric, you can use label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "le"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:23.040493Z",
          "iopub.execute_input": "2024-09-10T16:32:23.040886Z",
          "iopub.status.idle": "2024-09-10T16:32:23.058538Z",
          "shell.execute_reply.started": "2024-09-10T16:32:23.040842Z",
          "shell.execute_reply": "2024-09-10T16:32:23.057633Z"
        },
        "trusted": true,
        "id": "Qr6Fa4uXVIeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 7 - Target column split and test"
      ],
      "metadata": {
        "id": "x9Pc9Dz5VIeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Training and testing division\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Viewing training data\n",
        "print(\"Viewing rows and columns given by X train\", X_train.shape)\n",
        "\n",
        "# Viewing test data\n",
        "print(\"Viewing rows and columns given y train\", y_train.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:23.059871Z",
          "iopub.execute_input": "2024-09-10T16:32:23.060248Z",
          "iopub.status.idle": "2024-09-10T16:32:23.073632Z",
          "shell.execute_reply.started": "2024-09-10T16:32:23.060205Z",
          "shell.execute_reply": "2024-09-10T16:32:23.072586Z"
        },
        "trusted": true,
        "id": "QgjGEPHwVIeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your TF-IDF sparse matrix to a dense matrix\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:23.07478Z",
          "iopub.execute_input": "2024-09-10T16:32:23.075137Z",
          "iopub.status.idle": "2024-09-10T16:32:24.256689Z",
          "shell.execute_reply.started": "2024-09-10T16:32:23.075103Z",
          "shell.execute_reply": "2024-09-10T16:32:24.255609Z"
        },
        "trusted": true,
        "id": "tQ47ZDJCVIeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we conducted the training of the model using a train-test split. We adopted an 80/20 division, where 80% of the data was used for training and the remaining 20% was reserved for testing. This procedure is crucial for accurately evaluating the model's performance. The training set allows the model to learn patterns and relationships within the data, while the test set, which the model has not seen during training, is used to validate its ability to generalize and predict new data. Additionally, this approach helps identify and mitigate issues such as overfitting, ensuring that the model not only memorizes the training data but also performs well on unseen data."
      ],
      "metadata": {
        "id": "J39rvaUzVIeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 8) Machine learning model training"
      ],
      "metadata": {
        "id": "AIB30AQ8VIeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Models to be evaluated\n",
        "models = [\n",
        "            # Naive Bayes Model (requires dense matrix)\n",
        "            GaussianNB(),\n",
        "\n",
        "            # Decision Tree Model\n",
        "            DecisionTreeClassifier(random_state=42),\n",
        "\n",
        "            # Random forest model\n",
        "            RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "\n",
        "            # Logistic regression model\n",
        "            LogisticRegression(random_state=50),\n",
        "\n",
        "            # Ada Boost Model\n",
        "            AdaBoostClassifier(random_state=45),\n",
        "\n",
        "            # XGBoost Model (can use sparse matrix)\n",
        "            XGBClassifier(tree_method='gpu_hist', random_state=42),\n",
        "\n",
        "            # LightGBM Model (can use sparse matrix)\n",
        "            LGBMClassifier(num_leaves=31,\n",
        "                           boosting_type='gbdt',\n",
        "                           bagging_fraction=0.9,\n",
        "                           learning_rate=0.05,\n",
        "                           feature_fraction=0.9,\n",
        "                           bagging_freq=50,\n",
        "                           verbose=50,\n",
        "                           device='gpu'),\n",
        "\n",
        "            # K-Nearest Neighbors Model\n",
        "            KNeighborsClassifier(n_neighbors=13),\n",
        "\n",
        "            # Gradient Boosting Classifier\n",
        "            GradientBoostingClassifier(random_state=42)]\n",
        "\n",
        "# Evaluate each model\n",
        "for i, model in enumerate(models):\n",
        "    # For GaussianNB (requires dense matrix)\n",
        "    if isinstance(model, GaussianNB):\n",
        "        model.fit(X_train_dense, y_train)\n",
        "        train_accuracy = accuracy_score(y_train, model.predict(X_train_dense))\n",
        "        test_accuracy = accuracy_score(y_test, model.predict(X_test_dense))\n",
        "    else:\n",
        "        # For all other models\n",
        "        model.fit(X_train, y_train)\n",
        "        train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "        test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "    print(f\"Model {i+1}: {type(model).__name__}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T16:32:24.258183Z",
          "iopub.execute_input": "2024-09-10T16:32:24.258608Z"
        },
        "trusted": true,
        "id": "u8dr6Gu4VIeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 8.1 - Feature importances**\n",
        "\n",
        "- Feature importances refers to the measure of how important each feature is for a machine learning model in making predictions or classifications. In other words, it is a way to quantify the impact or contribution of each feature to the decisions made by the model. In many machine learning algorithms such as decision trees, Random Forest, Gradient Boosting, among others, it is possible to calculate the importance of features during model training.\n",
        "\n",
        "- This is done by observing how each feature influences the decisions made by the model when dividing the data into decision tree nodes or by weighing the features in other model structures.\n",
        "\n",
        "- Analyzing feature importances is valuable because it can provide insights into which features are most relevant to the problem at hand. This information can be used to optimize the model, remove irrelevant or redundant features, identify important factors for prediction, and even assist in interpreting the model's results."
      ],
      "metadata": {
        "id": "Be-Kh608VIed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models that support feature importances\n",
        "models_with_feature_importances = [(\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=42)),\n",
        "                                   (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "                                   (\"XGBClassifier\", XGBClassifier(random_state=42)),\n",
        "                                   (\"LGBMClassifier\", LGBMClassifier(random_state=42))]\n",
        "\n",
        "# Get feature names from the TfidfVectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Iterate over models\n",
        "for model_name, model in models_with_feature_importances:\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Get importance of features\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        feature_importances = model.feature_importances_\n",
        "    else:\n",
        "        # If the model does not have feature_importances_, continue to the next model\n",
        "        print(f\"{model_name} does not support feature importances.\")\n",
        "        continue\n",
        "\n",
        "    # Create DataFrame for easier viewing\n",
        "    feature_importances_df = pd.DataFrame({'Feature': feature_names,\n",
        "                                           'Importance': feature_importances})\n",
        "\n",
        "    # Sort by importance\n",
        "    feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importances_df[:10])\n",
        "    plt.title(f\"Top 10 Features - {model_name}\")\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JE-w5loNVIed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 9) Evaluation metrics"
      ],
      "metadata": {
        "id": "E41JLFwsVIed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "# Define your sentiment labels\n",
        "labels = ['Positive',\n",
        "          'Neutral',\n",
        "          'Negative']\n",
        "\n",
        "# Convert your TF-IDF sparse matrix to a dense matrix for models that require dense input\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Evaluate each model\n",
        "for i, model in enumerate(models):\n",
        "    # Check if the model requires dense data (like GaussianNB)\n",
        "    if isinstance(model, GaussianNB):\n",
        "        model.fit(X_train_dense, y_train)\n",
        "        y_train_pred = model.predict(X_train_dense)\n",
        "        y_test_pred = model.predict(X_test_dense)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"Model {i+1}: {type(model).__name__}\")\n",
        "    print(f\"Training Accuracy: {train_accuracy}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy}\")\n",
        "    print()\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "    print(f'Confusion matrix for Model {i+1}: {type(model).__name__} \\n\\n', cm)\n",
        "\n",
        "    # Plot the confusion matrix with annotations for three classes\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(f\"Confusion Matrix - Model {i+1}: {type(model).__name__}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"------------------\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Limmt9rKVIed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 10) Model Evaluation"
      ],
      "metadata": {
        "id": "12pHw8FHVIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Convert sparse matrix to dense for models that require dense input\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Binarize the output labels for multiclass ROC-AUC\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "# Adjust this depending on the number of classes\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Models to be evaluated\n",
        "models = [\n",
        "    GaussianNB(),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    KNeighborsClassifier(),\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    LogisticRegression(random_state=42, max_iter=1000),\n",
        "    AdaBoostClassifier(random_state=42),\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    XGBClassifier(random_state=42),\n",
        "    LGBMClassifier(),\n",
        "    CatBoostClassifier(task_type='GPU', iterations=1000, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "]\n",
        "\n",
        "# Evaluate each model\n",
        "for i, model in enumerate(models):\n",
        "    print(f\"Model {i+1}: {type(model).__name__}\")\n",
        "\n",
        "    # Check if the model requires dense data (like GaussianNB)\n",
        "    if isinstance(model, (GaussianNB, KNeighborsClassifier)):\n",
        "        model.fit(X_train_dense, y_train)\n",
        "        y_train_pred = model.predict(X_train_dense)\n",
        "        y_test_pred = model.predict(X_test_dense)\n",
        "        y_probs = model.predict_proba(X_test_dense)\n",
        "    else:\n",
        "        # For all other models, use sparse matrices\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_probs = model.predict_proba(X_test)\n",
        "        else:\n",
        "            print(f\"{type(model).__name__} does not support predict_proba, skipping AUC/ROC plot.\")\n",
        "            continue\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Calculate ROC curve and AUC for each class (multiclass)\n",
        "    if 'y_probs' in locals():\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "\n",
        "        for j in range(n_classes):\n",
        "            fpr[j], tpr[j], _ = roc_curve(y_test_bin[:, j], y_probs[:, j])\n",
        "            roc_auc[j] = roc_auc_score(y_test_bin[:, j], y_probs[:, j])\n",
        "\n",
        "        # Plot ROC curve for each class\n",
        "        plt.figure()\n",
        "        colors = ['blue', 'red', 'green']\n",
        "        for j, color in enumerate(colors):\n",
        "            plt.plot(fpr[j], tpr[j], color=color, lw=2, label=f'Class {j} (AUC = {roc_auc[j]:.2f})')\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Multiclass ROC Curve - Model {i+1}: {type(model).__name__}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(False)\n",
        "        plt.show()\n",
        "\n",
        "    print(\"------------------\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1AOytqBoVIee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1 - Classification report**"
      ],
      "metadata": {
        "id": "klu4lmmJVIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiment labels (assuming 3-class classification)\n",
        "sentiment_labels = ['Positive', 'Neutral', 'Negative']\n",
        "\n",
        "# Convert sparse matrix to dense for models that require dense input\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Models to be evaluated\n",
        "models = [\n",
        "          GaussianNB(),\n",
        "          DecisionTreeClassifier(random_state=42),\n",
        "          KNeighborsClassifier(),\n",
        "          RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "          LogisticRegression(random_state=42, max_iter=1000),\n",
        "          AdaBoostClassifier(random_state=42),\n",
        "          XGBClassifier(random_state=42),\n",
        "          LGBMClassifier(),\n",
        "          CatBoostClassifier(task_type='GPU', iterations=1000, learning_rate=0.1, depth=6, verbose=0, random_state=42)]\n",
        "\n",
        "# Evaluate each model\n",
        "for i, model in enumerate(models):\n",
        "\n",
        "    print(f\"Model {i+1}: {type(model).__name__}\")\n",
        "\n",
        "    # For models that require dense matrices\n",
        "    if isinstance(model, (GaussianNB, KNeighborsClassifier)):\n",
        "        model.fit(X_train_dense, y_train)\n",
        "        y_train_pred = model.predict(X_train_dense)\n",
        "        y_test_pred = model.predict(X_test_dense)\n",
        "    else:\n",
        "        # For models that work with sparse matrices\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"Training Accuracy: {train_accuracy}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Generate classification report with sentiment labels\n",
        "    report = classification_report(y_test, y_test_pred, target_names=sentiment_labels)\n",
        "    print()\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"=======================================\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "2rY-mkihVIee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 11 - Result models"
      ],
      "metadata": {
        "id": "WWVmjy2EVIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sparse matrix to dense for models that require dense input\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Models to be evaluated\n",
        "models = [GaussianNB(),\n",
        "          DecisionTreeClassifier(random_state=42),\n",
        "          KNeighborsClassifier(),\n",
        "          RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "          LogisticRegression(random_state=42, max_iter=1000),\n",
        "          AdaBoostClassifier(random_state=42),\n",
        "          XGBClassifier(random_state=42),\n",
        "          GradientBoostingClassifier(random_state=42),\n",
        "          LGBMClassifier()]\n",
        "\n",
        "# List to store metrics for each model\n",
        "metricas = []\n",
        "\n",
        "# Evaluate each model\n",
        "for model in models:\n",
        "    print(f\"Evaluating {type(model).__name__}\")\n",
        "\n",
        "    # For models that require dense matrices\n",
        "    if isinstance(model, (GaussianNB, KNeighborsClassifier)):\n",
        "        model.fit(X_train_dense, y_train)\n",
        "        train_accuracy = accuracy_score(y_train, model.predict(X_train_dense))\n",
        "        test_accuracy = accuracy_score(y_test, model.predict(X_test_dense))\n",
        "        report = classification_report(y_test, model.predict(X_test_dense), output_dict=True)\n",
        "    else:\n",
        "        # For models that work with sparse matrices\n",
        "        model.fit(X_train, y_train)\n",
        "        train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "        test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "        report = classification_report(y_test, model.predict(X_test), output_dict=True)\n",
        "\n",
        "    # Extract metrics of interest from the report\n",
        "    metrics = {\"Model\": type(model).__name__,\n",
        "               \"Accuracy\": test_accuracy,\n",
        "               \"Precision\": report['weighted avg']['precision'],\n",
        "               \"Recall\": report['weighted avg']['recall'],\n",
        "               \"F1-score\": report['weighted avg']['f1-score'],\n",
        "               \"Support\": report['weighted avg']['support']}\n",
        "    metricas.append(metrics)\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "df_metricas = pd.DataFrame(metricas)\n",
        "\n",
        "# Function to highlight the maximum value in each column\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['background-color: yellow' if v else '' for v in is_max]\n",
        "\n",
        "# Apply the highlighting function\n",
        "df_metricas_styled = df_metricas.style.apply(highlight_max, subset=['Accuracy', 'Precision', 'Recall', 'F1-score'])\n",
        "\n",
        "# Display the styled DataFrame with metrics\n",
        "df_metricas_styled"
      ],
      "metadata": {
        "trusted": true,
        "id": "ovDxK6shVIee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 11 - Conclusion\n",
        "\n",
        "Based on the performance metrics of the different models, here are the key insights:\n",
        "\n",
        "1. **LGBMClassifier** emerges as the best-performing model across all metrics, achieving the highest **Accuracy (92.84%)**, **Precision (92.97%)**, **Recall (92.84%)**, and **F1-score (92.80%)**. This suggests that LightGBM is highly effective for this classification task, possibly due to its ability to handle large datasets efficiently while maintaining high predictive performance.\n",
        "\n",
        "2. **DecisionTreeClassifier** and **RandomForestClassifier** also performed well, with Decision Tree achieving an **Accuracy of 90.88%** and Random Forest reaching **Accuracy of 91.04%**. These models are known for their interpretability and capability of handling nonlinear relationships, which could explain their strong results.\n",
        "\n",
        "3. **LogisticRegression** also performed admirably, with an **Accuracy of 91.45%**, suggesting that even a simpler, linear model can achieve strong performance on this dataset.\n",
        "\n",
        "4. **KNeighborsClassifier** and **GaussianNB** struggled in comparison, with KNeighborsClassifier achieving only **32.82% Accuracy**, and GaussianNB scoring **46.14% Accuracy**. This indicates that these models are less suitable for this specific task, possibly due to their assumptions about data distribution or inability to handle complex relationships in the data.\n",
        "\n",
        "5. **AdaBoostClassifier** and **GradientBoostingClassifier** showed moderate performance, with AdaBoostClassifier achieving an **Accuracy of 79.74%**, and GradientBoostingClassifier at **76.77%**. While they performed better than KNeighbors and GaussianNB, they were outclassed by models like LGBM, Random Forest, and Decision Tree.\n",
        "\n",
        "### Recommendation:\n",
        "- **LightGBM** is the optimal model for this problem, considering its superior performance in all key metrics.\n",
        "- **Random Forest** and **Decision Tree** can also be considered as reliable models, especially if interpretability is a priority.\n",
        "- It's recommended to avoid **KNeighborsClassifier** and **GaussianNB** for this task as their performance is subpar compared to other models.\n",
        "\n",
        "Overall, **LGBMClassifier** should be used in production for this classification task due to its high predictive power, speed, and ability to handle large and complex datasets effectively."
      ],
      "metadata": {
        "id": "Pl_VparJVIee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section B) Generative AI LLM Gemma-2-2b"
      ],
      "metadata": {
        "id": "KJPGFx5_VIee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 12 - Model LLM Gemma"
      ],
      "metadata": {
        "id": "hnhbgND9VIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries natural language processing\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Downloading package nlp punkt\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Authentication with HUGGING FACE\n",
        "import os\n",
        "HUGGING_FACE_ACCESS_TOKEN = os.environ['HUGGING_FACE_ACCESS_TOKEN'] = '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'"
      ],
      "metadata": {
        "trusted": true,
        "id": "m7noW0ZCVIee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer, BertTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Model parameters: specifying the model name to use\n",
        "model_name = 'google/gemma-2-2b-it'\n",
        "\n",
        "# Load the pre-trained Causal Language Model with specific configurations\n",
        "# torch_dtype is set to float16 to optimize GPU memory usage for faster training or inference\n",
        "# The Hugging Face access token is needed for models hosted on private repositories\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # Using half precision for memory efficiency\n",
        "    token=HUGGING_FACE_ACCESS_TOKEN  # Authentication token for Hugging Face if required\n",
        ").to('cuda')  # Moving model to GPU\n",
        "\n",
        "# Check if GPU is available and use it if possible, otherwise, use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the correct device (GPU or CPU)\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rrdV2BO1VIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer with the specified token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_ACCESS_TOKEN)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HtRPs2C_VIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create the prompt with a size limit\n",
        "def criar_prompt(texto):\n",
        "    # Shorten the text to a maximum of 512 characters\n",
        "    texto_curto = texto[:512]\n",
        "\n",
        "    # Create the prompt for sentiment classification\n",
        "    # It asks the model to classify the text as Positive, Negative, or Neutral\n",
        "    prompt = f\"Classify the sentiment of the following text as Positive, Negative, or Neutral:\\n\\n'{texto_curto}'\\n\\nSentiment:\"\n",
        "\n",
        "    # Return the created prompt\n",
        "    return prompt"
      ],
      "metadata": {
        "trusted": true,
        "id": "SsdLuMS1VIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify sentiment with reduced generation time\n",
        "def classificar_sentimento(model, tokenizer, texto):\n",
        "    # Create the prompt using the earlier function\n",
        "    prompt = criar_prompt(texto)\n",
        "\n",
        "    # Tokenize the prompt and move the tensors to the correct device (GPU or CPU)\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate the model output with a reduced max_length for faster inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=inputs['input_ids'].shape[1] + 2)  # Limiting max_length to input length + 2\n",
        "\n",
        "    # Decode the model output to extract the sentiment classification\n",
        "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    sentimento = resposta.split('Sentimento:')[-1].strip()  # Extract the sentiment part from the response\n",
        "    return sentimento\n",
        "\n",
        "# Define the number of epochs\n",
        "EPOCHS = 2\n",
        "\n",
        "# Loop to run sentiment classification over multiple epochs\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    # Apply the sentiment classification function to each review in the DataFrame\n",
        "    df['Predicted_Sentiment'] = df['Cleaned_Review'].apply(lambda x: classificar_sentimento(model, tokenizer, x))\n",
        "\n",
        "    # Display the reviews along with the original and predicted sentiment\n",
        "    print(df[['Cleaned_Review', 'Sentiment', 'Predicted_Sentiment']])\n",
        "\n",
        "\n",
        "# Save dataframe\n",
        "df.to_csv(\"dataset_LLM.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "5jf9yRhaVIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of incorrect values seen previously\n",
        "# We replace truncated values with their correct equivalents\n",
        "sentimento_correcao = {'**__': 'Neutral',\n",
        "                       '**Ne': 'Neutral',\n",
        "                       '**Posi': 'Positive',\n",
        "                       '**Neg': 'Negative',\n",
        "                       '': 'Neutral',   # Assuming empty values are Neutral\n",
        "                       '*': 'Neutral'   # Assuming '*' means Neutral\n",
        "                      }\n",
        "\n",
        "# Correcting the values in the 'Predicted_Sentiment' column\n",
        "df['Predicted_Sentiment'] = df['Predicted_Sentiment'].map(sentimento_correcao)\n",
        "\n",
        "# Check the corrected values\n",
        "print(\"Corrected values in 'Predicted_Sentiment':\")\n",
        "print(df['Predicted_Sentiment'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "439YolvRVIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTjsIIPCVIeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}