{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoria técnica Data Science\n",
    "\n",
    "1. [Introducción](#introducción)\n",
    "2. [Problemática](#problemática)\n",
    "3. [Propuesta](#propuesta)\n",
    "4. [BBDD SQL Relacional (PostgreSQL)](#bbdd-sql-relacional-postgresql)\n",
    "5. [Automatismos](#automatismos)\n",
    "   - 5.1 [Web Scraping (Selenium)](#web-scraping-selenium)\n",
    "   - 5.2 [Tratamiento de imágenes y PDF (OpenCV, PyMuPDF, Regex)](#scrapeo-de-pdf-e-imágenes-opencv-pymupdf-regex)\n",
    "6. [Modelo de Machine Learning](#modelo-de-machine-learning)\n",
    "7. [Reporte Analítico](#reporte-analítico)\n",
    "8. [Librerías](#librerias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "Esta memoria representa el guión principal e hilo conductor del trabajo realizado por los compañeros de Data Science 2309 del grupo 3 del desafío de tripulaciones Javier de Alcazar, Maria Neches, Hugo Martín y Alejandro Campos, siendo tan solo una de las piezas que encajan en la propuesta integrada diseñada para solventar la necesidad y la propuesta realizada por Several Energy, la cual propone una actualización y mejoría de su sistema y flujo internos de trabajo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Análisis detallado del problema.\n",
    "\n",
    "Se nos presenta la necesidad de crear una WebApp que sustituya, y amplie, las funciones de una tabla de excel.\n",
    "Las deficiencias de la manera de trabajo actual son varias:\n",
    "\n",
    "+ Mala escalabilidad. La empresa ha comenzado a tener problemas a la hora de usarla en diferentes situaciones.\n",
    "+ Poco control del trabajo realizado. Enfarragosa manera de guardar historiales.\n",
    "+ Exceso de trabajo por parte del asesor a la hora de introducir datos, provocando un largo tiempo de producción en la generación de propuestas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Propuesta de solución tecnológica\n",
    "\n",
    "La tarea de mejorar el trabajo realizado por una hoja excell con una WebApp plantea la oportunidad de usar una BBDD relacional, que acabe con los problemas de escalabilidad y registro de históricos. A ello añadimos la posibilidad de obtener datos de forma automatizada, aliviando la carga de trabajo del asesor.\n",
    "\n",
    "+ BBDD SQL Relacional (PostgreSQL)\n",
    "+ Webscrapping (Selenium)\n",
    "+ Scrapeo de PDF e imágenes (OpenCV, PyMuPDF, Regex)\n",
    "+ Modelo de predicción \n",
    "+ Dashboard (PowerBi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.BBDD SQL\n",
    "\n",
    "Una vez analizadas las necesidades de la WebApp para solucionar los problemas actuales, se diseña la BBDD que cumpla las funciones necesarias para el proyecto.\n",
    "Se trata de una BBDD relacional en PostreSQL 15.0, preparada para alojarse en la nube Azure. Se realizan los planos necesarios para el manejo de la misma un modelo lógico y un diagrama entidad relación conteniendo 12 entidades y 5 relaciones. [Link a la query de creación](/database/sql/create_tables.sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura\n",
    "Esquema de la BBDD.\n",
    "\n",
    "<img src=\"../aux_temp/Esquema.png\" width=\"1900\" />\n",
    "\n",
    "[Link para ampliar](https://www.canva.com/design/DAF5e_nv_Bk/PNXGmx8l0Xajcoh2LPONFQ/edit?utm_content=DAF5e_nv_Bk&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)\n",
    "\n",
    "### Relación\n",
    "Diagrama de relación.\n",
    "\n",
    "<img src=\"../aux_temp/Diagrama.png\" width=\"1900\" />\n",
    "\n",
    "[Link para ampliar](https://www.canva.com/design/DAFy6dl3Pe4/yYekcQiBpDDn7MCyx_-qiw/edit?utm_content=DAFy6dl3Pe4&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Automatismos\n",
    "\n",
    "El flujo del trabajo manual del asesor actual tiene dos tediosas tareas en tiempo que son candidatas a ser sustituidas por automatismos.\n",
    "Se trata, por un lado,de la obtención de datos desde una intranet a la que se accede vía web, y con un código \"CUPs\" se extraen datos. Y por otro lado, la lectura personal de facturas para obtener datos de consumo y precios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1WebScraping\n",
    "\n",
    "Mediante el uso de la librería Selenium, creamos un script que accede a la web de Candela energía para obtener la información necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraping_chrome_candelas(cups):\n",
    "\n",
    "    path_driver = os.getcwd() + \"\\..\\webscraping\\chromedriver-win64\\chromedriver.exe\"\n",
    "    # Create driver Chrome\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "    servicio = Service(path_driver)\n",
    "    driver = webdriver.Chrome(service=servicio, options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    assert \"Candela\"\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Login in candelas web\n",
    "\n",
    "    driver.find_element(By.ID, \"select_1\").click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.ID, \"select_option_3\").click()\n",
    "    driver.find_element(By.NAME, \"usuario\").send_keys(USER)\n",
    "    driver.find_element(By.NAME, \"password\").send_keys(PASSWORD)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/form/button').click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Download info\n",
    "\n",
    "    driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[1]/div[1]/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.ID, \"input_6\").send_keys(cups)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[2]/md-tabs/md-tabs-content-wrapper/md-tab-content/div[1]/md-card/div[1]/form/div[4]/button').click()\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[2]/md-tabs/md-tabs-content-wrapper/md-tab-content/div[1]/md-content/md-card/md-toolbar/div[1]/button[1]').click()\n",
    "\n",
    "    info = get_soup_info(driver)\n",
    "\n",
    "    # Close driver\n",
    "    driver.quit()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcion genera un retorno en formato json con la tarifa y los consumos y potencia anuales a las cuales podemos acceder mediante las credenciales de several energy en la web de candela energía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'rate': '2.0TD',\n",
    " 'anual_consumption': '2.103 KWh',\n",
    " 'anual_consumption_p1': '502 KWh',\n",
    " 'anual_consumption_p2': '472 KWh',\n",
    " 'anual_consumption_p3': '1129 KWh',\n",
    " 'anual_consumption_p4': '0 KWh',\n",
    " 'anual_consumption_p5': '0 KWh',\n",
    " 'anual_consumption_p6': '0 KWh',\n",
    " 'anual_power_p1': '4,6',\n",
    " 'anual_power_p2': '4,6',\n",
    " 'anual_power_p3': '',\n",
    " 'anual_power_p4': '',\n",
    " 'anual_power_p5': '',\n",
    " 'anual_power_p6': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de imágenes y PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción y autorellenado de datos se afianzó como parte clave de la mejora en el proceso de creación de propuestas. Tras el estudio con cliente sabemos que las facturas no les llegan todas en el mismo formato y de una manera standar si no que pueden ser desde PDF's hasta capturas de pantalla enviadas por whatsapp o fotos a documentos o pantallas.\n",
    "\n",
    "Esto nos suponía un reto y la realización de 2 maneras de tratar esto. La idea es que tanto de los PDF's (de manera más sencilla) como en formato imagen JPG o PNG se extraiga todo el texto a texto plano y con regex se extraiga la información de los campos concretos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizaron un repositorio de diferentes funciones con varias librerias para ver de que manera podíamos extraer mejor los datos. Destacable el uso de la libreria langchain y la API de Open AI para usarlo como intérprete de las facturas, finalmente fue desestimado por que con regex se podría extraer los datos de igual o mejor manera y reduciamos los costes y el tiempo, al evitar una llamada a una API y estar haciendose en \"local\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tres son las tareas a realizar en este apartado.\n",
    "\n",
    "1. Conversión de PDF a texto\n",
    "2. Conversión de Imagen a texto\n",
    "3. Scrapeo de Texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "p_counter_kW=0\n",
    "p_counter_kWh=0\n",
    "\n",
    "def create_qa_chain(): \n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=OPENAI_API_KEY)\n",
    "    qa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n",
    "    qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
    "\n",
    "    return qa_document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf (file-like object): PDF file object.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content from the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf.read(), filetype=\"pdf\")\n",
    "        full_text = \"\"\n",
    "\n",
    "        for page_num in range(doc.page_count - 1):\n",
    "            page = doc[page_num]\n",
    "            page_text = page.get_text()\n",
    "            full_text += page_text\n",
    "\n",
    "        with open('data/pdf/invoice.pdf', 'wb') as output_pdf:\n",
    "            pdf.seek(0)\n",
    "            output_pdf.write(pdf.read())\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_to_txt(text, txt_path):\n",
    "    \"\"\"\n",
    "    Save text content to a text file.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text content to be saved.\n",
    "        txt_path (str): Path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.dirname(txt_path)):\n",
    "        try:\n",
    "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(text)\n",
    "            print(f'Archivo guardado exitosamente en: {txt_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error al guardar el archivo: {e}')\n",
    "    else:\n",
    "        print(f'La ruta especificada no existe: {os.path.dirname(txt_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_question_langchain(qa_document_chain, question):\n",
    "    \"\"\"\n",
    "    Get responses to a question from a question-answering document chain.\n",
    "\n",
    "    Args:\n",
    "        qa_document_chain (AnalyzeDocumentChain): Question-answering document chain.\n",
    "        question (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the question, responses, and any errors.\n",
    "    \"\"\"\n",
    "    fragment_size = 4096\n",
    "    all_responses= {\"question\": [],\"response\" : [], \"error\": []}\n",
    "    with open(\"data/txt/invoice.txt\", 'r', encoding='utf-8') as file:\n",
    "        while True:\n",
    "            part = file.read(fragment_size)\n",
    "            if not part:\n",
    "                break\n",
    "            try:\n",
    "                response = qa_document_chain.run(\n",
    "                    input_document=part,\n",
    "                    question=question,\n",
    "                )\n",
    "                all_responses[\"question\"].append(question)\n",
    "                all_responses[\"response\"].append(response)\n",
    "            except Exception as e:\n",
    "                all_responses[\"error\"].append(str(e))\n",
    "    all_responses[\"question\"] = all_responses[\"question\"][0]\n",
    "    return all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoice_clean_data(response):\n",
    "    \"\"\"\n",
    "    Clean and process responses obtained from a question-answering task.\n",
    "\n",
    "    Args:\n",
    "        response (dict): Original responses containing question, responses, and errors.\n",
    "\n",
    "    Returns:\n",
    "        dict: Cleaned responses with irrelevant answers replaced and numeric values extracted.\n",
    "    \"\"\"\n",
    "    clean_response = copy.deepcopy(response)\n",
    "    float_patron = r'\\b\\d+[.,]\\d+\\b'\n",
    "    not_answer = [\"lo siento\",\"no se\", \"no puedo\", \"no se menciona\"]\n",
    "    for i,r in enumerate(response[\"response\"]):\n",
    "        if any(word.lower() in r.lower() for word in not_answer):\n",
    "            clean_response[\"response\"][i] = \" \"\n",
    "        else:\n",
    "            result = re.findall(float_patron,r)\n",
    "            clean_response[\"response\"][i] = result\n",
    "    return clean_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(pdf_data):\n",
    "    \"\"\"\n",
    "    Upload a PDF file, extract text, and save it to a text file.\n",
    "\n",
    "    Args:\n",
    "        pdf_data (file-like object): PDF file object.\n",
    "\n",
    "    Returns:\n",
    "        dict: Response indicating the success or failure of the operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        path_txt = \"data\\\\txt\\\\invoice.txt\"\n",
    "\n",
    "        # read PDF\n",
    "        print(pdf_data)\n",
    "        pdf_txt = extract_text_from_pdf(pdf_data)\n",
    "        save_text_to_txt(pdf_txt, path_txt)\n",
    "\n",
    "        return {'response': \"Se ha subido correctamente el pdf\"}\n",
    "    except Exception as e:\n",
    "        return {'error': f\"Error al subir el pdf: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link():\n",
    "    \"\"\"\n",
    "    Extract a link (URL) from a PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted link or None if no link is found.\n",
    "    \"\"\"\n",
    "    doc = fitz.open('data/pdf/invoice.pdf')\n",
    "\n",
    "    for pages_num in range(doc.page_count):\n",
    "        page = doc[pages_num]\n",
    "        enlaces = page.get_links()\n",
    "\n",
    "        for enlace in enlaces:\n",
    "            url = enlace.get('uri')\n",
    "            if url:\n",
    "                return url\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_info(link):\n",
    "\n",
    "    link_info = {}\n",
    "    cups_pattern = re.compile(r'cups=[A-Z0-9]+')\n",
    "    matches = cups_pattern.findall(link)\n",
    "\n",
    "    cleaned_matches = [''.join(match.split()) for match in matches]\n",
    "\n",
    "    link_info[\"cups20\"] = cleaned_matches[0][5:]\n",
    "    peak_regex = r'pP1=([0-9]+(?:\\.[0-9]+)?)'\n",
    "    link_info[\"peak_power\"] = re.findall(peak_regex, link)[0].replace(\"pP1=\", \"\")\n",
    "    valley_regex = r'pP2=([0-9]+(?:\\.[0-9]+)?)'\n",
    "    link_info[\"valley_power\"] = re.findall(valley_regex, link)[0].replace(\"pP2=\", \"\")\n",
    "    flat_regex = r'pP3=([0-9]+(?:\\.[0-9]+)?)'\n",
    "    if re.findall(flat_regex, link):\n",
    "        link_info[\"flat_power\"] = re.findall(flat_regex, link)[0].replace(\"pP3=\", \"\")\n",
    "    sd_regex = r'iniF=([0-9]{4}-[0-9]{2}-[0-9]{2})'\n",
    "    link_info[\"start_date\"] = re.findall(sd_regex, link)[0].replace(\"iniF=\", \"\")\n",
    "    ed_regex = r'finF=([0-9]{4}-[0-9]{2}-[0-9]{2})'\n",
    "    link_info[\"end_date\"] = re.findall(ed_regex, link)[0].replace(\"finF=\", \"\") \n",
    "    id_regex = r'fFact=([0-9]{4}-[0-9]{2}-[0-9]{2})'\n",
    "    link_info[\"invoice_date\"] = re.findall(id_regex, link)[0].replace(\"fFact=\", \"\")\n",
    "\n",
    "    return link_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_txt(link):\n",
    "    \"\"\"\n",
    "    Extract the number of days and iva from a text file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The extracted number of days or None if no match is found and iva or None.\n",
    "    \"\"\"\n",
    "    info={}\n",
    "    with open(\"data/txt/invoice.txt\", 'r', encoding='utf-8') as file:\n",
    "        file_txt = file.read()\n",
    "\n",
    "    cups_pattern = re.compile(r'\\b\\d+\\s*(?=\\bdías\\b)')\n",
    "    info[\"days_invoice\"] = cups_pattern.findall(file_txt)[0].replace(\" \", \"\")\n",
    "    percentage_pattern = r'\\b(\\d+(?:[.,]\\d+)?)%\\b'\n",
    "    info[\"iva\"] = re.findall(percentage_pattern, file_txt)[0].replace(\",\", \".\")\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_days():\n",
    "    \"\"\"\n",
    "    Extract the number of days from a text file.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted number of days or None if no match is found.\n",
    "    \"\"\"\n",
    "    with open(\"data/txt/invoice.txt\", 'r', encoding='utf-8') as file:\n",
    "        file_txt = file.read()\n",
    "    cups_pattern = re.compile(r'\\b\\d+\\s*(?=\\bdías\\b)')\n",
    "    matches = cups_pattern.findall(file_txt)\n",
    "\n",
    "    if matches:\n",
    "        return matches[0].replace(\" \", \"\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_text(res_img):\n",
    "    \"\"\"\n",
    "    Convert an image to text using OCR and save the text to a text file.\n",
    "\n",
    "    Args:\n",
    "        res_img: Image data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Response indicating the success or failure of the operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_path = \"data\\\\txt\\\\invoice.txt\"\n",
    "        save_txt = \"\"\n",
    "\n",
    "        temp_name = f\"temp_image.png\"\n",
    "        temp_path = os.path.join(\"data\\\\image\", temp_name)\n",
    "        res_img.save(temp_path)\n",
    "\n",
    "        img = cv.imread(temp_path)\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        reader = easyocr.Reader(['es'])\n",
    "        img_txt = reader.readtext(gray)\n",
    "\n",
    "        for n in img_txt:\n",
    "            save_txt += n[1] + \" \"\n",
    "\n",
    "        save_text_to_txt(save_txt, img_path)\n",
    "\n",
    "        return {'response': \"Se ha subido la imagen\"}\n",
    "    except Exception as e:\n",
    "        return {'error': f\"Error al subir la imagen: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prices_invoice():\n",
    "    patron = re.compile(r'\\b\\d+\\,\\d{6}\\b')\n",
    "    with open(\"data/txt/invoice.txt\", 'r', encoding='utf-8') as txt_file:\n",
    "        txt_file = txt_file.read()\n",
    "\n",
    "    matches = patron.findall(txt_file)\n",
    "\n",
    "    cleaned_matches = [''.join(match.split()) for match in matches]\n",
    "\n",
    "    patron = r'€/kWh|€/kW'\n",
    "    measured = re.findall(patron, txt_file)\n",
    "    return measured,cleaned_matches\n",
    "\n",
    "def df_create(measured, cleaned_matches):\n",
    "    data = {'precios': cleaned_matches, 'unidades': measured}\n",
    "    df = pd.DataFrame.from_dict(data, orient='index').transpose()\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_p_values(df):\n",
    "    global p_counter_kW, p_counter_kWh\n",
    "    \n",
    "    if df['unidades'] == '€/kW':\n",
    "        p_counter_kW += 1\n",
    "        return f'p{p_counter_kW}'\n",
    "    elif df['unidades'] == '€/kWh':\n",
    "        p_counter_kWh += 1\n",
    "        return f'p{p_counter_kWh}'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_prices(df):\n",
    "    pdf_scarp_info = {\n",
    "    \"p1_price_kw\": [],\n",
    "    \"p2_price_kw\": [],\n",
    "    \"p3_price_kw\": [],\n",
    "    \"p4_price_kw\": [],\n",
    "    \"p5_price_kw\": [],\n",
    "    \"p6_price_kw\": [],\n",
    "    \"p1_price_kwh\": [],\n",
    "    \"p2_price_kwh\": [],\n",
    "    \"p3_price_kwh\": [],\n",
    "    \"p4_price_kwh\": [],\n",
    "    \"p5_price_kwh\": [],\n",
    "    \"p6_price_kwh\": [],\n",
    "        }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        price_type = f\"{row['P_values']}_price_{row['unidades'][2:].lower()}\"\n",
    "        pdf_scarp_info[price_type].append(row['precios'])\n",
    "    \n",
    "    return pdf_scarp_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Several Energy se propone la elaboración de un modelo de machine learning para establecer una predicción del precio del mercado de la luz en España. \n",
    "\n",
    "Este reto lo afrontamos haciendo primero una búsqueda del histórico de datos del precio de la luz. Finalmente recurrimos a www.omie.es de donde obtenemos un dataset con los precios por hora desde el 01/01/1998 hasta el 31/12/2023 , 25 años de datos listos para ser tratados.\n",
    "\n",
    "Todo apuntaba a que podíamos enfocar el modelo de machine learning desde el punto de vista de una Serie temporal , empezamos a trababajar los datos para pasar de 24 feateures a 1 para usarla como serie , tratamos diferentes formas estadísticas de establecer esa única variable, viendo la desviación estandar de cada día, la media y precios máximos y minimos. Finalmente decidiamos que la media del precio diario seria la variable elegida para trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Estacionalidad](../aux_temp/estacionalidad_mercado_diario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribución de la serie temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Distribución](../aux_temp/distribucion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Residuos](../aux_temp/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de autocorrelación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Autocorrelación](../aux_temp/autocorrelacion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultado de la predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![intento](../aux_temp/intento_35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reporte analítico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de reporte analíco nos llevaba a ofrecer visualizaciones en tiempo real sobre las ventas realizadas y todos los KPIs (Key perfomance indicators) de ventas de los asesores. Nos enfrentabamos a que por razones de seguridad el cliente no podia darnos acceso al ERP para obtener acceso de los datos de ventas por lo que solo podríamos en el mejor de los casos tener datos de la parte previa a la venta, es decir el análisis del cliente, los datos de la factura,la propuesta y del asesor, para poder hacer un dashboard de visualizaciones y demostrar sus funcionalidades hemos creado un dataset falso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe title=\"Dashboard Several Energy\" width=\"1140\" height=\"541.25\" src=\"https://app.powerbi.com/reportEmbed?reportId=fa721417-ad80-425e-bc24-96b7a04ea0b1&autoAuth=true&ctid=bf86fbdb-f8c2-440e-923c-05a60dc2bc9b\" frameborder=\"0\" allowFullScreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Librerías usadas e importaciones\n",
    "\n",
    "## -----Web Development -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_limiter import Limiter\n",
    "from flask_cors import CORS, cross_origin\n",
    "from cerberus import Validator\n",
    "from werkzeug.serving import make_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- General Utilities -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import environ\n",
    "import sys\n",
    "import signal\n",
    "import threading\n",
    "from queue import Queue\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Custom Functions -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Computer Vision and OCR -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Database Interaction -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Web Scraping -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Document Analysis -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Time Series Analysis -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima, ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Machine Learning and Data Analysis -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Data Visualization -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- Streamlit and Data Handling -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import streamlit.components.v1 as c\n",
    "import requests\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
