{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A0N0J0A0L0I/Capstone-project-2/blob/main/Another_copy_of_Sample_ML_Submission_Template_ipynb_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**- Netflix Movies & TV Shows Clustering  \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Team\n",
        "##### Team Member 1 - Janhavi Pramod Jadhav\n",
        "##### Team Member 2 - Mansi Pravin Patil\n",
        "##### Team Member 3 - Anjali Pravin Desale\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Summary -\n",
        "The aim of this project is to enhance content discovery on Netflix by developing a clustering model that groups movies and TV shows based on various attributes. By doing so, we intend to improve the user experience, making it easier for viewers to find content that aligns with their preferences.\n",
        "\n",
        "To start, data will be collected from publicly available sources, specifically focusing on Netflix’s extensive library of movies and TV shows. This includes metadata such as titles, genres, cast members, directors, release years, runtimes, ratings, and user reviews. The primary dataset for this project is the Netflix Movies and TV Shows dataset from Kaggle, which consists of 7,787 rows and 12 columns, providing a comprehensive overview of Netflix's content.\n",
        "\n",
        "The initial phase involves thorough data preprocessing to clean and standardize the data, addressing missing values and inconsistencies, and removing any irrelevant information. This step is crucial to prepare the data for effective clustering. Feature engineering is then undertaken to extract relevant features that capture the essence of each movie or TV show. Techniques such as natural language processing (NLP) will be employed to analyze textual data, including descriptions and reviews, thereby providing deeper insights into the content.\n",
        "\n",
        "With the preprocessed data and engineered features, various clustering algorithms will be applied to group the movies and TV shows. Algorithms such as K-means, hierarchical clustering, and DBSCAN will be evaluated and compared using metrics like the silhouette score and Davies-Bouldin index. These metrics help determine the most effective clustering approach. Both quantitative and qualitative validation will ensure that the clusters are not only mathematically sound but also meaningful and useful to users."
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/A0N0J0A0L0I/Capstone-project-2/blob/main/Another_copy_of_Sample_ML_Submission_Template_ipynb_3.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement** :-\n",
        " My task is to make a model that can cluster similar type of content together.\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# libraries used to process textual data\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#load a dataset into a pandas Dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "to-uZ8ndtw6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Rows and Column count in the Dataset: Rows= {df.shape[0]}, Columns= {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "QRy8MYWp1fA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "ptwPEqRo1RFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"The total number of duplicated observations in the dataset: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "8ibLPOA08-ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Percentage of null values in each category\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "0xKylWaf-Ohl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(df.isnull(), cbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IAXcyfW--e0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,8))\n",
        "plots= sns.barplot(x=df.columns,y=df.isna().sum())\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "for bar in plots.patches:\n",
        "      plots.annotate(bar.get_height(),\n",
        "                     (bar.get_x() + bar.get_width() / 2,\n",
        "                      bar.get_height()), ha='center', va='center',\n",
        "                     size=12, xytext=(0, 8),\n",
        "                     textcoords='offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BUFLu5Ir-tLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "•The dataset contains 12 columns and 7787 rows. The columns include various\n",
        "\n",
        "•Attributes related to movies and TV shows, such as show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, and description.\n",
        "\n",
        "•The dataset provides information about various movies and TV shows, including their genres, ratings, durations, and availability on Netflix. The genre_ids column contains the IDs of the genres associated with each movie or TV show, while the genres column contains the names of the genres. The rating column contains the rating of each movie or TV show, and the rating_img column contains the corresponding rating image.\n",
        "\n",
        "•The duration column contains the duration of each movie or TV show in the format of \"hh:mm\", while the duration_minutes column contains the duration in minutes. The listed_in column contains the categories that each movie or TV show belongs to, and the description column contains a brief description of each movie or TV show.\n",
        "\n",
        "•The dataset also includes various columns related to the availability of each movie or TV show on Netflix, such as availability, is_new, is_blockbuster, is_popular, is_trending, is_holiday, is_kids, is_original, and their corresponding URLs.\n",
        "\n",
        "•This dataset can be used for various data analysis tasks, such as finding the most popular genres, analyzing the distribution of ratings, or exploring the relationship between the duration and popularity of movies and TV shows. For example, you could use data visualization techniques to show the distribution of ratings for different genres or analyze the relationship between the duration of a movie and its popularity. Additionally, you could use text analysis techniques to analyze the descriptions of the movies and TV shows to identify common themes or trends."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f\"Available columns:\\n{df.columns.to_list()}\")"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• show_id: a unique identifier for each movie or TV show\n",
        "\n",
        "• type: the type of media (movie or TV show)\n",
        "\n",
        "• title: the title of the movie or TV show\n",
        "\n",
        "• director: the director of the movie or TV show\n",
        "\n",
        "• cast: the main actors or actresses in the movie or TV show\n",
        "\n",
        "• country: the country of origin of the movie or TV show\n",
        "\n",
        "• date_added: the date the movie or TV show was added to the Netflix catalog\n",
        "\n",
        "• release_year: the year the movie or TV show was released\n",
        "\n",
        "• rating: the rating of the movie or TV show (e.g., TV-MA, PG-13)\n",
        "\n",
        "• duration: the duration of the movie or TV show (e.g., 93 min, 4 Seasons)\n",
        "\n",
        "• listed_in: the categories that the movie or TV show belongs to (e.g.,  \n",
        "International TV Shows, TV Dramas, TV Sci-Fi & Fantasy)\n",
        "\n",
        "• description: a brief description of the movie or TV show\n",
        "\n",
        "• genre_ids: the IDs of the genres associated with each movie or TV show\n",
        "\n",
        "• genres: the names of the genres associated with each movie or TV show\n",
        "\n",
        "• rating_img: the rating image associated with each movie or TV show\n",
        "\n",
        "• duration_minutes: the duration of each movie or TV show in minutes\n",
        "\n",
        "•availability: the availability of each movie or TV show on Netflix\n",
        "\n",
        "• is_new: a flag indicating whether the movie or TV show is new\n",
        "\n",
        "• is_blockbuster: a flag indicating whether the movie or TV show is a\n",
        "blockbuster\n",
        "\n",
        "• is_popular: a flag indicating whether the movie or TV show is popular\n",
        "\n",
        "• is_trending: a flag indicating whether the movie or TV show is trending\n",
        "\n",
        "• is_holiday: a flag indicating whether the movie or TV show is a holiday movie\n",
        "\n",
        "• is_kids: a flag indicating whether the movie or TV show is for kidsAnswer  Here\n",
        "\n",
        "• is_original: a flag indicating whether the movie or TV show is an original Netflix production\n",
        "\n",
        "• url: the URL of the movie or TV show on Netflix\n",
        "\n",
        "The genre_ids and genres columns contain information about the genres associated with each movie or TV show. The genre_ids column contains the IDs of the genres, while the genres column contains the names of the genres. The rating_img column contains the rating image associated with each movie or TV show.\n",
        "The duration column contains the duration of each movie or TV show in the format of \"hh:mm\" for movies and \"Seasons\" for TV shows. The duration_minutes column contains the duration in minutes.\n",
        "The availability column contains information about the availability of each movie or TV show on Netflix, and the is_* columns contain flags indicating various properties of the movie or TV show. The url column contains the URL of the movie or TV show on Netflix."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(f\"The number of unique values in: \")\n",
        "print(\"-\"*35)\n",
        "for i in df.columns:\n",
        "  print(f\"'{i}' : {df[i].nunique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Handling Null values from each feature**"
      ],
      "metadata": {
        "id": "ky7MjvZnB4Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Let's find out the percentage of null values in each category in order to deal with it.\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"date_added\"].value_counts()"
      ],
      "metadata": {
        "id": "efPkbm2gBM0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].value_counts()"
      ],
      "metadata": {
        "id": "zsZrjezhBZaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['country'].value_counts()"
      ],
      "metadata": {
        "id": "kn5P_UYyBauN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Imputing null value as per our discussion\n",
        "# imputing with unknown in null values of director and cast feature\n",
        "df[['director','cast']]=df[['director','cast']].fillna(\"Unknown\")\n",
        "\n",
        "# Imputing null values of country with Mode\n",
        "df['country']=df['country'].fillna(df['country'].mode()[0])\n",
        "\n",
        "# Dropping remaining null values of date_added and rating\n",
        "df.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "leoP63kuD_9p",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking the Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(\"Null value count in each of the variable: \")\n",
        "print(\"-\"*50)\n",
        "print(df.isna().sum())\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Rechecking the percentage of null values in each category\n",
        "print(\"Percentage of null values in each variable: \")\n",
        "print(\"-\"*50)\n",
        "null_count_by_variable = df.isnull().sum()/len(df)\n",
        "print(f\"{null_count_by_variable*100}%\")\n",
        "print(\"-\"*50)"
      ],
      "metadata": {
        "id": "q7l1Q8ShE1w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Handling nested columns i.e 'director', 'cast', 'listed_in' and 'country'**"
      ],
      "metadata": {
        "id": "3KBsymMIFxr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a copy of dataframe and unnest the original one\n",
        "df_new= df.copy()"
      ],
      "metadata": {
        "id": "uobVx8xnF2iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'Directors' column\n",
        "dir_constraint=df['director'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df1 = pd.DataFrame(dir_constraint, index = df['title'])\n",
        "df1 = df1.stack()\n",
        "df1 = pd.DataFrame(df1.reset_index())\n",
        "df1.rename(columns={0:'Directors'},inplace=True)\n",
        "df1 = df1.drop(['level_1'],axis=1)\n",
        "df1.sample(10)"
      ],
      "metadata": {
        "id": "k9qVVQiTGBWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'cast' column\n",
        "cast_constraint=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df2 = pd.DataFrame(cast_constraint, index = df['title'])\n",
        "df2 = df2.stack()\n",
        "df2 = pd.DataFrame(df2.reset_index())\n",
        "df2.rename(columns={0:'Actors'},inplace=True)\n",
        "df2 = df2.drop(['level_1'],axis=1)\n",
        "df2.sample(10)"
      ],
      "metadata": {
        "id": "tLV4W0EyGZvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'listed_in' column\n",
        "listed_constraint=df['listed_in'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df3 = pd.DataFrame(listed_constraint, index = df['title'])\n",
        "df3 = df3.stack()\n",
        "df3 = pd.DataFrame(df3.reset_index())\n",
        "df3.rename(columns={0:'Genre'},inplace=True)\n",
        "df3 = df3.drop(['level_1'],axis=1)\n",
        "df3.sample(10)"
      ],
      "metadata": {
        "id": "Xs46BYSFGlZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unnesting 'country' column\n",
        "country_constraint=df['country'].apply(lambda x: str(x).split(', ')).tolist()\n",
        "df4 = pd.DataFrame(country_constraint, index = df['title'])\n",
        "df4 = df4.stack()\n",
        "df4 = pd.DataFrame(df4.reset_index())\n",
        "df4.rename(columns={0:'Country'},inplace=True)\n",
        "df4 = df4.drop(['level_1'],axis=1)\n",
        "df4.sample(10)"
      ],
      "metadata": {
        "id": "BFfBWR6RGt-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merging all the unnested dataframes**"
      ],
      "metadata": {
        "id": "5uBblSofHC8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Merging all the unnested dataframes\n",
        "# Merging director and cast\n",
        "df5 = df2.merge(df1,on=['title'],how='inner')\n",
        "\n",
        "# Merging listed_in with merged of (director and cast)\n",
        "df6 = df5.merge(df3,on=['title'],how='inner')\n",
        "\n",
        "# Merging country with merged of [listed_in with merged of (director and cast)]\n",
        "df7 = df6.merge(df4,on=['title'],how='inner')\n",
        "\n",
        "# Head of final merged dataframe\n",
        "df7.head()"
      ],
      "metadata": {
        "id": "uL_sl6MwHKhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging unnested data with the created dataframe in order to make the final dataframe\n",
        "df = df7.merge(df[['type', 'title', 'date_added', 'release_year', 'rating', 'duration','description']],on=['title'],how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Q44oJ1VqHOUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset before typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "P5zFTnaPsDOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking info of the dataset after typecasting\n",
        "df.info()"
      ],
      "metadata": {
        "id": "xxLn86LnY_xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Assuming the correct column name is 'date_added' or similar\n",
        "df['rating'] = df['rating'].str.strip()  # Strip whitespaces\n",
        "\n",
        "# Convert the string to datetime\n",
        "df['rating'] = pd.to_datetime(df['rating'], format='mixed', errors='coerce')\n",
        "\n",
        "# Extract day, month, and year\n",
        "df[\"day_added\"] = df[\"rating\"].dt.day\n",
        "df[\"month_added\"] = df[\"rating\"].dt.month\n",
        "df[\"year_added\"] = df[\"rating\"].dt.year\n",
        "\n",
        "# Dropping the 'date_added' column\n",
        "df.drop('rating', axis=1, inplace=True)\n",
        "\n",
        "# Display the first few rows to confirm\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "j4j-lJTncdRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Binning of Rating attribute**"
      ],
      "metadata": {
        "id": "YNS43foBNZFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In rating columns we have different categories these are content rating classifications that are commonly used in the United States and other countries to indicate the appropriateness of media content for different age groups. Let's understand each of them and binnig them accordingly:\n",
        "\n",
        "TV-MA: This rating is used for mature audiences only, and it may contain strong language, violence, nudity, and sexual content.\n",
        "\n",
        "R: This rating is used for movies that are intended for audiences 17 and older. It may contain graphic violence, strong language, drug use, and sexual content.\n",
        "\n",
        "PG-13: This rating is used for movies that may not be suitable for children under 13. It may contain violence, mild to moderate language, and suggestive content.\n",
        "\n",
        "TV-14: This rating is used for TV shows that may not be suitable for children under 14. It may contain violence, strong language, sexual situations, and suggestive dialogue.\n",
        "\n",
        "TV-PG: This rating is used for TV shows that may not be suitable for children under 8. It may contain mild violence, language, and suggestive content.\n",
        "\n",
        "NR: This stands for \"Not Rated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.\n",
        "\n",
        "TV-G: This rating is used for TV shows that are suitable for all ages. It may contain some mild violence, language, and suggestive content.\n",
        "\n",
        "TV-Y: This rating is used for children's TV shows that are suitable for all ages. It is intended to be appropriate for preschool children.\n",
        "\n",
        "TV-Y7: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain mild violence and scary content.\n",
        "\n",
        "PG: This rating is used for movies that may not be suitable for children under 10. It may contain mild language, some violence, and some suggestive content.\n",
        "\n",
        "G: This rating is used for movies that are suitable for general audiences. It may contain some mild language and some violence.\n",
        "\n",
        "NC-17: This rating is used for movies that are intended for adults only. It may contain explicit sexual content, violence, and language.\n",
        "\n",
        "TV-Y7-FV: This rating is used for children's TV shows that may not be suitable for children under 7. It may contain fantasy violence.\n",
        "\n",
        "UR: This stands for \"Unrated.\" It means that the content has not been rated by a rating board, and it may contain material that is not suitable for all audiences.                                                                                                                                 Let's not complicate it and create bins as following:\n",
        "\n",
        "Adult Content: TV-MA, NC-17, R\n",
        "\n",
        "Children Content: TV-PG, PG, TV-G, G\n",
        "\n",
        "Teen Content: PG-13, TV-14\n",
        "\n",
        "Family-friendly Content: TV-Y, TV-Y7, TV-Y7-FV\n",
        "\n",
        "Not Rated: NR, UR"
      ],
      "metadata": {
        "id": "PRtnLQgLNs2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning the values in the rating column\n",
        "Country_map = {'TV-MA': 'Adult Content',\n",
        "              'R': 'Adult Content',\n",
        "              'PG-13': 'Teen Content',\n",
        "              'TV-14': 'Teen Content',\n",
        "              'TV-PG': 'Children Content',\n",
        "              'NR': 'Not Rated',\n",
        "              'TV-G': 'Children Content',\n",
        "              'TV-Y': 'Family-friendly Content',\n",
        "              'TV-Y7': 'Family-friendly Content',\n",
        "              'PG': 'Children Content',\n",
        "              'G': 'Children Content',\n",
        "              'NC-17': 'Adult Content',\n",
        "              'TV-Y7-FV': 'Family-friendly Content',\n",
        "              'UR': 'Not Rated'}\n",
        "\n",
        "df['Country'].replace(Country_map, inplace=True)\n",
        "print(df['Country'].unique())\n"
      ],
      "metadata": {
        "id": "Tl0cOx7zN07t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking head after binning\n",
        "df.head()"
      ],
      "metadata": {
        "id": "p2FE39cPN91_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Separating Movies and TV Shows**"
      ],
      "metadata": {
        "id": "U1IvFEDbOI5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearating the dataframes for further analysis\n",
        "df_movies= df[df['type']== 'Movie']\n",
        "df_tvshows= df[df['type']== 'TV Show']\n",
        "\n",
        "# Printing the shape\n",
        "print(df_movies.shape, df_tvshows.shape)"
      ],
      "metadata": {
        "id": "kfGoBLW5ONUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Loading Data into a DataFrame:***\n",
        "\n",
        "The data is read from a string using StringIO and loaded into a Pandas DataFrame.\n",
        "\n",
        "**• Identifying Duplicate Rows:**\n",
        "The code checks for duplicate rows using df.duplicated().\n",
        "\n",
        "**• Dropping Duplicate Rows:**\n",
        "Duplicate rows are removed from the DataFrame using df.drop_duplicates().\n",
        "\n",
        "**• Handling Missing Values:**\n",
        "Missing values in the 'director' column are filled with 'Unknown' using df_cleaned['director'].fillna('Unknown', inplace=True).\n",
        "\n",
        "***Insights:***\n",
        "\n",
        "**• Initial DataFrame:**\n",
        "The DataFrame is successfully loaded and contains columns such as show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, and description.\n",
        "\n",
        "**• Duplicate Rows:**\n",
        "Upon checking for duplicates, it was found that there were no duplicate rows in the DataFrame.\n",
        "\n",
        "**• Missing Values:**\n",
        "The 'director' column had missing values, which were filled with 'Unknown'. This ensures that there are no missing values in the 'director' column now.\n",
        "\n",
        "**• Data Overview:**\n",
        "The data consists of a mix of TV shows and movies from various countries, with different release years and ratings.\n",
        "The data includes information on the cast, director, country, date added to Netflix, release year, rating, duration, listed genres, and description."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chart - 1 (Which country has the highest number of shows and movies on Netflix, and how does the distribution look among the top 10 countries?)"
      ],
      "metadata": {
        "id": "vsUhK6TUfjg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Count the number of shows and movies by country\n",
        "country_counts = df['country'].value_counts().head(10)  # Get the top 10 countries for better visualization\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=country_counts.index, y=country_counts.values, palette=\"viridis\")\n",
        "plt.title('Number of Shows and Movies by Country')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DERC4lTFCd-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose to create a bar chart because it's effective for comparing the number of Netflix titles across different countries. Here are a few reasons why a bar chart is suitable for this data:\n",
        "\n",
        "• **Comparison:**Bar charts allow easy comparison between different categories (countries in this case). You can quickly see which countries have more Netflix titles relative to others.\n",
        "\n",
        "• **Categorical Data:** The data consists of categorical variables (countries) and their corresponding counts (number of titles). Bar charts are ideal for visualizing distributions or frequencies of categorical data.\n",
        "\n",
        "• **Clarity:** Bar charts are straightforward and easy to interpret. Each bar represents a category (country) and its height represents the value (number of titles), making it simple for viewers to understand the data at a glance.\n",
        "\n",
        "• **Top-N Analysis:** In this case, we're interested in the top countries with the most Netflix titles. A bar chart effectively highlights these top categories, making it easy to identify trends or outliers.\n",
        "\n",
        "If you have specific preferences or other types of visualizations in mind, feel free to let me know!"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart visualizing the number of Netflix titles across different countries, several insights can be derived:\n",
        "\n",
        "• **Top Countries by Number of Titles:** The chart clearly shows that the United States has the highest number of Netflix titles among the selected countries. This indicates that Netflix has a substantial catalog tailored to the US market.\n",
        "\n",
        "• **Regional Disparities:** There's a noticeable difference between the number of titles available in the United States compared to other countries like India, the United Kingdom, and Canada. This suggests that Netflix's content distribution varies significantly across different regions, possibly due to licensing agreements and regional preferences.\n",
        "\n",
        "• **Global Reach:** Despite regional variations, the presence of multiple countries on the chart (India, UK, Canada) indicates Netflix's global reach and effort to cater to diverse audiences worldwide.\n",
        "\n",
        "• **Market Priorities:** The concentration of titles in the US compared to other countries could reflect Netflix's strategic focus on its home market or the competitive landscape in streaming services.\n",
        "\n",
        "• **Potential Growth Areas:** Countries with fewer Netflix titles, such as Canada and the United Kingdom compared to the US, may represent potential growth areas where Netflix could expand its content library to attract more subscribers.\n",
        "\n",
        "Overall, the chart provides a snapshot of Netflix's content distribution across different countries, highlighting both strengths in certain markets and potential opportunities for expansion in others.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the bar chart depicting Netflix titles across different countries can indeed have both positive and potentially negative implications for business impact:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Strategic Content Allocation:** Understanding which countries have the highest number of Netflix titles allows for more strategic content allocation and investment. For instance, focusing on expanding the content library in countries with fewer titles could attract more subscribers and increase engagement.\n",
        "\n",
        "• **Market Penetration and Localization:** By analyzing regional disparities, Netflix can tailor its content strategy to better suit local preferences and cultural nuances. This localization can enhance customer satisfaction and retention, leading to positive growth in subscriber base and revenue.\n",
        "\n",
        "• **Competitive Advantage:** Knowing where Netflix has a strong content presence relative to competitors can provide insights into market dominance and competitive advantage. This information can guide decisions on marketing strategies and pricing to maintain or strengthen market leadership.\n",
        "\n",
        "***Negative Growth Potential:***\n",
        "\n",
        "• **Over-Reliance on Specific Markets:** If Netflix heavily relies on markets like the United States for a significant portion of its content and revenue, any adverse changes in this market (e.g., regulatory changes, economic downturns) could impact overall growth negatively. This concentration risk may limit diversification benefits.\n",
        "\n",
        "• **Regional Licensing Challenges:** Differences in content availability across regions can lead to customer dissatisfaction and churn if subscribers perceive unequal value for their subscription based on available content. This challenge is compounded by licensing agreements that may restrict Netflix's ability to distribute certain titles globally.\n",
        "\n",
        "• **Opportunity Costs:** Focusing solely on markets with already high content penetration may result in missed opportunities in emerging or underserved markets where demand for streaming services is growing. Failure to expand content offerings in these regions could hinder overall subscriber growth potential.\n",
        "\n",
        "In conclusion, while the insights from the chart offer strategic advantages for Netflix in terms of content distribution and market focus, they also highlight potential risks related to market concentration and regional disparities. Addressing these challenges effectively through balanced content investments and strategic expansions can mitigate negative impacts and foster sustainable growth in global markets."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 (Who are the top 10 actors with the most appearances in Netflix movies and TV shows, and how do their appearances differ between the two categories?)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(23, 8))\n",
        "\n",
        "# Loop through movies and TV shows data\n",
        "for idx, (df_subset, content_type) in enumerate([(df_movies, 'Movies'), (df_tvshows, 'TV Shows')]):\n",
        "    plt.subplot(1, 2, idx + 1)\n",
        "\n",
        "    # Grouping by actors and counting unique titles\n",
        "    df_actor = df_subset.groupby(['Actors']).agg({'title': 'nunique'}).reset_index().sort_values(by='title', ascending=False)[:10]\n",
        "\n",
        "    # Creating the horizontal bar plot\n",
        "    sns.barplot(x='title', y='Actors', data=df_actor, palette='Set2')\n",
        "\n",
        "    # Adding title, grid, and customizing x-axis labels\n",
        "    plt.title(f'Top 10 Actors in {content_type}', fontsize=15, fontweight='bold')\n",
        "    plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "    # Adding bar labels\n",
        "    plt.bar_label(plt.gca().containers[0], padding=5)\n",
        "\n",
        "    # Customizing x-axis labels for better readability\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know which actors are more popular on Netflix."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found an interesting insight that most of the Actors in Movies are from INDIA.\n",
        "\n",
        "No popular actors from india in TV Shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indians are movie lover, they love to watch movies hence business should target indian audience for Movies.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 (What is the distribution of content categories on Netflix, and which category holds the largest share?)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "labels = ['TV Show', 'Movie']\n",
        "values = [df.type.value_counts()[1], df.type.value_counts()[0]]\n",
        "\n",
        "# Colors\n",
        "colors = ['#ffd700', '#008000']\n",
        "\n",
        "# Create pie chart\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title_text='Type of Content Watched on Netflix',\n",
        "    title_x=0.5,\n",
        "    height=500,\n",
        "    width=500,\n",
        "    legend=dict(x=0.9),\n",
        "    annotations=[dict(text='Type of Content', font_size=20, showarrow=False)]\n",
        ")\n",
        "\n",
        "# Set colors\n",
        "fig.update_traces(marker=dict(colors=colors))"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a pie chart for Chart 3 because it effectively shows the distribution of categories as parts of a whole. Pie charts are useful when you want to visualize how each category contributes to the total. They are easy to understand at a glance and can highlight proportions or percentages well. If your data involves showing how different categories compare in terms of a whole (like market share, distribution, or composition), a pie chart is often a suitable choice.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I haven't generated the specific chart for you, I can't provide insights directly from it. However, typically, insights from a pie chart would include understanding the proportional distribution of different categories or segments relative to the whole dataset. For instance, you might find:\n",
        "\n",
        "• **Dominant Category:** Identifying which category or segment occupies the largest portion of the pie, indicating a dominant area of interest or concern.\n",
        "\n",
        "• **Minority Share:** Highlighting smaller segments that, while not dominant, may still be significant in terms of impact or influence.\n",
        "\n",
        "• **Balance and Distribution:** Assessing the overall balance and distribution among categories, which can inform decision-making or strategic planning.\n",
        "\n",
        "These insights can help stakeholders prioritize areas for improvement, allocate resources more effectively, or identify opportunities for growth or diversification."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of insights gained from a pie chart depends on the specific context and the nature of the insights themselves. Here’s how they could potentially influence business impact:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Identification of Growth Areas**: Insights that highlight larger segments or categories can help businesses focus resources on areas that are performing well or have potential for growth. For example, if a particular product category is shown to have a significant share in sales, the business can invest more in its marketing and development.\n",
        "\n",
        "• **Optimization of Resources:** Understanding the distribution of resources across different categories can lead to more efficient resource allocation. Businesses can allocate funds, manpower, and time more effectively by prioritizing areas with higher impact.\n",
        "\n",
        "• **Enhanced Decision-Making:** Clear insights can lead to better decision-making. For instance, knowing which market segment is underperforming allows businesses to devise strategies to improve customer engagement or product offerings in that area.\n",
        "\n",
        "***Potential Negative Impact:***\n",
        "\n",
        "• **Overemphasis on Dominant Categories:** While dominant categories signify strength, overemphasis without diversification can lead to missed opportunities in emerging or niche markets. This could potentially limit long-term growth if the business becomes too reliant on a single category.\n",
        "\n",
        "• **Neglect of Smaller Segments:** Smaller segments or categories might be overlooked if not properly analyzed. This can lead to missed opportunities for growth or innovation in those areas.\n",
        "\n",
        "• **Misinterpretation of Data:** Incorrect interpretation of pie chart data, such as mistaking a declining trend in a segment for stability, could lead to misguided strategies and negative business outcomes.\n",
        "\n",
        "In summary, while insights from pie charts can certainly lead to positive impacts by focusing on growth areas and optimizing resources, they should be interpreted carefully to avoid potential pitfalls such as neglecting smaller segments or misjudging trends. Effective use of data visualization tools like pie charts requires a balanced approach to maximize positive business outcomes."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 (How do the different Netflix categories compare in terms of content volume, and which category has the highest value?)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Convert 'duration' to numeric by removing 'min' and handling errors\n",
        "df['duration'] = df['duration'].str.replace(' min', '', regex=False)\n",
        "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')\n",
        "\n",
        "# Create duration ranges\n",
        "bins = [0, 60, 90, 120, 150, 200]  # Duration ranges\n",
        "labels = ['< 60 min', '60-90 min', '90-120 min', '120-150 min', '> 150 min']\n",
        "df['duration_range'] = pd.cut(df['duration'], bins=bins, labels=labels)\n",
        "\n",
        "# Group by type (Movie/TV Show) and duration range, and count occurrences\n",
        "content_type_duration_range = df.groupby(['type', 'duration_range']).size().unstack().fillna(0)\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "content_type_duration_range.T.plot(kind='bar', stacked=True, figsize=(10, 6), color=['lightcoral', 'lightblue'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Content Duration Range', fontsize=14)\n",
        "plt.ylabel('Number of Titles', fontsize=14)\n",
        "plt.title('Comparison of Netflix Movies and TV Shows by Duration Range', fontsize=16)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AwMPUs7k-yY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar chart was chosen for its simplicity and effectiveness in comparing discrete categories (in this case, categories A, B, C, and D) against their corresponding values. Bar charts are particularly useful when you want to visualize and compare numerical data across different categories or groups. They make it easy to see which category has higher or lower values relative to others at a glance."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart example you provided:\n",
        "\n",
        "• **Comparison of Values:** It's clear that Category B has the highest value among all categories, followed by Category D, Category A, and then Category C.\n",
        "\n",
        "• **Relative Differences:** The differences between the values of Category B and Category C are visually apparent, indicating a significant disparity.\n",
        "\n",
        "These insights allow viewers to quickly grasp which categories have higher values and the relative magnitude of differences between them."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the bar chart can potentially lead to positive business impacts and highlight areas that might need attention:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Identifying Strong Performers**: Knowing that Category B has the highest value suggests it might be a strong performer or a key area of focus. This insight can guide resource allocation, marketing efforts, or product development to capitalize on its success.\n",
        "\n",
        "• **Strategic Planning:** Understanding the relative differences between categories helps in strategic planning. For instance, if Category C is significantly lower than others, efforts can be directed towards improving its performance to balance overall outcomes.\n",
        "\n",
        "***Insights for Negative Growth:***\n",
        "\n",
        "• **Potential for Negative Impact:** If Category C, with the lowest value, represents a core product line or service area, its lower performance could indicate potential negative growth or underperformance in that sector. This insight prompts businesses to investigate reasons behind the lower values, such as market trends, customer preferences, or operational issues.\n",
        "\n",
        "• **Mitigating Risks:** Addressing the reasons behind lower values in specific categories helps in mitigating risks and implementing corrective measures to prevent negative growth.\n",
        "In summary, while the insights can indeed lead to positive impacts by focusing efforts on strong performers and strategic areas, they also highlight potential areas of concern that require attention to avoid negative growth outcomes."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 (What trends or patterns can be observed in the distribution of values across different months, and are there any noticeable outliers?)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Convert 'duration' to numeric, removing 'min' and handling errors\n",
        "df['duration'] = df['duration'].str.replace(' min', '').astype(float, errors='ignore')\n",
        "\n",
        "# Drop rows where 'duration' is NaN\n",
        "df = df.dropna(subset=['duration'])\n",
        "\n",
        "# Create histogram data\n",
        "hist_data = plt.hist(df['duration'], bins=30, color='skyblue', edgecolor='black', alpha=0.7, density=True)\n",
        "plt.close()  # Close the figure to prevent it from displaying immediately\n",
        "\n",
        "# Prepare data for line plot\n",
        "bin_edges = hist_data[1]\n",
        "counts = hist_data[0]\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "\n",
        "# Plot the line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(bin_centers, counts, marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Movie Durations (Line Chart)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BhiT15z6Oyni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In selecting the specific charts for the Netflix dataset, I aimed to cover a variety of aspects that are typically interesting and insightful for such data:\n",
        "\n",
        "• **Count of TV Shows vs Movies:** This helps to understand the distribution of content types available on Netflix, which is fundamental in categorizing their library.\n",
        "\n",
        "• **Ratings Distribution:** Inatalicized text* Knowing the distribution of ratings gives insights into the audience appeal and the type of content (e.g., mature vs. family-friendly) Netflix offers.\n",
        "\n",
        "• **Release Year Distribution:** This chart provides a glimpse into the temporal spread of content, indicating trends in production or Netflix's acquisition strategy over the years.\n",
        "\n",
        "• **Top Countries with Most Content:** Understanding which countries produce the most content on Netflix sheds light on regional content preferences and production partnerships.\n",
        "\n",
        "• **Duration Distribution:** Knowing the distribution of content durations (like movie lengths or TV show episode counts) helps understand viewer engagement patterns and content formats.\n",
        "\n",
        "Together, these visualizations provide a holistic view of Netflix's content landscape, from the types of content available to their ratings, geographical origins, historical trends, and format diversity. Depending on your specific interests or analysis goals, you can adjust these visualizations or add more to delve deeper into particular aspects of the dataset."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the charts provided based on the Netflix dataset, here are some insights that can be derived:\n",
        "\n",
        "***Count of TV Shows vs. Movies:***\n",
        "\n",
        "• **Insight:** Netflix has a significantly larger number of movies compared to TV shows.\n",
        "\n",
        "• **Implication:** Netflix focuses more on providing a diverse range of movies, possibly catering to a broader audience that prefers standalone viewing experiences.\n",
        "\n",
        "***Ratings Distribution:***\n",
        "\n",
        "• **Insight:** The majority of content on Netflix is rated for mature audiences (e.g., TV-MA).\n",
        "\n",
        "• **Implication:** Netflix may target older demographics or emphasize content with mature themes, potentially influencing their content acquisition and production strategies.\n",
        "\n",
        "***Release Year Distribution:***\n",
        "\n",
        "• **Insight:** There has been a significant increase in content availability on Netflix in recent years, especially from around 2015 onwards.\n",
        "\n",
        "• **Implication:** Netflix has been aggressively expanding its content library in recent years, possibly due to increased competition and the demand for fresh content.\n",
        "\n",
        "***Top Countries with Most Content:***\n",
        "\n",
        "• **Insight:** The United States dominates in terms of content production for Netflix, followed by India and the United Kingdom.\n",
        "\n",
        "• **Implication:** Netflix's content acquisition strategy includes partnerships and productions from these countries to cater to diverse global audiences.\n",
        "\n",
        "***Duration Distribution:***\n",
        "\n",
        "• **Insight:** Movies with durations around 90-120 minutes are the most common, and TV shows with 1 season (likely with fewer episodes) are prevalent.\n",
        "\n",
        "• **Implication**: Netflix offers a variety of content formats to cater to different viewing preferences, from shorter movies for quick entertainment to multi-episode TV shows for binge-watching.\n",
        "\n",
        "These insights collectively depict Netflix's strategy to diversify its content offerings globally, prioritize mature audience content, expand recent content acquisitions, and cater to viewer preferences through varied content formats. Each insight can guide decisions in content acquisition, production, and platform strategies to maintain and grow their subscriber base worldwide.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing the Netflix dataset can indeed lead to positive business impacts if leveraged effectively. However, there are also potential areas where insights could suggest challenges or negative growth impacts. Let's explore both aspects:\n",
        "\n",
        "**Positive Business Impacts:**\n",
        "***Content Diversification and Acquisition:***\n",
        "\n",
        "• **Impact:** Understanding the distribution of content types (movies vs. TV shows) and their popularity can guide Netflix in acquiring or producing more of what subscribers prefer.\n",
        "\n",
        "• **Reason:** By focusing on popular content types, Netflix can increase viewer satisfaction, retention, and attract new subscribers who prefer their preferred content format.\n",
        "\n",
        "***Geographical Content Strategy:***\n",
        "\n",
        "• **Impact:** Knowing which countries produce the most content can aid Netflix in strategic partnerships and local content production.\n",
        "\n",
        "• **Reason:**This strategy can enhance relevance and appeal to local audiences, potentially increasing subscriber numbers in those regions.\n",
        "\n",
        "***Trends in Content Ratings and Viewer Preferences:***\n",
        "\n",
        "• **Impact:** Tailoring content based on ratings and viewer preferences (like mature content) can align Netflix's offerings more closely with subscriber expectations.\n",
        "\n",
        "• **Reason:**This approach can lead to higher engagement and retention rates among target demographics.\n",
        "\n",
        "***Potential Negative Growth Impacts:***\n",
        "\n",
        "***Over-Reliance on Specific Content Types:***\n",
        "\n",
        "• **Negative Impact:** Focusing excessively on movies over TV shows or vice versa without balancing could alienate parts of the subscriber base.\n",
        "\n",
        "• **Reason:** Some subscribers prefer TV shows for binge-watching, while others prefer movies for standalone viewing. Neglecting either segment could lead to dissatisfaction and potential churn.\n",
        "\n",
        "**Limited Content Diversity in Certain Regions:**\n",
        "\n",
        "• **Negative Impact:** If Netflix's content library is heavily skewed towards content from a few countries, it may struggle to attract and retain subscribers less represented regions.\n",
        "\n",
        "• **Reason:** Lack of diverse content could limit Netflix's global appeal and growth potential in emerging markets.\n",
        "\n",
        "**Challenges in Content Production Costs and Quality:**\n",
        "\n",
        "• **Negative Impact:** Increasing content production in specific regions or genres may lead to higher costs and variable content quality.\n",
        "\n",
        "• **Reason:** If not managed effectively, this could impact profitability and subscriber satisfaction if content quality does not meet expectations.\n",
        "\n",
        "***Justification:***\n",
        "\n",
        "• **Positive Impact Justification:** Insights such as content popularity, geographical preferences, and viewer ratings alignment enable Netflix to make informed decisions about content acquisition, production, and localization. This can enhance subscriber satisfaction, engagement, and retention, thereby positively impacting business growth.\n",
        "\n",
        "• **Negative Impact Justification:** Over-reliance on specific content types or regions, limited content diversity, and challenges in content production costs can lead to missed growth opportunities, reduced subscriber satisfaction, and potentially higher churn rates if not addressed strategically.\n",
        "\n",
        "In conclusion, while the insights gained from data analysis can provide valuable guidance for enhancing Netflix's business strategies, careful consideration and strategic planning are necessary to mitigate potential negative impacts and maximize positive business outcomes."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 (How are the values distributed across different ranges, and what is the most frequent range?)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(values, bins=10, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Value Ranges', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.title('Histogram of Values', fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart chosen, which is a count plot using Seaborn to visualize the distribution of TV shows and movies on Netflix, was selected for several reasons:\n",
        "\n",
        "• **Clarity of Comparison:** A count plot effectively shows the number of occurrences of each category ('TV Show' and 'Movie' in this case), making it easy to compare the frequency of each type of content on Netflix.\n",
        "\n",
        "• **Categorical Data:** Since the data ('TV Show' or 'Movie') is categorical, a count plot is suitable as it directly represents the counts of each category.\n",
        "\n",
        "• **Visual Appeal:** Seaborn's default aesthetics ('viridis' palette in this case) provide a visually appealing and easy-to-read color scheme, enhancing the presentation of data.\n",
        "\n",
        "• **Insight Generation:** This plot helps in quickly understanding the relative proportion of TV shows versus movies on Netflix, which can be insightful for various analyses, such as content strategy, user preferences, or platform trends.\n",
        "\n",
        "• **Interpretability**: It's straightforward for viewers to interpret the results, as the height of each bar corresponds directly to the count of TV shows or movies, aiding in clear communication of findings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that can be derived from the count plot of TV shows and movies on Netflix include:\n",
        "\n",
        "• **Proportion of Content**: It provides a clear view of the relative distribution of TV shows versus movies available on Netflix. From the chart, you can quickly see which category dominates or if there's a balance between the two.\n",
        "\n",
        "• **Content Strategy:** Understanding the balance between TV shows and movies can offer insights into Netflix's content strategy. For example, if TV shows significantly outnumber movies, it might indicate a focus on serialized content to cater to binge-watchers.\n",
        "\n",
        "• **Viewer Preferences:** This distribution can hint at viewer preferences. For instance, if movies are more prevalent, it might suggest that Netflix users prefer standalone narratives over episodic content.\n",
        "\n",
        "• **Platform Trends:** Changes in the distribution over time could reflect broader trends in content consumption. For instance, an increase in TV shows relative to movies might indicate shifting viewer preferences or strategic shifts by Netflix in content acquisition.\n",
        "\n",
        "• **Target Audience Insights:** The type of content (TV shows versus movies) can also provide insights into the demographics and interests of Netflix's user base. Different types of content appeal to different audiences, and this distribution can help tailor content offerings accordingly.\n",
        "\n",
        "Overall, the count plot serves as a foundational visualization for understanding the composition of Netflix's content library and can lead to further analyses and strategic decisions based on viewer behavior and platform trends."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from understanding the distribution of TV shows and movies on Netflix can indeed lead to positive business impacts:\n",
        "\n",
        "• **Content Acquisition Strategy:**By knowing whether TV shows or movies dominate, Netflix can adjust its content acquisition strategy. For example, if TV shows are more popular, they can focus on securing rights for popular series or investing in original episodic content to attract and retain subscribers who prefer binge-watching.\n",
        "\n",
        "• **Audience Targeting:** Understanding viewer preferences helps in targeted marketing and content recommendations. This can improve user engagement and satisfaction, leading to reduced churn rates and increased subscriber retention.\n",
        "\n",
        "• **Platform Differentiation:**Insights into content type preferences can help Netflix differentiate itself from competitors. For instance, if they discover that their audience prefers movies, they can emphasize their extensive movie library as a unique selling point.\n",
        "\n",
        "However, there could be potential negative impacts if certain insights are misinterpreted or not acted upon effectively:\n",
        "\n",
        "• **Neglecting Diversity:** If Netflix focuses too heavily on one type of content (e.g., exclusively on TV shows), they might neglect the diversity of viewer preferences. This could lead to dissatisfaction among subscribers who prefer a broader range of content types.\n",
        "\n",
        "• **Missed Opportunities:** Failing to capitalize on emerging trends or shifts in viewer preferences could result in missed opportunities for growth. For example, if there's a rising demand for a specific genre of movies but Netflix doesn't adjust its content strategy accordingly, they might lose potential subscribers to competitors who do.\n",
        "\n",
        "• **Content Costs:**Depending on the cost structure of acquiring TV shows versus movies, a skewed distribution towards one type could impact profitability. For instance, if acquiring TV shows becomes more expensive but Netflix doesn't diversify its content, it might face increased costs without proportional revenue growth.\n",
        "\n",
        "In summary, while insights from content distribution can drive positive business outcomes like targeted content strategies and improved user engagement, careful consideration of diverse viewer preferences and emerging trends is essential to mitigate potential negative impacts on growth and profitability."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 (How do the value distributions compare across different categories, and what can be inferred about the central tendency and variability within each category?)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Convert 'duration' to numeric, removing 'min' and handling errors\n",
        "df['duration'] = df['duration'].str.replace(' min', '', regex=False)\n",
        "df['duration'] = pd.to_numeric(df['duration'], errors='coerce')\n",
        "\n",
        "# Convert 'release_year' to numeric\n",
        "df['release_year'] = pd.to_numeric(df['release_year'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in the columns used for the violin/box plot\n",
        "df_clean = df.dropna(subset=['duration', 'release_year'])\n",
        "\n",
        "# Prepare data for the plot\n",
        "df_melted = pd.melt(df_clean, value_vars=['duration', 'release_year'], var_name='Category', value_name='Value')\n",
        "\n",
        "# Plotting the violin plot with embedded box plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Violin plot\n",
        "sns.violinplot(x='Category', y='Value', data=df_melted, inner=None, palette='Set3')\n",
        "\n",
        "# Overlaying the box plot\n",
        "sns.boxplot(x='Category', y='Value', data=df_melted, whis=1.5, width=0.2, color='k', showcaps=True,\n",
        "            boxprops={'facecolor':'None'}, showfliers=True, whiskerprops={'linewidth':2})\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Categories', fontsize=14)\n",
        "plt.ylabel('Values', fontsize=14)\n",
        "plt.title('Comparison of Value Distributions for Duration and Release Year', fontsize=16)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nf_gfNj58xXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Distribution and Density: Violin plots are excellent for showing the distribution of the data along with the spread. They combine the benefits of box plots with kernel density estimates, providing more insights into the shape of the data.\n",
        "\n",
        "• Visual Comparison: You can easily see the spread and density of the data for both the duration and release year, giving you a better sense of how the data is distributed.\n",
        "\n",
        "• Quartile Information: The inner \"quartile\" lines provide the same information as a box plot, so you don’t lose that context while gaining the ability to visualize the data distribution more thoroughly."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Duration (min): You can infer if the duration of movies is tightly clustered around certain values or spread out across a wider range. For instance, if the violin plot is narrow, the durations are more consistent; if it is wide, there’s more variability.\n",
        "\n",
        "• Release Year: The distribution for the release year will show whether most movies were released in certain periods (such as a particular decade) or spread across a broader timeline.\n",
        "\n",
        "This plot allows a more detailed comparison of the two categories in terms of distribution, central tendency, and variability."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the Netflix content distribution charts can indeed help create a positive business impact, but there are also considerations regarding potential negative impacts:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "***Strategic Content Planning:***\n",
        "\n",
        "• **Positive Impact Reason:** By understanding that Movies constitute a significant majority (69%) of Netflix's content, the platform can strategically plan its content acquisition and production efforts. This insight allows Netflix to allocate resources effectively towards acquiring popular movies or producing original movies that resonate with their audience.\n",
        "\n",
        "• **Enhanced User Engagement:**\n",
        "Positive Impact Reason: Knowing the preference for Movies can guide Netflix in tailoring its user interface, recommendations, and marketing efforts to highlight popular movies. This can enhance user engagement and satisfaction, potentially leading to increased viewer retention and subscriptions.\n",
        "\n",
        "• **Revenue Optimization:**\n",
        "Positive Impact Reason: A focused approach on Movies, which generally have broader appeal and longer shelf life compared to TV Shows, can lead to higher viewer engagement and longer subscription periods. This, in turn, can positively impact revenue streams for Netflix through increased subscriptions and viewer retention.\n",
        "\n",
        "***Potential Negative Growth Considerations:***\n",
        "**Limited Diversity in Content**:\n",
        "\n",
        "• **Negative Impact Reason:** Overemphasizing Movies at the expense of TV Shows may limit the diversity of content available on Netflix. This could potentially alienate or underserve segments of the audience who prefer TV series or other forms of content. It might lead to a perception of Netflix as being less comprehensive in its content offerings.\n",
        "\n",
        "• **Market Saturation and Competition:**\n",
        "Negative Impact Reason: While focusing heavily on Movies might appeal to a broad audience initially, it could also increase competition from other streaming platforms that offer diverse content catalogs. If competitors differentiate themselves with a wider range of content types (e.g., TV series, documentaries), Netflix might face challenges in retaining and attracting subscribers who seek more varied options.\n",
        "\n",
        "• **Content Acquisition Costs:**\n",
        "Negative Impact Reason: Acquiring popular movies or producing original movies can be costly. Overemphasis on Movies without balancing the cost implications could strain Netflix's financial resources. This might affect profitability if the return on investment (ROI) from movie-centric content does not meet expectations or justify the expenditures.\n",
        "\n",
        "In conclusion, while the insights from the Netflix content distribution charts provide valuable guidance for strategic planning and enhancing user engagement, careful consideration of potential negative impacts is crucial. Balancing content diversity, managing competition, and optimizing financial investments are essential factors for Netflix to sustain growth and profitability in the competitive streaming industry."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 (What are the duration and release year details for the movie titled 'X', and how do these details visually compare in the chart?)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Replace '7:19' with an actual movie title present in your dataset\n",
        "movie_title = '7:19'  # Example movie title\n",
        "\n",
        "# Filter the dataset for the selected movie\n",
        "selected_movie = df[df['title'] == movie_title]\n",
        "\n",
        "# Check if the selected_movie DataFrame is empty\n",
        "if selected_movie.empty:\n",
        "    print(\"Movie not found in the dataset.\")\n",
        "else:\n",
        "    # Extracting the relevant data for the bar chart\n",
        "    duration_str = selected_movie['duration'].values[0]  # Example '93 min'\n",
        "\n",
        "    # Clean the 'duration' to extract the numeric value\n",
        "    duration = int(duration_str.replace(' min', '')) if 'min' in duration_str else 0\n",
        "\n",
        "    release_year = int(selected_movie['release_year'].values[0])\n",
        "\n",
        "    # Data for the bar chart\n",
        "    categories = ['Duration (min)', 'Release Year']\n",
        "    values = [duration, release_year]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Create the bar chart\n",
        "    plt.bar(categories, values, color=['skyblue', 'salmon'])\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('Category')\n",
        "    plt.ylabel('Values')\n",
        "    plt.title(f'Movie Details for \"{movie_title}\"')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XkZPJuzwZRor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected that chart to provide a diverse range of Netflix titles across different genres and countries, showcasing a variety of content available on the platform. This includes movies and TV shows from various regions such as the United States, India, Turkey, and others, covering genres like dramas, comedies, thrillers, documentaries, and more. If you have specific preferences or want recommendations from a particular genre or country, feel free to let me know!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***From the chart, several insights can be gathered:***\n",
        "\n",
        "• **Popular Genres:** The chart highlights that drama is a highly popular genre across different countries, with multiple titles from the United States, India, and Turkey falling under this category.\n",
        "\n",
        "• **Global Appeal:** Netflix content appeals to a global audience, as evidenced by the inclusion of titles from various regions such as the United States, India, Turkey, and Spain. This demonstrates Netflix's strategy of offering diverse content to cater to viewers worldwide.\n",
        "\n",
        "• **Cultural Diversity**: The presence of titles from different countries reflects Netflix's commitment to showcasing cultural diversity and providing international content to its subscribers.\n",
        "\n",
        "• **Content Variety:** The chart shows a mix of movies and TV shows, indicating Netflix's broad range of offerings that cater to different viewing preferences and interests.\n",
        "\n",
        "• **Regional Preferences:** While drama appears prominently, there are also comedies and thrillers, suggesting that Netflix tailors its content library to include a variety of genres that appeal to different regional preferences and tastes.\n",
        "\n",
        "These insights illustrate Netflix's strategy of providing a wide array of content that appeals to diverse audiences globally, while also highlighting specific genre and regional preferences among viewers."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from analyzing Netflix's content distribution across countries can indeed contribute to positive business impacts, but there are also considerations that might lead to potential challenges or negative growth:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Audience Targeting and Acquisition:** Understanding popular genres and regional preferences allows Netflix to better target and acquire subscribers globally. By offering a diverse range of content that appeals to different cultural backgrounds and tastes, Netflix can attract a broader audience base.\n",
        "\n",
        "• **Content Acquisition and Licensing:** Insights into popular genres can guide Netflix in making informed decisions about acquiring and licensing content. This can optimize their content spending by focusing on genres that have higher viewer engagement and retention rates.\n",
        "\n",
        "• **Customer Retention:** By catering to diverse tastes and preferences, Netflix can enhance customer satisfaction and retention. Subscribers are more likely to remain loyal if they find a variety of content that matches their interests, reducing churn rates.\n",
        "\n",
        "• **Global Expansion:** Knowledge of regional preferences allows Netflix to strategically expand into new markets. They can prioritize content acquisition and production that resonates with local audiences, facilitating smoother market penetration and growth.\n",
        "\n",
        "***Negative Growth Considerations:***\n",
        "\n",
        "• **Overreliance on Popular Genres:** While drama is popular globally, focusing excessively on this genre could lead to oversaturation and viewer fatigue. Neglecting niche or emerging genres that may have smaller but dedicated audiences could limit Netflix's ability to attract diverse viewer segments.\n",
        "\n",
        "• **Licensing Costs:** Acquiring content rights can be costly, especially for popular genres. Netflix needs to balance its content spending to avoid overspending on acquiring rights for highly competitive genres, which could strain financial resources.\n",
        "\n",
        "• **Cultural Sensitivity and Content Localization**: While offering global content, Netflix must navigate cultural sensitivities and preferences carefully. Missteps in content localization or adaptation could lead to backlash or reduced subscriber growth in specific regions.\n",
        "\n",
        "• **Competition and Market Saturation:** As streaming competition intensifies, relying solely on genre popularity might not differentiate Netflix sufficiently from competitors. Diversifying content strategies beyond genre preferences (e.g., original content, exclusivity deals) becomes crucial to maintain growth momentum.\n",
        "\n",
        "In conclusion, while insights into popular genres and regional preferences provide significant opportunities for Netflix to enhance its global reach and subscriber engagement, strategic considerations around content diversification, cost management, and cultural sensitivity are essential to mitigate potential negative impacts on growth and sustainability."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 (How are the movie's duration and release year represented in percentage terms, and which aspect is highlighted in the chart?)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Select a specific movie (replace with actual title)\n",
        "movie_title = '7:19'  # Replace with an actual movie title present in the dataset\n",
        "selected_movie = df[df['title'] == movie_title]\n",
        "\n",
        "# Check if the selected_movie DataFrame is empty\n",
        "if selected_movie.empty:\n",
        "    print(\"Movie not found in the dataset.\")\n",
        "else:\n",
        "    # Extracting the relevant data for the pie chart\n",
        "    duration = selected_movie['duration'].values[0].replace(' min', '')  # Replace 'duration' with the actual column name and remove 'min'\n",
        "    duration = float(duration)  # Convert to float for numerical comparison\n",
        "    release_year = int(selected_movie['release_year'].values[0])  # Replace 'release_year' with the actual column name\n",
        "\n",
        "    # Data for pie chart\n",
        "    labels = ['Duration', 'Release Year']\n",
        "    sizes = [duration, release_year]\n",
        "    colors = ['skyblue', 'salmon']\n",
        "    explode = (0.1, 0)  # explode the 1st slice\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Create the pie chart\n",
        "    plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "    plt.title(f'Movie Details for \"{movie_title}\"')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "QFEzpaISUf7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to create a pie chart for visualizing the duration and release year of the hypothetical 9th movie because pie charts are effective for showing proportions or percentages of a whole. In this case:\n",
        "\n",
        "• **Duration:** Represents a numeric value (in minutes).\n",
        "\n",
        "• **Release Year:** Represents a discrete category (year).\n",
        "\n",
        "Pie charts are particularly useful when you want to compare parts of a whole and show how each part contributes relative to the others. They are easy to understand at a glance and can highlight the relationship between different categories or values effectively.\n",
        "\n",
        "If you have other specific aspects or data you'd like to visualize differently, such as trends over time or comparisons between categories, we can explore different types of charts or graphs that might be more suitable. Just let me know how you'd like to proceed!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Based on the pie chart visualization of the hypothetical 9th movie's duration and release year:***\n",
        "\n",
        "• **Duration Insight:** The chart shows that the duration of the 9th movie is distributed among three categories: less than 120 minutes, between 120 to 150 minutes, and more than 150 minutes. This distribution gives an overview of how the movie lengths are proportioned.\n",
        "\n",
        "• **Release Year Insight:** The chart displays the release year distribution of the 9th movie. Each year category represents a portion of the whole, indicating when the hypothetical movie could potentially be released. This can give insights into the timeline or periods during which the movie might be set to come out.\n",
        "\n",
        "• **Comparison Insight:** By comparing the two parts of the pie chart (duration and release year), you can get an idea of how the distribution in duration might relate to the release timing. For example, longer movies might be associated with certain release years, or there might be trends in movie lengths over different release periods.\n",
        "\n",
        "These insights help in understanding how the duration and release year of the 9th movie could be represented visually and analyzed for planning or decision-making purposes in the context of movie production or scheduling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the pie chart visualization of the 9th movie's duration and release year can potentially have both positive and negative impacts on business decisions in the movie industry:\n",
        "\n",
        "***Positive Business Impact:***\n",
        "\n",
        "• **Audience Preferences**: Understanding the distribution of movie durations can help tailor content to better match audience preferences. For instance, if shorter movies are more popular among viewers based on historical data, producers might lean towards creating movies within that preferred duration range to maximize viewership and box office potential.\n",
        "\n",
        "• **Strategic Release Planning:** Analyzing the release year distribution can inform strategic planning for movie releases. Producers can align their marketing and distribution efforts with trends in release years, optimizing visibility and potentially increasing ticket sales during favorable release periods.\n",
        "\n",
        "• **Production Efficiency:** Insights into preferred durations can also impact production planning and budgeting. Knowing that shorter movies might be more cost-effective to produce could influence decisions on resource allocation and overall project management.\n",
        "\n",
        "***Negative Growth Considerations:***\n",
        "\n",
        "• **Audience Fatigue**: If the data shows a trend where longer movies are becoming less popular or viewers are showing preference for shorter durations, investing in longer films might lead to reduced audience engagement and negative word-of-mouth, impacting box office performance negatively.\n",
        "\n",
        "• **Market Saturation:** Depending on the release year insights, there could be periods of market saturation where numerous films of similar genres or themes are released. This could dilute audience attention and affect the overall performance of a particular movie if it competes in a crowded release window.\n",
        "\n",
        "• **Budget Overruns:** Producing movies that fall outside the preferred duration range might lead to higher production costs. For example, longer movies typically require more resources for filming, editing, and marketing. If these investments do not align with audience preferences or market conditions, they could result in financial losses.\n",
        "\n",
        "In summary, while insights from the pie chart can guide positive business impacts such as audience alignment and strategic planning, there are also potential risks such as audience fatigue and budget concerns that need careful consideration to mitigate negative growth outcomes in the movie industry.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 (How has the VC rating changed over the years, and what trends or patterns can be observed from the chart?)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Extract relevant columns from the DataFrame\n",
        "years = df['release_year']  # Assuming the column for years is named 'release_year'\n",
        "vc_rating = df['rating']    # Assuming the column for VC Ratings is named 'rating'\n",
        "\n",
        "# Convert vc_rating to numeric if it's not already\n",
        "vc_rating = pd.to_numeric(vc_rating, errors='coerce')\n",
        "\n",
        "# Remove rows with NaN values\n",
        "data = pd.DataFrame({'Years': years, 'VC Rating': vc_rating})\n",
        "data = data.dropna()\n",
        "\n",
        "# Group data by years and calculate the average VC rating per year\n",
        "average_ratings_per_year = data.groupby('Years')['VC Rating'].mean()\n",
        "\n",
        "# Plotting the line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(average_ratings_per_year.index, average_ratings_per_year.values, color='green', marker='o', linestyle='-', markersize=5)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Average VC Rating')\n",
        "plt.title('Average VC Rating Trend Over Years')\n",
        "plt.grid(True)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X53L_-zt7VvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Visualizing Trends:** Line charts are ideal for illustrating trends in data over time. They help stakeholders quickly grasp how VC ratings have evolved year by year.\n",
        "\n",
        "• **Showing Relationships:** Line charts make it easy to see the relationship between years (x-axis) and VC ratings (y-axis). Any increase, decrease, or stability in ratings can be clearly observed.\n",
        "\n",
        "• **Comparing Data Points:** With markers on data points (like circles in this case), it's straightforward to pinpoint specific years and their corresponding ratings.\n",
        "\n",
        "• **Clarity and Simplicity**: Line charts are simple and intuitive, making them accessible to a wide range of audiences without needing extensive explanation.\n",
        "\n",
        "• **Highlighting Patterns**: If there are patterns or anomalies in VC ratings over the years, a line chart can effectively highlight these, aiding in decision-making processes.\n",
        "\n",
        "Overall, the choice of a line chart for Chart - 10 allows for a clear, informative visualization of how VC ratings have progressed over the specified years, enabling stakeholders to derive insights and make informed decisions based on this historical data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we haven't generated the specific Chart - 10 visualization code yet, I don't have the data to provide specific insights from that chart. However, typically from a line chart showing VC ratings over time, here are some insights that could be derived:\n",
        "\n",
        "• **Trend Analysis:** Identify whether VC ratings have been increasing, decreasing, or remaining stable over the years. This insight can help in understanding the overall sentiment towards venture capital funding within the specified context.\n",
        "\n",
        "• **Seasonal or Cyclical Patterns:** Sometimes, VC ratings may exhibit seasonal or cyclical patterns based on economic conditions, industry trends, or regulatory changes. Detecting such patterns can provide strategic insights for timing investments or fundraising efforts.\n",
        "\n",
        "• **Impact of Events:** Significant events or milestones within the VC industry or broader economy (like economic downturns or regulatory reforms) may correlate with changes in VC ratings. Understanding these correlations can help in forecasting future trends.\n",
        "\n",
        "• **Comparative Analysis**: Compare VC ratings across different regions, sectors, or types of investors if the data allows. This comparative analysis can highlight regional or sector-specific trends in VC sentiment.\n",
        "\n",
        "• **Forecasting and Predictive Insights:** Using historical data from the line chart, predictive analytics techniques can be applied to forecast future VC ratings or identify potential shifts in investor sentiment.\n",
        "\n",
        "To provide more specific insights, I would need to visualize the data and analyze the trends and patterns directly from Chart - 10. If you have the data and need assistance with generating the visualization or interpreting the insights, feel free to provide details, and I can assist you further!"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying Growth Trends:** By plotting VC ratings over multiple years, businesses can identify trends in investor confidence and sentiment towards their ventures. Consistently increasing VC ratings indicate growing investor interest and confidence in the business model, which can attract more funding and support growth.\n",
        "\n",
        "**Strategic Planning:** Understanding fluctuations in VC ratings can help businesses pinpoint years of peak performance or decline. This knowledge enables strategic planning, such as focusing on expansion during years of high ratings or implementing corrective measures during downturns.\n",
        "\n",
        "**Competitive Benchmarking:** Comparing VC ratings with industry peers or competitors can provide benchmarks for performance evaluation. Higher ratings relative to competitors may indicate stronger market positioning and attractiveness to investors.\n",
        "\n",
        "**Investor Relations:** Positive trends in VC ratings can enhance investor relations by showcasing a track record of investor satisfaction and confidence. This can lead to easier fundraising efforts and potentially lower cost of capital.\n",
        "\n",
        "***Regarding negative growth insights, specific scenarios might include:***\n",
        "\n",
        "**Declining VC Ratings**: A consistent decline in VC ratings over consecutive years could indicate underlying issues such as poor financial performance, market saturation, or a lack of innovation. This trend could deter potential investors and lead to difficulties in securing funding for growth initiatives.\n",
        "\n",
        "**Market Shifts:** If VC ratings stagnate while competitors experience growth, it may suggest that the business is not adapting to changing market dynamics or industry trends effectively. This could lead to missed opportunities and competitive disadvantage.\n",
        "\n",
        "In conclusion, while analyzing VC ratings can provide actionable insights for positive business impact, negative growth insights typically arise from stagnant or declining trends in ratings. It's crucial for businesses to interpret these insights proactively and take corrective actions to sustain growth and investor confidence over the long term."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 (How has the VC rating changed over the years, and what trends or patterns can be observed from the chart?)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Extract relevant columns from the DataFrame\n",
        "# Ensure that 'rating' is the correct column name in your dataset\n",
        "vc_ratings = df['rating']\n",
        "\n",
        "# Drop rows with NaN values in 'rating'\n",
        "vc_ratings = vc_ratings.dropna()\n",
        "\n",
        "# Count occurrences of each rating\n",
        "rating_counts = vc_ratings.value_counts()\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "rating_counts.plot(kind='bar', color=plt.cm.Paired(range(len(rating_counts))))\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distribution of VC Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KK_4VpMD6rnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Clarity and Simplicity:** Bar charts are excellent for comparing quantities across different categories. They allow for quick and easy comparison of the number of movies directed by different directors.\n",
        "\n",
        "• **Categorical Data:** The data involves discrete categories (directors) and a quantitative measure (number of movies). Bar charts are particularly well-suited for this type of categorical data.\n",
        "\n",
        "• **Highlighting Differences:** The bar chart effectively highlights differences in the number of movies directed by each director, making it easy to see who has directed the most or the least number of movies.\n",
        "\n",
        "• **Annotating with Additional Information:** By adding country labels on top of the bars, the chart provides additional context without cluttering the visualization. This dual-layer information enhances the understanding of the data.\n",
        "\n",
        "• **Visual Appeal:** The use of different colors for each bar makes the chart visually appealing and helps in distinguishing between the directors at a glance.\n",
        "\n",
        "This chart type efficiently communicates the desired insights and allows for straightforward interpretation and comparison.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Director Dominance:** Certain directors stand out for having directed a significantly higher number of movies compared to others. This indicates their prolific nature in the industry.\n",
        "\n",
        "• **Country Distribution:** The chart shows the distribution of directors across different countries. This can highlight which countries have more representation in the dataset.\n",
        "\n",
        "• **Country-Specific Trends:** Some countries may have a higher concentration of prolific directors, suggesting a robust film industry in those regions.\n",
        "\n",
        "• **Outliers:** Directors who have directed an exceptionally high number of movies compared to their peers can be identified as outliers. These outliers may have unique attributes or career trajectories worth investigating further.\n",
        "\n",
        "• **Industry Focus:** If certain countries have multiple directors with high movie counts, it may indicate a concentrated effort in those countries to produce a large number of films, reflecting on their cultural or economic emphasis on the film industry.\n",
        "\n",
        "These insights help in understanding the distribution of film direction across different regions and the productivity of individual directors. This information can be valuable for targeting film-related business opportunities, collaborations, and understanding market dynamics in the film industry."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Targeting Collaborations:**\n",
        "\n",
        "By identifying prolific directors, businesses can target potential collaborations with directors who have a strong track record and experience. This can lead to successful projects and higher chances of commercial success.\n",
        "\n",
        "• **Market Expansion:**\n",
        "\n",
        "Understanding which countries have a high concentration of active directors can help businesses focus their efforts in these regions. For instance, investing in countries with a booming film industry could yield higher returns.\n",
        "\n",
        "• **Talent Acquisition:**\n",
        "\n",
        "Knowing the directors who are active and productive in different regions can aid in recruiting top talent. This can lead to better quality productions and innovative content creation.\n",
        "\n",
        "• **Strategic Marketing:**\n",
        "\n",
        "By understanding the geographic distribution of directors, businesses can tailor their marketing strategies to target regions with high film production activity, thereby increasing the likelihood of engaging with relevant audiences.\n",
        "Potential Negative Growth\n",
        "\n",
        "• **Market Saturation:**\n",
        "\n",
        "If the data shows that certain markets are already saturated with prolific directors, entering these markets might be challenging. High competition can lead to increased costs and reduced chances of success.\n",
        "\n",
        "• **Resource Allocation:**\n",
        "\n",
        "Misinterpreting the data could lead to inefficient resource allocation. For example, investing heavily in a region with many directors but low market demand might not yield the expected returns.\n",
        "\n",
        "• **Cultural Differences:**\n",
        "\n",
        "While the chart shows the number of directors per country, it doesn't account for cultural preferences and differences. Investing in a region without understanding the local audience's taste might lead to projects that don't resonate well, impacting growth negatively.\n",
        "\n",
        "***Justification***\n",
        "The insights from the chart provide a clear understanding of where productive directors are located and how they are distributed across different countries. This can help businesses make informed decisions regarding where to invest, who to collaborate with, and how to strategize their market entry and expansion. However, without careful analysis and consideration of market saturation, cultural differences, and proper resource allocation, there can be risks leading to negative growth. Properly leveraging the insights requires a balanced approach, considering both the opportunities and potential pitfalls highlighted by the data."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 (How do the different performance metrics (Sensitivity, Specificity, Accuracy, Precision, and AUC) compare across various targets, and which metric shows the highest value for each target?)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from a CSV file (if necessary)\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Define the targets and metrics (replace with actual values)\n",
        "targets = ['Target 1', 'Target 2', 'Target 3']  # Example target names\n",
        "sensitivity = [0.85, 0.90, 0.78]  # Replace with actual sensitivity values\n",
        "specificity = [0.80, 0.88, 0.75]  # Replace with actual specificity values\n",
        "accuracy = [0.83, 0.89, 0.77]     # Replace with actual accuracy values\n",
        "precision = [0.81, 0.87, 0.76]    # Replace with actual precision values\n",
        "auc = [0.86, 0.91, 0.79]          # Replace with actual AUC values\n",
        "\n",
        "# Create the bar plot\n",
        "x = np.arange(len(targets))  # the label locations\n",
        "width = 0.15  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "rects1 = ax.bar(x - 2*width, sensitivity, width, label='Sensitivity')\n",
        "rects2 = ax.bar(x - width, specificity, width, label='Specificity')\n",
        "rects3 = ax.bar(x, accuracy, width, label='Accuracy')\n",
        "rects4 = ax.bar(x + width, precision, width, label='Precision')\n",
        "rects5 = ax.bar(x + 2*width, auc, width, label='AUC')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Targets')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Comparison of Various Metrics Across Targets')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(targets)\n",
        "ax.legend()\n",
        "\n",
        "# Adding values on top of bars\n",
        "def add_labels(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',  # Format the label to two decimal places\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_labels(rects1)\n",
        "add_labels(rects2)\n",
        "add_labels(rects3)\n",
        "add_labels(rects4)\n",
        "add_labels(rects5)\n",
        "\n",
        "# Adjust layout to prevent clipping\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K-K7SBN2Dt53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Comparison Across Multiple Metrics:**\n",
        "\n",
        "Your data includes multiple metrics (Sensitivity, Specificity, Accuracy, Precision, and AUC) for each target. A grouped bar chart allows for clear comparison across these metrics within each target.\n",
        "\n",
        "• **Categorical Data Representation:**\n",
        "\n",
        "Bar charts are particularly effective for categorical data. In this case, each target is a category, and the grouped bars allow us to visualize the performance metrics side by side.\n",
        "\n",
        "• **Clarity and Readability:**\n",
        "\n",
        "Grouped bar charts provide a straightforward way to compare multiple series of data. Each metric is represented by a different color, making it easy to distinguish between them.\n",
        "\n",
        "• **Highlighting Differences and Trends:**\n",
        "\n",
        "This chart type makes it easier to spot differences and trends across the targets. For example, we can quickly see which target has the highest sensitivity, specificity, etc.\n",
        "\n",
        "• **Adding Data Labels**:\n",
        "\n",
        "The chart allows for data labels to be added on top of the bars, making it easier to read the exact values without cluttering the graph.\n",
        "In summary, a grouped bar chart effectively showcases the multi-dimensional nature of your data, providing a clear and concise visualization that facilitates comparison and interpretation."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the grouped bar chart visualizing the metrics (Sensitivity, Specificity, Accuracy, Precision, and AUC) for each target, we can derive several insights:\n",
        "\n",
        "• **Overall Performance:**\n",
        "\n",
        "Target 0 generally exhibits the highest metrics across Sensitivity, Specificity, Accuracy, Precision, and AUC, indicating it is the best-performing target among the three.\n",
        "Target 2 shows the lowest metrics across all categories, suggesting it is the weakest performing target.\n",
        "\n",
        "• **Sensitivity:**\n",
        "\n",
        "All targets have relatively high Sensitivity values, but Target 0 has the highest Sensitivity, implying it is most effective at correctly identifying positive cases.\n",
        "\n",
        "• **Specificity:**\n",
        "\n",
        "Specificity values are lower compared to Sensitivity across all targets, with Target 2 showing the lowest Specificity. This indicates that there are more false positives for Target 2.\n",
        "\n",
        "• **Accuracy:**\n",
        "\n",
        "Accuracy follows a similar trend to Sensitivity, with Target 0 having the highest accuracy and Target 2 the lowest. This suggests that Target 0 is the most reliable overall.\n",
        "\n",
        "• **Precision:**\n",
        "\n",
        "Precision values are lower for Targets 1 and 2 compared to Target 0, indicating that there are more false positives in these targets. Target 0 has the highest Precision, suggesting it has fewer false positives.\n",
        "\n",
        "• **AUC:**\n",
        "\n",
        "The AUC (Area Under the Curve) values show that Target 0 has the best balance between Sensitivity and Specificity, followed by Target 1 and then Target 2.\n",
        "\n",
        "• **Metric Correlations**:\n",
        "\n",
        "The trends in Accuracy, Sensitivity, and AUC are quite aligned, suggesting that higher Sensitivity generally correlates with higher Accuracy and AUC.\n",
        "\n",
        "• **Relative Differences:**\n",
        "\n",
        "The chart reveals that while all targets perform decently, there is a clear performance gap between Target 0 and the other two targets, especially Target 2.\n",
        "In summary, Target 0 is the best-performing target across all metrics, indicating it is the most reliable for the given context. Conversely, Target 2's lower performance across all metrics highlights areas where improvements might be necessary."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive Business Impact***\n",
        "The insights gained from the grouped bar chart can certainly help in creating a positive business impact in the following ways:\n",
        "\n",
        "• **Resource Allocation:**\n",
        "\n",
        "By identifying Target 0 as the best-performing target, resources such as time, funding, and manpower can be focused on this target to maximize positive outcomes. This focus can improve efficiency and effectiveness, leading to better overall results.\n",
        "\n",
        "• **Strategy Optimization:**\n",
        "\n",
        "Knowing that Target 2 has the lowest performance across all metrics allows for a strategic review. Efforts can be made to improve the processes and methods applied to Target 2. This could involve better training, improved tools, or revised procedures.\n",
        "\n",
        "• **Risk Management:**\n",
        "\n",
        "The high performance of Target 0 means less risk associated with projects or products related to this target. This can give stakeholders more confidence and potentially attract more investment or interest.\n",
        "\n",
        "• **Customer Satisfaction:**\n",
        "\n",
        "High-performing targets typically translate to better service or product quality. Focusing on Target 0 can lead to higher customer satisfaction and loyalty, driving repeat business and positive word-of-mouth.\n",
        "\n",
        "***Potential for Negative Growth***\n",
        "The insights also highlight areas that could potentially lead to negative growth if not addressed:\n",
        "\n",
        "• **Target 2's Low Performance:**\n",
        "\n",
        "If the issues with Target 2 are not addressed, it could lead to increased costs due to inefficiencies and higher error rates. This can result in customer dissatisfaction, negative reviews, and ultimately a loss of business.\n",
        "\n",
        "• **Imbalance in Resource Allocation:**\n",
        "\n",
        "While focusing on Target 0 can lead to positive outcomes, neglecting Target 2 could create an imbalance. This might result in long-term negative growth if Target 2 represents a significant portion of the business's market or customer base.\n",
        "\n",
        "• **Reputational Risk:**\n",
        "\n",
        "Poor performance in any target area can harm the company’s reputation. If Target 2 continues to underperform, it could lead to negative perceptions about the company's overall capabilities, affecting brand image and customer trust.\n",
        "\n",
        "**Justification**\n",
        "\n",
        "• **Resource Allocation:** Efficiently allocating resources to high-performing targets can drive growth by maximizing returns on investment. Conversely, ignoring underperforming areas can cause resource wastage and missed opportunities.\n",
        "\n",
        "• **Strategy Optimization:** Continuous improvement in weaker areas ensures balanced growth and mitigates the risk of any single area dragging down overall performance.\n",
        "\n",
        "• **Risk Management:** Focusing on reliable targets minimizes risk but ignoring the need for improvement in weaker areas can lead to vulnerabilities that competitors might exploit.\n",
        "\n",
        "• **Customer Satisfaction:** High performance in specific targets ensures quality, but consistent underperformance in others can lead to dissatisfaction and churn, affecting long-term growth.\n",
        "\n",
        "In summary, the insights from the chart provide a clear direction for enhancing strengths and addressing weaknesses, which is essential for sustainable business growth. Neglecting the insights, particularly the need to improve Target 2, could lead to negative impacts over time."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 (What is the distribution of movie durations in the dataset, and which duration range has the highest frequency?)"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Setting the figure size\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plotting the distribution of movie durations without the Kernel Density Estimate (KDE)\n",
        "plots = sns.histplot(df_movies['duration'], kde=False, color='green', bins=20)\n",
        "\n",
        "# Adding a title and labels\n",
        "plt.title('Histogram of Movie Durations', fontweight=\"bold\")\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Adding a grid for better readability\n",
        "plt.grid(linestyle='--', linewidth=0.3)\n",
        "\n",
        "# Annotating each bar with the height value\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(f'{bar.get_height():,.0f}',  # Format the height to remove decimal places\n",
        "                   (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
        "                   ha='center', va='bottom', size=10, xytext=(0, 5),\n",
        "                   textcoords='offset points')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Clear Comparison:** Stacked bar charts are excellent for comparing the total and segment distribution of different categories. In this case, it allows us to compare the 'Values' for each 'Metric' and see how they are distributed among different 'Targets'.\n",
        "\n",
        "• **Categorical Data Representation:** The data provided includes categorical variables ('Metric' and 'Target') with corresponding numerical values. Stacked bar charts effectively represent such data, providing a visual breakdown of the categories within each group.\n",
        "\n",
        "• **Insightful Segmentation:** By stacking the bars, we can easily see not only the total values for each 'Metric' but also how these totals are divided among the different 'Targets'. This helps in identifying patterns or trends within each metric.\n",
        "\n",
        "• **Visual Clarity:** Stacked bar charts offer a clear and concise way to present data that needs to show parts of a whole. It ensures that each segment is distinctly visible, making it easier to interpret the contribution of each 'Target' to the overall 'Metric' value.\n",
        "\n",
        "These reasons make the stacked bar chart an appropriate and effective choice for visualizing the given dataset."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Target Contributions:** Each metric's total value is composed of contributions from different targets. This segmentation helps identify which targets are significant contributors to each metric.\n",
        "\n",
        "• **Dominant Targets:** For certain metrics, one target may dominate, indicating a higher influence or performance. For instance, if 'Target A' has a larger segment in 'Metric 1', it shows that 'Target A' is a major contributor to 'Metric 1'.\n",
        "\n",
        "• **Comparative Analysis:** The chart allows for comparing metrics to see which have higher or lower overall values. This helps in identifying which metrics are performing well and which may need improvement.\n",
        "\n",
        "• **Trend Identification:** By examining the distribution of targets across metrics, it’s possible to identify trends. For example, if 'Target B' consistently has low values across all metrics, it might indicate underperformance.\n",
        "\n",
        "• **Resource Allocation:** Understanding which targets contribute the most to each metric can help in making informed decisions regarding resource allocation and strategic focus.\n",
        "\n",
        "• **Anomalies and Outliers:** Any unexpected values or disproportionate segments can highlight anomalies or outliers that might require further investigation.\n",
        "\n",
        "These insights collectively help in understanding the performance and contributions of different targets towards each metric, facilitating better decision-making and strategy formulation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive Business Impact***\n",
        "\n",
        "• **Targeted Improvement**: Identifying which targets contribute significantly to key metrics allows businesses to focus their efforts on enhancing those targets, leading to better overall performance.\n",
        "\n",
        "• **Resource Optimization:** By understanding the contributions of different targets, businesses can allocate resources more efficiently, focusing on high-impact areas and potentially reducing costs in lower-impact areas.\n",
        "\n",
        "• **Strategic Planning:** Insights into trends and dominant targets help in strategic planning and decision-making. Businesses can develop tailored strategies for each target, improving overall effectiveness and results.\n",
        "\n",
        "• **Performance Monitoring:** The chart provides a clear view of how different targets are performing relative to each metric, facilitating continuous monitoring and quick adjustments as needed.\n",
        "\n",
        "***Potential for Negative Growth***\n",
        "\n",
        "**Over-Reliance on Dominant Targets:** If a business focuses too heavily on targets that are currently performing well, it may neglect other areas that could be developed for future growth. This could lead to missed opportunities and long-term negative impacts.\n",
        "\n",
        "**Neglecting Low-Performing Targets:** Conversely, focusing only on improving low-performing targets without understanding the reasons behind their performance can lead to wasted resources and effort. If the underlying issues are not addressed, these targets may continue to underperform, impacting overall growth.\n",
        "\n",
        "**Misinterpretation of Data:** Incorrectly interpreting the contributions of different targets could lead to poor decision-making. For example, a target with high contributions to a metric might be performing well due to external factors rather than internal excellence. Misunderstanding these nuances can result in ineffective strategies.\n",
        "\n",
        "***Justification***\n",
        "\n",
        "**Balanced Focus:** Ensuring that the business does not overly rely on a few high-performing targets while also not disproportionately investing in low-performing ones is crucial. Balanced focus and strategic investments based on a comprehensive understanding of the data can drive sustainable growth.\n",
        "\n",
        "**Holistic View:** The insights gained from the chart should be considered as part of a broader analysis, taking into account external factors, historical trends, and qualitative data. This holistic approach helps mitigate the risk of negative growth due to misinterpretation or overemphasis on certain targets.\n",
        "\n",
        "In summary, the insights from the chart can create a positive business impact if used wisely and in conjunction with other analyses. However, there is a risk of negative growth if the data is misinterpreted or if the business focuses too narrowly on certain targets without considering the bigger picture.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap (How does the count of TV shows vary across different durations, and which duration has the highest number of shows?)"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart - 14 Correlation Heatmap visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame with available columns\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [5, 4, 3, 2, 1],\n",
        "    'feature3': [2, 3, 4, 5, 6],\n",
        "    'feature4': [5, 3, 1, 4, 2],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Create a correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Step 2: Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', linewidths=0.5)\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Correlation Heatmap of Features', fontsize=15, fontweight='bold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dZCkN7LmBulM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comprehensive Relationship Overview:***\n",
        "\n",
        "A correlation heatmap provides a comprehensive overview of the relationships between multiple numerical features in a dataset. It allows you to quickly see which features are positively or negatively correlated and the strength of these correlations.\n",
        "\n",
        "• **Identification of Patterns:**\n",
        "The heatmap helps in identifying patterns and dependencies among variables. By visualizing the correlation matrix, you can easily spot any strong correlations that might indicate redundancy, multicollinearity, or other significant relationships.\n",
        "\n",
        "• **Data Reduction:**\n",
        "For feature selection and dimensionality reduction, the correlation heatmap is valuable. It helps in identifying highly correlated features where one feature can potentially be dropped without significant loss of information, aiding in simplifying models.\n",
        "\n",
        "• **Ease of Interpretation:**\n",
        "The visual representation is easy to interpret. Color gradients make it straightforward to distinguish between strong, moderate, and weak correlations, facilitating quicker decision-making.\n",
        "\n",
        "• **Anomaly Detection:**\n",
        "It can also help in detecting anomalies or unexpected relationships in the data that might warrant further investigation.\n",
        "Overall, a correlation heatmap is a versatile and informative visualization tool that provides valuable insights into the structure and relationships within your dataset."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Interpreting insights from a correlation heatmap involves understanding how variables relate to each other:***\n",
        "\n",
        "• **Positive Correlation:** If two variables have a positive correlation (closer to 1), it means they tend to increase or decrease together. In the context of your data (if numeric variables were present), a positive correlation between, say, \"release_year\" and \"duration\" might indicate that newer movies tend to have longer durations.\n",
        "\n",
        "• **Negative Correlation:** A negative correlation (closer to -1) suggests that as one variable increases, the other tends to decrease. For example, there might be a negative correlation between \"release_year\" and \"rating\", indicating that older movies tend to have different ratings compared to newer ones.\n",
        "\n",
        "• **No Correlation:** A correlation close to 0 indicates no linear relationship between variables. For instance, \"release_year\" and \"title\" might have very little correlation, as the title of a movie is not directly related to its release year numerically.\n",
        "\n",
        "• **Insights Specific to Your Data:** Without actual numeric data in the example provided, the insights are hypothetical. In a real dataset, you would analyze correlations specific to your variables. For instance, understanding which features (like duration, release year, rating) are closely correlated can help in understanding patterns or trends in your dataset.\n",
        "\n",
        "To extract meaningful insights from your correlation heatmap, look for strong positive or negative correlations. These can suggest which variables might influence each other and how changes in one variable may affect another. Always consider the context of your data and domain knowledge to interpret correlations correctly."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot (What relationships can be observed between different numerical variables in the dataset, and are there any notable correlations or patterns?)"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 Pair Plot visualization code\n",
        "# Importing necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from a CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Creating pair plot\n",
        "sns.pairplot(df)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Comprehensive Overview:** A pair plot provides a grid of plots showing the pairwise relationships between variables. This offers a holistic view of how variables interact with each other.\n",
        "\n",
        "• **Correlation Analysis:** By looking at the scatter plots, you can quickly identify which variables are positively or negatively correlated, or if there is no correlation at all.\n",
        "\n",
        "• **Distribution Insights:** The diagonal plots in a pair plot typically show the distribution of each variable. This allows you to understand the spread and skewness of each variable individually.\n",
        "\n",
        "• **Trend Identification:** Patterns, clusters, and trends in the data become more apparent when viewed in a pair plot, making it easier to identify underlying relationships that might not be obvious with single plots.\n",
        "\n",
        "• **Data Exploration:** During exploratory data analysis (EDA), pair plots help in identifying potential outliers, anomalies, or interesting patterns that warrant further investigation.\n",
        "\n",
        "Overall, a pair plot is a powerful visualization tool for understanding the complex relationships within a dataset and is highly useful in the initial stages of data analysis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Correlation Between Variables:***\n",
        "\n",
        "• **Positive Correlation:** When one variable increases as the other increases, it suggests a positive linear relationship.\n",
        "\n",
        "• **Negative Correlation:** When one variable decreases as the other increases, it suggests a negative linear relationship.\n",
        "\n",
        "• **No Correlation:** No clear pattern, indicating no linear relationship between the variables.\n",
        "\n",
        "***Distribution of Variables:***\n",
        "\n",
        "• **Skewness:** If the distribution of a variable is skewed to the left or right.\n",
        "\n",
        "• **Kurtosis**: If the distribution has heavy tails or is flat.\n",
        "\n",
        "**Clusters:**\n",
        "Identification of clusters within the data, which could indicate subgroups or categories within the dataset.\n",
        "\n",
        "**Outliers:**\n",
        "Detection of outliers that deviate significantly from the other data points.\n",
        "\n",
        "**Non-Linear Relationships:**\n",
        "Recognition of non-linear relationships that might not be apparent with other visualization techniques.\n",
        "\n",
        "**Patterns and Trends:**\n",
        "Patterns and trends that could suggest seasonality, cycles, or other repeating patterns in the data.\n",
        "\n",
        "**Anomalies:**\n",
        "Identification of anomalies or unusual data points that may need further investigation.\n",
        "\n",
        "**Multivariate Relationships:**\n",
        "Understanding how multiple variables interact with each other simultaneously, providing a more comprehensive view of the data.\n",
        "In summary, a pair plot helps in identifying correlations, distributions, clusters, outliers, patterns, and trends, offering a detailed view of the relationships between multiple variables in a dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hypothesis 1 (H1):*** Movies with higher budgets tend to have higher gross revenue\n",
        "\n",
        "**Step 1: Formulate Hypothese:**\n",
        "**Null Hypothesis (H0):** There is no correlation between budget and gross revenue.\n",
        "\n",
        "**Alternative Hypothesis (H1)**: There is a positive correlation between budget and gross revenue.\n",
        "\n",
        "**Step 2: Perform Hypothesis Testing**\n",
        "Let's use Pearson's correlation coefficient to test this hypothesis.\n",
        "\n",
        "**Hypothesis 2 (H2):**\n",
        "There is a significant difference in average gross revenue between movies directed by Christopher Nolan and Steven Spielberg\n",
        "\n",
        "**Step 1: Formulate Hypotheses**\n",
        "\n",
        "**Null Hypothesis (H0):** The average gross revenue of movies directed by Christopher Nolan is equal to that of movies directed by Steven Spielberg.\n",
        "\n",
        "**Alternative Hypothesis (H1):** The average gross revenue of movies directed by Christopher Nolan is different from that of movies directed by Steven Spielberg.\n",
        "\n",
        "**Step 2: Perform Hypothesis Testing**\n",
        "Let's use an independent t-test to test this hypothesis.\n",
        "\n",
        "***Hypothesis 3 (H3): ***\n",
        "Movies with a higher IMDb rating have a longer duration\n",
        "\n",
        "**Step 1: Formulate Hypotheses**\n",
        "\n",
        "**Null Hypothesis (H0):** There is no correlation between IMDb rating and duration.\n",
        "\n",
        "**Alternative Hypothesis (H1):** There is a positive correlation between IMDb rating and duration.\n",
        "Step 2: Perform Hypothesis Testing\n",
        "Let's use Pearson's correlation coefficient to test this hypothesis."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Null Hypothesis (H0):** There is no significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg.\n",
        "\n",
        "• **Alternative Hypothesis (H1):** There is a significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg.\n",
        "\n",
        "This hypothesis will guide us in conducting statistical tests to determine if there exists a significant difference in revenue between movies directed by Christopher Nolan and those directed by Steven Spielberg.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Generate hypothetical data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "\n",
        "# Assume gross revenues (in millions) for movies by Christopher Nolan\n",
        "nolan_revenues = np.random.normal(loc=150, scale=30, size=30)  # Mean 150, SD 30, 30 samples\n",
        "\n",
        "# Assume gross revenues (in millions) for movies by Steven Spielberg\n",
        "spielberg_revenues = np.random.normal(loc=140, scale=25, size=30)  # Mean 140, SD 25, 30 samples\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_statistic, p_value = stats.ttest_ind(nolan_revenues, spielberg_revenues)\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in average gross revenue.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in average gross revenue.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine if there is a significant difference in the average revenue generated by movies directed by Christopher Nolan compared to movies directed by Steven Spielberg, you can perform an independent samples t-test. This test is appropriate when comparing the means of two independent groups (in this case, movies directed by Nolan vs. Spielberg) to assess whether there is evidence to reject the null hypothesis of equal means.\n",
        "\n",
        "***Here's a step-by-step outline of how you can perform this test:***\n",
        "\n",
        "• **Define Hypotheses:**\n",
        "\n",
        "• **Null Hypothesis (H0):** There is no significant difference in the average revenue between movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "\n",
        "• **Alternative Hypothesis (H1):** There is a significant difference in the average revenue between movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "\n",
        "• **Collect Data:**\n",
        "Gather revenue data for movies directed by Christopher Nolan and movies directed by Steven Spielberg.\n",
        "\n",
        "***Assumptions:***\n",
        "\n",
        "**Independent samples:** The revenue data for movies directed by Nolan and Spielberg are independent of each other.\n",
        "\n",
        "**Normality:** Each group's revenue data should be approximately normally distributed.\n",
        "\n",
        "**Equal variance:** The variances of the two groups (Nolan's movies and Spielberg's movies) should be equal.\n",
        "Perform the t-test:\n",
        "\n",
        "Calculate the t-statistic and corresponding p-value using statistical software or programming languages like Python (using libraries such as scipy.stats).\n",
        "\n",
        "***Interpret Results:***\n",
        "\n",
        "If the p-value is less than a chosen significance level (commonly 0.05), you reject the null hypothesis, indicating that there is a significant difference in average revenue between movies directed by Nolan and Spielberg.\n",
        "If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting no significant difference in average revenue between the two directors' movies.\n",
        "Let me know if you need assistance with the actual implementation of this test in Python or any other statistical details!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I recommended the independent samples t-test for hypothesis test 1 (comparing the average revenue of movies directed by Christopher Nolan and Steven Spielberg) for several reasons:\n",
        "\n",
        "• **Comparison of Means:** The t-test is suitable when comparing the means of two independent groups, which aligns perfectly with our scenario of comparing revenue between two different directors' movies.\n",
        "\n",
        "• **Assumption of Normality:** While it's ideal for the data to be normally distributed within each group, the t-test is robust against moderate departures from normality, especially with larger sample sizes. This assumption is generally reasonable for revenue data.\n",
        "\n",
        "• **Assumption of Equal Variances:** The t-test assumes that the variances of the two groups (movies directed by Nolan and Spielberg) are equal. This assumption can be checked using statistical tests like Levene's test or by visual inspection of the data.\n",
        "\n",
        "• **Interpretability:** The t-test provides a straightforward interpretation of results, specifically whether there is a statistically significant difference in means between the two groups (directors' movies).\n",
        "\n",
        "• **Widely Accepted:** The t-test is a widely used and accepted method for comparing means in statistical analysis, making it appropriate for hypothesis testing in many research contexts.\n",
        "\n",
        "If you have any specific concerns or considerations regarding the assumptions or applicability of the t-test to your dataset, feel free to ask for further clarification or assistance!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Hypothesis (Hypothesis Statement 2):**\n",
        "There is a significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "• **Null Hypothesis (H₀):**\n",
        "There is no significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "• **Alternate Hypothesis (H₁):**\n",
        "There is a significant difference in the average IMDb ratings of movies directed by Christopher Nolan and Steven Spielberg.\n",
        "\n",
        "These hypotheses suggest that we are testing whether there is a statistically significant difference in IMDb ratings between movies directed by Nolan and Spielberg."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "\n",
        "# Example data (replace with actual box office revenue data)\n",
        "action_movies = [10000000, 15000000, 12000000, 18000000, 9000000]\n",
        "comedy_movies = [8000000, 9500000, 11000000, 8500000, 10500000]\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(action_movies, comedy_movies)\n",
        "\n",
        "# Print results\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in box office revenues between action and comedy movies.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in box office revenues between action and comedy movies.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For hypothesis statement 2, which involves comparing the mean box office revenues of action movies and comedy movies, the appropriate statistical test used to obtain the p-value is the two-sample t-test. This test is chosen because it allows us to compare the means of two independent groups (in this case, action movies and comedy movies) to determine if there is a statistically significant difference between their average box office revenues."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Comparison of Means:** The hypothesis involves comparing the mean box office revenues of two independent groups (action movies and comedy movies). The two-sample t-test is specifically designed for comparing means between two groups.\n",
        "\n",
        "• **Assumption of Normality:** The t-test assumes that the data within each group (box office revenues of action and comedy movies) are approximately normally distributed. This assumption is reasonable for many types of continuous data, such as financial metrics like box office revenues.\n",
        "\n",
        "• **Independence:** The t-test assumes that the observations within each group are independent of each other, which is typically the case in movie box office data where each movie's performance is considered independently of others.\n",
        "\n",
        "• **Parametric Test:** The t-test is a parametric test that provides a robust way to test differences between means when the data meet the assumptions. It is sensitive to differences in means and widely used for comparing continuous variables.\n",
        "\n",
        "Given these reasons, the two-sample t-test is appropriate for hypothesis 2 to determine if there is a statistically significant difference in mean box office revenues between action and comedy movies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• **Null Hypothesis (H₀):** There is no significant difference in audience ratings between movies directed by male directors and movies directed by female directors.\n",
        "\n",
        "• **Alternative Hypothesis (H₁):** There is a significant difference in audience ratings between movies directed by male directors and movies directed by female directors.\n",
        "\n",
        "This hypothesis aims to explore if there's a statistically significant disparity in audience ratings based on the gender of the movie directors."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming sales data for Product A and Product B are stored in arrays or DataFrames\n",
        "# Replace 'data_product_a' and 'data_product_b' with your actual data\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "data_product_a = [10, 12, 15, 8, 11]\n",
        "data_product_b = [13, 16, 14, 9, 12]\n",
        "\n",
        "# Perform independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(data_product_a, data_product_b)\n",
        "\n",
        "# Output the results\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject null hypothesis: There is a significant difference in mean sales.\")\n",
        "else:\n",
        "    print(\"Fail to reject null hypothesis: There is no significant difference in mean sales.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Comparing Means (Continuous Data):***\n",
        "\n",
        "• **Student's t-test:** Used to compare the means of two groups.\n",
        "\n",
        "• **ANOVA (Analysis of Variance):** Used to compare means of more than two groups.\n",
        "\n",
        "• **Paired t-test:** Used when comparing means of the same group under different conditions.\n",
        "\n",
        "***Comparing Proportions (Categorical Data):***\n",
        "\n",
        "• **Chi-square test:** Used to determine if there is a significant association between categorical variables.\n",
        "\n",
        "• **Fisher's exact test:** Similar to the chi-square test but used when sample sizes are small.\n",
        "\n",
        "***Regression Analysis:***\n",
        "\n",
        "• **Linear regression:** Used to assess the relationship between one dependent (continuous) variable and one or more independent variables.\n",
        "Logistic regression: Used when the dependent variable is categorical (binary or multinomial).\n",
        "\n",
        "• **Non-parametric Tests:**\n",
        "Mann-Whitney U test: Non-parametric alternative to the t-test for comparing two independent groups.\n",
        "\n",
        "• **Kruskal-Wallis test:** Non-parametric alternative to ANOVA for comparing more than two independent groups.\n",
        "\n",
        "***Choosing the Test:***\n",
        "\n",
        "• **Nature of Data:** Determine if your data is continuous or categorical.\n",
        "\n",
        "• **Number of Groups:** Decide if you are comparing two groups, more than two groups, or multiple variables simultaneously.\n",
        "Specific Hypothesis: Tailor the test to match the specific hypothesis statement and data characteristics."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***If Hypothesis 3 involves comparing means:***\n",
        "\n",
        "• **Scenario:** You want to test if there is a significant difference in mean customer satisfaction scores between two different service models (continuous data).\n",
        "\n",
        "• **Statistical Test:** Use a two-sample t-test if comparing two groups, or ANOVA if comparing more than two groups.\n",
        "\n",
        "***If Hypothesis 3 involves comparing proportions:***\n",
        "\n",
        "• ***Scenario:*** You want to determine if there is a significant difference in the proportion of customers who prefer product A versus product B (categorical data).\n",
        "\n",
        "• **Statistical Test:** Chi-square test or Fisher's exact test could be appropriate depending on sample sizes and assumptions.\n",
        "\n",
        "***If Hypothesis 3 involves relationship or correlation:***\n",
        "\n",
        "**Scenario:** You want to examine if there is a significant linear relationship between advertising spending and sales revenue (continuous data).\n",
        "Statistical Test: Pearson correlation coefficient or linear regression analysis would be suitable."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Mean Imputation***\n",
        "\n",
        "• **Description:** This technique involves replacing missing values with the mean (average) value of the observed data in the column.\n",
        "\n",
        "• **Use Case:** Suitable for numerical data without significant outliers.\n",
        "\n",
        "• **Justification:**It preserves the mean of the dataset and is simple to implement. However, it can distort the variance and correlation structures.\n",
        "\n",
        "***Median Imputation***\n",
        "\n",
        "• **Description:** Missing values are replaced with the median value of the observed data in the column.\n",
        "\n",
        "• **Use Case:** Preferred for numerical data with outliers or skewed distributions.\n",
        "\n",
        "• **Justification:** Median imputation is robust to outliers and maintains the central tendency without being influenced by extreme values.\n",
        "\n",
        "**Most Frequent Imputation**\n",
        "\n",
        "• **Description:** This method replaces missing values with the most frequently occurring value (mode) in the column.\n",
        "\n",
        "• **Use Case:** Useful for both numerical and categorical data where a single value dominates.\n",
        "\n",
        "• **Justification:** It is effective in maintaining the mode of the dataset, especially for categorical features.\n",
        "\n",
        "**Constant Value Imputation**\n",
        "\n",
        "• **Description:**Missing values are replaced with a specified constant value, such as zero or a placeholder category.\n",
        "\n",
        "• **Use Case:** When there is a meaningful constant value that can be used, such as zero in financial datasets or a specific category in categorical data.\n",
        "\n",
        "• **Justification:** It ensures that all missing values are filled with a contextually appropriate constant, avoiding the introduction of biases from statistical measures like mean or median.\n",
        "\n",
        "**Forward Fill and Backward Fill**\n",
        "\n",
        "• **Description:** Missing values are filled using the previous or next observed value in the column, respectively.\n",
        "\n",
        "• **Use Case:** Time-series data where the assumption is that the missing value is similar to the previous or next value.\n",
        "\n",
        "• **Justification:** Preserves the temporal structure of the data, which is crucial in time-series analysis.\n",
        "\n",
        "**Interpolation**\n",
        "\n",
        "• **Description:** Estimates missing values by interpolating between the known values before and after the missing value.\n",
        "\n",
        "• **Use Case:** Suitable for numerical time-series data.\n",
        "\n",
        "• **Justification:** Provides a smooth transition between data points, maintaining the overall trend and pattern in the data.\n",
        "\n",
        "**K-Nearest Neighbors (KNN) Imputation**\n",
        "\n",
        "• **Description:** Uses the k-nearest neighbors algorithm to impute missing values based on the similarity of other observations.\n",
        "\n",
        "• **Use Case:** Numerical and categorical data where the assumption is that similar data points have similar values.\n",
        "\n",
        "• **Justification:** Captures the underlying patterns and correlations in the data, leading to more accurate imputations.\n",
        "Regression Imputation"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Sample DataFrame for demonstration purposes\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100],\n",
        "    'B': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 200]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Removing Outliers\n",
        "Technique: Directly removing the rows that contain outliers.\n",
        "\n",
        "Reason: This technique is useful when outliers are likely to be data errors or anomalies that do not represent the underlying data distribution. By removing them, we ensure that these anomalies do not skew our analysis.\n",
        "\n",
        "2. Capping (Winsorizing)\n",
        "Technique: Limiting extreme values at specified percentiles.\n",
        "\n",
        "Reason: This method is used when we want to reduce the impact of extreme values without completely removing them. Capping ensures that extreme outliers are brought within a certain range, thus minimizing their influence on statistical measures like mean and variance.\n",
        "\n",
        "3. Transformation\n",
        "Technique: Applying a mathematical transformation, such as log transformation, to reduce skewness.\n",
        "\n",
        "Reason: Transformations can help make the data more normally distributed, especially when the data is positively skewed. This is particularly useful for techniques that assume normality, such as certain regression models and hypothesis tests.\n",
        "\n",
        "4. Imputation\n",
        "Technique: Replacing outliers with a central tendency measure, such as the median.\n",
        "\n",
        "Reason: This method retains all data points but reduces the influence of extreme values by replacing them with a less extreme value (e.g., median). This is useful when outliers are legitimate values but still disproportionately affect the analysis.\n",
        "\n",
        "5. Clustering Methods (DBSCAN)\n",
        "Technique: Using clustering algorithms like DBSCAN to identify outliers as points that do not belong to any cluster.\n",
        "\n",
        "Reason: Clustering methods help in identifying outliers based on the distribution and density of the data. This technique is effective when outliers do not fit well into the overall data pattern. DBSCAN, in particular, can identify outliers without making any assumptions about the distribution of data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Sample DataFrame with categorical columns\n",
        "data = {\n",
        "    'Category': ['A', 'B', 'C', 'A', 'C'],\n",
        "    'Status': ['Active', 'Inactive', 'Active', 'Active', 'Inactive']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the original DataFrame\n",
        "print(\"Original DataFrame:\\n\", df)\n",
        "\n",
        "# Perform Label Encoding on 'Category'\n",
        "label_encoder = LabelEncoder()\n",
        "df['Category_LabelEncoded'] = label_encoder.fit_transform(df['Category'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In data preprocessing, various categorical encoding techniques are used to convert categorical variables into numerical representations that machine learning algorithms can process effectively. Here are some commonly used techniques and their rationales:\n",
        "\n",
        "**Label Encoding:**\n",
        "\n",
        "Technique: Assigns a unique integer to each category in a categorical variable.\n",
        "\n",
        "Rationale: Suitable for ordinal data where there is an inherent order among categories (e.g., low, medium, high). Helps in preserving ordinal relationships.\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "\n",
        "Technique: Creates binary columns for each category and assigns a 1 or 0 (True/False) to indicate the presence of a category in each observation.\n",
        "\n",
        "Rationale: Ideal for nominal data without an inherent order (e.g., colors, countries). Prevents ordinal relationships from being inferred and avoids bias in models.\n",
        "\n",
        "**Dummy Encoding:**\n",
        "\n",
        "Technique: Similar to One-Hot Encoding but drops one of the binary columns to avoid multicollinearity in linear models.\n",
        "\n",
        "Rationale: Useful when using linear models where multicollinearity (high correlation among predictors) can affect model performance.\n",
        "\n",
        "**Effect Encoding:**\n",
        "\n",
        "Technique: Represents each level of a categorical variable relative to a chosen reference level.\n",
        "\n",
        "Rationale: Useful in regression models where you want to interpret coefficients relative to a baseline level. It can handle multicollinearity and provide meaningful interpretation.\n",
        "\n",
        "**Binary Encoding:**\n",
        "\n",
        "Technique: Converts each category into binary code, then splits the binary digits into separate columns.\n",
        "\n",
        "Rationale: Reduces the number of columns compared to One-Hot Encoding while still capturing the uniqueness of each category. It's efficient for high-cardinality categorical variables.\n",
        "\n",
        "**Hashing Encoding:**\n",
        "\n",
        "Technique: Hashes categorical values into a specified number of bins and assigns each category to a bin.\n",
        "\n",
        "Rationale: Useful when dealing with very large categorical variables to reduce memory usage and dimensionality.\n",
        "\n",
        "**Selection Criteria:**\n",
        "\n",
        "Nature of Data: Choose based on whether the categorical variable is ordinal or nominal.\n",
        "\n",
        "Model Requirements: Consider the model's sensitivity to encoding methods (e.g., linear models and multicollinearity).\n",
        "\n",
        "Performance: Evaluate encoding techniques based on how they impact model performance, especially in terms of accuracy, interpretability, and computational efficiency.\n",
        "\n",
        "By understanding the nature of your categorical data and the requirements of your machine learning model, you can select the most appropriate encoding technique to preprocess your data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define a mapping dictionary for common English contractions\n",
        "contraction_mapping = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions in a text using the mapping dictionary\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "    \"\"\"\n",
        "    Expand contractions in a piece of text using a mapping dictionary.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text containing contractions.\n",
        "    - contraction_mapping (dict): Mapping dictionary for expanding contractions.\n",
        "\n",
        "    Returns:\n",
        "    - str: Text with expanded contractions.\n",
        "    \"\"\"\n",
        "    # Regular expression pattern to find contractions\n",
        "    contraction_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
        "                                     flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        \"\"\"\n",
        "        Function to expand a single contraction match using the mapping dictionary.\n",
        "\n",
        "        Args:\n",
        "        - contraction (str): Single contraction match.\n",
        "\n",
        "        Returns:\n",
        "        - str: Expanded form of the contraction.\n",
        "        \"\"\"\n",
        "        match = contraction.group(0)\n",
        "        expanded_contraction = contraction_mapping.get(match) \\\n",
        "            if contraction_mapping.get(match) \\\n",
        "            else contraction_mapping.get(match.lower())\n",
        "        return expanded_contraction\n",
        "\n",
        "    # Replace contractions in text using the expand_match function\n",
        "    expanded_text = contraction_pattern.sub(expand_match, text)\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage\n",
        "text_with_contractions = \"I can't believe we've made it!\"\n",
        "expanded_text = expand_contractions(text_with_contractions, contraction_mapping)\n",
        "print(\"Original Text:\", text_with_contractions)\n",
        "print(\"Text after Expanding Contractions:\", expanded_text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Example text with mixed cases\n",
        "text = \"Hello World! This Is a Sample Text With MIXED Cases.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "# Print the original and lowercased text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lowercased Text:\", lowercased_text)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import re\n",
        "\n",
        "# Example text with punctuations\n",
        "text = \"Hello, World! This is a sample text with punctuations.\"\n",
        "\n",
        "# Define a function to remove punctuations using regex\n",
        "def remove_punctuations(text):\n",
        "    # Define regex pattern for punctuations\n",
        "    pattern = r'[^\\w\\s]'  # Matches any character that is not alphanumeric or whitespace\n",
        "\n",
        "    # Use re.sub to substitute punctuations with an empty string\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove punctuations from the text\n",
        "clean_text = remove_punctuations(text)\n",
        "\n",
        "# Print the original and cleaned text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Text without Punctuations:\", clean_text)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Define the regex pattern for URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Remove URLs from the text using the sub method\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "# Example usage\n",
        "text_with_urls = \"Check out this cool website: https://example.com. Also visit www.anotherexample.com\"\n",
        "clean_text = remove_urls(text_with_urls)\n",
        "print(\"Text with URLs:\", text_with_urls)\n",
        "print(\"Text without URLs:\", clean_text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # Load stopwords from NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the filtered words back into a single string\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "# Example usage\n",
        "text_with_stopwords = \"This is a sample sentence, demonstrating the removal of stopwords.\"\n",
        "clean_text = remove_stopwords(text_with_stopwords)\n",
        "print(\"Text with stopwords:\", text_with_stopwords)\n",
        "print(\"Text without stopwords:\", clean_text)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespace(text):\n",
        "    # Remove leading and trailing white spaces\n",
        "    return text.strip()\n",
        "\n",
        "# Example usage\n",
        "text_with_whitespace = \"   This is a sample sentence with white spaces.    \"\n",
        "clean_text = remove_whitespace(text_with_whitespace)\n",
        "print(\"Text with white spaces:\", repr(text_with_whitespace))  # repr() to show white spaces\n",
        "print(\"Text without white spaces:\", repr(clean_text))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "movies = [\n",
        "    {\"title\": \"3%\", \"description\": \"A dystopian future where the elite 3% escape poverty.\"},\n",
        "    {\"title\": \"7:19\", \"description\": \"Survivors trapped in Mexico City after an earthquake.\"},\n",
        "    {\"title\": \"23:59\", \"description\": \"Soldiers face supernatural forces on a jungle island.\"},\n",
        "    {\"title\": \"9\", \"description\": \"Rag-doll robots fight for survival in a post-apocalyptic world.\"},\n",
        "    {\"title\": \"21\", \"description\": \"Brilliant students become blackjack experts in Las Vegas.\"},\n",
        "    {\"title\": \"46\", \"description\": \"A genetics professor experiments to save his sister.\"},\n",
        "    {\"title\": \"122\", \"description\": \"A couple faces horror in a hospital after an accident.\"},\n",
        "    {\"title\": \"187\", \"description\": \"A teacher faces new challenges after leaving New York City.\"},\n",
        "    {\"title\": \"706\", \"description\": \"A psychiatrist investigates a psychic patient's condition.\"},\n",
        "    {\"title\": \"1920\", \"description\": \"Architect encounters supernatural forces in a castle.\"},\n",
        "    {\"title\": \"1922\", \"description\": \"A farmer's confession triggers horrific events in a town.\"},\n",
        "    {\"title\": \"1983\", \"description\": \"Law student and detective uncover a hidden conspiracy.\"},\n",
        "    {\"title\": \"1994\", \"description\": \"Examines Mexican politics during a pivotal year.\"},\n",
        "    {\"title\": \"2,215\", \"description\": \"Rock star's charity run across Thailand.\"},\n",
        "    {\"title\": \"3022\", \"description\": \"Astronauts battle isolation on a stranded space station.\"},\n",
        "    {\"title\": \"Oct-01\", \"description\": \"Murder investigation during Nigeria's struggle for independence.\"},\n",
        "    {\"title\": \"Feb-09\", \"description\": \"Family dynamics and Alzheimer's affect relationships.\"},\n",
        "    {\"title\": \"22-Jul\", \"description\": \"Norway's response to devastating terror attacks.\"},\n",
        "    {\"title\": \"15-Aug\", \"description\": \"Mumbai chawl's unity on India's Independence Day.\"},\n",
        "    {\"title\": \"89\", \"description\": \"Chronicles Arsenal's championship victory in 1989.\"},\n",
        "    {\"title\": \"Kuch Bheege Alfaaz\", \"description\": \"Two strangers form a deep online friendship.\"},\n",
        "    {\"title\": \"Goli Soda 2\", \"description\": \"Characters strive for better lives amid corruption.\"},\n",
        "    {\"title\": \"Maj Rati Keteki\", \"description\": \"Writer reunites with hometown, evoking memories.\"},\n",
        "    {\"title\": \"Mayurakshi\", \"description\": \"Middle-aged divorcee confronts past emotions.\"},\n",
        "    {\"title\": \"SAINT SEIYA: Knights of the Zodiac\", \"description\": \"Knights protect Athena amid prophecy.\"},\n",
        "    {\"title\": \"(T)ERROR\", \"description\": \"Real-life glimpse into FBI counterterrorism operations.\"},\n",
        "    {\"title\": \"(Un)Well\", \"description\": \"Explores commercialized promises in the wellness industry.\"},\n",
        "    {\"title\": \"#Alive\", \"description\": \"Surviving a zombie apocalypse in urban isolation.\"}\n",
        "]\n",
        "\n",
        "# Accessing each movie and its description\n",
        "for movie in movies:\n",
        "    print(f\"Title: {movie['title']}\")\n",
        "    print(f\"Description: {movie['description']}\")\n",
        "    print()  # Blank line for separation"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import spacy\n",
        "\n",
        "# Load the SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"I love programming. It's very fulfilling!\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Word tokenization\n",
        "word_tokens = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "\n",
        "# Sentence tokenization\n",
        "sentence_tokens = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"happily\", \"cats\", \"fishing\", \"fished\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided comprehensive example of text preprocessing, I primarily used lemmatization along with part-of-speech (POS) tagging. Here’s why lemmatization was chosen and the rationale behind the combination of techniques used:\n",
        "\n",
        "**Why Lemmatization?**\n",
        "\n",
        "**Context-Awareness:**\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (lemma), considering the context provided by POS tags. For example, \"better\" is lemmatized to \"good\" when identified as an adjective, and \"running\" is lemmatized to \"run\" when identified as a verb.\n",
        "\n",
        "**Accuracy:**\n",
        "\n",
        "Lemmatization is generally more accurate than stemming because it produces valid words that retain their meaning. This is crucial for applications where understanding the semantics of the text is important, such as in sentiment analysis, machine translation, and text summarization.\n",
        "\n",
        "**Why Part-of-Speech Tagging?**\n",
        "\n",
        "**Improved Lemmatization:**\n",
        "POS tagging provides the necessary context to perform accurate lemmatization. Different forms of a word (e.g., \"run\" as a noun and \"run\" as a verb) are lemmatized correctly based on their POS tags.\n",
        "Other Techniques Used\n",
        "\n",
        "**Text Cleaning:**\n",
        "\n",
        "Lowercasing: Converts all text to lowercase to ensure uniformity.\n",
        "Removing Numbers and Punctuation: Simplifies the text and removes noise that might not contribute to the analysis.\n",
        "Removing Extra Whitespace: Ensures that the text is clean and uniformly spaced.\n",
        "\n",
        "**Tokenization:**\n",
        "\n",
        "Word Tokenization: Splits the text into individual words, which is a fundamental step before any further processing.\n",
        "\n",
        "Stop Word Removal:\n",
        "Removes common words that do not carry significant meaning, such as \"the\", \"is\", \"in\", etc., to reduce the dimensionality of the data and focus on more meaningful words.\n",
        "Summary\n",
        "Lemmatization was chosen for its accuracy and context-awareness, which are critical for understanding the semantics of text.\n",
        "POS Tagging was used to improve the accuracy of lemmatization by providing context.\n",
        "Other preprocessing steps like text cleaning, tokenization, and stop word removal were included to prepare the text comprehensively for further analysis and modeling.\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(word_tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"I love programming.\",\n",
        "    \"Programming is fun.\",\n",
        "    \"I love learning new things.\"\n",
        "]\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"BoW Vector:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bag of Words (BoW)\n",
        "Used For:\n",
        "\n",
        "Simpler NLP tasks where the context and semantics of words are less important, such as text classification with a small dataset.\n",
        "Initial feature extraction to get a quick overview of the most frequent words.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "Used For:\n",
        "\n",
        "Tasks where the relative importance of words matters, such as document classification and information retrieval.\n",
        "Improving upon BoW by reducing the weight of commonly occurring words\n",
        " Word Embeddings (Word2Vec)\n",
        "Used For:\n",
        "\n",
        "Capturing semantic relationships between words for tasks like word similarity, sentiment analysis, and more complex NLP applications.\n",
        "Sentence Embeddings (BERT)\n",
        "Used For:\n",
        "\n",
        "Tasks requiring context-aware understanding, such as text classification, question answering, and other advanced NLP applications.\n",
        "Summary of Choices\n",
        "In summary, the choice of text vectorization technique depends on the specific requirements of your NLP task:\n",
        "\n",
        "BoW and TF-IDF: Simple, interpretable, and computationally inexpensive. Suitable for basic text classification and retrieval tasks.\n",
        "Word Embeddings: Capture semantic relationships and are suitable for tasks requiring word-level understanding.\n",
        "Sentence Embeddings: Provide deep contextual understanding, ideal for complex tasks that need context-aware representations.\n",
        "By choosing the appropriate vectorization technique, you can better prepare your text data for subsequent machine learning or NLP tasks, ensuring that the models you build are as effective and accurate as possible."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': np.random.rand(100),\n",
        "    'Feature2': np.random.rand(100),\n",
        "    'Feature3': np.random.rand(100),\n",
        "    'Feature4': np.random.rand(100)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some correlation for demonstration\n",
        "df['Feature2'] = df['Feature1'] + np.random.normal(0, 0.1, 100)\n",
        "df['Feature3'] = df['Feature1'] * 2 + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Check for low variance features\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': np.random.rand(100),\n",
        "    'Feature2': np.random.rand(100),\n",
        "    'Feature3': np.random.rand(100),\n",
        "    'Feature4': np.random.rand(100)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some low variance for demonstration\n",
        "df['Feature5'] = 1  # Zero variance feature\n",
        "df['Feature6'] = df['Feature1'] + 1e-9 * np.random.rand(100)  # Very low variance\n",
        "\n",
        "# Apply Variance Threshold\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "selector.fit(df)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = df.columns[selector.get_support()]\n",
        "df_selected = df[selected_features]\n",
        "\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Variance Thresholding\n",
        "Why Used:\n",
        "\n",
        "To remove features with low variance that do not contribute much information to the model. Features with zero or near-zero variance can be considered uninformative as they do not vary much between different samples.\n",
        "This method is simple and quick to implement.\n",
        "2. Feature Importance from Models\n",
        "Why Used:\n",
        "\n",
        "Tree-based models (like Random Forest) can naturally provide feature importance scores based on how useful each feature is in reducing impurity.\n",
        "This method helps in identifying which features are most influential in making predictions.\n",
        "3. Recursive Feature Elimination (RFE)\n",
        "Why Used:\n",
        "\n",
        "RFE is a wrapper method that recursively removes the least important features based on the model’s performance, helping to identify a subset of features that contribute most to the model’s accuracy.\n",
        "This method is iterative and provides a ranking of features, making it more thorough\n",
        "4. Statistical Tests (Chi-Squared Test)\n",
        "Why Used:\n",
        "\n",
        "Statistical tests can identify which features have the strongest relationship with the target variable. Chi-squared tests are particularly useful for categorical features.\n",
        "This method is based on statistical significance, which can provide a quantitative basis for feature selection\n",
        "5. Principal Component Analysis (PCA)\n",
        "Why Used:\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms the features into a set of linearly uncorrelated components, preserving as much variance as possible.\n",
        "This method helps in reducing the dimensionality of the data while retaining most of the important information\n",
        "\n",
        "In the examples provided, several feature selection methods were demonstrated, each chosen for its specific advantages in reducing dimensionality, improving model performance, and mitigating overfitting. Here’s a summary of the methods used and the rationale behind their selection:\n",
        "\n",
        "1. Variance Thresholding\n",
        "\n",
        "Why Used:\n",
        "\n",
        "To remove features with low variance that do not contribute much information to the model. Features with zero or near-zero variance can be considered uninformative as they do not vary much between different samples.\n",
        "This method is simple and quick to implement.\n",
        "\n",
        "Tree-based models (like Random Forest) can naturally provide feature importance scores based on how useful each feature is in reducing impurity.\n",
        "This method helps in identifying which features are most influential in making predictions.\n",
        "\n",
        "RFE is a wrapper method that recursively removes the least important features based on the model’s performance, helping to identify a subset of features that contribute most to the model’s accuracy.\n",
        "This method is iterative and provides a ranking of features, making it more thorough.\n",
        "\n",
        "\n",
        "Statistical tests can identify which features have the strongest relationship with the target variable. Chi-squared tests are particularly useful for categorical features.\n",
        "This method is based on statistical significance, which can provide a quantitative basis for feature selection.\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms the features into a set of linearly uncorrelated components, preserving as much variance as possible.\n",
        "This method helps in reducing the dimensionality of the data while retaining most of the important information.\n",
        "\n",
        "Regularization methods like Lasso (L1) regression add a penalty for large coefficients and can help in feature selection by shrinking less important feature coefficients to zero.\n",
        "This method is effective in selecting a sparse set of features and reducing model complexity.\n",
        "Summary of Feature Selection Methods and Their Use Cases\n",
        "\n",
        "Variance Thresholding: Quickly remove features with little to no variance.\n",
        "\n",
        "Feature Importance from Models: Identify and prioritize influential features.\n",
        "\n",
        "Recursive Feature Elimination (RFE): Iteratively select features by recursively considering smaller sets.\n",
        "\n",
        "Statistical Tests: Select features based on statistical significance.\n",
        "\n",
        "Principal Component Analysis (PCA): Reduce dimensionality while preserving variance.\n",
        "\n",
        "Regularization (Lasso Regression): Select a sparse set of features by penalizing large coefficients."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature Importance from Tree-Based Models\n",
        "Tree-based models like Random Forest or Gradient Boosting Machines (GBM) provide a feature importance score based on how much each feature contributes to reducing the impurity in the nodes of the trees. Features with higher importance scores are considered more influential in making predictions.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "If a Random Forest model indicates that Feature1 has the highest importance score, it suggests that Feature1 provides the most predictive power for the target variable compared to other features.\n",
        "2. Coefficient Magnitudes from Linear Models\n",
        "Linear models like Logistic Regression or Linear Regression provide coefficients for each feature, indicating the strength and direction of their relationship with the target variable. Larger magnitude coefficients suggest stronger influence on the target variable.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "In a Logistic Regression model, if the coefficient for Feature2 is significantly positive, it indicates that an increase in Feature2 positively impacts the predicted outcome.\n",
        "3. Statistical Tests (e.g., Chi-Squared Test)\n",
        "Statistical tests such as Chi-Squared test for feature selection in categorical variables provide a statistical significance measure. Features with higher test statistics or lower p-values are considered more important as they exhibit stronger associations with the target variable.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "A Chi-Squared test might indicate that Feature3 is highly significant (low p-value), suggesting it has a strong relationship with the target variable in a categorical analysis context.\n",
        "4. Principal Component Analysis (PCA)\n",
        "PCA does not directly provide feature importance scores but identifies principal components that explain the maximum variance in the data. Features that contribute more to these principal components can be considered more important in capturing the overall variability of the dataset.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "After performing PCA, if Feature4 contributes significantly to the variance explained by the first principal component, it suggests that Feature4 is crucial in describing the underlying structure of the data.\n",
        "5. Regularization (e.g., Lasso Regression)\n",
        "Regularization techniques like Lasso Regression penalize the coefficients of less important features, effectively shrinking them towards zero. Features with non-zero coefficients after regularization are considered important.\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "If Lasso Regression retains non-zero coefficients for Feature5 and Feature6, it indicates that these features are essential in predicting the outcome, despite potential collinearity or redundancy.\n",
        "General Considerations for Feature Importance:\n",
        "Domain Knowledge: Understanding the context and domain-specific relevance of features can provide insights into their importance.\n",
        "Collinearity: Features that are highly correlated with the target variable but less with each other might be more informative.\n",
        "Iterative Evaluation: Combining multiple feature selection methods and evaluating the consistency of results across different approaches can enhance confidence in feature importance assessments.\n",
        "In practice, feature importance is a critical step in model interpretability and performance optimization. It helps in focusing on relevant features, reducing model complexity, and improving generalization to new data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40],\n",
        "    'Feature2': [0.1, 0.5, 0.2, 0.3]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for visualization\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Feature1': [10, 20, 30, 40],\n",
        "    'Feature2': [0.1, 0.5, 0.2, 0.3]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert back to DataFrame for visualization\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "print(scaled_df)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Standardization (Z-score Normalization)\n",
        "Method Used:\n",
        "\n",
        "StandardScaler from sklearn.preprocessing\n",
        "Why Use Standardization:\n",
        "\n",
        "Standardization transforms data to have a mean of 0 and a standard deviation of 1, assuming the data follows a Gaussian distribution.\n",
        "It is effective when the features in your dataset have varying scales and when the algorithm you are using assumes normally distributed data, such as SVMs or linear regression.\n",
        "Standardization also preserves outliers, which can be important in certain modeling scenarios where outlier information is significant.\n",
        "2. Min-Max Scaling (Normalization)\n",
        "Method Used:\n",
        "\n",
        "MinMaxScaler from sklearn.preprocessing\n",
        "Why Use Min-Max Scaling:\n",
        "\n",
        "Min-Max scaling transforms data to a fixed range, typically [0, 1] or [-1, 1].\n",
        "It preserves the original distribution of the data and is suitable for algorithms like neural networks or algorithms that require features to be within a specific range.\n",
        "Min-Max scaling is sensitive to outliers, so it should be used when the dataset does not contain outliers that could significantly affect the scaling.\n",
        "Choice of Scaling Method\n",
        "Standardization (StandardScaler) is often preferred when the distribution of data is approximately Gaussian and when the algorithm is not sensitive to the range of features but to their distribution.\n",
        "Min-Max Scaling (MinMaxScaler) is useful when you need to scale features to a specific range and when your data does not contain outliers that could distort the scaling process.\n",
        "Considerations\n",
        "Impact on Algorithm: Different scaling methods can impact the performance of algorithms differently. It’s important to experiment and evaluate which scaling method works best for your specific dataset and machine learning model.\n",
        "Handling Outliers: If your dataset contains outliers, consider using robust scaling methods like RobustScaler or standardization (StandardScaler) which are less affected by outliers compared to min-max scaling."
      ],
      "metadata": {
        "id": "IusM8BP9jnnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Curse of Dimensionality:\n",
        "As the number of features (dimensions) increases, the amount of data needed to generalize accurately grows exponentially. This can lead to increased computational complexity and memory requirements.\n",
        "2. Improved Model Performance:\n",
        "High-dimensional data often contains redundant or irrelevant features that can degrade the performance of machine learning models. Dimensionality reduction can mitigate this by focusing on the most informative features, thereby improving model accuracy and efficiency.\n",
        "3. Overfitting Prevention:\n",
        "High-dimensional datasets are prone to overfitting, where a model learns noise and specific details of the training data rather than the underlying patterns. Dimensionality reduction helps in reducing overfitting by simplifying the model and making it more generalizable to unseen data.\n",
        "4. Visualization and Interpretability:\n",
        "Dimensionality reduction techniques like PCA (Principal Component Analysis) can transform high-dimensional data into lower-dimensional representations that are easier to visualize. This enables better understanding of the data and its relationships.\n",
        "5. Computational Efficiency:\n",
        "Reduced dimensionality simplifies the computational burden for many algorithms, making model training and prediction faster and more efficient.\n",
        "Common Techniques for Dimensionality Reduction:\n",
        "Principal Component Analysis (PCA): Linear transformation technique that identifies the directions (principal components) of maximum variance in high-dimensional data.\n",
        "\n",
        "Linear Discriminant Analysis (LDA): Supervised dimensionality reduction technique that finds the linear combinations of features that best separate different classes.\n",
        "\n",
        "t-Distributed Stochastic Neighbor Embedding (t-SNE): Non-linear technique for embedding high-dimensional data into a lower-dimensional space, often used for visualization.\n",
        "\n",
        "Autoencoders: Neural network-based approach for learning efficient representations of data by compressing it into a lower-dimensional space."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['species'] = y\n",
        "print(df.head())\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio:\", explained_variance)\n",
        "\n",
        "# Plotting the PCA transformed data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Iris Dataset')\n",
        "plt.colorbar(label='Species')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction. Here’s why PCA was chosen and the benefits it offers:\n",
        "\n",
        "**Why PCA?**\n",
        "\n",
        "**Variance Preservation:**\n",
        "\n",
        "PCA aims to capture the maximum variance in the data with the fewest number of principal components. This means that the new features (principal components) will retain most of the important information from the original dataset.\n",
        "\n",
        "**Simplicity and Efficiency:**\n",
        "\n",
        "PCA is a linear technique that is computationally efficient and relatively simple to implement. It transforms the data into a new coordinate system, making it easier to work with and interpret.\n",
        "\n",
        "**Reduction of Dimensionality:**\n",
        "\n",
        "By reducing the number of features, PCA helps mitigate the curse of dimensionality, which can lead to overfitting and increased computational complexity.\n",
        "\n",
        "**Feature Decorrelation:**\n",
        "\n",
        "PCA generates principal components that are orthogonal (uncorrelated) to each other, which can improve the performance of machine learning algorithms that are sensitive to feature correlations.\n",
        "\n",
        "**Visualization:**\n",
        "\n",
        "For high-dimensional data, PCA can reduce the data to 2 or 3 dimensions, enabling easier visualization and interpretation of the data structure and patterns.\n",
        "When PCA is Appropriate\n",
        "\n",
        "High-Dimensional Data: When dealing with datasets that have many features, especially if many of those features are correlated.\n",
        "\n",
        "Exploratory Data Analysis: To visualize and understand the structure and patterns in high-dimensional data.\n",
        "\n",
        "Preprocessing for Machine Learning: To reduce the number of features before feeding the data into machine learning models, potentially improving performance and reducing overfitting.\n",
        "\n",
        "Example: PCA on the Iris Dataset\n",
        "Here's a recap of the PCA implementation on the Iris dataset:\n",
        "\n",
        "Standardization: Standardize the data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "PCA Transformation: Fit and transform the standardized data to reduce it to 2 principal components.\n",
        "\n",
        "Visualization: Plot the transformed data to visualize the distribution of different species in the reduced feature space.\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the shapes of the splits\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasons for Using an 80-20 Split Ratio:\n",
        "\n",
        "**Sufficient Training Data:**\n",
        "\n",
        "The training set (80%) provides a sufficient amount of data for the model to learn patterns and relationships within the data. More data often leads to better model performance.\n",
        "\n",
        "**Adequate Testing Data:**\n",
        "\n",
        "The testing set (20%) is large enough to evaluate the model's performance effectively. It ensures that the model's performance metrics, such as accuracy or error rate, are reliable and not overly sensitive to the particular data points in the test set.\n",
        "\n",
        "**Balancing Training and Testing:**\n",
        "\n",
        "It strikes a balance between having enough data to train the model effectively and having enough data to assess its performance accurately on unseen data.\n",
        "\n",
        "**Common Practice:**\n",
        "\n",
        "The 80-20 split is a widely accepted standard in machine learning and data science communities, making it easier to compare results across different studies and implementations."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assessing Imbalance in a Dataset\n",
        "\n",
        "Class Distribution:\n",
        "\n",
        "Check the distribution of classes in the target variable. If there is a significant disparity in the number of instances between different classes, the dataset is considered imbalanced.\n",
        "\n",
        "Visual Inspection:\n",
        "\n",
        "Plotting histograms or bar charts of the class labels can provide a visual indication of class distribution. Classes with disproportionately fewer instances compared to others indicate imbalance.\n",
        "\n",
        "Imbalance Ratio:\n",
        "\n",
        "Compute the imbalance ratio, which is the ratio of instances in the minority class to the majority class. For example, if one class has 10% of the instances and another has 90%, the imbalance ratio is 1:9.\n",
        "\n",
        "Why Imbalance Matters\n",
        "\n",
        "Model Bias: Imbalanced datasets can lead to biased models that favor the majority class, as they have more examples to learn from.\n",
        "\n",
        "Performance Metrics: Traditional metrics like accuracy can be misleading on imbalanced datasets, as a model predicting only the majority class can still achieve high accuracy.\n",
        "\n",
        "Cost-Sensitive Learning: In real-world scenarios, misclassifying instances of the minority class (often the class of interest) can be more costly. Thus, it's crucial to account for imbalance to optimize model performance.\n",
        "\n",
        "Example of Imbalanced Dataset:\n",
        "In a medical diagnosis dataset, where positive cases (disease presence) are rare compared to negative cases (disease absence), the dataset is imbalanced. Predicting disease absence accurately might lead to high accuracy but fail to detect positive cases.\n",
        "Conclusion\n",
        "\n",
        "Assessing dataset imbalance involves understanding the distribution of class labels and its implications for model training and evaluation. Techniques such as resampling (oversampling minority class or undersampling majority class) or using class-weighted algorithms can help mitigate imbalance and improve model performance on minority classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Sample DataFrame with imbalanced classes\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'feature2': [0, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
        "    'target': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  # Imbalanced target variable\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Apply RandomOverSampler\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Display resampled data\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=['feature1', 'feature2'])\n",
        "resampled_df['target'] = y_resampled\n",
        "print(\"Resampled DataFrame:\\n\", resampled_df)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SMOTE (Synthetic Minority Over-sampling Technique). Here’s why SMOTE was chosen and the steps involved:\n",
        "\n",
        "**Why SMOTE?**\n",
        "\n",
        "**Synthetic Data Generation:**\n",
        "\n",
        "SMOTE generates synthetic samples for the minority class, rather than simply duplicating existing ones. This helps create a more diverse training set, leading to a better generalization of the model.\n",
        "\n",
        "**Balancing the Dataset:**\n",
        "\n",
        "By creating synthetic examples, SMOTE effectively balances the class distribution, making the model less biased towards the majority class.\n",
        "\n",
        "**Preserving Information:**\n",
        "\n",
        "Unlike random oversampling, which can lead to overfitting due to the repetition of the same data points, SMOTE generates new examples based on feature space similarities, thus preserving the information content and diversity.\n",
        "\n",
        "Example Implementation of SMOTE\n",
        "Step-by-Step Implementation\n",
        "\n",
        "Import Libraries:\n",
        "\n",
        "Import necessary libraries for data handling, preprocessing, and SMOTE.\n",
        "\n",
        "Load and Preprocess the Dataset:\n",
        "\n",
        "Load the dataset, standardize the features, and split it into training and testing sets.\n",
        "\n",
        "Apply SMOTE:\n",
        "\n",
        "Use SMOTE to oversample the minority class in the training set."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset (replace with your actual dataset)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [0, 1, 1, 0, 0],\n",
        "    'target': [0, 0, 0, 1, 1]  # Binary classification\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Choose a Model: Select a suitable machine learning algorithm based on the problem at hand and the characteristics of the dataset.\n",
        "\n",
        "Preprocessed Data: Use the preprocessed and split data (X_train, y_train) for training the model.\n",
        "\n",
        "Train the Model: Fit the model to the training data using .fit() method.\n",
        "\n",
        "Predictions: After fitting the model, make predictions on the testing data (X_test) using .predict() or .predict_proba() methods.\n",
        "\n",
        "Evaluate Performance: Evaluate the model's performance using appropriate metrics such as accuracy, precision, recall, F1-score, etc., on the testing data."
      ],
      "metadata": {
        "id": "YdaPmupkxfBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a model (Logistic Regression in this case)\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on new data (e.g., X_test)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the predicted values\n",
        "print(\"Predicted values:\", y_pred)"
      ],
      "metadata": {
        "id": "wIIy1sWwxXRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a model (Logistic Regression in this case)\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2'],  # Penalty norm\n",
        "    'solver': ['liblinear'],  # Optimization algorithm\n",
        "    'max_iter': [100, 200, 300, 400]  # Maximum number of iterations taken for the solvers to converge\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data: Load the Iris dataset, standardize the features using StandardScaler, and split the data into training and testing sets (X_train, X_test, y_train, y_test).\n",
        "\n",
        "Define the Model: Define the machine learning model (LogisticRegression in this case).\n",
        "\n",
        "Hyperparameter Optimization: Define a parameter grid (param_grid) specifying different values for hyperparameters like C (regularization strength), penalty (norm for regularization), solver (optimization algorithm), and max_iter (maximum number of iterations).\n",
        "\n",
        "GridSearchCV Setup: Initialize GridSearchCV with the model, parameter grid, cross-validation (cv=5), and scoring metric (scoring='accuracy').\n",
        "\n",
        "Fit GridSearchCV: Fit GridSearchCV to the training data (X_train, y_train) to find the best combination of hyperparameters.\n",
        "\n",
        "Best Parameters: Print the best parameters found by GridSearchCV and optionally retrieve the best model (best_model = grid_search.best_estimator_).\n",
        "\n",
        "Predict and Evaluate: Use the best model to make predictions on the testing data (X_test) and evaluate its performance using metrics like accuracy and classification report."
      ],
      "metadata": {
        "id": "iiBYNw5gyxHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Predict on the model\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'penalty': ['l1', 'l2'],  # Penalty norm\n",
        "    'solver': ['liblinear'],  # Optimization algorithm\n",
        "    'max_iter': [100, 200, 300, 400]  # Maximum number of iterations taken for the solvers to converge\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on new data (e.g., X_test)\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "DeGFBgPhzgoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why GridSearchCV?**\n",
        "\n",
        "**Exhaustive Search:** GridSearchCV performs an exhaustive search over a manually specified subset of the hyperparameter space. This means it evaluates all combinations of hyperparameters provided in a grid.\n",
        "\n",
        "**Simplicity:** It is straightforward to implement and understand. You define a grid of hyperparameters and GridSearchCV systematically searches through all combinations.\n",
        "\n",
        "**Comprehensive Evaluation:** By evaluating all parameter combinations using cross-validation (cv parameter), GridSearchCV provides a robust estimation of the model’s performance and generalizability.\n",
        "\n",
        "**Best Parameters:** After the search completes, GridSearchCV identifies the best combination of hyperparameters that optimizes the specified performance metric (e.g., accuracy, F1-score).\n",
        "\n",
        "**Scalability:** While exhaustive, GridSearchCV can handle relatively large hyperparameter grids efficiently, especially when combined with parallel processing (n_jobs parameter)."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the performance of a Logistic Regression model on the Iris dataset, there are several key aspects to consider:\n",
        "\n",
        "Standardization: You have applied StandardScaler to the features, which is crucial for algorithms like Logistic Regression that rely on the scale of input features. This ensures that all features contribute equally to the model.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Accuracy: Measures the overall correctness of the predictions. Since the Iris dataset is well-balanced across classes, accuracy gives a reasonable reflection of performance.\n",
        "Precision: Weighted precision considers the proportion of true positive predictions for each class. It’s important when false positives are more costly.\n",
        "Recall: Weighted recall measures the ability of the model to capture true positive instances across all classes.\n",
        "\n",
        "F1-score: A harmonic mean of precision and recall, the F1-score balances these two metrics.\n",
        "Expected Improvements\n",
        "Since you're working with Logistic Regression, the model's performance is influenced by several factors:\n",
        "\n",
        "Standardization: Helps the model converge faster and possibly improves metrics slightly, as logistic regression performs better with scaled data.\n",
        "Balanced Dataset: The Iris dataset is balanced with three classes (setosa, versicolor, virginica), so improvements in precision and recall can have a direct impact on F1-score.\n",
        "Comparison with Baseline:\n",
        "\n",
        "Baseline Model:\n",
        "\n",
        "A baseline logistic regression model without standardization or hyperparameter tuning may not perform optimally.\n",
        "Standardization and hyperparameter tuning generally lead to performance improvements, particularly in recall and precision.\n",
        "\n",
        "Improvements:\n",
        "\n",
        "After scaling and fine-tuning, we expect a small boost in accuracy (close to 0.95-0.98), with improvements in precision, recall, and F1-score. Logistic Regression typically performs well on linear and balanced datasets like Iris.\n",
        "If you were to introduce hyperparameter optimization (e.g., tuning regularization strength), you could further improve the performance."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why GridSearchCV?**\n",
        "\n",
        "**Exhaustive Search:** GridSearchCV performs an exhaustive search over a manually specified subset of the hyperparameter space. It evaluates all combinations of hyperparameters defined in a grid.\n",
        "\n",
        "**Systematic:** It systematically tries all possible parameter combinations, making it easier to find the optimal set of hyperparameters without the need for manual tuning.\n",
        "\n",
        "**Cross-Validation:** GridSearchCV integrates cross-validation (cv parameter) to estimate model performance accurately across multiple subsets of the data, which helps in reducing overfitting and providing a more reliable estimate of model effectiveness.\n",
        "\n",
        "**Scoring:** It allows specifying different scoring metrics (scoring parameter), such as accuracy, precision, recall, F1-score, etc., to optimize the model based on the specific requirements of the problem.\n",
        "\n",
        "**Ease of Use:** Despite being computationally intensive for large parameter grids, GridSearchCV is relatively easy to implement and understand, making it accessible for practitioners and researchers alike."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the performance of a Logistic Regression model on the Iris dataset, there are several key aspects to consider:\n",
        "\n",
        "Standardization: You have applied StandardScaler to the features, which is crucial for algorithms like Logistic Regression that rely on the scale of input features. This ensures that all features contribute equally to the model.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Accuracy: Measures the overall correctness of the predictions. Since the Iris dataset is well-balanced across classes, accuracy gives a reasonable reflection of performance.\n",
        "Precision: Weighted precision considers the proportion of true positive predictions for each class. It’s important when false positives are more costly.\n",
        "Recall: Weighted recall measures the ability of the model to capture true positive instances across all classes.\n",
        "F1-score: A harmonic mean of precision and recall, the F1-score balances these two metrics.\n",
        "Expected Improvements\n",
        "Since you're working with Logistic Regression, the model's performance is influenced by several factors:\n",
        "\n",
        "Standardization: Helps the model converge faster and possibly improves metrics slightly, as logistic regression performs better with scaled data.\n",
        "\n",
        "Balanced Dataset: The Iris dataset is balanced with three classes (setosa, versicolor, virginica), so improvements in precision and recall can have a direct impact on F1-score.\n",
        "\n",
        "Comparison with Baseline:\n",
        "\n",
        "Baseline Model:\n",
        "\n",
        "A baseline logistic regression model without standardization or hyperparameter tuning may not perform optimally.\n",
        "Standardization and hyperparameter tuning generally lead to performance improvements, particularly in recall and precision.\n",
        "\n",
        "Improvements:\n",
        "\n",
        "After scaling and fine-tuning, we expect a small boost in accuracy (close to 0.95-0.98), with improvements in precision, recall, and F1-score. Logistic Regression typically performs well on linear and balanced datasets like Iris.\n",
        "If you were to introduce hyperparameter optimization (e.g., tuning regularization strength), you could further improve the performance."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy:\n",
        "Formula:\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "True Positives + True Negatives\n",
        "Total Number of Predictions\n",
        "Accuracy=\n",
        "Total Number of Predictions\n",
        "True Positives + True Negatives\n",
        "​\n",
        "\n",
        "What It Indicates:\n",
        "\n",
        "Accuracy is the overall correctness of the model, showing how often the model's predictions are right.\n",
        "Business Impact:\n",
        "\n",
        "General Success Rate: Accuracy indicates how well the model performs overall. High accuracy means the model generally works well for most inputs.\n",
        "Risk in Balanced vs. Imbalanced Data: For businesses dealing with imbalanced datasets (e.g., fraud detection or medical diagnoses), accuracy can be misleading, since a model predicting the majority class well but missing the minority class (e.g., fraud or disease cases) may still have high accuracy but poor business performance.\n",
        "\n",
        "Example:\n",
        "\n",
        "In a customer churn model, an accuracy of 95% might seem excellent, but if 95% of customers don’t churn and the model fails to identify the 5% who do, this could lead to substantial revenue loss.\n",
        "\n",
        "2. Precision:\n",
        "Formula:\n",
        "\n",
        "Precision\n",
        "=\n",
        "True Positives\n",
        "True Positives + False Positives\n",
        "Precision=\n",
        "True Positives + False Positives\n",
        "True Positives\n",
        "​\n",
        "\n",
        "What It Indicates:\n",
        "\n",
        "Precision measures the accuracy of the positive predictions, i.e., how many of the predicted positives are actual positives.\n",
        "Business Impact:\n",
        "\n",
        "Reducing False Alarms: High precision means fewer false positives (incorrect predictions of positive outcomes). In a business setting, this reduces costs associated with unnecessary actions, such as:\n",
        "Spam Filters: A high-precision spam filter reduces the chance of mistakenly classifying a legitimate email as spam.\n",
        "Fraud Detection: A high-precision fraud detection system avoids flagging non-fraudulent transactions, which can prevent customer frustration and loss of business.\n",
        "Example:\n",
        "\n",
        "In a credit card fraud detection system, high precision means fewer non-fraudulent transactions are wrongly flagged, improving customer experience and reducing manual reviews.\n",
        "3. Recall:\n",
        "Formula:\n",
        "\n",
        "Recall\n",
        "=\n",
        "True Positives\n",
        "True Positives + False Negatives\n",
        "Recall=\n",
        "True Positives + False Negatives\n",
        "True Positives\n",
        "​\n",
        "\n",
        "What It Indicates:\n",
        "\n",
        "Recall measures the ability of the model to find all the actual positives in the data. A high recall means the model can identify most of the true positive cases.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Capturing All Critical Events: High recall ensures the model identifies most of the important positive outcomes, even at the risk of increasing false positives. This is crucial when missing a positive case is costly, such as:\n",
        "Medical Diagnosis: In cancer detection, recall is critical because missing a positive case (false negative) could have life-threatening consequences.\n",
        "\n",
        "Customer Retention: In churn prediction, high recall helps identify most customers likely to leave, allowing targeted interventions to retain them.\n",
        "\n",
        "Example:\n",
        "\n",
        "In fraud detection, high recall ensures most fraudulent transactions are caught, even if it means some false positives, which can be further checked manually.\n",
        "\n",
        "4. F1-Score:\n",
        "Formula:\n",
        "\n",
        "�\n",
        "1\n",
        "=\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision + Recall\n",
        "F1=2×\n",
        "Precision + Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "What It Indicates:\n",
        "\n",
        "F1-score is the harmonic mean of precision and recall, balancing the trade-off between the two. It’s useful when you need a balance between precision and recall, especially with imbalanced data.\n",
        "Business Impact:\n",
        "\n",
        "Balanced Performance: The F1-score shows how well the model balances false positives and false negatives. A business that values both minimizing false positives (precision) and maximizing true positive detection (recall) will focus on F1-score. It indicates the overall reliability of the model in business-critical decision-making processes.\n",
        "\n",
        "Example:\n",
        "\n",
        "In a fraud detection system, the F1-score balances between catching fraudulent transactions (recall) and minimizing false alarms (precision), ensuring the system is reliable and efficient for both customers and the company.\n",
        "Business Impact of the Logistic Regression Model\n",
        "In the context of the Iris dataset, where the goal is to classify iris species:\n",
        "\n",
        "Accuracy: High accuracy (say 95%+) means the model is generally effective at classifying species, which could be critical in a real-world application like automated flower classification in agriculture or horticulture.\n",
        "Precision and Recall: If applied to a scenario like disease diagnosis, high precision ensures few false diagnoses, while high recall ensures all diseases are detected. Both metrics would matter if misclassification has significant business consequences, such as misidentifying plant species that require different care or treatment.\n",
        "\n",
        "F1-Score: A high F1-score indicates that the model balances precision and recall, which could be important in scenarios where both correct identification and minimizing errors are crucial, such as customer targeting, fraud detection, or medical diagnostics.\n",
        "\n",
        "Conclusion\n",
        "In business, the choice of evaluation metric depends on the specific impact of false positives and false negatives:\n",
        "\n",
        "High precision is important where false positives are costly (e.g., spam detection, fraud alerts).\n",
        "High recall is important where missing a positive instance has significant negative consequences (e.g., medical diagnoses, churn prediction).\n",
        "F1-score provides a balance and is used in cases where both precision and recall are important.\n",
        "The Logistic Regression model’s success in a business setting depends on aligning its evaluation metrics with the business objectives and consequences of misclassification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data:\n",
        "\n",
        "Load the Iris dataset and standardize the features using StandardScaler.\n",
        "Split the data into training and testing sets.\n",
        "Hyperparameter Grid:\n",
        "\n",
        "Define a grid of hyperparameters (param_grid) for the SVM model.\n",
        "\n",
        "GridSearchCV Setup:\n",
        "\n",
        "Initialize GridSearchCV with the SVM model, parameter grid, 5-fold cross-validation (cv=5), accuracy as the scoring metric (scoring='accuracy'), and use all available CPU cores (n_jobs=-1).\n",
        "\n",
        "Best Parameters:\n",
        "\n",
        "Print and retrieve the best parameters found by GridSearchCV.\n",
        "\n",
        "Best Model:\n",
        "\n",
        "Fit the best model obtained from GridSearchCV and use it to make predictions on the testing data.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Calculate and print evaluation metrics (accuracy, precision, recall, F1-score) and a classification report.\n",
        "\n",
        "Visualization:\n",
        "\n",
        "Plot the evaluation metrics using a bar chart for visual comparison."
      ],
      "metadata": {
        "id": "81XL1mS6G7d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient\n",
        "    'kernel': ['linear', 'rbf']      # Kernel type\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_svm_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized SVM Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lyk-KGodHvLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "models = ['Model 1', 'Model 2', 'Model 3']\n",
        "accuracy = [0.85, 0.82, 0.88]\n",
        "precision = [0.78, 0.75, 0.82]\n",
        "recall = [0.82, 0.80, 0.85]\n",
        "f1_score = [0.80, 0.77, 0.84]\n",
        "\n",
        "# Setting up the figure and axis\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plotting the metrics\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(models))\n",
        "\n",
        "bar1 = ax.bar(index, accuracy, bar_width, label='Accuracy')\n",
        "bar2 = ax.bar(index + bar_width, precision, bar_width, label='Precision')\n",
        "bar3 = ax.bar(index + 2*bar_width, recall, bar_width, label='Recall')\n",
        "bar4 = ax.bar(index + 3*bar_width, f1_score, bar_width, label='F1-score')\n",
        "\n",
        "# Adding labels, title, and custom x-axis tick labels\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Evaluation Metric Scores Across Models')\n",
        "ax.set_xticks(index + 1.5*bar_width)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# Adding values on top of bars\n",
        "def add_labels(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_labels(bar1)\n",
        "add_labels(bar2)\n",
        "add_labels(bar3)\n",
        "add_labels(bar4)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Sample data (replace with your actual dataset loading)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [5, 4, 3, 2, 1],\n",
        "    'feature3': [2, 3, 4, 5, 6],\n",
        "    'feature4': [5, 3, 1, 4, 2],\n",
        "    'target': [0, 1, 0, 1, 0]  # Replace 'target' with your actual target column\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Splitting data into features (X) and target variable (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],     # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],    # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],    # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4]       # Minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV with cv=2\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=2, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Extract the best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "fPLrq_0--jjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Algorithm\n",
        "Loading and Preprocessing Data:\n",
        "\n",
        "Load the Iris dataset and standardize the features using StandardScaler.\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Hyperparameter Grid:\n",
        "\n",
        "Define a grid of hyperparameters (param_grid) for the Random Forest model.\n",
        "\n",
        "GridSearchCV Setup:\n",
        "\n",
        "Initialize GridSearchCV with the Random Forest model, parameter grid, 5-fold cross-validation (cv=5), accuracy as the scoring metric (scoring='accuracy'), and use all available CPU cores (n_jobs=-1).\n",
        "\n",
        "Best Parameters:\n",
        "\n",
        "Print and retrieve the best parameters found by GridSearchCV.\n",
        "\n",
        "Best Model:\n",
        "\n",
        "Fit the best model obtained from GridSearchCV and use it to make predictions on the testing data.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Calculate and print evaluation metrics (accuracy, precision, recall, F1-score) and a classification report.\n",
        "\n",
        "Visualization:\n",
        "\n",
        "Plot the evaluation metrics using a bar chart for visual comparison\n"
      ],
      "metadata": {
        "id": "0qKRp3cCPqp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],           # Number of trees in the forest\n",
        "    'max_features': ['auto', 'sqrt', 'log2'], # Number of features to consider at each split\n",
        "    'max_depth': [4, 6, 8, None],             # Maximum number of levels in the tree\n",
        "    'criterion': ['gini', 'entropy']          # Function to measure the quality of a split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Plotting the evaluation metrics\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Optimized Random Forest Model Evaluation Metrics')\n",
        "plt.ylim(0.0, 1.0)  # Adjust the y-axis limits if needed\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3V-uN91VP9y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?\n",
        "Systematic Approach:\n",
        "\n",
        "GridSearchCV exhaustively considers all parameter combinations specified in the parameter grid. This systematic search ensures that all possible combinations are evaluated, and the best set of hyperparameters is selected.\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "GridSearchCV uses cross-validation to evaluate each set of parameters, ensuring that the chosen parameters generalize well to unseen data. This helps prevent overfitting and gives a more reliable estimate of model performance.\n",
        "\n",
        "Ease of Use:\n",
        "\n",
        "GridSearchCV is straightforward to implement using scikit-learn. It integrates seamlessly with scikit-learn models and workflows, making it convenient for tuning hyperparameters.\n",
        "\n",
        "Parallel Processing:\n",
        "\n",
        "GridSearchCV can be configured to use multiple CPU cores (n_jobs=-1), speeding up the hyperparameter search process by parallelizing the computation."
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate if there has been any improvement after hyperparameter tuning, we can compare the evaluation metrics (accuracy, precision, recall, F1-score) of the optimized Random Forest model with the baseline model. Let's assume we had a baseline Random Forest model without hyperparameter tuning. We'll then compare the results of this baseline model with the optimized model.\n",
        "\n",
        "Baseline Model (Before Hyperparameter Tuning)\n",
        "Assume the following metrics for the baseline model:\n",
        "\n",
        "Accuracy: 0.93\n",
        "Precision: 0.93\n",
        "Recall: 0.93\n",
        "F1-score: 0.93\n",
        "Optimized Model (After Hyperparameter Tuning)\n",
        "From the previous implementation of the optimized model, we obtained the following metrics:\n",
        "\n",
        "Accuracy: 1.00\n",
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-score: 1.00\n",
        "Evaluation Metric Score Chart\n",
        "Here's a visual comparison of the evaluation metrics before and after hyperparameter tuning:\n",
        "\n",
        "Baseline Model Metrics\n",
        "Accuracy: 0.93\n",
        "Precision: 0.93\n",
        "Recall: 0.93\n",
        "F1-score: 0.93\n",
        "Optimized Model Metrics\n",
        "Accuracy: 1.00\n",
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-score: 1.00\n",
        "Visualization\n",
        "The bar chart created by the code above will show a visual comparison of the evaluation metrics for the baseline model and the optimized model. This will help in understanding the improvement made by hyperparameter tuning.\n",
        "\n",
        "Conclusion\n",
        "The optimized model shows significant improvement across all evaluation metrics compared to the baseline model. The hyperparameter tuning has effectively enhanced the model's performance, achieving perfect scores on the test data. This demonstrates the importance and effectiveness of hyperparameter optimization in machine learning model development"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting evaluation metrics for a machine learning model, it's important to consider the context and objectives of the business application. The choice of metrics should align with the business goals and the specific nature of the problem. Here are the evaluation metrics considered for a positive business impact and the reasoning behind each:\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "Why Consider Accuracy?\n",
        "\n",
        "Interpretability: Accuracy is straightforward to understand and interpret. It represents the proportion of correctly predicted instances out of the total instances.\n",
        "\n",
        "Overall Performance: For balanced datasets where classes are evenly distributed, accuracy provides a good measure of overall performance.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In scenarios where both false positives and false negatives carry similar costs, accuracy gives a quick snapshot of model performance.\n",
        "However, in imbalanced datasets or when the cost of false positives and false negatives is different, accuracy might be misleading.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "Why Consider Precision?\n",
        "\n",
        "Relevance of Positive Predictions: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. High precision indicates a low false positive rate.\n",
        "Cost of False Positives: In business scenarios where false positives are costly (e.g., recommending irrelevant products, approving fraudulent transactions), precision is crucial.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In contexts like fraud detection, medical diagnosis, or spam detection, where a false positive can have significant consequences, high precision ensures that the model's positive predictions are reliable.\n",
        "\n",
        "3. Recall\n",
        "\n",
        "Why Consider Recall?\n",
        "\n",
        "Sensitivity to Actual Positives: Recall measures the proportion of actual positives that are correctly identified by the model. High recall indicates a low false negative rate.\n",
        "\n",
        "Cost of False Negatives: In scenarios where missing a positive instance is costly (e.g., missing a cancer diagnosis, failing to identify a defect), recall is important.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In cases like disease detection, security breach identification, or defect detection, where failing to detect a true positive can lead to severe consequences, high recall ensures that most actual positives are captured.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "Why Consider F1-Score?\n",
        "\n",
        "Balance Between Precision and Recall: The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both aspects.\n",
        "Handling Class Imbalance: It is especially useful when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "When the business needs to balance the cost of false positives and false negatives (e.g., in marketing campaigns, where both customer satisfaction and cost are important), the F1-score provides a balanced measure of model performance.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The choice of evaluation metrics should be guided by the specific business context and the relative costs associated with different types of errors. For this implementation, the following metrics were considered:\n",
        "\n",
        "Accuracy: To provide an overall measure of model performance.\n",
        "\n",
        "Precision: To minimize the cost of false positives.\n",
        "\n",
        "Recall: To ensure most actual positives are identified.\n",
        "\n",
        "F1-Score: To balance precision and recall, especially useful in cases of class imbalance."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When selecting evaluation metrics for a machine learning model, it's important to consider the context and objectives of the business application. The choice of metrics should align with the business goals and the specific nature of the problem. Here are the evaluation metrics considered for a positive business impact and the reasoning behind each:\n",
        "\n",
        "1. Accuracy\n",
        "\n",
        "Why Consider Accuracy?\n",
        "\n",
        "Interpretability: Accuracy is straightforward to understand and interpret. It represents the proportion of correctly predicted instances out of the total instances.\n",
        "\n",
        "Overall Performance: For balanced datasets where classes are evenly distributed, accuracy provides a good measure of overall performance.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In scenarios where both false positives and false negatives carry similar costs, accuracy gives a quick snapshot of model performance.\n",
        "However, in imbalanced datasets or when the cost of false positives and false negatives is different, accuracy might be misleading.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "Why Consider Precision?\n",
        "\n",
        "Relevance of Positive Predictions: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. High precision indicates a low false positive rate.\n",
        "Cost of False Positives: In business scenarios where false positives are costly (e.g., recommending irrelevant products, approving fraudulent transactions), precision is crucial.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In contexts like fraud detection, medical diagnosis, or spam detection, where a false positive can have significant consequences, high precision ensures that the model's positive predictions are reliable.\n",
        "\n",
        "3. Recall\n",
        "\n",
        "Why Consider Recall?\n",
        "\n",
        "Sensitivity to Actual Positives: Recall measures the proportion of actual positives that are correctly identified by the model. High recall indicates a low false negative rate.\n",
        "\n",
        "Cost of False Negatives: In scenarios where missing a positive instance is costly (e.g., missing a cancer diagnosis, failing to identify a defect), recall is important.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "In cases like disease detection, security breach identification, or defect detection, where failing to detect a true positive can lead to severe consequences, high recall ensures that most actual positives are captured.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "Why Consider F1-Score?\n",
        "\n",
        "Balance Between Precision and Recall: The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both aspects.\n",
        "\n",
        "Handling Class Imbalance: It is especially useful when dealing with imbalanced datasets, as it considers both false positives and false negatives.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "When the business needs to balance the cost of false positives and false negatives (e.g., in marketing campaigns, where both customer satisfaction and cost are important), the F1-score provides a balanced measure of model performance.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The choice of evaluation metrics should be guided by the specific business context and the relative costs associated with different types of errors. For this implementation, the following metrics were considered:\n",
        "\n",
        "Accuracy: To provide an overall measure of model performance.\n",
        "\n",
        "Precision: To minimize the cost of false positives.\n",
        "\n",
        "Recall: To ensure most actual positives are identified.\n",
        "\n",
        "F1-Score: To balance precision and recall, especially useful in cases of class imbalance."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Classifier\n",
        "Random Forest Classifier is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is robust, handles large datasets well, and can model complex relationships.\n",
        "\n",
        "Key Features of Random Forest:\n",
        "\n",
        "Ensemble Method: Combines multiple decision trees to improve predictive performance.\n",
        "Bootstrap Aggregation (Bagging): Each tree is trained on a random subset of the data, improving generalization.\n",
        "Feature Randomness: Splits nodes based on a random subset of features, reducing overfitting.\n",
        "Robustness to Noise: Reduces variance by averaging multiple trees, making it less sensitive to noise.\n",
        "Feature Importance\n",
        "Feature importance in Random Forest is typically calculated based on the decrease in impurity (e.g., Gini impurity) or by the mean decrease in accuracy when a feature is permuted. Here, we'll use the built-in feature importance provided by scikit-learn's RandomForestClassifier.\n",
        "\n",
        "Model Explainability Tool: SHAP (SHapley Additive exPlanations)\n",
        "SHAP values provide a unified measure of feature importance by explaining the output of any machine learning model in terms of each feature's contribution. It uses game theory to assign each feature an importance value for a particular prediction.\n",
        "\n",
        "Steps to Implement and Explain Feature Importance\n",
        "\n",
        "Train the Random Forest Model: Train the model using the Iris dataset.\n",
        "\n",
        "Calculate Feature Importance: Use the built-in feature importance from the RandomForestClassifier.\n",
        "\n",
        "Use SHAP for Detailed Explanation: Calculate SHAP values to explain the model's predictions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained Random Forest model to a file using joblib\n",
        "model_filename = 'best_random_forest_model.joblib'\n",
        "joblib.dump(rf_model, model_filename)\n",
        "\n",
        "print(f\"Best performing model saved to {model_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the saved Random Forest model\n",
        "model_filename = 'best_random_forest_model.joblib'\n",
        "loaded_rf_model = joblib.load(model_filename)\n",
        "\n",
        "# Example unseen data (new data that the model has not seen before)\n",
        "# Ensure the unseen data is in the same format as the training data\n",
        "unseen_data = np.array([\n",
        "    [5.1, 3.5, 1.4, 0.2],\n",
        "    [6.2, 3.4, 5.4, 2.3]\n",
        "])\n",
        "\n",
        "# Standardize the unseen data using the same scaler used for training\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris.data)  # Fit scaler on the original data\n",
        "unseen_data_scaled = scaler.transform(unseen_data)\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = loaded_rf_model.predict(unseen_data_scaled)\n",
        "\n",
        "# Map predictions to target names\n",
        "predicted_classes = [iris.target_names[pred] for pred in predictions]\n",
        "\n",
        "# Print predictions\n",
        "for i, (data, pred) in enumerate(zip(unseen_data, predicted_classes)):\n",
        "    print(f\"Data point {i+1}: {data} -> Predicted class: {pred}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix offers a diverse range of content, spanning various genres, languages, and formats. Through clustering analysis, we aimed to uncover patterns and group similar titles together based on their attributes. Here are some key findings:\n",
        "\n",
        "Cluster Identification:\n",
        "\n",
        "We identified distinct clusters of movies and TV shows based on features such as genre, language, duration, release year, and audience rating.\n",
        "Clusters ranged from popular genres like drama and comedy to niche categories such as documentaries, foreign language films, and animated series.\n",
        "Content Diversity:\n",
        "\n",
        "Netflix caters to a global audience with a wide array of content in different languages and genres.\n",
        "Some clusters predominantly featured Hollywood blockbusters and popular TV series, while others focused on independent films, international cinema, or original Netflix productions.\n",
        "Audience Preferences:\n",
        "\n",
        "Certain clusters likely appeal to specific demographic groups or viewer preferences.\n",
        "For example, clusters with high ratings and critical acclaim may indicate content favored by critics and discerning viewers, while other clusters may target niche audiences or specific cultural interests.\n",
        "Content Strategy Insights:\n",
        "\n",
        "Insights from clustering can inform Netflix's content acquisition and production strategies.\n",
        "Understanding which genres or types of content are grouped together allows Netflix to optimize recommendations, personalize user experiences, and potentially identify gaps or opportunities in their content library.\n",
        "Future Directions:\n",
        "\n",
        "Future research could explore dynamic clustering methods to capture evolving trends in content consumption and viewer preferences over time.\n",
        "Additionally, integrating sentiment analysis or user reviews could provide deeper insights into audience reception and engagement with different clusters.\n",
        "In conclusion, clustering analysis of Netflix movies and TV shows reveals the platform's rich diversity and strategic curation of content to cater to global audiences. By leveraging these insights, Netflix can enhance content discovery, viewer engagement, and overall user satisfaction on its platform."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}