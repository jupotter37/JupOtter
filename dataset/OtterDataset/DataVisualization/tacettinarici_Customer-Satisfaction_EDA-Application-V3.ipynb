{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Application with a Real Data Set - Draft Version 1\n",
    "## https://www.kaggle.com/c/santander-customer-satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "\n",
    "- Santander Bank is asking to help identify dissatisfied customers in their relationship with hundreds of anonymized features. Please visit the website and read details about it.\n",
    "\n",
    "- 743 columns (ID, TARGET, and features), 76020 rows: train and test data sets.\n",
    "\n",
    "- The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers.\n",
    "\n",
    "- Data file descriptions:\n",
    "\n",
    "        train.csv - the training set including the target\n",
    "        test.csv - the test set without the target\n",
    "        sample_submission.csv - a sample submission file in the correct format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "- The task: \n",
    "    1. to describe the data set\n",
    "        - data type for each column \n",
    "        - missingness\n",
    "        - univariate summaries\n",
    "        - bivariate summaries\n",
    "            - associations\n",
    "            - correlations\n",
    "            - patterns\n",
    "        - multivariate summaries\n",
    "        - more\n",
    "    2. to predict the probability that each customer in the test set is an unsatisfied customer.\n",
    "    3. to predict if a customer is satisfied or dissatisfied with their banking experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping\n",
    "\n",
    "Know: \n",
    "- where you are working\n",
    "- version check\n",
    "- shorcuts: https://youtu.be/d0oBRIONOEw\n",
    "\n",
    "Basic packs to use:\n",
    "- numpy\n",
    "- pandas\n",
    "- seaborn\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory working\n",
    "%pwd #look at the current work dir. import os\n",
    "#%cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Source and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and load the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check version\n",
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baurjan's importing code\n",
    "# Skip\n",
    "#import boto3\n",
    "#client = boto3.client('s3') #amazon web service usage -- with s3 -- boto3 is library. Run this in SageMake. Next Workshop.\n",
    "#train_path = 's3://classdataset/train.csv'\n",
    "#test_path = 's3://classdataset/test.csv'\n",
    "#train= pd.read_csv(train_path)\n",
    "#test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from directory\n",
    "train = pd.read_csv(\"santander-customer-satisfaction/train.csv\", index_col='ID') # the train dataset is now a Pandas DataFrame\n",
    "test = pd.read_csv(\"santander-customer-satisfaction/test.csv\", index_col='ID') # the train dataset is now a Pandas DataFrame\n",
    "#train = pd.read_csv(\"Documents/GitHub/ML/santander-customer-satisfaction/train.csv\", index_col='ID') # the train dataset is now a Pandas DataFrame\n",
    "#test = pd.read_csv(\"Documents/GitHub/ML/santander-customer-satisfaction/test.csv\", index_col='ID') # the train dataset is now a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way if data is undet the same folder\n",
    "train = pd.read_csv(\"train.csv\")                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use train data set for EDA\n",
    "# Look the number of rows and columns\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the data types, number of entries and other details\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the first 5 rows of the data set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given we couldn't see NaNs from the df.info(), let's do this another way\n",
    "train.isnull().sum() #.nlargest() #.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns with all zeros as average\n",
    "np.sum(train.mean()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which columns have zeros?\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with all zeros\n",
    "# This code is NOT recommended b/c mean=0 doesn't mean all values are ZERO\n",
    "zero_mean = (train.mean()==0).to_frame(name='ZeroMean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mean = zero_mean.loc[zero_mean.ZeroMean]\n",
    "print(zero_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get rid of the columns with all zeros, there is no information to gain from them\n",
    "# This is dangerous way to do!\n",
    "remove_features = list(zero_mean.index)\n",
    "print(remove_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(remove_features, axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the number of columns that decreased from original 371 to 337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns with all zeros\n",
    "np.sum(train.mean()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness of values using train.unique for each column\n",
    "# Effective and automatic way?\n",
    "train['TARGET'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at classes of the target \n",
    "train[['TARGET', 'var3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of happy-unhappy customers\n",
    "train['TARGET'].value_counts() / train.shape[0]\n",
    "\n",
    "# See this data is a unbalanced data. When modeling, this produces an issue tht needs to be handled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy customers: TARGET==0 vs Unhappy custormers: TARGET==1\n",
    "df = pd.DataFrame(train.TARGET.value_counts())\n",
    "df['Percentage'] = 100*df['TARGET']/train.shape[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover data related issues, type issues, all issues before running EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of each feature and target values\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a code to get all the ranges of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = []\n",
    "\n",
    "for i in train.columns:\n",
    "    ab.append([i, train[i].min(),train[i].max(), train[i].max()-train[i].min()])\n",
    "\n",
    "print(ab) #see this is not good way to print!\n",
    "\n",
    "#? Research: Effect of Kurtosis and Skewness on Scaling Method (Feature Engineering)\n",
    "#? Shape of Distri\n",
    "#? Use of Skewness in Modeling. Next Topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give column names and make data frame of pandas, then print\n",
    "cols = ['cols','min','max', 'range'] #add more statistic\n",
    "df_ranges = pd.DataFrame(ab,columns=cols)\n",
    "df_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When scaling, high values will skew the transformation. So use percentile or robust scaling method\n",
    "# How to handle these high values or coded values in a variable? -999999\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranges.sort_values(by=['range'], ascending=False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See one -999999 in var3\n",
    "train.var3.value_counts() #all\n",
    "train.var3.value_counts()[:10] #first 10 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dig into issues and redefine\n",
    "train.loc[train.var3==-999999].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -999999 with 0\n",
    "train = train.replace(-999999,0)\n",
    "train.loc[train.var3==-999999].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature that points this unusual values\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task. Your observations and data wrangling to discover issues\n",
    "Please play with the data set. Use basic pandas codes and observe and explore issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory Data Analysis (EDA) with Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of Each Feature\n",
    "# This will generate bar charts/histogram for each. Later, after feature reductions, you can use it again.\n",
    "class color:\n",
    "   BOLD = '\\033[1m'\n",
    "   END = '\\033[0m'\n",
    "for i, col in enumerate(train.columns):\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(5,3)) \n",
    "    plt.hist(train[col], bins=30)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "df_corr = train.corr()\n",
    "print(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task. Sort, get highest correlations (+-) using abs(), unstack(), sort_values(kind=\"quicksort\") and Sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pick some variables and get correlation\n",
    "train[['var36', 'num_var5', 'var38', 'TARGET']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap using seaborn\n",
    "# You may want to run this after eliminating features\n",
    "import seaborn as sns\n",
    "sns.heatmap(train[['var36', 'num_var5', 'var38', 'TARGET']].corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with fitted line\n",
    "sns.lmplot(x='var38', y='num_var5', data=train)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the plot. Discuss does this tell us anythings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='var38', y='num_var5', hue='TARGET', data=train, fit_reg=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "sns.boxplot(data=train[['var36','var38','num_var5', 'TARGET']])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Discuss: On the same scale plot of fatures, it is not good idea. Need to do seperate or scale.\n",
    "# Task: How to scale and how to seperately plot boxplots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=train[['var38']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change figure size\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "sns.violinplot(x='TARGET', y='var38', data=train)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "sns.distplot(train['var36'], color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot\n",
    "sns.countplot(x='var36', data=train)\n",
    "\n",
    "# Rotate labels\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint plot: bivariate\n",
    "# This take time\n",
    "sns.jointplot(train.var38, train.num_var5, data=train, kind='kde', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly\n",
    "\n",
    "It is interactive effective tool for data visualization and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "fig = px.scatter(train, x='var38', y='num_var5')\n",
    "\n",
    "fig.show()\n",
    "# This may not work right away. May need to install pack from pip or terminal. Jupyter-lab has issues with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hovering features\n",
    "fig = px.scatter(train, x='var38', y='num_var5', hover_data=['TARGET'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing total along with hovering features\n",
    "# Fix it\n",
    "fig = px.scatter(train, x='var36', y='num_var5', hover_data=['TARGET'], size='var38', color='blue')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing total along with hovering features\n",
    "fig = px.box(train[['var36','var38']],  color='blue')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: get correlation matrix as corr_matrix, list it, label it by labels=list(corr_matrix), use px.imshow(corr_matrix,x=labels, y=labels) and fig.show()\n",
    "# px.histogram(), px.density_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: use of list(), range(), list(rage(train.shape[0])), .sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefel codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is data set: Then\n",
    "\n",
    "# if you want to get the rows where the color is green, then you’ll need to apply:\n",
    "df.loc[df[‘Color’] == ‘Green’]\n",
    "\n",
    "# Green and Rectangle\n",
    "df.loc[(df[‘Color’] == ‘Green’) & (df[‘Shape’] == ‘Rectangle’)]\n",
    "\n",
    "# Use of OR\n",
    "df.loc[(df[‘Color’] == ‘Green’) | (df[‘Shape’] == ‘Rectangle’)]\n",
    "\n",
    "#To get all the rows where the price is equal or greater than 10, you’ll need to apply this condition:\n",
    "df.loc[df[‘Price’] >= 10]\n",
    "\n",
    "#You can use the combination of symbols != to select the rows where the price is not equal to 15:\n",
    "df.loc[df[‘Price’] != 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3b: Issues with This Type of Data\n",
    "\n",
    "- Too many zeros. How to handle?\n",
    "\n",
    "- How to detect associations among any two combination? (EDA with no Target) \n",
    "\n",
    "- Association with Target Variable (so feature selection can be employed)\n",
    "\n",
    "- When to Use and Why Association?\n",
    "    - To measure association (chi-sq, RR, OR, conditional proportion etc.) between two categorical variables\n",
    "\n",
    "- When to Use and Why Correlation?\n",
    "    - To measure between two numerical linear relationships\n",
    "    \n",
    "\n",
    "- When to not Use?\n",
    "\n",
    "- Alternatives?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search: Why Mr. B write this? Make critiques\n",
    "\n",
    "Notice how we can't even see the correlation matrix for all of the variables due to large number of columns. We will try the following methods:\n",
    "\n",
    "    1. Melt the data that will result into decrease in number of columns but increase in number of rows. \n",
    "    \n",
    "    2. Shrink the size of the columns to the most important 10 features by using Univariate feature selection technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3c: Feature Importance and Selection\n",
    "\n",
    "This is just one application. Many methods and approach exist. The instructor will briefly go over the file, 'Feature Engineering, Data Reduction, and Curse.doc'\n",
    "\n",
    "In general, the choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: \n",
    "\n",
    "- wrappers, \n",
    "- filters and \n",
    "- embedded methods.\n",
    "\n",
    "Filters are most common and cheap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale the features first before feature elimination: Why?\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = train.drop(['TARGET'], axis=1)\n",
    "y = train['TARGET']\n",
    "scaler = MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "# Task: Use other scaling method suited for sparse data. See 'Scaling, Transformation, Metrics, Curse.doc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SelectKBest class to extract the 10 features that best explains the association relationships between Target and Univariate feature (chi-squa results are ranked) \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 #see other options https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=50)\n",
    "fit = bestfeatures.fit(x_scaled, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Feature', 'Score']\n",
    "print(featureScores.nlargest(50, 'Score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_10best_chi = ['num_meses_var5_ult3', 'ind_var5', 'ind_var30', \n",
    "           'var36', 'ind_var8_0', 'ind_var13', \n",
    "           'ind_var13_0', 'ind_var12_0', \n",
    "          'num_var5', 'ind_var13_corto', 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have 11+1 features from train set to proceed\n",
    "sub_train = train[columns_10best_chi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And take the same features from test set\n",
    "columns_10best_chi = ['num_meses_var5_ult3', 'ind_var5', 'ind_var30', \n",
    "           'var36', 'ind_var8_0', 'ind_var13', \n",
    "           'ind_var13_0', 'ind_var12_0', \n",
    "          'num_var5', 'ind_var13_corto']\n",
    "sub_test = test[columns_10best_chi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_train.shape, sub_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Elimination Techniques\n",
    "\n",
    "Univariate Feature Selection from Scikit\n",
    "\n",
    "    2.1. GenericUnivarietSelect\n",
    "    2.2. SelectKBest\n",
    "    2.3. SelectPercentile\n",
    "    2.4. SelectFpr\n",
    "    2.5. SelectFdr\n",
    "    2.6. SelectFwe\n",
    "    2.7. chi2\n",
    "    2.8. f_classif\n",
    "    2.9. f_regerssion\n",
    "    2.10. mutual_info_classif\n",
    "    2.11. mutual_info_regression\n",
    "    \n",
    "Also usefel methods below. Plase play wtih these codes. May need fixing:\n",
    "\n",
    "0) Removing Constant Features\n",
    "1) Removing Quasi-constant: Quasi-constant features are those that show the same value for the great majority of the observations of the dataset.\n",
    "2) Removing Feature From Low Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Removing Constant Features\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# reload the original data, remove -99999, and then run this: see how zeros are removes easily, better that mean.zero method.\n",
    "# fit finds the features with zero variance\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(train)\n",
    "X1 = train.drop(columns=train.columns[~sel.get_support()])\n",
    "#X1t = test.drop(columns=test.columns[~sel.get_support()])\n",
    "\n",
    "print(X1.shape)\n",
    "#print(X1t.shape)\n",
    "\n",
    "print(f'New sahape {X1.shape} number of feature removed {train.shape[1] - X1.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Removing Quasi-constant: Quasi-constant features are those that show the same value for the great majority of the observations of the dataset.\n",
    "\n",
    "sel_quasi = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "sel_quasi.fit(X1)\n",
    "\n",
    "X2 = X1.drop(columns=X1.columns[~sel_quasi.get_support()])\n",
    "#X2t = X1t.drop(columns=X1t.columns[~sel_quasi.get_support()])\n",
    "\n",
    "print(f'New sahape {X2.shape} number of feature removed {X1.shape[1] - X2.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(sel_quasi.fit_transform(train))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View amount of variances\n",
    "print(sel_quasi.variances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the remaining column id\n",
    "remain_features_id = np.where(sel_quasi.variances_ > sel.threshold)\n",
    "\n",
    "# Assign remaining column name\n",
    "result.columns = train.columns[remain_features_id]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Removing Feature From Low Variance\n",
    "# Briefly in a toy example. Some codes are useful to apply to the data set.\n",
    "#VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "#As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n",
    "#so we can select using the threshold .8 * (1 - .8):\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# First Example \n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)\n",
    "\n",
    "print(f'Variance value per columng {sel.variances_}, threshold {sel.threshold : 2.2f}')\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.01)\n",
    "result = pd.DataFrame(sel.fit_transform(df_train))\n",
    "\n",
    "sel.variances_\n",
    "\n",
    "# Find the remaining column id\n",
    "remain_features_id = np.where(sel.variances_ > sel.threshold)\n",
    "\n",
    "# Assign remaining column name\n",
    "result.columns = df_train.columns[remain_features_id]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get best 20 features using constant, quasi variances, and Chi-squ best selection method\n",
    "# Rerun the original data set, remove -99999 and run next:\n",
    "sel_quasi = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "sel_quasi.fit(X1)\n",
    "\n",
    "train_v = train.drop(columns=X1.columns[~sel_quasi.get_support()])\n",
    "\n",
    "print(f'New sahape {train_v.shape} number of feature removed {train.shape[1] - train_v.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without applying scaling: run next. Then Use scaling and rerun. See what changes\n",
    "# Apply SelectKBest class to extract the 10 features that best explains the association relationships between Target and Univariate feature (chi-squa results are ranked) \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2 #see other options https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "\n",
    "x = train_v.drop(['TARGET'], axis=1)\n",
    "y = train_v['TARGET']\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=50)\n",
    "fit = bestfeatures.fit(x, y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['Feature', 'Score']\n",
    "print(featureScores.nlargest(50, 'Score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "Are all of these categorical variables? is it a problem when building predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need Powerful Tools and Packs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package 1: Pandas-Profiling\n",
    "\n",
    "Import the pack or upoad it from https://github.com/pandas-profiling/pandas-profiling.\n",
    "\n",
    "Shortly, this generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install here or using pip: >> pip install pandas-profiling\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need if not running\n",
    "sub_train.profile_report(style={‘full_width’:True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(sub_train, title=\"Pandas Profiling Report\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"your_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: Pandas-profiling may NOT process the data due to its size and not enough compute resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package 2:  ClfAutoEDA\n",
    "\n",
    "- Read the article https://medium.com/analytics-vidhya/automated-eda-for-classification-77c25b847e43\n",
    "- Download the py code in the directory you are working: https://github.com/jatinkataria94/EDA-Classification/blob/master/ClfAutoEDA.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the autoEDA module\n",
    "from ClfAutoEDA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Just play with small data portion\n",
    "#Setting parameter values\n",
    "target_variable_name='TARGET'\n",
    "labels=['Unhappy','Happy']\n",
    "#Calling EDA function with parameters of choice\n",
    "df_processed,num_features,cat_features=EDA(df=sub_train,labels=labels,\n",
    "                                         target_variable_name=target_variable_name,\n",
    "                                         data_summary_figsize=(6,6),\n",
    "                                         corr_matrix_figsize=(6,6), \n",
    "                                         corr_matrix_annot=True,\n",
    "                                         pairplt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: Pandas-profiling couldn't process the data due to its size and not enough compute resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack 3. Sweetviz\n",
    "\n",
    "Read the article https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34\n",
    "\n",
    "Install it using pip install sweetviz and run the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will tke too much time if \n",
    "my_report = sweetviz.compare([sub_train, \"Train\"], [sub_test, \"Test\"], \"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_report.show_html(\"Report.html\") # Not providing a filename will default to SWEETVIZ_REPORT.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack 4. Pycaret\n",
    "\n",
    "A complex level pack that does all!\n",
    "\n",
    "Read the article https://github.com/pycaret/pycaret and examples https://github.com/pycaret/pycaret/tree/master/examples.\n",
    "\n",
    "Install it using pip install pycaret.\n",
    "\n",
    "Practice it with the dataset here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Resources to Practice\n",
    "- https://www.kaggle.com/c/santander-customer-satisfaction and see many notebooks prepared\n",
    "- https://www.kaggle.com/cast42/exploring-features ==> nice notebook to practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
