{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 CSCI E-7\n",
    "## Introduction to Scientific Computing and Data Analysis\n",
    "##### Nenad Svrzikapa April 12 2017\n",
    "#### NumPy, Pandas, Matplotlib, Bokeh and more\n",
    "\n",
    "The following lecture uses code and examples from Python for Data Analysis by Wes McKinney.  Wes is the creator of Pandas and I highly recommend that you purchase this book if you are interested in Data Analysis with Python.  In addition to that this lecture contains Machine Learning code from Machine Learning Mastery by Jason Brownlee.  I recommend Jason's book if you are interested in getting into Machine Learning.  There is an excellent Python Machine Learning tutorial by Josh Gordon - Google on YouTube.  I also like Siraj Raval's videos.\n",
    "\n",
    "The following code requires installs of the following libraries:\n",
    "\n",
    " * matplotlib\n",
    " * bokeh\n",
    " * numpy\n",
    " * pandas\n",
    " * seaborn\n",
    " * plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy note that np is convention, but up to you\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "my_list = [0,1,2,2,2,3.5,4,5]\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "n1 = np.array(my_list)\n",
    "n2 = np.linspace(0,1,5)\n",
    "print (n2)\n",
    "\n",
    "#mean\n",
    "print (n1.mean())\n",
    "print (np.mean(n1))\n",
    "#mode have to import scipy for mode\n",
    "print (stats.mode(n1))\n",
    "#median\n",
    "print (np.median(n1))\n",
    "#max\n",
    "print (np.max(n1))\n",
    "#min\n",
    "print (np.min(n1))\n",
    "\n",
    "#shape of the array\n",
    "print (np.shape(n1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Array Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [[1,2,3,4],[6,4,6,8],[3,5,6,8]]\n",
    "my_array = np.array(my_list)\n",
    "print (np.shape(my_array)) #rows and columns\n",
    "\n",
    "#accessing the first row\n",
    "print (my_array[0])\n",
    "\n",
    "#accessing the last row\n",
    "print (my_array[-1])\n",
    "\n",
    "#accessing element in specific row and column\n",
    "print (my_array[2][1])\n",
    "\n",
    "#accessing a column\n",
    "\n",
    "#column 1\n",
    "print (my_array[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Array Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as long as they have the same shape\n",
    "np1 = np.array([1,2,3])\n",
    "np2 = np.array([2,3,4])\n",
    "\n",
    "np3 = np1 + np2\n",
    "np4 = np3 - np2 - np1\n",
    "\n",
    "print (np3)\n",
    "print (np4)\n",
    "\n",
    "#you can also do math with a numpy array\n",
    "#multiplies all np values with 5\n",
    "np5 = np1 * 5\n",
    "print (np5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "#making a numpy array of a range of numbers from 1-10\n",
    "n1 = numpy.array(range(1,11))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Awesome Title', fontsize=20)\n",
    "\n",
    "plt.plot(n1)\n",
    "plt.xlabel('some x axis')\n",
    "plt.ylabel('some y axis')\n",
    "plt.show()\n",
    "fig.savefig('myfig.jpg')\n",
    "plt.clf() #clear figure\n",
    "\n",
    "n2 = numpy.array([5,7,9,10,11,12,13,16,25,30])\n",
    "plt.scatter(n1,n2,color=['red','green','blue','yellow',\n",
    "                         'purple','orange','pink','brown','cyan',                       \n",
    "                        'darkred'])\n",
    "plt.xlabel('Time [min]')\n",
    "plt.ylabel('Values [kg]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas\n",
    "There are two main data structures you will need to master:\n",
    "* Series\n",
    "* DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas Series\n",
    "\n",
    "Series are one-dimensional objects containing an array of data and an index associated with that data.  Note that this data has to be of the same type.  Check what happens if one value is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "quiz_scores = Series([12,11,14,15])\n",
    "\n",
    "quiz_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the index and values of the Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatically generated index is useful, but more often than not you will want to have your own index for the data.  Let's make our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores.index = [\"Quiz 1\",\"Quiz 2\",\"Quiz 3\",\"Quiz 4\"]\n",
    "quiz_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series can be easily accessed by index.  We can do this either for a single value or a list of specifix indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores['Quiz 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores[['Quiz 1','Quiz 3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending values to pandas series is simple.  We can accomplish this with the append method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores2 = Series([9,8],index = ['Quiz 5','Quiz 6'])\n",
    "print (quiz_scores2)\n",
    "quiz_scores = quiz_scores.append(quiz_scores2)\n",
    "quiz_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have powerful manipulation techniques at our disposal.  Let's try a few.  Let's say the Max value of the quiz is 15 and we want to convert these values to percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can do math on all the values immediately\n",
    "quiz_scores = quiz_scores /15 *100\n",
    "print (quiz_scores)\n",
    "#we can round our scores\n",
    "quiz_scores = quiz_scores.round(1)\n",
    "print (quiz_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_scores[quiz_scores > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so it appears that series map the indexes to some values.  In a way they are kind of like dictionaries.  In fact making a series out of a dictionary-like data, or json is quite direct.  All you have to do is call Series(your dictionary) and you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = {'NY City':8000000, 'LA':4000000,'Boston':650000}\n",
    "s_pop = Series(population)\n",
    "s_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something?  The keys got ordered!  Series are in a way like ordered dictionaries when made out of a dictionary.\n",
    "\n",
    "Now that we have done this, we can create new series from existing series by specifying the index keys.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Boston\",\"LA\"]\n",
    "s_pop_smaller = Series(s_pop,index=cities)\n",
    "s_pop_smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we don't really know the indexes and we are just interested to make a new series with cities of interest.  Let's say that we want to know about Cambridge and Somerville too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = [\"Boston\",\"LA\",\"Cambridge\",\"Somerville\"]\n",
    "s_pop_larger = Series(s_pop, index = cities2)\n",
    "s_pop_larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are probably wondering what NaN is.  It means: Not A Number.  It's basically pandas for missing values.  You can test if Index values are missing with pd.isnull(your series) or the pd.notnull(your series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(s_pop_larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most awesome thing about series is that you can do mathematical operations with multiple series and they will automagically align even if they are not indexed the same way.  Let's assume we have the original one and the larger one I made with the fake values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pop + s_pop_larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, both the Series object and the index have a name attribute that can be set for downstream functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pop_larger.name = 'population'\n",
    "s_pop_larger.index.name = 'cities'\n",
    "s_pop_larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often than not you will want your plots to be inline and enclosed in your notebook.  This is possible.  Let's do something more fun:\n",
    "\n",
    "https://data.world/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas Dataframes\n",
    "\n",
    "Here is how we can create a very simple dataframe our of a 3x3 numpy array.  The columns list holds the header names of the columns, and the index list holds the row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(np.random.random([3, 3]),columns=['Pset 1', 'Pset 2', 'Pset 3'], index=['Student 1', 'Student 2', 'Student 3'])\n",
    "my_df*=100\n",
    "my_df = my_df.round(2)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to creat a dataframe is probably from equal length dictionaries where the keys of those dictionaries will become the colum index names.  Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"name\":['Jen','Ali','Kwabena','Goran'],\n",
    "     \"dob\":[1970,1965,1980,1983],\n",
    "    \"salary\":[100000,90000,95000,70000]}\n",
    "\n",
    "my_df1 = DataFrame(d)\n",
    "my_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df2 = DataFrame(d,columns = ['name','dob','salary'])\n",
    "my_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example you can see that by specifying the column names I reordered the columns the way I like them to be.  But, this is even more powerful.  I can in theory initialize another column.  Similar to the Series objects, all the values of that column (if not present in the dictionary) will get initialized as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3 = DataFrame(d,columns = ['name','dob','salary','bonus'])\n",
    "my_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but let's say HR has worked out the bonuses and you want to include values here and replace NaN with actuall values.  But, first let's discuss the two ways to access data in the dataFrame.  One is like typical dictionary access i.e. my_df3['bonus']  the other is by attribute with the dot notation. my_df3.bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (my_df3['bonus'])\n",
    "print (my_df3.bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to access  this we can assign some value.  Let's say they all got 10000 bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3['bonus'] = 10000\n",
    "my_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!  However, in real life they are unlikely to all get the same bonus.  We can do this with a list of values but note that the length of that list must match or things will get broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3['bonus'] = [10000,11000,9000,8000]\n",
    "my_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the bonuses all set, but HR sends you an email and says that they have made a mistake and that Ali and Goran's bonuses are 12000 and 9000 respectively.  We can use a series to quickly update these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = Series([1200,9000],index=[1,3])\n",
    "my_df3.bonus = update\n",
    "my_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading your data into a pandas dataframe\n",
    "* Getting your .csv into a Pandas dataframe\n",
    "* Naming your columns\n",
    "* Using Plotly to make a nice visualization of your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "\n",
    "#loading a csv into a Pandas dataframe from link.\n",
    "#I knew that there was no header in this data\n",
    "#If I didn't declare that the first row would have been assumed to be the header\n",
    "\n",
    "df = pd.read_csv(\"https://query.data.world/s/32o6gpwtme7iz5n22npxp2rea\",header=None)\n",
    "\n",
    "#let's name our dataframe columns\n",
    "df.columns = ['Sepal Length', 'Sepal Width',\n",
    "             'Petal Length','Petal Width',\n",
    "             'Species']\n",
    "#creating nice tables with plotly from your dataframe\n",
    "my_table = ff.create_table(df)\n",
    "\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Iris\n",
    "\n",
    "Machine Learning is a subset of Artificial Intelligence.  Machine learning is a study of algorithms that learn from examples and experience rather than hard-coded rules.  The machine learning portion of the code in this example is by Jason Brownlee, with my commentary and some alternate visualizations.  See more in references.\n",
    "\n",
    "* Data Collection\n",
    "* Pick a Model (based suited for data)\n",
    "* Train the Model\n",
    "* Test the Model\n",
    "\n",
    "\n",
    "### Classifier I/O (Data Input, Assigns some Label as Output)\n",
    "### Training a classifier\n",
    "### Supervised vs. Unsupervised learning\n",
    "\n",
    "Let's start with the simplest possible example.  This example is described in Josh Gordon's youtube series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "features = [[139,\"Smooth\"],[130,\"Smooth\"],[135,\"Smooth\"],[149,\"Bumpy\"],[151,\"Bumpy\"],[160,\"Bumpy\"]]\n",
    "labels = [\"Apple\",\"Apple\",\"Apple\",\"Orange\",\"Orange\",\"Orange\"]\n",
    "\n",
    "#binarize the data\n",
    "features = [[139,1],[130,1],[135,1],[149,0],[151,0],[160,0]]\n",
    "labels = [1,1,1,0,0,0]\n",
    "\n",
    "#we will use a Decision Tree Classifier\n",
    "clf = sklearn.tree.DecisionTreeClassifier()\n",
    "#Now we will train the classifier\n",
    "clf = clf.fit(features,labels)\n",
    "\n",
    "d = {1:\"Apple\",0:\"Orange\"}\n",
    "\n",
    "#Now let's test this!\n",
    "results =  (clf.predict([[155,0],[140,1],[133,0]]))\n",
    "for r in results:\n",
    "    print (d[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "First we can take a peak at a portion of the data.  The head method\n",
    "will allow us to take a look at the beginning of the dataframe.  We can specify a value df.head(n) where n tells us how many values we want to look at.  If we don't specify this df.head() will show us the first 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the end of the dataframe by using the df.tail() method.  This works in much the same way as the df.head().  It's a quick way to see how many rows you have and if the data at the end is in some way different.  Again, we can specify df.tail(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will have too many columns and rows and you want to get an idea about the shape of your data.  This can be accomplished with the shape method.  You can notice that the dataframe object stores a tuple of its shape.  The first value of the tuple is the number of rows in your data and the second it's the number of columns.  Notice that the index column is not counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(df.shape))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "my_table = ff.create_table(df.describe(),index=True)\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='iris_vc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Favorite topic: Rounding :)\n",
    "\n",
    "There are several ways to round.  The first and easiest way is to just round the whole dataframe. df.round(n) where n is the number of decimal digits required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_table = ff.create_table(df.describe().round(2),index=True)\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='rounded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note:\n",
    "You can notice that I am passing this data to plotly, but the dataframe itself is still not rounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "description = df.describe()\n",
    "description = description.round(2)\n",
    "my_table = ff.create_table(description,index=True)\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing specific categories\n",
    "\n",
    "Sometimes if your data is categorized you will want to take a look at the descriptive statistics of those categories.  This is not hard to do.  In the example below I am making a new dataframe based on some criteria.  This criteria is tested by the following line: \n",
    "df['Species'] == \"Iris-versicolor\"\n",
    "\n",
    "ok so everywhere where in the column Species the value is equal to \"Iris-versicolor\".  Not so bad.\n",
    "\n",
    "The next thing I do here is some more specific rounding.  Previously we reounded the whole dataframe, but what if we wanted to round only specific columns?  Examine the rounding before.  What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_iris_versicolor = df.loc[df['Species'] == \"Iris-versicolor\"]\n",
    "a = df_iris_versicolor.describe()\n",
    "a = a.round({'Sepal Length':2,'Petal Length':2})\n",
    "#Note here that I said that index is True\n",
    "my_table = ff.create_table(a,index=True)\n",
    "\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='iris_vc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use a pandas series to specify how the dataframe should be rounded.  This gives us the power to predefine how to round in a series structure that can be used in the round method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = pd.Series([0,1,2,3], index=['Sepal Length', 'Sepal Width',\n",
    "                                       'Petal Length', 'Petal Width'])\n",
    "a_rounded = a.round(decimals)\n",
    "my_table = ff.create_table(a_rounded,index=True)\n",
    "\n",
    "#a paid account will get you private files and folders\n",
    "#you can create private folders if you specify it in the filename\n",
    "# i.e. my_project1/my_table1 will create a folder with the file in it.\n",
    "py.iplot(my_table, filename='iris_vc_series_rounded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting counts of a particular column\n",
    "\n",
    "There are several ways to do this.  The value_counts() is somewhat like a Counter, but it returns a pandas series.  If you run that method on any column you can see that it will count how many times all the values appear.  This is useful because you can take a peak of the representation of the data and if some value is particularly enriched in your dataset.  Depending on the circumstances that may be interesting to know.  The value_counts() method returns back a pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sepal Width\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "print(df.groupby('Species').size())\n",
    "type(df.groupby('Species').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put on same axis by modifying sharex and sharey\n",
    "df.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate histograms to look at the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the Sepal measurments are close to normally distributted.\n",
    "### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sea.FacetGrid(df, hue=\"Species\", \n",
    "    size=6).map(plt.scatter, \"Sepal Width\", \"Petal Width\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check with hue=\"Species\" and without\n",
    "sea.pairplot(df,hue=\"Species\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sea.set(style=\"darkgrid\")\n",
    "g = sea.PairGrid(df)\n",
    "g.map_diag(sea.kdeplot)\n",
    "g.map_offdiag(sea.kdeplot, cmap=\"Greens\",shade=True, n_levels=10);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "We have our dataset but we want to split it in such a way that a portion of it will be dedicated to training our models (80% in this case) and 20% will be allocated for validation of the models.  Note that those 20% are known real data measurements so if we are good in detecting them then our model is performing well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,0:4]\n",
    "Y = array[:,4]\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a mesaure of performance\n",
    "\n",
    "Accuracy is defined as the percent of data that is classified correctly and is calculated by taking the count of the data that is called correctly divided by the total data points number times 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring model performance\n",
    "\n",
    "We are now going to take a look at 6 different models and test them.  In order to train the models we will devide our training data into 10 even parts.  9/10 of those will be used for training and 1/10 of those data splits will be used to test the accuracy of the model.  Let's see how that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holds our models\n",
    "models = []\n",
    "#we add our models to our list of models\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "#we iterate through all models and test their performance\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab #need this to change the Y axis range\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Model Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "pylab.ylim([0.9,1]) #I changed the Y axis range here\n",
    "\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok for me SVM seems to be working the best, so let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, Y_train)\n",
    "predictions = svm.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotly\n",
    "First you will need to make an account and get your api_key.\n",
    "Second you will want to specify if your plot will be viewable by the world.  If True you will want to set your sharing to public.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='Your User Name', api_key='Your API Key')\n",
    "plotly.tools.set_config_file(world_readable=True,\n",
    "                             sharing='public')\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "trace0 = Scatter(\n",
    "    x=[1, 2, 3, 4, 5],\n",
    "    y=[1, 2, 3, 4, 5]\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x=[1, 2, 3, 4, 5],\n",
    "    y=[10, 20, 30, 40, 50]\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x=[1, 2, 3, 4, 5],\n",
    "    y=[5, 10, 15, 20, 25]\n",
    ")\n",
    "data = Data([trace0, trace1,trace2])\n",
    "\n",
    "py.plot(data, filename = 'basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birthplaces of Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "mapbox_access_token = 'pk.eyJ1IjoiY2hlbHNlYXBsb3RseSIsImEiOiJjaXFqeXVzdDkwMHFrZnRtOGtlMGtwcGs4In0.SLidkdBMEap9POJGIe1eGw'\n",
    "\n",
    "rating_one_site_lat = [41.11722,33.5061877,42.814401,3.9055556,29.7807902]\n",
    "rating_one_site_lon = [20.80194, -86.80343,-70.890917,-76.50333333333333,-95.3977855]\n",
    "locations_name = ['Nenad','Alan','Kaleigh','Jose','Joe']\n",
    "\n",
    "\n",
    "\n",
    "data = Data([\n",
    "    Scattermapbox(\n",
    "        lat=rating_one_site_lat,\n",
    "        lon=rating_one_site_lon,\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=18,\n",
    "            color='rgb(155, 240, 225)',\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=locations_name,\n",
    "        hoverinfo='text'\n",
    "    ),\n",
    "    Scattermapbox(\n",
    "        lat=rating_one_site_lat,\n",
    "        lon=rating_one_site_lon,\n",
    "        mode='markers',\n",
    "        marker=Marker(\n",
    "            size=8,\n",
    "            color='rgb(205, 245, 100)'\n",
    "        ),\n",
    "        hoverinfo='skip'\n",
    "    )]\n",
    ")\n",
    "        \n",
    "layout = Layout(\n",
    "    title='Birthplaces of Staff Members',\n",
    "    autosize=True,\n",
    "    hovermode='closest',\n",
    "    showlegend=False,\n",
    "    mapbox=dict(\n",
    "        accesstoken=mapbox_access_token,\n",
    "        bearing=0,\n",
    "        center=dict(\n",
    "            lat=38,\n",
    "            lon=-94\n",
    "        ),\n",
    "        pitch=0,\n",
    "        zoom=3,\n",
    "        style='dark'\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='StaffBirthPlaces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "from bokeh.charts import Donut, show, output_notebook, vplot\n",
    "from bokeh.charts.utils import df_from_json\n",
    "from bokeh.io import output_notebook,hplot\n",
    "from bokeh.models import HoverTool, layouts\n",
    "\n",
    "\n",
    "output_notebook() #to generate plots inline\n",
    "\n",
    "a_at_SNP1 = 500\n",
    "c_at_SNP1 = 400\n",
    "g_at_SNP1 = 300\n",
    "t_at_SNP1 = 200\n",
    "\n",
    "baseDistSNP1 = [a_at_SNP1, c_at_SNP1, g_at_SNP1, t_at_SNP1]\n",
    "\n",
    "\n",
    "\n",
    "d_snp1 = Donut(pd.Series(baseDistSNP1, index=['A', 'C', 'G', 'T']),title = \"My Base Distribution\")\n",
    "\n",
    "\n",
    "\n",
    "p = layouts.Row(d_snp1)\n",
    "\n",
    "# show the results\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDistSNP1_percent = [70,20,5,5]\n",
    "TOOLS = 'pan,wheel_zoom,box_zoom,resize,reset,save,box_select,hover'\n",
    "df1 = pd.DataFrame({'Base':['A', 'C', 'G', 'T'],'Counts': baseDistSNP1_percent})\n",
    "\n",
    "d_snp1 = Donut(df1,label = 'Base',values ='Counts', legend = True,title = \"Base Distribution At rs362307\",color=['#ffa700','#d62d20','#0057e7','#008744'],tools=TOOLS)\n",
    "\n",
    "\n",
    "hover1 = d_snp1.select(dict(type=HoverTool))\n",
    "hover1.tooltips = [(\"Percent\",\"@values%\")]\n",
    "\n",
    "\n",
    "p = layouts.Row(d_snp1)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://plot.ly/python/ipython-notebook-tutorial/\n",
    "\n",
    "https://data.world/\n",
    "\n",
    "https://seaborn.pydata.org/\n",
    "\n",
    "Python for Data Analysis - Wes McKinney\n",
    "\n",
    "Machine Learning Mastery - Jason Brownlee\n",
    "\n",
    "Machine Learning Videos YouTube - Josh Gordon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
