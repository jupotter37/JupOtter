{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data With Pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration is an important step in the data analysis process. It involves investigating the dataset to gain insights into its characteristics, such as its size, shape, and content. Python is a powerful programming language that provides a variety of tools for data exploration. In this tutorial, we will cover the following topics:\n",
    "\n",
    "1. Importing the dataset\n",
    "2. Understanding the dataset\n",
    "3. Visualizing the dataset\n",
    "4. Cleaning the dataset\n",
    "5. Handling missing data\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in data exploration is to import the dataset into Python. Python provides a variety of libraries for reading different types of datasets, such as CSV, Excel, JSON, HTML, and SQL. In this tutorial, we will use the Pandas library, which provides a powerful data structure called DataFrame that allows us to manipulate and analyze the data easily.\n",
    "\n",
    "Here's an example of how to import a CSV dataset using Pandas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Reading a CSV file\n",
    "df = pd.read_csv(\"path/to/file.csv\")\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filepaths can be relative or absolute. If relative, the file will be read in relation to the current working directory, which typically is the directory in which the Python script is located. If absolute, the file will be read in relation to the root directory. The `read_csv()` function returns a `DataFrame` object.\n",
    "\n",
    "The `read_csv()` function takes a number of optional arguments. For a complete list of arguments, refer to the official `pandas` documentation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data from a URL\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv()` function can also be used to read data from a URL. The following example demonstrates how to read in CSV data from a URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading a CSV file from a URL\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/AB_NYC_2019.csv\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example acquires the [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) from Kaggle. The data contains information about Airbnb listings in New York City. The data is stored in a CSV file on the web. The `read_csv()` function is used to read the data from the URL and store it in a DataFrame.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have imported the dataset, we need to understand its characteristics, such as its size, shape, and content. Here are some useful methods for exploring the dataset:\n",
    "\n",
    "- `df.head(n)` : displays the first n rows of the dataset (by default, n=5)\n",
    "- `df.tail(n)` : displays the last n rows of the dataset (by default, n=5)\n",
    "- `df.shape` : displays the number of rows and columns in the dataset\n",
    "- `df.describe()` : displays statistical information about the dataset, such as mean, standard deviation, and quartiles\n",
    "- `df.info()` : displays information about the dataset, such as column names, data types, and missing values\n",
    "- `df[\"column_name\"].value_counts()` : displays the unique values and their counts in a column\n",
    "- `df[\"column_name\"].unique()` : displays the unique values in a column\n",
    "- `df[\"column_name\"].nunique()` : displays the number of unique values in a column\n",
    "\n",
    "Here's an example of how to use these methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"neighbourhood\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"room_type\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"neighbourhood\"].nunique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the dataset can help us gain insights into its characteristics and relationships between variables. Python provides a variety of libraries for data visualization, such as Matplotlib, Seaborn, and Plotly. In this tutorial, we will use Matplotlib, which is a widely used library for creating basic visualizations.\n",
    "\n",
    "Here's an example of how to create a histogram using Matplotlib:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(df[\"price\"], bins=np.linspace(0, 1000, 100))\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a histogram of a column named column_name in the dataset. You can replace column_name with the name of your own column.\n",
    "\n",
    "Alternatively, Pandas provides built-in methods for plotting. For example, creating a histogram using Pandas is as simple as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=\"price\", bins=np.linspace(0, 1000, 100))\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Price\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning the dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before analyzing the dataset, we need to clean it by removing irrelevant or redundant information, dealing with missing data, and transforming data if necessary. Here are some common data cleaning tasks:\n",
    "\n",
    "- Removing duplicates: `df.drop_duplicates()`\n",
    "- Removing irrelevant columns: `df.drop(['column_name'], axis=1)`\n",
    "- Renaming columns: `df.rename(columns={'old_name': 'new_name'})`\n",
    "- Transforming data: `df['new_column'] = df['old_column'] * 2`\n",
    "\n",
    "Here's an example of how to remove duplicates and irrelevant columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"host_id\"], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling missing data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial step in data exploration as missing data can cause errors and bias in analysis. There are different strategies for handling missing data, such as removing missing values, imputing missing values, or using advanced techniques like data interpolation. Pandas provides various functions for handling missing data, including:\n",
    "\n",
    "- `df.isnull()`: returns a Boolean DataFrame indicating which values are missing\n",
    "- `df.dropna()`: removes rows or columns with missing values\n",
    "- `df.fillna(value)`: fills missing values with a specified value\n",
    "- `df.interpolate()`: performs linear interpolation to fill missing values\n",
    "\n",
    "Here's an example of how to fill missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviews_per_month\"].fillna(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fills missing values in a column named `column_name` with the mean of the column. You can replace `column_name` with the name of your own column.\n",
    "\n",
    "In conclusion, data exploration is a crucial step in the data analysis process, and Python provides a powerful set of tools for exploring, visualizing, cleaning, and handling missing data. By following these steps, you can gain insights into your dataset and prepare it for further analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Exploring Airbnb listings in New York City\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to explore a dataset of Airbnb listings in New York City using Pandas. We will perform various data exploration tasks to gain insights into the dataset and answer some questions about Airbnb in NYC.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using an unclean version of the [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) dataset from Kaggle. The data contains information about Airbnb listings in New York City. The data is stored in a CSV file on the GitHub repository under the `data` folder. The `read_csv()` function is used to read the data from the URL and store it in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading a CSV file from a URL\n",
    "df = pd.read_csv(\n",
    "    \"../../data/AB_NYC_2019_unclean.csv\"  # Replace with your own path if necessary.\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "The tasks below are merely suggestions for exploring the dataset. Feel free to explore and analyze the dataset in any way you like.\n",
    "\n",
    "1. Import the dataset into a Pandas DataFrame.\n",
    "2. Explore the dataset, for example using the `head()`, `tail()` and `info()` methods.\n",
    "3. Explore the basic statistics of the dataset using the `describe()` method.\n",
    "4. Clean the dataset. For example:\n",
    "   - Remove unnecessary columns\n",
    "   - Handle missing data\n",
    "   - Remove duplicate records\n",
    "   - Fix typos\n",
    "   - Deal with outliers\n",
    "   - Ensure the data types are correct\n",
    "5. Visualize the distribution of prices using a histogram.\n",
    "6. Identify the top 10 neighborhoods with the highest number of listings and create a bar chart to visualize the results.\n",
    "7. Analyze the relationship between price and availability by creating a scatter plot.\n",
    "8. Use the method to calculate the average price by neighborhood and room type.\n",
    "9. Create a heatmap to visualize the availability of listings by neighborhood.\n",
    "   - Hint: Use the `seaborn` library to create a heatmap. You can install the library using the command `pip install seaborn`. For more information, refer to the [official documentation](https://seaborn.pydata.org/).\n",
    "10. Identify the top 10 hosts with the most listings and create a bar chart to visualize the results.\n",
    "11. Analyze the relationship between the number of reviews and price by creating a scatter plot.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
