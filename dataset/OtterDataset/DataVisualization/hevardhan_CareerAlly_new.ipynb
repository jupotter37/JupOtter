{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF report created successfully as 'report_with_charts.pdf'.\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "# Define the content\n",
    "name = \"Hevardhan\"\n",
    "md_content = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            color: #333;\n",
    "        }}\n",
    "        h1 {{\n",
    "            color: #0056b3;\n",
    "        }}\n",
    "        h2 {{\n",
    "            color: #0066cc;\n",
    "        }}\n",
    "        p {{\n",
    "            font-size: 14px;\n",
    "            line-height: 1.6;\n",
    "        }}\n",
    "        ul {{\n",
    "            margin-left: 20px;\n",
    "        }}\n",
    "        li {{\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .section-title {{\n",
    "            font-weight: bold;\n",
    "            margin-top: 20px;\n",
    "        }}\n",
    "        .chart {{\n",
    "            margin-top: 20px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Report Title</h1>\n",
    "    <p>Name : {name}</p>\n",
    "    <p>This is an example report generated with Python and ReportLab.</p>\n",
    "\n",
    "    <h2>Section 1</h2>\n",
    "    <p>Here is some sample content for section 1.</p>\n",
    "\n",
    "    <h2>Section 2</h2>\n",
    "    <p>More content goes here in section 2.</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "\n",
    "    <p>Thank you for reading this report!</p>\n",
    "\n",
    "    <div class=\"chart\">\n",
    "        <h2 class=\"section-title\">Bar Chart</h2>\n",
    "        <img src=\"bar_chart.png\" alt=\"Bar Chart\" />\n",
    "    </div>\n",
    "\n",
    "    <div class=\"chart\">\n",
    "        <h2 class=\"section-title\">Pie Chart</h2>\n",
    "        <img src=\"pie_chart.png\" alt=\"Pie Chart\" />\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "html_content = markdown.markdown(md_content)\n",
    "\n",
    "# Generate a Bar Chart\n",
    "def create_bar_chart():\n",
    "    categories = ['A', 'B', 'C', 'D']\n",
    "    values = [10, 20, 30, 40]\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(categories, values, color='blue')\n",
    "    plt.title('Sample Bar Chart')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Values')\n",
    "    \n",
    "    # Save to a byte stream\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png')\n",
    "    img_stream.seek(0)\n",
    "    plt.close()\n",
    "    return img_stream\n",
    "\n",
    "# Generate a Pie Chart\n",
    "def create_pie_chart():\n",
    "    labels = ['Category A', 'Category B', 'Category C']\n",
    "    sizes = [15, 30, 55]\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['red', 'green', 'blue'])\n",
    "    plt.title('Sample Pie Chart')\n",
    "    \n",
    "    # Save to a byte stream\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png')\n",
    "    img_stream.seek(0)\n",
    "    plt.close()\n",
    "    return img_stream\n",
    "\n",
    "# Create a PDF document\n",
    "pdf_filename = 'report_with_charts.pdf'\n",
    "doc = SimpleDocTemplate(pdf_filename, pagesize=letter)\n",
    "\n",
    "# Add custom styles\n",
    "styles = getSampleStyleSheet()\n",
    "style_normal = styles['Normal']\n",
    "style_heading1 = styles['Heading1']\n",
    "style_heading2 = styles['Heading2']\n",
    "\n",
    "# Add background color to the PDF by modifying the page template\n",
    "def add_background(canvas, doc):\n",
    "    canvas.setFillColor(colors.lightblue)\n",
    "    canvas.rect(0, 0, letter[0], letter[1], fill=1)\n",
    "\n",
    "# Create a Story\n",
    "story = []\n",
    "\n",
    "# Add background before the content starts\n",
    "doc.build(story, onFirstPage=add_background, onLaterPages=add_background)\n",
    "\n",
    "# Manually process the HTML content into styled paragraphs\n",
    "lines = html_content.splitlines()\n",
    "\n",
    "# Process content\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Skip empty lines\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # Process <h1> and <h2> as headings\n",
    "    if line.startswith('<h1>'):\n",
    "        heading = line.replace('<h1>', '').replace('</h1>', '')\n",
    "        para = Paragraph(heading, style_heading1)\n",
    "        story.append(para)\n",
    "        story.append(Spacer(1, 12))  # Spacer after the heading\n",
    "    elif line.startswith('<h2>'):\n",
    "        heading = line.replace('<h2>', '').replace('</h2>', '')\n",
    "        para = Paragraph(heading, style_heading2)\n",
    "        story.append(para)\n",
    "        story.append(Spacer(1, 12))\n",
    "    elif line.startswith('<p>'):\n",
    "        paragraph = line.replace('<p>', '').replace('</p>', '')\n",
    "        para = Paragraph(paragraph, style_normal)\n",
    "        story.append(para)\n",
    "        story.append(Spacer(1, 12))\n",
    "    elif line.startswith('<ul>'):\n",
    "        # Handle list items\n",
    "        list_items = []\n",
    "        list_started = False\n",
    "        while line.startswith('<li>'):\n",
    "            list_item = line.replace('<li>', '').replace('</li>', '')\n",
    "            list_items.append(list_item)\n",
    "            list_started = True\n",
    "            line = lines.pop(0)  # Get the next line\n",
    "        if list_started:\n",
    "            para = Paragraph(\"<bullet>%s</bullet>\" % ', '.join(list_items), style_normal)\n",
    "            story.append(para)\n",
    "            story.append(Spacer(1, 12))\n",
    "\n",
    "# Generate and add the Bar Chart to the PDF\n",
    "bar_chart_stream = create_bar_chart()\n",
    "story.append(Spacer(1, 12))  # Add space before the image\n",
    "story.append(Paragraph(\"Bar Chart:\", style_heading2))\n",
    "\n",
    "# Convert image to a format suitable for reportlab\n",
    "bar_chart_image = Image(bar_chart_stream)\n",
    "bar_chart_image.drawHeight = 3*inch\n",
    "bar_chart_image.drawWidth = 4*inch\n",
    "story.append(bar_chart_image)\n",
    "\n",
    "# Generate and add the Pie Chart to the PDF\n",
    "pie_chart_stream = create_pie_chart()\n",
    "story.append(Spacer(1, 12))  # Add space before the image\n",
    "story.append(Paragraph(\"Pie Chart:\", style_heading2))\n",
    "\n",
    "# Convert image to a format suitable for reportlab\n",
    "pie_chart_image = Image(pie_chart_stream)\n",
    "pie_chart_image.drawHeight = 3*inch\n",
    "pie_chart_image.drawWidth = 4*inch\n",
    "story.append(pie_chart_image)\n",
    "\n",
    "# Build the PDF\n",
    "doc.build(story)\n",
    "\n",
    "print(f\"PDF report created successfully as '{pdf_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file loaded successfully: job.csv\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# %%\n",
    "excel_path = \"job.csv\"  # Path to your Excel file\n",
    "if excel_path:\n",
    "    df = pd.read_csv(excel_path)\n",
    "    print(f\"Excel file loaded successfully: {excel_path}\")\n",
    "else:\n",
    "    print(\"Upload an Excel file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>Responsible for designing and developing softw...</td>\n",
       "      <td>programming languages, algorithms, systems arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>Oversees project planning, execution, and deli...</td>\n",
       "      <td>leadership, communication, risk management, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Analyzes large datasets to identify trends, pa...</td>\n",
       "      <td>statistical analysis, machine learning, data v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing Specialist</td>\n",
       "      <td>Promotes products and services to customers. R...</td>\n",
       "      <td>creativity, market research, digital marketing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Manages employee relations, recruitment, and s...</td>\n",
       "      <td>interpersonal communication, human resources p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>Develops and maintains software applications f...</td>\n",
       "      <td>programming languages, software development to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Real Estate Project Manager</td>\n",
       "      <td>Leads real estate development projects, from p...</td>\n",
       "      <td>real estate development, market analysis, zoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Statistical Researcher</td>\n",
       "      <td>Conducts statistical research to support scien...</td>\n",
       "      <td>statistical methods, research design, R, Pytho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Marketing Specialist - Trade Show Marketing Co...</td>\n",
       "      <td>Manages marketing activities for trade shows a...</td>\n",
       "      <td>trade show planning, lead generation, vendor r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Directs recruitment campaigns and hiring proce...</td>\n",
       "      <td>recruitment, job postings, candidate sourcing,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                                              label  \\\n",
       "0     0                                           Engineer   \n",
       "1     1                                    Project Manager   \n",
       "2     2                                     Data Scientist   \n",
       "3     3                               Marketing Specialist   \n",
       "4     4                                         HR Manager   \n",
       "..  ...                                                ...   \n",
       "95   95                                 Software Developer   \n",
       "96   96                        Real Estate Project Manager   \n",
       "97   97                             Statistical Researcher   \n",
       "98   98  Marketing Specialist - Trade Show Marketing Co...   \n",
       "99   99                                         HR Manager   \n",
       "\n",
       "                                          description  \\\n",
       "0   Responsible for designing and developing softw...   \n",
       "1   Oversees project planning, execution, and deli...   \n",
       "2   Analyzes large datasets to identify trends, pa...   \n",
       "3   Promotes products and services to customers. R...   \n",
       "4   Manages employee relations, recruitment, and s...   \n",
       "..                                                ...   \n",
       "95  Develops and maintains software applications f...   \n",
       "96  Leads real estate development projects, from p...   \n",
       "97  Conducts statistical research to support scien...   \n",
       "98  Manages marketing activities for trade shows a...   \n",
       "99  Directs recruitment campaigns and hiring proce...   \n",
       "\n",
       "                                               skills  \n",
       "0   programming languages, algorithms, systems arc...  \n",
       "1   leadership, communication, risk management, pr...  \n",
       "2   statistical analysis, machine learning, data v...  \n",
       "3   creativity, market research, digital marketing...  \n",
       "4   interpersonal communication, human resources p...  \n",
       "..                                                ...  \n",
       "95  programming languages, software development to...  \n",
       "96  real estate development, market analysis, zoni...  \n",
       "97  statistical methods, research design, R, Pytho...  \n",
       "98  trade show planning, lead generation, vendor r...  \n",
       "99  recruitment, job postings, candidate sourcing,...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 100 chunks\n",
      "Vector database created successfully\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided information, I would recommend the Data Scientist role to you. Although your skills are not explicitly mentioned as being relevant to Natural Language Processing (NLP) or Machine Learning Engineering, your educational background and required skills align well with these areas.\n",
       "\n",
       "Your experience in statistical analysis, machine learning, data visualization, SQL, and data analytics suggests that you could be proficient in developing predictive models and algorithms to solve business problems. Additionally, your critical thinking skills and ability to work with data would likely serve you well in a Data Scientist role.\n",
       "\n",
       "While it's worth noting that your background does not explicitly mention NLP or Machine Learning Engineering, many of the required skills for these roles are also applicable to other areas, such as Business Analytics Consulting or Data Science. However, based on the available information, I believe that Data Scientist is the most suitable fit for you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.schema import Document  # Import Document\n",
    "# %%\n",
    "# Convert rows to text format for processing\n",
    "# data = [{\"text\": f\"Job Role: {row['label']}\\nDescription: {row['description']}\\nRequirements: {row['skills']}\"} \n",
    "#         for index, row in df.iterrows()]\n",
    "\n",
    "data = [\n",
    "    Document(page_content=f\"Job Role: {row['label']}\\nDescription: {row['description']}\\nRequirements: {row['skills']}\")\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# %%\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")\n",
    "\n",
    "# %%\n",
    "# Create vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n",
    "print(\"Vector database created successfully\")\n",
    "\n",
    "# %%\n",
    "local_model = \"ally\"  # Your preferred model\n",
    "llm = ChatOllama(model=local_model)\n",
    "\n",
    "# %%\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Consider Yourself as Ally, a Assistance Chatbot, Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# %%\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# %%\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# %%\n",
    "def chat_with_excel(question):\n",
    "    \"\"\"\n",
    "    Chat with the Excel data using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))\n",
    "\n",
    "# %%\n",
    "chat_with_excel(\"Which job role suits me best given my skills?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided job descriptions, here's a quiz to help you assess your skills and qualifications for an NLP (Natural Language Processing) Engineering role at Dassault Systemes:\n",
       "\n",
       "**Section 1: Language Skills**\n",
       "\n",
       "1. What is the primary focus of language processing in computer science?\n",
       "a) Machine learning\n",
       "b) Natural Language Processing (NLP)\n",
       "c) Data mining\n",
       "d) Text analysis\n",
       "\n",
       "Answer: b) Natural Language Processing (NLP)\n",
       "\n",
       "2. Which of the following programming languages is commonly used for NLP tasks?\n",
       "a) Python\n",
       "b) Java\n",
       "c) C++\n",
       "d) JavaScript\n",
       "\n",
       "Answer: a) Python\n",
       "\n",
       "3. What is the purpose of tokenization in NLP processing?\n",
       "a) To remove punctuation and special characters from text\n",
       "b) To convert text into a standard format for analysis\n",
       "c) To identify keywords and phrases within text\n",
       "d) To generate machine learning models\n",
       "\n",
       "Answer: c) To identify keywords and phrases within text\n",
       "\n",
       "**Section 2: Computational Skills**\n",
       "\n",
       "1. What is the primary function of an NLP algorithm?\n",
       "a) To train machine learning models\n",
       "b) To perform data analysis\n",
       "c) To generate predictions based on input data\n",
       "d) To optimize system performance\n",
       "\n",
       "Answer: c) To generate predictions based on input data\n",
       "\n",
       "2. Which of the following is a key challenge in training deep learning models for NLP tasks?\n",
       "a) Handling noisy or incomplete data\n",
       "b) Managing large datasets\n",
       "c) Identifying and mitigating bias\n",
       "d) Adapting to new languages and dialects\n",
       "\n",
       "Answer: d) Adapting to new languages and dialects\n",
       "\n",
       "**Section 3: Domain Knowledge**\n",
       "\n",
       "1. What is a common application of NLP in industries beyond computer science?\n",
       "a) Finance and banking\n",
       "b) Healthcare and medicine\n",
       "c) Manufacturing and logistics\n",
       "d) All of the above\n",
       "\n",
       "Answer: d) All of the above\n",
       "\n",
       "2. Which of the following domains benefits from the use of NLP techniques?\n",
       "a) Social media analytics\n",
       "b) Sentiment analysis for customer service\n",
       "c) Language translation services\n",
       "d) All of the above\n",
       "\n",
       "Answer: d) All of the above"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_excel(\"I am looking for a job as an NLP Engineer at Dassault Systemes. Generate a quiz on the skills required for this role.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the job descriptions provided, I'd recommend the following skills to enhance your chances of becoming an Agile Project Manager:\n",
       "\n",
       "1. **Agile Methodologies**: Familiarity with Agile frameworks such as Scrum or Kanban would be beneficial.\n",
       "2. **Project Management Tools**: Proficiency in tools like Jira, Trello, Asana, or MS Project would be advantageous.\n",
       "3. **Leadership and Communication Skills**: Effective leadership and communication skills are essential for managing project teams and stakeholders.\n",
       "4. **Risk Management**: Experience with risk management techniques, such as identifying, assessing, and mitigating risks, would be valuable.\n",
       "5. **Time Management and Organization**: The ability to prioritize tasks, manage multiple projects, and meet deadlines is crucial in an Agile environment.\n",
       "\n",
       "Additionally, having experience with software development tools like Git, or knowledge of systems engineering principles could also be beneficial for a career as an Agile Project Manager."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_excel(\"what skill i need for, Agile Project Manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As Ally, I can help you with that.\n",
       "\n",
       "Here's a simple Python code snippet that prints \"Hello World\":\n",
       "```python\n",
       "print(\"Hello World\")\n",
       "```\n",
       "When you run this code in your Python environment, it will output:\n",
       "```\n",
       "Hello World\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_excel(\"give me python code to print hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact Info: {'email': 'hevardhan2004@gmail.com', 'phone': '+91 9384565379', 'linkedin': 'https://www.linkedin.com/in/hevardhan-saravanan-33642024a/', 'github': 'https://github.com/hevardhan', 'website': 'https://hevardhan.me/'}\n",
      "\n",
      "Names: {'Pandas', 'Sklearn', 'Campus', 'Git Jun', 'Django', 'Bollinger Bands', 'Cloud Platform', 'IBM Watson', 'IBM Cloud', 'Vannila', 'Matplotlib', 'Long Short-Term Memory', 'Visual Studio', 'Java', 'PyTorch', 'Cloud Fundamentals', 'Hevardhan Saravanan\\n+91', 'Standard Arrival Routes'}\n",
      "Organizations: {'Support Vector Machine', 'Amazon ML Summer School', 'Oil', 'IBM', 'CNN', 'Sklearn, Tensorflow, Pytorch', 'Google Developer Student Club', 'Education\\nSymbiosis Institute of Technology Pune', 'Artificial Intelligence and', 'Sklearn, Tensorflow', '3rd Place', 'Random Forest Regressor', 'SIT', 'SQL', 'HTML', 'HTML/CSS', '• Implemented', 'IEEE Education Society', 'TFT', 'Random Forest Classifier', 'Convolutional Neural\\nNetworks', '• Working', 'Logistic Regression', 'SMA', 'Temporal Fusion Transformer', 'Predictive Maintenance', 'Google Solution Challenge', 'JavaScript', '• Gained', 'CSS', 'GDSC', '• Applied'}\n",
      "Locations: {'Node.js', 'Flask', 'Porto', 'NumPy', 'Portugal', 'India'}\n",
      "\n",
      "Education: Symbiosis Institute of Technology Pune, India B.Tech in Artificial Intelligence and Machine Learning Aug. 2022 – June 2026\n",
      "\n",
      "Experience: Research Intern June 2024 – Present Symbiosis Centre for Applied AI Pune, India • Responsible for conducting a research on Predictive Maintenance of urban metro public transportation service in Porto, Portugal. Web Development Lead July 2024 – Present Google Developer Student Club (SIT) Pune, India\n",
      "\n",
      "Projects: Campus Ambassador July 2024 – Present GeeksForGeeks India • Promoting GeeksForGeeks and engaging the student community to enhance programming skills and awareness. IBM SkillsBuild Intern July 2024 – Aug 2024 Edunet Foundation India • Gained expertise in IBM Cloud Fundamentals and IBM Watson. BullsEye (Trade Bot) | Python, Tensorflow, MetaTrader5, Sklearn, Git June 2023 – Present • Implementing live scraping of candlestick data across 1-minute, 10-minute, and 15-minute timeframes. • Applied statistical indicators such as Simple Moving Average (SMA), Moving Average Convergence Divergence (MACD), and Bollinger Bands to generate trade signals. • Analyzing the signals in real-time and optimizing them using machine learning and deep learning algorithms. Predictive Maintenance | Python, Sklearn, Tensorflow, PyTorch, Git Jun 2024 – Present • Conducted a detailed analysis of sensor data and classified failures into two categories: Air leak and Oil leak. • Applied various machine learning techniques, including Random Forest Classifier and Logistic Regression, resulting in an accuracy of 96.4%. • Working on the implementation Temporal Fusion Transformer (TFT) to get more accurate and reliable results. Flight Trajectory Prediction | Python, Django, Sklearn, Tensorflow, Git Jan 2024 – May 2024 • Developed a full-stack web application using Django, HTML, CSS and Vannila JS. • Implemented various machine learning models, including Random Forest Regressor and Support Vector Machine (SVM), along with deep learning models such as Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN), to classify flights based on Standard Arrival Routes (STAR), achieving a 97.5% accuracy.\n",
      "\n",
      "Achievements: Participant, Amazon ML Summer School July 2024 Amazon India Participant, Google Solution Challenge March 2024 Google Developer Student Club India 3rd Place, Build-a-Thon Hackathon Competition May 2024\n",
      "\n",
      "Technical Skills: \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's pre-trained model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract contact info using regex\n",
    "def extract_contact_info(text):\n",
    "    email = re.search(r'\\b[\\w.%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone = re.search(r'\\+?\\d[\\d -]{8,}\\d', text)\n",
    "    linkedin = re.search(r'https?://(www\\.)?linkedin\\.com/[^\\s]+', text)\n",
    "    github = re.search(r'https?://(www\\.)?github\\.com/[^\\s]+', text)\n",
    "    website = re.search(r'https?://(www\\.)?[^\\s]+', text)\n",
    "    return {\n",
    "        \"email\": email.group(0) if email else None,\n",
    "        \"phone\": phone.group(0) if phone else None,\n",
    "        \"linkedin\": linkedin.group(0) if linkedin else None,\n",
    "        \"github\": github.group(0) if github else None,\n",
    "        \"website\": website.group(0) if website else None\n",
    "    }\n",
    "\n",
    "# Function to extract sections based on keywords\n",
    "def extract_section(text, section_keyword):\n",
    "    lines = text.splitlines()\n",
    "    section_content = []\n",
    "    capture = False\n",
    "    for line in lines:\n",
    "        if section_keyword.lower() in line.lower():\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            # Stop capturing at the next section or empty line\n",
    "            if any(keyword in line.lower() for keyword in [\"experience\", \"projects\", \"education\", \"achievements\", \"technical skills\", \"languages\", \"frameworks\"]) or line.strip() == \"\":\n",
    "                break\n",
    "            section_content.append(line.strip())\n",
    "    return \" \".join(section_content)\n",
    "\n",
    "# Function to extract named entities like name, organization, and locations using NER\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\n",
    "        \"names\": set(),\n",
    "        \"organizations\": set(),\n",
    "        \"locations\": set()\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"names\"].add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities[\"organizations\"].add(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            entities[\"locations\"].add(ent.text)\n",
    "    return entities\n",
    "\n",
    "# Main parsing function\n",
    "def parse_resume(file_path):\n",
    "    # Extract the text from the PDF\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    # Extract contact info\n",
    "    contact_info = extract_contact_info(text)\n",
    "    \n",
    "    # Extract sections\n",
    "    education = extract_section(text, \"education\")\n",
    "    experience = extract_section(text, \"experience\")\n",
    "    projects = extract_section(text, \"projects\")\n",
    "    achievements = extract_section(text, \"achievements\")\n",
    "    technical_skills = extract_section(text, \"technical skills\")\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = extract_entities(text)\n",
    "\n",
    "    # Print out the parsed information\n",
    "    print(\"Contact Info:\", contact_info)\n",
    "    print(\"\\nNames:\", entities[\"names\"])\n",
    "    print(\"Organizations:\", entities[\"organizations\"])\n",
    "    print(\"Locations:\", entities[\"locations\"])\n",
    "    print(\"\\nEducation:\", education)\n",
    "    print(\"\\nExperience:\", experience)\n",
    "    print(\"\\nProjects:\", projects)\n",
    "    print(\"\\nAchievements:\", achievements)\n",
    "    print(\"\\nTechnical Skills:\", technical_skills)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Resume-1.pdf\"\n",
    "parse_resume(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Hevardhan Saravanan\n",
      "+91\n",
      "Email: hevardhan2004@gmail.com\n",
      "Phone: +91 9384565379\n",
      "Skills: {'css', 'flask', 'machine learning', 'pytorch', 'tensorflow', 'svm', 'azure', 'java', 'sql', 'artificial intelligence', 'aws', 'deep learning', 'django', 'javascript', 'python', 'html', 'git'}\n",
      "Experience: {'Support Vector Machine', 'Amazon ML Summer School', 'Flask', 'Oil', 'IBM', 'CNN', 'Sklearn, Tensorflow, Pytorch', 'Portugal', 'Google Developer Student Club', 'Education\\nSymbiosis Institute of Technology Pune', 'Artificial Intelligence and', 'Sklearn, Tensorflow', '3rd Place', 'Random Forest Regressor', 'SIT', 'SQL', 'HTML', 'Porto', 'HTML/CSS', '• Implemented', 'IEEE Education Society', 'TFT', 'Node.js', 'Random Forest Classifier', 'Convolutional Neural\\nNetworks', '• Working', 'Logistic Regression', 'NumPy', 'SMA', 'Temporal Fusion Transformer', 'Predictive Maintenance', 'Google Solution Challenge', 'JavaScript', '• Gained', 'CSS', 'GDSC', 'India', '• Applied'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(file_path):\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "# Function to extract email and phone number using regex\n",
    "def extract_contact_info(text):\n",
    "    email = re.findall(r'\\b[\\w.%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone = re.findall(r'\\+?\\d[\\d -]{8,}\\d', text)\n",
    "    return email[0] if email else None, phone[0] if phone else None\n",
    "\n",
    "# Define a comprehensive skill keywords list and pattern matcher for multi-word skills\n",
    "skill_keywords = [\n",
    "    \"Python\", \"Java\", \"Machine Learning\", \"Data Analysis\", \"SQL\",\n",
    "    \"Deep Learning\", \"Artificial Intelligence\", \"Natural Language Processing\",\n",
    "    \"NLP\", \"SVM\", \"Random Forest\", \"KNN\", \"Google Colab\", \"Data Analytics\",\n",
    "    \"Excel\", \"TensorFlow\", \"PyTorch\", \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\",\n",
    "    \"React\", \"JavaScript\", \"CSS\", \"HTML\", \"Git\", \"Flask\", \"Django\",\n",
    "    \"Communication\", \"Leadership\", \"Project Management\", \"Cloud Computing\",\n",
    "    \"API Development\", \"Data Engineering\", \"Big Data\", \"Scrum\"\n",
    "]\n",
    "\n",
    "# Define patterns for Matcher to capture multi-word skills\n",
    "patterns = [\n",
    "    [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
    "    [{\"LOWER\": \"data\"}, {\"LOWER\": \"analysis\"}],\n",
    "    [{\"LOWER\": \"deep\"}, {\"LOWER\": \"learning\"}],\n",
    "    [{\"LOWER\": \"artificial\"}, {\"LOWER\": \"intelligence\"}],\n",
    "    [{\"LOWER\": \"natural\"}, {\"LOWER\": \"language\"}, {\"LOWER\": \"processing\"}]\n",
    "]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in patterns:\n",
    "    matcher.add(\"SKILL\", [pattern])\n",
    "\n",
    "# Main function to parse resume and extract specific details\n",
    "def parse_resume(file_path):\n",
    "    # Extract text from the resume based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    # Process text with spaCy's NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize fields\n",
    "    person_name = None\n",
    "    skills = set()\n",
    "    experience = set()\n",
    "\n",
    "    # Extract Name and Experience using NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and not person_name:\n",
    "            person_name = ent.text  # Assuming the first PERSON entity is the name\n",
    "        elif ent.label_ == \"ORG\" or ent.label_ == \"GPE\":\n",
    "            experience.add(ent.text)  # ORG represents companies, GPE for geographic locations often part of experience\n",
    "\n",
    "    # Extract skills using keywords and Matcher patterns\n",
    "    for token in doc:\n",
    "        skill = token.text.lower()  # Standardize to lowercase\n",
    "        if skill in map(str.lower, skill_keywords):\n",
    "            skills.add(skill)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills.add(span.text.lower())  # Add multi-word skills in lowercase\n",
    "\n",
    "    # Extract email and phone number using regex\n",
    "    email, phone = extract_contact_info(text)\n",
    "\n",
    "    # Output the extracted information\n",
    "    print(\"Name:\", person_name)\n",
    "    print(\"Email:\", email)\n",
    "    print(\"Phone:\", phone)\n",
    "    print(\"Skills:\", skills)\n",
    "    print(\"Experience:\", experience)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Resume-1.pdf\"  # or \"path_to_resume.docx\"\n",
    "parse_resume(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Rohan Ingle\n",
      "Student\n",
      "Email: rohaningle911@gmail.com\n",
      "Phone: 9422184621\n",
      "Education: rohaningle911@gmail.com\n",
      "B.Tech in Artificial Intelligence and Machine Learning\n",
      "Symbiosis Institute of Technology, Pune.\n",
      "9422184621\n",
      "08/2022 - Present, CGPA: 7.45\n",
      "Pune, India\n",
      "Experience: rohan-ingle.github.io/ Research Intern\n",
      "Symbiosis Center for Behavioral Studies\n",
      "linkedin.com/in/rohan-ingle- 05/2024 - Present,\n",
      "b1b457249 Tasks\n",
      "Using Machine Learning to analyze social media content.\n",
      "Gathering insights on variety of social media posts.\n",
      "github.com/Rohan-ingle\n",
      "Helping in generating a Machine Learning model to understand sponsored posts.\n",
      "Projects: Matplotlib Seaborn\n",
      "Road Anomaly and Accident Detection\n",
      "Sklearn Pandas Realtime detection of Accidents and Potholes.\n",
      "Cloud Storage\n",
      "Numpy Linux A Java-based application designed to host a storage system on any computer, complete with a client-side\n",
      "interface for easy access and management.\n",
      "AWS Plotly Uses AES encryption to enhance security.\n",
      "Uses Hashing to store credentials.\n",
      "Uses H2 database running within JVM to store user information.\n",
      "HTML/CSS\n",
      "Youtube Comments Sentiment Analysis\n",
      "Machine Learning Analyze then sentiments of people within a Youtube video's comment section.\n",
      "Outputs pie-chart to help understand sentiments.\n",
      "Models used: KNN, Decision tree, naive Bayes and logistic regression.\n",
      "Virtual Voice Assistant\n",
      "INTERESTS Control various tasks within a PC using voice commands.\n",
      "Forex Market Trade Bot\n",
      "Hardware and\n",
      "Conducted a clear, data driven Analysis on the Forex Market of Currency pair EURUSD.\n",
      "Electronics\n",
      "Generated buy and sell signals using Moving Average Crossover Strategy.\n",
      "Implemented Machine Learning algorithms and improvised the Strategy to achieve significant profits.\n",
      "3D Modelling\n",
      "Football Players Analysis using Unsupervised Machine Learning\n",
      "Cloud Computing Analysis of various football players based on their different abilities.\n",
      "Used Unsupervised Machine Learning to understand performance of a certain player in certain scenario.\n",
      "OS Technology\n",
      "Achievements: \n",
      "Technical Skills: {'Python', 'R', 'AWS', 'Java', 'Machine Learning'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pdfplumber\n",
    "import re\n",
    "import docx2txt\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(file_path):\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "# Function to extract email and phone number using regex\n",
    "def extract_contact_info(text):\n",
    "    email = re.findall(r'\\b[\\w.%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone = re.findall(r'\\+?\\d[\\d -]{8,}\\d', text)\n",
    "    return email[0] if email else None, phone[0] if phone else None\n",
    "\n",
    "# Define skill keywords list to capture technical skills\n",
    "skill_keywords = [\n",
    "    \"Python\", \"Java\", \"Machine Learning\", \"Data Analysis\", \"SQL\", \"Data Analytics\", \"Critical Thinking\",\n",
    "    \"Problem Solving\", \"Leadership\", \"Communication\", \"Teamwork\", \"Risk Management\", \"Project Management Tools\",\n",
    "    \"Statistical Analysis\", \"Data Visualization\", \"R\", \"Tableau\", \"Big Data\", \"Agile Methodologies\", \"Digital Marketing\",\n",
    "    \"Marketing Software\", \"Customer Service\", \"Interpersonal Communication\", \"Human Resources Policies\", \"Recruitment\",\n",
    "    \"Software Development Methodologies\", \"Coding\", \"Debugging\", \"Software Testing\", \"Version Control\", \"Software Architecture\",\n",
    "    \"Agile\", \"Scrum\", \"Kanban\", \"Predictive Modeling\", \"Social Media Strategy\", \"Content Creation\", \"SEO\", \"Branding\",\n",
    "    \"Influencer Marketing\", \"HR Policies\", \"Employee Training\", \"Employee Wellness\", \"Budget Management\", \"Financial Analysis\",\n",
    "    \"Cost Control\", \"Strategic Planning\", \"Data Manipulation\", \"Excel\", \"Google Analytics\", \"Search Engine Rankings\",\n",
    "    \"Keyword Research\", \"Backlinking\", \"Website Audits\", \"Content Optimization\", \"Employee Engagement\", \"Workplace Safety\",\n",
    "    \"Materials Science\", \"Circuit Design\", \"Electrical Testing\", \"Cloud Platforms\", \"AWS\", \"Azure\", \"GCP\", \"Cloud Security\",\n",
    "    \"Project Planning\", \"Construction Management\", \"Geotechnical Engineering\", \"Healthcare Industry Knowledge\", \"Regulatory Compliance\",\n",
    "    \"Biotechnology\", \"Genetic Engineering\", \"Bioinformatics\", \"Cell Culture\", \"Data Analysis\", \"Electrical Engineering\",\n",
    "    \"Risk Assessment\", \"Technology Trends\", \"Mechanical Design Principles\", \"CAD Software\", \"Machine Design\", \"Quality Control\",\n",
    "    \"Event Planning\", \"Team Coordination\", \"Logistics Management\", \"Social Media Management\", \"Content Marketing\",\n",
    "    \"Stakeholder Management\", \"Supply Chain Management\", \"Lean Manufacturing\", \"Six Sigma\", \"Environmental Impact Assessment\",\n",
    "    \"Data Cleaning\", \"Feature Engineering\", \"Predictive Analytics\", \"Influencer Selection\", \"Content Collaboration\",\n",
    "    \"Financial Regulations\", \"Employee Morale\", \"Supplier Relationship Management\", \"Project Timelines\", \"Team Collaboration\",\n",
    "    \"IT Project Planning\", \"ETL Processes\", \"Big Data Technologies\", \"Hadoop\", \"Apache Spark\", \"Data Mining\", \"Data Preprocessing\",\n",
    "    \"Database Management\", \"Coding\", \"Debugging\", \"Version Control\", \"Software Testing\", \"Marketing Strategies\", \"Data Science\",\n",
    "    \"Software Design\", \"Problem-Solving\", \"Digital Advertising\", \"UX/UI Design\", \"B2B Marketing\", \"Lead Generation\",\n",
    "    \"Public Relations\", \"Compliance\", \"Regulatory Policies\", \"Budget Management\", \"Business Strategy\", \"Strategic HR Planning\",\n",
    "    \"Employee Retention\", \"Career Planning\", \"Mentorship\", \"Recruitment\", \"Diversity Management\", \"Customer Engagement\"\n",
    "]\n",
    "# Improved function to capture sections based on headings, capturing bullet points for projects\n",
    "def extract_section(text, keywords):\n",
    "    section_content = []\n",
    "    capture = False\n",
    "    for line in text.splitlines():\n",
    "        line_lower = line.lower()\n",
    "        if any(keyword in line_lower for keyword in keywords):\n",
    "            capture = True  # Start capturing when a section heading is found\n",
    "        elif capture and (line.strip() == \"\" or any(keyword in line_lower for keyword in section_keywords)):\n",
    "            break  # Stop capturing at the next empty line or a new section heading\n",
    "        elif capture:\n",
    "            section_content.append(line.strip())\n",
    "    return \"\\n\".join(section_content)\n",
    "\n",
    "# Extract skills from text by searching for keywords in the text\n",
    "def extract_skills(text):\n",
    "    found_skills = set()\n",
    "    for skill in skill_keywords:\n",
    "        if re.search(rf\"\\b{skill}\\b\", text, re.IGNORECASE):\n",
    "            found_skills.add(skill)\n",
    "    return found_skills\n",
    "\n",
    "# Define keywords for different sections in ATS-friendly resumes\n",
    "section_keywords = {\n",
    "    \"education\": [\"education\", \"academic\"],\n",
    "    \"experience\": [\"experience\", \"work history\", \"employment\"],\n",
    "    \"projects\": [\"projects\", \"responsibilities\"],\n",
    "    \"achievements\": [\"achievements\", \"awards\", \"honors\"],\n",
    "    \"skills\": [\"technical skills\", \"skills\", \"expertise\",\"programming language\",'languages','framework']\n",
    "}\n",
    "\n",
    "# Main function to parse resume and extract details\n",
    "def parse_resume(file_path):\n",
    "    # Extract text from the resume based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    # Process text with spaCy's NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract Name (using the first 'PERSON' entity found)\n",
    "    person_name = next((ent.text for ent in doc.ents if ent.label_ == \"PERSON\"), None)\n",
    "\n",
    "    # Extract email and phone number\n",
    "    email, phone = extract_contact_info(text)\n",
    "\n",
    "    # Extract different sections\n",
    "    education = extract_section(text, section_keywords[\"education\"])\n",
    "    experience = extract_section(text, section_keywords[\"experience\"])\n",
    "    projects = extract_section(text, section_keywords[\"projects\"])\n",
    "    achievements = extract_section(text, section_keywords[\"achievements\"])\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    # Output the extracted information\n",
    "    print(\"Name:\", person_name)\n",
    "    print(\"Email:\", email)\n",
    "    print(\"Phone:\", phone)\n",
    "    print(\"Education:\", education)\n",
    "    print(\"Experience:\", experience)\n",
    "    print(\"Projects:\", projects)\n",
    "    print(\"Achievements:\", achievements)\n",
    "    print(\"Technical Skills:\", skills)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"rohan resume.pdf\"  # or \"path/to/Resume.docx\"\n",
    "parse_resume(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Hevardhan Saravanan\n",
      "+91\n",
      "Email: hevardhan2004@gmail.com\n",
      "Phone: +91 9384565379\n",
      "Skills: ['Aws', 'Azure', 'Cloudplatforms', 'Java', 'Machinelearning', 'Python', 'R', 'Sql']\n",
      "Experience: {'Porto', 'Node.js', 'Random Forest Regressor', 'CSS', 'India', 'HTML/CSS', 'Random Forest Classifier', 'SMA', 'HTML', 'Oil', 'CNN', 'Support Vector Machine', 'GDSC', 'Sklearn, Tensorflow', 'TFT', 'Education\\nSymbiosis Institute of Technology Pune', 'IEEE Education Society', 'JavaScript', 'NumPy', 'Artificial Intelligence and', '• Implemented', 'Flask', 'Sklearn, Tensorflow, Pytorch', 'IBM', 'Google Developer Student Club', '• Gained', 'Convolutional Neural\\nNetworks', '3rd Place', 'Portugal', '• Working', 'SQL', 'Google Solution Challenge', 'Predictive Maintenance', 'Logistic Regression', 'SIT', 'Temporal Fusion Transformer', '• Applied', 'Amazon ML Summer School'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import re\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(file_path):\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "# Function to extract email and phone number using regex\n",
    "def extract_contact_info(text):\n",
    "    email = re.findall(r'\\b[\\w.%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone = re.findall(r'\\+?\\d[\\d -]{8,}\\d', text)\n",
    "    return email[0] if email else None, phone[0] if phone else None\n",
    "\n",
    "# Helper function to convert string to camel case\n",
    "def to_camel_case(text):\n",
    "    words = text.split()\n",
    "    return words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "\n",
    "# Define a comprehensive skill keywords list for various fields\n",
    "skill_keywords = [\n",
    "    \"Python\", \"Java\", \"Machine Learning\", \"Data Analysis\", \"SQL\", \"Data Analytics\", \"Critical Thinking\",\n",
    "    \"Problem Solving\", \"Leadership\", \"Communication\", \"Teamwork\", \"Risk Management\", \"Project Management Tools\",\n",
    "    \"Statistical Analysis\", \"Data Visualization\", \"R\", \"Tableau\", \"Big Data\", \"Agile Methodologies\", \"Digital Marketing\",\n",
    "    \"Marketing Software\", \"Customer Service\", \"Interpersonal Communication\", \"Human Resources Policies\", \"Recruitment\",\n",
    "    \"Software Development Methodologies\", \"Coding\", \"Debugging\", \"Software Testing\", \"Version Control\", \"Software Architecture\",\n",
    "    \"Agile\", \"Scrum\", \"Kanban\", \"Predictive Modeling\", \"Social Media Strategy\", \"Content Creation\", \"SEO\", \"Branding\",\n",
    "    \"Influencer Marketing\", \"HR Policies\", \"Employee Training\", \"Employee Wellness\", \"Budget Management\", \"Financial Analysis\",\n",
    "    \"Cost Control\", \"Strategic Planning\", \"Data Manipulation\", \"Excel\", \"Google Analytics\", \"Search Engine Rankings\",\n",
    "    \"Keyword Research\", \"Backlinking\", \"Website Audits\", \"Content Optimization\", \"Employee Engagement\", \"Workplace Safety\",\n",
    "    \"Materials Science\", \"Circuit Design\", \"Electrical Testing\", \"Cloud Platforms\", \"AWS\", \"Azure\", \"GCP\", \"Cloud Security\",\n",
    "    \"Project Planning\", \"Construction Management\", \"Geotechnical Engineering\", \"Healthcare Industry Knowledge\", \"Regulatory Compliance\",\n",
    "    \"Biotechnology\", \"Genetic Engineering\", \"Bioinformatics\", \"Cell Culture\", \"Data Analysis\", \"Electrical Engineering\",\n",
    "    \"Risk Assessment\", \"Technology Trends\", \"Mechanical Design Principles\", \"CAD Software\", \"Machine Design\", \"Quality Control\",\n",
    "    \"Event Planning\", \"Team Coordination\", \"Logistics Management\", \"Social Media Management\", \"Content Marketing\",\n",
    "    \"Stakeholder Management\", \"Supply Chain Management\", \"Lean Manufacturing\", \"Six Sigma\", \"Environmental Impact Assessment\",\n",
    "    \"Data Cleaning\", \"Feature Engineering\", \"Predictive Analytics\", \"Influencer Selection\", \"Content Collaboration\",\n",
    "    \"Financial Regulations\", \"Employee Morale\", \"Supplier Relationship Management\", \"Project Timelines\", \"Team Collaboration\",\n",
    "    \"IT Project Planning\", \"ETL Processes\", \"Big Data Technologies\", \"Hadoop\", \"Apache Spark\", \"Data Mining\", \"Data Preprocessing\",\n",
    "    \"Database Management\", \"Coding\", \"Debugging\", \"Version Control\", \"Software Testing\", \"Marketing Strategies\", \"Data Science\",\n",
    "    \"Software Design\", \"Problem-Solving\", \"Digital Advertising\", \"UX/UI Design\", \"B2B Marketing\", \"Lead Generation\",\n",
    "    \"Public Relations\", \"Compliance\", \"Regulatory Policies\", \"Budget Management\", \"Business Strategy\", \"Strategic HR Planning\",\n",
    "    \"Employee Retention\", \"Career Planning\", \"Mentorship\", \"Recruitment\", \"Diversity Management\", \"Customer Engagement\"\n",
    "]\n",
    "\n",
    "# Define patterns for Matcher to capture multi-word skills\n",
    "patterns = [\n",
    "    [{\"lower\": \"machine\"}, {\"lower\": \"learning\"}],\n",
    "    [{\"lower\": \"data\"}, {\"lower\": \"analysis\"}],\n",
    "    [{\"lower\": \"data\"}, {\"lower\": \"visualization\"}],\n",
    "    [{\"lower\": \"predictive\"}, {\"lower\": \"modeling\"}],\n",
    "    [{\"lower\": \"social\"}, {\"lower\": \"media\"}, {\"lower\": \"strategy\"}],\n",
    "    [{\"lower\": \"content\"}, {\"lower\": \"creation\"}],\n",
    "    [{\"lower\": \"search\"}, {\"lower\": \"engine\"}, {\"lower\": \"optimization\"}],\n",
    "    [{\"lower\": \"project\"}, {\"lower\": \"management\"}],\n",
    "    [{\"lower\": \"cloud\"}, {\"lower\": \"security\"}],\n",
    "    [{\"lower\": \"big\"}, {\"lower\": \"data\"}],\n",
    "    [{\"lower\": \"agile\"}, {\"lower\": \"methodologies\"}],\n",
    "    [{\"lower\": \"data\"}, {\"lower\": \"preprocessing\"}],\n",
    "    [{\"lower\": \"employee\"}, {\"lower\": \"engagement\"}],\n",
    "    [{\"lower\": \"employee\"}, {\"lower\": \"training\"}],\n",
    "    [{\"lower\": \"business\"}, {\"lower\": \"strategy\"}],\n",
    "    [{\"lower\": \"data\"}, {\"lower\": \"mining\"}],\n",
    "    [{\"lower\": \"cloud\"}, {\"lower\": \"platforms\"}],\n",
    "    [{\"lower\": \"data\"}, {\"lower\": \"cleaning\"}],\n",
    "    [{\"lower\": \"employee\"}, {\"lower\": \"morale\"}],\n",
    "    [{\"lower\": \"customer\"}, {\"lower\": \"engagement\"}],\n",
    "    [{\"lower\": \"event\"}, {\"lower\": \"planning\"}],\n",
    "    [{\"lower\": \"stakeholder\"}, {\"lower\": \"management\"}],\n",
    "    [{\"lower\": \"strategic\"}, {\"lower\": \"planning\"}],\n",
    "    [{\"lower\": \"employee\"}, {\"lower\": \"wellness\"}],\n",
    "    [{\"lower\": \"human\"}, {\"lower\": \"resources\"}]\n",
    "]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in patterns:\n",
    "    matcher.add(\"SKILL\", [pattern])\n",
    "\n",
    "# Main function to parse resume and extract specific details\n",
    "def parse_resume(file_path):\n",
    "    # Extract text from the resume based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    # Process text with spaCy's NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize fields\n",
    "    person_name = None\n",
    "    skills = set()\n",
    "    experience = set()\n",
    "\n",
    "    # Extract Name and Experience using NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and not person_name:\n",
    "            person_name = ent.text  # Assuming the first PERSON entity is the name\n",
    "        elif ent.label_ == \"ORG\" or ent.label_ == \"GPE\":\n",
    "            experience.add(ent.text)  # ORG represents companies, GPE for geographic locations often part of experience\n",
    "\n",
    "    # Extract skills using keywords and Matcher patterns\n",
    "    for token in doc:\n",
    "        skill = token.text.lower()  # Standardize to lowercase\n",
    "        if skill in map(str.lower, skill_keywords):\n",
    "            skills.add(to_camel_case(skill))  # Convert skill to camel case\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        skills.add(to_camel_case(span.text.lower()))  # Convert multi-word skills to camel case\n",
    "\n",
    "    # Extract email and phone number using regex\n",
    "    email, phone = extract_contact_info(text)\n",
    "\n",
    "    # Output the extracted information\n",
    "    print(\"Name:\", person_name)\n",
    "    print(\"Email:\", email)\n",
    "    print(\"Phone:\", phone)\n",
    "    \n",
    "    # Format the skills output in the same form as skill_keywords\n",
    "    if skills:\n",
    "        formatted_skills = sorted([skill.capitalize() for skill in skills])\n",
    "        print(\"Skills:\", formatted_skills)\n",
    "    else:\n",
    "        print(\"No skills matched\")\n",
    "\n",
    "    # Optionally, display experience\n",
    "    if experience:\n",
    "        print(\"Experience:\", experience)\n",
    "    else:\n",
    "        print(\"No experience matched\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Resume-1.pdf\"  # or \"path_to_resume.docx\"\n",
    "parse_resume(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Pandas\n",
      "Email: hevardhan2004@gmail.com\n",
      "Phone: +91 9384565379\n",
      "Organizations: {'Support Vector Machine', 'Amazon ML Summer School', 'Oil', 'IBM', 'CNN', 'Sklearn, Tensorflow, Pytorch', 'Google Developer Student Club', 'Education\\nSymbiosis Institute of Technology Pune', 'Artificial Intelligence and', 'Sklearn, Tensorflow', '3rd Place', 'Random Forest Regressor', 'SIT', 'SQL', 'HTML', 'HTML/CSS', '• Implemented', 'IEEE Education Society', 'TFT', 'Random Forest Classifier', 'Convolutional Neural\\nNetworks', '• Working', 'Logistic Regression', 'SMA', 'Temporal Fusion Transformer', 'Predictive Maintenance', 'Google Solution Challenge', 'JavaScript', '• Gained', 'CSS', 'GDSC', '• Applied'}\n",
      "\n",
      "Education: Symbiosis Institute of Technology Pune, India\n",
      "B.Tech in Artificial Intelligence and Machine Learning Aug. 2022 – June 2026\n",
      "\n",
      "Experience: Research Intern June 2024 – Present\n",
      "Symbiosis Centre for Applied AI Pune, India\n",
      "• Responsible for conducting a research on Predictive Maintenance of urban metro public transportation service in\n",
      "Porto, Portugal.\n",
      "Web Development Lead July 2024 – Present\n",
      "Google Developer Student Club (SIT) Pune, India\n",
      "\n",
      "Projects: Campus Ambassador July 2024 – Present\n",
      "GeeksForGeeks India\n",
      "\n",
      "Achievements: Participant, Amazon ML Summer School July 2024\n",
      "Amazon India\n",
      "Participant, Google Solution Challenge March 2024\n",
      "Google Developer Student Club India\n",
      "3rd Place, Build-a-Thon Hackathon Competition May 2024\n",
      "\n",
      "Technical Skills: {'Machine Learning', 'SQL', 'R', 'AWS', 'Java', 'Python', 'Azure'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pdfplumber\n",
    "import re\n",
    "import docx2txt\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Ensure text exists on the page\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_text_from_docx(file_path):\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "# Function to extract email and phone number using regex\n",
    "def extract_contact_info(text):\n",
    "    email = re.findall(r'\\b[\\w.%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone = re.findall(r'\\+?\\d[\\d -]{8,}\\d', text)\n",
    "    return email[0] if email else None, phone[0] if phone else None\n",
    "\n",
    "# Improved skill extraction using noun chunks and skill keywords\n",
    "def extract_skills(doc):\n",
    "    found_skills = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text.lower()\n",
    "        for skill in skill_keywords:\n",
    "            if skill.lower() in chunk_text:\n",
    "                found_skills.add(skill)\n",
    "    return found_skills\n",
    "\n",
    "# Improved section extraction by using patterns and looking for bullet points or phrases\n",
    "def extract_section(text, keywords):\n",
    "    section_content = []\n",
    "    capture = False\n",
    "    for line in text.splitlines():\n",
    "        line_lower = line.lower()\n",
    "        if any(keyword in line_lower for keyword in keywords):\n",
    "            capture = True\n",
    "            section_content = []  # Start fresh for each section\n",
    "            continue\n",
    "        elif capture and (line.strip() == \"\" or any(keyword in line_lower for keyword in section_keywords)):\n",
    "            break\n",
    "        elif capture:\n",
    "            section_content.append(line.strip())\n",
    "    return \"\\n\".join(section_content)\n",
    "\n",
    "# Extract name, organization, and other entities using refined NER patterns\n",
    "def extract_entities(doc):\n",
    "    entities = {\n",
    "        \"names\": set(),\n",
    "        \"organizations\": set()\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"names\"].add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            entities[\"organizations\"].add(ent.text)\n",
    "    # Return only the first name as the primary name\n",
    "    return list(entities[\"names\"])[0] if entities[\"names\"] else None, entities[\"organizations\"]\n",
    "\n",
    "# Define keywords for different sections in ATS-friendly resumes\n",
    "section_keywords = {\n",
    "    \"education\": [\"education\", \"academic\"],\n",
    "    \"experience\": [\"experience\", \"work history\", \"employment\"],\n",
    "    \"projects\": [\"projects\", \"responsibilities\"],\n",
    "    \"achievements\": [\"achievements\", \"awards\", \"honors\"],\n",
    "    \"skills\": [\"technical skills\", \"skills\", \"expertise\", \"programming languages\", \"languages\", \"frameworks\"]\n",
    "}\n",
    "\n",
    "# Define a refined list of skill keywords to capture technical and soft skills\n",
    "skill_keywords = [\n",
    "    \"Python\", \"Java\", \"Machine Learning\", \"Data Analysis\", \"SQL\", \"Data Analytics\", \"Critical Thinking\", \"Problem Solving\",\n",
    "    \"Leadership\", \"Communication\", \"Teamwork\", \"Risk Management\", \"Project Management\", \"Statistical Analysis\", \n",
    "    \"Data Visualization\", \"R\", \"Tableau\", \"Big Data\", \"Agile\", \"Scrum\", \"Predictive Modeling\", \"Digital Marketing\",\n",
    "    \"Marketing Strategy\", \"SEO\", \"Financial Analysis\", \"Content Marketing\", \"Cloud Computing\", \"AWS\", \"Azure\", \n",
    "    \"Machine Design\", \"CAD Software\", \"Quality Control\", \"UX/UI Design\", \"Data Science\", \"Software Testing\", \"Software Design\"\n",
    "]\n",
    "\n",
    "# Main function to parse resume and extract details\n",
    "def parse_resume(file_path):\n",
    "    # Extract text from the resume based on file type\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    # Process text with spaCy's NLP model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract contact info\n",
    "    email, phone = extract_contact_info(text)\n",
    "\n",
    "    # Extract name and organizations\n",
    "    person_name, organizations = extract_entities(doc)\n",
    "\n",
    "    # Extract skills\n",
    "    skills = extract_skills(doc)\n",
    "\n",
    "    # Extract sections\n",
    "    education = extract_section(text, section_keywords[\"education\"])\n",
    "    experience = extract_section(text, section_keywords[\"experience\"])\n",
    "    projects = extract_section(text, section_keywords[\"projects\"])\n",
    "    achievements = extract_section(text, section_keywords[\"achievements\"])\n",
    "\n",
    "    # Output the extracted information\n",
    "    print(\"Name:\", person_name)\n",
    "    print(\"Email:\", email)\n",
    "    print(\"Phone:\", phone)\n",
    "    print(\"Organizations:\", organizations)\n",
    "    print(\"\\nEducation:\", education)\n",
    "    print(\"\\nExperience:\", experience)\n",
    "    print(\"\\nProjects:\", projects)\n",
    "    print(\"\\nAchievements:\", achievements)\n",
    "    print(\"\\nTechnical Skills:\", skills)\n",
    "\n",
    "# Example usage\n",
    "file_path = \"Resume-1.pdf\"  # or \"path/to/Resume.docx\"\n",
    "parse_resume(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98b205f2a4143ad941d5ab99fb3b94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hevar\\.cache\\huggingface\\hub\\models--valhalla--t5-small-qg-prepend. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf576d13dd9b4a2ab1ae9d279a148474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df51df5fe5746188a30054d50566b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31110f4cc24f46ebac306b6d468f7579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf6bc7834d64dbb88744e67b165d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd35dfc19934f4183b7a06d81a7eed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which green plants and other organisms use sunlight to synthesize foods with the help of chlorophyll?\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load a question generation-specific T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "def generate_quiz_question(passage):\n",
    "    # Preprocess input with a clear instruction for question generation\n",
    "    input_text = f\"question: {passage}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate question with beam search and other parameters for better quality\n",
    "    outputs = model.generate(input_ids, max_length=50, num_beams=5, temperature=1.0, early_stopping=True)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return question\n",
    "\n",
    "# Example passage\n",
    "passage = \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\"\n",
    "print(generate_quiz_question(passage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which green plants and other organisms use sunlight to synthesize foods with the help of chlorophyll?\n"
     ]
    }
   ],
   "source": [
    "passage = \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\"\n",
    "print(generate_quiz_question(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Skill': 'Biology', 'Difficulty Level': 'Simple', 'Question': 'Photosynthesis is the process by which green plants and other organisms use sunlight to synthesize foods with the help of chlorophyll?', 'Choices': 'a. Option 1 related to the topic, b. Option 2 related to the topic, c. Option 3 (correct answer, extracted from passage), d. Option 4 related to the topic', 'Answer': 'c'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer (e.g., T5 for question generation)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "def generate_mcq(passage, skill=\"General Knowledge\", difficulty=\"Medium\"):\n",
    "    \"\"\"\n",
    "    Generates an MCQ based on a given passage.\n",
    "    \n",
    "    Parameters:\n",
    "    - passage: The input text for question generation\n",
    "    - skill: The skill or topic (optional)\n",
    "    - difficulty: Difficulty level of the question (optional)\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing the question, choices, and answer.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate the question from the passage\n",
    "    input_text = f\"question: {passage}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate question text\n",
    "    question_output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(question_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Step 2: Simulate answer options (in reality, a more complex setup is needed to get diverse options)\n",
    "    choices = [\n",
    "        \"a. Option 1 related to the topic\",\n",
    "        \"b. Option 2 related to the topic\",\n",
    "        \"c. Option 3 (correct answer, extracted from passage)\",\n",
    "        \"d. Option 4 related to the topic\"\n",
    "    ]\n",
    "    correct_answer = \"c\"  # This is set as a placeholder for demonstration\n",
    "\n",
    "    # Step 3: Output the MCQ in the desired format\n",
    "    mcq = {\n",
    "        \"Skill\": skill,\n",
    "        \"Difficulty Level\": difficulty,\n",
    "        \"Question\": question,\n",
    "        \"Choices\": \", \".join(choices),\n",
    "        \"Answer\": correct_answer\n",
    "    }\n",
    "    return mcq\n",
    "\n",
    "# Example passage\n",
    "passage = \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods with the help of chlorophyll.\"\n",
    "print(generate_mcq(passage, skill=\"Biology\", difficulty=\"Simple\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Explain standard deviation?\n",
      "Choices: a. Variation measure, b. Sum, c. Percentage, d. Average\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incorrect! The correct answer was 'a'. Quiz ended.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCongratulations! You answered all questions correctly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Run the quiz\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mrun_quiz_with_seq2seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 51\u001b[0m, in \u001b[0;36mrun_quiz_with_seq2seq\u001b[1;34m(num_questions)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Check if the answer is correct\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_answer \u001b[38;5;241m!=\u001b[39m correct_answer:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect! The correct answer was \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Quiz ended.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Incorrect! The correct answer was 'a'. Quiz ended."
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "# Load questions from the CSV file\n",
    "file_path = 'Skill_MCQs_with_Answers_updated.csv'\n",
    "mcq_data = pd.read_csv(file_path)\n",
    "\n",
    "def rephrase_question(passage):\n",
    "    \"\"\"Generates a rephrased question from a passage using a Seq2Seq model.\"\"\"\n",
    "    input_text = f\"question: {passage}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    question_output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    question = tokenizer.decode(question_output[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "def run_quiz_with_seq2seq(num_questions=5):\n",
    "    \"\"\"Run a quiz that mixes pre-existing and Seq2Seq-rephrased questions.\"\"\"\n",
    "    # Randomly sample questions from the dataset\n",
    "    questions = mcq_data.sample(n=num_questions).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        # Randomly decide to either use a pre-existing question or rephrase it with Seq2Seq\n",
    "        use_generated_question = random.choice([True, False])\n",
    "        \n",
    "        # if use_generated_question:\n",
    "            # Rephrase the question using Seq2Seq model\n",
    "        passage = questions.loc[i, 'Question']\n",
    "        question = rephrase_question(passage)\n",
    "        # else:\n",
    "            # Use the original question from the dataset\n",
    "            # question = questions.loc[i, 'Question']\n",
    "        \n",
    "        # Retrieve choices and correct answer directly from the dataset\n",
    "        choices = questions.loc[i, 'Choices']\n",
    "        correct_answer = questions.loc[i, 'Answer'].strip().lower()\n",
    "        \n",
    "        # Display the question and choices\n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "        print(f\"Choices: {choices}\")\n",
    "        \n",
    "        # Get the user's answer\n",
    "        user_answer = input(\"Enter the correct option (e.g., 'a', 'b', etc.): \").strip().lower()\n",
    "        \n",
    "        # Check if the answer is correct\n",
    "        if user_answer != correct_answer:\n",
    "            raise ValueError(f\"Incorrect! The correct answer was '{correct_answer}'. Quiz ended.\")\n",
    "        else:\n",
    "            print(\"Correct!\\n\")\n",
    "\n",
    "    print(\"Congratulations! You answered all questions correctly.\")\n",
    "\n",
    "# Run the quiz\n",
    "run_quiz_with_seq2seq()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:40\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     BaseModelOutput,\n\u001b[0;32m     33\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_transforms.py:50\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:95\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:387\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:25\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m---> 29\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_bfloat16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_bfloat16_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes.DType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDType\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDType\u001b[39;00m(_dtypes\u001b[38;5;241m.\u001b[39mDType):\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "# Load your data from CSV\n",
    "file_path = 'Skill_MCQs_with_Answers_updated.csv'\n",
    "mcq_data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the data for training\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Prepares the data for training\"\"\"\n",
    "    inputs = [f\"question: {q}\" for q in data['Question']]\n",
    "    outputs = data['Answer']  # Use the answer column as the output (could be rephrased question)\n",
    "    return {\"input_text\": inputs, \"output_text\": outputs}\n",
    "\n",
    "# Preprocess data and create dataset\n",
    "train_data = preprocess_data(mcq_data)\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "\n",
    "# Tokenize the inputs and outputs\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the input and output pairs\n",
    "    inputs = tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = tokenizer(examples[\"output_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory to save the model\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=8,   # Batch size per device\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_t5_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:40\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     BaseModelOutput,\n\u001b[0;32m     33\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\image_transforms.py:50\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:95\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:387\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:25\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m---> 29\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_bfloat16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_bfloat16_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes.DType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDType\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDType\u001b[39;00m(_dtypes\u001b[38;5;241m.\u001b[39mDType):\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define your custom dataset\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\hevar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\nUnable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define your custom dataset\n",
    "class QGDataset(Dataset):\n",
    "    def __init__(self, input_texts, output_texts, tokenizer, max_length=128):\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        output_text = self.output_texts[idx]\n",
    "\n",
    "        # Tokenize inputs and outputs\n",
    "        inputs = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "        outputs = self.tokenizer(output_text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "        # Prepare input and target tensors\n",
    "        input_ids = torch.tensor(inputs['input_ids'])\n",
    "        attention_mask = torch.tensor(inputs['attention_mask'])\n",
    "        labels = torch.tensor(outputs['input_ids'])\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-small-qg-prepend\")\n",
    "\n",
    "# Example data\n",
    "input_texts = [\"question: What is Python's GIL?\"]\n",
    "output_texts = [\"Explain the Global Interpreter Lock in Python.\"]\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = QGDataset(input_texts, output_texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': \"What is the use of the 'final' keyword?\", 'options': ['a. To make a variable constant', 'b. Define loop', 'c. Create thread', 'd. None'], 'correct_answer': 'a'}, {'question': 'Explain the concept of JVM.', 'options': ['a. Hardware component', 'b. Memory manager', 'c. Java bytecode interpreter', 'd. None'], 'correct_answer': 'c'}, {'question': 'What is Java?', 'options': ['a. A script', 'b. Operating System', 'c. Programming Language', 'd. Database'], 'correct_answer': 'c'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('Skill_MCQs_with_Answers_updated.csv')  # Replace 'questions.csv' with your CSV file path\n",
    "\n",
    "# List of skills you want to filter by\n",
    "skills_list = ['{Python', 'Java', 'html']  # Replace with your actual skills\n",
    "\n",
    "# Filter questions based on the list of skills\n",
    "filtered_questions = df[df['Skill'].isin(skills_list)]\n",
    "\n",
    "# Select 3 random questions\n",
    "random_questions = filtered_questions.sample(n=3)\n",
    "\n",
    "# Convert the selected questions to the desired format\n",
    "quiz_data = [\n",
    "    {\n",
    "        \"question\": row['Question'],\n",
    "        \"options\": row['Choices'].split(\", \"),  # Assumes choices are comma-separated in the CSV\n",
    "        \"correct_answer\": row['Answer']\n",
    "    }\n",
    "    for _, row in random_questions.iterrows()\n",
    "]\n",
    "\n",
    "# Display the quiz data\n",
    "print(quiz_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
