{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a958a5-dc45-4f2c-bed3-44318ad0f657",
   "metadata": {},
   "source": [
    "Author of this notebook: Joshua Albiez <br>\n",
    "Mat. Nr.: 2124356"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5135214-99c1-4a37-a8bd-f599f715d139",
   "metadata": {},
   "source": [
    "# 1. Analysis of the Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70eef1-e2d1-4b29-8546-bb140730fdc1",
   "metadata": {},
   "source": [
    "## Understand the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed71874",
   "metadata": {},
   "source": [
    "This notebooks uses the following dataset <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures\n",
    "\n",
    "related paper:<br>\n",
    "https://ieeexplore.ieee.org/document/6909419<br>\n",
    "https://ieeexplore.ieee.org/document/6973901\n",
    "\n",
    "Dataset with\n",
    "* the hand gesture/'Class' (our target value)\n",
    "* the person which performed the hand gesture ('User')\n",
    "* feature vector that consists 11 subvectors. Each subvector contains X, Y and Z coordinates. Those coordinates belong to one of the detected markers on the hand glove the user is wearing\n",
    "\n",
    "Setup\n",
    "* motion capture camera records 12 users performing 5 hand postures with markers attached to a left-handed glove\n",
    "* rigid pattern of markers on the back of the glove -> establish local coordinate system for the hand\n",
    "* 11 markers were attached to the thumb and fingers of the glove\n",
    "* there is no a priori correspondence between the markers of two given records\n",
    "* due to the resolution of the capture volume and self-occlusion due to the orientation and configuration of the hand and fingers, many records have missing markers.\n",
    "    -> the number of visible markers in a record varied considerably.\n",
    "\n",
    "\n",
    "The Problem:\n",
    "We cannot easily apply traditional approaches because of two properties of point clouds:\n",
    "* unordered collection (Point 1 with X,Y and Z coordinates could refer to the marker on the ring finger in recording A and to the marker on the palm in recording B)\n",
    "* the size of the point cloud varies (due to occlusion, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6515bd4-4c0f-4900-8aca-67a236ca7cbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img/hand.png\" width=330 height=310 /> <br>\n",
    "source: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6909419"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c869c7",
   "metadata": {},
   "source": [
    "## Useful Information from the authors/paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0019a",
   "metadata": {},
   "source": [
    "Class label:<br>\n",
    "* 1=Fist(with thumb out)\n",
    "* 2=Stop(hand flat)\n",
    "* 3=Point1(point with pointer finger)\n",
    "* 4=Point2(point with pointer and middle fingers)\n",
    "* 5=Grab(fingers curled as if to grab)\n",
    "\n",
    "Preprocessing:<br>\n",
    "* all markers were transformed to the local coordinate system of the record containing them.\n",
    "\n",
    "Reduce number of records:<br>\n",
    "* each transformed marker with a norm greater than 200 millimeters was pruned. \n",
    "* records that contained fewer than 3 markers was removed. \n",
    "* the data has at most 12 markers per record and at least 3\n",
    "\n",
    "Be careful!!:<br>\n",
    "* It is likely that for a given record and user there exists a near duplicate record originating from the same user.\n",
    "    - \"[..] were captured as part of intermittent streams where the user held the posture for a short time\" (cite from paper)\n",
    "* -> evaluate on leave-one-user-out basis\n",
    "* This mistake was actually made by people on kaggle and medium.com:\n",
    "    * They simply mixxed the data (each user is spread across the data set)\n",
    "    * successive data points are very similar and describe almost the same point in the feature space.\n",
    "    * If we now \"blindly\" divide into training data and test data, then similar points are contained in both data sets\n",
    "    * Thus, an extremely high accuracy is possible (98 - 99%)\n",
    "* I included a small demo on how not to it (It is clearly denoted as \"WRONG WAY\" or \"only demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effceef9",
   "metadata": {},
   "source": [
    "## Ideas I want to try out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4c635",
   "metadata": {},
   "source": [
    "\n",
    "**My goal**\n",
    "\n",
    "* able to predict which gesture a person is performing\n",
    "\n",
    "* achieve a high accuracy\n",
    "\n",
    "**My idea/approach**\n",
    "\n",
    "* 1.approach: use the feature as they are - raw data (of course I will preprocess the data, therefore the name might be a bit missleading)\n",
    "\n",
    "* 2.approach: extract features that are meaningful for a given data point, train conventional models\n",
    "\n",
    "* 3.approach: use models that are adapted to point clouds and have been developed specifically for this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ff76df",
   "metadata": {},
   "source": [
    "# 2. Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e1faa-b515-472d-8b78-a5489e1d1003",
   "metadata": {},
   "source": [
    "## Preparing the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbe7a7-59b4-4e40-badd-440f2348011a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "%load_ext autotime\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 60\n",
    "import time\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "from tensorflow.keras import layers\n",
    "from typing import Tuple, Optional, Callable\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# My custom functions\n",
    "from scripts import analyze_helper, visualisation\n",
    "\n",
    "# Utility functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve, average_precision_score, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from os.path import dirname, abspath, join\n",
    "from tqdm.keras import TqdmCallback\n",
    "from random import shuffle\n",
    "\n",
    "# Models we want to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae7bb3",
   "metadata": {},
   "source": [
    "## Write utility functions I want to use multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3b63d",
   "metadata": {},
   "source": [
    "* it helps to simplify and streamline the code.\n",
    "* reusable pieces of code that perform specific tasks or calculations.\n",
    "* Even if I used a code snippet only twice -> I wrote a function (at least I tried)\n",
    "* improves the readability and maintainability of the code\n",
    "* By breaking down complex tasks into smaller, more manageable functions, the code becomes easier to understand and modify. can also help with testing and debugging. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(y_true, y_pred):\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    class_labels = ['Fist', 'Stop', 'Point1', 'Point2', 'Grab']\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "    #normalize\n",
    "    cf_matrix = (cf_matrix.T/cf_matrix.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(5,5)\n",
    "\n",
    "    custom_cmap = sns.light_palette(\"#009682\", as_cmap=True)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap=custom_cmap, xticklabels=class_labels, yticklabels=class_labels, cbar=False)\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report:\n",
    "\n",
    "    report: dict\n",
    "\n",
    "    def __init__(self, model_name:str, model, X_test, y_test, description:list=[]):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.description = description\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.report = self.create_report(model)\n",
    "\n",
    "\n",
    "    def create_report(self, model):\n",
    "\n",
    "        try:\n",
    "            if type(model) == keras.Sequential or type(model) == SVC:\n",
    "                y_score = model.predict(self.X_test)\n",
    "            else:\n",
    "                y_score = model.predict_proba(self.X_test)\n",
    "        except AttributeError:\n",
    "            print('No report possible, because no predict_proba method')\n",
    "            report = None\n",
    "            return\n",
    "        y_score_2 = model.predict(self.X_test)\n",
    "\n",
    "        if type(model) != keras.Sequential:\n",
    "            report = classification_report(self.y_test, y_score_2, output_dict=True)\n",
    "        else:\n",
    "            y_score_2 = to_categorical(np.argmax(y_score_2, axis=1))\n",
    "            report = classification_report(self.y_test, y_score_2, output_dict=True)\n",
    "            #NN report does not have accuracy score -> have a look why\n",
    "            accuracy = accuracy_score(self.y_test, y_score_2)\n",
    "            report['accuracy'] = accuracy\n",
    "\n",
    "        return report\n",
    "    \n",
    "    def get_report_as_df(self, df_to_save_to:pd.DataFrame):\n",
    "        'Get the report as a dataframe'\n",
    "\n",
    "        new_row = {'model': self.model_name,\n",
    "                   'Accuracy': round(self.report[\"accuracy\"],6), \n",
    "                   'Precision': round(self.report[\"macro avg\"][\"precision\"],5),\n",
    "                   'Recall': round(self.report[\"macro avg\"][\"recall\"],6),\n",
    "                   'F1_score': round(self.report[\"macro avg\"][\"f1-score\"],6)} \n",
    "        df_to_save_to = df_to_save_to.append(new_row, ignore_index=True)\n",
    "\n",
    "        return df_to_save_to\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        if self.report is None:\n",
    "            print('No return possbile')\n",
    "            return 'No valid return'\n",
    "\n",
    "        pattern = '''\n",
    "        ***********************REPORT*******************************\n",
    "        Average (macro) precision: {}\n",
    "        Average accuracy: {}\n",
    "        Average (macro) recall: {}\n",
    "        Average (macro) f1-score: {}\n",
    "        Description {}\n",
    "        ************************************************************\n",
    "        '''\n",
    "        return pattern.format(round(self.report[\"macro avg\"][\"precision\"],5), round(self.report[\"accuracy\"],6), round(self.report[\"macro avg\"][\"recall\"],6), round(self.report[\"macro avg\"][\"f1-score\"],6), ', '.join(self.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5843a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_dict_pkl(eval_dict, name):\n",
    "\n",
    "    # Get path to save the history\n",
    "    eval_folder = 'model/single_run/eval'\n",
    "    model_path = join(eval_folder, name)\n",
    "    \n",
    "    with open(model_path, 'wb') as file_pi:\n",
    "        pickle.dump(eval_dict, file_pi)\n",
    "\n",
    "def load_eval_dict_pkl(name) -> dict:\n",
    "\n",
    "    # Get path to save the history\n",
    "    eval_folder = 'model/single_run/eval'\n",
    "    model_path = join(eval_folder, name)\n",
    "\n",
    "    with open(model_path, 'rb') as file_pi:\n",
    "        eval_dict = pickle.load(file_pi)\n",
    "    \n",
    "    return eval_dict\n",
    "\n",
    "def save_to_eval_dict(eval_dict:dict, split_set:str, acc:float, precission:float, recall:float, f1:float):\n",
    "    ''' Save multiple metrics to a dictionary for evaluation '''\n",
    "\n",
    "    if split_set not in ['train', 'test']:\n",
    "        raise ValueError('split_set must be either train or test')\n",
    "    \n",
    "    eval_dict[split_set]['accuracy'].append(acc)\n",
    "    eval_dict[split_set]['precision'].append(precission)\n",
    "    eval_dict[split_set]['recall'].append(recall)\n",
    "    eval_dict[split_set]['f1'].append(f1)\n",
    "\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_dict_barplot_new(data):\n",
    "\n",
    "    # Extracting data\n",
    "    train_acc = data['train']['accuracy']\n",
    "    train_prec = data['train']['precision']\n",
    "    train_recall = data['train']['recall']\n",
    "    train_f1 = data['train']['f1']\n",
    "    test_acc = data['test']['accuracy']\n",
    "    test_prec = data['test']['precision']\n",
    "    test_recall = data['test']['recall']\n",
    "    test_f1 = data['test']['f1']\n",
    "\n",
    "    # Creating subplots for accuracy and precision\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 5))\n",
    "    bar_width = 0.2\n",
    "\n",
    "    # Plot for accuracy\n",
    "    axs[0][0].bar([i+bar_width/2 for i in range(len(train_acc))], train_acc, width=bar_width, color='b', label='Train Accuracy')\n",
    "    axs[0][0].bar([i-bar_width/2 for i in range(len(test_acc))], test_acc, width=bar_width, color='r', label='Test Accuracy')\n",
    "    axs[0][0].set_xticks(range(len(train_acc)))\n",
    "    axs[0][0].set_xticklabels([f'CV{i}' for i in range(len(train_acc))])\n",
    "    axs[0][0].set_ylabel('Accuracy')\n",
    "    axs[0][0].set_title('Accuracy')\n",
    "    axs[0][0].legend()\n",
    "\n",
    "    # Plot for precision\n",
    "    axs[0][1].bar([i+bar_width/2 for i in range(len(train_prec))], train_prec, width=bar_width, color='b', label='Train Precision')\n",
    "    axs[0][1].bar([i-bar_width/2 for i in range(len(test_prec))], test_prec, width=bar_width, color='r', label='Test Precision')\n",
    "    axs[0][1].set_xticks(range(len(train_acc)))\n",
    "    axs[0][1].set_xticklabels([f'CV{i}' for i in range(len(train_acc))])\n",
    "    axs[0][1].set_ylabel('Precision')\n",
    "    axs[0][1].set_title('Precision')\n",
    "    axs[0][1].legend()\n",
    "\n",
    "    # Plot for recall\n",
    "    axs[1][0].bar([i+bar_width/2 for i in range(len(train_recall))], train_recall, width=bar_width, color='b', label='Train Recall')\n",
    "    axs[1][0].bar([i-bar_width/2 for i in range(len(test_recall))], test_recall, width=bar_width, color='r', label='Test Recall')\n",
    "    axs[1][0].set_xticks(range(len(train_recall)))\n",
    "    axs[1][0].set_xticklabels([f'CV{i}' for i in range(len(train_recall))])\n",
    "    axs[1][0].set_ylabel('Recall')\n",
    "    axs[1][0].set_title('Recall')\n",
    "    axs[1][0].legend()\n",
    "\n",
    "    # Plot for f1_score\n",
    "    axs[1][1].bar([i+bar_width/2 for i in range(len(train_f1))], train_f1, width=bar_width, color='b', label='Train F1-Score')\n",
    "    axs[1][1].bar([i-bar_width/2 for i in range(len(test_f1))], test_f1, width=bar_width, color='r', label='Test F1-Score')\n",
    "    axs[1][1].set_xticks(range(len(train_f1)))\n",
    "    axs[1][1].set_xticklabels([f'CV{i}' for i in range(len(train_f1))])\n",
    "    axs[1][1].set_ylabel('F1 Score')\n",
    "    axs[1][1].set_title('F1 Score')\n",
    "    axs[1][1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(grid_search_list, grid_param_1, grid_param_2):\n",
    "    \n",
    "    for idx, grid_search in enumerate(grid_search_list):\n",
    "        print('Grid Search #', idx)\n",
    "        data = grid_search.cv_results_\n",
    "    \n",
    "        # extract relevant data\n",
    "        params = [d[grid_param_1] for d in data['params']]\n",
    "        param2_values = sorted(list(set([d[grid_param_2] for d in data['params']])))\n",
    "        scores = data['mean_test_score']#np.array([d['mean_test_score'] for d in data])\n",
    "\n",
    "        # create figure\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # plot scores for each value of param2\n",
    "        for i, param2_value in enumerate(param2_values):\n",
    "            x = [params[j] for j in range(len(params)) if data['params'][j][grid_param_2] == param2_value]\n",
    "            y = [scores[j] for j in range(len(params)) if data['params'][j][grid_param_2] == param2_value]\n",
    "\n",
    "            # Get all indices with the same x value:\n",
    "            # get unique values and their counts\n",
    "            unique_values, value_counts = np.unique(x, return_counts=True)\n",
    "            # loop through unique values and get indices of elements with that value\n",
    "            indices_by_value = []\n",
    "            for value in unique_values:\n",
    "                indices_by_value.append(np.where(x == value)[0])\n",
    "            x = unique_values\n",
    "            # Calculate the mean of y values that have the same x value\n",
    "            y = [np.mean([y[i] for i in indices_by_value[k]]) for k in range(len(unique_values))]\n",
    "            \n",
    "            ax.plot(x, y, label=f'{grid_param_2}={param2_value}')\n",
    "\n",
    "        # set axis labels and legend\n",
    "        ax.set_xlabel(grid_param_1)\n",
    "        ax.set_ylabel('Mean Test Score')\n",
    "        ax.legend(title=grid_param_2)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbaf5d-96bf-4e9c-baf4-905ed859f7c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07e663-c32d-4b98-9088-57c49d18b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_raw = os.path.join('data', 'Postures.csv')\n",
    "df_raw = pd.read_csv(file_path_raw, sep=',', na_values='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b72cb9",
   "metadata": {},
   "source": [
    "## Understand the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8e0c7",
   "metadata": {},
   "source": [
    "### Choose a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f530b1",
   "metadata": {},
   "source": [
    "To choose a metric we have to check whether the classes are balanced/unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc896a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_raw['Class'].value_counts(sort=False)\n",
    "class_counts.plot(kind='bar', title='Gestures Class Distibution')\n",
    "plt.xlim(0.5, df_raw['Class'].max()+0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b007c1",
   "metadata": {},
   "source": [
    "The classes are quite balanced. Therefore we do not have to adjust/use metrics that are for unbalanced datasets.\n",
    "\n",
    "To jugde the model perfomance we will choose a mixture of multiple metrics:\n",
    "* Confusion Matrix\n",
    "* Accuracy (we have balanced classes!)\n",
    "* Recall (made TP predictions divided by possible TP predictions)\n",
    "* precission-recall plot\n",
    "* F1-Score (harmonic mean between recall and precision, combines the two metrics into one value)\n",
    "\n",
    "How do we calculate the Precision-Recall-Curve? (just in theory to understand it better, we will use sklearn for convenience)\n",
    "- curve is constructed by calculating and plotting the **precision** \n",
    "against the **recall** for a single classifier at a variety of **thresholds**\n",
    "- Normally, if an observation is predicted to belong to the positive class at probability > 0.5, it is labeled as positive (binary case)\n",
    "- However, we could really choose any probability threshold \n",
    "between 0 and 1!\n",
    "- curve helps to visualize how the choice of threshold affects classifier performance, and can even help us select the best threshold for a specific problem.\n",
    "- To calculate the precision-recall curve, we need to vary the **probability threshold** \n",
    "that the classifier uses to predict whether an instance is positive (target=1) or negative (target=0) and calculate the precision and recall for each\n",
    "- at thresholds with low recall, the precision is correspondingly high, and at very high recall, the precision begins to drop.\n",
    "Note: we will use the One vs Rest strategy for Precision-Recall-Curve (because it is usally a metric for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bb13b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision_recall_multiclass(model, X_test, y_test):\n",
    "\n",
    "    plt.clf()\n",
    "    \n",
    "    if type(model) == keras.Sequential or type(model) == SVC:\n",
    "        y_score = model.predict(X_test)\n",
    "    else:\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    #print('Amount and Distribution of Test Data: \\n', y_test.value_counts())\n",
    "    y_bin = pp.label_binarize(y_test, classes=model.classes_)\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    classes = model.classes_\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_bin[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_bin[:, i], y_score[:, i])\n",
    "        #print('\\n average precision: ', classes[i], ': ', average_precision[i])\n",
    "        try:\n",
    "            plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(classes[i]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"precision vs. recall curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640a001-a477-4f13-a4ec-c9f9306bf727",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1b443-1126-4ad6-9236-4d325e12dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29bafb-29a4-4f25-9d9b-86cfba219b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns have missing values\n",
    "print(f\"Missing values in: {analyze_helper.check_for_missing_vals(df_raw)}\")\n",
    "# Compute missing ratio, hide columns with no missing values (0.0%)\n",
    "analyze_helper.compute_missing_ratio(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc34347",
   "metadata": {},
   "source": [
    "Hint 1 for preprocessing:\n",
    "Considering the fact that feature point 11 with its 3 coordinates is missing 99% of the data, we omit this data. Also considering the fact that we would not need point 11 in a possible inference, it is better to delete its data.\n",
    "&rarr; drop the coordinates (X,Y,Z) for point 10 and 11\n",
    "\n",
    "We can also see that every data entry has at least values for Point 0, 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba93bd9c-0bea-4be4-96f7-3d3c3b250437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cf151-48ba-421c-8d3a-0d3f7673004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head(5)\n",
    "df_raw.drop([0], inplace=True)\n",
    "df_raw.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of Instances  : ', df_raw.shape[0])\n",
    "print('Number of Attributes : ', df_raw.shape[1])\n",
    "print('Number of target classes   : ', df_raw['Class'].nunique())\n",
    "print('Number of users   : ', df_raw['User'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c73d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['User'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7cb979",
   "metadata": {},
   "source": [
    "The description says the data set contains 12 User. No information provided (why are 14 user in the data?).\n",
    "Hint for us to drop User 4 and 7 ?\n",
    "They both have signifiantly less data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848511d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_group = df_raw.groupby(['User'], sort=False)\n",
    "user_group.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabc13a-c57a-4318-92a6-1e5e364d8c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#table = pd.pivot_table(df_raw, index=['User', 'Class'], aggfunc='mean')\n",
    "table = df_raw.groupby(['User', 'Class']).agg(['mean', 'count'])\n",
    "# Here you have to use multindex indexation!\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60989e0-5d3b-4ad2-95c6-1fbdaa290726",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24b9ed",
   "metadata": {},
   "source": [
    "#### General plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084054c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltHand(handPoints):\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    for i in range(11):\n",
    "        pntx = f'X{i}'\n",
    "        pnty = f'Y{i}'\n",
    "        pntz = f'Z{i}'\n",
    "        \n",
    "        if(handPoints[pntx].values[0] == 0 or\n",
    "            handPoints[pnty].values[0] == 0 or\n",
    "            handPoints[pntz].values[0] == 0):\n",
    "            n = 0;\n",
    "        else:\n",
    "            xlocation = handPoints[pntx]\n",
    "            ylocation = handPoints[pnty]\n",
    "            zlocation = handPoints[pntz]\n",
    "            ax.scatter(xlocation, ylocation, zlocation, marker='v')\n",
    "    \n",
    "    crntClass = handPoints['Class'].values[0]\n",
    "    if (crntClass == 1):\n",
    "        title = 'Fist + Thumb out'\n",
    "    if(crntClass == 2):\n",
    "        title = 'Stop/Flat hand'\n",
    "    if (crntClass == 3):\n",
    "        title = 'Point with pointer finger'\n",
    "    if (crntClass == 4):\n",
    "        title = 'Point with pointer + middle finger'\n",
    "    if (crntClass == 5):\n",
    "        title = 'Grab'\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58feb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterPlot2D(x_DF, y_DF, labels:list):\n",
    "    #df = pd.DataFrame(data=x_DF.iloc[:,0:2], index=x_DF.index)\n",
    "    df = pd.DataFrame(data=x_DF, index=x_DF.index)\n",
    "    df = pd.concat((df,y_DF), axis=1, join=\"inner\")\n",
    "    df.columns = [\"First Dimension\", \"Second Dimension\", \"Class\"]\n",
    "    sns.lmplot(x=\"First Dimension\",y=\"Second Dimension\", hue=\"Class\", data=df, fit_reg=False)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Visualization of the data segregation using \")\n",
    "\n",
    "def scatterPlot3D(x_DF, y_DF, labels:dict):\n",
    "    df = pd.DataFrame(data=x_DF.iloc[:,0:3], index=x_DF.index)\n",
    "    df = pd.concat((df,y_DF), axis=1, join=\"inner\")\n",
    "    fig = px.scatter_3d(df, x=df.columns[0], y=df.columns[1], z=df.columns[2],\\\n",
    "                          color='Class', symbol='Class', opacity=0.7, \\\n",
    "                          color_continuous_scale=px.colors.sequential.Viridis, width = 600, height = 500,\n",
    "                       labels=labels) \n",
    "\n",
    "    title = \"Visualization of the data segragation using \"\n",
    "    fig.update_layout(title_text=title, showlegend = True, hovermode = False)\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "    fig.update_layout(legend=dict(yanchor=\"top\",y=0.99,xanchor=\"left\",x=0.01))\n",
    "\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68869c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random hand gesture from the dataset to get a an idea of the data\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "for _ in range(4):\n",
    "    rand_dp = np.random.randint(df_raw.shape[0], size=1)[0]\n",
    "    pltHand(df_raw[rand_dp:rand_dp+1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df: pd.DataFrame):\n",
    "\n",
    "    correlationMatrix = pd.DataFrame(df_raw).corr() \n",
    "    f = plt.figure(figsize=(14, 8))\n",
    "    plt.matshow(correlationMatrix, fignum=f.number)\n",
    "    plt.xticks(range(df_raw.shape[1]), df_raw.columns, fontsize=14, rotation=75)\n",
    "    plt.yticks(range(df_raw.shape[1]), df_raw.columns, fontsize=14)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd25f8-8672-4502-b91b-ba5774ca7fee",
   "metadata": {},
   "source": [
    "We can see that the features correlate in a threefold pattern: Coordinates (X, Y, Z) of the 1 point correlate with each other, those of the second point and so on. Thus, there is no possibility to neglect strongly correlated characteristics. (can also be considered good because we do not have to consider a strategy. If all strongly correlated features are used, they would be weighted more. This can lead to problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89d258-53ae-43fb-8b22-2c09e406af70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe we want to store x_mean, y_mean and z_mean in\n",
    "df_xyz = pd.DataFrame()\n",
    "\n",
    "# We dont want to change the original data set\n",
    "df_raw_dummy = df_raw.copy(deep=True)\n",
    "\n",
    "# Save the user and class column\n",
    "df_user = df_raw_dummy.pop('User')\n",
    "df_class = df_raw_dummy.pop('Class')\n",
    "\n",
    "# We replace all zeros with NaN, so the mean calculatiion ignores occluded data points\n",
    "df_raw_dummy = df_raw_dummy.replace(0, np.NaN)\n",
    "\n",
    "# Extract the X, Y and Z columns\n",
    "df_x_vals = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('X')]]\n",
    "df_y_vals = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('Y')]]\n",
    "df_z_vals = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('Z')]]\n",
    "\n",
    "# Extract the mean of the X, Y and Z columns\n",
    "for coordinate in ['X', 'Y', 'Z']:\n",
    "    coordinate_cols = df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith(coordinate)]\n",
    "    df_xyz[f'{coordinate}_mean'] = df_raw_dummy[coordinate_cols].mean(axis=1)\n",
    "    df_xyz[f'{coordinate}_min'] = df_raw_dummy[coordinate_cols].min(axis=1)\n",
    "    df_xyz[f'{coordinate}_max'] = df_raw_dummy[coordinate_cols].max(axis=1)\n",
    "\n",
    "# Add user and class label\n",
    "df_xyz['User'] = df_user\n",
    "df_xyz['Class'] = df_class\n",
    "# Change NaN back to 0 values\n",
    "df_raw_dummy = df_raw_dummy.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c80235-6a45-4fd3-92d2-f94d845e3353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789c671-17a3-4ed5-a741-cbc26b6173d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_xyz = df_xyz.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749aa36-6fd1-4ccf-90ab-771427fb022b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85188bca-0270-4873-9731-2b209e17816c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = ['Fist', 'Stop', 'Point1', 'Point2', 'Grab']\n",
    "df_xyz = df_xyz.reset_index(drop=True)\n",
    "scatterPlot3D(df_xyz[['X_mean', 'Y_mean', 'Z_mean']], df_class, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0d3ef-4573-4f09-8d32-8d856e6fc5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same plot with minimum values\n",
    "scatterPlot3D(df_xyz[['X_min', 'Y_min', 'Z_min']], df_class, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c02ccb-f6e0-48b8-8665-7c04ea290711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same plot with maximum values\n",
    "scatterPlot3D(df_xyz[['X_max', 'Y_max', 'Z_max']], df_class, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a0790-7986-4d54-9e4c-174488923637",
   "metadata": {},
   "source": [
    "Next, I want to look at the class distribution for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69fcc2-522d-40bd-afe6-d79a6b0d9935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig_size = 4\n",
    "fig, ax =plt.subplots(fig_size, fig_size,figsize=(20,20))\n",
    "plotcounter = 0\n",
    "for user in range(0,15):\n",
    "    df_user = pd.DataFrame(df_raw.loc[df_raw['User']==user])# df[df['User'] == user].index\n",
    "    df_user.reset_index(drop=True)\n",
    "    if df_user.shape[0] == 0:\n",
    "        continue\n",
    "    plotcounter += 1\n",
    "    sns.countplot(data = df_user, x = 'Class', ax=ax[(plotcounter-1)//fig_size, (plotcounter-1)%fig_size])\n",
    "    ax[(plotcounter-1)//fig_size, (plotcounter-1)%fig_size].set_title(f'User number {user}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168a436-35d3-4569-a9cf-21fa3e7bd7ed",
   "metadata": {},
   "source": [
    "Although there are some fluctuations in the class distribution per user, more than one user is always used for validation (for training anyway). This means that a user with an uneven class distribution is compensated for by other users. The overall class distribution (i.e. all users considered together) is balanced, as we have seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e871579",
   "metadata": {},
   "source": [
    "##### Feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800be8f5",
   "metadata": {},
   "source": [
    "In this section I want to analysis the features and their distribution in detail. This can give as further hints for the preprocession step. I am not only interested in the distribution of the features in isolation, but also about their interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58b2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boxplots(df: pd.DataFrame, y_label:str, title:str):\n",
    "    plt.clf()\n",
    "    ax = sns.boxplot(data = df, linewidth=1, showfliers=True)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    sns.set(rc = {'figure.figsize':(10,10)})\n",
    "    ax.set(ylabel=y_label)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753d75c-c7a6-4d8c-a2d3-12e9a7e1878f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, df_coord in zip(['X', 'Y', 'Z'] , [df_x_vals, df_y_vals, df_z_vals]):\n",
    "    boxplots(df_coord, f'{i}-value', f'Feature Distribution: {i}-Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbeab05-16a3-4525-b220-c4b197ce67ca",
   "metadata": {},
   "source": [
    "The distribution of feature 11 is clearly different from the others. Since we have already decided to discard this feature due to too much missing data, we will not look at it in more detail.\n",
    "\n",
    "We can see that some of the data is not tightly grouped. But since there are multiple \"outliers\" in almost all cases, we assume that the values are legitimate observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5807af-81a1-4592-bcab-a1859df39056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outlier_detection(df, col:str, n:int):\n",
    "    '''\n",
    "    n: number of smallest/biggest value to print\n",
    "    '''\n",
    "    # Sort the DataFrame by the col in ascending order\n",
    "    df_sorted = df.sort_values(col)\n",
    "    \n",
    "    # Print the n smallest values in the col\n",
    "    smallest_values = df_sorted[col].head(n)\n",
    "    print(f\"***The {n} smallest values in the {col} column are:***\\n{smallest_values}\")\n",
    "\n",
    "    # Print the n biggest values in the col\n",
    "    largest_values = df_sorted[col].tail(n)\n",
    "    print(f\"***The {n} largest values in the {col} column are:***\\n{largest_values}\")\n",
    "\n",
    "    # Print the stat values of the col\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    print(f\"***The mean value of the {col} column is: {mean} +- std: {std}***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b15be-d743-4c3d-af38-7d095816a933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outlier_detection(df_raw.copy(), 'Y0', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1cb1d-9515-4469-85ff-ba9df8b95c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outlier_detection(df_raw.copy(), 'Y9', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b3ab7-8034-4987-93ab-f0657bd5bb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outlier_detection(df_raw.copy(), 'Z1', 4)\n",
    "outlier_detection(df_raw.copy(), 'Z4', 4)\n",
    "outlier_detection(df_raw.copy(), 'Z8', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a94b87-ad1c-4629-b239-dbfa32cef3ef",
   "metadata": {},
   "source": [
    "Here I would like to emphasize again that the data points are unordered! For example, the coordinates of the marker from the thumb can be assigned to point 1 in one of the data sets and to point 9 in another data set. Therefore, it is impossible to analyze each feature column individually to understand the distribution of a point. All points are represented to a certain extent in each individual column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243aae0-96f6-4511-ac3e-5938cfc42760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for coord in ['X', 'Y', 'Z']:\n",
    "    plt.clf()\n",
    "    ax = sns.boxplot(data=df_xyz[[f'{coord}_mean', 'Class']], x=\"Class\", y=f'{coord}_mean')\n",
    "    ax.set_title(f'Distribution of {coord}_mean for each Class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad24aa0-764e-460a-9422-664adbbdf956",
   "metadata": {},
   "source": [
    "Let's see how the data is distributed among the individual users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d762-8e25-4208-a2ce-a14544382f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for coord in ['X', 'Y', 'Z']:\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    ax = sns.boxplot(data=df_xyz[[f'{coord}_mean', 'User']], x=\"User\", y=f'{coord}_mean')\n",
    "    sns.set(rc = {'figure.figsize':(20,10)})\n",
    "    ax.set_title(f'Distribution of {coord}_mean for each User')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a79ac-4893-4d8d-8748-bf94cc25efe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71ae2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf85385",
   "metadata": {},
   "source": [
    "Principal Component Analysis\n",
    "* reduce the dimensionality of the data.\n",
    "* This makes the data faster and easier to process.\n",
    "* when dimensionality is reduced in 2D or 3D, data of higher dimensions can be visualized in a human-readable graph.<br>\n",
    "\n",
    "Best case: the 2D and 3D graph can directly give an understanding of how well the data can be classified. If there are already well visible clusters in the 2D graph, it is reasonable to conclude that a classification algorithm can easily detect this pattern<br>\n",
    "\n",
    "Lets see how our data looks like after dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed6f95",
   "metadata": {},
   "source": [
    "First we right some functions to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c8780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "df_pca = df_raw.copy()\n",
    "df_pca.drop(['User', 'Class'], inplace=True, axis=1)\n",
    "pca.fit(df_pca)\n",
    "features_pca = pd.DataFrame(pca.transform(df_pca))\n",
    "classes = df_raw[['Class']].copy()\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"First 2: \", np.round(sum(pca.explained_variance_ratio_[0:1])*100,2),\"%\")\n",
    "print(\"First 3: \", np.round(sum(pca.explained_variance_ratio_[0:2])*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c94803-6cd2-4145-bb7c-3053b1498a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c016c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = ['Fist', 'Stop', 'Point1', 'Point2', 'Grab']\n",
    "scatterPlot2D(features_pca.iloc[:,0:2], classes, class_labels)\n",
    "scatterPlot3D(features_pca, classes, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967442a-81b4-4159-b2fa-ff2e2d589415",
   "metadata": {},
   "source": [
    "Results:<br>\n",
    "In these two representations we can clearly see that clusters of classes are partially formed. However, these overlap, i.e. the interclass distance is relatively small.\n",
    "In addition, we see that class 2 forms a second, smaller cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28aba7",
   "metadata": {},
   "source": [
    "Now we can also have a look how much variance of the data each pca component describes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04117d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(pca:PCA):\n",
    "\n",
    "    pca_variance_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum_eigenvalues = np.cumsum(pca_variance_ratio)\n",
    "    print(f'Total explained variance ratio:', sum(pca_variance_ratio))\n",
    "    \n",
    "    plt.bar(range(0,len(pca_variance_ratio)), pca_variance_ratio, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance', color='r')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8cddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df829be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO results of explained ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e371b-a8fb-4546-a596-19e5c60a0d83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67d8ed",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows of user 4 and 7\n",
    "# Because they have significantly less data points\n",
    "df_raw = df_raw[df_raw['User'] != 4]\n",
    "df_raw = df_raw[df_raw['User'] != 7]\n",
    "df_raw.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba31201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop coordinates of point 10 and 11\n",
    "# More than 90% of the data is missing\n",
    "# Search for \"Hint 1\" for further information\n",
    "df_raw.drop(['X10', 'Y10', 'Z10', 'X11', 'Y11', 'Z11'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5717a-e460-4985-8ffd-e90171aac4d2",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d1899",
   "metadata": {},
   "source": [
    "#### a. Extract features (min, max, mean, etc.) - df_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93f605-60b1-4ba3-9571-03afa0a4178a",
   "metadata": {},
   "source": [
    "Ideas for new features: (inspired from paper)<br>\n",
    "* number of markers\n",
    "* mean (per coordinate)\n",
    "* Eigenvalues and vectors of the points covariance matrix\n",
    "https://math.stackexchange.com/questions/2842830/why-does-the-eigen-decomposition-of-the-covariance-matrix-of-a-point-cloud-give\n",
    "* dimensions of the axis-aligned minimum bounding box centered on the mean\n",
    "\n",
    "Keep in mind that each feature has to aggregate the points in such a way that the result is order invariant!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data set we want to fill step by step\n",
    "df_aggregate= pd.DataFrame()\n",
    "# We dont want to change the original data set\n",
    "df_raw_dummy = df_raw.copy(deep=True)\n",
    "\n",
    "# Save the user and class column\n",
    "df_user = df_raw_dummy.pop('User')\n",
    "df_class = df_raw_dummy.pop('Class')\n",
    "\n",
    "# We replace all zeros with NaN, so the mean calculatiion ignores occluded data points\n",
    "df_raw_dummy = df_raw_dummy.replace(0, np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6aa07a-8418-42ce-b560-099ecece123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the X, Y and Z columns\n",
    "df_x = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('X')]]\n",
    "df_y = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('Y')]]\n",
    "df_z = df_raw_dummy[df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith('Z')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the mean, min, max etc. of the X, Y and Z columns\n",
    "for coordinate in ['X', 'Y', 'Z']:\n",
    "    coordinate_cols = df_raw_dummy.columns[pd.Series(df_raw_dummy.columns).str.startswith(coordinate)]\n",
    "    df_aggregate[f'{coordinate}_mean'] = df_raw_dummy[coordinate_cols].mean(axis=1)\n",
    "    df_aggregate[f'{coordinate}_min'] = df_raw_dummy[coordinate_cols].min(axis=1)\n",
    "    df_aggregate[f'{coordinate}_max'] = df_raw_dummy[coordinate_cols].max(axis=1)\n",
    "\n",
    "# Change NaN back to 0 values\n",
    "df_raw_dummy = df_raw_dummy.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b954b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of visible points (not occluded)\n",
    "df_aggregate['n_points'] = (df_raw_dummy.astype(bool).sum(axis=1))/3\n",
    "df_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf820fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0cdac",
   "metadata": {},
   "source": [
    "My idea:<br>\n",
    "find the orientation of a given cluster<br>\n",
    "(https://math.stackexchange.com/questions/2842830/why-does-the-eigen-decomposition-of-the-covariance-matrix-of-a-point-cloud-give)\n",
    "\n",
    "1. Rearange the dataset (1 point per row with X, Y, Z value)\n",
    "-> All points per row will be saved in a batch as new sub dataframe\n",
    "2. Compute the covariance matrix for each sub dataframe\n",
    "3. Calculate the Eigenvalues and Eigenvectors of the covariance matrix\n",
    "4. Concat created features to the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1ea52",
   "metadata": {},
   "source": [
    "DISCLAIMER: <br>\n",
    "The following function can take up to 1 min!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume you have a DataFrame called 'df' with the columns ['X0', 'Y0', 'Z0', 'X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']\n",
    "# Reorganize the dataframe to have each row as a batch of data\n",
    "# 'X0', 'Y0', 'Z0',\n",
    "# 'X1', 'Y1', 'Z1',\n",
    "# 'X2', 'Y2', 'Z2'\n",
    "\n",
    "# Create an empty list to store the batches of data\n",
    "batches = []\n",
    "\n",
    "# Iterate over the DataFrame and extract the batches of data\n",
    "for row in range(0, df_raw_dummy.shape[0]):\n",
    "\n",
    "    col_batch = []\n",
    "    # Extract a batch of data for the current row\n",
    "    for col in range(0, df_raw_dummy.shape[1], 3):\n",
    "        batch = df_raw_dummy.iloc[row, col:col+3]\n",
    "        # Rename the columns\n",
    "        batch.index = ['X', 'Y', 'Z']\n",
    "        #print(f'batch: {batch}')\n",
    "        col_batch.append(batch)\n",
    "    # Append the batch to the list\n",
    "    batches.append(col_batch)\n",
    "\n",
    "# Concatenate the batches of data under each other\n",
    "concat_batches = []\n",
    "for batch in batches:\n",
    "    concat_batch = pd.concat(batch, axis=1).transpose()\n",
    "    concat_batches.append(concat_batch)\n",
    "    # Remove rows with all zeros\n",
    "    concat_batch = concat_batch[(concat_batch.T != 0).any()]\n",
    "\n",
    "# Create a dictionary with the eigenvalues and eigenvectors as values\n",
    "eigen_dict = {'eigenvec_1_1': [],\n",
    "                'eigenvec_1_2': [],\n",
    "                'eigenvec_1_3': [],\n",
    "                'eigenvec_2_1': [],\n",
    "                'eigenvec_2_2': [],\n",
    "                'eigenvec_2_3': [],\n",
    "                'eigenvec_3_1': [],\n",
    "                'eigenvec_3_2': [],\n",
    "                'eigenvec_3_3': [],\n",
    "              'eigenval_1': [],\n",
    "              'eigenval_2': [],\n",
    "              'eigenval_3': []}\n",
    "\n",
    "# Create the DataFrame\n",
    "eigen_df = pd.DataFrame(eigen_dict)\n",
    "\n",
    "# Compute the covariance matrix for each batch\n",
    "for concat_batch in concat_batches:\n",
    "    # Compute the covariance matrix\n",
    "    cov_matrix = np.cov(concat_batch, rowvar=False)\n",
    "    # Compute the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Add the eigenvalues and eigenvectors to the DataFrame\n",
    "    # Store eigenvector 1\n",
    "    eigen_dict['eigenvec_1_1'].append(eigenvectors[0, 0])\n",
    "    eigen_dict['eigenvec_1_2'].append(eigenvectors[1, 0])\n",
    "    eigen_dict['eigenvec_1_3'].append(eigenvectors[2, 0])\n",
    "    # Store eigenvector 2\n",
    "    eigen_dict['eigenvec_2_1'].append(eigenvectors[0, 1])\n",
    "    eigen_dict['eigenvec_2_2'].append(eigenvectors[1, 1])\n",
    "    eigen_dict['eigenvec_2_3'].append(eigenvectors[2, 1])\n",
    "    # Store eigenvector 3\n",
    "    eigen_dict['eigenvec_3_1'].append(eigenvectors[0, 2])\n",
    "    eigen_dict['eigenvec_3_2'].append(eigenvectors[1, 2])\n",
    "    eigen_dict['eigenvec_3_3'].append(eigenvectors[2, 2])\n",
    "    # Store eigenvalues\n",
    "    eigen_dict['eigenval_1'].append(eigenvalues[0])\n",
    "    eigen_dict['eigenval_2'].append(eigenvalues[1])\n",
    "    eigen_dict['eigenval_3'].append(eigenvalues[2])\n",
    "\n",
    "# Finally add generated features to the DataFrame\n",
    "df_aggregate = pd.concat([df_aggregate, pd.DataFrame(eigen_dict)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485aefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user and class information to the new data set after the feature extraction\n",
    "df_aggregate['User'] = df_user\n",
    "df_aggregate['Class'] = df_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef523fc",
   "metadata": {},
   "source": [
    "**Evaluation Strategy**:<br>\n",
    "We will use a k-Fold cross-validation to evaluate our model <br>\n",
    "We randomly choose 2 User for the test set <br>\n",
    "Then we remove the users from the selectable list so that each user is in the test data set at most once over all k runs <br>\n",
    "For the next split we again choose 2 random User, and so on<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_train_test_user(df:pd.DataFrame, user_list:list, num_user_test:int=2):\n",
    "    '''This function returns the indices for the training and test set.\n",
    "    The function randomly selects two users for the test set and the remaining\n",
    "    users for the training set.\n",
    "    \n",
    "    return: train_indices, test_indices'''\n",
    "\n",
    "    # Create a list of indices for the training and test set\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Generate 2 random numbers between 0 and 14\n",
    "    test_user_1, test_user_2, test_user_3 = random.sample(user_list, num_user_test)\n",
    "    print(f'User picked for test set: {test_user_1}, {test_user_2}')\n",
    "    \n",
    "\n",
    "    # Iterate over the users\n",
    "    for user in user_list:\n",
    "        # Get the indices for the current user\n",
    "        indices = df[df['User'] == user].index\n",
    "        # Append the indices to the list\n",
    "        if user == test_user_1 or user == test_user_2 or user == test_user_3:\n",
    "            test_indices.extend(indices)\n",
    "        else:\n",
    "            train_indices.extend(indices)\n",
    "    \n",
    "    # Remove the test users from the user list so they cannot be selected again\n",
    "    user_list = [x for x in user_list if x != test_user_1 and x != test_user_2]\n",
    "\n",
    "    return train_indices, test_indices, user_list\n",
    "\n",
    "def custom_cv_approach(df:pd.DataFrame, user_list:list, num_user_test:int=2):\n",
    "    '''\n",
    "    each user is iteratively left out from training and used as a test set. \n",
    "    We then tests the generalization of the algorithm to new users. \n",
    "    A 'User' attribute is provided to accomodate this strategy. \n",
    "    '''\n",
    "    def cv_ratio(y_test, df):\n",
    "        print(f'Ratio of test set: {len(y_test)/len(df)}')\n",
    "    \n",
    "    # Get the indices for the training and test set\n",
    "    train_indices, test_indices, user_list = get_train_test_user(df, user_list, num_user_test)\n",
    "    # Create the training and test set\n",
    "    X_train = df.iloc[train_indices, :]\n",
    "    X_train = X_train.sample(frac=1, random_state=2023).reset_index(drop=True) #Shuffle\n",
    "    y_train = X_train.pop('Class')\n",
    "    X_train.pop('User')\n",
    "    X_test = df.iloc[test_indices, :]\n",
    "    X_test = X_test.sample(frac=1, random_state=2023).reset_index(drop=True) #Shuffle \n",
    "    y_test = X_test.pop('Class')\n",
    "    X_test.pop('User')\n",
    "\n",
    "    # Print the ratio of the test set\n",
    "    cv_ratio(y_test, df)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, user_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_group = df_aggregate.groupby(['User'], sort=False)\n",
    "user_group.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d51c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add augmentation to the data set.\n",
    "# e.g. add Jitter or Shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c27403-ded5-4ba2-911b-778112ab4445",
   "metadata": {},
   "source": [
    "#### b. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccb4fe",
   "metadata": {},
   "source": [
    "Of course we will also keep the raw data set and a data set with extracted features. In a later step we want to compare different data sets and their perfomance.<br>\n",
    "Here we will extract one data set that contains pca components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0027e-6c38-4e17-a19d-43397b492d48",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will choose n_components because their explained variance ratio is above 0.95\n",
    "n_components = 25\n",
    "pca = PCA(n_components=n_components)\n",
    "df_pca = df_raw.copy()\n",
    "\n",
    "# Drop the user and class information, they are not needed for the PCA\n",
    "df_user = df_pca.pop('User')\n",
    "df_class = df_pca.pop('Class')\n",
    "\n",
    "pca.fit(df_pca)\n",
    "df_pca_features = pd.DataFrame(pca.transform(df_pca))\n",
    "\n",
    "# Now add the user and class information again\n",
    "df_pca_features['User'] = df_user\n",
    "df_pca_features['Class'] = df_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd5b4a",
   "metadata": {},
   "source": [
    "#### c.I Split data - Wrong way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3b3ea",
   "metadata": {},
   "source": [
    "#### Demonstration data set (WRONG WAY) - mixed user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9487229",
   "metadata": {},
   "source": [
    "******************************************************\n",
    "DEMONSTRATION: this shows how NOT to do it:<br>\n",
    "Splitting the naive way (similar data points will be in both sets)\n",
    "* This is not to expose the mistakes of others! It is there so that other students may read this and pay attention to it from now on. (funny enough I made the same mistake in an internship and was proud of my 98% :D)\n",
    "******************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7940eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c585d02-1dc7-41bc-a8fe-e47d0d70e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into train and test set\n",
    "X_train_mixed, X_test_mixed, y_train_mixed, y_test_mixed = train_test_split(df_raw, df_raw['Class'], test_size=0.25)\n",
    "user_group = df_raw.groupby(['User'], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb68af6",
   "metadata": {},
   "source": [
    "We normalize the data using a Min-Max-Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data with MinMaxScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the training data\n",
    "scaler.fit(X_train_mixed)\n",
    "\n",
    "# Transform the training and test data\n",
    "X_train_mixed = scaler.transform(X_train_mixed)\n",
    "X_test_mixed = scaler.transform(X_test_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d5922-e57e-405f-b4a9-55a17c6e2dbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use this data set to train a Random Forest Classifier in a later Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d550bc",
   "metadata": {},
   "source": [
    "#### c.II Split data properly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941cf30",
   "metadata": {},
   "source": [
    "We now have a CV loop:\n",
    "I split the data in the same loop, where I train the model for a specific k-fold <br>\n",
    "Therefore I dont split the data beforehand in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab41a1-9b02-4916-9089-38a43d021429",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Testing Phase I: Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04e947-a546-4630-95b9-482101ec81c7",
   "metadata": {},
   "source": [
    "**My approach:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9896e-02af-4eb3-bb0c-8ed22bcf500e",
   "metadata": {},
   "source": [
    "For each baseline model, train model on \n",
    "* raw data\n",
    "* extracted features\n",
    "* Data from PCA <br>\n",
    "\n",
    "For some models:\n",
    "* additionally perform a GridSearch to find the best hyperparameters. Which dataset was used for this, I decided depending on the model, or how the perfomance on the dataset was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c4ee0",
   "metadata": {},
   "source": [
    "After we have trained all the models, we want to compare the model performance using selected metrics. Therefore we need to save the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b972908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['model', 'Accuracy', 'Precision', 'Recall', 'F1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0de32",
   "metadata": {},
   "source": [
    "First, we write a training function to have a common interface for all conventional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df:pd.DataFrame, model_func: Callable, scaler_choice:str, kwargs_dict:dict):\n",
    "    '''This function provides a common interface to train the classic ml models.\n",
    "    In a loop we iterate over different train and test sets and train the model.\n",
    "    We leave out k users from the data set and use them as test set.\n",
    "    '''\n",
    "\n",
    "    # Create a list of users\n",
    "    user_list = [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Number of users to be used for the test set\n",
    "    num_user_test = 3\n",
    "    num_of_iterations = 4\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    X_test_best = None\n",
    "    y_test_best = None\n",
    "\n",
    "    # Save evaluation metrics in a dictionary\n",
    "    eval_dict = {'train': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []},\n",
    "                    'test': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []}}\n",
    "\n",
    "    for i in range(num_of_iterations):\n",
    "\n",
    "        print('*'*50)\n",
    "        print(f'CV Run: {i}')\n",
    "\n",
    "        model = model_func(**kwargs_dict)\n",
    "\n",
    "        # Split the data\n",
    "        X_train, y_train, X_test, y_test, user_list = custom_cv_approach(df, user_list, num_user_test=num_user_test)\n",
    "\n",
    "        if scaler_choice == 'normalize':\n",
    "            # Normalize the data\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        elif scaler_choice == 'standard':\n",
    "            # Standard Scale the data\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        else:\n",
    "            print('No scaling applied')\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        model_acc = model.score(X_test, y_test)\n",
    "        \n",
    "        # Print the results\n",
    "        print('Train accuracy: ', model.score(X_train, y_train))\n",
    "        print('(CV-) Test accuracy: ', model_acc)\n",
    "\n",
    "        # Save the results\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        train_acc = accuracy_score(y_train, y_pred_train)\n",
    "        train_precision = precision_score(y_train, y_pred_train, average='macro')\n",
    "        train_recall = recall_score(y_train, y_pred_train, average='macro')\n",
    "        f1_score_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'train', train_acc, train_precision, train_recall, f1_score_train)\n",
    "\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        val_acc = accuracy_score(y_test, y_pred_test)\n",
    "        val_precision = precision_score(y_test, y_pred_test, average='macro')\n",
    "        val_recall = recall_score(y_test, y_pred_test, average='macro')\n",
    "        f1_score_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'test', val_acc, val_precision, val_recall, f1_score_test)\n",
    "\n",
    "        # Save best model\n",
    "        if model_acc > best_acc:\n",
    "            best_model = model\n",
    "            best_acc = model_acc\n",
    "            X_test_best = X_test\n",
    "            y_test_best = y_test\n",
    "\n",
    "    return best_model, X_test_best, y_test_best, eval_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9990a7-bfa6-4a4f-a796-6ace870559f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def grid_search(df:pd.DataFrame, model_func: Callable, scaler_choice:str, cv_k:int, kwargs_dict:dict):\n",
    "    '''\n",
    "    Grid search function for conventional ML models.\n",
    "    similiar to the train_model function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing the data, either raw or aggregated.\n",
    "    model_func : Callable\n",
    "        Function that returns a model object.\n",
    "    scaler_choice : str\n",
    "        Choice of scaler to be used. Either 'normalize' or 'standard'.\n",
    "    cv_k : int\n",
    "        Number of iterations for the cross validation.\n",
    "    '''\n",
    "\n",
    "    # Create a list of users\n",
    "    user_list = [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Number of users to be used for the test set\n",
    "    num_user_test = 3\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    best_param = None\n",
    "    X_test_best = None\n",
    "    y_test_best = None\n",
    "\n",
    "    # Save all grid search objects in a list\n",
    "    grid_search_list = []\n",
    "\n",
    "    for i in range(cv_k):\n",
    "\n",
    "        print('*'*50)\n",
    "        print(f'CV Run: {i}')\n",
    "\n",
    "        # Split the data\n",
    "        X_train, y_train, X_test, y_test, user_list = custom_cv_approach(df, user_list, num_user_test=num_user_test)\n",
    "\n",
    "        if scaler_choice == 'normalize':\n",
    "            # Normalize the data\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        elif scaler_choice == 'standard':\n",
    "            # Standard Scale the data\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        else:\n",
    "            print('No scaling applied')\n",
    "\n",
    "        # IMPORTANT: Define own train test split (otherwise GridSearch uses 5-CrossValidation)\n",
    "        X_complete = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_complete = np.concatenate((y_train, y_test), axis=0)\n",
    "        # The indices which have the value -1 will be kept in train.\n",
    "        # The indices which have zero or positive values, will be kept in test\n",
    "        test_fold = np.concatenate([np.full(len(X_train), -1, dtype=np.int8), np.zeros(len(X_test), dtype=np.int8)])\n",
    "        cv_train_test = PredefinedSplit(test_fold)\n",
    "        # Check how many splits will be done, based on test_fold\n",
    "        print('Number of splits: ',cv_train_test.get_n_splits())\n",
    "\n",
    "        # Create grid search object\n",
    "        grid_search = GridSearchCV(cv=cv_train_test, **kwargs_dict)\n",
    "        # Alternativce a Random Search\n",
    "        # rnd_search = RandomizedSearchCV(svm_model, n_jobs=n_jobs, param_distributions=params, n_iter=num_iter, cv=cv_train_test, verbose=3, retrain=False)\n",
    "\n",
    "        # Fit the model\n",
    "        grid_search.fit(X_complete, y_complete)\n",
    "    \n",
    "        grid_search_list.append(grid_search)\n",
    "\n",
    "        print(f\"Best score: {round(grid_search.best_score_, 3)}\")\n",
    "        print(f\"Best params: {grid_search.best_params_}\")\n",
    "\n",
    "        # Save best model\n",
    "        if grid_search.best_score_ > best_acc:\n",
    "            best_acc = grid_search.best_score_\n",
    "            X_train_best = X_train\n",
    "            y_train_best = y_train\n",
    "            X_test_best = X_test\n",
    "            y_test_best = y_test\n",
    "            best_param = grid_search.best_params_\n",
    "    \n",
    "    # Retrain the best model\n",
    "    # Usually we can use the refit parameter of the search object\n",
    "    # but the would also fit on the test data\n",
    "    model = model_func(**best_param)\n",
    "    model.fit(X_train_best, y_train_best)\n",
    "    \n",
    "    return model, X_test_best, y_test_best, grid_search_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88505742",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b14d2-50f2-439f-88cf-9ace62e40671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#You need to check model descriptions for the hyperparameters. \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier\n",
    "#-----------------------------------------------------------------\n",
    "# Number of trees in the forest:\n",
    "n_estimators = 100 # default\n",
    "# Number of features to consider when looking for the best split:\n",
    "max_features = 'sqrt' # 'auto' is deprecated\n",
    "# Maximum depth of the tree:\n",
    "max_depth = None\n",
    "# Minimum number of samples required to split an internal node:\n",
    "min_samples_split = 2\n",
    "# Minimum number of samples required to be at a leaf node:\n",
    "min_samples_leaf = 1\n",
    "# Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. \n",
    "max_leaf_nodes = None\n",
    "# Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree:\n",
    "bootstrap = False\n",
    "# Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.\n",
    "oob_score = False\n",
    "# Number of jobs to run in parallel. (-1) means use all.\n",
    "n_jobs = -1\n",
    "# Random state\n",
    "random_state = 2023\n",
    "#-----------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_rf(rfc_model:RandomForestClassifier, feature_names:list):\n",
    "    '''\n",
    "    This function plots the feature importance of the random forest classifier.\n",
    "\n",
    "    Parameters:\n",
    "        rfc_model: Random Forest Classifier model\n",
    "        X_train: Training data set, only for the feature names\n",
    "\n",
    "    '''\n",
    "\n",
    "    feature_importance = np.array(rfc_model.feature_importances_)\n",
    "    feature_names = np.array(feature_names)\n",
    "\n",
    "    data={'Feature names':feature_names,'Feature importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "    fi_df.sort_values(by=['Feature importance'], ascending=False,inplace=True)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x=fi_df['Feature importance'], y=fi_df['Feature names'])\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Feature Importance')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca841b",
   "metadata": {},
   "source": [
    "### a) Mixed dataset (wrong way, only demo how not to do it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35feea9-0ba5-44a0-8654-ccb4bf33744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classifier:\n",
    "RFC_demo = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \\\n",
    "                              max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,oob_score=oob_score, n_jobs=n_jobs, random_state=random_state)\n",
    "RFC_demo.fit(X_train_mixed, y_train_mixed)\n",
    "RFC_demo.score(X_test_mixed, y_test_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_data_report = Report('rfc_mixed', RFC_demo, X_test_mixed, y_test_mixed, description=['mixed_data', 'raw features'])\n",
    "print(mixed_data_report)\n",
    "precision_recall_multiclass(RFC_demo, X_test_mixed, y_test_mixed)\n",
    "y_pred_mixed = RFC_demo.predict(X_test_mixed)\n",
    "plot_cm(y_test_mixed, y_pred_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ce2e8",
   "metadata": {},
   "source": [
    "Too good to be true...<br>\n",
    "This was a demonstration how we should not split the data set!\n",
    "Many people on kaggel etc. made this mistake\n",
    "\n",
    "From now on we will not use this (wrong) processed data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc628b",
   "metadata": {},
   "source": [
    "### b) Custom CV - with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc78cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Random Forest raw features\n",
    "rf_raw_hyperparams = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'bootstrap': bootstrap,\n",
    "                'oob_score': oob_score,\n",
    "                'n_jobs': n_jobs,\n",
    "                'random_state': random_state}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_raw, RandomForestClassifier, 'normalize', rf_raw_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec0e68",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d553aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_w_cv_data_report = Report('rfc_raw', best_model, X_test_best, y_test_best, description=['cv_data', 'raw features'])\n",
    "df_results = raw_w_cv_data_report.get_report_as_df(df_results)\n",
    "print(raw_w_cv_data_report)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred_split_raw = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred_split_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbac506",
   "metadata": {},
   "source": [
    "Here we can see that its difficult for the model to classify the \"Fist\" and \"Point2\" data points. Another interesting point is that 28% of the instances of the class \"Point1\" are incorrectly assigned to the class \"Point2\". This stands to reason, since for these gestures a large part of the points are the same, except for the additional markers on the extra finger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca51555",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ae44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = df_raw.columns.to_list()\n",
    "feature_list.remove('User')\n",
    "feature_list.remove('Class')\n",
    "plot_feature_importance_rf(best_model, feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff838528",
   "metadata": {},
   "source": [
    "### c) Custom CV - with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on we will use the following data\n",
    "df_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Random Forest raw features\n",
    "rf_extract_hyperparams = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'bootstrap': bootstrap,\n",
    "                'oob_score': oob_score,\n",
    "                'n_jobs': n_jobs,\n",
    "                'random_state': random_state}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_aggregate, RandomForestClassifier, 'normalize', rf_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45dba1",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_w_cv_data_report = Report('rfc_extract', best_model, X_test_best, y_test_best, description=['cv_data', 'extracted features'])\n",
    "df_results = extract_w_cv_data_report.get_report_as_df(df_results)\n",
    "print(extract_w_cv_data_report)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred_cv_extract = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred_cv_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b330a",
   "metadata": {},
   "source": [
    "With the extracted features the model improved in predicting \"Point2\" data points, but the perfomance for predicting \"Grab\" decreased. Once again we see that the classes \"Point 2\" and \"Point 1\" are most often confused. But here it is the other way around: \"Point1\" has a high accuracy, but \"Point2\" has a low accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a49155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = df_aggregate.columns.to_list()\n",
    "feature_list.remove('User')\n",
    "feature_list.remove('Class')\n",
    "plot_feature_importance_rf(best_model, feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3493b-824c-45ee-b22d-0f2934e3a927",
   "metadata": {},
   "source": [
    "It seems that the features \"Y_min\" and \"n_points\" are selected most often. It is also interesting to see that the trees in the RFC use the eigenvectors and eigenvalues rather less (exception is the 3rd eigenvalue). However, these values are also not so representative, compared to the calculation for large point clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5494f",
   "metadata": {},
   "source": [
    "### d) Custom CV - with pca components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb82821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Random Forest pca features\n",
    "rf_pca_hyperparams = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'bootstrap': bootstrap,\n",
    "                'oob_score': oob_score,\n",
    "                'n_jobs': n_jobs,\n",
    "                'random_state': random_state}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_pca_features , RandomForestClassifier, 'normalize', rf_pca_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_w_cv_data_report = Report('rfc_pca', best_model, X_test_best, y_test_best, description=['cv_data', 'pca features'])\n",
    "df_results = pca_w_cv_data_report.get_report_as_df(df_results)\n",
    "print(pca_w_cv_data_report)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred_cv_pca = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred_cv_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dd072",
   "metadata": {},
   "source": [
    "### Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb903764",
   "metadata": {},
   "source": [
    "The question is which hyperparameters should we tune, since there are a few. We want to tune if possible only the hyperparameters that have a big influence or are not yet optimally defined in sklearn as default parameters. We will have a look in the following paper:\n",
    "\n",
    "\"Hyperparameters and Tuning Strategies for Random Forest\" (Probst et al, 2019):\n",
    "* \"It is well known that in most cases RF works reasonably well with the default values of the hyperparameters\n",
    "specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF\"\n",
    "* \"The number of trees should be set high [..] However, the improvement obtained by adding trees diminishes as more\n",
    "and more trees are added.\"\n",
    "* \"Out of these parameters, mtry is most influential both according to the literature and in our own experiments\"\n",
    "* \"Sample size and node size have a minor influence on the performance but are worth tuning in many cases\"\n",
    "\n",
    "Disclaimer: some of the parameter names might be confusing. The reason is that the authors of the paper used R instead of python.\n",
    "e.g. \n",
    "* mtry: number of variables considered as candidate splitting variables at each split or the sampling scheme used to generate\n",
    "the datasets on which the trees are built. In scikit-learn that would be the parameter max_features\n",
    "* sample size: Number of observations that are drawn for each tree. In scikit-learn that would be the parameter max_samples\n",
    "* node size: Minimum number of observations in a terminal node. In scikit-learn that would be the parameter min_samples_leaf\n",
    "\n",
    "For those who are interested: https://arxiv.org/pdf/1804.03515.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1353bc",
   "metadata": {},
   "source": [
    "#### I. Grid Search with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to run a GridSearch without internal CV!\n",
    "# We will run our own CV-Split because we have to split by User!\n",
    "# Have a look at this thread If you are interested: https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning/44682305#44682305\n",
    "# Create a parameter grid to search for the best RandomForestClassifier\n",
    "param_grid = {'n_estimators': [25, 50, 90, 120, 150],\n",
    "              'max_features': np.linspace(0.5, 1, 5),\n",
    "              'max_samples': np.linspace(0.8, 1, 2),\n",
    "              'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "static_params = {'max_depth': max_depth,\n",
    "                 'min_samples_split': min_samples_split\n",
    "}\n",
    "\n",
    "# Add static params to param_grid\n",
    "rf_extract_search = {'estimator': RandomForestClassifier(**static_params),\n",
    "                      'param_grid': param_grid,\n",
    "                      'refit' : False, #We later want to refit only on X_train, y_train\n",
    "                      'scoring': 'accuracy',\n",
    "                      'verbose': 1,\n",
    "                      'n_jobs': 5\n",
    "}\n",
    "cv_k = 2 # Number CV folds\n",
    "model, X_test_best, y_test_best, grid_search_list = grid_search(df_aggregate, RandomForestClassifier, 'standard', cv_k, rf_extract_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_search(grid_search_list, 'n_estimators', 'max_features')\n",
    "plot_grid_search(grid_search_list, 'n_estimators', 'max_samples')\n",
    "plot_grid_search(grid_search_list, 'n_estimators', 'min_samples_leaf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddace12-4fd8-44cf-a026-1b55b957469e",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f03b31-7d56-412e-b93f-6af18b8fe07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_grid_w_cv_data_report_rfc = Report('rf_grid_extract', model, X_test_best, y_test_best, description=['RFC', 'cv_data', 'extracted features', 'gridsearch'])\n",
    "df_results = extract_grid_w_cv_data_report_rfc.get_report_as_df(df_results)\n",
    "print(extract_grid_w_cv_data_report_rfc)\n",
    "precision_recall_multiclass(model, X_test_best, y_test_best)\n",
    "y_pred = model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb72f00-950f-41f3-96c0-7e91d07f6804",
   "metadata": {
    "tags": []
   },
   "source": [
    "## kNN - k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2756cd-111c-4fd1-9893-e0f4e4b1e29a",
   "metadata": {},
   "source": [
    "### a) Custom CV - with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11502e39-6a08-40ab-b52e-c96cae64f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#kNN raw features\n",
    "knn_raw_hyperparams = {'n_neighbors': 5,\n",
    "                       'algorithm': 'auto',\n",
    "                       'metric': 'minkowski', # Metric to use for distance computation\n",
    "                       'p': 2, #standard Euclidean distance\n",
    "                       'n_jobs': 5}\n",
    "#algorithm='brute' forces to use the vanilla kNN algorithm, otherwise it uses some tricks\n",
    "#(till now there was no difference in accuracy but in computation time!)\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_raw, KNeighborsClassifier, 'normalize', knn_raw_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359100f-345b-41c9-8399-0e50490a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_w_cv_data_report_knn = Report('knn_raw', best_model, X_test_best, y_test_best, description=['kNN', 'cv_data', 'raw features'])\n",
    "df_results = raw_w_cv_data_report_knn.get_report_as_df(df_results)\n",
    "print(raw_w_cv_data_report_knn)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15298bb6-5c93-47f3-aae7-fdb18c31ed48",
   "metadata": {},
   "source": [
    "We can see that the model is performing bad among all classes except the gesture \"Fist\". It is the only class where we might have clearly seperated cluster wich are not too close to other classes. The overall performance is worse than I expected. Lets try to lower the number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a9f2a-faa6-47e5-be52-5d30b10b637a",
   "metadata": {},
   "source": [
    "**Decrease k (number of neighbors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c22437-dbac-42be-87cc-733b5dbe320c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_raw_hyperparams['n_neighbors'] = 3\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_raw, KNeighborsClassifier, 'normalize', knn_raw_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22369529-8595-4684-9f4c-ce9fca34d075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_w_cv_data_report_knn = Report('knn_raw', best_model, X_test_best, y_test_best, description=['kNN', 'cv_data', 'raw features'])\n",
    "df_results = raw_w_cv_data_report_knn.get_report_as_df(df_results)\n",
    "print(raw_w_cv_data_report_knn)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f55552-da90-4b3e-b577-b9c95ab26501",
   "metadata": {
    "tags": []
   },
   "source": [
    "The perfomance increases but is still far from good. (at least to our expectations). Again we can see that the class \"Fist\" performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83e4e0-4cd1-4bb2-9eaa-6e7d631079c2",
   "metadata": {},
   "source": [
    "### b) Custom CV - with extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d47da0-84c6-4d1b-a0dd-f2c72d240579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#kNN raw features\n",
    "knn_extract_hyperparams = {'n_neighbors': 3,\n",
    "                       'algorithm': 'auto',\n",
    "                       'metric': 'minkowski', # Metric to use for distance computation\n",
    "                       'p': 2, #standard Euclidean distance\n",
    "                       'n_jobs': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e25f66-d034-4133-b490-190c1101ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_aggregate, KNeighborsClassifier, 'normalize', knn_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40e738-c83b-4d2c-90c3-dbcd6e3f0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_w_cv_data_report_knn = Report('knn_extract', best_model, X_test_best, y_test_best, description=['kNN', 'cv_data', 'extracted features'])\n",
    "df_results = extract_w_cv_data_report_knn.get_report_as_df(df_results)\n",
    "print(extract_w_cv_data_report_knn)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59569b49-be14-4a37-ab02-8bf5aaf34a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The kNN model is not performing better when we use the extracted data. Now we can differentiate the class \"Grab\" the best from the other. We are now in a different feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ed892-9094-4d40-97a2-8c47566d08ea",
   "metadata": {},
   "source": [
    "### c) Custom CV - with pca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583e896-1f70-42c0-b8e1-63daa2a2a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_pca_features, KNeighborsClassifier, 'normalize', knn_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef74b44-4d56-4c0a-b40b-e03da6dcd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_w_cv_data_report_knn = Report('knn_pca', best_model, X_test_best, y_test_best, description=['kNN', 'cv_data', 'pca features'])\n",
    "df_results = pca_w_cv_data_report_knn.get_report_as_df(df_results)\n",
    "print(pca_w_cv_data_report_knn)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1a3af",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "penalty = 'l2'\n",
    "C = 1.0 #regularization strength. The smaller the value, the stronger the regularization.\n",
    "random_state = 2023\n",
    "solver = 'lbfgs' # One of the possible solver for multiclass problems (newton-cg, lbfgs, liblinear, sag, saga)\n",
    "max_iter = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965311c8",
   "metadata": {},
   "source": [
    "### a) Custom CV - with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Logistic regression raw features\n",
    "lg_raw_hyperparams = {'penalty': penalty,\n",
    "                'C': C,\n",
    "                'random_state': random_state,\n",
    "                'solver': solver,\n",
    "                'max_iter': max_iter}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_raw, LogisticRegression, 'standard', lg_raw_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681f10a",
   "metadata": {},
   "source": [
    "Error occured:\n",
    "ConvergenceWarning: lbfgs failed to converge (status=1)\n",
    "-> increase max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ac0e8",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_w_cv_data_report_lg = Report('lg_raw', best_model, X_test_best, y_test_best, description=['LogReg', 'cv_data', 'raw features'])\n",
    "df_results = raw_w_cv_data_report_lg.get_report_as_df(df_results)\n",
    "print(raw_w_cv_data_report_lg)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99bd4d-2072-47e0-a0f4-5657952d7b08",
   "metadata": {},
   "source": [
    "Why is there a sudden/steep drop in the curve for small Precision?\n",
    "\n",
    "- PR curves assume a threshold value that decreases from left (treshold=1) to right (threshold=0)  of the chart.\n",
    "- As the threshold increases, we become more stringent about what we deem to be a positive example  We classify fewer examples as positive)\n",
    "- At high enough thresholds, we only have a handful of FPs and TPs\n",
    "- with Precision = TP / (TP + FP) , a difference of 1 positive classification could change precision a lot  we generally see more volatility at those regions\n",
    "- If the threshold is so high that TP = 0: precision is 0.\n",
    "- If both TP=0 and FP=0:  precision is undefined due to the denominator (TP + FP) being 0. Despite, the summary logic assigns precision a value of 1 visualization spikes up to 1 very close to the Y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb94285-1cbc-4314-a6ee-d7f87f07191a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The perfomance is similiar for all CrossValidation splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5efb2",
   "metadata": {},
   "source": [
    "### b) Custom CV - with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Logistic regression extract features\n",
    "lg_extract_hyperparams = {'penalty': penalty,\n",
    "                'C': C,\n",
    "                'random_state': random_state,\n",
    "                'solver': solver,\n",
    "                'max_iter': max_iter}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_aggregate, LogisticRegression, 'standard', lg_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a2be5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_w_cv_data_report_lg = Report('lg_extract', best_model, X_test_best, y_test_best, description=['LogReg', 'cv_data', 'extracted features'])\n",
    "df_results = extract_w_cv_data_report_lg.get_report_as_df(df_results)\n",
    "print(extract_w_cv_data_report_lg)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4fd40d-2cf4-4753-a075-5799c7e0f093",
   "metadata": {},
   "source": [
    "We achieved a significant perfomance gain by extracting the features (min, max, mean, etc.)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe646d4",
   "metadata": {},
   "source": [
    "### c) Custom CV - with pca features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#Logistic regression extract features\n",
    "lg_extract_hyperparams = {'penalty': penalty,\n",
    "                'C': C,\n",
    "                'random_state': random_state,\n",
    "                'solver': solver,\n",
    "                'max_iter': max_iter}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_pca_features, LogisticRegression, 'standard', lg_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5916b",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911feab-ee91-448c-8ad5-3632462b4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_w_cv_data_report_lg = Report('lg_pca', best_model, X_test_best, y_test_best, description=['LogReg', 'cv_data', 'pca features'])\n",
    "df_results = pca_w_cv_data_report_lg.get_report_as_df(df_results)\n",
    "print(pca_w_cv_data_report_lg)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596db19b-9a3b-4755-94b2-d1a7e487a80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a6534",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61acc8-cbe2-448e-8c49-21351b2db3b5",
   "metadata": {},
   "source": [
    "The perfomance gain while using the extracted dataset was huge. We will perform a Grid Search on this data to have a look whether we can improve even further by using another hyperparameter combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3703a8",
   "metadata": {},
   "source": [
    "#### I. Grid Search with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e20c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to run a GridSearch without internal CV!\n",
    "# We will run our own CV-Split because we have to split by User!\n",
    "# Have a look at this thread If you are interested: https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning/44682305#44682305\n",
    "\n",
    "# Inspiration from this blog https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "# Define the grid search parameters\n",
    "param_grid = {'C':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
    "              'penalty':['l2', None],\n",
    "              'solver': ['newton-cg', 'sag', 'lbfgs']\n",
    "}\n",
    "# newton-cg is the only solver that supports elasticnet penalty\n",
    "# Try in another section below\n",
    "\n",
    "# Define static parameters\n",
    "static_params = {'max_iter': 300,\n",
    "                 'random_state': random_state\n",
    "}\n",
    "\n",
    "\n",
    "# Add static params to param_grid\n",
    "lg_extract_search = {'estimator': LogisticRegression(**static_params),\n",
    "                      'param_grid': param_grid,\n",
    "                      'refit' : False, #We later want to refit only on X_train, y_train\n",
    "                      'scoring': 'accuracy',\n",
    "                      'verbose': 1,\n",
    "                      'n_jobs': 5\n",
    "}\n",
    "cv_k = 2 # Number CV folds\n",
    "model, X_test_best, y_test_best, grid_search_list = grid_search(df_aggregate, LogisticRegression, 'standard', cv_k, lg_extract_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d8443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_grid_search(grid_search_list, 'C', 'penalty')\n",
    "plot_grid_search(grid_search_list, 'C', 'solver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f7c05-44e4-4ecb-b91f-93a774abee75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_grid_w_cv_data_report_lg = Report('lg_grid_extract', model, X_test_best, y_test_best, description=['LogReg', 'cv_data', 'extracted features', 'gridsearch'])\n",
    "df_results = extract_grid_w_cv_data_report_lg.get_report_as_df(df_results)\n",
    "print(extract_grid_w_cv_data_report_lg)\n",
    "precision_recall_multiclass(model, X_test_best, y_test_best)\n",
    "y_pred = model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e0c40-4569-441a-93f7-d7b7bd635ca0",
   "metadata": {},
   "source": [
    "The confusing part is that we found a model that is performing worse for class \"Point2\" while improving the accuracy for class \"Stop\" (for which the prevouis model had a bad accuracy). This shows us that the model perfomance for each class is not fixed for certain dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08235963",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19328cee",
   "metadata": {},
   "source": [
    "### a) Custom CV - with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96991f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#SVC_raw\n",
    "svc_raw_hyperparams = {\n",
    "    'C': 1.0, #regularization strength. The smaller the value, the stronger the regularization.\n",
    "    'gamma':'scale',\n",
    "    'kernel': 'rbf',\n",
    "    'decision_function_shape': 'ovo',\n",
    "    'random_state': 2023,\n",
    "    'max_iter': -1, # -1 means no limit\n",
    "    'cache_size': 200, # in MB\n",
    "    'probability': True # needed for predict_proba later on\n",
    "}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_raw, SVC, 'standard', svc_raw_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a90c1",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06212600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_w_cv_data_report_svm = Report('svm_raw', best_model, X_test_best, y_test_best, description=['SVM', 'cv_data', 'raw features'])\n",
    "df_results = raw_w_cv_data_report_svm.get_report_as_df(df_results)\n",
    "print(raw_w_cv_data_report_svm)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bb289-4984-49f8-b2a9-ba637f88bff5",
   "metadata": {},
   "source": [
    "This result is surprising because the previous pattern is not evident here (Point 1 is confused with Point 2 and vice versa). The accuracy for \"Point1\" is also bad here but the instances of this class are confused with the class \"Fist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f68ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6f14d",
   "metadata": {},
   "source": [
    "### b) Custom CV - with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a0210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#SVC_aggregate\n",
    "svc_extract_hyperparams = {\n",
    "    'C': 1.0, #regularization strength. The smaller the value, the stronger the regularization.\n",
    "    'gamma':'scale',\n",
    "    'kernel': 'rbf',\n",
    "    'decision_function_shape': 'ovo',\n",
    "    'random_state': 2023,\n",
    "    'max_iter': -1, # -1 means no limit\n",
    "    'cache_size': 200, # in MB\n",
    "    'probability': True # needed for predict_proba later on\n",
    "}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_aggregate, SVC, 'standard', svc_extract_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09d117",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2577b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_w_cv_data_report_svm = Report('svm_extract', best_model, X_test_best, y_test_best, description=['SVM', 'cv_data', 'extracted features'])\n",
    "df_results = extract_w_cv_data_report_svm.get_report_as_df(df_results)\n",
    "print(extract_w_cv_data_report_svm)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27262a2f",
   "metadata": {},
   "source": [
    "### c) Custom CV - with pca features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "#SVC_pca\n",
    "svc_pca_hyperparams = {\n",
    "    'C': 1.0, #regularization strength. The smaller the value, the stronger the regularization.\n",
    "    'gamma':'scale',\n",
    "    'kernel': 'rbf',\n",
    "    'decision_function_shape': 'ovo',\n",
    "    'random_state': 2023,\n",
    "    'max_iter': -1, # -1 means no limit\n",
    "    'cache_size': 200, # in MB\n",
    "    'probability': True # needed for predict_proba later on\n",
    "}\n",
    "\n",
    "best_model, X_test_best, y_test_best, eval_dict = train_model(df_pca_features, SVC, 'standard', svc_pca_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a84d5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab157a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_w_cv_data_report_svm = Report('svm_pca', best_model, X_test_best, y_test_best, description=['SVM', 'cv_data', 'pca features'])\n",
    "df_results = pca_w_cv_data_report_svm.get_report_as_df(df_results)\n",
    "print(pca_w_cv_data_report_svm)\n",
    "precision_recall_multiclass(best_model, X_test_best, y_test_best)\n",
    "y_pred_pca_svm = best_model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred_pca_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bdd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd66664-7712-40e8-824c-ba7baf5ea7bb",
   "metadata": {},
   "source": [
    "### Grid Search - with extracted data and raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501f080-8c60-41de-b337-3531c4e5037f",
   "metadata": {},
   "source": [
    "#### I. Grid Search with extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac462fcd-8d93-4519-aa6c-b6e58567f8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to run a GridSearch without internal CV!\n",
    "# We will run our own CV-Split because we have to split by User!\n",
    "# Have a look at this thread If you are interested: https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning/44682305#44682305\n",
    "param_grid = {'C':[1,10,100,1000],\n",
    "              'gamma':[1,0.1,0.001], #,0.0001],\n",
    "              'kernel':['linear','rbf']} # alternative for C space: use np.logspace\n",
    "\n",
    "static_params = {'decision_function_shape': 'ovo',\n",
    "                    'random_state': 2023,\n",
    "                    'max_iter': -1, # -1 means no limit\n",
    "                    'cache_size': 200, # in MB\n",
    "                    'probability': True # needed for predict_proba later on\n",
    "}\n",
    "\n",
    "# Add static params to param_grid\n",
    "#param_grid.update(static_params)\n",
    "\n",
    "svc_extract_search = {'estimator': SVC(**static_params),\n",
    "                      'param_grid': param_grid,\n",
    "                      'refit' : False, #We later want to refit only on X_train, y_train\n",
    "                      'scoring': 'accuracy',\n",
    "                      'verbose': 1,\n",
    "                      'n_jobs': 10\n",
    "}\n",
    "cv_k = 3 # Number CV folds\n",
    "model, X_test_best, y_test_best, grid_search_list = grid_search(df_aggregate, SVC, 'standard', cv_k, svc_extract_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b472a-67c4-48b2-97e5-3afb28c4c30a",
   "metadata": {},
   "source": [
    "in the following I want to plot the influence of the different parameters. e.g. for a gamma value (label) and a C value (X-axis) search all occurring parameter sets (the kernel varies), average their accuracy value and plot this score in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dee54d-adae-4e30-a573-af4200344fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_grid_search(grid_search_list, 'C', 'kernel')\n",
    "plot_grid_search(grid_search_list, 'C', 'gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0bb7cc-15ae-4c5e-bdd4-5dbc290dd70b",
   "metadata": {},
   "source": [
    "Due to the limited value combinations of the hyperparameters, these graphs are of limited use. Moreover, only a multivariable feature count of two can be represented. (X-axis: feature 1, label/color: feature 2, Y-axis: accuracy score).\n",
    "However, we can deduce that small values for gamma achieve higher accuracy.\n",
    "A small gamma gives us a pointed curve in the higher dimensions (a large gamma would give us a softer, broader curve).<br>\n",
    "C = Cost of missclassification <br>\n",
    "A high C penalize missclassification alot. A low C allows some examples to be placed \"on the wrong side\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57cd3f-4f80-4695-a6c9-743735a7e17b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39f63d-1720-443a-940b-f77b95afc2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_grid_w_cv_data_report_svm = Report('svm_grid_extract', model, X_test_best, y_test_best, description=['SVM', 'cv_data', 'extracted features', 'gridsearch'])\n",
    "df_results = extract_grid_w_cv_data_report_svm.get_report_as_df(df_results)\n",
    "print(extract_grid_w_cv_data_report_svm)\n",
    "#precision_recall_multiclass(model, X_test_best, y_test_best)\n",
    "y_pred = model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf22b4-752a-4410-82d6-f4af067913ad",
   "metadata": {},
   "source": [
    "#### II. Grid Search with raw features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46929272-8b6e-476f-a501-1fec20e38d2c",
   "metadata": {},
   "source": [
    "We want to search in the same parameter space that we used for grid search with the extracted features. Therefore we simply have to call the grid_search function with the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228aca30-228d-4a4e-a1c1-14c137ff01cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model, X_test_best, y_test_best, grid_search_list = grid_search(df_raw, SVC, 'standard', cv_k, svc_extract_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9f4a6-4c9e-442b-8cf0-6771b9fe61a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_grid_search(grid_search_list, 'C', 'kernel')\n",
    "#plot_grid_search(grid_search_list, 'C', 'gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c8e87-e816-46e5-ba38-8a578f6b5512",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9b993-f794-4cb2-9554-31f163ad9dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "raw_grid_w_cv_data_report_svm = Report('svm_grid_raw', model, X_test_best, y_test_best, description=['SVM', 'cv_data', 'raw features', 'gridsearch'])\n",
    "df_results = raw_grid_w_cv_data_report_svm.get_report_as_df(df_results)\n",
    "print(raw_grid_w_cv_data_report_svm)\n",
    "#precision_recall_multiclass(model, X_test_best, y_test_best)\n",
    "y_pred = model.predict(X_test_best)\n",
    "plot_cm(y_test_best, y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da30056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO boxplot of the results\n",
    "'''\n",
    "ax = sns.boxplot(data = f2_df, linewidth=1, showfliers=False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "sns.set(rc = {'figure.figsize':(8,10)})\n",
    "ax.set(ylabel='F2-Score')\n",
    "ax.set_title('F2-Score Deviation of different Models')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57427d49",
   "metadata": {},
   "source": [
    "## Conclusion Testing Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66077b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_comparison(df_results):\n",
    "    '''\n",
    "    Plot a comparison of the models based on the evaluation metrics\n",
    "    '''\n",
    "\n",
    "    df = df_results.copy()\n",
    "    # Set the model as the index\n",
    "    df.set_index('model', inplace=True)\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    # Create color list\n",
    "    colors = ['tab:blue', 'tab:red', 'tab:green', 'tab:orange', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "\n",
    "\n",
    "    # Plot each metric in a subplot\n",
    "    for i, metric in enumerate(df.columns):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        axs[row, col].bar(df.index, df[metric], color=colors[:len(df.index)], width=0.3)\n",
    "        axs[row, col].set_xticklabels(df.index.to_list(), rotation=90)\n",
    "        axs[row, col].set_ylabel(metric)\n",
    "        axs[row, col].set_title(f'{metric} Comparison')\n",
    "        axs[row, col].set_ylim([0, 1])\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results\n",
    "plot_model_comparison(df_results)\n",
    "best_score = df_results.iloc[df_results['Accuracy'].idxmax()]\n",
    "print('**Model with highest accuracy score**')\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539e6b0",
   "metadata": {},
   "source": [
    "Extracted features?\n",
    "* Almost all models benefit from the extracted features (min, max , mean, eigenvectors, eigenvalues,..). This was also my expectation. Due to the extracted features, the representation of a data point is independent of the order. A surprising exception are the SVM: there the performance of the raw dataset is better than the extracted one.\n",
    "\n",
    "Confusion Matrix:\n",
    "* In almost all models, the Confusion Matrix time that the classes \"Point1\" and \"Point2\" are confused. Either the accuracy for \"Point1\" is high and that for \"Point2\" is low (its instances are confused with \"Point1\") or vice versa. For models with generally low accuracy, this pattern is difficult to detect or perhaps nonexistent. An exception was the logistic regression model, which had low accuracy for \"Point1\" but confused the data points of this class with the class \"Fist\". In the overall picture, however, it can be said that the models have difficulty distinguishing between the class \"Point1\" and \"Point2\".\n",
    "\n",
    "Feature reduction\n",
    "* The feature reduction in to the low dimensional space e.g. with PCA, is not suitable here to be used as training data. However, PCA was very useful for visualization in 2D and 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7e387-b801-43da-ba43-81f3c958aad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Testing Phase II: Model Developement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994e566",
   "metadata": {},
   "source": [
    "Help Section:<br>\n",
    "You have problems that the kernel freezes mulitple times while training? The Training process seems to run in the background and hence there will appear new models (if you save them as pickle files).\n",
    "But there is no information that the training process finished (running forever)\n",
    "follow this thread:<br>\n",
    "https://stackoverflow.com/questions/52261597/keras-model-fit-verbose-formatting <br>\n",
    "and install:\n",
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326581a",
   "metadata": {},
   "source": [
    "## Base MLP structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7bd1af",
   "metadata": {},
   "source": [
    "* Activation Functions\n",
    "    - output layer: one neuron for each class (5)\n",
    "    - we want the the probability of the each class, -> **softmax**\n",
    "    - hidden Layer: start with **ReLU**\n",
    "\n",
    "* Optimizer\n",
    "    - **ADAM**\n",
    "\n",
    "* loss: **categorical_crossentropy**\n",
    "    - the default loss function to use for multi-class classification problems.\n",
    "    - expect labels to be provided in a one_hot representation\n",
    "    - $$L_i = - \\sum_j{t_{i,j} \\log(p_{i,j})}$$\n",
    "    - p: predictions, t: targets, i: denotes the data point,j: denotes the class.\n",
    "    - Total loss is the average of the per-instance losses.: loss = (loss1 + loss2) / 2\n",
    "    - \"It is the loss function to be evaluated first and only changed if you have a good reason.\" (cite https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "* Hidden Layers and Number of Neurons\n",
    "    - start with small architecture, increase size\n",
    "\n",
    "* Metric\n",
    "    - **accuracy**\n",
    "    - **precission**\n",
    "    - **recall**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562b4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_mlp_model(name:str, hyperparams:dict, input_shape: tuple, output_shape: int) -> keras.Sequential:\n",
    "    'Build MLP classification network'\n",
    "\n",
    "    if hyperparams['hidden_layer'] != len(hyperparams['units_hidden_layer']):\n",
    "        raise ValueError('Number of hidden layers and values provided for number of units per hidden layer do not match')\n",
    "    \n",
    "    # Use l1, l2 regularization?\n",
    "    if hyperparams['weight regularisation l1'] and hyperparams['weight regularisation l2']:\n",
    "        kernel_regu = keras.regularizers.L1L2(l1=hyperparams['weight regularisation l1'],\n",
    "                                              l2=hyperparams['weight regularisation l2'])\n",
    "    elif hyperparams['weight regularisation l1']:\n",
    "        kernel_regu = keras.regularizers.L1(l1=hyperparams['weight regularisation l1'])\n",
    "    elif hyperparams['weight regularisation l2']:\n",
    "        kernel_regu = keras.regularizers.L2(l2=hyperparams['weight regularisation l2'])\n",
    "    else: # Do not use regularization\n",
    "        kernel_regu = None\n",
    "\n",
    "    model = keras.Sequential(name=name)\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "\n",
    "    # Hidden Layer\n",
    "    for num_hl in range(hyperparams['hidden_layer']):\n",
    "        model.add(layers.Dense(hyperparams['units_hidden_layer'][num_hl],\n",
    "                               activation=hyperparams['activation_hidden'],\n",
    "                               kernel_initializer=hyperparams['kernel_initializer'],\n",
    "                               bias_initializer=hyperparams['bias_initializer'],\n",
    "                               kernel_regularizer=kernel_regu\n",
    "                               ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(rate=hyperparams['dropout']))\n",
    "\n",
    "    model.add(layers.Dense(output_shape, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=hyperparams['optimizer'], loss=hyperparams['loss'], metrics=hyperparams['metrics'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7f9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, parameters:dict, name:str):\n",
    "    \n",
    "    plt.plot(hist['epoch'][:],hist['loss'][:], \"k--\", linewidth=1.5, label=\"Training\")\n",
    "    plt.plot(hist['epoch'][:],hist['val_loss'][:], \"b-.\", linewidth=1.5, label=\"CV test\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,max(hist['loss'][:].max(), hist['val_loss'][:].max())+0.2)\n",
    "    plt.xlabel(\"Epochs\"),  plt.ylabel(\"categorical_crossentropy\")\n",
    "    \n",
    "    # Write used hyperparameters as text next to the plot\n",
    "    # Create list of str, where each string contains \"key: val\" from the dictionary\n",
    "    hp_strings = [f'{key}: {val}' for key, val in parameters.items()]\n",
    "    parameter_legend = '\\n'.join(hp_strings)\n",
    "    plt.text(len(hist['epoch']) + len(hist['epoch'])/5, 0, parameter_legend, fontsize=12)\n",
    "\n",
    "    plt.title(f'Learning Curve: {name}', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65c2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multiple_learning_curves(name, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot the learning curves for each cv run\n",
    "    \"\"\"\n",
    "\n",
    "    # Load hist of every cv run\n",
    "\n",
    "    all_hist = []\n",
    "\n",
    "    for i in range(4):\n",
    "        hist = load_history(f'{name}_cv{i}')\n",
    "        all_hist.append(hist)\n",
    "        \n",
    "    # Plot the learning curves for each cv run\n",
    "    for i in range(4):\n",
    "        plot_learning_curves(all_hist[i], hyperparams, f'{name}_cv{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25776e73-0674-45e3-bba6-bfcbf8266e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_curves(hist, parameters, name:str):\n",
    "    '''\n",
    "    Plot the accuracy curves of the training and test set\n",
    "    '''\n",
    "    \n",
    "    plt.plot(hist['epoch'][:],hist['categorical_accuracy'][:], \"k--\", linewidth=1.5, label=\"Training\")\n",
    "    plt.plot(hist['epoch'][:],hist['val_categorical_accuracy'][:], \"b-.\", linewidth=1.5, label=\"CV test\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0,1.0)\n",
    "    plt.xlabel(\"Epochs\"),  plt.ylabel(\"categorical_accuracy\")\n",
    "\n",
    "    plt.title(f'Accuracy Curve: {name}', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c499a7-f83b-42d4-b131-3f043ff3104b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multiple_acc_curves(name, hyperparams):\n",
    "    \"\"\"\n",
    "    Plot the accuracy curves for each cv run\n",
    "    \"\"\"\n",
    "\n",
    "    # Load hist of every cv run\n",
    "\n",
    "    all_hist = []\n",
    "\n",
    "    for i in range(4):\n",
    "        hist = load_history(f'{name}_cv{i}')\n",
    "        all_hist.append(hist)\n",
    "        \n",
    "    # Plot the accuracy curves for each cv run\n",
    "    for i in range(4):\n",
    "        plot_accuracy_curves(all_hist[i], hyperparams, f'{name}_cv{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97854963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_history(hist, name):\n",
    "\n",
    "    # Get path to save the history\n",
    "    hist_folder = 'model/single_run/history'\n",
    "    model_path = join(hist_folder, name)\n",
    "    \n",
    "    with open(model_path, 'wb') as file_pi:\n",
    "        pickle.dump(hist, file_pi)\n",
    "\n",
    "def load_history(name, override_path:str=''):\n",
    "\n",
    "    # Get path to save the history\n",
    "    if not override_path:\n",
    "        hist_folder = 'model/single_run/history'\n",
    "    else:\n",
    "        hist_folder = override_path\n",
    "    model_path = join(hist_folder, name)\n",
    "    \n",
    "    with open(model_path, 'rb') as file_pi:\n",
    "        history = pickle.load(file_pi)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e74265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(name):\n",
    "\n",
    "    # Get path to save the history\n",
    "    model_folder = 'model/single_run'\n",
    "    model_path = join(model_folder, name)\n",
    "    \n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a376ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_used_parameters(parameters:dict):\n",
    "    print('Used parameters:')\n",
    "    for parameter, value in parameters.items():\n",
    "        print(f'{parameter}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a319e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_one_hot(one_hot_encoded):\n",
    "    return np.argmax(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb396a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_f1_score_mlp(model, X, y):\n",
    "    # Calculate f1 score of mlp\n",
    "    y_pred1 = model.predict(X)\n",
    "    y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "    # to one hot encoding\n",
    "    y_pred = to_categorical(y_pred)\n",
    "\n",
    "    return f1_score(y, y_pred , average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9460a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_k_mlp_model(name, df:pd.DataFrame, hyperparams:dict):\n",
    "    '''\n",
    "    Train a MLP model with k-fold cross validation (leave n users out strategy)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str, Name of the model.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    best_model : keras.Sequential, The best model.\n",
    "    best_acc : float, The best accuracy.\n",
    "    X_test_best : numpy.ndarray, The best test set.\n",
    "    y_test_best : numpy.ndarray, The best test set.\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # Create a list of users\n",
    "    user_list = [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Number of users to be used for the test set\n",
    "    num_user_test = 3\n",
    "    num_of_iterations = 4\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    X_test_best = None\n",
    "    y_test_best = None\n",
    "\n",
    "    all_acc_val = []\n",
    "    all_prec_val = []\n",
    "\n",
    "    # Evaluation dict\n",
    "    eval_dict = {'train': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []},\n",
    "                 'test': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []}}\n",
    "\n",
    "    describe_model = True\n",
    "\n",
    "    for i in range(num_of_iterations):\n",
    "\n",
    "        print('*'*50)\n",
    "        print(f'CV Run: {i}')\n",
    "\n",
    "        # Split the data #TODO rename to X_train etc. because function also use by raw and later pca\n",
    "        X_train_cv_extract, y_train_cv_extract, X_test_cv_extract, y_test_cv_extract, user_list = custom_cv_approach(df, user_list, num_user_test=num_user_test)\n",
    "\n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train_cv_extract)\n",
    "        X_train_cv_extract = scaler.transform(X_train_cv_extract)\n",
    "        X_test_cv_extract = scaler.transform(X_test_cv_extract)\n",
    "        # Befor one hot encoding, class has to start at 0\n",
    "        y_train_cv_extract = y_train_cv_extract-1\n",
    "        y_test_cv_extract = y_test_cv_extract-1\n",
    "        # y label to one hot encode\n",
    "        y_train_cv_extract = to_categorical(y_train_cv_extract)\n",
    "        y_test_cv_extract = to_categorical(y_test_cv_extract)\n",
    "\n",
    "        # Build the model\n",
    "        # TODO\n",
    "        mlp_model = build_mlp_model(name, hyperparams, input_shape=X_train_cv_extract.shape[1:], output_shape=y_train_cv_extract.shape[1])\n",
    "\n",
    "        # Only print the model summary once\n",
    "        if describe_model:\n",
    "            mlp_model.summary()\n",
    "            describe_model = False\n",
    "\n",
    "        # Create callback\n",
    "        # Use early stopping later because it is a form of regularization\n",
    "        #early_stop_callback = keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", min_delta=1e-4, patience=5, verbose=1)\n",
    "        checkpoint_callback = keras.callbacks.ModelCheckpoint(f'model/single_run/{name}_cv{i}.h5', save_best_only=True)\n",
    "        \n",
    "        callbacks = [TqdmCallback(verbose=0), checkpoint_callback]\n",
    "        if hyperparams['reduce_lr']:\n",
    "            reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2,\n",
    "                                           patience=5, min_lr=0.00001,\n",
    "                                           min_delta=0.005)\n",
    "            callbacks.append(reduce_lr_callback)\n",
    "\n",
    "        # Train the model\n",
    "        history = mlp_model.fit(X_train_cv_extract, \n",
    "                                y_train_cv_extract, \n",
    "                                epochs=hyperparams['num_epochs'], \n",
    "                                batch_size= hyperparams['batch_size'],\n",
    "                                validation_data=(X_test_cv_extract, y_test_cv_extract),\n",
    "                                verbose=0,\n",
    "                                shuffle=True,\n",
    "                                callbacks=[TqdmCallback(verbose=0), checkpoint_callback])\n",
    "\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        hist['epoch'] = history.epoch\n",
    "\n",
    "        # Save the history\n",
    "        save_history(hist, f'{name}_cv{i}')\n",
    "\n",
    "        #Evaluating the training performance:\n",
    "        train_loss, train_acc, train_precision, train_recall = mlp_model.evaluate(x=X_train_cv_extract, y=y_train_cv_extract, batch_size=hyperparams['batch_size'], verbose=3)\n",
    "        f1_score_train = calc_f1_score_mlp(mlp_model, X_train_cv_extract, y_train_cv_extract)\n",
    "\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'train', train_acc, train_precision, train_recall, f1_score_train)\n",
    "        print('-'*70)\n",
    "        print('Evaluation of training Data: \\n', 'training loss: ', train_loss, 'training accuracy: ', train_acc)\n",
    "\n",
    "        #Evaluating the CV pperformance:\n",
    "        val_loss, val_acc, val_precision, val_recall = mlp_model.evaluate(x=X_test_cv_extract, y=y_test_cv_extract, batch_size=hyperparams['batch_size'], verbose=0)\n",
    "        f1_score_test = calc_f1_score_mlp(mlp_model, X_test_cv_extract, y_test_cv_extract)\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'test', val_acc, val_precision, val_recall, f1_score_test)\n",
    "        all_acc_val.append(val_acc)\n",
    "        all_prec_val.append(val_precision)\n",
    "        print('Evaluation of validation Data: \\n', 'cv loss: ', val_loss, 'cv accuracy: ', val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_model = mlp_model\n",
    "            best_acc = val_acc\n",
    "            X_test_best = X_test_cv_extract\n",
    "            y_test_best = y_test_cv_extract\n",
    "\n",
    "    #Lets see the overall score as average of the scores of all the folds:\n",
    "    print('-'*70)\n",
    "    print('(all CV runs combined)')\n",
    "    print('Mean Accuracy  for the validation dataset: ', np.mean(all_acc_val))\n",
    "    print('Mean Precision for the validation dataset: ', np.mean(all_prec_val))\n",
    "    print('-'*70)\n",
    "\n",
    "    # Save the model\n",
    "    best_model.save(f'model/single_run/{name}_best_model.h5')\n",
    "\n",
    "    # Save the evaluation dict\n",
    "    save_eval_dict_pkl(eval_dict, f'{name}_eval_dict')\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plot_learning_curves(hist, hyperparams, name)\n",
    "\n",
    "    return best_model, best_acc, X_test_best, y_test_best, eval_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe257e",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed85ad",
   "metadata": {},
   "source": [
    "<ins>Disclaimer</ins>: If you are only interested in the results, you can skip this section! I would like to guarantee a red thread here and show that the models did not fall from the sky like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d25e83",
   "metadata": {},
   "source": [
    "Since I adjusted hyperparameters several times in the following sections, I would like to give the reader a brief overview here of how the adjustment steps went:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456c613",
   "metadata": {},
   "source": [
    "1. Model seems to overfit from the very beginning <br>\n",
    "I used a structure like this:<br>\n",
    "number of neurons in the hidden layers: [32, 64, 64, 32, 16]<br>\n",
    "<img src=\"img/overfit_big_model.png\" width=330 height=310 /> <br>\n",
    "I decided to reduce the number neurons and layers. The new structure looked like this:<br>\n",
    "number of neurons in the hidden layers: [16, 16, 8]<br>\n",
    "<img src=\"img/overfit_small_model.png\" width=330 height=310 /> <br>\n",
    "I added Batch normalization and drop out layer to prevent the model from overfitting. <br>\n",
    "Problem still seemed to be presence. So I reduced the network complexity even further:<br>\n",
    "number of neurons in the hidden layers: [8, 8, 8]<br>\n",
    "<img src=\"img/underfit.png\" width=330 height=310 /> <br>\n",
    "Now the model was underfitting. The training loss was not decreasing that much after a few epochs. The validation loss was on a same level. Another thing I was wondering about was the weird behaviour of the validation loss. Right from the beginning of training it was increasing and the curve has fluctuated very strongly.\n",
    "\n",
    "2. Validation loss is more fluctuating then the training loss<br>\n",
    "After some internet reasearch I got inspiration what to look for. I have added more validation data as it may not be representative of the data. Now I selected three user for the validation data instead of instead of two.\n",
    "3. Check preprocessing:\n",
    "Some posts also suggested two have a look into my preprocessing steps. Some people suggested I might have processed the validation and the training data in a different way\n",
    "I did not shuffle the data!\n",
    "```python\n",
    "model.fit(..., shuffle=True) #Keras model\n",
    "```\n",
    "-> only shuffels the training set!! not the the validation set\n",
    "\n",
    "4. First real baseline model\n",
    "After decreasing the learning rate and increased the drop out rate for the drop out layer, I got a quiet descent MLP model: <br>\n",
    "<img src=\"img/baseline_model.png\" width=330 height=310 /> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d357fe",
   "metadata": {},
   "source": [
    "### a) Custom CV - with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a1c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparams = {'num_epochs': 35, # 35,\n",
    "               'batch_size': 32,\n",
    "               'hidden_layer': 3,\n",
    "               'units_hidden_layer': [16, 16, 8],#[64, 64, 32, 16],\n",
    "               'activation_hidden': keras.layers.LeakyReLU(alpha=0.1), #'LeakyReLU' #'relu',\n",
    "               'activation_output': 'softmax',\n",
    "               'loss': 'categorical_crossentropy',\n",
    "               'metrics': ['categorical_accuracy', 'Precision', tf.keras.metrics.Recall()],\n",
    "               'optimizer': keras.optimizers.legacy.Adam(learning_rate=0.00005), #0.0003\n",
    "               'kernel_initializer': 'he_normal', # used for ReLU activation function\n",
    "               'bias_initializer': 'zeros',\n",
    "               'initialization': '-',\n",
    "               'weight regularisation l1': '',\n",
    "               'weight regularisation l2': '',\n",
    "               'dropout': 0.35,  # typically between 0.3 and 0.5 (half of weights get 0)\n",
    "               'early Stopping': 'False',\n",
    "               'reduce_lr': False} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aad82d",
   "metadata": {},
   "source": [
    "To get an Idea How long the model trains: <br>\n",
    "22min for 4 runs (because of CV)\n",
    "* each CV run trains for 35 epochs\n",
    "\n",
    "Highly depends on the used Hardware! <br>\n",
    "Here we use an M1 macbook with GPU tf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f0b2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'mlp_base_model_raw'\n",
    "df = df_raw\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict = train_k_mlp_model(name, df, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d36db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)\n",
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc184952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_report_mlp_base_raw = Report('mlp_raw', best_model, X_test_best, y_test_best, description=['MLP base', 'cv_data', 'raw features'])\n",
    "df_results = data_report_mlp_base_raw.get_report_as_df(df_results)\n",
    "print(data_report_mlp_base_raw)\n",
    "\n",
    "# Prepare data to be compatible with confusion matrix function\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "y_pred = np.argmax(y_pred, axis=1)+1\n",
    "y_test_int = np.argmax(y_test_best, axis=1)+1\n",
    "plot_cm(y_test_int, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bae3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad78e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_mode = 'retrain' # 'retrain' or 'load_model'\n",
    "if run_mode == 'load_model':\n",
    "    hist = load_history('mlp_base_model_raw_cv3')\n",
    "    plot_learning_curves(hist, hyperparams, 'test')\n",
    "    eval_dict = load_eval_dict_pkl('mlp_base_model_raw_eval_dict')\n",
    "    plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00860b9",
   "metadata": {},
   "source": [
    "### b) Custom CV - with extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98938d2b",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a6921-57fc-4646-b89c-00decf84d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I figured out the the model trained with extracted features need more epochs\n",
    "hyperparams['num_epochs'] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836b70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'mlp_base_model_extracted'\n",
    "df = df_aggregate\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict = train_k_mlp_model(name, df, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c4933",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b1e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d563fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd69e5c-5489-4647-9105-be5abf7b6e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfad07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_report_mlp_base_extract = Report('mlp_extract', best_model, X_test_best, y_test_best, description=['MLP base', 'cv_data', 'extracted features'])\n",
    "df_results = data_report_mlp_base_extract.get_report_as_df(df_results)\n",
    "print(data_report_mlp_base_extract)\n",
    "\n",
    "# Prepare data to be compatible with confusion matrix function\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "y_pred = np.argmax(y_pred, axis=1)+1\n",
    "y_test_int = np.argmax(y_test_best, axis=1)+1\n",
    "plot_cm(y_test_int, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe123af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eacfa-984f-4b53-b8ef-3d0d086e4c7b",
   "metadata": {},
   "source": [
    "### c) Custom CV -with pca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c4d41-bc20-423a-805f-181760f6b119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'mlp_base_model_pca'\n",
    "df = df_pca_features\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict = train_k_mlp_model(name, df, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44576068-c45e-4f37-8787-ffd2e140a3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)\n",
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52207f-ee18-4e73-838b-a062e0bbb5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_report_mlp_base_pca = Report('mlp_pca', best_model, X_test_best, y_test_best, description=['MLP base', 'cv_data', 'pca features'])\n",
    "df_results = data_report_mlp_base_pca.get_report_as_df(df_results)\n",
    "print(data_report_mlp_base_pca)\n",
    "\n",
    "# Prepare data to be compatible with confusion matrix function\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "y_pred = np.argmax(y_pred, axis=1)+1\n",
    "y_test_int = np.argmax(y_test_best, axis=1)+1\n",
    "plot_cm(y_test_int, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d806f7-c835-4142-b7ca-445d339b5fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_dict_barplot_new(eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57623d49",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6121db1",
   "metadata": {},
   "source": [
    "Already training the baseline MLP models has required several iterative fits. Partial regularization methods were already used, e.g. dropout layer. In this section, however, these were only applied to be able to train the model correctly at all.\n",
    "\n",
    "Now we have base mlp model that we trained in a stable way (no big fluctuations etc.). But the model is overfitted for almost all cross validation runs. So we see that there is still potential to find a better model by applying regularization and fine tuning (see section Testing Phase III)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0d448",
   "metadata": {},
   "source": [
    "## PointNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f5969",
   "metadata": {},
   "source": [
    "Note: for this archtitecture we have to use the original, raw data because we need a Point Cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dad2d5",
   "metadata": {},
   "source": [
    "From the Paper: \"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\" (Charles R. Qi et al)\n",
    "https://arxiv.org/abs/1612.00593\n",
    "\n",
    "Further information:\n",
    "https://arxiv.org/pdf/1912.12033.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de8251",
   "metadata": {},
   "source": [
    "DISCLAIMER:\n",
    "Implementation with Keras:<br>\n",
    "https://keras.io/examples/vision/pointnet/\n",
    "\n",
    "Here we replicate the network architecture published in the original paper with the help of this blogpost!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180ad6b",
   "metadata": {},
   "source": [
    "General\n",
    "* Point cloud =  a geometric data structure with irregular format\n",
    "* paper proposes a novel neural network called PointNet\n",
    "* PointNet: directly consumes point clouds + respects the permutation invariance of points in the input\n",
    "* unified architecture for object classification, part segmentation, and scene semantic parsing\n",
    "* simple, efficient, and effective\n",
    "\n",
    "Architecture\n",
    "* deal with unordered input set: use of a single symmetric function, max pooling\n",
    "* network learns a set of optimization functions/criteria that select interesting or informative points of the point cloud and encode the reason for their selection\n",
    "* final fully connected layers: aggregate these learnt optimal values into the global descriptor for the entire shape (shape classification) or are used to predict per point labels (shape segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df0649",
   "metadata": {},
   "source": [
    "Each convolution and fully-connected layer (with exception for end layers) consits of \n",
    "* Convolution / Dense\n",
    "* Batch Normalization\n",
    "* ReLU Activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbccb3-a318-4d3e-a6d8-c12cdb19126e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters, hyperparams):\n",
    "    kernel_reg = None\n",
    "    if hyperparams['l2']:\n",
    "        kernel_reg = keras.regularizers.L2(l2=hyperparams['l2'])\n",
    "    x = layers.Dense(filters, \n",
    "                     kernel_regularizer= keras.regularizers.L2(l2=0.01) #NEWLY ADDED FROM ME\n",
    "                    )(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0dadf-137d-4fc0-ab0a-1d5da9dac6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_bn_best(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fc07b",
   "metadata": {},
   "source": [
    "PointNet consists of two core components\n",
    "* primary MLP network\n",
    "* transformer net (T-net)\n",
    "    * aims to learn an affine transformation matrix by its own mini network\n",
    "    * used twice:\n",
    "        * 1.to transform the input features (n, 3) into a canonical representation\n",
    "        * 2.affine transformation for alignment in feature space (n, 3)\n",
    "\n",
    "What will we do?\n",
    "* implement main network \n",
    "* drop the t-net mini models as layers in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f788f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "    def get_config(self): #TODO required for saving model\n",
    "        return {'test': 'test'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0453b-16d8-469b-8c89-20799a4e4956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tnet_best(inputs, num_features):\n",
    "    '''Build T-net layers'''\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 16) # Original 32 in paper\n",
    "    x = conv_bn(x, 16) # Original 64 in paper\n",
    "    #x = conv_bn(x, 32) # Original 512 in paper\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 16, hyperparams) # Original 256 in paper\n",
    "    #NEWLY ADDED FROM ME\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    #x = dense_bn(x, 8) # Original 128 in paper\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef67fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tnet(inputs, num_features, hyperparams:dict):\n",
    "    '''Build T-net layers'''\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 16) # Original 32 in paper\n",
    "    x = conv_bn(x, 16) # Original 64 in paper\n",
    "    #x = conv_bn(x, 32) # Original 512 in paper\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 16, hyperparams) # Original 256 in paper\n",
    "    #NEWLY ADDED FROM ME\n",
    "    x = layers.Dropout(hyperparams['dropout_rate'])(x)\n",
    "    #x = dense_bn(x, 8) # Original 128 in paper\n",
    "    kernel_reg = None\n",
    "    if hyperparams['l2']:\n",
    "        kernel_reg = keras.regularizers.L2(l2=hyperparams['l2'])\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "        kernel_regularizer= kernel_reg #NEWLY ADDED FROM ME\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdcd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_pointnet(num_points, num_classes, hyperparams:dict):\n",
    "    '''Use the functional API to build a PointNet model (different from the Sequential API)'''\n",
    "    inputs = keras.Input(shape=(num_points, 3))\n",
    "\n",
    "    x = tnet(inputs, 3, hyperparams)\n",
    "    #x = conv_bn(x, 16) #OG 32\n",
    "    x = conv_bn(x, 16) #OG 32\n",
    "    x = tnet(x, 16, hyperparams) #OG 32\n",
    "    x = conv_bn(x, 16) #OG 32\n",
    "    x = conv_bn(x, 16) #OG 64\n",
    "    #x = conv_bn(x, 32) #OG 512\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 16, hyperparams) #OG 256\n",
    "    x = layers.Dropout(hyperparams['dropout_rate'])(x) #0.3\n",
    "    #x = dense_bn(x, 16) #OG 128\n",
    "    #x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17437083",
   "metadata": {},
   "source": [
    "Now we have all utility functions we need for our PointNet model<br>\n",
    "We have to preprocess the input data to be compatible with the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd660fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_cv_approach_point_clouds(np_batches:np.array, df:pd.DataFrame, user_list:list, num_user_test:int=2) -> Tuple[np.array, np.array, np.array, np.array, list]:\n",
    "    '''\n",
    "    MODIFIED for point cloud data\n",
    "    each user is iteratively left out from training and used as a test set. \n",
    "    We then tests the generalization of the algorithm to new users. \n",
    "    A 'User' attribute is provided to accomodate this strategy. \n",
    "    '''\n",
    "\n",
    "    def cv_ratio(y_test, df):\n",
    "        print(f'Ratio of test set: {len(y_test)/len(df)}')\n",
    "    \n",
    "    # Get the indices for the training and test set\n",
    "    train_indices, test_indices, user_list = get_train_test_user(df, user_list, num_user_test)\n",
    "    # Shuffle data\n",
    "    shuffle(train_indices)\n",
    "    shuffle(test_indices)\n",
    "    # Create the training and test set\n",
    "    # MODIFIED for point cloud data\n",
    "    X_train = np_batches[train_indices, :, :]\n",
    "    y_train = df.iloc[train_indices, :]['Class']\n",
    "    X_test = np_batches[test_indices, :, :]\n",
    "    y_test = df.iloc[test_indices, :]['Class']\n",
    "\n",
    "    cv_ratio(y_test, df)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd8a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_k_pn_model(name, np_batches:np.array, df:pd.DataFrame, hyperparams:dict):\n",
    "    '''\n",
    "    Train multiple PointNet models with k-fold cross validation (leave n users out strategy)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str, Name of the model.\n",
    "    np_batches : numpy.ndarray, The data.\n",
    "    df : pandas.DataFrame, We will use this dataframe ONLY to get the labels.\n",
    "    hyperparams : dict, The hyperparameters.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    best_model : keras.Sequential, The best model.\n",
    "    best_acc : float, The best accuracy.\n",
    "    X_test_best : numpy.ndarray, X_test set\n",
    "    y_test_best : numpy.ndarray, y_test set\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # Create a list of users\n",
    "    user_list = [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Number of users to be used for the test set\n",
    "    num_user_test = 3\n",
    "    num_of_iterations = 4\n",
    "\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "    X_test_best = None\n",
    "    y_test_best = None\n",
    "\n",
    "    all_acc_val = []\n",
    "    all_prec_val = []\n",
    "\n",
    "    # Evaluation dict\n",
    "    #TODO add recall and f1 score\n",
    "    eval_dict = {'train': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []},\n",
    "                 'test': {'accuracy':[], 'precision':[], 'recall':[], 'f1': []}}\n",
    "\n",
    "    describe_model = True\n",
    "\n",
    "    for i in range(num_of_iterations):\n",
    "\n",
    "        print('*'*50)\n",
    "        print(f'CV Run: {i}')\n",
    "\n",
    "        # Split the data into train and test set\n",
    "        X_train_cv_pn, y_train_cv_pn, X_test_cv_pn, y_test_cv_pn, user_list = custom_cv_approach_point_clouds(np_batches, df_raw, user_list, num_user_test=num_user_test)\n",
    "\n",
    "        # Normalize the data\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_cv_pn = scaler.fit_transform(X_train_cv_pn.reshape(-1, X_train_cv_pn.shape[-1])).reshape(X_train_cv_pn.shape)\n",
    "        X_test_cv_pn = scaler.transform(X_test_cv_pn.reshape(-1, X_test_cv_pn.shape[-1])).reshape(X_test_cv_pn.shape)\n",
    "        # Before one hot encoding, class has to start at 0\n",
    "        y_train_cv_pn = y_train_cv_pn-1\n",
    "        y_test_cv_pn = y_test_cv_pn-1\n",
    "        # y label to one hot encode\n",
    "        y_train_cv_pn = to_categorical(y_train_cv_pn)\n",
    "        y_test_cv_pn = to_categorical(y_test_cv_pn)\n",
    "\n",
    "        # Build model\n",
    "        print('X_train.shape: ', X_train_cv_pn.shape)\n",
    "        print('y_train.shape: ', y_train_cv_pn.shape)\n",
    "        \n",
    "        max_num_points = 10\n",
    "        #TODO add hyperparams\n",
    "        pointnet_model = build_pointnet(max_num_points, y_train_cv_pn.shape[1], hyperparams)\n",
    "\n",
    "        # Only print the model summary once\n",
    "        if describe_model:\n",
    "            pointnet_model.summary()\n",
    "            describe_model = False\n",
    "\n",
    "        # Create callback\n",
    "        #early_stop_callback = keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", min_delta=1e-4, patience=5, verbose=1)\n",
    "        checkpoint_callback = keras.callbacks.ModelCheckpoint(f'model/single_run/{name}_cv{i}.h5', save_best_only=True)\n",
    "        \n",
    "        pointnet_model.compile(\n",
    "            loss=hyperparams['loss'],\n",
    "            optimizer=hyperparams['optimizer'],\n",
    "            metrics=hyperparams['metrics'],\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        history = pointnet_model.fit(X_train_cv_pn, \n",
    "                                y_train_cv_pn, \n",
    "                                epochs=hyperparams['num_epochs'], \n",
    "                                batch_size= hyperparams['batch_size'],\n",
    "                                validation_data=(X_test_cv_pn, y_test_cv_pn),\n",
    "                                verbose=0,\n",
    "                                shuffle=True,\n",
    "                                callbacks=[TqdmCallback(verbose=0), checkpoint_callback])\n",
    "\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        hist['epoch'] = history.epoch\n",
    "\n",
    "        # Save the history\n",
    "        save_history(hist, f'{name}_cv{i}')\n",
    "\n",
    "        #Evaluating the training performance:\n",
    "        train_loss, train_acc, train_precision, train_recall = pointnet_model.evaluate(x=X_train_cv_pn, y=y_train_cv_pn, batch_size=hyperparams['batch_size'], verbose=1)\n",
    "        f1_score_train = calc_f1_score_mlp(pointnet_model, X_train_cv_pn, y_train_cv_pn)\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'train', train_acc, train_precision, train_recall, f1_score_train)\n",
    "        print('-'*70)\n",
    "        print('Evaluation of training Data: \\n', 'training loss: ', train_loss, 'training accuracy: ', train_acc)\n",
    "\n",
    "        #Evaluating the CV pperformance:\n",
    "        val_loss, val_acc, val_precision, val_recall = pointnet_model.evaluate(x=X_test_cv_pn, y=y_test_cv_pn, batch_size=hyperparams['batch_size'], verbose=1)\n",
    "        f1_score_test = calc_f1_score_mlp(pointnet_model, X_test_cv_pn, y_test_cv_pn)\n",
    "        eval_dict = save_to_eval_dict(eval_dict, 'train', val_acc, val_precision, val_recall, f1_score_test)\n",
    "        all_acc_val.append(val_acc)\n",
    "        all_prec_val.append(val_precision)\n",
    "        print('Evaluation of validation Data: \\n', 'cv loss: ', val_loss, 'cv accuracy: ', val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_model = pointnet_model\n",
    "            best_acc = val_acc\n",
    "            X_test_best = X_test_cv_pn\n",
    "            y_test_best = y_test_cv_pn\n",
    "\n",
    "    #Lets see the overall score as average of the scores of all the folds:\n",
    "    print('-'*70)\n",
    "    print('(all CV runs combined)')\n",
    "    print('Mean Accuracy  for the validation dataset: ', np.mean(all_acc_val))\n",
    "    print('Mean Precision for the validation dataset: ', np.mean(all_prec_val))\n",
    "    print('-'*70)\n",
    "\n",
    "    # Save the model\n",
    "    best_model.save(f'model/single_run/{name}_best_model.h5')\n",
    "\n",
    "    # Save the evaluation dict\n",
    "    save_eval_dict_pkl(eval_dict, f'{name}_eval_dict')\n",
    "\n",
    "    # Plot the learning curves\n",
    "    plot_learning_curves(hist, hyperparams, name)\n",
    "\n",
    "    return best_model, best_acc, X_test_best, y_test_best, eval_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa300e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each row of data (multiple data points) got stored as single dataframe where each row is one single data point\n",
    "# Convert the dataframe to 3D numpy Matrix \n",
    "np_batches = np.array(list(map(pd.DataFrame.to_numpy, concat_batches)))\n",
    "np_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a206d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to reshape the data to be able to normalize it\n",
    "# EXAMPLE: for 2 batches (for the real data we will do it in the training loop)\n",
    "print(np_batches[0:2].shape)\n",
    "reshaped = np_batches[0:2].reshape(-1, np_batches[0].shape[-1])\n",
    "print(reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4bd6b-45fb-42a0-936f-c785ecf6b0a2",
   "metadata": {},
   "source": [
    "### a) PointNet with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044be46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparams = {'num_epochs': 20,\n",
    "               'batch_size': 32, #32,\n",
    "               'activation_hidden': 'relu',\n",
    "               'activation_output': 'softmax',\n",
    "               'loss': 'categorical_crossentropy',\n",
    "               'metrics': ['categorical_accuracy', 'Precision', tf.keras.metrics.Recall()],\n",
    "               'optimizer': keras.optimizers.legacy.Adam(learning_rate=0.0001), #OG 0.001\n",
    "               'dropout_rate': 0.3,  # typically between 0.3 and 0.5 (half of weights get 0)\n",
    "               'l2': None #0.01,\n",
    "              } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc5ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "name = 'pointnet_base_raw'\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict_pn = train_k_pn_model(name, np_batches, df_raw, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb8457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)\n",
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34841bb6",
   "metadata": {},
   "source": [
    "Overview over my steps:<br>\n",
    "Since I followed this blog entry https://keras.io/examples/vision/pointnet/ I already knew that I have to adjust the Network.\n",
    "The proposed Network is inspired from the original paper, but is already adjusted to the Dataset the people from Keras are using.\n",
    "The parameter overview looks as follows:<br>\n",
    "\n",
    "Total params: 748,979<br>\n",
    "Trainable params: 742,899<br>\n",
    "Non-trainable params: 6,080<br>\n",
    "\n",
    "Way too big for our problem! Their data set has 2048 Data points per instance and 10 classes in total.\n",
    "We have 5 classes and only 11 points!\n",
    "\n",
    "I tried to decrease the network size by reducing the number of neurons in each layer:\n",
    "e.g. x = conv_bn(x, 64) and x = dense_bn(x, 256)\n",
    "\n",
    "The result was an architecture with 15,206 trainable parameters. Lets give it a try:<br>\n",
    "<img src=\"img/pn_oversized.png\" width=330 height=500 /> <br>\n",
    "\n",
    "We still have a heavily oversized model. The model is not learning on the validation set. Its just memorizing the training data.\n",
    "\n",
    "Then I tried to reduce the size even further:<br>\n",
    "<img src=\"img/pn_still_oversized.png\" width=330 height=500 /> <br>\n",
    "Still to many parameters to learn. While having a look at the accuracy plot we can see even more clear that the model is overfitting:<br>\n",
    "<img src=\"img/pn_overfit.png\" width=330 height=500 /> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61048ab3-aa16-4f0e-9740-16fe6de7e21a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_report_pn_base_raw = Report('pn_raw', best_model, X_test_best, y_test_best, description=['PointNet base', 'cv_data', 'raw features'])\n",
    "df_results = data_report_mlp_base_raw.get_report_as_df(df_results)\n",
    "print(data_report_pn_base_raw)\n",
    "\n",
    "# Prepare data to be compatible with confusion matrix function\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "y_pred = np.argmax(y_pred, axis=1)+1\n",
    "y_test_int = np.argmax(y_test_best, axis=1)+1\n",
    "plot_cm(y_test_int, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35471f1-f3fd-4e24-8e00-e55b0a8dadbb",
   "metadata": {},
   "source": [
    "**Adjust network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140efbf9-933c-40af-9c83-72e1ff2680f6",
   "metadata": {},
   "source": [
    "We will increase the failure rate from 0.3 to 0.5. In addition, we will use L2 Regularization. To reduce the volatility of the loss we will increase the batch size. We slightly increase the learning rate from 0.00005 to 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a478f9-2d51-4f5e-9a1f-73560e0c2f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparams = {'num_epochs': 20,\n",
    "               'batch_size': 64, #32,\n",
    "               'activation_hidden': 'relu',\n",
    "               'activation_output': 'softmax',\n",
    "               'loss': 'categorical_crossentropy',\n",
    "               'metrics': ['categorical_accuracy', 'Precision', tf.keras.metrics.Recall()],\n",
    "               'optimizer': keras.optimizers.legacy.Adam(learning_rate=0.0001), #OG 0.001\n",
    "               'dropout_rate': 0.5,  # typically between 0.3 and 0.5 (half of weights get 0)\n",
    "               'l2': 0.001,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec6d3f9-ba1c-4bff-ad97-aae63250f118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "name = 'pointnet_base'\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict_pn = train_k_pn_model(name, np_batches, df_raw, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862a576-a69c-4e20-8148-da906d7f57c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)\n",
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c23d5",
   "metadata": {},
   "source": [
    "### b) PointNet with extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82506cd-de89-4b0a-b6de-5b0127667e08",
   "metadata": {},
   "source": [
    "Of course, this is not possible because PointNet is designed for point clouds. I.e. an instance to be classified consists of several points (here a maximum of 11) with X, Y, Z coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74842476",
   "metadata": {},
   "source": [
    "## Conclusion Testing Phase II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad126243-daed-4a32-8250-7f866c955b8f",
   "metadata": {},
   "source": [
    "At least for some CV runs the validation loss is now stabilized. However, not for all. The hyperparamter adjustment shown above is only exemplary. Over several iterations I tried hyperparameters and tried to understand their effect. It is also difficult to adapt the proposed architecture from the paper to my problem. There is a risk of deleting essential parts from the architecture. However, some adjustments are necessary to train such a large network without overfitting. I will concentrate on the MLP from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d93751-254a-4e14-8206-57c62006cc1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Testing Phase III: Model Regularization and Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521b643",
   "metadata": {},
   "source": [
    "In this section we try to fine tune the model to achieve a better performance. We have already partially applied regularization methods in \"Testing Phase II\", but they were only used there to merely stabilize the training process. It would probably have been possible to achieve a stable training process (\"good\" loss curve) without regularization. But the bug fixxing was more an iterative process in which different error sources etc. should be excluded.\n",
    "\n",
    "In this section we focus on the fine tuning of these regularization methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3e95e-fdd2-4ee3-a262-bf72ea5f7bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "In a first step we will tune the model by hand. This is followed by a randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79e8bc",
   "metadata": {},
   "source": [
    "we will focus on following hyperparameter\n",
    "* number of neurons\n",
    "* regularization\n",
    "* number of layers\n",
    "* activation function\n",
    "* dropout\n",
    "* Initialization\n",
    "* Optimizer\n",
    "* Batch Size\n",
    "* Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe03ed-1008-4e05-81f7-970f38eac33a",
   "metadata": {},
   "source": [
    "## Manual Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54063b50-5e46-49f7-b525-7df57884abb7",
   "metadata": {},
   "source": [
    "Keeping in mind that the model with extracted features achieved a higher perfomance we will focus on this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3df217-eefe-4187-8713-9e17c5c978bc",
   "metadata": {},
   "source": [
    "In this section we start using EarlyStopping and LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1a83f-303d-46e7-94a0-8b0964e3e8fb",
   "metadata": {},
   "source": [
    "### Custom CV - with extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6112e2-85aa-4980-bcad-684b00de671d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparams = {'num_epochs': 55,\n",
    "               'batch_size': 32,\n",
    "               'hidden_layer': 3,\n",
    "               'units_hidden_layer': [16, 16, 8],#[64, 64, 32, 16],\n",
    "               'activation_hidden': keras.layers.LeakyReLU(alpha=0.1), #'LeakyReLU' #'relu',\n",
    "               'activation_output': 'softmax',\n",
    "               'loss': 'categorical_crossentropy',\n",
    "               'metrics': ['categorical_accuracy', 'Precision', tf.keras.metrics.Recall()],\n",
    "               'optimizer': keras.optimizers.legacy.Adam(learning_rate=0.0001), # 0.0001 #0.0003\n",
    "               'kernel_initializer': 'he_normal', # used for ReLU activation function\n",
    "               'bias_initializer': 'zeros',\n",
    "               'initialization': '-',\n",
    "               'weight regularisation l1': '',\n",
    "               'weight regularisation l2': 0.001, #0.001,\n",
    "               'dropout': 0.5,  # typically between 0.3 and 0.5 (half of weights get 0)\n",
    "               'early Stopping': 'False',\n",
    "               'reduce_lr': True} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ce74d-e908-41b4-b764-beadbefc32bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = 'mlp_base_model_extracted_tuned'\n",
    "df = df_aggregate\n",
    "best_model, best_acc, X_test_best, y_test_best, eval_dict = train_k_mlp_model(name, df, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d08c0-d4ef-4b51-83a7-665b27807d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_learning_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89572ec-f04a-40a5-9b4c-17cb6ca23125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_multiple_acc_curves(name, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da88a8b-0147-4486-b5e0-68da765db254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_report_mlp_tuned_extract = Report('mlp_extract_tuned', best_model, X_test_best, y_test_best, description=['MLP base', 'cv_data', 'extract features', 'manual tuned'])\n",
    "#df_results = data_report_mlp_base_raw.get_report_as_df(df_results)\n",
    "print(data_report_mlp_tuned_extract)\n",
    "\n",
    "# Prepare data to be compatible with confusion matrix function\n",
    "y_pred = best_model.predict(X_test_best)\n",
    "y_pred = np.argmax(y_pred, axis=1)+1\n",
    "y_test_int = np.argmax(y_test_best, axis=1)+1\n",
    "plot_cm(y_test_int, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e28def",
   "metadata": {},
   "source": [
    "### My steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebd80c",
   "metadata": {},
   "source": [
    "Manual fine tuning of the model is a highly iterative process. Instead of cluttering the entire notebook with multiple trial runs and making it confusing, I will describe the iterative process here. The actual changes were modified directly in the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b713ed",
   "metadata": {},
   "source": [
    "For example, we see here that the learning rate is too low. One could reach the goal with a low learning rate, but this wastes computing resources unnecessarily, because we need more epochs.\n",
    "I decide to increase the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2e19a",
   "metadata": {},
   "source": [
    "![title](img/lr_too_low.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9c28f-4c03-455e-b205-2704fd132c88",
   "metadata": {},
   "source": [
    "But choosing the learning rate too high leads too high fluctuation of the validation loss (here: 0.001, similiar with 0.0006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7205b5-b065-4905-afc4-5b56e10bef85",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](img/lr_too_high.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcae6d",
   "metadata": {},
   "source": [
    "I also figured out that reducing the learning rate after some time can help the model to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bbc3f-57f5-4065-82d6-54bb36484168",
   "metadata": {
    "tags": []
   },
   "source": [
    "I also tried different optimizers, e.g. the SGD Optimizer. However, I was not able to stabilize the training process with different values for momentum and learning rate. After a few epochs the validation loss starts to fluctuate strongly\n",
    "\n",
    "The SGD I used: keras.optimizers.legacy.SGD(learning_rate=0.0001, momentum=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a66a51-3fff-43a6-9d89-442d14dd1d21",
   "metadata": {},
   "source": [
    "![title](img/unsteady_sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c14fb-7d7e-45ec-aacc-7297b7c892f2",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25987ef2-dfaa-4c72-8cca-71a6b0e44bda",
   "metadata": {},
   "source": [
    "Hyperparameter tuning allowed us to partially prevent overfitting.We were able to achieve higher accuracy with 84% (the baseline model with extracted data had an accuracy of 81%). Especially a higher dropout rate could help that the model is not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a074907",
   "metadata": {},
   "source": [
    "## Automatic hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b9264-0a92-44b4-ad35-7fb12b4deaa1",
   "metadata": {},
   "source": [
    "This section takes a very long time to run (depending on the parameter combination and number of models studied: about 2 days). Therefore, in the final version only the latest hyperparamter search is loaded from the hard disk.\n",
    "\n",
    "Over time I have run several hyperparameter searches. Depending on the results, some of them were deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2096d7-3187-4a90-b6dd-e649eb95a501",
   "metadata": {},
   "source": [
    "To find a good value range for a specific hyperparamter, simply have a look online <br>\n",
    "e.g. https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "for L2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9928671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "\n",
    "class Classification_MLP_tuner(HyperModel):\n",
    "    '''\n",
    "    Build a HyperParameter Model with variable hyperparameters\n",
    "    e.g. Optimizer, learning rate \n",
    "    \n",
    "    Note: model.compile must be included in this function\n",
    "    '''\n",
    "\n",
    "    def __init__(self, name:str, input_shape, num_output, learning_rate:list, max_num_of_hidd_layer:int, max_num_of_neuron_per_layer:int, min_num_of_neuron_per_layer:int, activation_func_hidden_layer:list, weight_regularisation_l1:list, weight_regularisation_l2:list):\n",
    "        self.name = name\n",
    "        self.input_shape = input_shape\n",
    "        self.num_ouput = num_output\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_num_of_hidd_layer = max_num_of_hidd_layer\n",
    "        self.max_num_of_neuron_per_layer = max_num_of_neuron_per_layer\n",
    "        self.min_num_of_neuron_per_layer = min_num_of_neuron_per_layer\n",
    "        self.activation_func_hidden_layer = activation_func_hidden_layer\n",
    "        self.weight_regularisation_l1 = weight_regularisation_l1\n",
    "        self.weight_regularisation_l2 = weight_regularisation_l2\n",
    "\n",
    "    def build(self, hp):\n",
    "\n",
    "        model = keras.Sequential(name=self.name)\n",
    "        model.add(layers.InputLayer(input_shape=self.input_shape))\n",
    "        \n",
    "        l1 = hp.Float('l1',\n",
    "                 min_value=self.weight_regularisation_l1[0],\n",
    "                 max_value=self.weight_regularisation_l1[1],\n",
    "                 sampling='LOG',\n",
    "                default=0)\n",
    "        \n",
    "        l2 = hp.Float('l2',\n",
    "                 min_value=self.weight_regularisation_l2[0],\n",
    "                 max_value=self.weight_regularisation_l2[1],\n",
    "                 sampling='LOG',\n",
    "                default=0)\n",
    "\n",
    "        # Use l1, l2 regularization?\n",
    "        if l1 and l2:\n",
    "            kernel_regu = keras.regularizers.L1L2(l1=l1, l2=l2)\n",
    "        elif l1:\n",
    "            kernel_regu = keras.regularizers.L1(l1=l1)\n",
    "        elif l2:\n",
    "            kernel_regu = keras.regularizers.L2(l2=l2)\n",
    "        else: # Do not use regularization\n",
    "            kernel_regu = None\n",
    "            \n",
    "        # Choose activation function\n",
    "        hp_activation = hp.Choice('dense_activation', values=self.activation_func_hidden_layer)\n",
    "        if hp_activation == 'leaky_relu': # Leaky Relu does not have an alias in keras and the values list for choice can onl contain one type\n",
    "            hp_activation = keras.layers.LeakyReLU(alpha=0.1)\n",
    "            \n",
    "        for i in range(hp.Int('num_layers', 2, self.max_num_of_hidd_layer)):\n",
    "            model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                                min_value=self.min_num_of_neuron_per_layer,\n",
    "                                                max_value=self.max_num_of_neuron_per_layer,\n",
    "                                                step=2),\n",
    "                                    activation=hp_activation,\n",
    "                                    kernel_initializer='he_normal',\n",
    "                                    bias_initializer='zeros',\n",
    "                                    kernel_regularizer=kernel_regu ))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.Dropout(rate=hp.Choice(\n",
    "                                            'dropout_rate',\n",
    "                                            values=[0.3, 0.4, 0.5]\n",
    "                                            )))\n",
    "        model.add(layers.Dense(5, activation='softmax')) #TODO output variable\n",
    "\n",
    "        #Compile\n",
    "        opt = keras.optimizers.Adam(hp.Float(\n",
    "                                'learning_rate',\n",
    "                                min_value=self.learning_rate[0],\n",
    "                                max_value=self.learning_rate[1],\n",
    "                                sampling='LOG',\n",
    "                                default=1e-3)\n",
    "                                )\n",
    "        metrics = [keras.metrics.CategoricalAccuracy()]\n",
    "        loss_func = keras.losses.CategoricalCrossentropy()\n",
    "        model.compile(loss=loss_func, optimizer=opt, metrics=metrics)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacffca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp_hyperparameter_search(df, hyperparams_search:dict, load_search:Optional[str]=None):\n",
    "    '''\n",
    "    Search the best hyperparameters for our MLP\n",
    "    '''\n",
    "\n",
    "    # Create a list of users\n",
    "    user_list = [0, 1, 2, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Number of users to be used for the test set\n",
    "    num_user_test = 3\n",
    "    \n",
    "    #create folder\n",
    "    search_path = join('model/hp_search', 'mlp')\n",
    "\n",
    "    #Convert input data\n",
    "    X_train, y_train, X_test, y_test, user_list = custom_cv_approach(df, user_list, num_user_test=num_user_test)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # Before one hot encoding, class has to start at 0\n",
    "    y_train = y_train-1\n",
    "    y_test = y_test-1\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    #Transform to Dataset\n",
    "    X_train_stream_labeled = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    X_test_stream_labeled = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    # Shuffle Data\n",
    "    X_train_stream_labeled = X_train_stream_labeled.shuffle(buffer_size=df.shape[0])\n",
    "    X_test_stream_labeled = X_test_stream_labeled.shuffle(buffer_size=df.shape[0])\n",
    "    #create batches\n",
    "    X_train_stream_labeled = X_train_stream_labeled.batch(hyperparams_search['batch_size'])\n",
    "    X_test_stream_labeled = X_test_stream_labeled.batch(hyperparams_search['batch_size'])\n",
    "\n",
    "    #build model\n",
    "    input_shape = (X_train.shape[1], )\n",
    "    output_shape = len(np.unique(y_train))\n",
    "    \n",
    "    mlp_model = Classification_MLP_tuner('mlp', input_shape,\n",
    "                                         output_shape, learning_rate=hyperparams_search['learning_rate'],\n",
    "                                         max_num_of_hidd_layer=hyperparams_search['max_num_of_hidd_layer'], \n",
    "                                         max_num_of_neuron_per_layer=hyperparams_search['max_num_of_neuron_per_layer'], \n",
    "                                         min_num_of_neuron_per_layer=hyperparams_search['min_num_of_neuron_per_layer'], \n",
    "                                         activation_func_hidden_layer=hyperparams_search['activation_func_hidden_layer'],\n",
    "                                        weight_regularisation_l1 = hyperparams_search['weight_regularisation_l1'],\n",
    "                                        weight_regularisation_l2 = hyperparams_search['weight_regularisation_l2'])\n",
    "    \n",
    "    \n",
    "    hp_callback = random.choice(hyperparams_search['early_or_plateau'])\n",
    "    callbacks = []\n",
    "    #REGULARIZATION\n",
    "    if hp_callback == 'early':\n",
    "        early_stop_callback = keras.callbacks.EarlyStopping(monitor=\"val_categorical_accuracy\", min_delta=1e-3, patience=5, verbose=1)\n",
    "        callbacks.append(early_stop_callback)\n",
    "    if hp_callback == 'plateau':\n",
    "        reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2,\n",
    "                                       patience=4, min_lr=0.00001,\n",
    "                                       min_delta=0.008)\n",
    "        callbacks.append(reduce_lr_callback)\n",
    "\n",
    "    # Create name with timestamp\n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b-%d-%Y_%H%M', t)\n",
    "\n",
    "    #train\n",
    "    if load_search is None:\n",
    "        tuner_search = keras_tuner.RandomSearch(mlp_model, objective='val_categorical_accuracy', max_trials=hyperparams_search['max_trials'], project_name=join(search_path, f'prj_{timestamp}_mlp'))\n",
    "        tuner_search.search(X_train_stream_labeled, \n",
    "                            epochs=hyperparams_search['epochs'], \n",
    "                            validation_data=X_test_stream_labeled,\n",
    "                            callbacks=callbacks)\n",
    "\n",
    "        # Not required anymore, search can be easily restored from search path!\n",
    "        '''\n",
    "        pickle_path = join(search_path, f'mlp_search_{timestamp}.pkl')\n",
    "        with open(pickle_path, 'wb') as handle:\n",
    "            pickle.dump(tuner_search, handle)\n",
    "        '''\n",
    "    else:\n",
    "        tuner_search = keras_tuner.RandomSearch(mlp_model, objective='val_categorical_accuracy', max_trials=hyperparams_search['max_trials'], overwrite=False, project_name=load_search)\n",
    "        return tuner_search\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6284fb8",
   "metadata": {},
   "source": [
    "There are different hyperparameter we want to analyze the impact of:\n",
    "* Number of layers\n",
    "* Number of nodes\n",
    "* Activation function\n",
    "* Optimizer\n",
    "* Learning rate\n",
    "* Number of epochs -> we will use an Early stopping Callback\n",
    "* Batch size\n",
    "* Adding weight regularization\n",
    "* Adding dropout\n",
    "\n",
    "Due to the amount of hyperparameter combination:<br>\n",
    "a univariate hyperparemeter search is not target-oriented, because the parameters influence each other.\n",
    "we have a multivariate problem. The number of possible value combinations, however, cannot (at least with the hardware available to me) all be examined. A possible approach is to randomly select parameter sets, i.e. to perform a random search.\n",
    "\n",
    "Procedure:\n",
    "* for number of examined hyperparmeter sets (e.g. here 150)\n",
    "    * Define hyperparameter space (some parameters continuous, others discrete)\n",
    "    * draw random combination of values\n",
    "    * for #CV_folds:\n",
    "        * select n users for validation set\n",
    "        * use rest for training with the randomly drawn hyperparameter combination\n",
    "\n",
    "Finally, analyze which hyperparmeter combination has the best validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0916ce",
   "metadata": {},
   "source": [
    "First perform a random search in hyperparameter space with the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2c16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper param dict\n",
    "hyperparams_search = {'max_trials': 500,\n",
    "                          'epochs': 50,\n",
    "                          'batch_size': 32,\n",
    "                          'activation_func_hidden_layer': ['relu', 'leaky_relu', 'elu'], #['tanh', 'sigmoid', 'elu', 'relu']\n",
    "                          'min_num_of_neuron_per_layer': 2,\n",
    "                          'max_num_of_neuron_per_layer': 16,\n",
    "                          'max_num_of_hidd_layer': 5,\n",
    "                          'learning_rate': [5e-5, 1e-2],\n",
    "                          'weight_regularisation_l1': [0.000001, 0.1],\n",
    "                          'weight_regularisation_l2': [0.000001, 0.1],\n",
    "                          'early_or_plateau': ['early', 'plateau']\n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f9a2f-f64f-4e88-8aeb-8079160fe160",
   "metadata": {},
   "source": [
    "Reference time: 9hours to try 150 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1fb1d-7224-4490-96fa-f36ad1a867c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d083e-6cc0-47be-968a-3d88bca3d390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_hyperparameter_search(df_raw, hyperparams_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7419b9e-eb27-4fb1-ae06-57514e4a1441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end = datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a2f67",
   "metadata": {},
   "source": [
    "We should always make sure that we can restore or load important evvaluation data, whole models or search results.<br>\n",
    "Coming back after 2 days of random hyperparameter search only to find out that the kernel crashed or restarted at any point in time is suboptimal<br>\n",
    "Simply relying on the fact that the jupyter kernel will store the python objects is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34fc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_path = '/home/josh/dde1_hand_motion/model/hp_search/mlp/prj_Mar-09-2023_2332_mlp_raw'\n",
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    print('*'*50)\n",
    "    print('Reload tuner search')\n",
    "    tuner_search = mlp_hyperparameter_search(df_raw, hyperparams_search,load_search = project_path)\n",
    "    print(tuner_search.results_summary(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd33042",
   "metadata": {},
   "source": [
    "Now perform the same search with the extracted data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28618180",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_hyperparameter_search(df_aggregate, hyperparams_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc4cbe",
   "metadata": {},
   "source": [
    "As I said before:<br>\n",
    "always store the search object, in case we have to restore it after the kernel crashed. <br>\n",
    "Here we can simply load the Search Object where we used the extracted features (even if we come back after several days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c76e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import re\n",
    "\n",
    "project_path = '/home/josh/dde1_hand_motion/model/hp_search/mlp/prj_Mar-21-2023_1111_mlp'\n",
    "load_model = True\n",
    "\n",
    "# For some strange reason the creator of keras tuner do not return the values in results_summary() function\n",
    "# instead they print out the information directly. Therfore we have to catch the output\n",
    "df_search = pd.DataFrame()\n",
    "if load_model:\n",
    "    tuner_search = mlp_hyperparameter_search(df_aggregate, hyperparams_search,load_search = project_path)\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        tuner_search.results_summary(4)\n",
    "    out = f.getvalue()\n",
    "    score_list = [s for s in out.splitlines() if \"Score\" in s]\n",
    "    # Extract float from long string\n",
    "    scores = [round(float(re.findall(\"\\d+\\.\\d+\", sub_string)[0]), 5) for sub_string in score_list]\n",
    "    for idx, hp_set in enumerate(tuner_search.get_best_hyperparameters(4)):\n",
    "        values_to_append = hp_set.values\n",
    "        values_to_append['val_accuracy'] = scores[idx]\n",
    "        df_search = df_search.append(hp_set.values, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69225314-8e15-4ffe-b5c7-8c5e40ad2ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02dbbb-843b-48f4-b326-9744e3a25abd",
   "metadata": {},
   "source": [
    "be careful!: even for units_{i} values where a trial has no real value (because num_layers is too small), keras tuner is saving a value for!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9113c",
   "metadata": {},
   "source": [
    "# 6. Evaluation of the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120227c-6716-4d32-9f35-5c66e81b9efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  All Models/Overall Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbdf18-ff20-445a-bbf1-b716cb904d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_copy = df_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed0b60-4024-46ee-92a2-bdc95d31acc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_models = [\"rfc_extract\", \"lg_extract\", \"svm_raw\", \"mlp_raw\", \"mlp_extract\", \"mlp_pca\", \"mlp_extract_tuned\"]\n",
    "df_comparison = df_results.loc[df_results['model'].isin(select_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd51e2-1f3d-400d-bd57-77ef3daa0c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results\n",
    "plot_model_comparison(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631750c-8837-49c2-9765-72138570cbbe",
   "metadata": {},
   "source": [
    "In a direct comparison, we see that the neural networks after a fine tuning has comparable performance with conventional models. However, considering the time needed to train the neural networks and to do fine tuning, we have to consider whether the effort is worth is. Of course you can only say that after you have trained the models. For the vast majority of cases, neural networks are superior. But for this, certain conditions must be met: the data quality must be good and sufficient data points must be available. Because my data set consists of data points from different users, it is possible that the data points of a class are not similar enough. But for the model to generalize across multiple users, too few users are represented in the dataset! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856f41d-c408-4ea5-9a61-5d4b09fe41cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 7. Lessons Learnt and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60002ae0",
   "metadata": {},
   "source": [
    "tell us what you found and what you learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca424f7-525f-4a56-80c4-bcc82d339c78",
   "metadata": {},
   "source": [
    "* Do not blindly trust and reuse code you find on Kaggle, Medium, etc. There might be conceptual errors.\n",
    "    * Always carefully review any code you plan to use in your project, regardless of where you found it.\n",
    "    * Test the code thoroughly before incorporating it into your project, especially if you're not familiar with the author's background or expertise.\n",
    "\n",
    "* One of the hardest parts for me was deciding what results to keep (if an approach failed, should I keep it just to show that I tried it?). Finding the balance between documenting results and getting rid of unnecessary code was hard for me.\n",
    "    * I focused on keeping only the results that are relevant and meaningful to my project's goals and objectives.\n",
    "    * Be willing to let go of approaches or results that don't contribute to your project's overall success or narrative.\n",
    "\n",
    "* I'm used to programming in .py files/scripts. Iteratively changing code until I have the end result. In this project, the goal is also to show the \"way\" how we reached our goals.\n",
    "\n",
    "* It was a fun project where I learned a lot.\n",
    "    * There is no comparable lecture in which you are allowed to work practically on your own project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a71aa70298631feedc494dbad6cb00b706fca5b11f5a9d39eafcd631df241c69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
