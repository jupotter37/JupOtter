{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57935587",
   "metadata": {},
   "source": [
    "# 7. References\n",
    "Note: we used the ACM citation style for the referencing.\n",
    "\n",
    "[1] Arielle O'Shea (2023). “What is a stock?”. Nerd wallet. (Mar 31, 2023). Retrieved Jun 14, 2024 from https://www.nerdwallet.com/article/investing/what-is-a-stock#:~:text=A%20stock%20is%20a%20security,increases%20in%20value%20as%20well   \n",
    "\n",
    "[2] “Algorithmic Trading Market - Growth, Trends, COVID-19 Impact, and Forecasts (2022 - 2027)”. Yahoo finance. (Mar 18, 2022). Retrieved Jun 14, 2024 from\n",
    "https://finance.yahoo.com/news/algorithmic-trading-market-growth-trends-112400870.html \n",
    "\n",
    "[3] ADAM HAYES, Gordon Scott, DAVID RUBIN (2024). investopedia.\n",
    " “Position Definition—Short and Long Positions in Financial Markets”. (May 31, 2024).  Retrieved Jun 14, 2024 from  https://www.investopedia.com/terms/p/position.asp#:~:text=our%20editorial%20policies-,What%20Is%20a%20Position%3F,short%20securities%20with%20bearish%20intent.  \n",
    "\n",
    "[4] Nuti, Giuseppe (11/2011). \"Algorithmic Trading\". Computer (Long Beach, Calif.) (0018-9162), 44 (11), p. 61, Retrieved April 21,2024 from  https://ieeexplore.ieee.org/document/5696713 \n",
    "\n",
    "[5] Alzaman, Chaher (03/2024). \"Deep learning in stock portfolio selection and predictions\". Expert systems with applications (0957-4174), 237 , p. 121404, Retrieved April 21,2024 from https://www.sciencedirect.com/science/article/pii/S0957417423019061  \n",
    "\n",
    "[6] B L, Shilpa (2023). \"Deep Learning Models for Stock Market Prediction Using Optimization Approach\" in 2023 International Conference on Network, Multimedia and Information Technology (NMITCON) (979-83-503-0083-3), (p. 1), Retrieved April 21,2024 from \n",
    "https://ieeexplore.ieee.org/document/10275882  \n",
    "\n",
    "[7] IBRAHIM KARATAS (2023). “Machine Learning for Stocks Trading”. Kaggle. (2023). Retrieved April 21,2024 from https://www.kaggle.com/code/ibrahimkaratas/machine-learning-for-stocks-trading/notebook  \n",
    "\n",
    "[8] François Chollet (11, 2017), “Deep Learning with Python”, chapters (6, 7), Manning Publications Co, Retrieved May 14, 2024 from https://www.manning.com/books/deep-learning-with-python  \n",
    "\n",
    "[9] S&P 500 ETF Components. Slickcharts. (2024). Retrieved June 6, 2024 from https://www.slickcharts.com/sp500  \n",
    "\n",
    "[10] pandas.DataFrame.stack. Pandas. (2024). Retrieved May 16, 2024 from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html  \n",
    "\n",
    "[11] pandas.DataFrame.xs. Pandas. (2024). Retrieved May 16, 2024 from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html  \n",
    "\n",
    "[12] ADAM HAYES,  ERIC ESTEVEZ, Yarilet Perez (2023). “Behavioral Finance”. “What Is Behavioral Finance?”. investopedia. (Dec 19, 2023). Retrieved Jun 14, 2024 from https://www.investopedia.com/terms/b/behavioralfinance.asp  \n",
    "\n",
    "[13] mpariente (2017). “How to reshape input for keras LSTM?”.  stack overflow. (Dec 22, 2017). Retrieved May 20, 2024 from https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4  \n",
    "\n",
    "[14] tf.keras.utils.to_categorical. TensorFlow. (2024). Retrieved May 26, 2024 from https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical  \n",
    "\n",
    "[15] ericheindl (2021), “classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets”. stack overflow. (May 11, 2021). Retrieved Jun 15, 2024 from https://stackoverflow.com/questions/48987959/classification-metrics-cant-handle-a-mix-of-continuous-multioutput-and-multi-la  \n",
    "\n",
    "[16] sklearn.metrics.ConfusionMatrixDisplay. Sci-kit learn. (2024). Retrieved Jun 15, 2024 from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html  \n",
    "\n",
    "[17] François Chollet (11, 2017), “Deep Learning with Python”, chapters (6), Listing 6.38. Plotting results, Manning Publications Co, Retrieved Jun 15, 2024 from https://www.manning.com/books/deep-learning-with-python \n",
    "\n",
    "[18] Dataset obtained using yfinance: https://pypi.org/project/yfinance/ (\"\"\" it's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes. \"\"\")\n",
    "\n",
    "[19] ecatanzani (2022), \"SciKeras - RandomizedSearchCV for best hyper-parameters\". stack overflow. (May 26, 2022). Retrieved Jul 20, 2024 from  https://stackoverflow.com/questions/72392579/scikeras-randomizedsearchcv-for-best-hyper-parameters \n",
    "\n",
    "[20] Scikeras tutorials, Basic usage. \"7.1 Special prefixes\". SciKeras’s documentation (2024). Retrieved jul 21, 2024 from https://adriangb.com/scikeras/stable/notebooks/Basic_Usage.html\n",
    "\n",
    "\n",
    "[21] mins(2021), pixelistik (2023), \"turn warning off in a cell jupyter notebook\". stack overflow. (Jan 20, 2021). Retrieved Jul 22, 2024 from https://stackoverflow.com/questions/40105796/turn-warning-off-in-a-cell-jupyter-notebook\n",
    "\n",
    "[22] keras.callbacks.EarlyStopping. Keras. (2024). Retrieved Jul 25, 2024 from https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "[23] keras.callbacks.ReduceLROnPlateau. Keras. (2024). Retrieved Jul 25, 2024 from https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "\n",
    "[24] François Chollet (11, 2017), “Deep Learning with Python”, chapters (7), 7.2. Inspecting and monitoring deep-learning models using Keras callba- acks and TensorBoard, Manning Publications Co, Retrieved Jun 15, 2024 from https://www.manning.com/books/deep-learning-with-python \n",
    "\n",
    "[25] pandas.DataFrame.xs. Pandas. (2024). Retrieved Jul 22, 2024 from: https://www.statology.org/pandas-change-column-names-to-lowercase/ \n",
    "\n",
    "[26] Introduction to the Keras Tuner. TensorFlow. (2024). Retrieved Jul 26, 2024 from: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "[27] keras_tuner.Hyperband. Keras. (2024). Retrieved Jul 26, 2024 from: https://keras.io/api/keras_tuner/tuners/hyperband/ \n",
    "\n",
    "[28] The base Tuner class. Keras. (2024). Retrieved Jul 26, 2024 from: https://keras.io/api/keras_tuner/tuners/base_tuner/\n",
    "\n",
    "[29] Save and load models. TensorFlow. (2024). Retrieved Jul 26, 2024 from: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "\n",
    "[30] François Chollet (11, 2017). “Deep Learning with Python”. chapters (6), 6.2. Understanding recurrent neural networks, Manning Publications Co, Retrieved Jun 15, 2024 from https://www.manning.com/books/deep-learning-with-python \n",
    "\n",
    "[31] geeksforgeeks. (Jun 10, 2024). \"What is LSTM – Long Short Term Memory?\". Retrieved Aug 10, 2024 from: What is LSTM – Long Short Term Memory?, https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/\n",
    "\n",
    "[32] geeksforgeeks. (Mar 02, 2023). \"Gated Recurrent Unit Networks\". Retrieved Aug 10, 2024 from: https://www.geeksforgeeks.org/gated-recurrent-unit-networks/\n",
    "\n",
    "[33] JulieProst (2020). keras-tuner-tutorial/hypermodels.py. Retrieved Aug 20, 2024 from: https://github.com/JulieProst/keras-tuner-tutorial/blob/master/hypermodels.py \n",
    "\n",
    "[34] omalleyt12 (2019). \"How to tune the number of epochs and batch_size?\". Retrieved Aug 20, 2024 from: https://github.com/keras-team/keras-tuner/issues/122\n",
    "\n",
    "[35] The base Layer class. Keras. (2024). Retrieved Aug 23, 2024 from: https://keras.io/2.15/api/layers/base_layer/\n",
    "\n",
    "[36] François Chollet (11, 2017). “Deep Learning with Python”. chapters (7), Inception models. Manning Publications Co, Retrieved Jun 15, 2024 from https://www.manning.com/books/deep-learning-with-python \n",
    "\n",
    "[37] Kolasniwash (2020). \"Multi-Variate Time Series Forecasting Tensorflow\". CNN and LSTM Stacked. Kaggle. Retrieved September 9, 2024 from https://www.kaggle.com/code/nicholasjhana/multi-variate-time-series-forecasting-tensorflow#CNN-and-LSTM-Stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db4fd6",
   "metadata": {},
   "source": [
    "# 8. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969b230",
   "metadata": {},
   "source": [
    "(1) The implementation of add_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32808b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dataframe and create 'next_close' column based on its 'close' column\n",
    "def get_next_close(_df):\n",
    "    \n",
    "    # create the 'next_close' column to be equal to the next closing price\n",
    "    # this can be accomplished easily by shifting the close column backward by 1\n",
    "    return _df['close'].shift(-1)\n",
    "\n",
    "# create a function that returns 1 if the the next closing price is higher than current closing price and 0 otherwise.\n",
    "def assign_trend(row):\n",
    "    if row['next_close'] > row['close']:\n",
    "        return 1\n",
    "    elif row['next_close'] < row['close']:\n",
    "        return 0\n",
    "    else: # if the next value is missing then return NaN\n",
    "        return np.nan\n",
    "\n",
    "# create a function that add the target columns to the dataframe\n",
    "def add_targets(_df):\n",
    "    \n",
    "    # add the next_close column to the dataframe\n",
    "    _df['next_close'] = get_next_close(_df)\n",
    "    \n",
    "    # add the trend column to the dataframe\n",
    "    _df['trend'] = _df.apply(assign_trend, axis=1)\n",
    "    \n",
    "    # drop the NaN values\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    # fix the 'trend' data type to be int\n",
    "    _df = _df.astype({'trend': int})\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382241c",
   "metadata": {},
   "source": [
    "(2) The implmentation of add_technical_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0563b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the time being let's create a function that add all the technical indicators we want to a df\n",
    "def add_technical_indicators(_df):\n",
    "    \n",
    "    ##### indicators based on the closing price ##### index range: 6:36\n",
    "    # apply macd on the close column in a df and add it to the dataframe    \n",
    "    macd = ta.macd(_df['close'])\n",
    "    # The MACD (Moving Average Convergence/Divergence) is a popular indicator to that is used to identify a trend\n",
    "    _df.insert(6, 'macd', macd.iloc[:,0])\n",
    "    # Histogram is the difference of MACD and Signal\n",
    "    _df.insert(7, 'macd_histogram', macd.iloc[:,1])\n",
    "    # Signal is an EMA (exponential moving average) of MACD\n",
    "    _df.insert(8, 'macd_signal', macd.iloc[:,2])\n",
    "    \n",
    "    # apply RSI on the Close column in a df and add it to the dataframe    \n",
    "    # RSI (Relative Strength Index) is popular momentum oscillator. Measures velocity and magnitude a trend\n",
    "    rsi = ta.rsi(_df['close'])\n",
    "    _df.insert(9, 'rsi', rsi)\n",
    "\n",
    "    # apply SMA on the Close column in a df and add it to the dataframe    \n",
    "    # SMA (Simple Moving Average) is the classic moving average that is the equally weighted average over n periods.\n",
    "    sma = ta.sma(_df['close'])\n",
    "    _df.insert(10, 'sma', sma)\n",
    "\n",
    "    # apply EMA on the Close column in a df and add it to the dataframe    \n",
    "    # EMA (Exponential Moving Average). The weights are determined by alpha which is proportional to it's length.\n",
    "    ema = ta.ema(_df['close'])\n",
    "    _df.insert(11, 'ema', ema)\n",
    "    \n",
    "    ######## repeat the same proccess for all the technical indicators we want to include ##########\n",
    "    # bbands: A popular volatility indicator by John Bollinger.\n",
    "    bbands = ta.bbands(_df['close'])\n",
    "    _df.insert(12, 'bbands_lower', bbands.iloc[:,0])\n",
    "    _df.insert(13, 'bbands_mid', bbands.iloc[:,1])\n",
    "    _df.insert(14, 'bbands_upper', bbands.iloc[:,2])\n",
    "    _df.insert(15, 'bbands_bandwidth', bbands.iloc[:,3])\n",
    "    _df.insert(16, 'bbands_percent', bbands.iloc[:,4])\n",
    "    \n",
    "    # dema: The Double Exponential Moving Average attempts to a smoother average with less lag than the normal Exponential Moving Average (EMA).\n",
    "    dema = ta.dema(_df['close'])\n",
    "    _df.insert(17, 'dema', dema)\n",
    "    \n",
    "    # tema: A less laggy Exponential Moving Average.\n",
    "    tema = ta.tema(_df['close'])\n",
    "    _df.insert(18, 'tema', tema)\n",
    "\n",
    "    # roc: Rate of Change is an indicator is also referred to as Momentum. It is a pure momentum oscillator that measures the percent change in price with the previous price 'n' (or length) periods ago.\n",
    "    roc = ta.roc(_df['close'])\n",
    "    _df.insert(19, 'roc', roc)\n",
    "    \n",
    "    # mom: Momentum is an indicator used to measure a security's speed (or strength) of movement.  Or simply the change in price.\n",
    "    mom = ta.mom(_df['close'])\n",
    "    _df.insert(20, 'mom', mom)\n",
    "    \n",
    "    # kama: Developed by Perry Kaufman, Kaufman's Adaptive Moving Average (KAMA) is a moving average designed to account for market noise or volatility. KAMA will closely follow prices when the price swings are relatively small and the noise is low. KAMA will adjust when the price swings widen and follow prices from a greater distance. This trend-following indicator can be used to identify the overall trend, time turning points and filter price movements.\n",
    "    kama = ta.kama(_df['close'])\n",
    "    _df.insert(21, 'kama', kama)\n",
    "                       \n",
    "    # trix: is a momentum oscillator to identify divergences.\n",
    "    trix = ta.trix(_df['close'])\n",
    "    _df.insert(22, 'trix', trix.iloc[:,0])\n",
    "    _df.insert(23, 'trixs', trix.iloc[:,1])\n",
    "    \n",
    "    # hma: The Hull Exponential Moving Average attempts to reduce or remove lag in moving averages.\n",
    "    hma = ta.hma(_df['close'])\n",
    "    _df.insert(24, 'hma', hma)\n",
    "    \n",
    "    # alma: The ALMA moving average uses the curve of the Normal (Gauss) distribution, which can be shifted from 0 to 1. This allows regulating the smoothness and high sensitivity of the indicator. Sigma is another parameter that is responsible for the shape of the curve coefficients. This moving average reduces lag of the data in conjunction with smoothing to reduce noise.\n",
    "    alma = ta.alma(_df['close'])\n",
    "    _df.insert(25, 'alma', alma)\n",
    "    \n",
    "    # apo: The Absolute Price Oscillator is an indicator used to measure a security's momentum.  It is simply the difference of two Exponential Moving Averages (EMA) of two different periods. Note: APO and MACD lines are equivalent.\n",
    "    apo = ta.apo(_df['close'])\n",
    "    _df.insert(26, 'apo', apo)\n",
    "    \n",
    "    # cfo: The Forecast Oscillator calculates the percentage difference between the actualprice and the Time Series Forecast (the endpoint of a linear regression line).\n",
    "    cfo = ta.cfo(_df['close'])\n",
    "    _df.insert(27, 'cfo', cfo)\n",
    "    \n",
    "    # cg: The Center of Gravity Indicator by John Ehlers attempts to identify turning points while exhibiting zero lag and smoothing.\n",
    "    cg = ta.cg(_df['close'])\n",
    "    _df.insert(28, 'cg', cg)\n",
    "    \n",
    "    # cmo: Attempts to capture the momentum of an asset with overbought at 50 and oversold at -50.\n",
    "    cmo = ta.cmo(_df['close'])\n",
    "    _df.insert(29, 'cmo', cmo)\n",
    "    \n",
    "    # coppock: Coppock Curve (originally called the \"Trendex Model\") is a momentum indicator is designed for use on a monthly time scale.  Although designed for monthly use, a daily calculation over the same period can be made, converting the periods to 294-day and 231-day rate of changes, and a 210-day weighted moving average.\n",
    "    coppock = ta.coppock(_df['close'])\n",
    "    _df.insert(30, 'coppock', coppock)\n",
    "    \n",
    "    # cti: The Correlation Trend Indicator is an oscillator created by John Ehler in 2020. It assigns a value depending on how close prices in that range are to following a positively- or negatively-sloping straight line. Values range from -1 to 1. This is a wrapper for ta.linreg(close, r=True).\n",
    "    cti = ta.cti(_df['close'])\n",
    "    _df.insert(31, 'cti', cti)\n",
    "    \n",
    "    # decay: Creates a decay moving forward from prior signals like crosses. The default is \"linear\". Exponential is optional as \"exponential\" or \"exp\".\n",
    "    decay = ta.decay(_df['close'])\n",
    "    _df.insert(32, 'decay', decay)\n",
    "    \n",
    "    # decreasing: Returns True if the series is decreasing over a period, False otherwise. If the kwarg 'strict' is True, it returns True if it is continuously decreasing over the period. When using the kwarg 'asint', then it returns 1 for True or 0 for False.\n",
    "    decreasing = ta.decreasing(_df['close'])\n",
    "    _df.insert(33, 'decreasing', decreasing)\n",
    "    \n",
    "    # ebsw: This indicator measures market cycles and uses a low pass filter to remove noise. Its output is bound signal between -1 and 1 and the maximum length of a detected trend is limited by its length input.\n",
    "    ebsw = ta.ebsw(_df['close'])\n",
    "    _df.insert(34, 'ebsw', ebsw)\n",
    "    \n",
    "    # entropy: Introduced by Claude Shannon in 1948, entropy measures the unpredictability of the data, or equivalently, of its average information. A die has higher entropy (p=1/6) versus a coin (p=1/2).\n",
    "    entropy = ta.entropy(_df['close'])\n",
    "    _df.insert(35, 'entropy', entropy)\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the high and lows of the price ##### range= 36:67\n",
    "    \n",
    "    # aberration: A volatility indicator\n",
    "    aberration = ta.aberration(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(36, 'aberration_zg', aberration.iloc[:,0])\n",
    "    _df.insert(37, 'aberration_sg', aberration.iloc[:,1])\n",
    "    _df.insert(38, 'aberration_xg', aberration.iloc[:,2])\n",
    "    _df.insert(39, 'aberration_atr', aberration.iloc[:,3])\n",
    "    \n",
    "    # adx:  Average Directional Movement is meant to quantify trend strength by measuring the amount of movement in a single direction.    \n",
    "    adx = ta.adx(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(40, 'adx_adx', adx.iloc[:,0])\n",
    "    _df.insert(41, 'adx_dmp', adx.iloc[:,1])\n",
    "    _df.insert(42, 'adx_dmn', adx.iloc[:,2])\n",
    "\n",
    "    # atr: Averge True Range is used to measure volatility, especially volatility caused by gaps or limit moves.\n",
    "    atr = ta.atr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(43, 'atr', atr)\n",
    "    \n",
    "    # stoch: The Stochastic Oscillator (STOCH) was developed by George Lane in the 1950's. He believed this indicator was a good way to measure momentum because changes in momentum precede changes in price.\n",
    "    stoch = ta.stoch(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(44, 'stoch_k', stoch.iloc[:,0])\n",
    "    _df.insert(45, 'stoch_d', stoch.iloc[:,1])\n",
    "    \n",
    "    # Supertrend: is an overlap indicator. It is used to help identify trend direction, setting stop loss, identify support and resistance, and/or generate buy & sell signals.\n",
    "    supertrend = ta.supertrend(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(46, 'supertrend_trend', supertrend.iloc[:,0])\n",
    "    _df.insert(47, 'supertrend_direction', supertrend.iloc[:,1])\n",
    "    \n",
    "    # cci: Commodity Channel Index is a momentum oscillator used to primarily identify overbought and oversold levels relative to a mean.\n",
    "    cci = ta.cci(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(48, 'cci', cci)\n",
    "    \n",
    "    # aroon: attempts to identify if a security is trending and how strong.\n",
    "    aroon = ta.aroon(_df['high'], _df['low'])\n",
    "    _df.insert(49, 'aroon_up', aroon.iloc[:,0])\n",
    "    _df.insert(50, 'aroon_down', aroon.iloc[:,1])\n",
    "    _df.insert(51, 'aroon_osc', aroon.iloc[:,2])\n",
    "    \n",
    "    # natr: Normalized Average True Range attempt to normalize the average true range.\n",
    "    natr = ta.natr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(52, 'natr', natr)\n",
    "    \n",
    "    # William's Percent R is a momentum oscillator similar to the RSI that attempts to identify overbought and oversold conditions.\n",
    "    willr = ta.willr(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(53, 'willr', willr)\n",
    "    \n",
    "    # vortex: Two oscillators that capture positive and negative trend movement.\n",
    "    vortex = ta.vortex(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(54, 'vortex_vip', vortex.iloc[:,0])\n",
    "    _df.insert(55, 'vortex_vim', vortex.iloc[:,1])\n",
    "    \n",
    "    # hlc3: the average of high, low, and close prices\n",
    "    hlc3 = ta.hlc3(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(56, 'hlc3', hlc3)\n",
    "    \n",
    "    # ohlc4: the average of open, high, low, and close prices\n",
    "    ohlc4 = ta.ohlc4(_df['open'], _df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(57, 'ohlc4', ohlc4)\n",
    "    \n",
    "    # accbands: Acceleration Bands created by Price Headley plots upper and lower envelope bands around a simple moving average.\n",
    "    accbands = ta.accbands(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(58, 'accbands_lower', accbands.iloc[:,0])\n",
    "    _df.insert(59, 'accbands_mid', accbands.iloc[:,1])\n",
    "    _df.insert(60, 'accbands_upper', accbands.iloc[:,2])\n",
    "\n",
    "    # chop: The Choppiness Index was created by Australian commodity trader E.W. Dreiss and is designed to determine if the market is choppy (trading sideways) or not choppy (trading within a trend in either direction). Values closer to 100 implies the underlying is choppier whereas values closer to 0 implies the underlying is trending.\n",
    "    chop = ta.chop(_df['high'], _df['low'], _df['close'])\n",
    "    _df.insert(61, 'chop', chop)\n",
    "    \n",
    "    # dm: The Directional Movement was developed by J. Welles Wilder in 1978 attempts to determine which direction the price of an asset is moving. It compares prior highs and lows to yield to two series +DM and -DM.\n",
    "    dm = ta.dm(_df['high'], _df['low'])\n",
    "    _df.insert(62, 'dm_positive', dm.iloc[:,0])\n",
    "    _df.insert(63, 'dm_negative', dm.iloc[:,1])\n",
    "\n",
    "    # donchian: Donchian Channels are used to measure volatility, similar to Bollinger Bands and Keltner Channels.\n",
    "    donchian = ta.donchian(_df['high'], _df['low'])\n",
    "    _df.insert(64, 'donchian_lower', donchian.iloc[:,0])\n",
    "    _df.insert(65, 'donchian_mid', donchian.iloc[:,1])\n",
    "    _df.insert(66, 'donchian_upper', donchian.iloc[:,2])\n",
    "    \n",
    "    \n",
    "    ##### indicators based on the volume of the price ##### range= 67:72\n",
    "    \n",
    "    # obv: On Balance Volume is a cumulative indicator to measure buying and selling pressure.\n",
    "    obv = ta.obv(_df['close'], _df['volume'])\n",
    "    _df.insert(67, 'obv', obv)\n",
    "    \n",
    "    # vwma: Volume Weighted Moving Average.\n",
    "    vwma = ta.vwma(_df['close'], _df['volume'])\n",
    "    _df.insert(68, 'vwma', vwma)\n",
    "    \n",
    "    # adosc: Accumulation/Distribution Oscillator indicator utilizes Accumulation/Distribution and treats it similarily to MACD or APO.\n",
    "    adosc = ta.adosc(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(69, 'adosc', adosc)\n",
    "    \n",
    "    # cmf: Chailin Money Flow measures the amount of money flow volume over a specific period in conjunction with Accumulation/Distribution.\n",
    "    cmf = ta.cmf(_df['high'], _df['low'], _df['close'], _df['volume'])\n",
    "    _df.insert(70, 'cmf', cmf)\n",
    "    \n",
    "    # efi: Elder's Force Index measures the power behind a price movement using price and volume as well as potential reversals and price corrections.\n",
    "    efi = ta.efi(_df['close'], _df['volume'])\n",
    "    _df.insert(71, 'efi', efi)\n",
    "\n",
    "\n",
    "    #### we can add more technical indicators if we want using the same process ####\n",
    "    \n",
    "    # remove the NaN values and return the new dataframe\n",
    "    _df.dropna(inplace=True)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7376196",
   "metadata": {},
   "source": [
    "(3) The implementation of calculate_data_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf6ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to computes the ratio of trend = 1 for each individual dataframe in a dictionary \n",
    "def calculate_data_balance(_dfs):\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # get the number of values where trend = 1\n",
    "        trend_1 = _dfs[symbol]['trend'].value_counts()[1]\n",
    "        \n",
    "        # get the total number of rows in the dataframe\n",
    "        row_num = _dfs[symbol].shape[0]\n",
    "        \n",
    "        # percentage of 'trend up' to the whole column\n",
    "        trend_1_ratio = round(trend_1/row_num, 2)\n",
    "        \n",
    "        # print the ratio to the screen\n",
    "        print(f\"The Trend up ratio of {symbol} is {trend_1_ratio} for {row_num} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c05cf",
   "metadata": {},
   "source": [
    "(4) The implementation of calculate_common_sense_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e8970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_common_sense_baseline(_dfs):\n",
    "    \n",
    "    # iterate over the dataframes in the dictionary\n",
    "    for symbol in _dfs.keys():\n",
    "        \n",
    "        # since the common sense will be to assume the trend next is going to be the same as the trend now, we will shift the trend\n",
    "        # forward by one, this will give us a column that matches the common sense assumption we set\n",
    "        common_sense = _dfs[symbol]['trend'].shift(1)\n",
    "\n",
    "        # measure the average of when the common sense (naive) prediction matches the actual 'trend'\n",
    "        common_sense_score = (common_sense == _dfs[symbol]['trend']).mean()\n",
    "        common_sense_score = round(common_sense_score, 2)\n",
    "        \n",
    "        # print the score to the screen\n",
    "        print(f\"The common sense score of {symbol} is: {common_sense_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad249871",
   "metadata": {},
   "source": [
    "(5) The implementation of apply_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963dd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to apply a given scaler to the features\n",
    "def apply_scaler(scaler, features):\n",
    "    \n",
    "    # set the training and test ratio to be 80-20\n",
    "    training_ratio = int(len(features) * 0.8)\n",
    "\n",
    "    # devide the feature set into training and test set\n",
    "    X_train, X_test = features[:training_ratio], features[training_ratio:]\n",
    "    \n",
    "    # apply a scaler on the training and test sets in isolation so we don't allow the test set to influence the scaling process, which reduces the likelihood of overfitting \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # concat the two scaled sets into one\n",
    "    X = np.concatenate((X_train_scaled, X_test_scaled), axis=0)\n",
    "\n",
    "    # return the scaled features\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ab2ed",
   "metadata": {},
   "source": [
    "(6) The implementation of create_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f304d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of isnpiration: https://stackoverflow.com/questions/47945512/how-to-reshape-input-for-keras-lstm?rq=4 [13]\n",
    "# create a function to reshape X and y into sequences of x timesteps\n",
    "def create_seqs(features, target, num_rows):\n",
    "    # create 2 empty lists to store the newly shaped features and target lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # iterate over the features\n",
    "    for i in range(len(features) - num_rows):\n",
    "        # create indexes of the start and end of each sequence\n",
    "        seq_s = i\n",
    "        seq_e = i + num_rows\n",
    "        \n",
    "        # the ith sequence will be a slice of the features between the indexes, create it and add it to X\n",
    "        xi = features[seq_s : seq_e]\n",
    "        X.append(xi)\n",
    "        \n",
    "        # do the same for the target and add it to y\n",
    "        yi = target[seq_e]\n",
    "        y.append(yi)\n",
    "    \n",
    "    # return the X and y as numpy arraies\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1060c3",
   "metadata": {},
   "source": [
    "(7) The implementation of create_train_vald_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to convert a dataframe into training, validation and test sets in the shape of (rows, timesteps, features)\n",
    "def create_train_vald_test_sets(_df, scaler, target=\"classification\", timesteps=6):\n",
    "\n",
    "    # reset the index\n",
    "    _df.reset_index(inplace = True)\n",
    "    \n",
    "    # drop the Date column as it's not necessary for now\n",
    "    _df.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "    # set the features set\n",
    "    X = _df.iloc[:, :-2]\n",
    "    \n",
    "    # set the target \n",
    "    if (target == \"classification\"):\n",
    "        # trend is the target for classification\n",
    "        y = _df.iloc[:, -1]\n",
    "    else:\n",
    "        # next_close is the target for regression\n",
    "        y = _df.iloc[:, -2]\n",
    "\n",
    "    # apply a scaler on the features set\n",
    "    X = apply_scaler(scaler, X)\n",
    "    \n",
    "    # create sequences\n",
    "    X_seq, y_seq = create_seqs(X, y, timesteps)\n",
    "    \n",
    "    # source of inspiration: https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical [14]\n",
    "    # use to_categorical from tf to converts the target (trend) to binary class matrix, this will help us assign confidences to the classification prediction\n",
    "    if (target == \"classification\"):\n",
    "        y_seq = to_categorical(y_seq)\n",
    "\n",
    "    # devide the data into a training set and a test set in 80-20 ratio\n",
    "    training_ratio = int(len(X) * 0.8)\n",
    "    X_train, X_test = X_seq[:training_ratio], X_seq[training_ratio:]\n",
    "    y_train, y_test = y_seq[:training_ratio], y_seq[training_ratio:]\n",
    "\n",
    "    # return the sets\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18850833",
   "metadata": {},
   "source": [
    "(8) The implementation of prepare_data_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba560e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a dict of dataframes, and return a dict of training, validation and testing datasets\n",
    "def prepare_data_to_train(dfs_dict, scaler, target, timesteps):\n",
    "    \n",
    "    # create a dict of dicts to store training, validation and test sets for each stock\n",
    "    sets_dict = {}\n",
    "    \n",
    "    # iterate over each dataframe in the dictionary\n",
    "    for symbol in dfs_dict.keys():\n",
    "        \n",
    "        # convert the dataframe into training, testing sets\n",
    "        X_train, X_test, y_train, y_test = create_train_vald_test_sets(dfs_dict[symbol].copy(deep=True), scaler, target, timesteps)\n",
    "\n",
    "        # create a dict of the sets and add it to the sets_dict\n",
    "        sets_dict[symbol] = {\n",
    "            'X_train': X_train, 'X_test': X_test, \n",
    "            'y_train': y_train, 'y_test': y_test\n",
    "        }\n",
    "    \n",
    "    # return the sets\n",
    "    return sets_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f86f11",
   "metadata": {},
   "source": [
    "(9) The implementation of get_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a3d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to measure how long a process would take\n",
    "def get_time():\n",
    "    return datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc597f19",
   "metadata": {},
   "source": [
    "(10) The implementation of HP_RNN_CONV_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of inspiration: Introduction to the Keras Tuner, https://www.tensorflow.org/tutorials/keras/keras_tuner [26]\n",
    "# constuct the model which will perform hyperparameter optimization to choose layers count, neurons counts, recurrent_dropout, optimizer_type, optimizer learning rate with convulotion, flattening, and dropout layers\n",
    "class HP_RNN_CONV_Model(HyperModel):\n",
    "\n",
    "    # initialize the model upon creating a class instance\n",
    "    # source of inspiration on how to pass variables to the model before passing the model to keras tuner: https://github.com/JulieProst/keras-tuner-tutorial/blob/master/hypermodels.py [33]\n",
    "    def __init__(self, X_train_shape):\n",
    "        self.X_train_shape = X_train_shape\n",
    "    \n",
    "    # build the model\n",
    "    def build(self, hp):\n",
    "        \n",
    "        clear_session()\n",
    "        # initialize a sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        ### add the model layers (Model hyperparameters optimization)\n",
    "        # input layer\n",
    "        model.add(Input(shape=(self.X_train_shape[1], self.X_train_shape[2])))\n",
    "        \n",
    "        # add Conv1D and MaxPooling layers\n",
    "        model.add(Conv1D(64, kernel_size=6, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "\n",
    "        # source: Int method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "        # dynamically optimize the number of layers\n",
    "        hp_layers = hp.Int(name='hp_layers', \n",
    "                           min_value=2, \n",
    "                           max_value=4, \n",
    "                           step=2)\n",
    "\n",
    "        # for each optimized layer\n",
    "        for i in range(hp_layers):\n",
    "            \n",
    "            # optimize the layer type\n",
    "            hp_layer_type = hp.Choice(f'RNN_layer_{i}_type', values=['SimpleRNN', 'LSTM', 'GRU'])\n",
    "            if hp_layer_type == 'SimpleRNN':\n",
    "                layer_type = SimpleRNN\n",
    "            elif hp_layer_type == 'LSTM':\n",
    "                layer_type = LSTM\n",
    "            else:\n",
    "                layer_type = GRU\n",
    "\n",
    "            # dynamically tune the number of units in each layer, select a value between 64-128\n",
    "            hp_units = hp.Int(name=f'hp_units_at_hp_layer_{i}', \n",
    "                              min_value=64, \n",
    "                              max_value=128, \n",
    "                              step=64)\n",
    "            \n",
    "            # source: SimpleRNN layer, https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "            # return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "            # set the return_sequences parameter to true unless it's the last layer, set it to false\n",
    "            return_sequences_boolean = i != (hp_layers - 1)\n",
    "\n",
    "            # source: Float method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "            # source: SimpleRNN layer, https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "            # recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "            # dynamically tune the recurrent_dropout float value\n",
    "            recurrent_dropout = hp.Float(name=f'recurrent_dropout_{i}', \n",
    "                                         min_value=0.0, \n",
    "                                         max_value=0.5, \n",
    "                                         step=0.1)\n",
    "\n",
    "            # add a simpleRNN layer and pass the optimized number of unites, recurrent_dropout, and the return_sequences boolean\n",
    "            layer = layer_type(units=hp_units, \n",
    "                               activation='relu',\n",
    "                              return_sequences=return_sequences_boolean, \n",
    "                              recurrent_dropout=recurrent_dropout)\n",
    "            model.add(layer)\n",
    "        \n",
    "        # add a flatting layer\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # add a dropout layer\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # add a dense layer\n",
    "        model.add(Dense(128))\n",
    "        \n",
    "        # add a dropout layer\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # add the output layer\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        ### the model compiler (Algorithm hyperparameters optimization)\n",
    "        # source: Choice method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "        # dynamically optimize the optimizer type\n",
    "        hp_optimizer_type = hp.Choice('optimizer_type', values=['Adam', 'RMSprop', 'SGD'])\n",
    "        if hp_optimizer_type == 'Adam':\n",
    "            optimizer = Adam\n",
    "        elif hp_optimizer_type == 'RMSprop':\n",
    "            optimizer = RMSprop\n",
    "        else:\n",
    "            optimizer = SGD\n",
    "\n",
    "        # dynamically tune the learning rate for the optimizer\n",
    "        # When sampling=\"log\", the step is multiplied between samples.\n",
    "        hp_lr = hp.Float('learning_rate', \n",
    "                         min_value=0.0001, \n",
    "                         max_value=0.01, \n",
    "                         sampling='LOG')\n",
    "        hp_optimizer = optimizer(learning_rate=hp_lr)\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(optimizer=hp_optimizer, \n",
    "                      loss='categorical_crossentropy', # this is the most suitable one for predictions of one-hot encoded labels\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # return the model\n",
    "        return model\n",
    "    \n",
    "    # source: omalleyt12, https://github.com/keras-team/keras-tuner/issues/122 [34]\n",
    "    # define a fit function which will allow us to pass an optimized value for batch_size\n",
    "    # *args and **kwargs are the ones we pass through tuner.search()\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \n",
    "        # dynamically optimize the batch_size for the training process\n",
    "        hp_batch_size = hp.Choice(\"batch_size\", [32, 64])\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp_batch_size,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8583407",
   "metadata": {},
   "source": [
    "(11) The implementation of HP_RNN_SC_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of inspiration: Introduction to the Keras Tuner, https://www.tensorflow.org/tutorials/keras/keras_tuner [26]\n",
    "# source of inspiration: François Chollet (11, 2017), “Deep Learning with Python”, chapters (7), hyperparameter optimization\n",
    "# constuct the model which will perform hyperparameter optimization to choose layers count, neurons counts, recurrent_dropout, optimizer_type, optimizer learning rate\n",
    "class HP_RNN_SC_Model(HyperModel):\n",
    "\n",
    "    # initialize the model upon creating a class instance\n",
    "    # using a class structure instead of a function to construct the model will allow us to pass variables to it before passing it to keras tuner\n",
    "    # source of inspiration on how to pass variables to the model before passing the model to keras tuner: https://github.com/JulieProst/keras-tuner-tutorial/blob/master/hypermodels.py [33]\n",
    "    def __init__(self, X_train_shape):\n",
    "        self.X_train_shape = X_train_shape\n",
    "    \n",
    "    # build the model\n",
    "    def build(self, hp):\n",
    "        \n",
    "        # initialize a sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        ### add the model layers (Model hyperparameters optimization)\n",
    "        # input layer\n",
    "        model.add(Input(shape=(self.X_train_shape[1], self.X_train_shape[2])))\n",
    "        \n",
    "        # add a SeparableConv1D layer at the beginning of the network, source of inspiration: François Chollet (11, 2017). “Deep Learning with Python” chapter 7\n",
    "        model.add(SeparableConv1D(64, kernel_size=6, activation='relu'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "\n",
    "        # source: Int method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "        # dynamically optimize the number of layers\n",
    "        hp_layers = hp.Int(name='hp_layers', \n",
    "                           min_value=2, \n",
    "                           max_value=4, \n",
    "                           step=2)\n",
    "\n",
    "        # for each optimized layer\n",
    "        for i in range(hp_layers):\n",
    "            \n",
    "            # optimize the layer type\n",
    "            hp_layer_type = hp.Choice(f'RNN_layer_{i}_type', values=['SimpleRNN', 'LSTM', 'GRU'])\n",
    "            if hp_layer_type == 'SimpleRNN':\n",
    "                layer_type = SimpleRNN\n",
    "            elif hp_layer_type == 'LSTM':\n",
    "                layer_type = LSTM\n",
    "            else:\n",
    "                layer_type = GRU\n",
    "\n",
    "            # dynamically tune the number of units in each layer, select a value between 64-128\n",
    "            hp_units = hp.Int(name=f'hp_units_at_hp_layer_{i}', \n",
    "                              min_value=64, \n",
    "                              max_value=128, \n",
    "                              step=64)\n",
    "            \n",
    "            # source: SimpleRNN layer, https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "            # return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "            # set the return_sequences parameter to true unless it's the last layer, set it to false\n",
    "            return_sequences_boolean = i != (hp_layers - 1)\n",
    "\n",
    "            # source: Float method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "            # source: SimpleRNN layer, https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
    "            # recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "            # dynamically tune the recurrent_dropout float value\n",
    "            recurrent_dropout = hp.Float(name=f'recurrent_dropout_{i}', \n",
    "                                         min_value=0.0, \n",
    "                                         max_value=0.5, \n",
    "                                         step=0.1)\n",
    "\n",
    "            # add a simpleRNN layer and pass the optimized number of unites, recurrent_dropout, and the return_sequences boolean\n",
    "            layer = layer_type(units=hp_units, \n",
    "                               activation='relu',\n",
    "                              return_sequences=return_sequences_boolean, \n",
    "                              recurrent_dropout=recurrent_dropout)\n",
    "            model.add(layer)\n",
    "        \n",
    "        # add a flatting layer\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # add a dropout layer\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # add a dense layer\n",
    "        model.add(Dense(128))\n",
    "        \n",
    "        # add a dropout layer\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # add the output layer\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        ### the model compiler (Algorithm hyperparameters optimization)\n",
    "        # source: Choice method, https://keras.io/api/keras_tuner/hyperparameters/\n",
    "        # dynamically optimize the optimizer type\n",
    "        hp_optimizer_type = hp.Choice('optimizer_type', values=['Adam', 'RMSprop', 'SGD'])\n",
    "        if hp_optimizer_type == 'Adam':\n",
    "            optimizer = Adam\n",
    "        elif hp_optimizer_type == 'RMSprop':\n",
    "            optimizer = RMSprop\n",
    "        else:\n",
    "            optimizer = SGD\n",
    "\n",
    "        # dynamically tune the learning rate for the optimizer\n",
    "        # When sampling=\"log\", the step is multiplied between samples.\n",
    "        hp_lr = hp.Float('learning_rate', \n",
    "                         min_value=0.0001, \n",
    "                         max_value=0.01, \n",
    "                         sampling='LOG')\n",
    "        hp_optimizer = optimizer(learning_rate=hp_lr)\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(optimizer=hp_optimizer, \n",
    "                      loss='categorical_crossentropy', # this is the most suitable one for predictions of one-hot encoded labels\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # return the model\n",
    "        return model\n",
    "    \n",
    "    # source: omalleyt12, https://github.com/keras-team/keras-tuner/issues/122 [34]\n",
    "    # define a fit function which will allow us to pass an optimized value for batch_size\n",
    "    # *args and **kwargs are the ones we pass through tuner.search()\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        \n",
    "        # dynamically optimize the batch_size for the training process\n",
    "        hp_batch_size = hp.Choice(\"batch_size\", [32, 64])\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp_batch_size,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
