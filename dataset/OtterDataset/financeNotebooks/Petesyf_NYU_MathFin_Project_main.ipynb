{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from communities.algorithms import louvain_method\n",
    "import time\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import threading\n",
    "\n",
    "# get correlation matrix file names\n",
    "data_folder = \"Data/Corr_Mat\"\n",
    "output_folder = \"Data/Ticker_List\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def download_stock_pool_data():\n",
    "    # get data directory path\n",
    "    cur_path = os.path.dirname(__file__)\n",
    "    data_directory_path = os.path.relpath('../Data', cur_path)\n",
    "\n",
    "    # get latest russell 1000 constituents\n",
    "    russell1000_info = pd.read_excel(\n",
    "        io=data_directory_path + \"\\\\Russell_1000_Constituents_20221007.xlsx\",\n",
    "        sheet_name=\"Holdings\",\n",
    "        skiprows=range(7))\n",
    "    # get list of tickers\n",
    "    stocks_pool_list = list(russell1000_info.Ticker.values)\n",
    "    # get historical market data of current Russel 1000 constituents\n",
    "    stocks_pool_data = yf.Tickers(stocks_pool_list).history(start=\"2012-01-01\")[\"Close\"]\n",
    "\n",
    "    # get historical market data of S&P500\n",
    "    sp500_data = pd.DataFrame(yf.Ticker(\"^GSPC\").history(start=\"2012-01-01\")[\"Close\"])\n",
    "    # rename S&P 500 data column\n",
    "    sp500_data.columns = [\"SP500\"]\n",
    "\n",
    "    # merge two dataframes\n",
    "    raw_data = stocks_pool_data.join(sp500_data)\n",
    "\n",
    "    # drop stocks with more than 1000 NaNs\n",
    "    raw_data = raw_data.dropna(axis=\"columns\", thresh=2500)\n",
    "\n",
    "    # save stock pool data into csv\n",
    "    raw_data.to_csv(data_directory_path + \"\\\\Raw_Data_20221007.csv\")\n",
    "\n",
    "# # get stock pool data from yahoo finance\n",
    "# download_stock_pool_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Correlation Matrices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def generate_residual_matrices():\n",
    "    # get data directory path\n",
    "    cur_path = os.path.dirname(__file__)\n",
    "    data_directory_path = os.path.relpath('../Data', cur_path)\n",
    "    # read data from file\n",
    "    raw_data = pd.read_csv(data_directory_path + \"\\\\Raw_Data_20221007.csv\", index_col=0)\n",
    "\n",
    "    # set rebalancing frequency: every month\n",
    "    rebalance_freq_period = relativedelta(months=1)\n",
    "    # set business day convention for rebalancing\n",
    "    business_day_convention = \"Modified Following\"\n",
    "    # training set length\n",
    "    train_set_length_period = relativedelta(months=6)\n",
    "    # set date range\n",
    "    first_date = datetime.strptime(raw_data.index[0], \"%Y-%m-%d\")\n",
    "    last_date = datetime.strptime(raw_data.index[-1], \"%Y-%m-%d\")\n",
    "\n",
    "    # initialize date range\n",
    "    train_start_date = first_date\n",
    "    train_end_date = train_start_date + train_set_length_period - relativedelta(days=1)\n",
    "\n",
    "    test_start_date = train_end_date + relativedelta(days=1)\n",
    "    test_end_date = test_start_date + rebalance_freq_period\n",
    "\n",
    "    # traverse the data set\n",
    "    while test_end_date < last_date:\n",
    "        # do regression\n",
    "        temp_train_data = raw_data.loc[train_start_date.__str__()[:10]:test_start_date.__str__()[:10], :]\n",
    "        # save the residuals\n",
    "        temp_residuals = pd.DataFrame(index=temp_train_data.index, columns=temp_train_data.columns.drop(\"SP500\"))\n",
    "        for ticker in temp_residuals.columns:\n",
    "            # get data\n",
    "            y_x = temp_train_data[[ticker, \"SP500\"]]\n",
    "            # drop nas\n",
    "            y_x = y_x.dropna(axis=\"index\", how=\"any\")\n",
    "            # rename columns\n",
    "            y_x.columns = [\"y\", \"x\"]\n",
    "            # whether there's sufficient trading days\n",
    "            if len(y_x.index) < len(temp_train_data) * 0.9:\n",
    "                continue\n",
    "            else:\n",
    "                # calculate returns\n",
    "                y_x = np.log(y_x).diff().dropna(axis=\"index\", how=\"any\")\n",
    "                y = np.array(y_x[\"y\"])\n",
    "                x = np.array(y_x[\"x\"]).reshape(-1, 1)\n",
    "                # do regression\n",
    "                reg = LinearRegression(fit_intercept=True).fit(x, y)\n",
    "                # calculate residual\n",
    "                y_x.loc[:, \"res\"] = np.subtract(y, (reg.intercept_ - reg.coef_[0] * x)[:, 0])\n",
    "                # add residual to temp_residuals\n",
    "                temp_residuals.loc[y_x.index, ticker] = y_x.res\n",
    "        # drop nans in dataframe\n",
    "        temp_residuals = temp_residuals.dropna(axis=\"index\", how=\"all\")\n",
    "        temp_residuals = temp_residuals.dropna(axis=\"columns\", how=\"any\")\n",
    "        # calculate correlations\n",
    "        temp_residuals = temp_residuals.astype(float)\n",
    "        temp_corr_matrix = temp_residuals.corr()\n",
    "\n",
    "        # save the matrix to file\n",
    "        temp_corr_matrix.to_csv(data_directory_path + \"\\\\Corr_Mat\\\\\" + train_start_date.__str__()[:10] + \".csv\")\n",
    "\n",
    "        # update dates\n",
    "        train_start_date += rebalance_freq_period\n",
    "        test_start_date += rebalance_freq_period\n",
    "        test_end_date += rebalance_freq_period\n",
    "\n",
    "# # calculate correlation between residuals of stocks during 6M period\n",
    "# generate_residual_matrices()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Graph Into Communities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_communities(file_name:str, input_folder:str = data_folder):\n",
    "    print(file_name)\n",
    "    # get correlation matrix\n",
    "    corr_mat = pd.read_csv(input_folder+\"/\"+file_name, index_col=0)\n",
    "    # convert to adjacency matrix\n",
    "    adj_mat = np.abs(corr_mat - np.diag(np.diag(corr_mat)))\n",
    "    # using Louvain method to split the graph into 20 communities\n",
    "    communities, _ = louvain_method(adj_mat.values, 20)\n",
    "    # get ticker list of each community\n",
    "    cluster_list = []\n",
    "    for community in communities:\n",
    "        cluster = list(community)\n",
    "        cluster_list.append(adj_mat.columns[cluster])\n",
    "    # save the list to txt\n",
    "    global output_folder\n",
    "    with open(output_folder + \"/\"+file_name.replace(\".csv\",\".txt\"), 'w') as f:\n",
    "        for cluster in cluster_list:\n",
    "            f.write(str(list(cluster)) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class MyThread(threading.Thread):\n",
    "\n",
    "    def __init__(self, func, arg):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.arg = arg\n",
    "\n",
    "    def run(self):\n",
    "        self.func(*self.arg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_all_communities(file_list:list[str] = os.listdir(data_folder), input_folder:str = data_folder, thread_num:int = 8):\n",
    "    # create 8 threads\n",
    "    for i in range(0, len(file_list), thread_num):\n",
    "        thread_list = []\n",
    "        print(\"Iteration {} start, current time = {}\".format(i//8+1, datetime.now()))\n",
    "        for j in range(thread_num):\n",
    "            file_name = file_list[i + j]\n",
    "            thread = MyThread(get_communities, (file_name, input_folder))\n",
    "            thread.start()\n",
    "            thread_list.append(thread)\n",
    "        for thread in thread_list:\n",
    "            thread.join()\n",
    "\n",
    "# generate clusters of tickers and save them to txt file\n",
    "# file_name_list = os.listdir(data_folder)\n",
    "# get_all_communities(file_name_list, data_folder, 8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Centroids of Clusters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def unwrap_list_from_str(str_list:str)->list[str]:\n",
    "    return str_list.replace(\"\\'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\" \", \"\").split(\",\")\n",
    "\n",
    "def get_tickers_of_clusters():\n",
    "    # read clusters from file\n",
    "    file_name_list =os.listdir(output_folder)\n",
    "    df_cluster_data = pd.DataFrame(columns=[\"cluster_{}\".format(i) for i in range(20)])\n",
    "    for file_name in file_name_list:\n",
    "        # read string from file\n",
    "        with open(output_folder+\"/\"+file_name, \"r\") as f:\n",
    "            list_str_data = f.readlines()\n",
    "        list_data = []\n",
    "        for str_data in list_str_data:\n",
    "            list_data.append(unwrap_list_from_str(str_data))\n",
    "        df_cluster_data.loc[file_name[:10],:] = list_data\n",
    "    return df_cluster_data\n",
    "\n",
    "# # save clusters as a dataframe\n",
    "# df_cluster_data = get_tickers_of_clusters()\n",
    "# # save to local\n",
    "# df_cluster_data.to_csv(\"Data/Stock_Clusters.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def stock_selection(df_cluster_info:pd.DataFrame, df_price_info:pd.DataFrame)->(pd.DataFrame,pd.DataFrame):\n",
    "    # naive selection: choose first stock of each cluster\n",
    "    df_selected = pd.DataFrame(columns=df_cluster_info.columns, index=df_cluster_info.index)\n",
    "    for col in df_selected.columns:\n",
    "        for idx in df_selected.index:\n",
    "            df_selected.loc[idx, col] = df_cluster_info.loc[idx, col][0]\n",
    "    return df_selected\n",
    "\n",
    "def select_stock_from_clusters():\n",
    "    # read data from local\n",
    "    df_cluster_data = pd.read_csv(\"Data/Stock_Clusters.csv\",index_col=0)\n",
    "    # convert string to list\n",
    "    df_cluster_data = df_cluster_data.applymap(lambda x: unwrap_list_from_str(x))\n",
    "\n",
    "    # read stock price data from locals\n",
    "    performance_data = pd.read_csv(\"Data/Raw_Data_20221007.csv\", index_col=0)\n",
    "\n",
    "    # select stocks from clusters\n",
    "    df_selected = stock_selection(df_cluster_data, performance_data)\n",
    "\n",
    "    # get weights of constituents\n",
    "    df_coef = pd.DataFrame(columns=df_selected.columns, index=df_selected.index)\n",
    "    for idx in df_selected.index:\n",
    "        stock_list = list(df_selected.loc[idx,:].values)\n",
    "        end = str(datetime.strptime(idx, \"%Y-%m-%d\") + relativedelta(months=6))[:10]\n",
    "        df_x = np.log(performance_data.loc[idx:end,stock_list]).diff().dropna()\n",
    "        df_y = np.log(performance_data.loc[idx:end,[\"SP500\"]]).diff().dropna()\n",
    "\n",
    "        # regress to get optimal weights using Ridge regression\n",
    "        model =LinearRegression(fit_intercept=False, positive=True)\n",
    "        model.fit(df_x.values, df_y.values)\n",
    "        df_coef.loc[idx,:] = model.coef_[0]\n",
    "\n",
    "    # reset index of two dfs\n",
    "    df_coef.index = df_coef.index.map(lambda x: str(datetime.strptime(x, \"%Y-%m-%d\") + relativedelta(months=6))[:10])\n",
    "    df_selected.index = df_selected.index.map(lambda x: str(datetime.strptime(x, \"%Y-%m-%d\") + relativedelta(months=6))[:10])\n",
    "\n",
    "    return df_coef, df_selected\n",
    "\n",
    "# get coefficients of each constituent\n",
    "df_coef, df_selected = select_stock_from_clusters()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def traking_record(df_coef:pd.DataFrame, df_selected:pd.DataFrame):\n",
    "    # read stock price data from locals\n",
    "    performance_data = pd.read_csv(\"Data/Raw_Data_20221007.csv\", index_col=0)\n",
    "    # track performance of our portfolio\n",
    "    list_df_record = []\n",
    "    for idx in range(len(df_coef.index)):\n",
    "        start = df_coef.index[idx]\n",
    "        if idx < len(df_coef.index)-1:\n",
    "            end = df_coef.index[idx+1]\n",
    "        else:\n",
    "            end = \"2022-10-03\"\n",
    "        # get stock list\n",
    "        stock_list = list(df_selected.iloc[idx,:].values)\n",
    "        # get coefficients\n",
    "        coef_list = list(df_coef.iloc[idx,:].values) + [1 - sum(df_coef.iloc[idx,:].values)]\n",
    "        # performance of this month\n",
    "        temp_performance = performance_data.loc[start:end,stock_list].copy()\n",
    "        # set initial value as 1\n",
    "        temp_performance.loc[:,:] /= temp_performance.iloc[0,:]\n",
    "        # calculate portfolio return\n",
    "        temp_df_record = pd.DataFrame(index=temp_performance.index, columns=[\"tracking\", \"SP500\"])\n",
    "        temp_df_record.loc[temp_performance.index,[\"tracking\"]] = temp_performance.apply(lambda x: np.dot(coef_list[:-1],x)+coef_list[-1], axis=1)\n",
    "        # calculate SP500 return\n",
    "        temp_benchmark = performance_data.loc[start:end,[\"SP500\"]].copy()\n",
    "        temp_benchmark.loc[:,:] /= temp_benchmark.iloc[0,:]\n",
    "        temp_df_record.loc[temp_benchmark.index, [\"SP500\"]] = temp_benchmark[\"SP500\"]\n",
    "        list_df_record.append(temp_df_record.dropna(how=\"any\"))\n",
    "\n",
    "    # concat all the dataframes\n",
    "    df_record = pd.DataFrame(index=performance_data.index, columns=[\"tracking\",\"SP500\"])\n",
    "    init = [1,1]\n",
    "    for ele in list_df_record:\n",
    "        ele *= init\n",
    "        df_record.loc[ele.index, ele.columns] = ele.values\n",
    "        init = list(ele.iloc[-1,:].values)\n",
    "    # dropna\n",
    "    df_record = df_record.dropna()\n",
    "    df_record.plot()\n",
    "    df_record.to_csv(\"Data/Result.csv\")\n",
    "\n",
    "traking_record(df_coef, df_selected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
