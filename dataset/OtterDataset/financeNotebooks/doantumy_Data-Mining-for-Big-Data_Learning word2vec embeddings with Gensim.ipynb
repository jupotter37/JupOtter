{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning `word2vec` embeddings\n",
    "#### By: Tu My DOAN & Sali Dauda MOHAMNMED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "TensorFlow version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pickle, random\n",
    "from xml.dom import minidom\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "First, we will read all train data into dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>earnings: 0 no/ 1 yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.xml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.xml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.xml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.xml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.xml</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   file  earnings: 0 no/ 1 yes\n",
       "0   1  1.xml                      0\n",
       "1   2  2.xml                      0\n",
       "2   3  3.xml                      0\n",
       "3   4  4.xml                      0\n",
       "4   5  5.xml                      0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./dmdata/train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to read all of our xml files and return a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suffield Financial Corp said the\\nFederal Rese...</td>\n",
       "      <td>162.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;Mark Resources Inc&gt; said it\\nagreed to sell 5...</td>\n",
       "      <td>1390.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern New England\\nTelecommunications Inc s...</td>\n",
       "      <td>604.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;/BODY&gt;The investor group owning about 42 pct\\...</td>\n",
       "      <td>2699.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rexon Inc said it filed a\\nregistration statem...</td>\n",
       "      <td>2841.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content      file\n",
       "0  Suffield Financial Corp said the\\nFederal Rese...   162.xml\n",
       "1  <Mark Resources Inc> said it\\nagreed to sell 5...  1390.xml\n",
       "2  Southern New England\\nTelecommunications Inc s...   604.xml\n",
       "3  </BODY>The investor group owning about 42 pct\\...  2699.xml\n",
       "4  Rexon Inc said it filed a\\nregistration statem...  2841.xml"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_xml(xml_path):\n",
    "    empty_content = []\n",
    "    df = pd.DataFrame(columns=['content','file'])\n",
    "    file_list = glob2.glob(xml_path)\n",
    "    for file in file_list:\n",
    "        mydoc = minidom.parse(file)\n",
    "        file_name = file.split(\"/\")\n",
    "        items = mydoc.getElementsByTagName('BODY')\n",
    "        if items[0].firstChild != None:\n",
    "            content = items[0].firstChild.data\n",
    "            df = df.append({'content':content, 'file':file_name[2]},ignore_index=True)\n",
    "        else: \n",
    "            empty_content.append(str(file_name[2]))\n",
    "            df = df.append({'content':np.nan, 'file':file_name[2]},ignore_index=True)\n",
    "    return df, empty_content\n",
    "empty_content_lst = []\n",
    "df_xml, empty_content_lst = read_xml(\"./dmdata/*.xml\")\n",
    "df_xml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, we have a dataframe that contains all of the content of our xml files along with their file name. Later we will merge this dataframe with another train dataframe that contains the labels and file name together.\n",
    "\n",
    "The `empty_content_lst` is a list of article without any data in it. Please note that there are `416` items without any content that come from both train (`398 items`) and test set (`18 items`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416,\n",
       " ['1435.xml',\n",
       "  '4559.xml',\n",
       "  '77.xml',\n",
       "  '189.xml',\n",
       "  '4565.xml',\n",
       "  '3975.xml',\n",
       "  '2302.xml',\n",
       "  '1780.xml',\n",
       "  '4808.xml',\n",
       "  '2909.xml',\n",
       "  '994.xml',\n",
       "  '2060.xml',\n",
       "  '3419.xml',\n",
       "  '3343.xml',\n",
       "  '981.xml',\n",
       "  '2263.xml',\n",
       "  '4148.xml',\n",
       "  '2465.xml',\n",
       "  '3035.xml',\n",
       "  '4799.xml',\n",
       "  '76.xml',\n",
       "  '1346.xml',\n",
       "  '4558.xml',\n",
       "  '611.xml',\n",
       "  '3586.xml',\n",
       "  '2698.xml',\n",
       "  '607.xml',\n",
       "  '3235.xml',\n",
       "  '1436.xml',\n",
       "  '2103.xml',\n",
       "  '3976.xml',\n",
       "  '4758.xml',\n",
       "  '3037.xml',\n",
       "  '4770.xml',\n",
       "  '571.xml',\n",
       "  '3341.xml',\n",
       "  '4413.xml',\n",
       "  '3354.xml',\n",
       "  '216.xml',\n",
       "  '4836.xml',\n",
       "  '2466.xml',\n",
       "  '2328.xml',\n",
       "  '4759.xml',\n",
       "  '1437.xml',\n",
       "  '3591.xml',\n",
       "  '831.xml',\n",
       "  '1180.xml',\n",
       "  '400.xml',\n",
       "  '4761.xml',\n",
       "  '1143.xml',\n",
       "  '4832.xml',\n",
       "  '1989.xml',\n",
       "  '3146.xml',\n",
       "  '4417.xml',\n",
       "  '789.xml',\n",
       "  '2067.xml',\n",
       "  '2098.xml',\n",
       "  '993.xml',\n",
       "  '1988.xml',\n",
       "  '3621.xml',\n",
       "  '3190.xml',\n",
       "  '3741.xml',\n",
       "  '398.xml',\n",
       "  '2649.xml',\n",
       "  '1368.xml',\n",
       "  '830.xml',\n",
       "  '2675.xml',\n",
       "  '2113.xml',\n",
       "  '171.xml',\n",
       "  '99.xml',\n",
       "  '2105.xml',\n",
       "  '2893.xml',\n",
       "  '3964.xml',\n",
       "  '417.xml',\n",
       "  '4762.xml',\n",
       "  '2313.xml',\n",
       "  '1961.xml',\n",
       "  '3186.xml',\n",
       "  '4831.xml',\n",
       "  '4164.xml',\n",
       "  '985.xml',\n",
       "  '1593.xml',\n",
       "  '2703.xml',\n",
       "  '4415.xml',\n",
       "  '1592.xml',\n",
       "  '760.xml',\n",
       "  '1035.xml',\n",
       "  '3144.xml',\n",
       "  '4763.xml',\n",
       "  '2460.xml',\n",
       "  '4950.xml',\n",
       "  '1828.xml',\n",
       "  '3959.xml',\n",
       "  '4561.xml',\n",
       "  '2845.xml',\n",
       "  '600.xml',\n",
       "  '101.xml',\n",
       "  '673.xml',\n",
       "  '129.xml',\n",
       "  '1324.xml',\n",
       "  '459.xml',\n",
       "  '3719.xml',\n",
       "  '4062.xml',\n",
       "  '4843.xml',\n",
       "  '511.xml',\n",
       "  '1091.xml',\n",
       "  '4880.xml',\n",
       "  '4658.xml',\n",
       "  '713.xml',\n",
       "  '4499.xml',\n",
       "  '934.xml',\n",
       "  '4466.xml',\n",
       "  '2002.xml',\n",
       "  '4473.xml',\n",
       "  '3678.xml',\n",
       "  '3863.xml',\n",
       "  '2599.xml',\n",
       "  '2176.xml',\n",
       "  '1331.xml',\n",
       "  '102.xml',\n",
       "  '2174.xml',\n",
       "  '3083.xml',\n",
       "  '4698.xml',\n",
       "  '4840.xml',\n",
       "  '3134.xml',\n",
       "  '2564.xml',\n",
       "  '2216.xml',\n",
       "  '1253.xml',\n",
       "  '1521.xml',\n",
       "  '1247.xml',\n",
       "  '2014.xml',\n",
       "  '1508.xml',\n",
       "  '2798.xml',\n",
       "  '2940.xml',\n",
       "  '4114.xml',\n",
       "  '3653.xml',\n",
       "  '1050.xml',\n",
       "  '1939.xml',\n",
       "  '261.xml',\n",
       "  '2377.xml',\n",
       "  '3041.xml',\n",
       "  '2607.xml',\n",
       "  '1454.xml',\n",
       "  '103.xml',\n",
       "  '665.xml',\n",
       "  '1487.xml',\n",
       "  '675.xml',\n",
       "  '2171.xml',\n",
       "  '4528.xml',\n",
       "  '3247.xml',\n",
       "  '4925.xml',\n",
       "  '2401.xml',\n",
       "  '1120.xml',\n",
       "  '2207.xml',\n",
       "  '3125.xml',\n",
       "  '4460.xml',\n",
       "  '1530.xml',\n",
       "  '1256.xml',\n",
       "  '3333.xml',\n",
       "  '2038.xml',\n",
       "  '4449.xml',\n",
       "  '2004.xml',\n",
       "  '1109.xml',\n",
       "  '3087.xml',\n",
       "  '4924.xml',\n",
       "  '3911.xml',\n",
       "  '4515.xml',\n",
       "  '648.xml',\n",
       "  '886.xml',\n",
       "  '2166.xml',\n",
       "  '2600.xml',\n",
       "  '689.xml',\n",
       "  '879.xml',\n",
       "  '3913.xml',\n",
       "  '2402.xml',\n",
       "  '4701.xml',\n",
       "  '2358.xml',\n",
       "  '1719.xml',\n",
       "  '4675.xml',\n",
       "  '3481.xml',\n",
       "  '4463.xml',\n",
       "  '4311.xml',\n",
       "  '1268.xml',\n",
       "  '4462.xml',\n",
       "  '298.xml',\n",
       "  '2588.xml',\n",
       "  '2403.xml',\n",
       "  '4516.xml',\n",
       "  '663.xml',\n",
       "  '2181.xml',\n",
       "  '4282.xml',\n",
       "  '2624.xml',\n",
       "  '4533.xml',\n",
       "  '1107.xml',\n",
       "  '4862.xml',\n",
       "  '2220.xml',\n",
       "  '4889.xml',\n",
       "  '4484.xml',\n",
       "  '1517.xml',\n",
       "  '901.xml',\n",
       "  '900.xml',\n",
       "  '4334.xml',\n",
       "  '2037.xml',\n",
       "  '519.xml',\n",
       "  '2341.xml',\n",
       "  '3088.xml',\n",
       "  '445.xml',\n",
       "  '2157.xml',\n",
       "  '4532.xml',\n",
       "  '1462.xml',\n",
       "  '4281.xml',\n",
       "  '3288.xml',\n",
       "  '2169.xml',\n",
       "  '2141.xml',\n",
       "  '4530.xml',\n",
       "  '4929.xml',\n",
       "  '1851.xml',\n",
       "  '3061.xml',\n",
       "  '3049.xml',\n",
       "  '4652.xml',\n",
       "  '3897.xml',\n",
       "  '1058.xml',\n",
       "  '2784.xml',\n",
       "  '4487.xml',\n",
       "  '3465.xml',\n",
       "  '1514.xml',\n",
       "  '5000.xml',\n",
       "  '2035.xml',\n",
       "  '4445.xml',\n",
       "  '4479.xml',\n",
       "  '4486.xml',\n",
       "  '2949.xml',\n",
       "  '4653.xml',\n",
       "  '3114.xml',\n",
       "  '1717.xml',\n",
       "  '283.xml',\n",
       "  '2587.xml',\n",
       "  '2424.xml',\n",
       "  '3060.xml',\n",
       "  '3074.xml',\n",
       "  '4928.xml',\n",
       "  '4531.xml',\n",
       "  '3538.xml',\n",
       "  '3504.xml',\n",
       "  '1307.xml',\n",
       "  '2183.xml',\n",
       "  '4280.xml',\n",
       "  '2801.xml',\n",
       "  '4284.xml',\n",
       "  '3299.xml',\n",
       "  '132.xml',\n",
       "  '2811.xml',\n",
       "  '2622.xml',\n",
       "  '2178.xml',\n",
       "  '4509.xml',\n",
       "  '1698.xml',\n",
       "  '4904.xml',\n",
       "  '4723.xml',\n",
       "  '3851.xml',\n",
       "  '1049.xml',\n",
       "  '1713.xml',\n",
       "  '4482.xml',\n",
       "  '4455.xml',\n",
       "  '3448.xml',\n",
       "  '3312.xml',\n",
       "  '4468.xml',\n",
       "  '1538.xml',\n",
       "  '2743.xml',\n",
       "  '709.xml',\n",
       "  '2794.xml',\n",
       "  '4483.xml',\n",
       "  '4656.xml',\n",
       "  '4865.xml',\n",
       "  '4859.xml',\n",
       "  '3850.xml',\n",
       "  '2582.xml',\n",
       "  '4722.xml',\n",
       "  '1855.xml',\n",
       "  '4087.xml',\n",
       "  '1699.xml',\n",
       "  '1841.xml',\n",
       "  '2623.xml',\n",
       "  '4534.xml',\n",
       "  '2145.xml',\n",
       "  '1458.xml',\n",
       "  '2804.xml',\n",
       "  '133.xml',\n",
       "  '4293.xml',\n",
       "  '2621.xml',\n",
       "  '30.xml',\n",
       "  '694.xml',\n",
       "  '4278.xml',\n",
       "  '4913.xml',\n",
       "  '455.xml',\n",
       "  '1894.xml',\n",
       "  '3729.xml',\n",
       "  '3715.xml',\n",
       "  '1710.xml',\n",
       "  '2755.xml',\n",
       "  '3311.xml',\n",
       "  '1274.xml',\n",
       "  '3305.xml',\n",
       "  '3304.xml',\n",
       "  '3476.xml',\n",
       "  '2032.xml',\n",
       "  '4480.xml',\n",
       "  '2595.xml',\n",
       "  '4053.xml',\n",
       "  '440.xml',\n",
       "  '3099.xml',\n",
       "  '4279.xml',\n",
       "  '31.xml',\n",
       "  '2634.xml',\n",
       "  '2146.xml',\n",
       "  '625.xml',\n",
       "  '95.xml',\n",
       "  '4578.xml',\n",
       "  '4961.xml',\n",
       "  '433.xml',\n",
       "  '4949.xml',\n",
       "  '396.xml',\n",
       "  '1164.xml',\n",
       "  '3983.xml',\n",
       "  '2069.xml',\n",
       "  '792.xml',\n",
       "  '1206.xml',\n",
       "  '3363.xml',\n",
       "  '2041.xml',\n",
       "  '1548.xml',\n",
       "  '2726.xml',\n",
       "  '1549.xml',\n",
       "  '3362.xml',\n",
       "  '1987.xml',\n",
       "  '4828.xml',\n",
       "  '2336.xml',\n",
       "  '1165.xml',\n",
       "  '3766.xml',\n",
       "  '383.xml',\n",
       "  '3799.xml',\n",
       "  '4960.xml',\n",
       "  '3570.xml',\n",
       "  '94.xml',\n",
       "  '632.xml',\n",
       "  '2678.xml',\n",
       "  '2136.xml',\n",
       "  '2122.xml',\n",
       "  '430.xml',\n",
       "  '4786.xml',\n",
       "  '418.xml',\n",
       "  '1167.xml',\n",
       "  '1615.xml',\n",
       "  '3162.xml',\n",
       "  '3610.xml',\n",
       "  '593.xml',\n",
       "  '3406.xml',\n",
       "  '949.xml',\n",
       "  '4433.xml',\n",
       "  '2043.xml',\n",
       "  '1204.xml',\n",
       "  '790.xml',\n",
       "  '1589.xml',\n",
       "  '3611.xml',\n",
       "  '3836.xml',\n",
       "  '1953.xml',\n",
       "  '1166.xml',\n",
       "  '1199.xml',\n",
       "  '4977.xml',\n",
       "  '431.xml',\n",
       "  '2123.xml',\n",
       "  '2876.xml',\n",
       "  '78.xml',\n",
       "  '4032.xml',\n",
       "  '4185.xml',\n",
       "  '4620.xml',\n",
       "  '1758.xml',\n",
       "  '3359.xml',\n",
       "  '958.xml',\n",
       "  '3365.xml',\n",
       "  '3370.xml',\n",
       "  '4635.xml',\n",
       "  '2244.xml',\n",
       "  '3006.xml',\n",
       "  '2442.xml',\n",
       "  '2330.xml',\n",
       "  '4782.xml',\n",
       "  '636.xml',\n",
       "  '92.xml',\n",
       "  '608.xml',\n",
       "  '2124.xml',\n",
       "  '1411.xml',\n",
       "  '2497.xml',\n",
       "  '4757.xml',\n",
       "  '4743.xml',\n",
       "  '1613.xml',\n",
       "  '3616.xml',\n",
       "  '2939.xml',\n",
       "  '4390.xml',\n",
       "  '973.xml',\n",
       "  '4409.xml',\n",
       "  '796.xml',\n",
       "  '4434.xml',\n",
       "  '2723.xml',\n",
       "  '972.xml',\n",
       "  '3165.xml',\n",
       "  '4150.xml',\n",
       "  '3159.xml',\n",
       "  '1969.xml',\n",
       "  '2290.xml',\n",
       "  '1799.xml',\n",
       "  '392.xml',\n",
       "  '1612.xml',\n",
       "  '2327.xml',\n",
       "  '2441.xml',\n",
       "  '4959.xml',\n",
       "  '2125.xml',\n",
       "  '2870.xml'])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty_content_lst),empty_content_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                 content      file\n",
       "0     Suffield Financial Corp said the\\nFederal Rese...   162.xml\n",
       "1     <Mark Resources Inc> said it\\nagreed to sell 5...  1390.xml\n",
       "2     Southern New England\\nTelecommunications Inc s...   604.xml\n",
       "3     </BODY>The investor group owning about 42 pct\\...  2699.xml\n",
       "4     Rexon Inc said it filed a\\nregistration statem...  2841.xml\n",
       "5     </BODY>Hughes Tool Co said its board voted at\\...  3587.xml\n",
       "6     Standard and Poor's Corp said it\\nupgraded Geo...  2855.xml\n",
       "7     National Bank of Hungary first\\nvice-president...  3593.xml\n",
       "8     Sorg Inc said a group composed of\\none-third o...    88.xml\n",
       "9     Shr 27 cts vs 39 cts\\n    Net 481,189 vs 697,3...   610.xml\n",
       "10    Shr 16 cts vs 35 cts\\n    Net 476,000 vs 929,0...  1384.xml\n",
       "11    Liquefied natural gas imports from\\nAlgeria ar...   176.xml\n",
       "12    Shr 20 cts vs 28 cts\\n    Net 393,371 vs 555,9...   638.xml\n",
       "13    General Instrument Corp has received\\na 36.0 m...  2869.xml\n",
       "14                                                  NaN  1435.xml\n",
       "15    The Bank of Zambia has paid out foreign\\nexcha...  3222.xml\n",
       "16    ICO Inc said its common stock\\nwill be removed...  2882.xml\n",
       "17    Axlon Inc said Austin C.\\nMarshall has been na...  3544.xml\n",
       "18    General Motors Corp said its directors\\nauthor...  1353.xml\n",
       "19    </BODY>Exports of American wine rose 14.9\\nper...  1347.xml\n",
       "20    (Dumez Investments I Inc) said 94.7 pct\\nof We...  2896.xml\n",
       "21    The borrowing limit of the Hong Kong\\nExchange...  3550.xml\n",
       "22    The U.S. Department of Transportation\\nsaid it...  2128.xml\n",
       "23    </BODY>WearEver-ProctorSilex Inc said\\nit will...  3236.xml\n",
       "24                                                  NaN  4559.xml\n",
       "25    The International Swap Dealers\\nAssociation ha...  1421.xml\n",
       "26    NV Philips\\nGloeilampenfabrieken <PGLO.AS> sai...  2100.xml\n",
       "27    Former White House aide Kenneth\\nDuberstein ha...  4571.xml\n",
       "28    GTX Corp said Nippon Steel Co of Japan,\\nin a ...  1409.xml\n",
       "29    Canadian Finance Minister Michael Wilson\\nsaid...    63.xml\n",
       "...                                                 ...       ...\n",
       "4970  The World Bank is issuing a\\nmulticurrency fin...  2119.xml\n",
       "4971  </BODY>Shr loss 2.03 dlrs vs loss 85 cts\\n    ...  1438.xml\n",
       "4972  Chrysler Corp's Chrysler Motors unit\\nsaid it ...  4540.xml\n",
       "4973  P.A.M. Transportation Services\\nInc said presi...  2131.xml\n",
       "4974  America West Airlines said its\\nFebruary load ...  2657.xml\n",
       "4975  David Rockefeller, chairman of the\\ninternatio...  4226.xml\n",
       "4976  Canadian Prime Minister Brian Mulroney\\nannoun...  3549.xml\n",
       "4977  Commercial Credit Co is raising 150 mln\\ndlrs ...   812.xml\n",
       "4978  Arvin Industries Inc said L.K.\\nEvans has been...    52.xml\n",
       "4979  Tonka Corp said it agreed to\\nbecome the exclu...   806.xml\n",
       "4980  Sugar imports subject to the U.S.\\nsugar impor...    46.xml\n",
       "4981  <IBM Canada Ltd> said it filed a lawsuit\\nin t...  2643.xml\n",
       "4982  Phillips Petroleum Co will emphasize\\nimprovin...  4232.xml\n",
       "4983  Qtr ends Jan 30\\n    Oper shr loss 45 cts vs p...  4554.xml\n",
       "4984                                                NaN  2125.xml\n",
       "4985  A senior official of Drexel Burnham\\nLambert I...   153.xml\n",
       "4986                                                NaN  2870.xml\n",
       "4987  U.S. semiconductor manufacturers,\\nstruggling ...   635.xml\n",
       "4988  Shr profit six cts vs profit eight cts\\n    Ne...   621.xml\n",
       "4989  Advocates of a 0/92 plan for\\nfeedgrains will ...  2864.xml\n",
       "4990  Torchmark Corp said its board\\nauthorized the ...   147.xml\n",
       "4991  <Counsel Corp> said it plans a\\nthree-for-two ...  4583.xml\n",
       "4992  Consolidated Natural Gas\\nSystem's Consolidate...    91.xml\n",
       "4993  </BODY>Interface Flooring Systems Inc\\nsaid it...   609.xml\n",
       "4994  Moody's Investors Service Inc said it\\ndowngra...  2694.xml\n",
       "4995  Treasury balances at the Federal\\nReserve fell...  2858.xml\n",
       "4996  The Finance Ministry said it set a price\\nof 3...  2680.xml\n",
       "4997  Qtly div 25 cts vs 25 cts prior\\n    Pay April...    85.xml\n",
       "4998  British Aerospace inc said it would\\nhold a ne...  1389.xml\n",
       "4999  Federal Reserve Board Chairman Paul\\nVolcker s...  4597.xml\n",
       "\n",
       "[5000 rows x 2 columns]>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xml.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1435.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4559.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>189.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4565.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3975.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2302.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1780.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4808.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2909.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>NaN</td>\n",
       "      <td>994.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2060.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3419.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3343.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>NaN</td>\n",
       "      <td>981.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2263.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4148.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2465.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3035.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4799.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>NaN</td>\n",
       "      <td>76.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1346.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4558.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>NaN</td>\n",
       "      <td>611.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3586.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2698.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>NaN</td>\n",
       "      <td>607.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3235.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1436.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2103.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>92.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4680</th>\n",
       "      <td>NaN</td>\n",
       "      <td>608.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2124.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1411.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2497.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4757.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4743.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1613.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3616.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4806</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2939.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4807</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4390.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4816</th>\n",
       "      <td>NaN</td>\n",
       "      <td>973.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4409.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>NaN</td>\n",
       "      <td>796.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4434.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2723.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>NaN</td>\n",
       "      <td>972.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4877</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3165.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4150.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3159.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1969.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2290.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4913</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1799.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>NaN</td>\n",
       "      <td>392.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1612.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2327.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4938</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2441.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4959.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2125.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2870.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     content      file\n",
       "14       NaN  1435.xml\n",
       "24       NaN  4559.xml\n",
       "36       NaN    77.xml\n",
       "38       NaN   189.xml\n",
       "40       NaN  4565.xml\n",
       "59       NaN  3975.xml\n",
       "75       NaN  2302.xml\n",
       "85       NaN  1780.xml\n",
       "99       NaN  4808.xml\n",
       "131      NaN  2909.xml\n",
       "132      NaN   994.xml\n",
       "156      NaN  2060.xml\n",
       "159      NaN  3419.xml\n",
       "172      NaN  3343.xml\n",
       "179      NaN   981.xml\n",
       "193      NaN  2263.xml\n",
       "208      NaN  4148.xml\n",
       "239      NaN  2465.xml\n",
       "243      NaN  3035.xml\n",
       "262      NaN  4799.xml\n",
       "274      NaN    76.xml\n",
       "290      NaN  1346.xml\n",
       "292      NaN  4558.xml\n",
       "303      NaN   611.xml\n",
       "309      NaN  3586.xml\n",
       "311      NaN  2698.xml\n",
       "321      NaN   607.xml\n",
       "326      NaN  3235.xml\n",
       "338      NaN  1436.xml\n",
       "352      NaN  2103.xml\n",
       "...      ...       ...\n",
       "4669     NaN    92.xml\n",
       "4680     NaN   608.xml\n",
       "4692     NaN  2124.xml\n",
       "4709     NaN  1411.xml\n",
       "4721     NaN  2497.xml\n",
       "4735     NaN  4757.xml\n",
       "4744     NaN  4743.xml\n",
       "4746     NaN  1613.xml\n",
       "4791     NaN  3616.xml\n",
       "4806     NaN  2939.xml\n",
       "4807     NaN  4390.xml\n",
       "4816     NaN   973.xml\n",
       "4832     NaN  4409.xml\n",
       "4846     NaN   796.xml\n",
       "4850     NaN  4434.xml\n",
       "4852     NaN  2723.xml\n",
       "4854     NaN   972.xml\n",
       "4877     NaN  3165.xml\n",
       "4887     NaN  4150.xml\n",
       "4888     NaN  3159.xml\n",
       "4900     NaN  1969.xml\n",
       "4910     NaN  2290.xml\n",
       "4913     NaN  1799.xml\n",
       "4921     NaN   392.xml\n",
       "4925     NaN  1612.xml\n",
       "4933     NaN  2327.xml\n",
       "4938     NaN  2441.xml\n",
       "4953     NaN  4959.xml\n",
       "4984     NaN  2125.xml\n",
       "4986     NaN  2870.xml\n",
       "\n",
       "[416 rows x 2 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xml[df_xml['content'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge dataframe contents with datafram train labels on file name column which will result into a new dataframe with 4 different columns. Because in our training data, we only have `4,800` items so our resulted file will have that same number. `200` remaining items are in the test set which we haven't touched yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>Showers continued throughout the week in\\nthe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>Standard Oil Co and BP North America\\nInc said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>Texas Commerce Bancshares Inc's Texas\\nCommerc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>BankAmerica Corp is not under\\npressure to act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   file  label                                            content\n",
       "0   1  1.xml      0  Showers continued throughout the week in\\nthe ...\n",
       "1   2  2.xml      0  Standard Oil Co and BP North America\\nInc said...\n",
       "2   3  3.xml      0  Texas Commerce Bancshares Inc's Texas\\nCommerc...\n",
       "3   4  4.xml      0  BankAmerica Corp is not under\\npressure to act...\n",
       "4   5  5.xml      0  The U.S. Agriculture Department\\nreported the ..."
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df_train,df_xml, on=['file'])\n",
    "df = df.rename(columns={'earnings: 0 no/ 1 yes': 'label'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         id      file  earnings: 0 no/ 1 yes\n",
       "0        1     1.xml                      0\n",
       "1        2     2.xml                      0\n",
       "2        3     3.xml                      0\n",
       "3        4     4.xml                      0\n",
       "4        5     5.xml                      0\n",
       "5        6     6.xml                      0\n",
       "6        7     7.xml                      0\n",
       "7        8     8.xml                      0\n",
       "8        9     9.xml                      1\n",
       "9       10    10.xml                      0\n",
       "10      11    11.xml                      1\n",
       "11      12    12.xml                      1\n",
       "12      13    13.xml                      1\n",
       "13      14    14.xml                      1\n",
       "14      15    15.xml                      0\n",
       "15      16    16.xml                      0\n",
       "16      17    17.xml                      0\n",
       "17      18    18.xml                      1\n",
       "18      19    19.xml                      0\n",
       "19      21    21.xml                      0\n",
       "20      22    22.xml                      0\n",
       "21      23    23.xml                      1\n",
       "22      24    24.xml                      1\n",
       "23      25    25.xml                      0\n",
       "24      26    26.xml                      0\n",
       "25      27    27.xml                      1\n",
       "26      28    28.xml                      0\n",
       "27      29    29.xml                      0\n",
       "28      30    30.xml                      0\n",
       "29      31    31.xml                      0\n",
       "...    ...       ...                    ...\n",
       "4770  4971  4971.xml                      0\n",
       "4771  4972  4972.xml                      1\n",
       "4772  4973  4973.xml                      0\n",
       "4773  4974  4974.xml                      1\n",
       "4774  4975  4975.xml                      0\n",
       "4775  4976  4976.xml                      0\n",
       "4776  4977  4977.xml                      0\n",
       "4777  4978  4978.xml                      0\n",
       "4778  4979  4979.xml                      0\n",
       "4779  4980  4980.xml                      0\n",
       "4780  4981  4981.xml                      0\n",
       "4781  4982  4982.xml                      0\n",
       "4782  4983  4983.xml                      0\n",
       "4783  4984  4984.xml                      0\n",
       "4784  4985  4985.xml                      0\n",
       "4785  4986  4986.xml                      0\n",
       "4786  4987  4987.xml                      0\n",
       "4787  4988  4988.xml                      0\n",
       "4788  4989  4989.xml                      0\n",
       "4789  4990  4990.xml                      1\n",
       "4790  4991  4991.xml                      1\n",
       "4791  4992  4992.xml                      0\n",
       "4792  4993  4993.xml                      0\n",
       "4793  4994  4994.xml                      0\n",
       "4794  4995  4995.xml                      0\n",
       "4795  4996  4996.xml                      0\n",
       "4796  4997  4997.xml                      0\n",
       "4797  4998  4998.xml                      1\n",
       "4798  4999  4999.xml                      1\n",
       "4799  5000  5000.xml                      0\n",
       "\n",
       "[4800 rows x 3 columns]>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving them into csv format that can be used later for training prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('news_datafile.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the number of articles with `0` labels outnumbered that of `1` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3825\n",
       "1     975\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop all `NaN` values in the dataset because we want to build the word embeddings which is based on the words so any empty contents will not help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>30.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>31.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>76</td>\n",
       "      <td>76.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>77</td>\n",
       "      <td>77.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>78</td>\n",
       "      <td>78.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>92</td>\n",
       "      <td>92.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>94</td>\n",
       "      <td>94.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>95</td>\n",
       "      <td>95.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>99</td>\n",
       "      <td>99.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>101</td>\n",
       "      <td>101.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>102</td>\n",
       "      <td>102.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>103</td>\n",
       "      <td>103.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>129</td>\n",
       "      <td>129.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>132</td>\n",
       "      <td>132.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>133</td>\n",
       "      <td>133.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>171</td>\n",
       "      <td>171.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>189</td>\n",
       "      <td>189.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>216</td>\n",
       "      <td>216.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>261</td>\n",
       "      <td>261.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>283</td>\n",
       "      <td>283.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>298</td>\n",
       "      <td>298.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>383</td>\n",
       "      <td>383.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>392</td>\n",
       "      <td>392.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>396</td>\n",
       "      <td>396.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>398</td>\n",
       "      <td>398.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>400</td>\n",
       "      <td>400.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>417</td>\n",
       "      <td>417.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>418</td>\n",
       "      <td>418.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>430</td>\n",
       "      <td>430.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>431</td>\n",
       "      <td>431.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>4762</td>\n",
       "      <td>4762.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>4763</td>\n",
       "      <td>4763.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>4770</td>\n",
       "      <td>4770.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>4782</td>\n",
       "      <td>4782.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>4786</td>\n",
       "      <td>4786.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>4799</td>\n",
       "      <td>4799.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>4808</td>\n",
       "      <td>4808.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>4828</td>\n",
       "      <td>4828.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>4831</td>\n",
       "      <td>4831.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>4832</td>\n",
       "      <td>4832.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>4836</td>\n",
       "      <td>4836.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4642</th>\n",
       "      <td>4840</td>\n",
       "      <td>4840.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>4843</td>\n",
       "      <td>4843.xml</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4661</th>\n",
       "      <td>4859</td>\n",
       "      <td>4859.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>4862</td>\n",
       "      <td>4862.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>4865</td>\n",
       "      <td>4865.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4681</th>\n",
       "      <td>4880</td>\n",
       "      <td>4880.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>4889</td>\n",
       "      <td>4889.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>4904</td>\n",
       "      <td>4904.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>4913</td>\n",
       "      <td>4913.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4725</th>\n",
       "      <td>4924</td>\n",
       "      <td>4924.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4726</th>\n",
       "      <td>4925</td>\n",
       "      <td>4925.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>4928</td>\n",
       "      <td>4928.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>4929</td>\n",
       "      <td>4929.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>4949</td>\n",
       "      <td>4949.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>4950</td>\n",
       "      <td>4950.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>4959</td>\n",
       "      <td>4959.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4760</th>\n",
       "      <td>4961</td>\n",
       "      <td>4961.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>4977</td>\n",
       "      <td>4977.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      file  label content\n",
       "28      30    30.xml      0     NaN\n",
       "29      31    31.xml      0     NaN\n",
       "71      76    76.xml      0     NaN\n",
       "72      77    77.xml      0     NaN\n",
       "73      78    78.xml      0     NaN\n",
       "86      92    92.xml      0     NaN\n",
       "88      94    94.xml      0     NaN\n",
       "89      95    95.xml      0     NaN\n",
       "92      99    99.xml      0     NaN\n",
       "94     101   101.xml      0     NaN\n",
       "95     102   102.xml      0     NaN\n",
       "96     103   103.xml      0     NaN\n",
       "121    129   129.xml      1     NaN\n",
       "124    132   132.xml      0     NaN\n",
       "125    133   133.xml      0     NaN\n",
       "162    171   171.xml      0     NaN\n",
       "180    189   189.xml      0     NaN\n",
       "204    216   216.xml      0     NaN\n",
       "248    261   261.xml      0     NaN\n",
       "270    283   283.xml      0     NaN\n",
       "285    298   298.xml      0     NaN\n",
       "369    383   383.xml      0     NaN\n",
       "378    392   392.xml      1     NaN\n",
       "382    396   396.xml      1     NaN\n",
       "383    398   398.xml      0     NaN\n",
       "385    400   400.xml      0     NaN\n",
       "401    417   417.xml      0     NaN\n",
       "402    418   418.xml      0     NaN\n",
       "414    430   430.xml      0     NaN\n",
       "415    431   431.xml      0     NaN\n",
       "...    ...       ...    ...     ...\n",
       "4565  4762  4762.xml      0     NaN\n",
       "4566  4763  4763.xml      0     NaN\n",
       "4573  4770  4770.xml      0     NaN\n",
       "4585  4782  4782.xml      1     NaN\n",
       "4589  4786  4786.xml      0     NaN\n",
       "4602  4799  4799.xml      0     NaN\n",
       "4611  4808  4808.xml      0     NaN\n",
       "4630  4828  4828.xml      0     NaN\n",
       "4633  4831  4831.xml      0     NaN\n",
       "4634  4832  4832.xml      0     NaN\n",
       "4638  4836  4836.xml      0     NaN\n",
       "4642  4840  4840.xml      0     NaN\n",
       "4645  4843  4843.xml      1     NaN\n",
       "4661  4859  4859.xml      0     NaN\n",
       "4664  4862  4862.xml      0     NaN\n",
       "4667  4865  4865.xml      0     NaN\n",
       "4681  4880  4880.xml      0     NaN\n",
       "4690  4889  4889.xml      0     NaN\n",
       "4705  4904  4904.xml      0     NaN\n",
       "4714  4913  4913.xml      0     NaN\n",
       "4725  4924  4924.xml      0     NaN\n",
       "4726  4925  4925.xml      0     NaN\n",
       "4729  4928  4928.xml      0     NaN\n",
       "4730  4929  4929.xml      0     NaN\n",
       "4749  4949  4949.xml      0     NaN\n",
       "4750  4950  4950.xml      0     NaN\n",
       "4759  4959  4959.xml      0     NaN\n",
       "4760  4961  4961.xml      0     NaN\n",
       "4776  4977  4977.xml      0     NaN\n",
       "4799  5000  5000.xml      0     NaN\n",
       "\n",
       "[398 rows x 4 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['content'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         id      file  label                                            content\n",
       "0        1     1.xml      0  Showers continued throughout the week in\\nthe ...\n",
       "1        2     2.xml      0  Standard Oil Co and BP North America\\nInc said...\n",
       "2        3     3.xml      0  Texas Commerce Bancshares Inc's Texas\\nCommerc...\n",
       "3        4     4.xml      0  BankAmerica Corp is not under\\npressure to act...\n",
       "4        5     5.xml      0  The U.S. Agriculture Department\\nreported the ...\n",
       "5        6     6.xml      0  Argentine grain board figures show\\ncrop regis...\n",
       "6        7     7.xml      0  Red Lion Inns Limited Partnership\\nsaid it fil...\n",
       "7        8     8.xml      0  Moody's Investors Service Inc said it\\nlowered...\n",
       "8        9     9.xml      1  Champion Products Inc said its\\nboard of direc...\n",
       "9       10    10.xml      0  Computer Terminal Systems Inc said\\nit has com...\n",
       "10      11    11.xml      1  Shr 34 cts vs 1.19 dlrs\\n    Net 807,000 vs 2,...\n",
       "11      12    12.xml      1  Ohio Mattress Co said its first\\nquarter, endi...\n",
       "12      13    13.xml      1  Oper shr loss two cts vs profit seven cts\\n   ...\n",
       "13      14    14.xml      1  Shr one dlr vs 73 cts\\n    Net 12.6 mln vs 15....\n",
       "14      15    15.xml      0  National Intergroup Inc said it plans\\nto file...\n",
       "15      16    16.xml      0  BankAmerica Corp is not under\\npressure to act...\n",
       "16      17    17.xml      0  National Health Enhancement\\nSystems Inc said ...\n",
       "17      18    18.xml      1  Dean Foods Co expects earnings for the\\nfourth...\n",
       "18      19    19.xml      0  The Commodity Credit Corporation, CCC,\\nhas ac...\n",
       "19      21    21.xml      0  Hughes/Conserdyne Corp, a\\nunit of <Hughes Cap...\n",
       "20      22    22.xml      0  Magma Copper Co, a subsidiary of Newmont\\nMini...\n",
       "21      23    23.xml      1  Brown-Forman Inc said its board\\nhas approved ...\n",
       "22      24    24.xml      1  Shr profit 15 cts vs profit four cts\\n    Annu...\n",
       "23      25    25.xml      0  Shearson Lehman Brothers, a unit of\\nAmerican ...\n",
       "24      26    26.xml      0  Venezuela and its bank advisory\\ncommittee hav...\n",
       "25      27    27.xml      1  Shr 39 cts vs 50 cts\\n    Net 1,545,160 vs 2,1...\n",
       "26      28    28.xml      0  The Tower Commission report, which\\nsays Presi...\n",
       "27      29    29.xml      0  Sales of previously owned homes\\ndropped 14.5 ...\n",
       "30      32    32.xml      0  </BODY>Sens. Alan Cranston (D-Cal.) and\\nDanie...\n",
       "31      33    33.xml      0  Excelan Inc said it is making an\\ninitial publ...\n",
       "...    ...       ...    ...                                                ...\n",
       "4768  4969  4969.xml      0  CitiCorp <CCI> appears to be digging\\nin its h...\n",
       "4769  4970  4970.xml      0  ChemLawn corp said it has asked\\npotential pur...\n",
       "4770  4971  4971.xml      0  Central Bank President Jose Luis\\nMachinea sai...\n",
       "4771  4972  4972.xml      1  Shr loss 48 cts vs loss 10 cts\\n    Net loss 1...\n",
       "4772  4973  4973.xml      0  American Motors Corp said its\\nU.S. sales for ...\n",
       "4773  4974  4974.xml      1  Oper shr loss 44 cts vs oper loss eight cts\\n ...\n",
       "4774  4975  4975.xml      0  J.P. Morgan and Co said Peter F.\\nCulver, 42, ...\n",
       "4775  4976  4976.xml      0  SmartNames Inc said it reached\\nan agreement i...\n",
       "4777  4978  4978.xml      0  </BODY>Auxton Computer Enterprises Inc\\nsaid i...\n",
       "4778  4979  4979.xml      0  Standard and Poor's Corp said it\\nexpects the ...\n",
       "4779  4980  4980.xml      0  H.K. Porter Inc Co said it will\\nredeem its 5-...\n",
       "4780  4981  4981.xml      0  Atlantic Richfield Co said it sold\\nits Plan C...\n",
       "4781  4982  4982.xml      0  GTE Corp said it supports\\nrecommendations by ...\n",
       "4782  4983  4983.xml      0  Construction workers today began building\\na 2...\n",
       "4783  4984  4984.xml      0  The U.S. Agriculture Department's\\nExport Enha...\n",
       "4784  4985  4985.xml      0  <Drexel Burnham Lambert Inc> said\\nduring  the...\n",
       "4785  4986  4986.xml      0  Chrysler Corp's Chrysler Motors unit\\nsaid it ...\n",
       "4786  4987  4987.xml      0  The Netherlands recorded a flat trade\\nbalance...\n",
       "4787  4988  4988.xml      0  A greater than anticipated need,\\ncompetitive ...\n",
       "4788  4989  4989.xml      0  Fairchild Industries Inc said\\nit will close t...\n",
       "4789  4990  4990.xml      1  Qtly div 35 cts vs 35 cts previously\\n    Pay ...\n",
       "4790  4991  4991.xml      1  Qtly div Class B 1-1/2 cts vs 1-1/2 cts prior\\...\n",
       "4791  4992  4992.xml      0  Texas Utilities Co's TU Electric unit\\nsaid it...\n",
       "4792  4993  4993.xml      0  The growth rate of Brazilian\\nindustrial outpu...\n",
       "4793  4994  4994.xml      0  RJR Nabisco Inc is raising 500 mln\\ndlrs via a...\n",
       "4794  4995  4995.xml      0  Britain wants Japan to agree a timetable\\nfor ...\n",
       "4795  4996  4996.xml      0  Loadmaster Systems Inc said Mark\\nA Cote has b...\n",
       "4796  4997  4997.xml      0  Moody's Investors Service Inc said it\\nmay dow...\n",
       "4797  4998  4998.xml      1  Qtly div 15 cts vs 15 cts prior\\n    Pay April...\n",
       "4798  4999  4999.xml      1  Shr 25 cts vs 18 cts\\n    Net 109,131 vs 75,79...\n",
       "\n",
       "[4402 rows x 4 columns]>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have new dataframe with `4,402` items without any `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4402, 4)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\\n    ',\n",
       "       'Standard Oil Co and BP North America\\nInc said they plan to form a venture to manage the money market\\nborrowing and investment activities of both companies.\\n    BP North America is a subsidiary of British Petroleum Co\\nPlc <BP>, which also owns a 55 pct interest in Standard Oil.\\n    The venture will be called BP/Standard Financial Trading\\nand will be operated by Standard Oil under the oversight of a\\njoint management committee.\\n\\n Reuter\\n    ',\n",
       "       \"Texas Commerce Bancshares Inc's Texas\\nCommerce Bank-Houston said it filed an application with the\\nComptroller of the Currency in an effort to create the largest\\nbanking network in Harris County.\\n    The bank said the network would link 31 banks having\\n13.5 billion dlrs in assets and 7.5 billion dlrs in deposits.\\n       \\n Reuter\\n    \",\n",
       "       'BankAmerica Corp is not under\\npressure to act quickly on its proposed equity offering and\\nwould do well to delay it because of the stock\\'s recent poor\\nperformance, banking analysts said.\\n    Some analysts said they have recommended BankAmerica delay\\nits up to one-billion-dlr equity offering, which has yet to be\\napproved by the Securities and Exchange Commission.\\n    BankAmerica stock fell this week, along with other banking\\nissues, on the news that Brazil has suspended interest payments\\non a large portion of its foreign debt.\\n    The stock traded around 12, down 1/8, this afternoon,\\nafter falling to 11-1/2 earlier this week on the news.\\n    Banking analysts said that with the immediate threat of the\\nFirst Interstate Bancorp <I> takeover bid gone, BankAmerica is\\nunder no pressure to sell the securities into a market that\\nwill be nervous on bank stocks in the near term.\\n    BankAmerica filed the offer on January 26. It was seen as\\none of the major factors leading the First Interstate\\nwithdrawing its takeover bid on February 9.\\n    A BankAmerica spokesman said SEC approval is taking longer\\nthan expected and market conditions must now be re-evaluated.\\n    \"The circumstances at the time will determine what we do,\"\\nsaid Arthur Miller, BankAmerica\\'s Vice President for Financial\\nCommunications, when asked if BankAmerica would proceed with\\nthe offer immediately after it receives SEC approval.\\n    \"I\\'d put it off as long as they conceivably could,\" said\\nLawrence Cohn, analyst with Merrill Lynch, Pierce, Fenner and\\nSmith.\\n    Cohn said the longer BankAmerica waits, the longer they\\nhave to show the market an improved financial outlook.\\n    Although BankAmerica has yet to specify the types of\\nequities it would offer, most analysts believed a convertible\\npreferred stock would encompass at least part of it.\\n    Such an offering at a depressed stock price would mean a\\nlower conversion price and more dilution to BankAmerica stock\\nholders, noted Daniel Williams, analyst with Sutro Group.\\n    Several analysts said that while they believe the Brazilian\\ndebt problem will continue to hang over the banking industry\\nthrough the quarter, the initial shock reaction is likely to\\nease over the coming weeks.\\n    Nevertheless, BankAmerica, which holds about 2.70 billion\\ndlrs in Brazilian loans, stands to lose 15-20 mln dlrs if the\\ninterest rate is reduced on the debt, and as much as 200 mln\\ndlrs if Brazil pays no interest for a year, said Joseph\\nArsenio, analyst with Birr, Wilson and Co.\\n    He noted, however, that any potential losses would not show\\nup in the current quarter.\\n    With other major banks standing to lose even more than\\nBankAmerica if Brazil fails to service its debt, the analysts\\nsaid they expect the debt will be restructured, similar to way\\nMexico\\'s debt was, minimizing losses to the creditor banks.\\n Reuter\\n    ',\n",
       "       'The U.S. Agriculture Department\\nreported the farmer-owned reserve national five-day average\\nprice through February 25 as follows (Dlrs/Bu-Sorghum Cwt) -\\n         Natl   Loan           Release   Call\\n         Avge   Rate-X  Level    Price  Price\\n Wheat   2.55   2.40       IV     4.65     --\\n                            V     4.65     --\\n                           VI     4.45     --\\n Corn    1.35   1.92       IV     3.15   3.15\\n                            V     3.25     --\\n X - 1986 Rates.\\n\\n          Natl   Loan          Release   Call\\n          Avge   Rate-X  Level   Price  Price\\n Oats     1.24   0.99        V    1.65    -- \\n Barley   n.a.   1.56       IV    2.55   2.55\\n                             V    2.65    -- \\n Sorghum  2.34   3.25-Y     IV    5.36   5.36\\n                             V    5.54    -- \\n    Reserves I, II and III have matured. Level IV reflects\\ngrain entered after Oct 6, 1981 for feedgrain and after July\\n23, 1981 for wheat. Level V wheat/barley after 5/14/82,\\ncorn/sorghum after 7/1/82. Level VI covers wheat entered after\\nJanuary 19, 1984.  X-1986 rates. Y-dlrs per CWT (100 lbs).\\nn.a.-not available.\\n Reuter\\n    ',\n",
       "       'Argentine grain board figures show\\ncrop registrations of grains, oilseeds and their products to\\nFebruary 11, in thousands of tonnes, showing those for futurE\\nshipments month, 1986/87 total and 1985/86 total to February\\n12, 1986, in brackets:\\n    Bread wheat prev 1,655.8, Feb 872.0, March 164.6, total\\n2,692.4 (4,161.0).\\n    Maize Mar 48.0, total 48.0 (nil).\\n    Sorghum nil (nil)\\n    Oilseed export registrations were:\\n    Sunflowerseed total 15.0 (7.9)\\n    Soybean May 20.0, total 20.0 (nil)\\n    The board also detailed export registrations for\\nsubproducts, as follows,\\n    SUBPRODUCTS\\n    Wheat prev 39.9, Feb 48.7, March 13.2, Apr 10.0, total\\n111.8 (82.7) .\\n    Linseed prev 34.8, Feb 32.9, Mar 6.8, Apr 6.3, total 80.8\\n(87.4).\\n    Soybean prev 100.9, Feb 45.1, MAr nil, Apr nil, May 20.0,\\ntotal 166.1 (218.5).\\n    Sunflowerseed prev 48.6, Feb 61.5, Mar 25.1, Apr 14.5,\\ntotal 149.8 (145.3).\\n    Vegetable oil registrations were :         \\n    Sunoil prev 37.4, Feb 107.3, Mar 24.5, Apr 3.2, May nil,\\nJun 10.0, total 182.4 (117.6).                  \\n    Linoil prev 15.9, Feb 23.6, Mar 20.4, Apr 2.0, total 61.8,\\n(76.1).                         \\n    Soybean oil prev 3.7, Feb 21.1, Mar nil, Apr 2.0, May 9.0,\\nJun 13.0, Jul 7.0, total 55.8 (33.7).        REUTER\\n    ',\n",
       "       'Red Lion Inns Limited Partnership\\nsaid it filed a registration statement with the Securities and\\nExchange Commission covering a proposed offering of 4,790,000\\nunits of limited partnership interests.\\n    The company said it expects the offering to be priced at 20\\ndlrs per unit.\\n    It said proceeds from the offering, along with a 102.5 mln\\ndlr mortgage loan, will be used to finance its planned\\nacquisition of 10 Red Lion hotels.\\n Reuter\\n    ',\n",
       "       \"Moody's Investors Service Inc said it\\nlowered the debt and preferred stock ratings of USX Corp and\\nits units. About seven billion dlrs of securities is affected.\\n    Moody's said Marathon Oil Co's recent establishment of up\\nto one billion dlrs in production payment facilities on its\\nprolific Yates Field has significant negative implications for\\nUSX's unsecured creditors.\\n    The company appears to have positioned its steel segment\\nfor a return to profit by late 1987, Moody's added.\\n    Ratings lowered include those on USX's senior debt to BA-1\\nfrom BAA-3.\\n Reuter\\n    \",\n",
       "       'Champion Products Inc said its\\nboard of directors approved a two-for-one stock split of its\\ncommon shares for shareholders of record as of April 1, 1987.\\n    The company also said its board voted to recommend to\\nshareholders at the annual meeting April 23 an increase in the\\nauthorized capital stock from five mln to 25 mln shares.\\n Reuter\\n    ',\n",
       "       \"Computer Terminal Systems Inc said\\nit has completed the sale of 200,000 shares of its common\\nstock, and warrants to acquire an additional one mln shares, to\\n<Sedio N.V.> of Lugano, Switzerland for 50,000 dlrs.\\n    The company said the warrants are exercisable for five\\nyears at a purchase price of .125 dlrs per share.\\n    Computer Terminal said Sedio also has the right to buy\\nadditional shares and increase its total holdings up to 40 pct\\nof the Computer Terminal's outstanding common stock under\\ncertain circumstances involving change of control at the\\ncompany.\\n    The company said if the conditions occur the warrants would\\nbe exercisable at a price equal to 75 pct of its common stock's\\nmarket price at the time, not to exceed 1.50 dlrs per share.\\n    Computer Terminal also said it sold the technolgy rights to\\nits Dot Matrix impact technology, including any future\\nimprovements, to <Woodco Inc> of Houston, Tex. for 200,000\\ndlrs. But, it said it would continue to be the exclusive\\nworldwide licensee of the technology for Woodco.\\n    The company said the moves were part of its reorganization\\nplan and would help pay current operation costs and ensure\\nproduct delivery.\\n    Computer Terminal makes computer generated labels, forms,\\ntags and ticket printers and terminals.\\n Reuter\\n    \"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.content[0:10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings with `Gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data\n",
    "We will define a function to convert sentences into a list of words as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: [1]\n",
    "import re, nltk\n",
    "special_characters = re.compile(\"[^A-Za-z0-9 ]\")\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "def convert_to_sentences(data, tokenizer):\n",
    "    # First, converting each review into sentences\n",
    "    # Use NLTK Tokenizer to split review into sentences (punkt tokenizer - english.pickle)\n",
    "    data = data.lower().replace(\"<i>\", \"\")\n",
    "    data = data.replace(\"    \", \" \")\n",
    "    data = data.replace(\"\\n\", \". \")\n",
    "    #data = data.replace(\".\", \". \")\n",
    "    data = data.replace(\"reuter\", \"\")\n",
    "    #print(data)\n",
    "    data = re.sub(\"  \", \" \", data)\n",
    "    all_sentences = tokenizer.tokenize(data.strip())\n",
    "    # Second, converting each sentence into words\n",
    "    sentences = []\n",
    "    for words in all_sentences:\n",
    "        s = re.sub(special_characters, \"\", words.lower())\n",
    "        if (len(s)) > 0:\n",
    "            sentences.append(s.split())\n",
    "    # Finally, returning a list of sentences (containing words in each sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.xml</td>\n",
       "      <td>0</td>\n",
       "      <td>The U.S. Agriculture Department\\nreported the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   file  label                                            content\n",
       "4   5  5.xml      0  The U.S. Agriculture Department\\nreported the ..."
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new['file']=='5.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oper shr loss two cts vs profit seven cts\\n    Oper shr profit 442,000 vs profit 2,986,000\\n    Revs 291.8 mln vs 151.1 mln\\n    Avg shrs 51.7 mln vs 43.4 mln\\n    Six mths\\n    Oper shr profit nil vs profit 12 cts\\n    Oper net profit 3,376,000 vs profit 5,086,000\\n    Revs 569.3 mln vs 298.5 mln\\n    Avg shrs 51.6 mln vs 41.1 mln\\n    NOTE: Per shr calculated after payment of preferred\\ndividends.\\n    Results exclude credits of 2,227,000 or four cts and\\n4,841,000 or nine cts for 1986 qtr and six mths vs 2,285,000 or\\nsix cts and 4,104,000 or 11 cts for prior periods from\\noperating loss carryforwards.\\n Reuter\\n    '"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['content'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample result of item at index `12` in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oper', 'shr', 'loss', 'two', 'cts', 'vs', 'profit', 'seven', 'cts']\n",
      "['oper', 'shr', 'profit', '442000', 'vs', 'profit', '2986000', 'revs', '2918', 'mln', 'vs', '1511', 'mln']\n",
      "['avg', 'shrs', '517', 'mln', 'vs', '434', 'mln']\n",
      "['six', 'mths']\n",
      "['oper', 'shr', 'profit', 'nil', 'vs', 'profit', '12', 'cts']\n",
      "['oper', 'net', 'profit', '3376000', 'vs', 'profit', '5086000', 'revs', '5693', 'mln', 'vs', '2985', 'mln']\n",
      "['avg', 'shrs', '516', 'mln', 'vs', '411', 'mln']\n",
      "['note', 'per', 'shr', 'calculated', 'after', 'payment', 'of', 'preferred']\n",
      "['dividends', 'results', 'exclude', 'credits', 'of', '2227000', 'or', 'four', 'cts', 'and']\n",
      "['4841000', 'or', 'nine', 'cts', 'for', '1986', 'qtr', 'and', 'six', 'mths', 'vs', '2285000', 'or']\n",
      "['six', 'cts', 'and', '4104000', 'or', '11', 'cts', 'for', 'prior', 'periods', 'from']\n",
      "['operating', 'loss', 'carryforwards']\n"
     ]
    }
   ],
   "source": [
    "sample = convert_to_sentences(df_new['content'][12],tokenizer)\n",
    "for i in sample:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5.xml',\n",
       " 0,\n",
       " 'The U.S. Agriculture Department\\nreported the farmer-owned reserve national five-day average\\nprice through February 25 as follows (Dlrs/Bu-Sorghum Cwt) -\\n         Natl   Loan           Release   Call\\n         Avge   Rate-X  Level    Price  Price\\n Wheat   2.55   2.40       IV     4.65     --\\n                            V     4.65     --\\n                           VI     4.45     --\\n Corn    1.35   1.92       IV     3.15   3.15\\n                            V     3.25     --\\n X - 1986 Rates.\\n\\n          Natl   Loan          Release   Call\\n          Avge   Rate-X  Level   Price  Price\\n Oats     1.24   0.99        V    1.65    -- \\n Barley   n.a.   1.56       IV    2.55   2.55\\n                             V    2.65    -- \\n Sorghum  2.34   3.25-Y     IV    5.36   5.36\\n                             V    5.54    -- \\n    Reserves I, II and III have matured. Level IV reflects\\ngrain entered after Oct 6, 1981 for feedgrain and after July\\n23, 1981 for wheat. Level V wheat/barley after 5/14/82,\\ncorn/sorghum after 7/1/82. Level VI covers wheat entered after\\nJanuary 19, 1984.  X-1986 rates. Y-dlrs per CWT (100 lbs).\\nn.a.-not available.\\n Reuter\\n    ')"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['file'][4],df_new['label'][4],df_new['content'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the u.s. agriculture department. reported the farmer-owned reserve national five-day average. price through february 25 as follows (dlrs/bu-sorghum cwt) -.    natl   loan     release   call.    avge   rate-x  level price  price.  wheat   2.55   2.40    iv  4.65  --.        v  4.65  --.          vi  4.45  --.  corn 1.35   1.92    iv  3.15   3.15.        v  3.25  --.  x - 1986 rates.. .     natl   loan    release   call.     avge   rate-x  level   price  price.  oats  1.24   0.99  v 1.65 -- .  barley   n.a.   1.56    iv 2.55   2.55.         v 2.65 -- .  sorghum  2.34   3.25-y  iv 5.36   5.36.         v 5.54 -- .  reserves i, ii and iii have matured. level iv reflects. grain entered after oct 6, 1981 for feedgrain and after july. 23, 1981 for wheat. level v wheat/barley after 5/14/82,. corn/sorghum after 7/1/82. level vi covers wheat entered after. january 19, 1984.  x-1986 rates. y-dlrs per cwt (100 lbs).. n.a.-not available..  .  \n",
      "['the', 'us', 'agriculture', 'department']\n",
      "['reported', 'the', 'farmerowned', 'reserve', 'national', 'fiveday', 'average']\n",
      "['price', 'through', 'february', '25', 'as', 'follows', 'dlrsbusorghum', 'cwt']\n",
      "['natl', 'loan', 'release', 'call']\n",
      "['avge', 'ratex', 'level', 'price', 'price']\n",
      "['wheat', '255', '240', 'iv', '465']\n",
      "['v', '465']\n",
      "['vi', '445']\n",
      "['corn', '135', '192', 'iv', '315', '315', 'v', '325']\n",
      "['x', '1986', 'rates']\n",
      "['natl', 'loan', 'release', 'call']\n",
      "['avge', 'ratex', 'level', 'price', 'price']\n",
      "['oats', '124', '099', 'v', '165']\n",
      "['barley', 'na']\n",
      "['156', 'iv', '255', '255', 'v', '265']\n",
      "['sorghum', '234', '325y', 'iv', '536', '536', 'v', '554']\n",
      "['reserves', 'i', 'ii', 'and', 'iii', 'have', 'matured']\n",
      "['level', 'iv', 'reflects']\n",
      "['grain', 'entered', 'after', 'oct', '6', '1981', 'for', 'feedgrain', 'and', 'after', 'july']\n",
      "['23', '1981', 'for', 'wheat']\n",
      "['level', 'v', 'wheatbarley', 'after', '51482']\n",
      "['cornsorghum', 'after', '7182']\n",
      "['level', 'vi', 'covers', 'wheat', 'entered', 'after']\n",
      "['january', '19', '1984', 'x1986', 'rates']\n",
      "['ydlrs', 'per', 'cwt', '100', 'lbs', 'nanot', 'available']\n"
     ]
    }
   ],
   "source": [
    "sample = convert_to_sentences(df_new['content'][4],tokenizer)\n",
    "for i in sample:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for content in df_new.content:\n",
    "    sentences += convert_to_sentences(content, tokenizer)\n",
    "print(\"Done processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['showers', 'continued', 'throughout', 'the', 'week', 'in']\n",
      "\n",
      "['the', 'bahia', 'cocoa', 'zone', 'alleviating', 'the', 'drought', 'since', 'early']\n",
      "\n",
      "['january', 'and', 'improving', 'prospects', 'for', 'the', 'coming', 'temporao']\n",
      "\n",
      "['although', 'normal', 'humidity', 'levels', 'have', 'not', 'been', 'restored']\n",
      "\n",
      "['comissaria', 'smith', 'said', 'in', 'its', 'weekly', 'review', 'the', 'dry', 'period', 'means', 'the', 'temporao', 'will', 'be', 'late', 'this', 'year', 'arrivals', 'for', 'the', 'week', 'ended', 'february', '22', 'were', '155221', 'bags']\n",
      "\n",
      "['of', '60', 'kilos', 'making', 'a', 'cumulative', 'total', 'for', 'the', 'season', 'of', '593', 'mln', 'against', '581', 'at', 'the', 'same', 'stage', 'last', 'year']\n",
      "\n",
      "['again', 'it', 'seems']\n",
      "\n",
      "['that', 'cocoa', 'delivered', 'earlier', 'on', 'consignment', 'was', 'included', 'in', 'the']\n",
      "\n",
      "['arrivals', 'figures', 'comissaria', 'smith', 'said', 'there', 'is', 'still', 'some', 'doubt', 'as', 'to', 'how']\n",
      "\n",
      "['much', 'old', 'crop', 'cocoa', 'is', 'still', 'available', 'as', 'harvesting', 'has']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sentences[0:10]:\n",
    "    print(\"{}\\n\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52796"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training `word2vec` embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import *\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:52:30,081 : INFO : collecting all words and their counts\n",
      "2018-12-01 20:52:30,084 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-12-01 20:52:30,153 : INFO : PROGRESS: at sentence #10000, processed 108681 words, keeping 11280 word types\n",
      "2018-12-01 20:52:30,217 : INFO : PROGRESS: at sentence #20000, processed 215847 words, keeping 16546 word types\n",
      "2018-12-01 20:52:30,289 : INFO : PROGRESS: at sentence #30000, processed 324456 words, keeping 20959 word types\n",
      "2018-12-01 20:52:30,354 : INFO : PROGRESS: at sentence #40000, processed 434961 words, keeping 24652 word types\n",
      "2018-12-01 20:52:30,425 : INFO : PROGRESS: at sentence #50000, processed 545272 words, keeping 27779 word types\n",
      "2018-12-01 20:52:30,445 : INFO : collected 28595 word types from a corpus of 575961 raw words and 52796 sentences\n",
      "2018-12-01 20:52:30,450 : INFO : Loading a fresh vocabulary\n",
      "2018-12-01 20:52:30,506 : INFO : min_count=5 retains 7651 unique words (26% of original 28595, drops 20944)\n",
      "2018-12-01 20:52:30,511 : INFO : min_count=5 leaves 542768 word corpus (94% of original 575961, drops 33193)\n",
      "2018-12-01 20:52:30,565 : INFO : deleting the raw counts dictionary of 28595 items\n",
      "2018-12-01 20:52:30,567 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2018-12-01 20:52:30,570 : INFO : downsampling leaves estimated 415060 word corpus (76.5% of prior 542768)\n",
      "2018-12-01 20:52:30,628 : INFO : estimated required memory for 7651 words and 100 dimensions: 9946300 bytes\n",
      "2018-12-01 20:52:30,630 : INFO : resetting layer weights\n",
      "2018-12-01 20:52:30,789 : INFO : training model with 5 workers on 7651 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-12-01 20:52:32,116 : INFO : EPOCH 1 - PROGRESS: at 1.79% examples, 5576 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:33,587 : INFO : EPOCH 1 - PROGRESS: at 10.21% examples, 15690 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:34,824 : INFO : EPOCH 1 - PROGRESS: at 19.15% examples, 19832 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:52:36,026 : INFO : EPOCH 1 - PROGRESS: at 28.04% examples, 22099 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:37,395 : INFO : EPOCH 1 - PROGRESS: at 36.86% examples, 22996 words/s, in_qsize 10, out_qsize 1\n",
      "2018-12-01 20:52:38,754 : INFO : EPOCH 1 - PROGRESS: at 45.59% examples, 23589 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:40,006 : INFO : EPOCH 1 - PROGRESS: at 54.30% examples, 24304 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:52:41,396 : INFO : EPOCH 1 - PROGRESS: at 62.79% examples, 24507 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:42,917 : INFO : EPOCH 1 - PROGRESS: at 71.38% examples, 24384 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:52:44,309 : INFO : EPOCH 1 - PROGRESS: at 80.04% examples, 24535 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:45,804 : INFO : EPOCH 1 - PROGRESS: at 88.73% examples, 24494 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:52:46,076 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:52:46,095 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:52:46,807 : INFO : EPOCH 1 - PROGRESS: at 96.57% examples, 25073 words/s, in_qsize 2, out_qsize 1\n",
      "2018-12-01 20:52:46,808 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:52:46,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:52:47,055 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:52:47,056 : INFO : EPOCH - 1 : training on 575961 raw words (415228 effective words) took 16.2s, 25577 effective words/s\n",
      "2018-12-01 20:52:48,454 : INFO : EPOCH 2 - PROGRESS: at 1.79% examples, 5277 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:49,837 : INFO : EPOCH 2 - PROGRESS: at 10.30% examples, 15771 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:52:51,154 : INFO : EPOCH 2 - PROGRESS: at 19.21% examples, 19478 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:52,590 : INFO : EPOCH 2 - PROGRESS: at 28.04% examples, 20914 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:52:53,819 : INFO : EPOCH 2 - PROGRESS: at 36.86% examples, 22468 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:52:55,040 : INFO : EPOCH 2 - PROGRESS: at 45.59% examples, 23531 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:52:56,182 : INFO : EPOCH 2 - PROGRESS: at 54.35% examples, 24518 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:57,304 : INFO : EPOCH 2 - PROGRESS: at 62.87% examples, 25356 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:58,469 : INFO : EPOCH 2 - PROGRESS: at 71.29% examples, 25912 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:52:59,656 : INFO : EPOCH 2 - PROGRESS: at 80.04% examples, 26317 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:00,902 : INFO : EPOCH 2 - PROGRESS: at 88.73% examples, 26549 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:53:01,022 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:53:01,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:53:01,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:53:01,756 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:53:01,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:53:01,810 : INFO : EPOCH - 2 : training on 575961 raw words (414824 effective words) took 14.7s, 28173 effective words/s\n",
      "2018-12-01 20:53:03,034 : INFO : EPOCH 3 - PROGRESS: at 1.79% examples, 6045 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:04,353 : INFO : EPOCH 3 - PROGRESS: at 10.30% examples, 17209 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:05,726 : INFO : EPOCH 3 - PROGRESS: at 19.21% examples, 20336 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:07,152 : INFO : EPOCH 3 - PROGRESS: at 28.04% examples, 21615 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:53:08,542 : INFO : EPOCH 3 - PROGRESS: at 36.84% examples, 22499 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:09,845 : INFO : EPOCH 3 - PROGRESS: at 45.59% examples, 23335 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:11,222 : INFO : EPOCH 3 - PROGRESS: at 54.21% examples, 23767 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:12,630 : INFO : EPOCH 3 - PROGRESS: at 62.87% examples, 23987 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:53:13,838 : INFO : EPOCH 3 - PROGRESS: at 71.38% examples, 24555 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:15,107 : INFO : EPOCH 3 - PROGRESS: at 80.08% examples, 24923 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:16,345 : INFO : EPOCH 3 - PROGRESS: at 88.73% examples, 25278 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:53:16,559 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:53:16,630 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:53:17,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:53:17,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:53:17,360 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 26726 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:53:17,363 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:53:17,365 : INFO : EPOCH - 3 : training on 575961 raw words (414984 effective words) took 15.5s, 26719 effective words/s\n",
      "2018-12-01 20:53:18,790 : INFO : EPOCH 4 - PROGRESS: at 1.79% examples, 5121 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:20,201 : INFO : EPOCH 4 - PROGRESS: at 10.35% examples, 15376 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:53:21,433 : INFO : EPOCH 4 - PROGRESS: at 19.21% examples, 19544 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:53:22,727 : INFO : EPOCH 4 - PROGRESS: at 28.04% examples, 21537 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:24,057 : INFO : EPOCH 4 - PROGRESS: at 36.86% examples, 22669 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:25,626 : INFO : EPOCH 4 - PROGRESS: at 45.59% examples, 22698 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:26,903 : INFO : EPOCH 4 - PROGRESS: at 54.35% examples, 23428 words/s, in_qsize 9, out_qsize 1\n",
      "2018-12-01 20:53:28,155 : INFO : EPOCH 4 - PROGRESS: at 62.79% examples, 24031 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:53:29,611 : INFO : EPOCH 4 - PROGRESS: at 71.38% examples, 24113 words/s, in_qsize 10, out_qsize 1\n",
      "2018-12-01 20:53:31,046 : INFO : EPOCH 4 - PROGRESS: at 80.08% examples, 24220 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:32,237 : INFO : EPOCH 4 - PROGRESS: at 88.73% examples, 24697 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:53:32,326 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:53:32,357 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:53:32,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:53:33,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:53:33,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:53:33,122 : INFO : EPOCH - 4 : training on 575961 raw words (415063 effective words) took 15.7s, 26370 effective words/s\n",
      "2018-12-01 20:53:34,423 : INFO : EPOCH 5 - PROGRESS: at 1.79% examples, 5685 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:35,812 : INFO : EPOCH 5 - PROGRESS: at 10.35% examples, 16280 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:37,403 : INFO : EPOCH 5 - PROGRESS: at 19.21% examples, 18601 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:38,779 : INFO : EPOCH 5 - PROGRESS: at 28.04% examples, 20425 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:53:40,028 : INFO : EPOCH 5 - PROGRESS: at 36.86% examples, 21987 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:41,592 : INFO : EPOCH 5 - PROGRESS: at 45.50% examples, 22161 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:42,779 : INFO : EPOCH 5 - PROGRESS: at 54.35% examples, 23163 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:44,045 : INFO : EPOCH 5 - PROGRESS: at 62.79% examples, 23763 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:45,676 : INFO : EPOCH 5 - PROGRESS: at 71.29% examples, 23536 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:53:47,166 : INFO : EPOCH 5 - PROGRESS: at 80.04% examples, 23589 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:48,489 : INFO : EPOCH 5 - PROGRESS: at 88.73% examples, 23903 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:53:48,657 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:53:48,683 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:53:49,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:53:49,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:53:49,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:53:49,447 : INFO : EPOCH - 5 : training on 575961 raw words (414846 effective words) took 16.3s, 25455 effective words/s\n",
      "2018-12-01 20:53:50,700 : INFO : EPOCH 6 - PROGRESS: at 1.79% examples, 5831 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:52,139 : INFO : EPOCH 6 - PROGRESS: at 10.21% examples, 16230 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:53:53,458 : INFO : EPOCH 6 - PROGRESS: at 19.13% examples, 19831 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:54,803 : INFO : EPOCH 6 - PROGRESS: at 28.04% examples, 21550 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:56,175 : INFO : EPOCH 6 - PROGRESS: at 36.86% examples, 22545 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:57,575 : INFO : EPOCH 6 - PROGRESS: at 45.50% examples, 23087 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:53:58,902 : INFO : EPOCH 6 - PROGRESS: at 54.35% examples, 23638 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:00,050 : INFO : EPOCH 6 - PROGRESS: at 62.87% examples, 24484 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:54:01,254 : INFO : EPOCH 6 - PROGRESS: at 71.36% examples, 25020 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:02,699 : INFO : EPOCH 6 - PROGRESS: at 80.04% examples, 25010 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:04,048 : INFO : EPOCH 6 - PROGRESS: at 88.73% examples, 25171 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:54:04,176 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:54:04,227 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:54:04,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:54:04,856 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:54:04,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:54:04,948 : INFO : EPOCH - 6 : training on 575961 raw words (415122 effective words) took 15.5s, 26816 effective words/s\n",
      "2018-12-01 20:54:06,138 : INFO : EPOCH 7 - PROGRESS: at 1.79% examples, 6182 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:07,462 : INFO : EPOCH 7 - PROGRESS: at 10.21% examples, 17386 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:08,730 : INFO : EPOCH 7 - PROGRESS: at 19.15% examples, 21021 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:09,964 : INFO : EPOCH 7 - PROGRESS: at 27.98% examples, 23003 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:11,318 : INFO : EPOCH 7 - PROGRESS: at 36.77% examples, 23761 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:12,805 : INFO : EPOCH 7 - PROGRESS: at 45.59% examples, 23843 words/s, in_qsize 10, out_qsize 1\n",
      "2018-12-01 20:54:14,054 : INFO : EPOCH 7 - PROGRESS: at 54.30% examples, 24531 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:15,425 : INFO : EPOCH 7 - PROGRESS: at 62.87% examples, 24744 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:16,974 : INFO : EPOCH 7 - PROGRESS: at 71.36% examples, 24538 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:54:18,213 : INFO : EPOCH 7 - PROGRESS: at 80.04% examples, 24963 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:19,395 : INFO : EPOCH 7 - PROGRESS: at 88.73% examples, 25411 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:54:19,522 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:54:19,576 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:54:20,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:54:20,284 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:54:20,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:54:20,341 : INFO : EPOCH - 7 : training on 575961 raw words (414820 effective words) took 15.4s, 26980 effective words/s\n",
      "2018-12-01 20:54:21,573 : INFO : EPOCH 8 - PROGRESS: at 1.77% examples, 5917 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:22,841 : INFO : EPOCH 8 - PROGRESS: at 10.35% examples, 17423 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:24,336 : INFO : EPOCH 8 - PROGRESS: at 19.22% examples, 19852 words/s, in_qsize 10, out_qsize 1\n",
      "2018-12-01 20:54:25,692 : INFO : EPOCH 8 - PROGRESS: at 28.04% examples, 21557 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:26,995 : INFO : EPOCH 8 - PROGRESS: at 36.84% examples, 22741 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:28,428 : INFO : EPOCH 8 - PROGRESS: at 45.59% examples, 23175 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:29,656 : INFO : EPOCH 8 - PROGRESS: at 54.30% examples, 23998 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:31,155 : INFO : EPOCH 8 - PROGRESS: at 62.87% examples, 24001 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:32,434 : INFO : EPOCH 8 - PROGRESS: at 71.38% examples, 24419 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:33,591 : INFO : EPOCH 8 - PROGRESS: at 80.04% examples, 25002 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:34,755 : INFO : EPOCH 8 - PROGRESS: at 88.73% examples, 25482 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:54:34,865 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:54:34,927 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:54:35,541 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:54:35,736 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:54:35,794 : INFO : EPOCH 8 - PROGRESS: at 100.00% examples, 26882 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:54:35,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:54:35,796 : INFO : EPOCH - 8 : training on 575961 raw words (414993 effective words) took 15.4s, 26877 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:54:37,175 : INFO : EPOCH 9 - PROGRESS: at 1.79% examples, 5290 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:38,583 : INFO : EPOCH 9 - PROGRESS: at 10.35% examples, 15633 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:39,777 : INFO : EPOCH 9 - PROGRESS: at 19.21% examples, 19969 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:40,947 : INFO : EPOCH 9 - PROGRESS: at 28.04% examples, 22420 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:42,157 : INFO : EPOCH 9 - PROGRESS: at 36.86% examples, 23863 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:43,328 : INFO : EPOCH 9 - PROGRESS: at 45.50% examples, 24917 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:44,530 : INFO : EPOCH 9 - PROGRESS: at 54.35% examples, 25617 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:46,061 : INFO : EPOCH 9 - PROGRESS: at 62.79% examples, 25297 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:47,561 : INFO : EPOCH 9 - PROGRESS: at 71.38% examples, 25118 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:54:48,851 : INFO : EPOCH 9 - PROGRESS: at 80.04% examples, 25398 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:50,245 : INFO : EPOCH 9 - PROGRESS: at 88.73% examples, 25440 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:54:50,379 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:54:50,416 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:54:50,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:54:51,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:54:51,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:54:51,095 : INFO : EPOCH - 9 : training on 575961 raw words (415458 effective words) took 15.3s, 27181 effective words/s\n",
      "2018-12-01 20:54:52,289 : INFO : EPOCH 10 - PROGRESS: at 1.79% examples, 6150 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:53,612 : INFO : EPOCH 10 - PROGRESS: at 10.21% examples, 17362 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:54,946 : INFO : EPOCH 10 - PROGRESS: at 19.15% examples, 20687 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:56,285 : INFO : EPOCH 10 - PROGRESS: at 28.04% examples, 22252 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:54:57,627 : INFO : EPOCH 10 - PROGRESS: at 36.77% examples, 23220 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:54:58,918 : INFO : EPOCH 10 - PROGRESS: at 45.59% examples, 23989 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:55:00,264 : INFO : EPOCH 10 - PROGRESS: at 54.35% examples, 24397 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:01,550 : INFO : EPOCH 10 - PROGRESS: at 62.79% examples, 24829 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:02,842 : INFO : EPOCH 10 - PROGRESS: at 71.38% examples, 25158 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:04,181 : INFO : EPOCH 10 - PROGRESS: at 80.04% examples, 25337 words/s, in_qsize 10, out_qsize 1\n",
      "2018-12-01 20:55:05,542 : INFO : EPOCH 10 - PROGRESS: at 88.73% examples, 25447 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:55:05,706 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:55:05,737 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:55:06,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:55:06,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:55:06,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:55:06,417 : INFO : EPOCH - 10 : training on 575961 raw words (415410 effective words) took 15.3s, 27139 effective words/s\n",
      "2018-12-01 20:55:07,544 : INFO : EPOCH 11 - PROGRESS: at 1.79% examples, 6492 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:08,790 : INFO : EPOCH 11 - PROGRESS: at 10.21% examples, 18435 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:10,165 : INFO : EPOCH 11 - PROGRESS: at 19.15% examples, 21254 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:11,494 : INFO : EPOCH 11 - PROGRESS: at 28.04% examples, 22758 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:12,844 : INFO : EPOCH 11 - PROGRESS: at 36.86% examples, 23599 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:14,071 : INFO : EPOCH 11 - PROGRESS: at 45.50% examples, 24520 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:15,348 : INFO : EPOCH 11 - PROGRESS: at 54.30% examples, 25035 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:55:16,746 : INFO : EPOCH 11 - PROGRESS: at 62.87% examples, 25122 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:55:18,134 : INFO : EPOCH 11 - PROGRESS: at 71.38% examples, 25197 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:19,504 : INFO : EPOCH 11 - PROGRESS: at 80.04% examples, 25311 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:20,831 : INFO : EPOCH 11 - PROGRESS: at 88.73% examples, 25478 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:55:20,986 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:55:21,047 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:55:21,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:55:21,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:55:21,739 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:55:21,740 : INFO : EPOCH - 11 : training on 575961 raw words (415023 effective words) took 15.3s, 27114 effective words/s\n",
      "2018-12-01 20:55:22,987 : INFO : EPOCH 12 - PROGRESS: at 1.79% examples, 5855 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:24,127 : INFO : EPOCH 12 - PROGRESS: at 10.30% examples, 18305 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:25,505 : INFO : EPOCH 12 - PROGRESS: at 19.21% examples, 21133 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:26,911 : INFO : EPOCH 12 - PROGRESS: at 28.04% examples, 22327 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:28,272 : INFO : EPOCH 12 - PROGRESS: at 36.86% examples, 23217 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:29,656 : INFO : EPOCH 12 - PROGRESS: at 45.50% examples, 23724 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:55:31,057 : INFO : EPOCH 12 - PROGRESS: at 54.30% examples, 24025 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:55:32,436 : INFO : EPOCH 12 - PROGRESS: at 62.79% examples, 24281 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:33,872 : INFO : EPOCH 12 - PROGRESS: at 71.38% examples, 24361 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:35,119 : INFO : EPOCH 12 - PROGRESS: at 80.04% examples, 24776 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:36,394 : INFO : EPOCH 12 - PROGRESS: at 88.73% examples, 25080 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:55:36,598 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:55:36,611 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:55:37,140 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:55:37,297 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:55:37,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:55:37,352 : INFO : EPOCH - 12 : training on 575961 raw words (415170 effective words) took 15.6s, 26622 effective words/s\n",
      "2018-12-01 20:55:38,703 : INFO : EPOCH 13 - PROGRESS: at 1.79% examples, 5416 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:39,918 : INFO : EPOCH 13 - PROGRESS: at 10.21% examples, 17079 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:55:41,143 : INFO : EPOCH 13 - PROGRESS: at 19.21% examples, 20983 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:42,324 : INFO : EPOCH 13 - PROGRESS: at 28.04% examples, 23203 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:43,521 : INFO : EPOCH 13 - PROGRESS: at 36.86% examples, 24579 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:44,776 : INFO : EPOCH 13 - PROGRESS: at 45.50% examples, 25286 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:55:46,098 : INFO : EPOCH 13 - PROGRESS: at 54.30% examples, 25574 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:47,269 : INFO : EPOCH 13 - PROGRESS: at 62.79% examples, 26163 words/s, in_qsize 8, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:55:48,503 : INFO : EPOCH 13 - PROGRESS: at 71.38% examples, 26494 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:49,698 : INFO : EPOCH 13 - PROGRESS: at 80.04% examples, 26852 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:50,833 : INFO : EPOCH 13 - PROGRESS: at 88.73% examples, 27260 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:55:51,026 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:55:51,048 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:55:51,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:55:51,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:55:51,800 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:55:51,801 : INFO : EPOCH - 13 : training on 575961 raw words (415097 effective words) took 14.4s, 28762 effective words/s\n",
      "2018-12-01 20:55:52,915 : INFO : EPOCH 14 - PROGRESS: at 1.79% examples, 6687 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:54,104 : INFO : EPOCH 14 - PROGRESS: at 10.21% examples, 19143 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:55,315 : INFO : EPOCH 14 - PROGRESS: at 19.21% examples, 22752 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:56,423 : INFO : EPOCH 14 - PROGRESS: at 28.04% examples, 25070 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:57,604 : INFO : EPOCH 14 - PROGRESS: at 36.77% examples, 26188 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:58,786 : INFO : EPOCH 14 - PROGRESS: at 45.50% examples, 26900 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:55:59,951 : INFO : EPOCH 14 - PROGRESS: at 54.30% examples, 27476 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:01,095 : INFO : EPOCH 14 - PROGRESS: at 62.87% examples, 27963 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:02,271 : INFO : EPOCH 14 - PROGRESS: at 71.29% examples, 28242 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:56:03,457 : INFO : EPOCH 14 - PROGRESS: at 80.04% examples, 28453 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:56:04,645 : INFO : EPOCH 14 - PROGRESS: at 88.73% examples, 28631 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:56:04,825 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:56:04,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:56:05,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:56:05,467 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:56:05,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:56:05,510 : INFO : EPOCH - 14 : training on 575961 raw words (414836 effective words) took 13.7s, 30338 effective words/s\n",
      "2018-12-01 20:56:06,597 : INFO : EPOCH 15 - PROGRESS: at 1.68% examples, 6836 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:07,798 : INFO : EPOCH 15 - PROGRESS: at 10.21% examples, 19197 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:08,976 : INFO : EPOCH 15 - PROGRESS: at 19.15% examples, 23040 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:10,152 : INFO : EPOCH 15 - PROGRESS: at 27.98% examples, 24924 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:11,290 : INFO : EPOCH 15 - PROGRESS: at 36.86% examples, 26240 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:12,470 : INFO : EPOCH 15 - PROGRESS: at 45.50% examples, 26974 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:13,607 : INFO : EPOCH 15 - PROGRESS: at 54.35% examples, 27613 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:14,765 : INFO : EPOCH 15 - PROGRESS: at 62.87% examples, 28047 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:15,948 : INFO : EPOCH 15 - PROGRESS: at 71.38% examples, 28298 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:17,184 : INFO : EPOCH 15 - PROGRESS: at 80.04% examples, 28394 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:18,411 : INFO : EPOCH 15 - PROGRESS: at 88.73% examples, 28481 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:56:18,627 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:56:18,636 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:56:19,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:56:19,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:56:19,308 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:56:19,309 : INFO : EPOCH - 15 : training on 575961 raw words (414981 effective words) took 13.8s, 30119 effective words/s\n",
      "2018-12-01 20:56:20,462 : INFO : EPOCH 16 - PROGRESS: at 1.79% examples, 6378 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:21,705 : INFO : EPOCH 16 - PROGRESS: at 10.21% examples, 18314 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:22,959 : INFO : EPOCH 16 - PROGRESS: at 19.15% examples, 21844 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:24,124 : INFO : EPOCH 16 - PROGRESS: at 28.04% examples, 24012 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:25,549 : INFO : EPOCH 16 - PROGRESS: at 36.86% examples, 24326 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:27,034 : INFO : EPOCH 16 - PROGRESS: at 45.50% examples, 24322 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:28,199 : INFO : EPOCH 16 - PROGRESS: at 54.35% examples, 25172 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:56:29,376 : INFO : EPOCH 16 - PROGRESS: at 62.87% examples, 25816 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:30,513 : INFO : EPOCH 16 - PROGRESS: at 71.38% examples, 26396 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:31,676 : INFO : EPOCH 16 - PROGRESS: at 80.04% examples, 26828 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:32,835 : INFO : EPOCH 16 - PROGRESS: at 88.73% examples, 27190 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:56:33,083 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:56:33,144 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:56:33,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:56:33,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:56:33,824 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:56:33,825 : INFO : EPOCH - 16 : training on 575961 raw words (415224 effective words) took 14.5s, 28650 effective words/s\n",
      "2018-12-01 20:56:35,037 : INFO : EPOCH 17 - PROGRESS: at 1.79% examples, 6048 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:36,194 : INFO : EPOCH 17 - PROGRESS: at 10.30% examples, 18436 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:37,349 : INFO : EPOCH 17 - PROGRESS: at 19.21% examples, 22583 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:38,525 : INFO : EPOCH 17 - PROGRESS: at 28.04% examples, 24580 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:39,692 : INFO : EPOCH 17 - PROGRESS: at 36.86% examples, 25874 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:40,845 : INFO : EPOCH 17 - PROGRESS: at 45.50% examples, 26749 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:56:41,971 : INFO : EPOCH 17 - PROGRESS: at 54.30% examples, 27469 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:56:43,152 : INFO : EPOCH 17 - PROGRESS: at 62.87% examples, 27839 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:44,320 : INFO : EPOCH 17 - PROGRESS: at 71.38% examples, 28160 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:45,482 : INFO : EPOCH 17 - PROGRESS: at 80.04% examples, 28442 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:46,646 : INFO : EPOCH 17 - PROGRESS: at 88.73% examples, 28670 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:56:46,839 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:56:46,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:56:47,310 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:56:47,463 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:56:47,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:56:47,536 : INFO : EPOCH - 17 : training on 575961 raw words (415257 effective words) took 13.7s, 30319 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:56:48,641 : INFO : EPOCH 18 - PROGRESS: at 1.79% examples, 6669 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:49,814 : INFO : EPOCH 18 - PROGRESS: at 10.21% examples, 19265 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:50,992 : INFO : EPOCH 18 - PROGRESS: at 19.15% examples, 23089 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:52,121 : INFO : EPOCH 18 - PROGRESS: at 28.04% examples, 25205 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:53,359 : INFO : EPOCH 18 - PROGRESS: at 36.86% examples, 26041 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:54,678 : INFO : EPOCH 18 - PROGRESS: at 45.50% examples, 26272 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:55,950 : INFO : EPOCH 18 - PROGRESS: at 54.30% examples, 26574 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:56:57,082 : INFO : EPOCH 18 - PROGRESS: at 62.87% examples, 27187 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:56:58,233 : INFO : EPOCH 18 - PROGRESS: at 71.36% examples, 27608 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:56:59,384 : INFO : EPOCH 18 - PROGRESS: at 80.04% examples, 27962 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:00,467 : INFO : EPOCH 18 - PROGRESS: at 88.73% examples, 28392 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:57:00,618 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:57:00,704 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:57:01,103 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:57:01,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:57:01,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:57:01,334 : INFO : EPOCH - 18 : training on 575961 raw words (414713 effective words) took 13.8s, 30101 effective words/s\n",
      "2018-12-01 20:57:02,412 : INFO : EPOCH 19 - PROGRESS: at 1.79% examples, 6875 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:03,623 : INFO : EPOCH 19 - PROGRESS: at 10.21% examples, 19202 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:04,767 : INFO : EPOCH 19 - PROGRESS: at 19.15% examples, 23257 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:05,890 : INFO : EPOCH 19 - PROGRESS: at 27.98% examples, 25412 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:07,023 : INFO : EPOCH 19 - PROGRESS: at 36.86% examples, 26685 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:08,209 : INFO : EPOCH 19 - PROGRESS: at 45.59% examples, 27314 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:09,325 : INFO : EPOCH 19 - PROGRESS: at 54.30% examples, 28020 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:10,454 : INFO : EPOCH 19 - PROGRESS: at 62.87% examples, 28493 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:11,589 : INFO : EPOCH 19 - PROGRESS: at 71.38% examples, 28834 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:57:12,777 : INFO : EPOCH 19 - PROGRESS: at 80.04% examples, 28986 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:13,876 : INFO : EPOCH 19 - PROGRESS: at 88.73% examples, 29320 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:57:14,003 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:57:14,046 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:57:14,490 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:57:14,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:57:14,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:57:14,702 : INFO : EPOCH - 19 : training on 575961 raw words (415273 effective words) took 13.3s, 31117 effective words/s\n",
      "2018-12-01 20:57:15,790 : INFO : EPOCH 20 - PROGRESS: at 1.79% examples, 6733 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:16,935 : INFO : EPOCH 20 - PROGRESS: at 10.21% examples, 19559 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:18,125 : INFO : EPOCH 20 - PROGRESS: at 19.15% examples, 23210 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:19,264 : INFO : EPOCH 20 - PROGRESS: at 27.98% examples, 25298 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:20,377 : INFO : EPOCH 20 - PROGRESS: at 36.86% examples, 26699 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:21,500 : INFO : EPOCH 20 - PROGRESS: at 45.50% examples, 27585 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:22,608 : INFO : EPOCH 20 - PROGRESS: at 54.35% examples, 28262 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:23,813 : INFO : EPOCH 20 - PROGRESS: at 62.87% examples, 28478 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:24,974 : INFO : EPOCH 20 - PROGRESS: at 71.29% examples, 28757 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:26,113 : INFO : EPOCH 20 - PROGRESS: at 80.04% examples, 29034 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:27,220 : INFO : EPOCH 20 - PROGRESS: at 88.73% examples, 29341 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:57:27,365 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:57:27,374 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:57:27,843 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:57:28,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:57:28,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:57:28,070 : INFO : EPOCH - 20 : training on 575961 raw words (415018 effective words) took 13.4s, 31078 effective words/s\n",
      "2018-12-01 20:57:29,126 : INFO : EPOCH 21 - PROGRESS: at 1.62% examples, 6996 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:30,278 : INFO : EPOCH 21 - PROGRESS: at 10.21% examples, 19846 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:31,405 : INFO : EPOCH 21 - PROGRESS: at 19.15% examples, 23897 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:32,519 : INFO : EPOCH 21 - PROGRESS: at 27.98% examples, 25998 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:33,718 : INFO : EPOCH 21 - PROGRESS: at 36.77% examples, 26838 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:34,856 : INFO : EPOCH 21 - PROGRESS: at 45.59% examples, 27643 words/s, in_qsize 7, out_qsize 2\n",
      "2018-12-01 20:57:35,986 : INFO : EPOCH 21 - PROGRESS: at 54.35% examples, 28239 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:37,482 : INFO : EPOCH 21 - PROGRESS: at 62.87% examples, 27581 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:38,912 : INFO : EPOCH 21 - PROGRESS: at 71.38% examples, 27247 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:40,317 : INFO : EPOCH 21 - PROGRESS: at 80.04% examples, 27063 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:41,755 : INFO : EPOCH 21 - PROGRESS: at 88.73% examples, 26844 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:57:41,959 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:57:41,984 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:57:42,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:57:42,784 : INFO : EPOCH 21 - PROGRESS: at 98.32% examples, 27742 words/s, in_qsize 1, out_qsize 1\n",
      "2018-12-01 20:57:42,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:57:42,860 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:57:42,861 : INFO : EPOCH - 21 : training on 575961 raw words (415035 effective words) took 14.8s, 28092 effective words/s\n",
      "2018-12-01 20:57:44,195 : INFO : EPOCH 22 - PROGRESS: at 1.79% examples, 5524 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:45,718 : INFO : EPOCH 22 - PROGRESS: at 10.21% examples, 15364 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:46,962 : INFO : EPOCH 22 - PROGRESS: at 19.15% examples, 19447 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:48,171 : INFO : EPOCH 22 - PROGRESS: at 27.98% examples, 21797 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:49,341 : INFO : EPOCH 22 - PROGRESS: at 36.86% examples, 23415 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:50,493 : INFO : EPOCH 22 - PROGRESS: at 45.50% examples, 24617 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:51,609 : INFO : EPOCH 22 - PROGRESS: at 54.35% examples, 25583 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:57:52,769 : INFO : EPOCH 22 - PROGRESS: at 62.87% examples, 26217 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:57:53,930 : INFO : EPOCH 22 - PROGRESS: at 71.38% examples, 26704 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:55,068 : INFO : EPOCH 22 - PROGRESS: at 80.04% examples, 27154 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:56,176 : INFO : EPOCH 22 - PROGRESS: at 88.73% examples, 27597 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:57:56,360 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:57:56,367 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:57:56,837 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:57:57,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:57:57,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:57:57,073 : INFO : EPOCH - 22 : training on 575961 raw words (415051 effective words) took 14.2s, 29244 effective words/s\n",
      "2018-12-01 20:57:58,168 : INFO : EPOCH 23 - PROGRESS: at 1.79% examples, 6728 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:57:59,345 : INFO : EPOCH 23 - PROGRESS: at 10.21% examples, 19284 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:00,507 : INFO : EPOCH 23 - PROGRESS: at 19.15% examples, 23248 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:01,629 : INFO : EPOCH 23 - PROGRESS: at 27.98% examples, 25385 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:02,771 : INFO : EPOCH 23 - PROGRESS: at 36.86% examples, 26633 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:03,905 : INFO : EPOCH 23 - PROGRESS: at 45.50% examples, 27487 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:05,011 : INFO : EPOCH 23 - PROGRESS: at 54.35% examples, 28165 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:06,151 : INFO : EPOCH 23 - PROGRESS: at 62.87% examples, 28600 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:07,330 : INFO : EPOCH 23 - PROGRESS: at 71.38% examples, 28808 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:08,486 : INFO : EPOCH 23 - PROGRESS: at 80.04% examples, 29046 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:58:09,583 : INFO : EPOCH 23 - PROGRESS: at 88.73% examples, 29376 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:58:09,781 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:58:09,792 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:58:10,265 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:58:10,408 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:58:10,467 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:58:10,468 : INFO : EPOCH - 23 : training on 575961 raw words (415077 effective words) took 13.4s, 31029 effective words/s\n",
      "2018-12-01 20:58:11,553 : INFO : EPOCH 24 - PROGRESS: at 1.79% examples, 6765 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:12,700 : INFO : EPOCH 24 - PROGRESS: at 10.30% examples, 19526 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:13,883 : INFO : EPOCH 24 - PROGRESS: at 19.15% examples, 23287 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:15,026 : INFO : EPOCH 24 - PROGRESS: at 28.04% examples, 25316 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:16,143 : INFO : EPOCH 24 - PROGRESS: at 36.77% examples, 26698 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:17,280 : INFO : EPOCH 24 - PROGRESS: at 45.50% examples, 27534 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:18,431 : INFO : EPOCH 24 - PROGRESS: at 54.30% examples, 28050 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:19,574 : INFO : EPOCH 24 - PROGRESS: at 62.79% examples, 28455 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:58:20,723 : INFO : EPOCH 24 - PROGRESS: at 71.38% examples, 28765 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:58:21,872 : INFO : EPOCH 24 - PROGRESS: at 80.08% examples, 29025 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:23,058 : INFO : EPOCH 24 - PROGRESS: at 88.73% examples, 29155 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:58:23,233 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:58:23,254 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:58:23,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:58:23,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:58:23,956 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:58:23,957 : INFO : EPOCH - 24 : training on 575961 raw words (414892 effective words) took 13.5s, 30780 effective words/s\n",
      "2018-12-01 20:58:25,068 : INFO : EPOCH 25 - PROGRESS: at 1.79% examples, 6552 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:26,260 : INFO : EPOCH 25 - PROGRESS: at 10.21% examples, 18962 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:27,517 : INFO : EPOCH 25 - PROGRESS: at 19.15% examples, 22355 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:28,715 : INFO : EPOCH 25 - PROGRESS: at 27.98% examples, 24275 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:58:29,930 : INFO : EPOCH 25 - PROGRESS: at 36.77% examples, 25365 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:58:31,062 : INFO : EPOCH 25 - PROGRESS: at 45.59% examples, 26387 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:32,241 : INFO : EPOCH 25 - PROGRESS: at 54.30% examples, 26974 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:33,445 : INFO : EPOCH 25 - PROGRESS: at 62.87% examples, 27338 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:58:34,585 : INFO : EPOCH 25 - PROGRESS: at 71.38% examples, 27771 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:35,731 : INFO : EPOCH 25 - PROGRESS: at 80.04% examples, 28128 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:36,913 : INFO : EPOCH 25 - PROGRESS: at 88.73% examples, 28343 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:58:37,016 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:58:37,051 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:58:37,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:58:37,778 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:58:37,794 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:58:37,795 : INFO : EPOCH - 25 : training on 575961 raw words (414962 effective words) took 13.8s, 30017 effective words/s\n",
      "2018-12-01 20:58:38,871 : INFO : EPOCH 26 - PROGRESS: at 1.79% examples, 6785 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:40,036 : INFO : EPOCH 26 - PROGRESS: at 10.21% examples, 19452 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:41,195 : INFO : EPOCH 26 - PROGRESS: at 19.15% examples, 23378 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:58:42,335 : INFO : EPOCH 26 - PROGRESS: at 28.04% examples, 25401 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:43,481 : INFO : EPOCH 26 - PROGRESS: at 36.86% examples, 26634 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:44,665 : INFO : EPOCH 26 - PROGRESS: at 45.59% examples, 27278 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:58:45,778 : INFO : EPOCH 26 - PROGRESS: at 54.30% examples, 27982 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:46,906 : INFO : EPOCH 26 - PROGRESS: at 62.79% examples, 28448 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:48,049 : INFO : EPOCH 26 - PROGRESS: at 71.38% examples, 28769 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:49,198 : INFO : EPOCH 26 - PROGRESS: at 80.04% examples, 29020 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:50,334 : INFO : EPOCH 26 - PROGRESS: at 88.73% examples, 29265 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:58:50,494 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:58:50,535 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:58:50,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:58:51,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:58:51,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:58:51,207 : INFO : EPOCH - 26 : training on 575961 raw words (414611 effective words) took 13.4s, 30950 effective words/s\n",
      "2018-12-01 20:58:52,291 : INFO : EPOCH 27 - PROGRESS: at 1.79% examples, 6796 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:53,448 : INFO : EPOCH 27 - PROGRESS: at 10.21% examples, 19573 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:54,584 : INFO : EPOCH 27 - PROGRESS: at 19.21% examples, 23547 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:55,703 : INFO : EPOCH 27 - PROGRESS: at 27.98% examples, 25737 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:56,883 : INFO : EPOCH 27 - PROGRESS: at 36.86% examples, 26735 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:58,093 : INFO : EPOCH 27 - PROGRESS: at 45.50% examples, 27273 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:58:59,223 : INFO : EPOCH 27 - PROGRESS: at 54.30% examples, 27896 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:00,340 : INFO : EPOCH 27 - PROGRESS: at 62.87% examples, 28430 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:01,471 : INFO : EPOCH 27 - PROGRESS: at 71.38% examples, 28783 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:59:02,623 : INFO : EPOCH 27 - PROGRESS: at 80.04% examples, 29042 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:03,784 : INFO : EPOCH 27 - PROGRESS: at 88.73% examples, 29225 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:59:03,907 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:59:03,940 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:59:04,417 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:59:04,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:59:04,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:59:04,636 : INFO : EPOCH - 27 : training on 575961 raw words (415177 effective words) took 13.4s, 30952 effective words/s\n",
      "2018-12-01 20:59:05,721 : INFO : EPOCH 28 - PROGRESS: at 1.79% examples, 6715 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:06,877 : INFO : EPOCH 28 - PROGRESS: at 10.21% examples, 19470 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:59:08,005 : INFO : EPOCH 28 - PROGRESS: at 19.21% examples, 23563 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:09,135 : INFO : EPOCH 28 - PROGRESS: at 28.04% examples, 25631 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:10,323 : INFO : EPOCH 28 - PROGRESS: at 36.86% examples, 26631 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:11,465 : INFO : EPOCH 28 - PROGRESS: at 45.50% examples, 27479 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:12,616 : INFO : EPOCH 28 - PROGRESS: at 54.35% examples, 28018 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:59:13,746 : INFO : EPOCH 28 - PROGRESS: at 62.79% examples, 28487 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:14,891 : INFO : EPOCH 28 - PROGRESS: at 71.38% examples, 28810 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:59:16,039 : INFO : EPOCH 28 - PROGRESS: at 80.04% examples, 29066 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:17,233 : INFO : EPOCH 28 - PROGRESS: at 88.73% examples, 29182 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:59:17,408 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:59:17,418 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:59:17,869 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:59:18,062 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:59:18,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:59:18,097 : INFO : EPOCH - 28 : training on 575961 raw words (415375 effective words) took 13.4s, 30890 effective words/s\n",
      "2018-12-01 20:59:19,172 : INFO : EPOCH 29 - PROGRESS: at 1.79% examples, 6847 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:20,343 : INFO : EPOCH 29 - PROGRESS: at 10.21% examples, 19510 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:21,489 : INFO : EPOCH 29 - PROGRESS: at 19.15% examples, 23556 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:22,640 : INFO : EPOCH 29 - PROGRESS: at 28.04% examples, 25464 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:23,767 : INFO : EPOCH 29 - PROGRESS: at 36.86% examples, 26785 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:24,917 : INFO : EPOCH 29 - PROGRESS: at 45.50% examples, 27556 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:26,048 : INFO : EPOCH 29 - PROGRESS: at 54.21% examples, 28172 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:27,204 : INFO : EPOCH 29 - PROGRESS: at 62.79% examples, 28508 words/s, in_qsize 10, out_qsize 0\n",
      "2018-12-01 20:59:28,324 : INFO : EPOCH 29 - PROGRESS: at 71.36% examples, 28894 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:29,483 : INFO : EPOCH 29 - PROGRESS: at 80.04% examples, 29114 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:30,579 : INFO : EPOCH 29 - PROGRESS: at 88.73% examples, 29437 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:59:30,760 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:59:30,818 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:59:31,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:59:31,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:59:31,440 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:59:31,441 : INFO : EPOCH - 29 : training on 575961 raw words (415018 effective words) took 13.3s, 31139 effective words/s\n",
      "2018-12-01 20:59:32,513 : INFO : EPOCH 30 - PROGRESS: at 1.79% examples, 6879 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:33,713 : INFO : EPOCH 30 - PROGRESS: at 10.21% examples, 19260 words/s, in_qsize 8, out_qsize 1\n",
      "2018-12-01 20:59:34,861 : INFO : EPOCH 30 - PROGRESS: at 19.21% examples, 23213 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:35,993 : INFO : EPOCH 30 - PROGRESS: at 28.04% examples, 25322 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:37,219 : INFO : EPOCH 30 - PROGRESS: at 36.86% examples, 26223 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:38,470 : INFO : EPOCH 30 - PROGRESS: at 45.59% examples, 26651 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:39,678 : INFO : EPOCH 30 - PROGRESS: at 54.30% examples, 27121 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:40,811 : INFO : EPOCH 30 - PROGRESS: at 62.79% examples, 27678 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:41,917 : INFO : EPOCH 30 - PROGRESS: at 71.38% examples, 28178 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:43,066 : INFO : EPOCH 30 - PROGRESS: at 80.04% examples, 28491 words/s, in_qsize 9, out_qsize 0\n",
      "2018-12-01 20:59:44,243 : INFO : EPOCH 30 - PROGRESS: at 88.73% examples, 28687 words/s, in_qsize 7, out_qsize 0\n",
      "2018-12-01 20:59:44,381 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-01 20:59:44,412 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-01 20:59:44,999 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-01 20:59:45,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-01 20:59:45,306 : INFO : EPOCH 30 - PROGRESS: at 100.00% examples, 29964 words/s, in_qsize 0, out_qsize 1\n",
      "2018-12-01 20:59:45,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-01 20:59:45,309 : INFO : EPOCH - 30 : training on 575961 raw words (414966 effective words) took 13.9s, 29958 effective words/s\n",
      "2018-12-01 20:59:45,310 : INFO : training on a 17278830 raw words (12451504 effective words) took 434.5s, 28657 effective words/s\n"
     ]
    }
   ],
   "source": [
    "num_feature = 100\n",
    "min_word_count = 5\n",
    "num_thread = 5\n",
    "window_size = 10\n",
    "down_sampling = 0.001\n",
    "iteration = 30\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = word2vec.Word2Vec(sentences, \n",
    "                          iter = iteration,\n",
    "                          size=num_feature, \n",
    "                          min_count = min_word_count, \n",
    "                          window = window_size, \n",
    "                          sample = down_sampling, \n",
    "                          workers=num_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-01 20:59:45,349 : INFO : saving Word2Vec object under ./gensim_word2vec_model_011218, separately None\n",
      "2018-12-01 20:59:45,352 : INFO : not storing attribute vectors_norm\n",
      "2018-12-01 20:59:45,363 : INFO : not storing attribute cum_table\n",
      "2018-12-01 20:59:45,568 : INFO : saved ./gensim_word2vec_model_011218\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./gensim_word2vec_model_011218\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words: 7651\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of words: {}\".format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the quality of model in terms of semantic and syntactic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we can see that the similarity of two number `1` and `2` is quite high at `0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7935787295718684\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('1', '2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is similarity for `loan` and `money`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36741977267850645\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('loan', 'money'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the most similar words for a specific word like `bank`. The most similar word will be placed at the top of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.5285881161689758),\n",
       " ('bankers', 0.45777976512908936),\n",
       " ('authorities', 0.423633337020874),\n",
       " ('measure', 0.40743112564086914),\n",
       " ('money', 0.393973708152771),\n",
       " ('dealers', 0.39064037799835205),\n",
       " ('lending', 0.38939034938812256),\n",
       " ('deposits', 0.38620954751968384),\n",
       " ('requirement', 0.37992456555366516),\n",
       " ('bundesbank', 0.3742866814136505)]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('bank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 4 words, we can point out that the word `weather` is the most dissimilar one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"money bank loan weather\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the vector representation of the word `loan`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for word 'loan': \n",
      "[-3.2303596e-01 -2.4156787e+00  4.4412516e-02  4.5770600e-01\n",
      " -2.5768003e+00 -2.7601206e-01  1.2320732e+00 -9.4704521e-01\n",
      "  7.5148098e-02 -7.6155037e-01 -9.3001324e-01  3.0199960e-01\n",
      " -1.2191356e+00 -2.3171395e-01 -6.5511161e-01  1.6387726e+00\n",
      "  1.9019133e-01  1.2324833e+00 -6.8817651e-01  1.8227566e+00\n",
      " -2.2284260e+00 -3.9081306e+00 -1.4234127e+00 -2.7999072e+00\n",
      "  1.3827580e+00  3.1156006e-01 -1.4501753e+00 -3.0824441e-01\n",
      "  7.7271469e-02  3.0485579e-01 -1.8449939e+00  3.4685783e+00\n",
      "  8.2687175e-01  6.0459113e-01 -1.9342793e+00 -3.5091162e-02\n",
      " -1.4164740e+00  3.4551995e+00  3.8812149e+00 -6.1105329e-01\n",
      " -1.7352334e-01  3.4135205e-01  7.5961894e-01  2.8761439e+00\n",
      " -1.3272648e+00 -4.3128464e-01  1.1381413e-01  1.7972025e-01\n",
      " -5.4642242e-01 -1.9100033e+00 -9.3116874e-01  2.4653428e+00\n",
      "  1.2924670e+00  2.7447934e+00  2.6395755e+00  8.2297546e-01\n",
      " -3.0392332e+00  7.5616819e-01  2.4810507e+00 -1.0917925e+00\n",
      " -1.6387278e+00  5.5766636e-01  2.4589796e-03  1.4736474e-01\n",
      "  7.6167601e-01 -5.7093716e-01  1.0420450e+00  9.7727761e-02\n",
      "  1.1360010e+00  7.9553449e-01  7.0302445e-01  7.6010454e-01\n",
      " -3.1210715e-01 -4.2491382e-01 -4.4116926e+00 -1.2450074e+00\n",
      " -8.7176579e-01 -5.9563434e-01 -1.6273747e-01 -1.8583897e+00\n",
      "  1.2827094e+00  1.1069635e+00 -1.7257416e+00  9.8806328e-01\n",
      " -8.1891769e-01  4.0261638e-01  6.5463257e-01 -1.2215393e+00\n",
      " -4.4078743e-01 -1.6892220e+00 -2.3373562e-01 -4.3342301e-01\n",
      "  9.1961533e-01  8.5264194e-01  4.8075643e-01  1.2172883e+00\n",
      " -1.7963122e+00  3.5136458e-01 -9.3309879e-01  4.8422447e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector for word 'loan': \\n{}\".format(model.wv.get_vector('loan')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At index 15 is the word: bank\n"
     ]
    }
   ],
   "source": [
    "print(\"At index 15 is the word: {}\".format(model.wv.index2word[43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'money' is: 43\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'money' is: {}\".format(model.wv.vocab['bank'].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding vector list for words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing training the `word2vec` model, we have to seperate words in vocabulary and their corresponding vectors into 2 different list which will be used as inputs for our deep learning model later. Here, we will add 1 character `-` at the beginning as our padding value and `unk` at the end of the list as our unknown word. More details about the use of padding and unknown characters will be discussed later in the deep learning model section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst = []\n",
    "# adding padding value\n",
    "word_lst.append('-')\n",
    "length = len(model.wv.vocab)\n",
    "# vector dimension will be the same with the word2vec model\n",
    "vec_dim = 100\n",
    "# adding '-' for padding value and 'unk' for word that can't be found in the model\n",
    "word_vec = np.zeros((length + 2, vec_dim))\n",
    "for i in range (length):\n",
    "    word = model.wv.index2word[i]\n",
    "    vector = model.wv[word]\n",
    "    word_vec[i+1] = vector\n",
    "    word_lst.append(word)\n",
    "# adding unknown word\n",
    "word_lst.append('unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in list: 7653\n",
      "Shape of word vector: (7653, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in list: {}\".format(len(word_lst)))\n",
    "print(\"Shape of word vector: {}\".format(word_vec.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both padding and unknown character, they will have zero vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in vocab: -\n",
      "\n",
      "Vector values: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"First word in vocab: {}\\n\".format(word_lst[0]))\n",
    "print(\"Vector values: \\n{}\".format(word_vec[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word in vocab: unk\n",
      "Vector value: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Last word in vocab: {}\".format(word_lst[-1]))\n",
    "print(\"Vector value: \\n{}\".format(word_vec[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save them so later we can recall them in training text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('word_lst_gensim_w2v', word_lst)\n",
    "#np.save('word_vec_gensim_w2v', word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some checking on the saved lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_word_lst = np.load('word_lst_gensim_w2v.npy').tolist()\n",
    "load_word_vec = np.load('word_vec_gensim_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23191258,  3.7256334 , -4.3452015 ,  1.0801154 , -3.7996838 ,\n",
       "       -1.1255977 , -1.7133667 ,  1.6471709 ,  2.073226  ,  1.1289417 ,\n",
       "       -1.1795563 ,  0.14911233,  0.89349145,  2.4277902 , -4.1869807 ,\n",
       "       -2.626644  , -0.6793582 ,  6.502421  , -3.83722   ,  2.8329139 ,\n",
       "        2.7426562 , -3.8311145 ,  3.1579711 , -1.022657  ,  0.4647392 ,\n",
       "        0.9899303 , -1.4741098 , -1.7771564 ,  0.82412255, -0.6090162 ,\n",
       "       -2.5584276 ,  5.6834025 ,  1.3674535 , -0.15203974, -2.073013  ,\n",
       "        3.1131816 ,  0.8168806 , -0.508884  , -0.9480829 , -1.2174823 ,\n",
       "       -0.04121821,  3.1202126 , -1.4758865 ,  4.934309  , -0.6888991 ,\n",
       "        1.0782565 , -1.8044974 ,  0.6940486 , -1.0348865 ,  0.14809468,\n",
       "       -1.7579285 , -2.8125982 , -1.2737167 ,  1.6908863 ,  3.747259  ,\n",
       "        0.95193034, -3.3165982 , -0.27914476, -0.7361819 ,  0.7466166 ,\n",
       "       -4.218593  ,  1.4799529 , -2.7139335 , -1.0337161 , -1.6900539 ,\n",
       "        0.7303991 ,  0.23056044,  4.2733393 , -4.370119  ,  3.0245948 ,\n",
       "        0.30346823,  0.4819392 ,  1.7682562 ,  3.0824165 , -1.7094458 ,\n",
       "        1.7801512 ,  0.55406004, -0.5935638 , -3.0801113 , -1.5785261 ,\n",
       "        4.337188  , -1.6646698 ,  0.11552414, -3.2892056 ,  0.09237408,\n",
       "        2.798072  , -0.68291444, -0.67072654, -0.53505737,  0.5891601 ,\n",
       "        0.18107696,  1.265421  , -1.1216896 ,  1.0256852 , -0.8897321 ,\n",
       "        2.5508196 ,  0.17843504,  1.6782439 , -2.8855848 , -0.2117705 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of word 'bank' in word_list: 44\n",
      "Vector of word 'bank':\n",
      "[ 0.77163297  0.72085327 -0.17937817 -0.20104121 -0.69574487  0.43511963\n",
      " -0.57506287 -0.01126903  0.77312046  0.47661039 -0.62783295 -0.50222337\n",
      " -1.01169503 -0.73183298  1.15113175  1.01346171  3.33355951 -0.3658703\n",
      " -1.68693757  0.52608061 -2.01098037  0.20490231  0.05878011 -0.70031399\n",
      " -2.17487311  1.16204977 -2.3982594   1.77197659 -0.88600147  1.37544596\n",
      " -0.74634874  0.66399968 -2.09872293 -0.28664479 -1.54224348  0.04331245\n",
      " -2.49897432  2.14435387 -0.64037412 -1.57764399  1.54033315  1.75365996\n",
      " -1.55189586  0.64460242 -0.19954933 -2.0546608  -0.24295682 -0.33705029\n",
      "  0.33937937  1.14508641  0.21413738 -1.78668499 -2.04637361  0.23480865\n",
      " -1.13142443 -0.61997014  0.59658432 -0.00985729  1.17620337  0.06949378\n",
      "  1.05523515  2.03563571  0.29630861  0.12656495  0.4541429  -2.73681521\n",
      "  0.92469925 -1.2683326  -1.79471838  0.43595925  0.57826906 -0.37551475\n",
      " -1.47156572  2.06419945  1.63619328  1.51565218 -0.58776575  1.61784887\n",
      " -1.52668023  4.13499308  0.06202471 -0.83560961  1.84465766 -0.51899683\n",
      "  1.35448253  0.96688378  0.93229771 -0.57695597  0.02896765 -0.1053737\n",
      " -1.64353263  1.99453318  0.42556727  0.20548943 -1.45546484 -1.37480891\n",
      " -0.33118385  1.39924228 -1.31367958  0.92413813]\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of word 'bank' in word_list: {}\".format(load_word_lst.index(\"bank\")))\n",
    "print(\"Vector of word 'bank':\\n{}\".format(load_word_vec[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Result: \\n{}\".format(model.wv.word_vec('bank') - load_word_vec[44]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DOAN Tu My, Learning Word Embeddings - https://github.com/doantumy/Word-Embeddings\n",
    "2. Gensim Package - https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
