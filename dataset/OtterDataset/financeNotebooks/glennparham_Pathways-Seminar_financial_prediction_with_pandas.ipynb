{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 104: Time Series Basics with Pandas and Finance Data\n",
    "\n",
    "Contributed by: Avi Thaker https://github.com/athaker/CNTK\n",
    "November 20, 2016\n",
    "\n",
    "**[More Microsoft CNTK tutorials](https://notebooks.azure.com/CNTK/libraries/tutorials)**\n",
    "\n",
    "This tutorial will introduce the use of the Cognitive Toolkit for time series data. We show how to prepare time series data for deep learning algorithms. We will cover training a neural network and evaluating the neural network model. We will also look at the predictive potential on classification of an Exchange-traded Funds ([ETF](https://en.wikipedia.org/wiki/Exchange-traded_fund)), and in this simplified setting how one could trade it. This tutorial serves **only** as an example of how to use neural networks for time series analysis. \n",
    "    \n",
    "It is important to note that the stock market is extremely noisy and is difficult to predict. This is best done by professionals with domain expertise. It is more important to make sure the model is correct before setting up a trading system (there are many factors to consider including but not limited to: [curve fitting bias](https://en.wikipedia.org/wiki/Overfitting), [forward looking bias](http://www.investopedia.com/terms/l/lookaheadbias.asp?lgl=no-infinite), profitability etc.). The learnings and anecdotes presented in this tutorial is only for exemplary purposes with the goal of introducing an approach to analyze time series data.\n",
    "\n",
    "This tutorial introduces how to use pandas_datareader package and pandas. Please note, this tutorial will utilize the numpy interface to CNTK which interfaces well with [Pandas dataframes](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) (a structure that is well suited towards timeseries analysis). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import cntk as C\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for CNTK version\n",
    "if not C.__version__ == \"2.0\":\n",
    "    raise Exception(\"this notebook was designed to work with 2.0. Current Version: \" + C.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing stock data\n",
    "We first retrieve stock data using the method `get_stock_data`. This method downloads stock data on a daily timescale from Yahoo finance (can be modified to get data from Google Finance and many other sources). [Pandas datareader]( http://pandas-datareader.readthedocs.io/en/latest/remote_data.html) shows many use cases for this data reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method which obtains stock data from Yahoo finance\n",
    "# Requires that you have an internet connection to retreive stock data from Yahoo finance\n",
    "\n",
    "import time\n",
    "try:\n",
    "    from  pandas_datareader import data\n",
    "except ImportError:\n",
    "    !pip install pandas_datareader\n",
    "    from  pandas_datareader import data \n",
    "    \n",
    "# Set a random seed\n",
    "np.random.seed(123)\n",
    "\n",
    "def get_stock_data(contract, s_year, s_month, s_day, e_year, e_month, e_day):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        contract (str): the name of the stock/etf\n",
    "        s_year (int): start year for data\n",
    "        s_month (int): start month\n",
    "        s_day (int): start day\n",
    "        e_year (int): end year\n",
    "        e_month (int): end month\n",
    "        e_day (int): end day\n",
    "    Returns:\n",
    "        Pandas Dataframe: Daily OHLCV bars\n",
    "    \"\"\"\n",
    "    start = datetime.datetime(s_year, s_month, s_day)\n",
    "    end = datetime.datetime(e_year, e_month, e_day)\n",
    "    \n",
    "    retry_cnt, max_num_retry = 0, 3\n",
    "    \n",
    "    while(retry_cnt < max_num_retry):\n",
    "        try:\n",
    "            bars = data.DataReader(contract,\"google\", start, end)\n",
    "            return bars\n",
    "        except:\n",
    "            retry_cnt += 1\n",
    "            time.sleep(np.random.randint(1,10)) \n",
    "            \n",
    "    print(\"Google Finance is not reachable\")\n",
    "    raise Exception('Google Finance is not reachable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as  pkl\n",
    "\n",
    "# We search in cached stock data set with symbol SPY.               \n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): return envvar in os.environ\n",
    "\n",
    "def download(data_file):\n",
    "    try:\n",
    "        data = get_stock_data(\"SPY\", 2000, 1,2,2017,1,1)\n",
    "    except:\n",
    "        raise Exception(\"Data could not be downloaded\")\n",
    "        \n",
    "    dir = os.path.dirname(data_file)\n",
    "        \n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "    if not os.path.isfile(data_file):\n",
    "        print(\"Saving\", data_file )\n",
    "        with open(data_file, 'wb') as f:\n",
    "            pkl.dump(data, f, protocol = 2)\n",
    "    return data\n",
    "\n",
    "data_file = os.path.join(\"data\", \"Stock\", \"stock_SPY.pkl\")\n",
    "\n",
    "# Check for data in local cache\n",
    "if os.path.exists(data_file):\n",
    "        print(\"File already exists\", data_file)\n",
    "        data = pd.read_pickle(data_file) \n",
    "else: \n",
    "    # If not there we might be running in CNTK's test infrastructure\n",
    "    if is_test():\n",
    "        test_file = os.path.join(os.environ[envvar], 'Tutorials','data','stock','stock_SPY.pkl')\n",
    "        if os.path.isfile(test_file):\n",
    "            print(\"Reading data from test data directory\")\n",
    "            data = pd.read_pickle(test_file)\n",
    "        else:\n",
    "            print(\"Test data directory missing file\", test_file)\n",
    "            print(\"Downloading data from Google Finance\")\n",
    "            data = download(data_file)         \n",
    "    else:\n",
    "        # Local cache is not present and not test env\n",
    "        # download the data from Yahoo finance and cache it in a local directory\n",
    "        # Please check if there is trade data for the chosen stock symbol during this period\n",
    "        data = download(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the training paramaters\n",
    "\n",
    "The stock market behavior exhibits substantial [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation) ([reference](http://epchan.blogspot.com/2016/04/mean-reversion-momentum-and-volatility.html)). We use [ETF](http://www.investopedia.com/terms/e/etf.asp) `SPY` index representing the \"market\" of stock. This is the ETF that encompasses around top 500 companies in America by market capitalization. We will trade under the assumption that there is some short term autocorrelation that have predictive power in the market. \n",
    "\n",
    "### Predicting\n",
    "* Whether or not the next data for the given stock/ETF will be above or below the current day.\n",
    "\n",
    "### Predictors\n",
    "* The previous 8 days, classified if greater than the current day,\n",
    "\n",
    "* The volume changes as a percentage,\n",
    "\n",
    "* The percentage change from the previous day.\n",
    "\n",
    "Note, we are not feeding the neural network the price itself. Financial timeseries data are noisy. It is important not to overfit the data. There is a lot we can do here (smoothing, adding more features, etc.), but we will keep this tutorial simple, and demonstrate CNTK's ability to interface with timeseries data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature name list\n",
    "predictor_names = []\n",
    "\n",
    "# Compute price difference as a feature\n",
    "# 'df.shift' is a very pertinent function right now\n",
    "data[\"diff\"] = ...\n",
    "predictor_names.append(\"diff\")\n",
    "\n",
    "# Compute the volume difference as a feature\n",
    "# 'df.shift' is a very pertinent function right now\n",
    "data[\"v_diff\"] = \n",
    "predictor_names.append(\"v_diff\")\n",
    "\n",
    "# Compute the stock being up (1) or down (0) over different day offsets compared to current dat closing price\n",
    "num_days_back = ...\n",
    "\n",
    "for i in range(1, 'INSERT VARIABLE HERE'+1):\n",
    "    data[\"p_\" + str(i)] = np.where(data[\"Close\"] > data[\"Close\"].shift('INSERT VARIABLE HERE'), 1, 0) # i: number of look back days\n",
    "    predictor_names.append(\"p_\" + str(i))\n",
    "    \n",
    "# If you want to save the file to your local drive\n",
    "#data.to_csv(\"PATH_TO_SAVE.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we are trying to predict\n",
    "\n",
    "Here we are trying to predict whether or not the next days' trading will be above or below the current day. We will represent a predicted up day as a 1, else a 0 if the next day is the same or below. (Note: the market is unlikely going to close at the same price as it did the previous day). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"next_day\"] = np.where(data[\"Close\"].shift(-1) > data[\"Close\"], 1, 0)\n",
    "data[\"next_day_opposite\"] = np.where(data[\"next_day\"]==1,0,1) # The label must be one-hot encoded\n",
    "\n",
    "# Establish the start and end date of our training timeseries (picked 2000 days before the market crash)\n",
    "# start date: February 5, 2001\n",
    "# end date: January 20, 2009\n",
    "training_data = data[\"2001-02-05\":\"2009-01-20\"] \n",
    "\n",
    "# We define our test data as: data[\"2008-01-02\":]\n",
    "# This example allows to to include data up to current date\n",
    "\n",
    "test_data= data[\"2009-01-20\":\"2016-12-29\"] \n",
    "training_features = np.asarray(training_data[predictor_names], dtype = \"float32\")\n",
    "training_labels = np.asarray(training_data[[\"next_day\",\"next_day_opposite\"]], dtype=\"float32\")\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Why do we represent an up/down day as a 1 or 0 (respectively)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we are actually building the neural network itself. We will use a simple feedforward neural network (represented as `NN` in the plots) with 10 inputs and 50 dimensions. **You don't need to understand this...but it's good to familiarize yourself with it.**  You can read more about minibatch [here](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) and the pros/cons of larger values [here](https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we are using `layers` library in this tutorial. The details documentation can be found [here](https://cntk.ai/pythondocs/layerref.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build the network\n",
    "input_dim = 2 + num_days_back\n",
    "num_output_classes = 2 #Remember we need to have 2 since we are trying to classify if the market goes up or down 1 hot encoded\n",
    "num_hidden_layers = 2\n",
    "hidden_layers_dim = 2 + num_days_back\n",
    "input_dynamic_axes = [C.Axis.default_batch_axis()]\n",
    "input = C.input_variable(input_dim, dynamic_axes=input_dynamic_axes)\n",
    "label = C.input_variable(num_output_classes, dynamic_axes=input_dynamic_axes)\n",
    "\n",
    "def create_model(input, num_output_classes):\n",
    "    h = input\n",
    "    with C.layers.default_options(init = C.glorot_uniform()):\n",
    "        for i in range(0,num_hidden_layers):\n",
    "            h = C.layers.Dense(hidden_layers_dim, \n",
    "                               activation = C.relu)(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation=None)(h)   \n",
    "    return r\n",
    "    \n",
    "z = create_model(input, num_output_classes)\n",
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "label_error = C.classification_error(z, label)\n",
    "lr_per_minibatch = C.learning_rate_schedule(0.125,C.UnitType.minibatch)\n",
    "trainer = C.Trainer(z, (loss, label_error), [C.sgd(z.parameters, lr=lr_per_minibatch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the parameters for the trainer, we will train in large minibatches in sequential order\n",
    "minibatch_size = 100\n",
    "num_minibatches = len(training_data.index) // minibatch_size\n",
    "\n",
    "#Run the trainer on and perform model training\n",
    "training_progress_output_freq = 1\n",
    "\n",
    "# Visualize the loss over minibatch\n",
    "plotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n",
    "plotdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of passes through the data, how we train time series data\n",
    "This tutorial will differ from other tutorials in the sense that here we will not randomly send data to the trainer, instead each minibatch will be fed sequentially in the order of the time dimension. This is key to time series data-handling where we want to \"weigh\" the data at the end of our sample slightly higher. You can put in multiple passes, however you will notice significant performance degradation. Try it out! Additionally, multiple passes tend to overfit the financial timeseries data. This overfitting can be mitigated using standard ML approaches such as [L1 regularization](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.split(training_features,num_minibatches)\n",
    "\n",
    "print(\"Number of mini batches\")\n",
    "print(len(tf))\n",
    "\n",
    "print(\"The shape of the training feature minibatch\")\n",
    "print(tf[0].shape)\n",
    "\n",
    "tl = np.split(training_labels, num_minibatches)\n",
    "\n",
    "# It is key that we make only one pass through the data linearly in time\n",
    "num_passes = 1 \n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our neural network\n",
    "tf = np.split(training_features,num_minibatches)\n",
    "tl = np.split(training_labels, num_minibatches)\n",
    "\n",
    "for i in range(num_minibatches*num_passes): # multiply by the \n",
    "    features = np.ascontiguousarray(tf[i%num_minibatches])\n",
    "    labels = np.ascontiguousarray(tl[i%num_minibatches])\n",
    "    \n",
    "    # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n",
    "    trainer.train_minibatch({input : features, label : labels})\n",
    "    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "    if not (loss == \"NA\" or error ==\"NA\"):\n",
    "        plotdata[\"batchsize\"].append(batchsize)\n",
    "        plotdata[\"loss\"].append(loss)\n",
    "        plotdata[\"error\"].append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"loss\"], 'b--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Minibatch run vs. Training loss ')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(plotdata[\"batchsize\"], plotdata[\"error\"], 'r--')\n",
    "plt.xlabel('Minibatch number')\n",
    "plt.ylabel('Label Prediction Error')\n",
    "plt.title('Minibatch run vs. Label Prediction Error ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the trend for the label prediction error is still close to 50%. Remember that this is time variant, therefore it is expected that the system will have some noise as it trains through time. It should be noted; the model is still learning the market. Additionally, since this time series data is so noisy, having an error rate below 50% is good (many trading firms have win-rates of near 50% and have made money nearly every day [VIRTU](https://en.wikipedia.org/wiki/Virtu_Financial#Trading_activity)). However note they are high frequency trading firm and can leverage themselves up with low winrate strategies (51%). Trying to classify and trade every single day is expensive from transaction fees perspective. Therefore, one approach would be to trade when we think we are more likely to win?\n",
    "\n",
    "Let us try this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained the net, and we will do out of sample test to see how we did.\n",
    "# and then more importantly analyze how that set did\n",
    "\n",
    "test_features = np.ascontiguousarray(test_data[predictor_names], dtype = \"float32\")\n",
    "test_labels = np.ascontiguousarray(test_data[[\"next_day\",\"next_day_opposite\"]], dtype=\"float32\")\n",
    "\n",
    "avg_error = trainer.test_minibatch({input : test_features, label : test_labels})\n",
    "print(\"Average error: {0:2.2f}%\".format(avg_error * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have an error rate near 50%. At first glance this may appear to not have learned the network, but let us examine further and see if we have some predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = C.softmax(z)\n",
    "predicted_label_prob = out.eval({input:test_features})\n",
    "test_data[\"p_up\"] = pd.Series(predicted_label_prob[:,0], index = test_data.index)\n",
    "test_data[\"p_down\"] = predicted_label_prob[:,1]\n",
    "test_data['long_entries'] = np.where((test_data.p_up > 0.55), 1, 0)\n",
    "test_data['short_entries'] = np.where((test_data.p_down > 0.55) , -1, 0)\n",
    "test_data['positions'] = test_data['long_entries'].fillna(0) + test_data['short_entries'].fillna(0)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Data\n",
    "Here we take the output of our test set and compute the probabilities from the softmax function. Since we have probabilities we want to trade when there is a \"higher\" chance that we will be right, instead of just a >50% chance that the market will go in one direction. The goal is to find a signal, instead of trying to classify the market. Since the market is so noisy we want to only trade when we have an \"edge\" on the market. Moreover, trading frequently has higher fees (you have to pay each time you trade).\n",
    "\n",
    "We will say that if the prediction probability is greater than 55% (in either direction) we will take a position in the market. If it shows that the market will be up the next day with greater than 55% probability, we will take a 1-day long (make money off a stock when it goes up in price). If it is greater than a 55% chance that the next day will be below today's position we will take 1-day [short] (http://www.investopedia.com/university/shortselling/shortselling1.asp)(make money off the stock going down in value). \n",
    "\n",
    "We will then evaluate this timeseries performance by looking at some more metrics: average monthly return, standard deviation of monthly returns, the [Sharpe ratio](http://www.investopedia.com/terms/s/sharperatio.asp), and the [Maximum drawdown](https://en.wikipedia.org/wiki/Drawdown_%28economics%29). The Sharpe ratio is the average return minus the risk free rate (which is basically zero) over the standard deviation of returns normalized to a year. \n",
    "\n",
    "$$Sharpe = \\frac{r_p - r_f}{\\sigma_p}$$\n",
    "$$r_p = \\text{portfolio return}$$\n",
    "$$r_f = \\text{risk free rate}$$\n",
    "$$\\sigma_p = \\text{standard deviation of portfolio return}$$\n",
    "\n",
    "Here is how the financial industry ranks Sharpe Ratios:\n",
    "\n",
    "Below 1:  :/\n",
    "\n",
    "1: Good\n",
    "\n",
    "2: Great\n",
    "\n",
    "3: Great!!\n",
    "\n",
    "\n",
    "Generally, **the higher the Sharpe ratio, the better you are taking less risk for each unit of reward**. This assumes the mean return and standard deviation are sufficient to describe the distribution of returns, akin to an assumption of normally distributed returns.\n",
    "\n",
    "A trading strategy can be profitable even if the winrate is 50% or lower, if the wins are greater than the losses you can have a less than 50% winrate and still make some money (usually classified as momentum strategies). Finally, even if we do not beat the market by trading it individually, we can still outperform it by trading multiple assets that are uncorrelated with each other (**also known as \"diversification\"**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drawdowns(equity_curve):\n",
    "    \"\"\"\n",
    "    Calculate the largest peak-to-trough drawdown of the PnL curve\n",
    "    as well as the duration of the drawdown. Requires that the \n",
    "    pnl_returns is a pandas Series.\n",
    "\n",
    "    Parameters:\n",
    "    pnl - A pandas Series representing period percentage returns.\n",
    "\n",
    "    Returns:\n",
    "    drawdown, duration - Highest peak-to-trough drawdown and duration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the cumulative returns curve \n",
    "    # and set up the High Water Mark\n",
    "    # Then create the drawdown and duration series\n",
    "    hwm = [0]\n",
    "    eq_idx = equity_curve.index\n",
    "    drawdown = pd.Series(index = eq_idx)\n",
    "    duration = pd.Series(index = eq_idx)\n",
    "\n",
    "    # Loop over the index range\n",
    "    for t in range(1, len(eq_idx)):\n",
    "        cur_hwm = max(hwm[t-1], equity_curve[t])\n",
    "        hwm.append(cur_hwm)\n",
    "        drawdown[t]= (hwm[t] - equity_curve[t]) \n",
    "        duration[t]= 0 if drawdown[t] == 0 else duration[t-1] + 1\n",
    "    return drawdown.max(), duration.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "test_data[\"p_up\"].hist(bins=20, alpha=0.4)\n",
    "test_data[\"p_down\"].hist(bins=20, alpha=0.4)\n",
    "plt.title(\"Distribution of Probabilities\")\n",
    "plt.legend([\"p_up\", \"p_down\"])\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Probablity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"pnl\"] = test_data[\"Close\"].diff().shift(-1).fillna(0)*test_data[\"positions\"]/np.where(test_data[\"Close\"]!=0,test_data[\"Close\"],1)\n",
    "test_data[\"perc\"] = (test_data[\"Close\"] - test_data[\"Close\"].shift(1)) / test_data[\"Close\"].shift(1)\n",
    "monthly = test_data.pnl.resample(\"M\").sum()\n",
    "monthly_spy = test_data[\"perc\"].resample(\"M\").sum()\n",
    "avg_return = np.mean(monthly)\n",
    "std_return = np.std(monthly)\n",
    "sharpe = np.sqrt(12) * avg_return / std_return\n",
    "drawdown = create_drawdowns(monthly.cumsum())\n",
    "spy_drawdown = create_drawdowns(monthly_spy.cumsum())\n",
    "print(\"TRADING STATS\")\n",
    "print(\"AVG Monthly Return :: \" + \"{0:.2f}\".format(round(avg_return*100,2))+ \"%\")\n",
    "print(\"STD Monthly        :: \" + \"{0:.2f}\".format(round(std_return*100,2))+ \"%\")\n",
    "print(\"SHARPE             :: \" + \"{0:.2f}\".format(round(sharpe,2)))\n",
    "print(\"MAX DRAWDOWN       :: \" + \"{0:.2f}\".format(round(drawdown[0]*100,2)) + \"%, \" + str(drawdown[1]) + \" months\" )\n",
    "print(\"Correlation to SPY :: \" + \"{0:.2f}\".format(round(np.corrcoef(test_data[\"pnl\"], test_data[\"diff\"])[0][1],2)))\n",
    "print(\"NUMBER OF TRADES   :: \" + str(np.sum(test_data.positions.abs())))\n",
    "print(\"TOTAL TRADING DAYS :: \" + str(len(data)))\n",
    "print(\"SPY MONTHLY RETURN :: \" + \"{0:.2f}\".format(round(monthly_spy.mean()*100,2)) + \"%\")\n",
    "print(\"SPY STD RETURN     :: \" + \"{0:.2f}\".format(round(monthly_spy.std()*100,2)) + \"%\")\n",
    "print(\"SPY SHARPE         :: \" + \"{0:.2f}\".format(round(monthly_spy.mean()/monthly_spy.std()*np.sqrt(12),2)))\n",
    "print(\"SPY DRAWDOWN       :: \" + \"{0:.2f}\".format(round(spy_drawdown[0]*100,2)) + \"%, \"  + str(spy_drawdown[1]) + \" months\" )\n",
    "\n",
    "print(drawdown[0])\n",
    "(monthly.cumsum()*100).plot()\n",
    "(monthly_spy.cumsum()*100).plot()\n",
    "plt.legend([\"NN\", \"SPY\"],loc=2)\n",
    "plt.ylabel(\"% Return\")\n",
    "plt.title(\"TRADING SPY OUT OF SAMPLE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the % returns when we trade using SPY and NN based models only when we are > 55% sure of the predicted directionality.\n",
    "\n",
    "It is interesting to see here how the NN performs during the [financial crisis](https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008). This strategy makes money when many are losing ( refer to the dip in the green line in the plot between 2008-2010)! More importantly, this strategy has a significantly lower volatility (lower standard deviation in the blue line relative to the green line). \n",
    "\n",
    "In the summary statistic, you can also see a lower maximum drawdown of 13.62% for the NN model compared to the 55.37% for the SPY index over the same period of time. Similarly, a higher Sharpe ratio close to 1 from the NN model compared to 0.6 from the SPY index for the same period indicates the NN based decisions to be more stable (less risky). Thus, from a quant perspective it is more likely to be profitable (it makes money in many different market time periods). More importantly, the NN strategy is relatively uncorrelated with the SPY index (only 0.06 close to 0). \n",
    "\n",
    "**Suggested task**: Try to experiment with this network with different stocks and see what happens to the Sharpe ratio when you have multiple trading streams that are uncorrelated with each other. \n",
    "\n",
    "Remember when trading you can trade with leverage, and increase your return profile (as long as you are willing to handle the down times). The best trading strategies are the ones that can consistently make money no matter which market they are trading and no matter what is happening in the market. This \"edge\" is termed alpha in the quant industry.\n",
    "\n",
    "It is important to notice how much of a drawdown you will experience when trading (http://www.investopedia.com/terms/m/maximum-drawdown-mdd.asp?lgl=no-infinite). Since our current model is calculating only the percentage returns where returns are not reinvested, and we are not holding but trading, the larger the drawdown the harder it is to recover (if you lost 90% of your portfolio then have a 90% gain, you will only be at 19% of your starting value). \n",
    "\n",
    "Note, we should be tracking PNL on adjusted close prices(but not training), since the adjusted close prices incorporate data from the future (such as stock splits and dividends) which can incorporate bias into our network. Since Google Finance does not give adjusted prices, this is not a problem. However, PNL should be taken carefully due to splits and dividends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"p_up\"] = pd.Series(predicted_label_prob[:,0], index = test_data.index)\n",
    "test_data[\"p_down\"] = predicted_label_prob[:,1]\n",
    "test_data['long_entries'] = np.where((test_data.p_up > 0.50) , 1, 0)\n",
    "test_data['short_entries'] = np.where((test_data.p_down > 0.50) , -1, 0)\n",
    "test_data['positions'] = test_data['long_entries'].fillna(0) + test_data['short_entries'].fillna(0)\n",
    "test_data[\"pnl\"] = test_data[\"Close\"].diff().shift(-1).fillna(0)*test_data[\"positions\"]/np.where(test_data[\"Close\"]!=0,test_data[\"Close\"],1)\n",
    "monthly = test_data.pnl.resample(\"M\").sum()\n",
    "avg_return = np.mean(monthly)\n",
    "std_return = np.std(monthly)\n",
    "sharpe = np.sqrt(12) * avg_return / std_return\n",
    "drawdown = create_drawdowns(monthly.cumsum())\n",
    "\n",
    "print(\"TRADING STATS\")\n",
    "print(\"AVG Monthly Return :: \" + \"{0:.2f}\".format(round(avg_return*100,2))+ \"%\")\n",
    "print(\"STD Monthly        :: \" + \"{0:.2f}\".format(round(std_return*100,2))+ \"%\")\n",
    "print(\"SHARPE             :: \" +\"{0:.2f}\".format(round(sharpe,2)))\n",
    "print(\"MAX DRAWDOWN       :: \" + \"{0:.2f}\".format(round(drawdown[0]*100,2)) + \"%, \" + str(drawdown[1]) + \" months\" )\n",
    "print(\"Correlation to SPY :: \" + \"{0:.2f}\".format(round(np.corrcoef(test_data[\"pnl\"], test_data[\"diff\"])[0][1],2)))\n",
    "(monthly.cumsum()*100).plot()\n",
    "(monthly_spy.cumsum()*100).plot()\n",
    "plt.legend([\"NN\", \"SPY\"],loc=2)\n",
    "plt.ylabel(\"% Return\")\n",
    "plt.title(\"TRADING SPY OUT OF SAMPLE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The plot above shows the % returns when we trade every day using SPY and NN based models as compared to a confidence based trading show in previous plot. With frequent trading the volatility is higher and transaction fees (not accounted in this plot) will greatly eat into any profits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeatable factor (used for runtime verification)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Appendix\n",
    "A great resource for getting started as a Quant: https://www.quantstart.com/\n",
    "An online back-testing and open source trading platform, hosts quant competitions and a forum: https://www.quantopian.com/\n",
    "[A summary of backtesting and biases](https://www.quantstart.com/articles/successful-backtesting-of-algorithmic-trading-strategies-part-i)\n",
    "[For those who want to automatically trade in python](https://github.com/blampe/IbPy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
