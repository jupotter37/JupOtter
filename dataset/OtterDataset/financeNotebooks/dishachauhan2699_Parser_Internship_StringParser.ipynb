{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "import spacy\n",
    "\n",
    "import textblob\n",
    "from textblob import Word\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting spelling on words usin a big corpus of words, which finds the probability of the closest word possible to the wrong spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the string to get rid of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def pre_process(text):\n",
    "    #text = \" \".join(word.lower() for word in text.split())\n",
    "    text = re.sub(\"  \",\" \", text)         #Replacing double space with single space\n",
    "    text = re.sub(r'''[-()\\\"#/@;:{}`+=~|.!?,']''', \"\", text)     #Replacing special character with none\n",
    "    text = \" \".join(text for text in text.split() if text not in stop) #Removing stop words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Output Revenue, EBITDA margin for Steel and Metal stocks for past 10 qtrs\"\n",
    "sentence = pre_process(sentence)\n",
    "words = sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Output',\n 'Revenue',\n 'EBITDA',\n 'margin',\n 'Steel',\n 'Metal',\n 'stocks',\n 'past',\n '10',\n 'qtrs']"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correecting all the errors in Time period by creating a dictionary of each and every possible mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations  \n",
    "    \n",
    "def error_permutator( st): \n",
    "    start = st[0]\n",
    "    st = st[1:] \n",
    "    # Number of subsequences is (2**n -1) \n",
    "    n = len(st)\n",
    "    opsize = pow(2, n) \n",
    "   \n",
    "    # Generate all subsequences of a given string. \n",
    "    #  using counter 000..1 to 111..1 \n",
    "    for counter in range(1, opsize): \n",
    "      \n",
    "        subs = \"\" \n",
    "        for j in range(n): \n",
    "          \n",
    "            # Check if jth bit in the counter is set \n",
    "            #   If set then print jth element from arr[]  \n",
    "            if (counter & (1<<j)): \n",
    "                subs += (st[j]) \n",
    "   \n",
    "        # Print all permutations of current subsequence  \n",
    "        perm = permutations(subs) \n",
    "          \n",
    "        for i in perm: \n",
    "            errors[start+''.join(i)] = start+st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "time_period = ['years', 'year', 'quarter', 'quarters', 'month', 'months', 'day', 'days', 'hours', 'hour']\n",
    "for val in time_period:\n",
    "    error_permutator(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a fuction to check that if word can be converted into a int float or not and then using a for loop checking if the word next to that word is a time period or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_from_string(word):\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_indices = []\n",
    "for i in range(len(words)):\n",
    "    if no_from_string(words[i]):\n",
    "        if words[i+1] in errors.keys():\n",
    "            properties['Time'] = int(words[i])\n",
    "            words[i+1] = errors[words[i+1]]\n",
    "            properties['Time Period'] = words[i+1]\n",
    "            pop_indices += [words[i], words[i+1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting spelling using TextBlob class, is minutely slower but the code is small, the function defined above can be deployed easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [str(TextBlob(x).correct()) for x in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a list of sectors and fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sectors = ['Cement', 'Fertilizers', 'Trading', 'Pharmaceuticals', 'Paper', 'Bearings', 'Tyres', 'Textiles', 'Automobile', 'Hotels &                  Restaurants', 'Paints/Varnish', 'Mining & Mineral products', 'Chemicals', 'Auto Ancillaries', 'Finance', 'Consumer Durables',              'Sugar','Leather', 'Agro Chemicals', 'Capital Goods - Electrical Equipment', 'Castings, Forgings & Fastners', 'Plantation &                Plantation Products', 'Power Generation & Distribution', 'Glass & Glass Products', 'FMCG', 'Capital Goods-Non Electrical                Equipment','Construction', 'Packaging', 'Petrochemicals', 'Cement – Products', 'Cables', 'Miscellaneous', 'Engineering',                 'Entertainment','Shipping','Sanitaryware', 'Non Ferrous Metals', 'IT – Software', 'Steel', 'Tobacco Products', 'Alcoholic                  Beverages','Diversified', \"Dry cells\", 'Retail', 'Infrastructure Developers & Operators', 'Realty', 'Telecomm Equipment &                 Infra Services', 'Refineries', 'Ceramic Products','Plastic products', 'Healthcare', 'Edible Oil', 'Diamond, Gems and Jewellery', 'IT - Hardware', 'Refractories', 'Crude Oil & Natural Gas','Printing & Stationery', 'Oil Drill/Allied', 'Electronics', 'Banks', 'Telecom-Handsets/Mobile', 'Logistics', 'Media - Print/Television/Radio', 'Education', 'Telecomm-Service', 'Readymade Garments/ Apparells', 'Computer Education', 'Credit Rating Agencies', 'Air Transport Service', 'Stock/ Commodity Brokers', 'Insurance', 'Ship Building', 'Marine Port & Services', 'Gas Distribution', 'Infrastructure Investment Trusts','Power Infrastructure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamentals = ['Revenue', 'Inventory']  # This list will further expand when a proper fundamentals list is                                                                        # there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching syntatctice similarities has been done using nltk library, this was an initial approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patching Syntatic similarities in Sectors\n",
    "for word in Sectors:\n",
    "    for dat in words:\n",
    "        if word in dat:\n",
    "            properties['Sector'].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of NLTK functionality and what it can extract from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \" \".join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search('Revenue',sen):\n",
    "    sen_tokenized = nltk.word_tokenize(sen)\n",
    "    tagged = nltk.pos_tag(sen_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Output', 'NNP'),\n ('Revenue', 'NNP'),\n ('EBITDA', 'NNP'),\n ('margin', 'NN'),\n ('Steel', 'NNP'),\n ('Metal', 'NNP'),\n ('stocks', 'NNS'),\n ('past', 'NN')]"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing on the basis of word 'for' which breaks the Sentence into different units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Sector': None, 'Fundamentals': None, 'Time Period': None, 'Time': None}"
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "properties = dict.fromkeys(['Sector' , 'Fundamentals', 'Time Period' , 'Time'])\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Output Revenue, EBITDA margin for Steel and Metal stocks for past 10 qtrs\".split('for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Output Revenue, EBITDA margin ', ' Steel and Metal stocks ', ' past 10 qtrs']"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_parser_list = []\n",
    "sector_parser_list = []\n",
    "fundamental_parser_list = []\n",
    "number_in_sen = False\n",
    "period_in_sen = False\n",
    "\n",
    "# Extracting the list with time\n",
    "for sen in sentence:\n",
    "    sen_token = nltk.word_tokenize(sen)\n",
    "    for word in sen_token:\n",
    "        if no_from_string(word):\n",
    "            number_in_sen = True\n",
    "        if word in errors.keys():\n",
    "            period_in_sen = True\n",
    "    if( number_in_sen and period_in_sen):\n",
    "        time_parser_list = sen_token\n",
    "\n",
    "## Extracting sub sentence with sectors and fundamentals\n",
    "for sen in sentence:\n",
    "    sen_token = nltk.word_tokenize(sen)\n",
    "    for word in sen_token:\n",
    "        if word in Sectors:\n",
    "            sector_parser_list = sen_token\n",
    "        elif word in fundamentals:\n",
    "            fundamental_parser_list = sen_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['past', '10', 'qtrs']"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "time_parser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Steel', 'and', 'Metal', 'stocks']"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "sector_parser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Output', 'Revenue', ',', 'EBITDA', 'margin']"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "fundamental_parser_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sen = ' '.join(word for word in time_parser_list)\n",
    "for i in range(len(time_parser_list)):\n",
    "    if no_from_string(time_parser_list[i]):\n",
    "        if time_parser_list[i+1] in errors.keys():\n",
    "            properties['Time'] = int(time_parser_list[i])\n",
    "            time_parser_list[i+1] = errors[time_parser_list[i+1]]\n",
    "            properties['Time Period'] = time_parser_list[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Sector': None, 'Fundamentals': None, 'Time Period': 'quarters', 'Time': 10}"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "sector_sen = \" \".join(word for word in sector_parser_list)\n",
    "sector_sen = pre_process(sector_sen) # will remove all stop words, useless signs \n",
    "sen_tokenized = nltk.word_tokenize(sector_sen)\n",
    "tagged = nltk.pos_tag(sen_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Steel', 'NNP'), ('Metal', 'NNP'), ('stocks', 'NNS')]"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['Sector'] = []\n",
    "for tags in tagged:\n",
    "    if 'NNP' in tags or 'NN' in tags or tags[0] in Sector:\n",
    "        properties['Sector'].append(tags[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Steel', 'Metal']"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "properties['Sector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "Fundamentals_sen = \" \".join(word for word in fundamental_parser_list)\n",
    "Fundamentals_sen = pre_process(Fundamentals_sen) # will remove all stop words, useless signs \n",
    "sen_tokenized = nltk.word_tokenize(Fundamentals_sen)\n",
    "tagged = nltk.pos_tag(sen_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('Output', 'NNP'), ('Revenue', 'NNP'), ('EBITDA', 'NNP'), ('margin', 'NN')]"
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generally fundamentals are followed by word that are either in FULL CAPS or are themselves in full caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties['Fundamentals'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Output', 'Revenue', ',', 'EBITDA', 'margin']"
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "fundamental_parser_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fundamental_parser_list)):\n",
    "    if fundamental_parser_list[i].isupper():\n",
    "        if not fundamental_parser_list[i+1].isupper():\n",
    "            properties['Fundamentals'].append(fundamental_parser_list[i]+' '+fundamental_parser_list[i+1])\n",
    "        else:\n",
    "            properties['Fundamentals'].append(fundamental_parser_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags in tagged:\n",
    "    if ('NNP' in tags or 'NN' in tags) and tags[0] in fundamentals:\n",
    "        properties['Fundamentals'].append(tags[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['EBITDA margin', 'Revenue']"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "properties['Fundamentals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Sector': ['Steel', 'Metal'],\n 'Fundamentals': ['EBITDA margin', 'Revenue'],\n 'Time Period': 'quarters',\n 'Time': 10}"
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitkeshavirtualenv31f71cf241fa410f8abf6a58dc6422d0",
   "display_name": "Python 3.7.3 64-bit ('kesha': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}