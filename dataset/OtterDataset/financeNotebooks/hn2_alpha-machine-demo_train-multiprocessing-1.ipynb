{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we show positive training and testing returns using RL agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7665e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing library\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "# import optuna\n",
    "import gym\n",
    "from torch import nn as nn\n",
    "import joblib\n",
    "import gym_anytrading\n",
    "import yfinance as yf\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from matplotlib import pyplot as plt\n",
    "# import finta as TA #technical indicators library imported..\n",
    "from gym_anytrading.envs import StocksEnv\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for trading\n",
    "# This is the trading training environment\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    metadata = { 'render.modes': [ 'human' ] }\n",
    "\n",
    "    def __init__(self, df, loss_constraint=0.4):\n",
    "\n",
    "        super(TradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        #print(df.shape)\n",
    "        self.metrics = []\n",
    "        self.step_index = 0\n",
    "        \n",
    "        # Position size from 1 to 10\n",
    "        self._position = 0\n",
    "        # Initial Action \n",
    "        self._action = \"NONE\"\n",
    "        self._reward = 0\n",
    "        self._cumulative_reward = 0\n",
    "        self._max_cumulative_reward = 0\n",
    "        self.loss_constraint = loss_constraint\n",
    "        \n",
    "        self.action_space = spaces.Discrete(20)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float16)\n",
    "\n",
    "    def step(self, action):\n",
    "        done=False\n",
    "        self._take_action(action)\n",
    "        self._reward = self.compute_reward()\n",
    "        reward = self._reward\n",
    "        self._cumulative_reward = self._cumulative_reward + reward\n",
    "        self.step_index +=1\n",
    "        \n",
    "        # keeping track of maximum cumulative return\n",
    "        if self._cumulative_reward > self._max_cumulative_reward:\n",
    "            self._max_cumulative_reward = self._cumulative_reward\n",
    "\n",
    "        # if current accumulated reward is less than max reward achieved in the past by more than 40%\n",
    "        # you have lost 40% of accrued capital, it's time to stop-out\n",
    "        if (self._cumulative_reward - self._max_cumulative_reward) < (self.loss_constraint*-1):\n",
    "            #print('Agent losses')\n",
    "            #reward = \n",
    "            #self._cumulative_reward = self._cumulative_reward + reward\n",
    "            done=True\n",
    "        \n",
    "        info = self._compute_step_info(reward)\n",
    "        self.metrics.append(info)\n",
    "\n",
    "        if self.step_index == (self.df.shape[0]-1):\n",
    "            done = True\n",
    "        \n",
    "        if done:\n",
    "            self.metrics = []\n",
    "            self.step_index = 0\n",
    "                #print(\"Agent Gets Positive Returns!!!\")\n",
    "        obs = self._next_observation()\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        #self.df=df\n",
    "        self.metrics = []\n",
    "        self.step_index = 0\n",
    "        self._reward = 0\n",
    "        self._cumulative_reward = 0\n",
    "        self._max_cumulative_reward = 0\n",
    "        \n",
    "        # initial position is zero contracts\n",
    "        self._position = 0\n",
    "        # Initial Action \n",
    "        self._action = \"NONE\"\n",
    "        #print(self.df.iloc[0,1])\n",
    "\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _compute_step_info(self, reward):\n",
    "        return {\n",
    "            'Date': self.df.index[self.step_index],\n",
    "            'Action': self._direction,\n",
    "            'Contracts': np.abs(self._position),\n",
    "            'UnLevered Return': np.round(reward*100/np.abs(self._position),3),\n",
    "            'Levered Return': np.round(reward*100,3),\n",
    "            'Cumulative Levered Return': np.round(self._cumulative_reward*100,3),\n",
    "            'Max Cumulative Levered Return': np.round(self._max_cumulative_reward*100,3)\n",
    "            \n",
    "           \n",
    "        }\n",
    "\n",
    "    def _next_observation(self):\n",
    "        \n",
    "        #print(df.shape)\n",
    "        obs = np.array([\n",
    "            self.df.iloc[self.step_index, 1],\n",
    "            #self.df.iloc[self.step_index, 2],\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        action = action + 1\n",
    "        if action <=10:\n",
    "            self._position = action\n",
    "            self._direction = \"BUY\"\n",
    "        \n",
    "        else:\n",
    "            self._position = 10-action\n",
    "            self._direction = \"SELL\"\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        \n",
    "        reward = self._position * self.df.iloc[self.step_index, 0]\n",
    "        \n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df with return as the dependent variable and lagged returns as feature\n",
    "df = yf.download('SPY', start='2021-11-06', end='2022-02-06', progress=False, index_col=0)\n",
    "df.drop(\"Close\", axis=1, inplace=True)\n",
    "df.rename(columns = {'Adj Close':'Close'}, inplace = True)\n",
    "df.head()\n",
    "df.fillna(0,inplace=True)\n",
    "df.drop(df.tail(1).index,inplace=True) # drop last n rows\n",
    "df.head(5)\n",
    "df.drop(\"Open\", axis=1, inplace=True)\n",
    "df.drop(\"High\", axis=1, inplace=True)\n",
    "df.drop(\"Low\", axis=1, inplace=True)\n",
    "df.drop(\"Volume\", axis=1, inplace=True)\n",
    "df[\"Close\"].plot()\n",
    "df[\"Dependent\"]=df[\"Close\"].pct_change(1) # 1 for ONE DAY lookback\n",
    "df['Dependent'] = np.log(1 + df.Dependent)\n",
    "df['Feature1']=df['Dependent'].shift(1)\n",
    "\n",
    "df.drop(\"Close\", axis=1, inplace=True)\n",
    "\n",
    "df.fillna(0,inplace=True)\n",
    "\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4619415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiple environments\n",
    "\n",
    "def make_env(rank, df):\n",
    "    #\"\"\"\n",
    "    #Utility function for multiprocessed env.\n",
    "\n",
    "    #:param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    #:param rank: (int) index of the subprocess\n",
    "    #\"\"\"\n",
    "    #print(\"what up out\")\n",
    "    def _init():\n",
    "        env = TradingEnv(df=df, loss_constraint=0.4)\n",
    "        \n",
    "        return env\n",
    "    \n",
    "    return _init\n",
    "num_cpu = 4\n",
    "env = SubprocVecEnv([make_env(i, df) for i in range(num_cpu)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model. \n",
    "#best_trial_parameters_model = PPO('MlpPolicy', env, verbose=0, gamma=0.99, learning_rate=0.0006723344399829697, batch_size=64, seed=100)\n",
    "best_trial_parameters_model = PPO('MlpPolicy', env, verbose=1, gamma=0.995, learning_rate=0.014016620989668828, batch_size=512, seed=100)\n",
    "\n",
    "best_trial_parameters_model.learn(500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2242bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "\n",
    "env = TradingEnv(df=df, loss_constraint=0.4)\n",
    "env_maker = lambda: env\n",
    "env = DummyVecEnv([env_maker])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
    "env.save(\"vec_normalize.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model returns\n",
    "rewards = []\n",
    "n_episodes, reward_sum = 0, 0.0\n",
    "cumulative_returns =pd.DataFrame(columns=['Date', 'Cumulative Leveraged Return'])\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while n_episodes < 1:\n",
    "    \n",
    "    action, _ = best_trial_parameters_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    df2 = pd.DataFrame({'Date': [info[0][\"Date\"]],\n",
    "                    'Cumulative Leveraged Return' : [info[0][\"Cumulative Levered Return\"]]})\n",
    "\n",
    "\n",
    "    cumulative_returns = pd.concat([cumulative_returns, df2], ignore_index = True, axis = 0)\n",
    "\n",
    " \n",
    "    print(info)\n",
    "    \n",
    "    if done:\n",
    "        rewards = (info[0][\"Cumulative Levered Return\"])\n",
    "        n_episodes += 1\n",
    "        obs = env.reset()\n",
    "\n",
    "\n",
    "env.close()\n",
    "last_reward = rewards\n",
    "print(\"Total Cumulative Levered Return (%): \",last_reward)\n",
    "cumulative_returns['Date'] = pd.to_datetime(cumulative_returns['Date'])\n",
    "cumulative_returns.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd277a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns.plot(title=\"Train: Cumulative Levered SPY Returns\", figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data \n",
    "\n",
    "df = yf.download('SPY', start='2022-02-07', end='2022-03-06', progress=False)\n",
    "df.drop(\"Close\", axis=1, inplace=True)\n",
    "df.rename(columns = {'Adj Close':'Close'}, inplace = True)\n",
    "df.head()\n",
    "df.fillna(0,inplace=True)\n",
    "df.drop(df.tail(1).index,inplace=True) # drop last n rows\n",
    "df.head(5)\n",
    "df.drop(\"Open\", axis=1, inplace=True)\n",
    "df.drop(\"High\", axis=1, inplace=True)\n",
    "df.drop(\"Low\", axis=1, inplace=True)\n",
    "df.drop(\"Volume\", axis=1, inplace=True)\n",
    "df[\"Close\"].plot()\n",
    "df[\"Dependent\"]=df[\"Close\"].pct_change(1) # 1 for ONE DAY lookback\n",
    "df['Dependent'] = np.log(1 + df.Dependent)\n",
    "df['Feature1']=df['Dependent'].shift(1)\n",
    "df.drop(\"Close\", axis=1, inplace=True)\n",
    "df.fillna(0,inplace=True)\n",
    "df.head(10)\n",
    "\n",
    "# create environment\n",
    "env = TradingEnv(df=df, loss_constraint=0.4)\n",
    "env_maker = lambda: env\n",
    "env = DummyVecEnv([env_maker])\n",
    "env = VecNormalize.load(\"vec_normalize.pkl\", env)\n",
    "env.norm_obs =True\n",
    "# reward normalization is not needed at test time\n",
    "env.norm_reward = True\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d88c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards = []\n",
    "cumulative_returns =pd.DataFrame(columns=['Date', 'Cumulative Leveraged Return'])\n",
    "n_episodes, reward_sum = 0, 0.0\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "while n_episodes < 1:\n",
    "    \n",
    "    action, _ = best_trial_parameters_model.predict(obs, deterministic=True)\n",
    "    \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    newRow={'Date':info[0][\"Cumulative Levered Return\"],'Cumulative Leveraged Return':info[0][\"Cumulative Levered Return\"]}\n",
    "    df2 = pd.DataFrame({'Date': [info[0][\"Date\"]],\n",
    "                    'Cumulative Leveraged Return' : [info[0][\"Cumulative Levered Return\"]]})\n",
    "\n",
    "    cumulative_returns = pd.concat([cumulative_returns, df2], ignore_index = True, axis = 0)\n",
    "\n",
    "    #print(info)\n",
    "    \n",
    "    if done:\n",
    "        rewards = (info[0][\"Cumulative Levered Return\"])\n",
    "        n_episodes += 1\n",
    "        obs = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "last_reward = rewards\n",
    "print(\"Total Cumulative Levered Return (%): \",last_reward)\n",
    "cumulative_returns['Date'] = pd.to_datetime(cumulative_returns['Date'])\n",
    "cumulative_returns.set_index('Date', inplace=True)\n",
    "cumulative_returns.head(5)\n",
    "cumulative_returns.plot(title=\"Test: Cumulative Levered Returns\", figsize=(12,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
