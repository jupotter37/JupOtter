{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework3, Neural Network\n",
    "\n",
    "In this section, you will implement a non-linear classifier using the Neural Network.\n",
    "\n",
    "Let's start with a simple code using Neural Network.\n",
    "\n",
    "First, get prepared by importing the numpy library as 'np'.(Or just run the code below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from Quandl import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning AND, OR by Single Perceptron\n",
    "\n",
    "We will start with an intuitive Neural Network, which has no hidden layer at all.\n",
    "\n",
    "> First we will learn the two basic linear logistic classification problems, 'AND' and 'OR'.\n",
    "\n",
    "<img src=\"img/hw3/AND.jpg\" alt=\"Drawing\" style=\"width: 400px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = 1\n",
    "\n",
    "X = np.array([\n",
    "        [bias, 0,0],\n",
    "        [bias, 0,1],\n",
    "        [bias, 1,0],\n",
    "        [bias, 1,1]\n",
    "    ])\n",
    "\n",
    "Y_AND = np.array([\n",
    "        [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1]\n",
    "    ])\n",
    "\n",
    "Y_OR = np.array([\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem1. Weights for a single perceptron\n",
    "\n",
    "The given theta for learning 'AND' and 'OR' are provided below.\n",
    "\n",
    "Run the code and provide your system the given weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The given weight is only appliable for sigmoid function.\n",
    "\n",
    "TO DO :\n",
    "\n",
    "Fill in the weights w1, w2, w3 for each theta so that the result gives the right answer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#your code here\n",
    "theta_AND = np.array([\n",
    "        [and_w1, and_w2, and_w3]\n",
    "    ])\n",
    "\n",
    "theta_OR = np.array([\n",
    "        [or_w1, or_w2, or_w3]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an activation function, we are going to use both the softmax function and the sigmoid function.\n",
    "\n",
    "> Codes for both implementations are described below.\n",
    "\n",
    "<img src=\"img/hw3/sigmoid.png\" alt=\"Drawing\" style=\"width: 350px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem2. Sigmoid function\n",
    "\n",
    "Fill in the missing part of the sigmoid function with your code.\n",
    "\n",
    "For additional explanation, d=True will activate the differentiation of the sigmoid function,\n",
    "\n",
    "while d=False will return the pure sigmoid function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(g, d=False):\n",
    "        if(d):\n",
    "            gradientSigmoid = # your code here. The differentiation of sigmoid by z\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = # your code here. The normal sigmoid\n",
    "            return normalSigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code below, you can confirm that both 'AND' and 'OR' has been successfully implemented using Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flag = False # gradients are only needed in the case for back propagation.\n",
    "\n",
    "\"\"\"\n",
    "Check if AND = [0, 0, 0, 1], OR = [0, 1, 1, 1]\n",
    "\"\"\"\n",
    "\n",
    "h_AND_sigmoid = np.round(sigmoid(np.dot(X, theta_AND.T), flag))\n",
    "print \"AND\"\n",
    "print h_AND_sigmoid.T\n",
    "\n",
    "print \"\"\n",
    "\n",
    "h_OR_sigmoid = np.round(sigmoid(np.dot(X, theta_OR.T), flag))\n",
    "print \"OR\"\n",
    "print h_OR_sigmoid.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning XOR by Neural Network\n",
    "\n",
    "\n",
    "Now, let's move on to the non-linear classifying problems.\n",
    "\n",
    "Until now, 'AND' and 'OR' where both linear classifications, so we could implemented\n",
    "\n",
    "both methods by forward propagation by easily combinating theta's without a hidden layer.\n",
    "\n",
    "However, in the case of non-linear functions such as 'XOR', it's not quite easy to come up with an appropriate theta.\n",
    "\n",
    "From now on, we will adopt another layer(which is called the hidden layer) in order to successfully classify this problem.\n",
    "\n",
    "> Our target is that to return 0 if the two inputs are the same, and 1 if the two are differrent.\n",
    "\n",
    "> Fill in the indicated spaces with your own code for the function 'sigmoid' and 'forward_propagation'.\n",
    "\n",
    "> The 'backward_propagation' learning code part is already implemented.\n",
    "\n",
    "> If your algorithm is correct, the code below will run without any trouble and successfully learn the results  of 'XOR'.\n",
    "\n",
    "<img src=\"img/hw3/XOR.jpg\" alt=\"Drawing\" style=\"width: 400px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem3. Forward Propagation and Cost Function for Neural Network.\n",
    "\n",
    "Neural Network is consisted with 3 important features.\n",
    "\n",
    "> 1. Forward Propagation for prediction.\n",
    "\n",
    "> 2. Backward Propagation for Learning.\n",
    "\n",
    "> 3. Cost Function for providing a threshold to learning.\n",
    "\n",
    "As implementing Backward Propagation by your own will be very tricky, I have wrote the code down for you.\n",
    "\n",
    "You will need to fill in the rest of the function consisting the class.\n",
    "\n",
    "If all your implementations are correct, you might see that the XOR learning below will operate properly.\n",
    "\n",
    "This is a \"Hello World!\" example in the field of Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This will create a Neural Network with specified inputs and outputs.\n",
    "\n",
    "Input : When building a constructor, you should pass on the size of the input layer, the output layer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, InputLayer, OutputLayer):\n",
    "        # Define the number of nodes in each layers\n",
    "        self.inputLayer = InputLayer\n",
    "        self.hiddenLayer = InputLayer\n",
    "        self.output = OutputLayer\n",
    "\n",
    "        # create node weight matrices\n",
    "        self.theta0 = 2*np.random.random((self.hiddenLayer, self.inputLayer+1))-1\n",
    "        self.theta1 = 2*np.random.random((self.output, self.hiddenLayer+1))-1\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, g, d=False):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        if(d):\n",
    "            gradientSigmoid = # your code here. Same code above.\n",
    "            return gradientSigmoid\n",
    "        else:\n",
    "            normalSigmoid = # your code here. Same code above.\n",
    "            return normalSigmoid\n",
    "\n",
    "        \n",
    "    def costFunction(self, X, y):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        m = len(y) # 'm' represents the number of training examples.\n",
    "        cost = # write your code here. Code for the cost of 'logistic regression'\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        #####################################################################\n",
    "        ####################### Write your code here ########################\n",
    "        #####################################################################\n",
    "        \n",
    "        # Forward Propagate Inputs Through Network\n",
    "        m = # your code here. 'm' means the total number of rows(training examples) in your input.\n",
    "        \n",
    "        # Before starting your propagation, you need to add bias in your input unit\n",
    "        self.a1 = # your code here. Add bias nodes to the 2 dimensional array representing your input.\n",
    "        \n",
    "        # Input Layer -> Hidden Layer\n",
    "        self.z2 = # your code here. The dot product of a1 and theta0\n",
    "        self.a2 = # your code here. The sigmoid activated result of z2\n",
    "        \n",
    "        # Before starting next propagation, add bias 1 in your hidden layer unit\n",
    "        self.a2 = # your code here. Stack the hidden nodes with bias '1'\n",
    "        \n",
    "        # Hidden Layer -> Output Layer\n",
    "        self.z3 = # your code here. The dot product of a2 and theta1\n",
    "        self.ho = # your code here. The sigmoid activated result of z3\n",
    "        \n",
    "        # Final output of the neural network\n",
    "        return self.ho\n",
    "\n",
    "    def backward_propagation(self, X, y, alpha):\n",
    "        # for e in range(epoch):\n",
    "        max_iteration = 500\n",
    "        self.err = np.ones(max_iteration)\n",
    "        final_error = 1\n",
    "        iteration = 0\n",
    "        \n",
    "        while self.err[iteration] > 1e-10 :\n",
    "            # Forward propogate\n",
    "            a3 = self.forward_propagation(X)\n",
    "            \n",
    "            # Backward Propagate\n",
    "            delta3 = a3 - y\n",
    "            theta1_grad = np.dot(delta3.T, self.a2)\n",
    "            delta2 = np.dot(delta3, self.theta1[:, 1:]) * self.sigmoid(self.z2, d=True)\n",
    "            theta0_grad = np.dot(delta2.T, self.a1)\n",
    "\n",
    "            change = theta0_grad\n",
    "            self.theta0 -= (alpha-change*0.1) * theta0_grad\n",
    "            change = theta1_grad\n",
    "            self.theta1 -= (alpha-change*0.1) * theta1_grad\n",
    "            \n",
    "            if(iteration + 1 == 500):\n",
    "                return self.err\n",
    "            \n",
    "            final_error = self.costFunction(X, y)\n",
    "            self.err[iteration+1] = final_error\n",
    "            iteration = iteration+1\n",
    "            \n",
    "        return self.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beneath is the training example for XOR code. The results should be an hypothesis array of [0, 1, 1, 0]\n",
    "\n",
    "<img src=\"img/hw3/XOR_network.jpg\" alt=\"Drawing\" style=\"width: 200px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR training data\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your own sake, we also implemented a plotting system so you could visually observe the error eventually converging to 0.\n",
    "\n",
    "We also implemented a variety of skills in order to prevent the gradient descent from falling into a local minimum.\n",
    "\n",
    "Check out the code for further notations.\n",
    "\n",
    "> You should close the pop-up screen(showing you the cost graph, visualizing your error respect to iterations)\n",
    "\n",
    "> in order to observe your final result.\n",
    "\n",
    "> The final result should be a numpy array of [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2, 1)\n",
    "\n",
    "print \"Error before training = \", nn.costFunction(X, y)\n",
    "\n",
    "# learning thetas\n",
    "max_iteration = 500\n",
    "min_err = 1\n",
    "for i in range(10):\n",
    "    nn = NeuralNetwork(2, 1)\n",
    "    \n",
    "    cost = np.zeros(max_iteration)\n",
    "    cost = nn.backward_propagation(X, y, 0.5)         # learning rate = 0.5 , epoch = 1\n",
    "    err = nn.costFunction(X, y)\n",
    "    if (min_err > err):\n",
    "        min_err = err\n",
    "        result = nn.forward_propagation(X)\n",
    "        min_cost = cost\n",
    "\n",
    "print \"Error after training = \", min_err\n",
    "print np.round(result.T)\n",
    "\n",
    "# visualize cost: y-axis, iterations(epochs): x-axis\n",
    "plt.plot(np.arange(0., max_iteration, 1.0), min_cost.T)\n",
    "plt.axis([0, max_iteration, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network in Finance\n",
    "\n",
    "Now we are going to move on to the application of Neural Network, especially in the field of finance.\n",
    "\n",
    "We are going to take 3 simple features in finance(S&P500, Dow Jones IA, PIR) and will predict the NASDAQ index.\n",
    "\n",
    "We will crawl the data we need from Yahoo finance.\n",
    "\n",
    "But, at first, beneath is a brief explanation of the indices above for those you are not familiar with finance.\n",
    "\n",
    "> Reading the material below is purely optional, so you may as well skip and move on to the coding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pattern import web\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. S&P500\n",
    "\n",
    "The S&P500 is a free-float capitalization-weighted index published since 1957 of the prices of 500 large-cap common stocks actively traded in the United States. The stocks included in the S&P500 are those of large publicly held companies that trade on either of the two largest American stock market companies; the NYSE Euronext and the NASDAQ OMX. Actually, the S&P500 is one of the most widely followed indexes of large-cap American stocks. It is considered a bellwether for the American economy, and is included in the Index of Leading Indicators. S&P500 index fluctuations are dependent upon a lot of factors, thus the entire prediction pattern is very complex. In this application, the input data is represented only by historical items of 4 important economical indicators. It is essential to mention that if you want a better predictor, you should feed your neural network with more indicators that are more or less important for the entire interpolation. \n",
    "\n",
    "<img src=\"img/hw3/SP500.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem4. Crawling Information from Yahoo Finance.\n",
    "\n",
    "As you have previously done in Homework1, you have to scrap the information from Yahoo finance.\n",
    "\n",
    "Those who have done Homework1 on there own will have no trouble in following the rest of the implementation.\n",
    "\n",
    "Questions about specific methods for Crawling information will NOT be guided.\n",
    "\n",
    "<img src=\"img/hw3/Screenshot.png\" alt=\"Drawing\" style=\"float: left;\"/>\n",
    "\n",
    "Above is a screenshot for the Yahoo Finance S&P500 page.\n",
    "\n",
    "> Fill in the function 'scrap_page' below in order to crawl the given URL and retreive the information\n",
    "\n",
    "(Monthly Historical prices for Adjacent Close).\n",
    "\n",
    "The URLs you need are provided below.\n",
    "\n",
    "> S&P500 URL = http://finance.yahoo.com/q/hp?s=%5EGSPC&g=m\n",
    "\n",
    "> Dow Jones Industrial Average URL = http://finance.yahoo.com/q/hp?s=%5EDJI&g=m\n",
    "\n",
    "> NASDAQ Composite URL = http://finance.yahoo.com/q/hp?s=NDAQ&g=m\n",
    "\n",
    "> PIR Composite URL = http://finance.yahoo.com/q/hp?s=PIR&g=m\n",
    "\n",
    "You will notice that the URL is consisted with 'http://finance.yahoo.com/q/hp?s=' + id + '&g=m'\n",
    "\n",
    "Utilize this in order to fill in the 'scrap_page' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_page(request_id):\n",
    "    print \"fetching \" , request_id\n",
    "        \n",
    "    target_url = # your code here. Consist a pattern for the URL with the input.\n",
    "    r = # your code here. Get requests for the given URL.\n",
    "    \n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SP500_code = '%5EGSPC'\n",
    "SP500_html = scrap_page(SP500_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beneath is a str2number function you have already implemented in your previous Homework1.\n",
    "\n",
    "You DON'T need to do anything to the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2number(number_str):\n",
    "        if number_str is None:\n",
    "            return None\n",
    "        else:\n",
    "            number_str = re.sub(\"[^0-9]\", \"\", number_str) \n",
    "            # return int(number_str)\n",
    "            if(number_str == \"\") : \n",
    "                number_str = '1';\n",
    "            return int(float(number_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem5. Parsing HTML\n",
    "\n",
    "Parse the html code retrieved in order to get only the informations we need.\n",
    "\n",
    "We only need the information for the Adj. Close for each month.\n",
    "\n",
    "Ignore the other data.\n",
    "\n",
    "Write your code so the function beneath plays the exact roll I described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsing(html):\n",
    "    element = web.Element(html)\n",
    "    \n",
    "    # When you detect multiple <tr> tags, approach the last one.\n",
    "    index = # Your code here. Notice that misusing 'tr' approach will lead to the wrong result.\n",
    "    # Check what is being retrieved in your system constantly.\n",
    "    \n",
    "    lst = # Construct a list to contain all the prices you need.\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the result list should be 67 as it is represented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SP500 = parsing(SP500_html)\n",
    "len(SP500) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the 3 indices below, we will do the same operations as we did to the 'S&P500' index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dow Jones Industrial Average\n",
    "\n",
    "The Dow Jones Industrial Average (DJIA), also referred to as the Industrial Average, the Dow Jones, the Dow 30, or simply the Dow, is a stock market index, and one of several indices created by Wall Street Journal editor and Dow Jones & Company co-founder Charles Dow. It is an index that shows how 30 large, publicly owned companies based in the United States have traded during a standard trading session in the stock market. Along with the NASDAQ Composite, the S&P500 Index, and the Russell 2000 Index, the Dow is among the most closely watched benchmark indices tracking targeted stock market activity. To calculate the DJIA, the sum of the prices of all 30 stocks is divided by a Divisor, the Dow Divisor. The divisor is adjusted in case of stock splits, spinoffs or similar structural changes, to ensure that such events do not in themselves alter the numerical value of the DJIA.\n",
    "\n",
    "<img src=\"img/hw3/DowJones.jpg\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DowJones_code = '%5EDJI'\n",
    "DowJones_html = scrap_page(DowJones_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DJI = parsing(DowJones_html)\n",
    "len(DJI) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prime Interest Rate\n",
    "\n",
    "Prime rate, or Prime Lending Rate, is a term applied in many countries to a reference interest rate used by banks. The term originally indicated the rate of interest at which banks lent to favored customers, i.e., those with high credibility, though this is no longer always the case. Some variable interest rates may be expressed as a percentage above or below prime rate. Generally, prime interest rate is a significant determinant in the world of financial marketing. This is because monetary policy is aimed at influencing domestic interest rates, which drive currency rates relative to other currencies with different interest rates. Domestic interest rates also influence overall economic activity, with lower interest rates typically stimulating borrowing, investment, and consumption, while higher interest rates tend to reduce borrowing, and increase saving over consumption. Below is shown Federal Funds Rate History graph. This data will be used in the current application.\n",
    "\n",
    "<img src=\"img/hw3/PIR.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIR_code = 'PIR'\n",
    "PIR_html = scrap_page(PIR_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIR = parsing(PIR_html)\n",
    "len(PIR) # This should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NASDAQ Composite\n",
    "\n",
    "The NASDAQ Composite is a stock market index of the common stocks and similar securities listed on the NASDAQ stock market, meaning that it has over 3,000 components. It is highly followed in the U.S. as an indicator of the performance of stocks of technology companies and growth companies. Since both U.S. and non-U.S. companies are listed on the NASDAQ stock market, the index is not exclusively a U.S. index.\n",
    "\n",
    "<img src=\"img/hw3/NASDAQ.png\" alt=\"Drawing\" style=\"width: 500px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NASDAQ_code = 'NDAQ'\n",
    "NASDAQ_html = scrap_page(NASDAQ_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NSDQ = parsing(NASDAQ_html)\n",
    "len(NSDQ) # this should be 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting it together\n",
    "\n",
    "### Normalization\n",
    "\n",
    "This code is optional. Try it out after you have finished your code and checked your original results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SP500 = (SP500 - np.mean(SP500))/np.std(SP500)\n",
    "# DJI = (DJI-np.mean(DJI))/np.std(DJI)\n",
    "# PIR = (PIR-np.mean(PIR))/np.std(PIR)\n",
    "# NSDQ = (NSDQ-np.mean(NSDQ))/np.std(NSDQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem6. Constructing an Input Matrix\n",
    "\n",
    "First, in order to shuffle your data set to prevent it from getting biased by time, you will need to construct a one large\n",
    "\n",
    "matrix containing all the informations you have previously crawled from 'Yahoo Finance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtrx = # your code here. Matrix for every indices you have retrieved. It should have the size of (67 x 4)\n",
    "# Notice that the last column will be seperated as your target value 'y'.\n",
    "\n",
    "# So make sure that the 'NASDAQ' index comes in the last column of your matrix.\n",
    "# For example, [[S&P500 column], [PIR column], [Dow Jones column], [NASDAQ column]] will be a valid matrix\n",
    "\n",
    "mtrx = # your code here. Convert the matrix into a numpy 2 dimensional array\n",
    "\n",
    "np.random.shuffle(mtrx) # Shuffle the matrix using 'shuffle' function in 'numpy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem7. Constructing Data sets\n",
    "\n",
    "\n",
    "#### (1) X, y\n",
    " Now on, you will construct data sets X and Y.\n",
    "\n",
    "DO NOT split your data into training and cross validation sets yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = # your code here.\n",
    "y = # your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-1) train_X\n",
    "\n",
    "Now, spilt your training data, index starting from 13 to the last index 67.\n",
    "\n",
    "The total dimension of the final train_X will be (55L, 3L)\n",
    "\n",
    "Do NOT use the function provied from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = # your code here.\n",
    "train_X.shape # this should be (55L, 3L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (2-2) train_Y\n",
    "\n",
    "Once again, split your Y training labels, index starting from 13 to the last index 67.\n",
    "\n",
    "The total dimension of the final train_Y will be (55L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Y = # your code here.\n",
    "train_Y.shape # this should be (55L, 1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3-1) test_X\n",
    "\n",
    "Now, take the rest and make it as your test set.\n",
    "\n",
    "This set will be only used for evaluation, and will NOT be used in the process of training.\n",
    "\n",
    "The final dimension of test_X will be (12L, 3L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = # your code here.\n",
    "test_X.shape # this should be (12L, 3L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3-2) test_Y\n",
    "\n",
    "Do the same for 'y'.\n",
    "\n",
    "The final dimension for test_Y will be (12L, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Y = # your code here.\n",
    "test_Y.shape # this should be (12L, 1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem8. Using LASSO to predict the NASDAQ index\n",
    "\n",
    "In this section, you will predict the PIR using the other 3 indices, and get the accuracy of the prediction.\n",
    "\n",
    "We will use the 'Lasso Function' to fit the given model.\n",
    "\n",
    "In order to use Lasso, use 'var = Lasso(input)'\n",
    "\n",
    "A following example is given below.  Fill in the variables with the appropriate methods.\n",
    "\n",
    "Remember, the test set SHOULD NOT BE INCLUDED IN THE TRAINING PROCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = # your code here. Follow the code above.\n",
    "clf.fit(# your code, # your code)\n",
    "pred = clf.predict(#your code)\n",
    "pred = # your code here. Make all your predictions into a list.\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_Y # Compare with the prediction you did just above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem9. Using Ridge to predict the NASDAQ index\n",
    "\n",
    "In this section, we will do the same steps above, only changing that\n",
    "\n",
    "we will use the 'pipeline model. I already have written down the code \n",
    "\n",
    "In order to use Ridge, use 'var = Ridge(input)'\n",
    "\n",
    "A following example is given below. Fill in the variables with the appropriate methods.\n",
    "\n",
    "Remember, the test set SHOULD NOT BE INCLUDED IN THE TRAINING PROCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "max = 0\n",
    "\n",
    "for degree in range(5):\n",
    "    model = make_pipeline(PolynomialFeatures(#your code here), Ridge())\n",
    "    model.fit(# your code here, # your code here)\n",
    "    if(model.score(# your code here, # your code here)) > max:\n",
    "        fin_model = model\n",
    "        max = model.score(# your code here, # your code here)\n",
    "        \n",
    "Xf = fin_model.predict(# your code here)\n",
    "Xf # Compare the result from the test_Y above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin_model.score(test_X, test_Y) # Observe the results of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework3 , Support Vector Machine \n",
    "In this section, we will use new dataset,\n",
    "\n",
    "__blog corpus__, which was published at 2006.\n",
    "\n",
    "First of all, download the corpus,\n",
    "\n",
    "from this site → (http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm)\n",
    "\n",
    "And then, unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from glob import glob \n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# set your path to the unzipped blog folder.\n",
    "DATA_FILE_PATH_PATTERN = \"_your_folder_path_here_/*.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common methods\n",
    "\n",
    "We will use new package, __\"nltk\"__.\n",
    "\n",
    "Using the package, we can process natural language easily.\n",
    "\n",
    "First of all, we must download some _dictionaries_ to use the package's functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yuminhwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuminhwan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionaries, to be used in spliting the words and removing the stop words like \"a, the, ...\"\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we will get the stopwords,\n",
    "\n",
    "and we will make new object, which is from stemmer class.\n",
    "\n",
    "which makes redundancy between two different form of words lower,\n",
    "\n",
    "like beauty and beautiful being merged into \"beaut\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop word set and the stemmer object,\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem10.\n",
    "\n",
    "Now, we need to load files.\n",
    "\n",
    "Each file names are formatted like \"_user_ _ _id_._gender_._age_._fields_._constellation_.xml\".\n",
    "\n",
    "Fill each blanks, and make expected format shown below.\n",
    "\n",
    "(For the age field, we will mark teenager as 1, otherwise 0)\n",
    "\n",
    "__Notice.__ _Because the data is not valid XML format (e.g. include '&' character), don't use XML parser._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_user_info(file_path):\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the file_path,\n",
    "    \n",
    "    extract each records,\n",
    "    \n",
    "    and return dictionary,\n",
    "    \n",
    "    which contains each records,\n",
    "    \n",
    "    user_id, gender, age, fields, constellation\n",
    "    \n",
    "    (WARN: \"age\" column should be labelled with 1, 0,\n",
    "    \n",
    "    which represents the user is teenager or not.)\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    return file_info_dict\n",
    "        \n",
    "def parse_file(file_path):\n",
    "    # parse user_info from file_path string\n",
    "    user_info = parse_user_info(file_path)\n",
    "    \n",
    "    f = open(file_path, \"r\")\n",
    "    tmp_str = \"\"\n",
    "    for l in f:\n",
    "        tmp_str += l.strip()\n",
    "    f.close()\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Get only post section from \"tmp_str\"\n",
    "    \n",
    "    Hint: use re package to parse the string.\n",
    "    \n",
    "    wr_tmp_str contains all of the post data.\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    # merge post text\n",
    "    user_info['post'] = wr_tmp_str.decode('utf-8', errors = \"replace\")\n",
    "    \n",
    "    return user_info\n",
    "\n",
    "def load_data(file_path_pattern, ratio=1.):\n",
    "    file_paths = glob(file_path_pattern)\n",
    "    file_cnt = int(len(file_paths) * ratio)\n",
    "    \n",
    "    data = list()\n",
    "    cnt = 0\n",
    "    for file_path in file_paths[:file_cnt]:\n",
    "        user_data = parse_file(file_path)\n",
    "        data.append(user_data)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print cnt,'/',file_cnt\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "data = load_data(DATA_FILE_PATH_PATTERN, ratio=0.1)\n",
    "# Even with 16GB RAM, it was not enough to hold whole data,\n",
    "# so please use small data, but more than 10% of whole data.\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem11.\n",
    "\n",
    "Now, we will split the data into three sets,\n",
    "\n",
    "train, cv, and test sets.\n",
    "\n",
    "Our goal is to predict whether the blogger is teenager or not.\n",
    "\n",
    "Please make split_dataset function.\n",
    "\n",
    "which returns (feature_train, feature_cv, feature_test, answer_train, answer_cv, answer_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Train, CV, Test\n",
    "def split_dataset(data, ratio=(.6, .2, .2), shuffle=False):\n",
    "    if sum(ratio) != 1.:\n",
    "        print \"Invalid data split ratio\", ratio\n",
    "        return\n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    Add the condition whether shuffle the data or not.\n",
    "    \n",
    "    And then, split the data into train, validation, test set\n",
    "    \n",
    "    with given ratio.\n",
    "    \n",
    "    y values are from age column,\n",
    "    \n",
    "    and x values are from post column.\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    return (x_train, x_cv, x_test, y_train, y_cv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, x_cv, x_test, y_train, y_cv, y_test) = split_dataset(df)\n",
    "print \"Train set: \" , len(x_train)\n",
    "print \"CV set    : \", len(x_cv)\n",
    "print \"Test set : \", len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem12.\n",
    "\n",
    "From the previous assignment,\n",
    "\n",
    "we learned what BOW(bag of words) is.\n",
    "\n",
    "Now, we will make BOW from the training set's post data.\n",
    "\n",
    "Please fill the marked blank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow_matrix(data, word_index=None, tf_mode=False):\n",
    "    # For Training set (with creating Word index)\n",
    "    if word_index == None:\n",
    "        return __get_bow_matrix_trains(data, tf_mode=tf_mode)\n",
    "    # for CV or Test set\n",
    "    else:\n",
    "        return __get_bow_matrix(data, word_index=word_index, tf_mode=tf_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix_trains(train_set, tf_mode):\n",
    "    bows = list()\n",
    "    \n",
    "    bows = [\n",
    "        __get_bow_dict(d, tf_mode=tf_mode)\n",
    "        for d in train_set\n",
    "    ]\n",
    "                \n",
    "    # Create word length dictionary\n",
    "    word_len = dict()\n",
    "    for bow in bows:\n",
    "        #bow of each post\n",
    "        for word in bow.keys():\n",
    "            if word not in word_len:\n",
    "                word_len[word] = 1\n",
    "            else:\n",
    "                word_len[word] += 1\n",
    "                \n",
    "    final_word_list = __get_word_list_by_cf(word_len,5000)\n",
    "    \n",
    "    # bow dict -> matrix\n",
    "    if tf_mode:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.uint16)\n",
    "    else:\n",
    "        bow_matrix = np.zeros((len(bows), len(final_word_list)), dtype=np.bool)\n",
    "    \n",
    "    for bow_idx in range(len(bows)):\n",
    "        bow = bows[bow_idx]\n",
    "        for w in bow:\n",
    "            if w in final_word_list:\n",
    "                bow_matrix[bow_idx, final_word_list.index(w)] = bow[w]\n",
    "            \n",
    "    return bow_matrix, final_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Training set.\n",
    "def __get_bow_dict(text, word_index=None, tf_mode = False):\n",
    "    bow = dict()\n",
    "\n",
    "    for word in wordpunct_tokenize(text):\n",
    "        # if word is in stopword_set\n",
    "        # only accept fully alphabet\n",
    "        word = word.lower()\n",
    "        if word in stopword_set or not word.isalpha():\n",
    "            continue      \n",
    "            \n",
    "        stem = stemmer.stem(word)\n",
    "        if word_index is not None:\n",
    "            if word not in word_index:\n",
    "                continue\n",
    "                \n",
    "        \"\"\"\n",
    "        Create BOW dict\n",
    "        \"\"\"\n",
    "        # boolean mode (true or false)\n",
    "        if not tf_mode:\n",
    "            bow[stem] = 1\n",
    "        # Term Frequency Mode (bow[stem] = stemmed word occurance)\n",
    "        else:\n",
    "            if stem not in bow:\n",
    "                bow[stem] = 0\n",
    "            bow[stem] = bow[stem] + 1\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because the word list is too big, reduce word list, by collection frequency.\n",
    "def __get_word_list_by_cf(word_len_dict, k = 1000):\n",
    "    \n",
    "    \n",
    "    ################ your code here #################\n",
    "    \"\"\"\n",
    "    From the word_len_dict,\n",
    "    \n",
    "    please make frequency top k word list\n",
    "    \n",
    "    and return the list\n",
    "    \"\"\"\n",
    "    ####################################################\n",
    "    \n",
    "    return top_k_word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __get_bow_matrix(dataset, word_index, tf_mode):\n",
    "    if tf_mode:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.uint16)\n",
    "    else:\n",
    "        matrix = np.zeros((len(dataset), len(word_index)), dtype=np.bool)\n",
    "    \n",
    "    d_idx = 0\n",
    "    for data in dataset:\n",
    "        bow = __get_bow_dict(data, word_index, tf_mode)\n",
    "        for w in bow.keys():\n",
    "            if w in word_index:\n",
    "                matrix[d_idx, word_index.index(w)] = bow[w]\n",
    "        d_idx += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "bow_matrix_train, word_index = get_bow_matrix(x_train, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_cv = get_bow_matrix(x_cv, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)\n",
    "start_t = time.time()\n",
    "bow_matrix_test = get_bow_matrix(x_test, word_index, tf_mode=False)\n",
    "end_t = time.time()\n",
    "print \"Elapsed Time: \",  (end_t - start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem13.\n",
    "\n",
    "Now, it's time to fit the model,\n",
    "\n",
    "and get the result.\n",
    "\n",
    "Please use your validation set to fit the model,\n",
    "\n",
    "and then predict the value of test set.\n",
    "\n",
    "Use sklearn.metrics.precision_recall_fscore_support method ([description](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)),\n",
    "\n",
    "and use fscore as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_model = SVC()\n",
    "\n",
    "###########################################################\n",
    "########## fit the model by svc model              ########\n",
    "########## and predict the value of validation set ########\n",
    "########## set the parameter,                      ########\n",
    "########## then predict the test set               ########\n",
    "###########################################################\n",
    "svc_model.fit(bow_matrix_train, y_train)\n",
    "pred_cv = svc_model.predict(bow_matrix_cv)\n",
    "prf = sklearn.metrics.precision_recall_fscore_support(y_cv, pred_cv)\n",
    "print prf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the prediction result of test set\n",
    "pred_test = svc_model.predict(bow_matrix_test)\n",
    "prf_test = sklearn.metrics.precision_recall_fscore_support(y_test, pred_test)\n",
    "print prf_test[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
