{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c794422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5961cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tweepy nltk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e85cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade yfinance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ebeb9",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "877c84b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize PRAW with your credentials\n",
    "reddit = praw.Reddit(client_id='IFRFuqUUyPaFsqKCKl9TaA',\n",
    "                     client_secret='Adp9lyh4Y79swxDgxhLfnbUE29Nzcg',\n",
    "                     user_agent='script:stockX:v1.0 (by /u/Perman_27)')\n",
    "\n",
    "# Choose the subreddit you're interested in\n",
    "subreddits_to_analyze = ['wallstreetbets', 'stocks', 'investing']\n",
    "\n",
    "# Define a list to store the scraped data\n",
    "all_posts_data = []\n",
    "\n",
    "# Define the time frame for historical discussions\n",
    "time_frame = 'month'  # can be 'day', 'week', 'month', 'year', 'all'\n",
    "\n",
    "# Loop through each subreddit\n",
    "for subreddit_name in subreddits_to_analyze:\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Use the search function to look for posts containing 'AAPL' or 'Apple'\n",
    "    search_query = 'AAPL OR Apple'\n",
    "    for post in subreddit.search(search_query, time_filter=time_frame, limit=100):  \n",
    "        post_dict = {\n",
    "            \"title\": post.title,\n",
    "            \"subreddit\": subreddit_name,\n",
    "            \"content\": post.selftext,\n",
    "            \"score\": post.score,  # Proxy for user engagement\n",
    "            \"upvote_ratio\": post.upvote_ratio,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"created_utc\": post.created_utc,\n",
    "            \"date\": datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),  # Human-readable date\n",
    "            \"flair\": post.link_flair_text,\n",
    "            \"tickers\": [word for word in post.title.split() if word.startswith(\"$\")]\n",
    "        }\n",
    "        \n",
    "        # Scrape top comments for each post (limit to top 10 for brevity)\n",
    "        post.comments.replace_more(limit=0)\n",
    "        comments_data = []\n",
    "        for comment in post.comments.list()[:10]:\n",
    "            comments_data.append({\n",
    "                \"content\": comment.body,\n",
    "                \"score\": comment.score,\n",
    "                \"created_utc\": comment.created_utc,\n",
    "                \"date\": datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')  # Human-readable date\n",
    "            })\n",
    "        \n",
    "        # Add comments to the post dictionary\n",
    "        post_dict[\"comments\"] = comments_data\n",
    "        \n",
    "        # Append post data to the list\n",
    "        all_posts_data.append(post_dict)\n",
    "\n",
    "# Optionally save the scraped data to a JSON file\n",
    "with open('reddit/apple_posts_data.json', 'w') as json_file:\n",
    "    json.dump(all_posts_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e2d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cfcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c2261da",
   "metadata": {},
   "source": [
    "## Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de655228",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adapter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Call the revised scrape function\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m stock_details, recommendation_trends \u001b[38;5;241m=\u001b[39m scrape_yahoo_finance(ticker_symbol)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(stock_details)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommendation_trends)\n",
      "Cell \u001b[0;32mIn[25], line 118\u001b[0m, in \u001b[0;36mscrape_yahoo_finance\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# ... Retry strategy setup remains the same ...\u001b[39;00m\n\u001b[1;32m    117\u001b[0m session \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m--> 118\u001b[0m session\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m\"\u001b[39m, adapter)\n\u001b[1;32m    119\u001b[0m session\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m\"\u001b[39m, adapter)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adapter' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_recommendation_trends(soup):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Locate the section containing analyst ratings\n",
    "    ratings_section = soup.find('section', {'data-test': 'analyst-ratings'})\n",
    "    \n",
    "    # Initialize a dictionary to store the recommendation categories\n",
    "    recommendation_trends = {}\n",
    "    \n",
    "    if ratings_section:\n",
    "        # Locate the list items representing each recommendation category\n",
    "        recommendation_list = ratings_section.find_all('li')\n",
    "        \n",
    "        for recommendation in recommendation_list:\n",
    "            # The text for the recommendation is the second child (after the <i> element)\n",
    "            category_text = recommendation.text.strip()\n",
    "            \n",
    "            # Assuming the structure is \"<icon> Category Name\", split and get the last element\n",
    "            category = category_text.split()[-1]\n",
    "            recommendation_trends[category] = category_text\n",
    "\n",
    "    return recommendation_trends\n",
    "\n",
    "def extract_stock_details(soup):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Extract company name\n",
    "    company_name = soup.find('h1').text.strip()\n",
    "\n",
    "    # Extract current price details\n",
    "    current_price = soup.find('fin-streamer', {'data-field': 'regularMarketPrice'}).text\n",
    "    price_change = soup.find('fin-streamer', {'data-field': 'regularMarketChange'}).text\n",
    "    percentage_change = soup.find('fin-streamer', {'data-field': 'regularMarketChangePercent'}).text.strip(\"()\")\n",
    "\n",
    "    # Extract after-hours price details\n",
    "    after_hours_price = soup.find('fin-streamer', {'data-field': 'postMarketPrice'}).text\n",
    "    after_hours_change = soup.find('fin-streamer', {'data-field': 'postMarketChange'}).text\n",
    "    after_hours_change_percent = soup.find('fin-streamer', {'data-field': 'postMarketChangePercent'}).text.strip(\"()\")\n",
    "\n",
    "    # Extract dividend information\n",
    "    dividend_info = soup.find('p', text='Dividend').find_next_sibling('p').text\n",
    "\n",
    "    # Construct a dictionary with the extracted information\n",
    "    stock_details = {\n",
    "        'Company Name': company_name,\n",
    "        'Current Price': current_price,\n",
    "        'Price Change': price_change,\n",
    "        'Percentage Change': percentage_change,\n",
    "        'After Hours Price': after_hours_price,\n",
    "        'After Hours Change': after_hours_change,\n",
    "        'After Hours Change Percent': after_hours_change_percent,\n",
    "        'Dividend Information': dividend_info,\n",
    "    }\n",
    "    \n",
    "    return stock_details\n",
    "\n",
    "# Fetches dividend data using yfinance\n",
    "def fetch_dividend_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    dividends = stock.dividends\n",
    "    return dividends\n",
    "\n",
    "# Fetches historical earnings data using yfinance\n",
    "def fetch_earnings_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    earnings = stock.earnings\n",
    "    return earnings\n",
    "\n",
    "# Fetches options data using yfinance\n",
    "def fetch_options_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    options_dates = stock.options\n",
    "    options_data = {}\n",
    "    for date in options_dates:\n",
    "        options_data[date] = stock.option_chain(date)\n",
    "    return options_data\n",
    "\n",
    "# Fetches historical stock prices using yfinance\n",
    "def fetch_stock_prices(ticker, period='1mo'):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    hist = stock.history(period=period)\n",
    "    return hist[['Open', 'Close', 'High', 'Low', 'Volume']]\n",
    "\n",
    "# Fetches financial reports using yfinance\n",
    "def fetch_financial_reports(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    balance_sheet = stock.balance_sheet\n",
    "    income_statement = stock.financials\n",
    "    cashflow = stock.cashflow\n",
    "    return balance_sheet, income_statement, cashflow\n",
    "# Saves data to CSV\n",
    "def save_data_to_csv(data, filename, folder='data'):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    data.to_csv(os.path.join(folder, f'{filename}.csv'))\n",
    "\n",
    "def scrape_yahoo_finance(ticker):\n",
    "    url = f\"https://finance.yahoo.com/quote/{ticker}/analysis\"  # URL to the analysis page\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    # ... Retry strategy setup remains the same ...\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Pass the soup object to the extraction functions\n",
    "        stock_details = extract_stock_details(soup)\n",
    "        recommendation_trends = extract_recommendation_trends(soup)\n",
    "        \n",
    "        return stock_details, recommendation_trends\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f'An error occurred while scraping Yahoo Finance: {e}')\n",
    "        return None, None\n",
    "\n",
    "# Call the revised scrape function\n",
    "stock_details, recommendation_trends = scrape_yahoo_finance(ticker_symbol)\n",
    "print(stock_details)\n",
    "print(recommendation_trends)\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "ticker_symbol = 'AAPL'\n",
    "stock_details, recommendation_trends = scrape_yahoo_finance(ticker_symbol)\n",
    "print(stock_details)\n",
    "print(recommendation_trends)\n",
    "\n",
    "# Fetch analyst ratings by scraping\n",
    "def fetch_all_earnings_tables(url, table_class, timeout=5):\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Set up retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session = requests.Session()\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "\n",
    "    all_tables_data = []\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=timeout)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            earnings_tables = soup.find_all('table', {'class': table_class})\n",
    "            \n",
    "            for table in earnings_tables:\n",
    "                earnings_rows = table.find_all('tr')\n",
    "                if earnings_rows:\n",
    "                    headers = [header.text.strip() for header in earnings_rows[0].find_all('th')]\n",
    "                    table_data = []\n",
    "                    for row in earnings_rows[1:]:\n",
    "                        cols = row.find_all('td')\n",
    "                        if cols:\n",
    "                            table_data.append({headers[i]: cols[i].text.strip() for i in range(len(cols))})\n",
    "                    all_tables_data.append(table_data)\n",
    "                else:\n",
    "                    logger.info('No rows found in one of the earnings tables.')\n",
    "            return all_tables_data\n",
    "        else:\n",
    "            logger.error(f'Failed to retrieve the webpage, status code: {response.status_code}')\n",
    "            for table in table_data:\n",
    "                 print(table)\n",
    "            return []\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f'An error occurred: {e}')\n",
    "        return []\n",
    "\n",
    "    \n",
    "def save_data_to_csv(data, filename, folder='data'):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    data.to_csv(os.path.join(folder, f'{filename}.csv'))\n",
    "    \n",
    "    \n",
    "    \n",
    "def save_to_csv(data, filename):\n",
    "    if not os.path.isdir('data'):\n",
    "        os.mkdir('data')\n",
    "    data.to_csv(f'data/{filename}.csv')\n",
    "    \n",
    "# Usage:\n",
    "ticker_symbol = 'AAPL'  # Replace with your ticker\n",
    "\n",
    "# Fetch and save dividend data\n",
    "dividends = fetch_dividend_data(ticker_symbol)\n",
    "save_data_to_csv(dividends, f'{ticker_symbol}_dividends')\n",
    "\n",
    "# Fetch and save analyst recommendations\n",
    "recommendations = fetch_analyst_recommendations(ticker_symbol)\n",
    "save_data_to_csv(recommendations, f'{ticker_symbol}_analyst_recommendations')\n",
    "\n",
    "# Fetch and save historical earnings data\n",
    "earnings = fetch_earnings_data(ticker_symbol)\n",
    "save_data_to_csv(earnings, f'{ticker_symbol}_earnings')\n",
    "\n",
    "# Fetch and save options data\n",
    "options_data = fetch_options_data(ticker_symbol)\n",
    "for date, option_data in options_data.items():\n",
    "    save_data_to_csv(option_data.calls, f'{ticker_symbol}_options_calls_{date}')\n",
    "    save_data_to_csv(option_data.puts, f'{ticker_symbol}_options_puts_{date}')\n",
    "\n",
    "\n",
    "# Usage:\n",
    "prices = fetch_stock_prices(ticker_symbol, period='1mo')\n",
    "save_to_csv(prices, f'{ticker_symbol}_prices')\n",
    "\n",
    "\n",
    "# Usage:\n",
    "balance_sheet, income_statement, cashflow = fetch_financial_reports(ticker_symbol)\n",
    "save_to_csv(balance_sheet, f'{ticker_symbol}_balance_sheet')\n",
    "save_to_csv(income_statement, f'{ticker_symbol}_income_statement')\n",
    "save_to_csv(cashflow, f'{ticker_symbol}_cashflow')\n",
    "\n",
    "\n",
    "def save_list_of_dicts_to_csv(data, filename):\n",
    "    if not data:\n",
    "        return\n",
    "\n",
    "    keys = data[0].keys()\n",
    "    with open(f'data/{filename}.csv', 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "\n",
    "# Usage:\n",
    "\n",
    "\n",
    "\n",
    "def save_scraped_tables(all_tables_data, base_filename):\n",
    "    for idx, table_data in enumerate(all_tables_data):\n",
    "        save_list_of_dicts_to_csv(table_data, f'{base_filename}_table_{idx+1}')\n",
    "\n",
    "# Usage:\n",
    "all_tables_data = fetch_all_earnings_tables(url, 'W(100%) M(0) BdB Bdc($seperatorColor) Mb(25px)')\n",
    "save_scraped_tables(all_tables_data, ticker_symbol)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51921403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nr/0m3_2w416k95_79fx2rpjb7h0000gn/T/ipykernel_33632/2689235472.py:55: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  dividend_section = soup.find('p', text=lambda x: x and 'Dividend' in x)\n",
      "INFO:__main__:Stock details saved to data/AAPL_stock_details.csv\n",
      "INFO:__main__:Recommendation trends saved to data/AAPL_recommendation_trends.csv\n",
      "INFO:__main__:Stock Details for AAPL: {'Company Name': 'Apple Inc. (AAPL)', 'Current Price': '4,358.34', 'Price Change': '+40.56', 'Percentage Change': '+0.94%', 'After Hours Price': '176.65', 'After Hours Change': '+0.01', 'After Hours Change Percent': '+0.00%', 'Dividend Information': ''}\n",
      "INFO:__main__:Recommendation Trends for AAPL: {}\n",
      "INFO:__main__:Table data saved to data/AAPL_table_1.csv\n",
      "INFO:__main__:Table data saved to data/AAPL_table_2.csv\n",
      "INFO:__main__:Table data saved to data/AAPL_table_3.csv\n",
      "INFO:__main__:Table data saved to data/AAPL_table_4.csv\n",
      "INFO:__main__:Table data saved to data/AAPL_table_5.csv\n",
      "INFO:__main__:Table data saved to data/AAPL_table_6.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from time import sleep\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "def extract_recommendation_trends(soup):\n",
    "    # Initialize a dictionary to store the recommendation categories\n",
    "    recommendation_trends = {}\n",
    "    \n",
    "    # Locate the section containing analyst ratings\n",
    "    ratings_section = soup.find('section', {'data-test': 'analyst-ratings'})\n",
    "    \n",
    "    if ratings_section:\n",
    "        # Locate the list items representing each recommendation category\n",
    "        recommendation_list = ratings_section.find_all('li')\n",
    "        \n",
    "        for recommendation in recommendation_list:\n",
    "            # The text for the recommendation is the entire text of the list item\n",
    "            category_text = recommendation.text.strip()\n",
    "            \n",
    "            # The category name is the text of the recommendation minus the icons and numbers\n",
    "            category = \" \".join(category_text.split()[1:])  # Skip the icon symbol (first element)\n",
    "            recommendation_trends[category] = category_text\n",
    "\n",
    "    return recommendation_trends\n",
    "\n",
    "def extract_stock_details(soup):\n",
    "    # Extract company name\n",
    "    company_name = soup.find('h1').text.strip()\n",
    "\n",
    "    # Extract current price details\n",
    "    current_price = soup.find('fin-streamer', {'data-field': 'regularMarketPrice'}).text\n",
    "    price_change = soup.find('fin-streamer', {'data-field': 'regularMarketChange'}).text\n",
    "    percentage_change = soup.find('fin-streamer', {'data-field': 'regularMarketChangePercent'}).text.strip(\"()\")\n",
    "\n",
    "    # Extract after-hours price details\n",
    "    after_hours_price = soup.find('fin-streamer', {'data-field': 'postMarketPrice'}).text\n",
    "    after_hours_change = soup.find('fin-streamer', {'data-field': 'postMarketChange'}).text\n",
    "    after_hours_change_percent = soup.find('fin-streamer', {'data-field': 'postMarketChangePercent'}).text.strip(\"()\")\n",
    "\n",
    "    # Extract dividend information\n",
    "    dividend_info = \"\"\n",
    "    dividend_section = soup.find('p', text=lambda x: x and 'Dividend' in x)\n",
    "    if dividend_section and dividend_section.find_next_sibling('p'):\n",
    "        dividend_info = dividend_section.find_next_sibling('p').text\n",
    "\n",
    "    # Construct a dictionary with the extracted information\n",
    "    stock_details = {\n",
    "        'Company Name': company_name,\n",
    "        'Current Price': current_price,\n",
    "        'Price Change': price_change,\n",
    "        'Percentage Change': percentage_change,\n",
    "        'After Hours Price': after_hours_price,\n",
    "        'After Hours Change': after_hours_change,\n",
    "        'After Hours Change Percent': after_hours_change_percent,\n",
    "        'Dividend Information': dividend_info,\n",
    "    }\n",
    "    \n",
    "    return stock_details\n",
    "\n",
    "# Scrape Yahoo Finance for stock details and recommendation trends\n",
    "def scrape_yahoo_finance(ticker):\n",
    "    url = f\"https://finance.yahoo.com/quote/{ticker}/analysis\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        stock_details = extract_stock_details(soup)\n",
    "        recommendation_trends = extract_recommendation_trends(soup)\n",
    "        return stock_details, recommendation_trends\n",
    "    else:\n",
    "        logger.error(f\"Failed to scrape Yahoo Finance for {ticker}. Status code: {response.status_code}\")\n",
    "        return {}, {}\n",
    "    \n",
    "def fetch_all_earnings_tables(url, table_class, timeout=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Set up a session with retries\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    all_tables_data = []\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            earnings_tables = soup.find_all('table', {'class': table_class})\n",
    "            \n",
    "            for table in earnings_tables:\n",
    "                earnings_rows = table.find_all('tr')\n",
    "                if earnings_rows:\n",
    "                    headers = [header.text.strip() for header in earnings_rows[0].find_all('th')]\n",
    "                    table_data = []\n",
    "                    for row in earnings_rows[1:]:\n",
    "                        cols = row.find_all('td')\n",
    "                        if cols:\n",
    "                            row_data = {headers[i]: cols[i].text.strip() for i in range(len(cols))}\n",
    "                            table_data.append(row_data)\n",
    "                    all_tables_data.append(table_data)\n",
    "            return all_tables_data\n",
    "        else:\n",
    "            logger.error(f'Failed to retrieve the webpage, status code: {response.status_code}')\n",
    "            return []\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f'An error occurred: {e}')\n",
    "        return []\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Other imports and function definitions remain unchanged...\n",
    "\n",
    "# Define a function to save stock details to CSV\n",
    "def save_stock_details_to_csv(stock_details, filename, folder='data'):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    \n",
    "    filepath = os.path.join(folder, f'{filename}.csv')\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=stock_details.keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerow(stock_details)\n",
    "    \n",
    "    logger.info(f\"Stock details saved to {filepath}\")\n",
    "\n",
    "# Define a function to save recommendation trends to CSV\n",
    "def save_recommendation_trends_to_csv(recommendation_trends, filename, folder='data'):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    \n",
    "    filepath = os.path.join(folder, f'{filename}.csv')\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Category', 'Details'])\n",
    "        for category, details in recommendation_trends.items():\n",
    "            writer.writerow([category, details])\n",
    "    \n",
    "    logger.info(f\"Recommendation trends saved to {filepath}\")\n",
    "\n",
    "def save_scraped_tables(all_tables_data, base_filename, folder='data'):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    \n",
    "    for idx, table_data in enumerate(all_tables_data):\n",
    "        filename = f\"{base_filename}_table_{idx+1}.csv\"\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            dict_writer = csv.DictWriter(csvfile, fieldnames=table_data[0].keys())\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(table_data)\n",
    "        \n",
    "        logger.info(f\"Table data saved to {file_path}\")\n",
    "\n",
    "def main(ticker_symbol):\n",
    "    # Scrape Yahoo Finance for stock details and recommendation trends\n",
    "    stock_details, recommendation_trends = scrape_yahoo_finance(ticker_symbol)\n",
    "    \n",
    "    # Save the scraped stock details and recommendation trends\n",
    "    save_stock_details_to_csv(stock_details, f\"{ticker_symbol}_stock_details\")\n",
    "    save_recommendation_trends_to_csv(recommendation_trends, f\"{ticker_symbol}_recommendation_trends\")\n",
    "\n",
    "    # Log the scraped stock details and recommendation trends\n",
    "    logger.info(f\"Stock Details for {ticker_symbol}: {stock_details}\")\n",
    "    logger.info(f\"Recommendation Trends for {ticker_symbol}: {recommendation_trends}\")\n",
    "    \n",
    "    # Fetch and save all earnings tables data\n",
    "    earnings_tables_url = f\"https://finance.yahoo.com/quote/{ticker_symbol}/analysis\"\n",
    "    earnings_tables_class = 'W(100%) M(0) BdB Bdc($seperatorColor) Mb(25px)'\n",
    "    all_tables_data = fetch_all_earnings_tables(earnings_tables_url, earnings_tables_class)\n",
    "    if all_tables_data:\n",
    "        save_scraped_tables(all_tables_data, ticker_symbol)\n",
    "    else:\n",
    "        logger.info(f\"No table data found for {ticker_symbol}.\")\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = 'AAPL'  # Replace with your ticker symbol\n",
    "    main(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99aac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffe53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d09737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f513765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find the recommendations table.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def fetch_analyst_recommendations(ticker):\n",
    "    url = f'https://finance.yahoo.com/quote/{ticker}/analysis'\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # You would need to inspect the webpage to find the correct selector for the recommendations\n",
    "        recommendations_table = soup.find('table', {'class': 'recommendations'})\n",
    "        if recommendations_table:\n",
    "            rows = recommendations_table.find_all('tr')\n",
    "            data = []\n",
    "            for row in rows[1:]:  # Skip header row\n",
    "                cols = row.find_all('td')\n",
    "                if cols:\n",
    "                    data.append({\n",
    "                        'Date': cols[0].text.strip(),\n",
    "                        'Rating': cols[1].text.strip(),\n",
    "                        # Add more columns as needed based on the structure of the table\n",
    "                    })\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Couldn't find the recommendations table.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(\"Couldn't retrieve the page.\")\n",
    "        return []\n",
    "\n",
    "# Usage\n",
    "ticker_symbol = 'AAPL'\n",
    "recommendations = fetch_analyst_recommendations(ticker_symbol)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea74979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b589816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between discussion volume and trading volume: 0.41376442111383244\n",
      "Correlation between sentiment and closing price: -0.2902920283344923\n",
      "Correlation between sentiment and trading volume: 0.05095853574382976\n",
      "Merged data saved to 'merged_reddit_finance_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sentiment Analyzer function using TextBlob\n",
    "def sentiment_analyzer(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# Load Reddit data\n",
    "with open('reddit/apple_posts_data.json', 'r') as file:\n",
    "    reddit_data = pd.json_normalize(json.load(file))\n",
    "\n",
    "# Load stock prices data\n",
    "prices_data = pd.read_csv('data/AAPL_prices.csv')\n",
    "\n",
    "# Preprocess and merge data\n",
    "def preprocess_and_merge(reddit_df, prices_df):\n",
    "    # Convert 'created_utc' to dates\n",
    "    reddit_df['Date'] = pd.to_datetime(reddit_df['created_utc'], unit='s').dt.date\n",
    "    \n",
    "    # Assign sentiment scores to Reddit posts\n",
    "    reddit_df['sentiment'] = reddit_df['content'].apply(sentiment_analyzer)\n",
    "\n",
    "    # Aggregate the number of posts and average sentiment per day\n",
    "    discussion_volume = reddit_df.groupby('Date').agg({\n",
    "        'sentiment': 'mean',\n",
    "        'title': 'size'  # Assuming 'title' column exists for counting posts\n",
    "    }).rename(columns={'title': 'post_count'}).reset_index()\n",
    "\n",
    "    # Ensure 'Date' in prices_df is a datetime object\n",
    "    prices_df['Date'] = pd.to_datetime(prices_df['Date']).dt.date\n",
    "\n",
    "    # Merge the datasets on the 'Date' column\n",
    "    merged_df = pd.merge(discussion_volume, prices_df, on='Date', how='inner')\n",
    "\n",
    "    # Add any additional merging requirements here, such as tickers if needed\n",
    "    # Sentiment Score Association and Volume of Discussion Correlation\n",
    "    # Ensure data types match before merging\n",
    "    #reddit_df['post_count'] = reddit_df['post_count'].astype(float)\n",
    "    #prices_df['Volume'] = prices_df['Volume'].astype(float)\n",
    "    #merged_df['discussion_volume'] = reddit_df['post_count']\n",
    "    # merged_df = pd.merge(merged_df, additional_data, on=['Date', 'ticker'], how='inner')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Perform preprocessing and merging\n",
    "merged_data = preprocess_and_merge(reddit_data, prices_data)\n",
    "\n",
    "# Calculate correlations if merging is successful\n",
    "if merged_data is not None:\n",
    "    correlation_discussion_trading = merged_data['post_count'].corr(merged_data['Volume'])\n",
    "    correlation_price_sentiment = merged_data['sentiment'].corr(merged_data['Close'])\n",
    "    correlation_volume_sentiment = merged_data['sentiment'].corr(merged_data['Volume'])\n",
    "    # ... rest of your code ...\n",
    "\n",
    "    print(f\"Correlation between discussion volume and trading volume: {correlation_discussion_trading}\")\n",
    "    print(f\"Correlation between sentiment and closing price: {correlation_price_sentiment}\")\n",
    "    print(f\"Correlation between sentiment and trading volume: {correlation_volume_sentiment}\")\n",
    "    \n",
    "    # Save the new merged data to a CSV file\n",
    "    merged_data.to_csv('merged_reddit_finance_data.csv', index=False)\n",
    "    print(\"Merged data saved to 'merged_reddit_finance_data.csv'.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Data merging was unsuccessful. Please check the data formats and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e189d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aab6d47b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: merged_reddit_finance_data.csv\n",
      "Columns and Types: {'Date': 'object', 'sentiment': 'float64', 'post_count': 'int64', 'Open': 'float64', 'Close': 'float64', 'High': 'float64', 'Low': 'float64', 'Volume': 'int64'}\n",
      "Number of Rows: 3\n",
      "Number of Columns: 8\n",
      "NaN values per column: {'Date': 0, 'sentiment': 0, 'post_count': 0, 'Open': 0, 'Close': 0, 'High': 0, 'Low': 0, 'Volume': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: reddit/apple_posts_data.json\n",
      "Columns and Types: {'title': 'object', 'subreddit': 'object', 'content': 'object', 'score': 'int64', 'upvote_ratio': 'float64', 'num_comments': 'int64', 'created_utc': 'float64', 'date': 'object', 'flair': 'object', 'tickers': 'object', 'comments': 'object'}\n",
      "Number of Rows: 107\n",
      "Number of Columns: 11\n",
      "NaN values per column: {'title': 0, 'subreddit': 0, 'content': 0, 'score': 0, 'upvote_ratio': 0, 'num_comments': 0, 'created_utc': 0, 'date': 0, 'flair': 29, 'tickers': 0, 'comments': 0}\n",
      "-----------------------------------\n",
      "\n",
      "title: str\n",
      "subreddit: str\n",
      "content: str\n",
      "score: int\n",
      "upvote_ratio: float\n",
      "num_comments: int\n",
      "created_utc: float\n",
      "date: str\n",
      "flair: str\n",
      "tickers:\n",
      "Could not read reddit/apple_posts_data.json: list index out of range\n",
      "File: data/AAPL_cashflow.csv\n",
      "Columns and Types: {'Unnamed: 0': 'object', '2023-09-30': 'float64', '2022-09-30': 'float64', '2021-09-30': 'float64', '2020-09-30': 'float64'}\n",
      "Number of Rows: 55\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Unnamed: 0': 0, '2023-09-30': 9, '2022-09-30': 3, '2021-09-30': 2, '2020-09-30': 1}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_prices.csv\n",
      "Columns and Types: {'Date': 'object', 'Open': 'float64', 'Close': 'float64', 'High': 'float64', 'Low': 'float64', 'Volume': 'int64'}\n",
      "Number of Rows: 23\n",
      "Number of Columns: 6\n",
      "NaN values per column: {'Date': 0, 'Open': 0, 'Close': 0, 'High': 0, 'Low': 0, 'Volume': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_income_statement.csv\n",
      "Columns and Types: {'Unnamed: 0': 'object', '2023-09-30': 'float64', '2022-09-30': 'float64', '2021-09-30': 'float64', '2020-09-30': 'float64'}\n",
      "Number of Rows: 39\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Unnamed: 0': 0, '2023-09-30': 6, '2022-09-30': 0, '2021-09-30': 0, '2020-09-30': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_6.csv\n",
      "Columns and Types: {'Growth Estimates': 'object', 'AAPL': 'object', 'Industry': 'float64', 'Sector(s)': 'float64', 'S&P 500': 'float64'}\n",
      "Number of Rows: 6\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Growth Estimates': 0, 'AAPL': 0, 'Industry': 6, 'Sector(s)': 6, 'S&P 500': 6}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_5.csv\n",
      "Columns and Types: {'EPS Revisions': 'object', 'Current Qtr. (Sep 2023)': 'float64', 'Next Qtr. (Dec 2023)': 'float64', 'Current Year (2023)': 'float64', 'Next Year (2024)': 'float64'}\n",
      "Number of Rows: 4\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'EPS Revisions': 0, 'Current Qtr. (Sep 2023)': 4, 'Next Qtr. (Dec 2023)': 2, 'Current Year (2023)': 2, 'Next Year (2024)': 3}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_4.csv\n",
      "Columns and Types: {'EPS Trend': 'object', 'Current Qtr. (Sep 2023)': 'float64', 'Next Qtr. (Dec 2023)': 'float64', 'Current Year (2023)': 'float64', 'Next Year (2024)': 'float64'}\n",
      "Number of Rows: 5\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'EPS Trend': 0, 'Current Qtr. (Sep 2023)': 0, 'Next Qtr. (Dec 2023)': 0, 'Current Year (2023)': 0, 'Next Year (2024)': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_1.csv\n",
      "Columns and Types: {'Earnings Estimate': 'object', 'Current Qtr. (Sep 2023)': 'float64', 'Next Qtr. (Dec 2023)': 'float64', 'Current Year (2023)': 'float64', 'Next Year (2024)': 'float64'}\n",
      "Number of Rows: 5\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Earnings Estimate': 0, 'Current Qtr. (Sep 2023)': 0, 'Next Qtr. (Dec 2023)': 0, 'Current Year (2023)': 0, 'Next Year (2024)': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_3.csv\n",
      "Columns and Types: {'Earnings History': 'object', '9/29/2022': 'object', '12/30/2022': 'object', '3/30/2023': 'object', '6/29/2023': 'object'}\n",
      "Number of Rows: 4\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Earnings History': 0, '9/29/2022': 0, '12/30/2022': 0, '3/30/2023': 0, '6/29/2023': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_table_2.csv\n",
      "Columns and Types: {'Revenue Estimate': 'object', 'Current Qtr. (Sep 2023)': 'object', 'Next Qtr. (Dec 2023)': 'object', 'Current Year (2023)': 'object', 'Next Year (2024)': 'object'}\n",
      "Number of Rows: 6\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Revenue Estimate': 0, 'Current Qtr. (Sep 2023)': 0, 'Next Qtr. (Dec 2023)': 2, 'Current Year (2023)': 0, 'Next Year (2024)': 0}\n",
      "-----------------------------------\n",
      "\n",
      "File: data/AAPL_balance_sheet.csv\n",
      "Columns and Types: {'Unnamed: 0': 'object', '2023-09-30': 'float64', '2022-09-30': 'float64', '2021-09-30': 'float64', '2020-09-30': 'float64'}\n",
      "Number of Rows: 60\n",
      "Number of Columns: 5\n",
      "NaN values per column: {'Unnamed: 0': 0, '2023-09-30': 10, '2022-09-30': 2, '2021-09-30': 2, '2020-09-30': 2}\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to analyze a single CSV file\n",
    "def analyze_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        analyze_dataframe(df, file_path)\n",
    "    except Exception as e:\n",
    "        print(f'Could not read {file_path}: {e}')\n",
    "\n",
    "# Function to analyze a single JSON file with nested structure details\n",
    "def analyze_json(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.json_normalize(data, sep='_')\n",
    "            analyze_dataframe(df, file_path)\n",
    "            if isinstance(data, dict):\n",
    "                print_nested_json_structure(data, level=0)\n",
    "            elif isinstance(data, list) and data:\n",
    "                print_nested_json_structure(data[0], level=0)  # Assumes first element is representative of the list\n",
    "    except Exception as e:\n",
    "        print(f'Could not read {file_path}: {e}')\n",
    "\n",
    "# Helper function to print nested JSON structure\n",
    "def print_nested_json_structure(data, level):\n",
    "    indent = '  ' * level\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict) or isinstance(value, list):\n",
    "                print(f'{indent}{key}:')\n",
    "                print_nested_json_structure(value, level + 1)\n",
    "            else:\n",
    "                print(f'{indent}{key}: {type(value).__name__}')\n",
    "    elif isinstance(data, list):\n",
    "        print(f'{indent}List of {len(data)} items: {type(data[0]).__name__}')\n",
    "        if data and isinstance(data[0], (dict, list)):\n",
    "            print_nested_json_structure(data[0], level + 1)\n",
    "\n",
    "# General function to analyze DataFrames\n",
    "def analyze_dataframe(df, file_path):\n",
    "    column_details = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    num_rows, num_columns = df.shape\n",
    "    nan_values = df.isna().sum().to_dict()\n",
    "\n",
    "    print(f'File: {file_path}')\n",
    "    print(f'Columns and Types: {column_details}')\n",
    "    print(f'Number of Rows: {num_rows}')\n",
    "    print(f'Number of Columns: {num_columns}')\n",
    "    print(f'NaN values per column: {nan_values}')\n",
    "    print('-----------------------------------\\n')\n",
    "\n",
    "# Function to traverse directories and find CSV and JSON files\n",
    "def find_files(directory):\n",
    "    pathlist = Path(directory).rglob('*.*')\n",
    "    for path in pathlist:\n",
    "        # Analyze files based on extension\n",
    "        if path.suffix.lower() == '.csv':\n",
    "            analyze_csv(str(path))\n",
    "        elif path.suffix.lower() == '.json':\n",
    "            analyze_json(str(path))\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory containing your CSV and JSON files\n",
    "your_directory_path = './'\n",
    "find_files(your_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de214fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
