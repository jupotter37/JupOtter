{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AgentTrading.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "vMMX2vebhlV-",
        "3Ta3rnUuhxR1",
        "-SBhFpqZiEfx",
        "mu7wAQEIikv3",
        "viN03Xp5i7DD",
        "8Pfd6VDvjQL8",
        "CSVcFMujj0jV",
        "OqEiMmYxkAEs",
        "YCBzml2rkNDK",
        "PtTAdFevkqP8",
        "MuwQb3OOkzzT",
        "zAzCQAS3lLVz",
        "gU8qKzfPlU_t",
        "6TvlxiuKleUB",
        "JZ1MRol_lwFJ",
        "7bEvRLGpl-gr",
        "oHhTVooXmJNz",
        "WDTXyW8cmS9M",
        "BBq6lbZLmlxa",
        "Ra90QUK5mupJ",
        "FW6xxeNRm9QZ",
        "MUZcLPJAnEAT"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eduzc07/machinelearning/blob/master/2_Finance/AgentTrading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MZZdvAY5-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Further  recommendation, test multiple frameworks, see what works better on average. \n",
        "## Open source RL: https://docs.google.com/spreadsheets/d/1EeFPd-XIQ3mq_9snTlAZSsFY7Hbnmd7P5bbT8LPuMn0/edit#gid=0\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d-d2slt0njw",
        "colab_type": "text"
      },
      "source": [
        "## Trading Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc8KBpFO0uxz",
        "colab_type": "text"
      },
      "source": [
        "This notebook is part of a series on machine learning asset managment. https://ssrn.com/abstract=3420952\n",
        "\n",
        "I have had some enquiries about future releases, I will post future content on the [FirmAI](https://www.linkedin.com/company/18004273/admin/) Linkedin page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbWK98EqZ4HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yfinance --upgrade --no-cache-dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT72lycyY5-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save future files to your drive\n",
        "## In this notebook control for multiple testing\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "%cd \"/content/drive/My Drive/Data/Stocks\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "547ijRuxZ60f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas_datareader import data as pdr\n",
        "import fix_yahoo_finance as yf\n",
        "yf.pdr_override()\n",
        "df_full = pdr.get_data_yahoo(\"JPM\", start=\"2018-01-01\").reset_index()\n",
        "df_full.to_csv('JPM.csv',index=False)\n",
        "df_full.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW6a358naLnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_full = pd.read_csv('JPM.csv')\n",
        "df_full.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX2i3BI8gAw9",
        "colab_type": "text"
      },
      "source": [
        "#  Agents\n",
        "\n",
        "* Turtle Trading agent\n",
        "* Moving Average agent\n",
        "* Signal Rolling agent\n",
        "* Policy Gradient agent\n",
        "* Q-learning agent\n",
        "* Evolution Strategy agent\n",
        "* Double Q-learning agent\n",
        "* Recurrent Q-learning agent\n",
        "* Double Recurrent Q-learning agent\n",
        "* Duel Q-learning agent\n",
        "* Double Duel Q-learning agent\n",
        "* Duel Recurrent Q-learning agent\n",
        "* Double Duel Recurrent Q-learning agent\n",
        "* Actor-critic agent\n",
        "* Actor-critic Duel agent\n",
        "* Actor-critic Recurrent agent\n",
        "* Actor-critic Duel Recurrent agent\n",
        "* Curiosity Q-learning agent\n",
        "* Recurrent Curiosity Q-learning agent\n",
        "* Duel Curiosity Q-learning agent\n",
        "* Neuro-evolution agent\n",
        "* Neuro-evolution with Novelty search agent\n",
        "* ABCD strategy agent\n",
        "* Deep Evolution Strategy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMJLXIxsfRGR",
        "colab_type": "text"
      },
      "source": [
        "## Turtle Trading Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFQn4WTaY5-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Turtle Trading Agent'\n",
        "count = int(np.ceil(len(df) * 0.1))\n",
        "signals = pd.DataFrame(index=df.index)\n",
        "signals['signal'] = 0.0\n",
        "signals['trend'] = df['Close']\n",
        "signals['RollingMax'] = (signals.trend.shift(1).rolling(count).max())\n",
        "signals['RollingMin'] = (signals.trend.shift(1).rolling(count).min())\n",
        "signals.loc[signals['RollingMax'] < signals.trend, 'signal'] = -1\n",
        "signals.loc[signals['RollingMin'] > signals.trend, 'signal'] = 1\n",
        "signals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuxvi06DY5_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buy_stock(\n",
        "    real_movement,\n",
        "    signal,\n",
        "    initial_money = 10000,\n",
        "    max_buy = 1,\n",
        "    max_sell = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    real_movement = actual movement in the real world\n",
        "    delay = how much interval you want to delay to change our decision from buy to sell, vice versa\n",
        "    initial_state = 1 is buy, 0 is sell\n",
        "    initial_money = 1000, ignore what kind of currency\n",
        "    max_buy = max quantity for share to buy\n",
        "    max_sell = max quantity for share to sell\n",
        "    \"\"\"\n",
        "    starting_money = initial_money\n",
        "    states_sell = []\n",
        "    states_buy = []\n",
        "    current_inventory = 0\n",
        "\n",
        "    def buy(i, initial_money, current_inventory):\n",
        "        shares = initial_money // real_movement[i]\n",
        "        if shares < 1:\n",
        "            print(\n",
        "                'day %d: total balances %f, not enough money to buy a unit price %f'\n",
        "                % (i, initial_money, real_movement[i])\n",
        "            )\n",
        "        else:\n",
        "            if shares > max_buy:\n",
        "                buy_units = max_buy\n",
        "            else:\n",
        "                buy_units = shares\n",
        "            initial_money -= buy_units * real_movement[i]\n",
        "            current_inventory += buy_units\n",
        "            print(\n",
        "                'day %d: buy %d units at price %f, total balance %f'\n",
        "                % (i, buy_units, buy_units * real_movement[i], initial_money)\n",
        "            )\n",
        "            states_buy.append(0)\n",
        "        return initial_money, current_inventory\n",
        "\n",
        "    for i in range(real_movement.shape[0] - int(0.025 * len(df))):\n",
        "        state = signal[i]\n",
        "        if state == 1:\n",
        "            initial_money, current_inventory = buy(\n",
        "                i, initial_money, current_inventory\n",
        "            )\n",
        "            states_buy.append(i)\n",
        "        elif state == -1:\n",
        "            if current_inventory == 0:\n",
        "                    print('day %d: cannot sell anything, inventory 0' % (i))\n",
        "            else:\n",
        "                if current_inventory > max_sell:\n",
        "                    sell_units = max_sell\n",
        "                else:\n",
        "                    sell_units = current_inventory\n",
        "                current_inventory -= sell_units\n",
        "                total_sell = sell_units * real_movement[i]\n",
        "                initial_money += total_sell\n",
        "                try:\n",
        "                    invest = (\n",
        "                        (real_movement[i] - real_movement[states_buy[-1]])\n",
        "                        / real_movement[states_buy[-1]]\n",
        "                    ) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell %d units at price %f, investment %f %%, total balance %f,'\n",
        "                    % (i, sell_units, total_sell, invest, initial_money)\n",
        "                )\n",
        "            states_sell.append(i)\n",
        "            \n",
        "    invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "    total_gains = initial_money - starting_money\n",
        "    return states_buy, states_sell, total_gains, invest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWS0JuY6Y5_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = buy_stock(df.Close, signals['signal'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo2Pg-dcY5_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df['Close']\n",
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ckZ3efUhWSM",
        "colab_type": "text"
      },
      "source": [
        "## Moving Average agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPfxAch5Y5_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Moving Average agent'\n",
        "\n",
        "short_window = int(0.025 * len(df))\n",
        "long_window = int(0.05 * len(df))\n",
        "\n",
        "signals = pd.DataFrame(index=df.index)\n",
        "signals['signal'] = 0.0\n",
        "\n",
        "signals['short_ma'] = df['Close'].rolling(window=short_window, min_periods=1, center=False).mean()\n",
        "signals['long_ma'] = df['Close'].rolling(window=long_window, min_periods=1, center=False).mean()\n",
        "\n",
        "signals['signal'][short_window:] = np.where(signals['short_ma'][short_window:] \n",
        "                                            > signals['long_ma'][short_window:], 1.0, 0.0)   \n",
        "signals['positions'] = signals['signal'].diff()\n",
        "\n",
        "signals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x79XSrLY5_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buy_stock(\n",
        "    real_movement,\n",
        "    signal,\n",
        "    initial_money = 10000,\n",
        "    max_buy = 1,\n",
        "    max_sell = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    real_movement = actual movement in the real world\n",
        "    delay = how much interval you want to delay to change our decision from buy to sell, vice versa\n",
        "    initial_state = 1 is buy, 0 is sell\n",
        "    initial_money = 1000, ignore what kind of currency\n",
        "    max_buy = max quantity for share to buy\n",
        "    max_sell = max quantity for share to sell\n",
        "    \"\"\"\n",
        "    starting_money = initial_money\n",
        "    states_sell = []\n",
        "    states_buy = []\n",
        "    current_inventory = 0\n",
        "\n",
        "    def buy(i, initial_money, current_inventory):\n",
        "        shares = initial_money // real_movement[i]\n",
        "        if shares < 1:\n",
        "            print(\n",
        "                'day %d: total balances %f, not enough money to buy a unit price %f'\n",
        "                % (i, initial_money, real_movement[i])\n",
        "            )\n",
        "        else:\n",
        "            if shares > max_buy:\n",
        "                buy_units = max_buy\n",
        "            else:\n",
        "                buy_units = shares\n",
        "            initial_money -= buy_units * real_movement[i]\n",
        "            current_inventory += buy_units\n",
        "            print(\n",
        "                'day %d: buy %d units at price %f, total balance %f'\n",
        "                % (i, buy_units, buy_units * real_movement[i], initial_money)\n",
        "            )\n",
        "            states_buy.append(0)\n",
        "        return initial_money, current_inventory\n",
        "\n",
        "    for i in range(real_movement.shape[0] - int(0.025 * len(df))):\n",
        "        state = signal[i]\n",
        "        if state == 1:\n",
        "            initial_money, current_inventory = buy(\n",
        "                i, initial_money, current_inventory\n",
        "            )\n",
        "            states_buy.append(i)\n",
        "        elif state == -1:\n",
        "            if current_inventory == 0:\n",
        "                    print('day %d: cannot sell anything, inventory 0' % (i))\n",
        "            else:\n",
        "                if current_inventory > max_sell:\n",
        "                    sell_units = max_sell\n",
        "                else:\n",
        "                    sell_units = current_inventory\n",
        "                current_inventory -= sell_units\n",
        "                total_sell = sell_units * real_movement[i]\n",
        "                initial_money += total_sell\n",
        "                try:\n",
        "                    invest = (\n",
        "                        (real_movement[i] - real_movement[states_buy[-1]])\n",
        "                        / real_movement[states_buy[-1]]\n",
        "                    ) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell %d units at price %f, investment %f %%, total balance %f,'\n",
        "                    % (i, sell_units, total_sell, invest, initial_money)\n",
        "                )\n",
        "            states_sell.append(i)\n",
        "    invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "    total_gains = initial_money - starting_money\n",
        "    return states_buy, states_sell, total_gains, invest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8qJ0LgfY5_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = buy_stock(df.Close, signals['positions'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v45Kgse1Y6AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df['Close']\n",
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMMX2vebhlV-",
        "colab_type": "text"
      },
      "source": [
        "## Signal Rolling agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJIKMWCaUlET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Signal Rolling agent'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjPay_IIY6Al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buy_stock(\n",
        "    real_movement,\n",
        "    delay = 5,\n",
        "    initial_state = 1,\n",
        "    initial_money = 10000,\n",
        "    max_buy = 1,\n",
        "    max_sell = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    real_movement = actual movement in the real world\n",
        "    delay = how much interval you want to delay to change our decision from buy to sell, vice versa\n",
        "    initial_state = 1 is buy, 0 is sell\n",
        "    initial_money = 1000, ignore what kind of currency\n",
        "    max_buy = max quantity for share to buy\n",
        "    max_sell = max quantity for share to sell\n",
        "    \"\"\"\n",
        "    starting_money = initial_money\n",
        "    delay_change_decision = delay\n",
        "    current_decision = 0\n",
        "    state = initial_state\n",
        "    current_val = real_movement[0]\n",
        "    states_sell = []\n",
        "    states_buy = []\n",
        "    current_inventory = 0\n",
        "\n",
        "    def buy(i, initial_money, current_inventory):\n",
        "        shares = initial_money // real_movement[i]\n",
        "        if shares < 1:\n",
        "            print(\n",
        "                'day %d: total balances %f, not enough money to buy a unit price %f'\n",
        "                % (i, initial_money, real_movement[i])\n",
        "            )\n",
        "        else:\n",
        "            if shares > max_buy:\n",
        "                buy_units = max_buy\n",
        "            else:\n",
        "                buy_units = shares\n",
        "            initial_money -= buy_units * real_movement[i]\n",
        "            current_inventory += buy_units\n",
        "            print(\n",
        "                'day %d: buy %d units at price %f, total balance %f'\n",
        "                % (i, buy_units, buy_units * real_movement[i], initial_money)\n",
        "            )\n",
        "            states_buy.append(0)\n",
        "        return initial_money, current_inventory\n",
        "\n",
        "    if state == 1:\n",
        "        initial_money, current_inventory = buy(\n",
        "            0, initial_money, current_inventory\n",
        "        )\n",
        "\n",
        "    for i in range(1, real_movement.shape[0], 1):\n",
        "        if real_movement[i] < current_val and state == 0:\n",
        "            if current_decision < delay_change_decision:\n",
        "                current_decision += 1\n",
        "            else:\n",
        "                state = 1\n",
        "                initial_money, current_inventory = buy(\n",
        "                    i, initial_money, current_inventory\n",
        "                )\n",
        "                current_decision = 0\n",
        "                states_buy.append(i)\n",
        "        if real_movement[i] > current_val and state == 1:\n",
        "            if current_decision < delay_change_decision:\n",
        "                current_decision += 1\n",
        "            else:\n",
        "                state = 0\n",
        "\n",
        "                if current_inventory == 0:\n",
        "                    print('day %d: cannot sell anything, inventory 0' % (i))\n",
        "                else:\n",
        "                    if current_inventory > max_sell:\n",
        "                        sell_units = max_sell\n",
        "                    else:\n",
        "                        sell_units = current_inventory\n",
        "                    current_inventory -= sell_units\n",
        "                    total_sell = sell_units * real_movement[i]\n",
        "                    initial_money += total_sell\n",
        "                    try:\n",
        "                        invest = (\n",
        "                            (real_movement[i] - real_movement[states_buy[-1]])\n",
        "                            / real_movement[states_buy[-1]]\n",
        "                        ) * 100\n",
        "                    except:\n",
        "                        invest = 0\n",
        "                    print(\n",
        "                        'day %d, sell %d units at price %f, investment %f %%, total balance %f,'\n",
        "                        % (i, sell_units, total_sell, invest, initial_money)\n",
        "                    )\n",
        "\n",
        "                current_decision = 0\n",
        "                states_sell.append(i)\n",
        "        current_val = real_movement[i]\n",
        "    invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "    total_gains = initial_money - starting_money\n",
        "    return states_buy, states_sell, total_gains, invest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o01YIflY6Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = buy_stock(df.Close, initial_state = 1, \n",
        "                                                         delay = 4, initial_money = 10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAUEZBEiY6A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df['Close']\n",
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ta3rnUuhxR1",
        "colab_type": "text"
      },
      "source": [
        "## Policy Gradient agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "917fxMmyY6Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_full.copy()\n",
        "name = 'Policy Gradient agent'\n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 1e-4\n",
        "    LAYER_SIZE = 256\n",
        "    GAMMA = 0.9\n",
        "    OUTPUT_SIZE = 3\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.X = tf.placeholder(tf.float32, (None, self.state_size))\n",
        "        self.REWARDS = tf.placeholder(tf.float32, (None))\n",
        "        self.ACTIONS = tf.placeholder(tf.int32, (None))\n",
        "        feed_forward = tf.layers.dense(self.X, self.LAYER_SIZE, activation = tf.nn.relu)\n",
        "        self.logits = tf.layers.dense(feed_forward, self.OUTPUT_SIZE, activation = tf.nn.softmax)\n",
        "        input_y = tf.one_hot(self.ACTIONS, self.OUTPUT_SIZE)\n",
        "        loglike = tf.log((input_y * (input_y - self.logits) + (1 - input_y) * (input_y + self.logits)) + 1)\n",
        "        rewards = tf.tile(tf.reshape(self.REWARDS, (-1,1)), [1, self.OUTPUT_SIZE])\n",
        "        self.cost = -tf.reduce_mean(loglike * (rewards + 1)) \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def predict(self, inputs):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X:inputs})\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "    \n",
        "    def discount_rewards(self, r):\n",
        "        discounted_r = np.zeros_like(r)\n",
        "        running_add = 0\n",
        "        for t in reversed(range(0, r.size)):\n",
        "            running_add = running_add * self.GAMMA + r[t]\n",
        "            discounted_r[t] = running_add\n",
        "        return discounted_r\n",
        "    \n",
        "    def get_predicted_action(self, sequence):\n",
        "        prediction = self.predict(np.array(sequence))[0]\n",
        "        return np.argmax(prediction)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.get_predicted_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "                \n",
        "                \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            ep_history = []\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                action = self.get_predicted_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                if action == 1 and starting_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= close[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory):\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                ep_history.append([state,action,starting_money,next_state])\n",
        "                state = next_state\n",
        "            ep_history = np.array(ep_history)\n",
        "            ep_history[:,2] = self.discount_rewards(ep_history[:,2])\n",
        "            cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict={self.X:np.vstack(ep_history[:,0]),\n",
        "                                                    self.REWARDS:ep_history[:,2],\n",
        "                                                    self.ACTIONS:ep_history[:,1]})\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nORY6xY6Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "agent = Agent(state_size = window_size,\n",
        "             window_size = window_size,\n",
        "             trend = close,\n",
        "             skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Udxa6ptY6By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guJCSIkIY6B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBhFpqZiEfx",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Q9zy5hY6CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Q-learning agent'\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_size, window_size, trend, skip, batch_size):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.action_size = 3\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen = 1000)\n",
        "        self.inventory = []\n",
        "\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.X = tf.placeholder(tf.float32, [None, self.state_size])\n",
        "        self.Y = tf.placeholder(tf.float32, [None, self.action_size])\n",
        "        feed = tf.layers.dense(self.X, 256, activation = tf.nn.relu)\n",
        "        self.logits = tf.layers.dense(feed, self.action_size)\n",
        "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(\n",
        "            self.cost\n",
        "        )\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        return np.argmax(\n",
        "            self.sess.run(self.logits, feed_dict = {self.X: state})[0]\n",
        "        )\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        mini_batch = []\n",
        "        l = len(self.memory)\n",
        "        for i in range(l - batch_size, l):\n",
        "            mini_batch.append(self.memory[i])\n",
        "        replay_size = len(mini_batch)\n",
        "        X = np.empty((replay_size, self.state_size))\n",
        "        Y = np.empty((replay_size, self.action_size))\n",
        "        states = np.array([a[0][0] for a in mini_batch])\n",
        "        new_states = np.array([a[3][0] for a in mini_batch])\n",
        "        Q = self.sess.run(self.logits, feed_dict = {self.X: states})\n",
        "        Q_new = self.sess.run(self.logits, feed_dict = {self.X: new_states})\n",
        "        for i in range(len(mini_batch)):\n",
        "            state, action, reward, next_state, done = mini_batch[i]\n",
        "            target = Q[i]\n",
        "            target[action] = reward\n",
        "            if not done:\n",
        "                target[action] += self.gamma * np.amax(Q_new[i])\n",
        "            X[i] = state\n",
        "            Y[i] = target\n",
        "        cost, _ = self.sess.run(\n",
        "            [self.cost, self.optimizer], feed_dict = {self.X: X, self.Y: Y}\n",
        "        )\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        return cost\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "                \n",
        "                \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                action = self.act(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                self.memory.append((state, action, invest, \n",
        "                                    next_state, starting_money < initial_money))\n",
        "                state = next_state\n",
        "                batch_size = min(self.batch_size, len(self.memory))\n",
        "                cost = self.replay(batch_size)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oq5Qn2GY6CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip, \n",
        "              batch_size = batch_size)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH3RzMl8Y6Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo3iCOu2Y6Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu7wAQEIikv3",
        "colab_type": "text"
      },
      "source": [
        "## Evolution Strategy agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dlij-wJY6C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkg_resources\n",
        "import types\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Evolution Strategy agent'\n",
        "\n",
        "\n",
        "def get_imports():\n",
        "    for name, val in globals().items():\n",
        "        if isinstance(val, types.ModuleType):\n",
        "            name = val.__name__.split('.')[0]\n",
        "        elif isinstance(val, type):\n",
        "            name = val.__module__.split('.')[0]\n",
        "        poorly_named_packages = {'PIL': 'Pillow', 'sklearn': 'scikit-learn'}\n",
        "        if name in poorly_named_packages.keys():\n",
        "            name = poorly_named_packages[name]\n",
        "        yield name\n",
        "\n",
        "\n",
        "imports = list(set(get_imports()))\n",
        "requirements = []\n",
        "for m in pkg_resources.working_set:\n",
        "    if m.project_name in imports and m.project_name != 'pip':\n",
        "        requirements.append((m.project_name, m.version))\n",
        "\n",
        "for r in requirements:\n",
        "    print('{}=={}'.format(*r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNpK2cRY6DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep_Evolution_Strategy:\n",
        "\n",
        "    inputs = None\n",
        "\n",
        "    def __init__(\n",
        "        self, weights, reward_function, population_size, sigma, learning_rate\n",
        "    ):\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_function\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _get_weight_from_population(self, weights, population):\n",
        "        weights_population = []\n",
        "        for index, i in enumerate(population):\n",
        "            jittered = self.sigma * i\n",
        "            weights_population.append(weights[index] + jittered)\n",
        "        return weights_population\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def train(self, epoch = 100, print_every = 1):\n",
        "        lasttime = time.time()\n",
        "        for i in range(epoch):\n",
        "            population = []\n",
        "            rewards = np.zeros(self.population_size)\n",
        "            for k in range(self.population_size):\n",
        "                x = []\n",
        "                for w in self.weights:\n",
        "                    x.append(np.random.randn(*w.shape))\n",
        "                population.append(x)\n",
        "            for k in range(self.population_size):\n",
        "                weights_population = self._get_weight_from_population(\n",
        "                    self.weights, population[k]\n",
        "                )\n",
        "                rewards[k] = self.reward_function(weights_population)\n",
        "            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n",
        "            for index, w in enumerate(self.weights):\n",
        "                A = np.array([p[index] for p in population])\n",
        "                self.weights[index] = (\n",
        "                    w\n",
        "                    + self.learning_rate\n",
        "                    / (self.population_size * self.sigma)\n",
        "                    * np.dot(A.T, rewards).T\n",
        "                )\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\n",
        "                    'iter %d. reward: %f'\n",
        "                    % (i + 1, self.reward_function(self.weights))\n",
        "                )\n",
        "        print('time taken to train:', time.time() - lasttime, 'seconds')\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, layer_size, output_size):\n",
        "        self.weights = [\n",
        "            np.random.randn(input_size, layer_size),\n",
        "            np.random.randn(layer_size, output_size),\n",
        "            np.random.randn(1, layer_size),\n",
        "        ]\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\n",
        "        decision = np.dot(feed, self.weights[1])\n",
        "        return decision\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UZ69p0CY6DL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "\n",
        "    POPULATION_SIZE = 15\n",
        "    SIGMA = 0.1\n",
        "    LEARNING_RATE = 0.03\n",
        "\n",
        "    def __init__(self, model, window_size, trend, skip, initial_money):\n",
        "        self.model = model\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.initial_money = initial_money\n",
        "        self.es = Deep_Evolution_Strategy(\n",
        "            self.model.get_weights(),\n",
        "            self.get_reward,\n",
        "            self.POPULATION_SIZE,\n",
        "            self.SIGMA,\n",
        "            self.LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "    def act(self, sequence):\n",
        "        decision = self.model.predict(np.array(sequence))\n",
        "        return np.argmax(decision[0])\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "\n",
        "    def get_reward(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        self.model.weights = weights\n",
        "        state = self.get_state(0)\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and starting_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                starting_money -= close[t]\n",
        "                \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                starting_money += self.trend[t]\n",
        "\n",
        "            state = next_state\n",
        "        return ((starting_money - initial_money) / initial_money) * 100\n",
        "\n",
        "    def fit(self, iterations, checkpoint):\n",
        "        self.es.train(iterations, print_every = checkpoint)\n",
        "\n",
        "    def buy(self):\n",
        "        initial_money = self.initial_money\n",
        "        state = self.get_state(0)\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            state = next_state\n",
        "\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngdPbts9Y6DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "window_size = 30\n",
        "skip = 1\n",
        "initial_money = 10000\n",
        "\n",
        "model = Model(input_size = window_size, layer_size = 500, output_size = 3)\n",
        "agent = Agent(model = model, \n",
        "              window_size = window_size,\n",
        "              trend = close,\n",
        "              skip = skip,\n",
        "              initial_money = initial_money)\n",
        "agent.fit(iterations = 500, checkpoint = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p-enWJxY6DZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOg5ZkJY6D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viN03Xp5i7DD",
        "colab_type": "text"
      },
      "source": [
        "## Double Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncrkX6cOY6ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Double Q-learning agent'\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, output_size, layer_size, learning_rate):\n",
        "        self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "        feed_forward = tf.layers.dense(self.X, layer_size, activation = tf.nn.relu)\n",
        "        self.logits = tf.layers.dense(feed_forward, output_size)\n",
        "        self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 500\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.model = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.model_negative = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.trainable = tf.trainable_variables()\n",
        "    \n",
        "    def _assign(self):\n",
        "        for i in range(len(self.trainable)//2):\n",
        "            assign_op = self.trainable[i+len(self.trainable)//2].assign(self.trainable[i])\n",
        "            self.sess.run(assign_op)\n",
        "\n",
        "    def _memorize(self, state, action, reward, new_state, done):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, done))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.predict(states)\n",
        "        Q_new = self.predict(new_states)\n",
        "        Q_new_negative = self.sess.run(self.model_negative.logits, feed_dict={self.model_negative.X:new_states})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, done_r = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not done_r:\n",
        "                target[action_r] += self.GAMMA * Q_new_negative[i, np.argmax(Q_new[i])]\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "        return X, Y\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.sess.run(self.model.logits, feed_dict={self.model.X:inputs})\n",
        "    \n",
        "    def get_predicted_action(self, sequence):\n",
        "        prediction = self.predict(np.array(sequence))[0]\n",
        "        return np.argmax(prediction)\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign()\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                state = next_state\n",
        "                X, Y = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.model.cost, self.model.optimizer], \n",
        "                                        feed_dict={self.model.X: X, self.model.Y:Y})\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFML6aw0Y6EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uiABCNXY6Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhaJxtjyY6Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pfd6VDvjQL8",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3De2HdkaY6FH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Recurrent Q-learning agent'\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.X = tf.placeholder(tf.float32, (None, None, self.state_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, self.OUTPUT_SIZE))\n",
        "        cell = tf.nn.rnn_cell.LSTMCell(self.LAYER_SIZE, state_is_tuple = False)\n",
        "        self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * self.LAYER_SIZE))\n",
        "        self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "        self.logits = tf.layers.dense(self.rnn[:,-1], self.OUTPUT_SIZE)\n",
        "        self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.logits, feed_dict={self.X:states, self.hidden_layer:init_values})\n",
        "        Q_new = self.sess.run(self.logits, feed_dict={self.X:new_states, self.hidden_layer:init_values})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, 4, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        INIT_VAL = np.empty((replay_size, 2 * self.LAYER_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, dead_r, rnn_memory = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not dead_r:\n",
        "                target[action_r] += self.GAMMA * np.amax(Q_new[i])\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "            INIT_VAL[i] = rnn_memory\n",
        "        return X, Y, INIT_VAL\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action, last_state = self.sess.run([self.logits,self.last_state],\n",
        "                                                feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                            self.hidden_layer:init_value})\n",
        "            action, init_value = np.argmax(action[0]), last_state\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.logits,\n",
        "                                                  self.last_state],\n",
        "                                                  feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                X, Y, INIT_VAL = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.cost, self.optimizer], \n",
        "                                        feed_dict={self.X: X, self.Y:Y,\n",
        "                                                  self.hidden_layer: INIT_VAL})\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "                \n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynsI0rAgY6FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2_IseMoY6FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-SAzKY9Y6FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSVcFMujj0jV",
        "colab_type": "text"
      },
      "source": [
        "## Double Recurrent Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBtNunUBY6Fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Double Recurrent Q-learning agent'\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, output_size, layer_size, learning_rate, name):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(layer_size, state_is_tuple = False)\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * layer_size))\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            self.logits = tf.layers.dense(self.rnn[:,-1], output_size)\n",
        "            self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.model = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE,\n",
        "                           'real_model')\n",
        "        self.model_negative = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE,\n",
        "                                   'negative_model')\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.trainable = tf.trainable_variables()\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "\n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.model.logits, feed_dict={self.model.X:states, \n",
        "                                                   self.model.hidden_layer:init_values})\n",
        "        Q_new = self.sess.run(self.model.logits, feed_dict={self.model.X:new_states, \n",
        "                                                       self.model.hidden_layer:init_values})\n",
        "        Q_new_negative = self.sess.run(self.model_negative.logits, \n",
        "                                  feed_dict={self.model_negative.X:new_states, \n",
        "                                             self.model_negative.hidden_layer:init_values})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, 4, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        INIT_VAL = np.empty((replay_size, 2 * self.LAYER_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, dead_r, rnn_memory = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not dead_r:\n",
        "                target[action_r] += self.GAMMA * Q_new_negative[i, np.argmax(Q_new[i])]\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "            INIT_VAL[i] = rnn_memory\n",
        "        return X, Y, INIT_VAL\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action, last_state = self.sess.run([self.model.logits,self.model.last_state],\n",
        "                                                feed_dict={self.model.X:[self.INITIAL_FEATURES],\n",
        "                                                            self.model.hidden_layer:init_value})\n",
        "            action, init_value = np.argmax(action[0]), last_state\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('real_model', 'negative_model')\n",
        "                \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.model.logits,\n",
        "                                                  self.model.last_state],\n",
        "                                                  feed_dict={self.model.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.model.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                \n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                X, Y, INIT_VAL = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.model.cost, self.model.optimizer], \n",
        "                                        feed_dict={self.model.X: X, self.model.Y:Y,\n",
        "                                                  self.model.hidden_layer: INIT_VAL})\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg_KX0osY6F1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0EYcm2QY6F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UiDs2g9Y6GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqEiMmYxkAEs",
        "colab_type": "text"
      },
      "source": [
        "## Duel Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeqtoEHjY6Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Duel Q-learning agent'\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_size, window_size, trend, skip, batch_size):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.action_size = 3\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen = 1000)\n",
        "        self.inventory = []\n",
        "\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.X = tf.placeholder(tf.float32, [None, self.state_size])\n",
        "        self.Y = tf.placeholder(tf.float32, [None, self.action_size])\n",
        "        feed = tf.layers.dense(self.X, 512, activation = tf.nn.relu)\n",
        "        tensor_action, tensor_validation = tf.split(feed,2,1)\n",
        "        feed_action = tf.layers.dense(tensor_action, self.action_size)\n",
        "        feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "        self.logits = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(\n",
        "            self.cost\n",
        "        )\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        return np.argmax(\n",
        "            self.sess.run(self.logits, feed_dict = {self.X: state})[0]\n",
        "        )\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        mini_batch = []\n",
        "        l = len(self.memory)\n",
        "        for i in range(l - batch_size, l):\n",
        "            mini_batch.append(self.memory[i])\n",
        "        replay_size = len(mini_batch)\n",
        "        X = np.empty((replay_size, self.state_size))\n",
        "        Y = np.empty((replay_size, self.action_size))\n",
        "        states = np.array([a[0][0] for a in mini_batch])\n",
        "        new_states = np.array([a[3][0] for a in mini_batch])\n",
        "        Q = self.sess.run(self.logits, feed_dict = {self.X: states})\n",
        "        Q_new = self.sess.run(self.logits, feed_dict = {self.X: new_states})\n",
        "        for i in range(len(mini_batch)):\n",
        "            state, action, reward, next_state, done = mini_batch[i]\n",
        "            target = Q[i]\n",
        "            target[action] = reward\n",
        "            if not done:\n",
        "                target[action] += self.gamma * np.amax(Q_new[i])\n",
        "            X[i] = state\n",
        "            Y[i] = target\n",
        "        cost, _ = self.sess.run(\n",
        "            [self.cost, self.optimizer], feed_dict = {self.X: X, self.Y: Y}\n",
        "        )\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        return cost\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "                \n",
        "                \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                action = self.act(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t] and t < (len(self.trend) - self.half_window):\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                self.memory.append((state, action, invest, \n",
        "                                    next_state, starting_money < initial_money))\n",
        "                state = next_state\n",
        "                batch_size = min(self.batch_size, len(self.memory))\n",
        "                cost = self.replay(batch_size)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM7bti2oY6Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip, \n",
        "              batch_size = batch_size)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yvtgOnYY6Gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdF9jhW9Y6Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCBzml2rkNDK",
        "colab_type": "text"
      },
      "source": [
        "## Double Duel Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBXz9F_oY6Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Double Duel Q-learning agent'\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, output_size, layer_size, learning_rate):\n",
        "        self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "        feed = tf.layers.dense(self.X, layer_size, activation = tf.nn.relu)\n",
        "        tensor_action, tensor_validation = tf.split(feed,2,1)\n",
        "        feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "        feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "        self.logits = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "        self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 500\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.model = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.model_negative = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.trainable = tf.trainable_variables()\n",
        "    \n",
        "    def _assign(self):\n",
        "        for i in range(len(self.trainable)//2):\n",
        "            assign_op = self.trainable[i+len(self.trainable)//2].assign(self.trainable[i])\n",
        "            self.sess.run(assign_op)\n",
        "\n",
        "    def _memorize(self, state, action, reward, new_state, done):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, done))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.predict(states)\n",
        "        Q_new = self.predict(new_states)\n",
        "        Q_new_negative = self.sess.run(self.model_negative.logits, feed_dict={self.model_negative.X:new_states})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, done_r = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not done_r:\n",
        "                target[action_r] += self.GAMMA * Q_new_negative[i, np.argmax(Q_new[i])]\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "        return X, Y\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.sess.run(self.model.logits, feed_dict={self.model.X:inputs})\n",
        "    \n",
        "    def get_predicted_action(self, sequence):\n",
        "        prediction = self.predict(np.array(sequence))[0]\n",
        "        return np.argmax(prediction)\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign()\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                X, Y = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.model.cost, self.model.optimizer], \n",
        "                                        feed_dict={self.model.X: X, self.model.Y:Y})\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LWDczUmY6Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLVffjKaY6Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YukOB8aY6H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtTAdFevkqP8",
        "colab_type": "text"
      },
      "source": [
        "## Duel Recurrent Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F910c0BkY6IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Duel Recurrent Q-learning agent'\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.X = tf.placeholder(tf.float32, (None, None, self.state_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, self.OUTPUT_SIZE))\n",
        "        cell = tf.nn.rnn_cell.LSTMCell(self.LAYER_SIZE, state_is_tuple = False)\n",
        "        self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * self.LAYER_SIZE))\n",
        "        self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "        tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
        "        feed_action = tf.layers.dense(tensor_action, self.OUTPUT_SIZE)\n",
        "        feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "        self.logits = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "        self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.cost)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.logits, feed_dict={self.X:states, self.hidden_layer:init_values})\n",
        "        Q_new = self.sess.run(self.logits, feed_dict={self.X:new_states, self.hidden_layer:init_values})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, 4, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        INIT_VAL = np.empty((replay_size, 2 * self.LAYER_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, dead_r, rnn_memory = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not dead_r:\n",
        "                target[action_r] += self.GAMMA * np.amax(Q_new[i])\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "            INIT_VAL[i] = rnn_memory\n",
        "        return X, Y, INIT_VAL\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action, last_state = self.sess.run([self.logits,self.last_state],\n",
        "                                                feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                            self.hidden_layer:init_value})\n",
        "            action, init_value = np.argmax(action[0]), last_state\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.logits,\n",
        "                                                  self.last_state],\n",
        "                                                  feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                X, Y, INIT_VAL = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.cost, self.optimizer], \n",
        "                                        feed_dict={self.X: X, self.Y:Y,\n",
        "                                                  self.hidden_layer: INIT_VAL})\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "                \n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ToYk3F4Y6IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWkyTt7NY6Ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Puz6jlY6Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuwQb3OOkzzT",
        "colab_type": "text"
      },
      "source": [
        "## Double Duel Recurrent Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A55H6rUmY6Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Double Duel Recurrent Q-learning agent'\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, input_size, output_size, layer_size, learning_rate, name):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(layer_size, state_is_tuple = False)\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * layer_size))\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            self.logits = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            self.cost = tf.reduce_sum(tf.square(self.Y - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.model = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE,\n",
        "                           'real_model')\n",
        "        self.model_negative = Model(self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE,\n",
        "                                   'negative_model')\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.trainable = tf.trainable_variables()\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "\n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "\n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "\n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.model.logits, feed_dict={self.model.X:states, \n",
        "                                                   self.model.hidden_layer:init_values})\n",
        "        Q_new = self.sess.run(self.model.logits, feed_dict={self.model.X:new_states, \n",
        "                                                       self.model.hidden_layer:init_values})\n",
        "        Q_new_negative = self.sess.run(self.model_negative.logits, \n",
        "                                  feed_dict={self.model_negative.X:new_states, \n",
        "                                             self.model_negative.hidden_layer:init_values})\n",
        "        replay_size = len(replay)\n",
        "        X = np.empty((replay_size, 4, self.state_size))\n",
        "        Y = np.empty((replay_size, self.OUTPUT_SIZE))\n",
        "        INIT_VAL = np.empty((replay_size, 2 * self.LAYER_SIZE))\n",
        "        for i in range(replay_size):\n",
        "            state_r, action_r, reward_r, new_state_r, dead_r, rnn_memory = replay[i]\n",
        "            target = Q[i]\n",
        "            target[action_r] = reward_r\n",
        "            if not dead_r:\n",
        "                target[action_r] += self.GAMMA * Q_new_negative[i, np.argmax(Q_new[i])]\n",
        "            X[i] = state_r\n",
        "            Y[i] = target\n",
        "            INIT_VAL[i] = rnn_memory\n",
        "        return X, Y, INIT_VAL\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action, last_state = self.sess.run([self.model.logits,self.model.last_state],\n",
        "                                                feed_dict={self.model.X:[self.INITIAL_FEATURES],\n",
        "                                                            self.model.hidden_layer:init_value})\n",
        "            action, init_value = np.argmax(action[0]), last_state\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "            \n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('real_model', 'negative_model')\n",
        "                \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.model.logits,\n",
        "                                                  self.model.last_state],\n",
        "                                                  feed_dict={self.model.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.model.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                \n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                X, Y, INIT_VAL = self._construct_memories(replay)\n",
        "                \n",
        "                cost, _ = self.sess.run([self.model.cost, self.model.optimizer], \n",
        "                                        feed_dict={self.model.X: X, self.model.Y:Y,\n",
        "                                                  self.model.hidden_layer: INIT_VAL})\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_IrLTwjY6Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYvB0HddY6Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlRP3GDnY6I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAzCQAS3lLVz",
        "colab_type": "text"
      },
      "source": [
        "## Actor-critic agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PliOmM9JY6JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Actor-critic agent'\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size, size_layer):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            feed_actor = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_actor, output_size)\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            feed_critic = tf.layers.dense(feed_critic, output_size, activation = tf.nn.relu) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "            \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
        "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "            \n",
        "    def _memorize(self, state, action, reward, new_state, dead):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads})\n",
        "        \n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits, \n",
        "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-1]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
        "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards})\n",
        "        return cost\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                state = next_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PZo8zYuY6JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIfLf9RfY6JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8-eGw7KY6JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU8qKzfPlU_t",
        "colab_type": "text"
      },
      "source": [
        "## Actor-critic Duel agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOIC7kuAY6Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Actor-critic Duel agent'\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size, size_layer):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            feed_actor = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            tensor_action, tensor_validation = tf.split(feed_actor,2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            self.logits = feed_validation + tf.subtract(feed_action,\n",
        "                                                        tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            tensor_action, tensor_validation = tf.split(feed_critic,2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            feed_critic = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            feed_critic = tf.nn.relu(feed_critic) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "            \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
        "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "            \n",
        "    def _memorize(self, state, action, reward, new_state, dead):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads})\n",
        "        \n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits, \n",
        "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-1]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
        "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards})\n",
        "        return cost\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiEQxOF8Y6Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaiHbiIRY6Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFMaEsheY6Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TvlxiuKleUB",
        "colab_type": "text"
      },
      "source": [
        "## Actor-critic Recurrent agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3qVL6J5Y6J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Actor-critic Recurrent agent'\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size, size_layer):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            self.logits = tf.layers.dense(self.rnn[:,-1], output_size)\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            feed_critic = tf.layers.dense(self.rnn[:,-1], output_size, activation = tf.nn.relu) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "            \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
        "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "            \n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states,\n",
        "                                                       self.actor.hidden_layer: init_values})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states,\n",
        "                                                                     self.actor_target.hidden_layer: init_values})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q,\n",
        "                                                          self.critic.hidden_layer: init_values})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads,\n",
        "                                                self.actor.hidden_layer: init_values})\n",
        "        \n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits, \n",
        "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target,\n",
        "                                                 self.critic_target.hidden_layer: init_values})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-2]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
        "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards,\n",
        "                                          self.critic.hidden_layer: init_values})\n",
        "        return cost\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            \n",
        "            if np.random.rand() < self.EPSILON:\n",
        "                action = np.random.randint(self.OUTPUT_SIZE)\n",
        "            else:\n",
        "                action, last_state = self.sess.run([self.actor.logits,\n",
        "                                                  self.actor.last_state],\n",
        "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.actor.hidden_layer:init_value})\n",
        "                action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                    \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.actor.logits,\n",
        "                                                  self.actor.last_state],\n",
        "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.actor.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-LtXHfY6KA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGom_6RdY6KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjGsoOgY6KM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ1MRol_lwFJ",
        "colab_type": "text"
      },
      "source": [
        "## Actor-critic Duel Recurrent agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_P3A6sAY6KW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Actor-critic Duel Recurrent agent'\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, name, input_size, output_size, size_layer):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            self.logits = feed_validation + tf.subtract(feed_action,\n",
        "                                                        tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, name, input_size, output_size, size_layer, learning_rate):\n",
        "        with tf.variable_scope(name):\n",
        "            self.X = tf.placeholder(tf.float32, (None, None, input_size))\n",
        "            self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
        "            self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * size_layer))\n",
        "            self.REWARD = tf.placeholder(tf.float32, (None, 1))\n",
        "            feed_critic = tf.layers.dense(self.X, size_layer, activation = tf.nn.relu)\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
        "            self.rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X, cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            tensor_action, tensor_validation = tf.split(self.rnn[:,-1],2,1)\n",
        "            feed_action = tf.layers.dense(tensor_action, output_size)\n",
        "            feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "            feed_critic = feed_validation + tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            feed_critic = tf.nn.relu(feed_critic) + self.Y\n",
        "            feed_critic = tf.layers.dense(feed_critic, size_layer//2, activation = tf.nn.relu)\n",
        "            self.logits = tf.layers.dense(feed_critic, 1)\n",
        "            self.cost = tf.reduce_mean(tf.square(self.REWARD - self.logits))\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "            \n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.001\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 256\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    MEMORY_SIZE = 300\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "\n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.actor = Actor('actor-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.actor_target = Actor('actor-target', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE)\n",
        "        self.critic = Critic('critic-original', self.state_size, self.OUTPUT_SIZE, self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.critic_target = Critic('critic-target', self.state_size, self.OUTPUT_SIZE, \n",
        "                                    self.LAYER_SIZE, self.LEARNING_RATE)\n",
        "        self.grad_critic = tf.gradients(self.critic.logits, self.critic.Y)\n",
        "        self.actor_critic_grad = tf.placeholder(tf.float32, [None, self.OUTPUT_SIZE])\n",
        "        weights_actor = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
        "        self.grad_actor = tf.gradients(self.actor.logits, weights_actor, -self.actor_critic_grad)\n",
        "        grads = zip(self.grad_actor, weights_actor)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE).apply_gradients(grads)\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _assign(self, from_name, to_name):\n",
        "        from_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_name)\n",
        "        to_w = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_name)\n",
        "        for i in range(len(from_w)):\n",
        "            assign_op = to_w[i].assign(from_w[i])\n",
        "            self.sess.run(assign_op)\n",
        "            \n",
        "    def _memorize(self, state, action, reward, new_state, dead, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, dead, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            prediction = self.sess.run(self.actor.logits, feed_dict={self.actor.X:[state]})[0]\n",
        "            action = np.argmax(prediction)\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories_and_train(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        Q = self.sess.run(self.actor.logits, feed_dict={self.actor.X: states,\n",
        "                                                       self.actor.hidden_layer: init_values})\n",
        "        Q_target = self.sess.run(self.actor_target.logits, feed_dict={self.actor_target.X: states,\n",
        "                                                                     self.actor_target.hidden_layer: init_values})\n",
        "        grads = self.sess.run(self.grad_critic, feed_dict={self.critic.X:states, self.critic.Y:Q,\n",
        "                                                          self.critic.hidden_layer: init_values})[0]\n",
        "        self.sess.run(self.optimizer, feed_dict={self.actor.X:states, self.actor_critic_grad:grads,\n",
        "                                                self.actor.hidden_layer: init_values})\n",
        "        \n",
        "        rewards = np.array([a[2] for a in replay]).reshape((-1, 1))\n",
        "        rewards_target = self.sess.run(self.critic_target.logits, \n",
        "                                       feed_dict={self.critic_target.X:new_states,self.critic_target.Y:Q_target,\n",
        "                                                 self.critic_target.hidden_layer: init_values})\n",
        "        for i in range(len(replay)):\n",
        "            if not replay[0][-2]:\n",
        "                rewards[i] += self.GAMMA * rewards_target[i]\n",
        "        cost, _ = self.sess.run([self.critic.cost, self.critic.optimizer], \n",
        "                                feed_dict={self.critic.X:states, self.critic.Y:Q, self.critic.REWARD:rewards,\n",
        "                                          self.critic.hidden_layer: init_values})\n",
        "        return cost\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            \n",
        "            if np.random.rand() < self.EPSILON:\n",
        "                action = np.random.randint(self.OUTPUT_SIZE)\n",
        "            else:\n",
        "                action, last_state = self.sess.run([self.actor.logits,\n",
        "                                                  self.actor.last_state],\n",
        "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.actor.hidden_layer:init_value})\n",
        "                action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if (self.T_COPY + 1) % self.COPY == 0:\n",
        "                    self._assign('actor-original', 'actor-target')\n",
        "                    self._assign('critic-original', 'critic-target')\n",
        "                    \n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.actor.logits,\n",
        "                                                  self.actor.last_state],\n",
        "                                                  feed_dict={self.actor.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.actor.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories_and_train(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7wk6NuQY6Ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqxgWjdCY6Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE5_hlSQY6Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEvRLGpl-gr",
        "colab_type": "text"
      },
      "source": [
        "## Curiosity Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRhSCa_6Y6K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Curiosity Q-learning agent'\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 500\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.X = tf.placeholder(tf.float32, (None, self.state_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, self.state_size))\n",
        "        self.ACTION = tf.placeholder(tf.float32, (None))\n",
        "        self.REWARD = tf.placeholder(tf.float32, (None))\n",
        "        self.batch_size = tf.shape(self.ACTION)[0]\n",
        "        \n",
        "        with tf.variable_scope('curiosity_model'):\n",
        "            action = tf.reshape(self.ACTION, (-1,1))\n",
        "            state_action = tf.concat([self.X, action], axis=1)\n",
        "            save_state = tf.identity(self.Y)\n",
        "            \n",
        "            feed = tf.layers.dense(state_action, 32, activation=tf.nn.relu)\n",
        "            self.curiosity_logits = tf.layers.dense(feed, self.state_size)\n",
        "            self.curiosity_cost = tf.reduce_sum(tf.square(save_state - self.curiosity_logits), axis=1)\n",
        "            \n",
        "            self.curiosity_optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE)\\\n",
        "            .minimize(tf.reduce_mean(self.curiosity_cost))\n",
        "        \n",
        "        total_reward = tf.add(self.curiosity_cost, self.REWARD)\n",
        "        \n",
        "        with tf.variable_scope(\"q_model\"):\n",
        "            with tf.variable_scope(\"eval_net\"):\n",
        "                x_action = tf.layers.dense(self.X, 128, tf.nn.relu)\n",
        "                self.logits = tf.layers.dense(x_action, self.OUTPUT_SIZE)\n",
        "            \n",
        "            with tf.variable_scope(\"target_net\"):\n",
        "                y_action = tf.layers.dense(self.Y, 128, tf.nn.relu)\n",
        "                y_q = tf.layers.dense(y_action, self.OUTPUT_SIZE)\n",
        "            \n",
        "            q_target = total_reward + self.GAMMA * tf.reduce_max(y_q, axis=1)\n",
        "            action = tf.cast(self.ACTION, tf.int32)\n",
        "            action_indices = tf.stack([tf.range(self.batch_size, dtype=tf.int32), action], axis=1)\n",
        "            q = tf.gather_nd(params=self.logits, indices=action_indices)\n",
        "            self.cost = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE).minimize(\n",
        "            self.cost, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"q_model/eval_net\"))\n",
        "            \n",
        "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/target_net')\n",
        "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/eval_net')\n",
        "        self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
        "        \n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _memorize(self, state, action, reward, new_state, done):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, done))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def predict(self, inputs):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X:inputs})\n",
        "    \n",
        "    def get_predicted_action(self, sequence):\n",
        "        prediction = self.predict(np.array(sequence))[0]\n",
        "        return np.argmax(prediction)\n",
        "    \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        actions = np.array([a[1] for a in replay])\n",
        "        rewards = np.array([a[2] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.target_replace_op)\n",
        "            \n",
        "        cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict = {\n",
        "            self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
        "        })\n",
        "        \n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.curiosity_optimizer, feed_dict = {\n",
        "                self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
        "            })\n",
        "        return cost\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PRT-_YZY6K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2V7eseGY6LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B970oAxOY6LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHhTVooXmJNz",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Curiosity Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnhuWrgbY6Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Recurrent Curiosity Q-learning agent'\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 128\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.INITIAL_FEATURES = np.zeros((4, self.state_size))\n",
        "        self.X = tf.placeholder(tf.float32, (None, None, self.state_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, None, self.state_size))\n",
        "        self.hidden_layer = tf.placeholder(tf.float32, (None, 2 * self.LAYER_SIZE))\n",
        "        self.ACTION = tf.placeholder(tf.float32, (None))\n",
        "        self.REWARD = tf.placeholder(tf.float32, (None))\n",
        "        self.batch_size = tf.shape(self.ACTION)[0]\n",
        "        self.seq_len = tf.shape(self.X)[1]\n",
        "        \n",
        "        with tf.variable_scope('curiosity_model'):\n",
        "            action = tf.reshape(self.ACTION, (-1,1,1))\n",
        "            repeat_action = tf.tile(action, [1,self.seq_len,1])\n",
        "            state_action = tf.concat([self.X, repeat_action], axis=-1)\n",
        "            save_state = tf.identity(self.Y)\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(self.LAYER_SIZE, state_is_tuple = False)\n",
        "            self.rnn,last_state = tf.nn.dynamic_rnn(inputs=state_action,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "            self.curiosity_logits = tf.layers.dense(self.rnn[:,-1], self.state_size)\n",
        "            self.curiosity_cost = tf.reduce_sum(tf.square(save_state[:,-1] - self.curiosity_logits), axis=1)\n",
        "            \n",
        "            self.curiosity_optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE)\\\n",
        "            .minimize(tf.reduce_mean(self.curiosity_cost))\n",
        "        \n",
        "        total_reward = tf.add(self.curiosity_cost, self.REWARD)\n",
        "        \n",
        "        with tf.variable_scope(\"q_model\"):\n",
        "            with tf.variable_scope(\"eval_net\"):\n",
        "                cell = tf.nn.rnn_cell.LSTMCell(self.LAYER_SIZE, state_is_tuple = False)\n",
        "                rnn,self.last_state = tf.nn.dynamic_rnn(inputs=self.X,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "                self.logits = tf.layers.dense(rnn[:,-1], self.OUTPUT_SIZE)\n",
        "            \n",
        "            with tf.variable_scope(\"target_net\"):\n",
        "                cell = tf.nn.rnn_cell.LSTMCell(self.LAYER_SIZE, state_is_tuple = False)\n",
        "                rnn,last_state = tf.nn.dynamic_rnn(inputs=self.Y,cell=cell,\n",
        "                                                    dtype=tf.float32,\n",
        "                                                    initial_state=self.hidden_layer)\n",
        "                y_q = tf.layers.dense(rnn[:,-1], self.OUTPUT_SIZE)\n",
        "            \n",
        "            q_target = total_reward + self.GAMMA * tf.reduce_max(y_q, axis=1)\n",
        "            action = tf.cast(self.ACTION, tf.int32)\n",
        "            action_indices = tf.stack([tf.range(self.batch_size, dtype=tf.int32), action], axis=1)\n",
        "            q = tf.gather_nd(params=self.logits, indices=action_indices)\n",
        "            self.cost = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE).minimize(\n",
        "            self.cost, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"q_model/eval_net\"))\n",
        "            \n",
        "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/target_net')\n",
        "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/eval_net')\n",
        "        self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
        "        \n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _memorize(self, state, action, reward, new_state, done, rnn_state):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, done, rnn_state))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        actions = np.array([a[1] for a in replay])\n",
        "        rewards = np.array([a[2] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        init_values = np.array([a[-1] for a in replay])\n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.target_replace_op)\n",
        "            \n",
        "        cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict = {\n",
        "            self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards,\n",
        "            self.hidden_layer: init_values\n",
        "        })\n",
        "        \n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.curiosity_optimizer, feed_dict = {\n",
        "                self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards,\n",
        "                self.hidden_layer: init_values\n",
        "            })\n",
        "        return cost\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "        for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "            self.INITIAL_FEATURES[k,:] = state\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            \n",
        "            if np.random.rand() < self.EPSILON:\n",
        "                action = np.random.randint(self.OUTPUT_SIZE)\n",
        "            else:\n",
        "                action, last_state = self.sess.run([self.logits,\n",
        "                                                  self.last_state],\n",
        "                                                  feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.hidden_layer:init_value})\n",
        "                action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "            self.INITIAL_FEATURES = new_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            init_value = np.zeros((1, 2 * self.LAYER_SIZE))\n",
        "            for k in range(self.INITIAL_FEATURES.shape[0]):\n",
        "                self.INITIAL_FEATURES[k,:] = state\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                if np.random.rand() < self.EPSILON:\n",
        "                    action = np.random.randint(self.OUTPUT_SIZE)\n",
        "                else:\n",
        "                    action, last_state = self.sess.run([self.logits,\n",
        "                                                  self.last_state],\n",
        "                                                  feed_dict={self.X:[self.INITIAL_FEATURES],\n",
        "                                                             self.hidden_layer:init_value})\n",
        "                    action, init_value = np.argmax(action[0]), last_state\n",
        "                    \n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                new_state = np.append([self.get_state(t + 1)], self.INITIAL_FEATURES[:3, :], axis = 0)\n",
        "                self._memorize(self.INITIAL_FEATURES, action, invest, new_state, \n",
        "                               starting_money < initial_money, init_value[0])\n",
        "                self.INITIAL_FEATURES = new_state\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOksmdGVY6Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy_y_wWZY6Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5Avw4iY6Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDTXyW8cmS9M",
        "colab_type": "text"
      },
      "source": [
        "## Duel Curiosity Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNfynT0bY6L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "\n",
        "df= df_full.copy()\n",
        "name = 'Duel Curiosity Q-learning agent'\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    LEARNING_RATE = 0.003\n",
        "    BATCH_SIZE = 32\n",
        "    LAYER_SIZE = 500\n",
        "    OUTPUT_SIZE = 3\n",
        "    EPSILON = 0.5\n",
        "    DECAY_RATE = 0.005\n",
        "    MIN_EPSILON = 0.1\n",
        "    GAMMA = 0.99\n",
        "    MEMORIES = deque()\n",
        "    COPY = 1000\n",
        "    T_COPY = 0\n",
        "    MEMORY_SIZE = 300\n",
        "    \n",
        "    def __init__(self, state_size, window_size, trend, skip):\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        tf.reset_default_graph()\n",
        "        self.X = tf.placeholder(tf.float32, (None, self.state_size))\n",
        "        self.Y = tf.placeholder(tf.float32, (None, self.state_size))\n",
        "        self.ACTION = tf.placeholder(tf.float32, (None))\n",
        "        self.REWARD = tf.placeholder(tf.float32, (None))\n",
        "        self.batch_size = tf.shape(self.ACTION)[0]\n",
        "        \n",
        "        with tf.variable_scope('curiosity_model'):\n",
        "            action = tf.reshape(self.ACTION, (-1,1))\n",
        "            state_action = tf.concat([self.X, action], axis=1)\n",
        "            save_state = tf.identity(self.Y)\n",
        "            \n",
        "            feed = tf.layers.dense(state_action, 32, activation=tf.nn.relu)\n",
        "            self.curiosity_logits = tf.layers.dense(feed, self.state_size)\n",
        "            self.curiosity_cost = tf.reduce_sum(tf.square(save_state - self.curiosity_logits), axis=1)\n",
        "            \n",
        "            self.curiosity_optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE)\\\n",
        "            .minimize(tf.reduce_mean(self.curiosity_cost))\n",
        "        \n",
        "        total_reward = tf.add(self.curiosity_cost, self.REWARD)\n",
        "        \n",
        "        with tf.variable_scope(\"q_model\"):\n",
        "            with tf.variable_scope(\"eval_net\"):\n",
        "                x_action = tf.layers.dense(self.X, 128, tf.nn.relu)\n",
        "                tensor_action, tensor_validation = tf.split(x_action,2,1)\n",
        "                feed_action = tf.layers.dense(tensor_action, self.OUTPUT_SIZE)\n",
        "                feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "                self.logits = feed_validation + \\\n",
        "                tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            \n",
        "            with tf.variable_scope(\"target_net\"):\n",
        "                y_action = tf.layers.dense(self.Y, 128, tf.nn.relu)\n",
        "                tensor_action, tensor_validation = tf.split(y_action,2,1)\n",
        "                feed_action = tf.layers.dense(tensor_action, self.OUTPUT_SIZE)\n",
        "                feed_validation = tf.layers.dense(tensor_validation, 1)\n",
        "                y_q = feed_validation + \\\n",
        "                tf.subtract(feed_action,tf.reduce_mean(feed_action,axis=1,keep_dims=True))\n",
        "            \n",
        "            q_target = total_reward + self.GAMMA * tf.reduce_max(y_q, axis=1)\n",
        "            action = tf.cast(self.ACTION, tf.int32)\n",
        "            action_indices = tf.stack([tf.range(self.batch_size, dtype=tf.int32), action], axis=1)\n",
        "            q = tf.gather_nd(params=self.logits, indices=action_indices)\n",
        "            self.cost = tf.losses.mean_squared_error(labels=q_target, predictions=q)\n",
        "            self.optimizer = tf.train.RMSPropOptimizer(self.LEARNING_RATE).minimize(\n",
        "            self.cost, var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"q_model/eval_net\"))\n",
        "            \n",
        "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/target_net')\n",
        "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_model/eval_net')\n",
        "        self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
        "        \n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    def _memorize(self, state, action, reward, new_state, done):\n",
        "        self.MEMORIES.append((state, action, reward, new_state, done))\n",
        "        if len(self.MEMORIES) > self.MEMORY_SIZE:\n",
        "            self.MEMORIES.popleft()\n",
        "            \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array(res)\n",
        "    \n",
        "    def predict(self, inputs):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X:inputs})\n",
        "    \n",
        "    def get_predicted_action(self, sequence):\n",
        "        prediction = self.predict(np.array(sequence))[0]\n",
        "        return np.argmax(prediction)\n",
        "    \n",
        "    def _select_action(self, state):\n",
        "        if np.random.rand() < self.EPSILON:\n",
        "            action = np.random.randint(self.OUTPUT_SIZE)\n",
        "        else:\n",
        "            action = self.get_predicted_action([state])\n",
        "        return action\n",
        "    \n",
        "    def _construct_memories(self, replay):\n",
        "        states = np.array([a[0] for a in replay])\n",
        "        actions = np.array([a[1] for a in replay])\n",
        "        rewards = np.array([a[2] for a in replay])\n",
        "        new_states = np.array([a[3] for a in replay])\n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.target_replace_op)\n",
        "            \n",
        "        cost, _ = self.sess.run([self.cost, self.optimizer], feed_dict = {\n",
        "            self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
        "        })\n",
        "        \n",
        "        if (self.T_COPY + 1) % self.COPY == 0:\n",
        "            self.sess.run(self.curiosity_optimizer, feed_dict = {\n",
        "                self.X: states, self.Y: new_states, self.ACTION: actions, self.REWARD: rewards\n",
        "            })\n",
        "        return cost\n",
        "    \n",
        "    def buy(self, initial_money):\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        state = self.get_state(0)\n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self._select_action(state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and initial_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((close[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, close[t], invest, initial_money)\n",
        "                )\n",
        "            \n",
        "            state = next_state\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "        \n",
        "    def train(self, iterations, checkpoint, initial_money):\n",
        "        for i in range(iterations):\n",
        "            total_profit = 0\n",
        "            inventory = []\n",
        "            state = self.get_state(0)\n",
        "            starting_money = initial_money\n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                \n",
        "                action = self._select_action(state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "                \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "                \n",
        "                elif action == 2 and len(inventory) > 0:\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    total_profit += self.trend[t] - bought_price\n",
        "                    starting_money += self.trend[t]\n",
        "                    \n",
        "                invest = ((starting_money - initial_money) / initial_money)\n",
        "                \n",
        "                self._memorize(state, action, invest, next_state, starting_money < initial_money)\n",
        "                batch_size = min(len(self.MEMORIES), self.BATCH_SIZE)\n",
        "                state = next_state\n",
        "                replay = random.sample(self.MEMORIES, batch_size)\n",
        "                cost = self._construct_memories(replay)\n",
        "                self.T_COPY += 1\n",
        "                self.EPSILON = self.MIN_EPSILON + (1.0 - self.MIN_EPSILON) * np.exp(-self.DECAY_RATE * i)\n",
        "            if (i+1) % checkpoint == 0:\n",
        "                print('epoch: %d, total rewards: %f.3, cost: %f, total money: %f'%(i + 1, total_profit, cost,\n",
        "                                                                                  starting_money))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keRxraIrY6L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "batch_size = 32\n",
        "agent = Agent(state_size = window_size, \n",
        "              window_size = window_size, \n",
        "              trend = close, \n",
        "              skip = skip)\n",
        "agent.train(iterations = 200, checkpoint = 10, initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYeD1rqjY6MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = agent.buy(initial_money = initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVy603RmY6MN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBq6lbZLmlxa",
        "colab_type": "text"
      },
      "source": [
        "## Neuro-evolution agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7HdJ5ThY6Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Neuro-evolution agent'\n",
        "\n",
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I6zR61AY6Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class neuralnetwork:\n",
        "    def __init__(self, id_, hidden_size = 128):\n",
        "        self.W1 = np.random.randn(window_size, hidden_size) / np.sqrt(window_size)\n",
        "        self.W2 = np.random.randn(hidden_size, 3) / np.sqrt(hidden_size)\n",
        "        self.fitness = 0\n",
        "        self.id = id_\n",
        "\n",
        "def relu(X):\n",
        "    return np.maximum(X, 0)\n",
        "    \n",
        "def softmax(X):\n",
        "    e_x = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def feed_forward(X, nets):\n",
        "    a1 = np.dot(X, nets.W1)\n",
        "    z1 = relu(a1)\n",
        "    a2 = np.dot(z1, nets.W2)\n",
        "    return softmax(a2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDgsg4tYY6Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuroEvolution:\n",
        "    def __init__(self, population_size, mutation_rate, model_generator,\n",
        "                state_size, window_size, trend, skip, initial_money):\n",
        "        self.population_size = population_size\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.model_generator = model_generator\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.initial_money = initial_money\n",
        "        \n",
        "    def _initialize_population(self):\n",
        "        self.population = []\n",
        "        for i in range(self.population_size):\n",
        "            self.population.append(self.model_generator(i))\n",
        "    \n",
        "    def mutate(self, individual, scale=1.0):\n",
        "        mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=individual.W1.shape)\n",
        "        individual.W1 += np.random.normal(loc=0, scale=scale, size=individual.W1.shape) * mutation_mask\n",
        "        mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=individual.W2.shape)\n",
        "        individual.W2 += np.random.normal(loc=0, scale=scale, size=individual.W2.shape) * mutation_mask\n",
        "        return individual\n",
        "    \n",
        "    def inherit_weights(self, parent, child):\n",
        "        child.W1 = parent.W1.copy()\n",
        "        child.W2 = parent.W2.copy()\n",
        "        return child\n",
        "    \n",
        "    def crossover(self, parent1, parent2):\n",
        "        child1 = self.model_generator((parent1.id+1)*10)\n",
        "        child1 = self.inherit_weights(parent1, child1)\n",
        "        child2 = self.model_generator((parent2.id+1)*10)\n",
        "        child2 = self.inherit_weights(parent2, child2)\n",
        "        # first W\n",
        "        n_neurons = child1.W1.shape[1]\n",
        "        cutoff = np.random.randint(0, n_neurons)\n",
        "        child1.W1[:, cutoff:] = parent2.W1[:, cutoff:].copy()\n",
        "        child2.W1[:, cutoff:] = parent1.W1[:, cutoff:].copy()\n",
        "        # second W\n",
        "        n_neurons = child1.W2.shape[1]\n",
        "        cutoff = np.random.randint(0, n_neurons)\n",
        "        child1.W2[:, cutoff:] = parent2.W2[:, cutoff:].copy()\n",
        "        child2.W2[:, cutoff:] = parent1.W2[:, cutoff:].copy()\n",
        "        return child1, child2\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "    \n",
        "    def act(self, p, state):\n",
        "        logits = feed_forward(state, p)\n",
        "        return np.argmax(logits, 1)[0]\n",
        "    \n",
        "    def buy(self, individual):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        state = self.get_state(0)\n",
        "        inventory = []\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        \n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(individual, state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and starting_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((self.trend[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, self.trend[t], invest, initial_money)\n",
        "                )\n",
        "            state = next_state\n",
        "        \n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def calculate_fitness(self):\n",
        "        for i in range(self.population_size):\n",
        "            initial_money = self.initial_money\n",
        "            starting_money = initial_money\n",
        "            state = self.get_state(0)\n",
        "            inventory = []\n",
        "            \n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                action = self.act(self.population[i], state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "            \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "\n",
        "                elif action == 2 and len(inventory):\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    starting_money += self.trend[t]\n",
        "\n",
        "                state = next_state\n",
        "            invest = ((starting_money - initial_money) / initial_money) * 100\n",
        "            self.population[i].fitness = invest\n",
        "    \n",
        "    def evolve(self, generations=20, checkpoint= 5):\n",
        "        self._initialize_population()\n",
        "        n_winners = int(self.population_size * 0.4)\n",
        "        n_parents = self.population_size - n_winners\n",
        "        for epoch in range(generations):\n",
        "            self.calculate_fitness()\n",
        "            fitnesses = [i.fitness for i in self.population]\n",
        "            sort_fitness = np.argsort(fitnesses)[::-1]\n",
        "            self.population = [self.population[i] for i in sort_fitness]\n",
        "            fittest_individual = self.population[0]\n",
        "            if (epoch+1) % checkpoint == 0:\n",
        "                print('epoch %d, fittest individual %d with accuracy %f'%(epoch+1, sort_fitness[0], \n",
        "                                                                          fittest_individual.fitness))\n",
        "            next_population = [self.population[i] for i in range(n_winners)]\n",
        "            total_fitness = np.sum([np.abs(i.fitness) for i in self.population])\n",
        "            parent_probabilities = [np.abs(i.fitness / total_fitness) for i in self.population]\n",
        "            parents = np.random.choice(self.population, size=n_parents, p=parent_probabilities, replace=False)\n",
        "            for i in np.arange(0, len(parents), 2):\n",
        "                child1, child2 = self.crossover(parents[i], parents[i+1])\n",
        "                next_population += [self.mutate(child1), self.mutate(child2)]\n",
        "            self.population = next_population\n",
        "        return fittest_individual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjahpzNY6My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "population_size = 100\n",
        "generations = 100\n",
        "mutation_rate = 0.1\n",
        "neural_evolve = NeuroEvolution(population_size, mutation_rate, neuralnetwork,\n",
        "                              window_size, window_size, close, skip, initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jVUEsy7Y6M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fittest_nets = neural_evolve.evolve(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ttTs-LY6M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = neural_evolve.buy(fittest_nets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSDO42dEY6NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra90QUK5mupJ",
        "colab_type": "text"
      },
      "source": [
        "## Neuro-evolution with Novelty search agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Sbn-74Y6NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Neuro-evolution with Novelty search agent'\n",
        "\n",
        "close = df.Close.values.tolist()\n",
        "initial_money = 10000\n",
        "window_size = 30\n",
        "skip = 1\n",
        "\n",
        "novelty_search_threshold = 6\n",
        "novelty_log_maxlen = 1000\n",
        "backlog_maxsize = 500\n",
        "novelty_log_add_amount = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRfPlPSAY6NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class neuralnetwork:\n",
        "    def __init__(self, id_, hidden_size = 128):\n",
        "        self.W1 = np.random.randn(window_size, hidden_size) / np.sqrt(window_size)\n",
        "        self.W2 = np.random.randn(hidden_size, 3) / np.sqrt(hidden_size)\n",
        "        self.fitness = 0\n",
        "        self.last_features = None\n",
        "        self.id = id_\n",
        "\n",
        "def relu(X):\n",
        "    return np.maximum(X, 0)\n",
        "    \n",
        "def softmax(X):\n",
        "    e_x = np.exp(X - np.max(X, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def feed_forward(X, nets):\n",
        "    a1 = np.dot(X, nets.W1)\n",
        "    z1 = relu(a1)\n",
        "    a2 = np.dot(z1, nets.W2)\n",
        "    return softmax(a2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po4cAOGfY6NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuroEvolution:\n",
        "    def __init__(self, population_size, mutation_rate, model_generator,\n",
        "                state_size, window_size, trend, skip, initial_money):\n",
        "        self.population_size = population_size\n",
        "        self.mutation_rate = mutation_rate\n",
        "        self.model_generator = model_generator\n",
        "        self.state_size = state_size\n",
        "        self.window_size = window_size\n",
        "        self.half_window = window_size // 2\n",
        "        self.trend = trend\n",
        "        self.skip = skip\n",
        "        self.initial_money = initial_money\n",
        "        self.generation_backlog = []\n",
        "        self.novel_backlog = []\n",
        "        self.novel_pop = []\n",
        "        \n",
        "    def _initialize_population(self):\n",
        "        self.population = []\n",
        "        for i in range(self.population_size):\n",
        "            self.population.append(self.model_generator(i))\n",
        "    \n",
        "    def _memorize(self, q, i, limit):\n",
        "        q.append(i)\n",
        "        if len(q) > limit:\n",
        "            q.pop()\n",
        "    \n",
        "    def mutate(self, individual, scale=1.0):\n",
        "        mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=individual.W1.shape)\n",
        "        individual.W1 += np.random.normal(loc=0, scale=scale, size=individual.W1.shape) * mutation_mask\n",
        "        mutation_mask = np.random.binomial(1, p=self.mutation_rate, size=individual.W2.shape)\n",
        "        individual.W2 += np.random.normal(loc=0, scale=scale, size=individual.W2.shape) * mutation_mask\n",
        "        return individual\n",
        "    \n",
        "    def inherit_weights(self, parent, child):\n",
        "        child.W1 = parent.W1.copy()\n",
        "        child.W2 = parent.W2.copy()\n",
        "        return child\n",
        "    \n",
        "    def crossover(self, parent1, parent2):\n",
        "        child1 = self.model_generator((parent1.id+1)*10)\n",
        "        child1 = self.inherit_weights(parent1, child1)\n",
        "        child2 = self.model_generator((parent2.id+1)*10)\n",
        "        child2 = self.inherit_weights(parent2, child2)\n",
        "        # first W\n",
        "        n_neurons = child1.W1.shape[1]\n",
        "        cutoff = np.random.randint(0, n_neurons)\n",
        "        child1.W1[:, cutoff:] = parent2.W1[:, cutoff:].copy()\n",
        "        child2.W1[:, cutoff:] = parent1.W1[:, cutoff:].copy()\n",
        "        # second W\n",
        "        n_neurons = child1.W2.shape[1]\n",
        "        cutoff = np.random.randint(0, n_neurons)\n",
        "        child1.W2[:, cutoff:] = parent2.W2[:, cutoff:].copy()\n",
        "        child2.W2[:, cutoff:] = parent1.W2[:, cutoff:].copy()\n",
        "        return child1, child2\n",
        "    \n",
        "    def get_state(self, t):\n",
        "        window_size = self.window_size + 1\n",
        "        d = t - window_size + 1\n",
        "        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n",
        "        res = []\n",
        "        for i in range(window_size - 1):\n",
        "            res.append(block[i + 1] - block[i])\n",
        "        return np.array([res])\n",
        "    \n",
        "    def act(self, p, state):\n",
        "        logits = feed_forward(state, p)\n",
        "        return np.argmax(logits, 1)[0]\n",
        "    \n",
        "    def buy(self, individual):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        state = self.get_state(0)\n",
        "        inventory = []\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        \n",
        "        for t in range(0, len(self.trend) - 1, self.skip):\n",
        "            action = self.act(individual, state)\n",
        "            next_state = self.get_state(t + 1)\n",
        "            \n",
        "            if action == 1 and starting_money >= self.trend[t]:\n",
        "                inventory.append(self.trend[t])\n",
        "                initial_money -= self.trend[t]\n",
        "                states_buy.append(t)\n",
        "                print('day %d: buy 1 unit at price %f, total balance %f'% (t, self.trend[t], initial_money))\n",
        "            \n",
        "            elif action == 2 and len(inventory):\n",
        "                bought_price = inventory.pop(0)\n",
        "                initial_money += self.trend[t]\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((self.trend[t] - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell 1 unit at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, self.trend[t], invest, initial_money)\n",
        "                )\n",
        "            state = next_state\n",
        "        \n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        total_gains = initial_money - starting_money\n",
        "        return states_buy, states_sell, total_gains, invest\n",
        "    \n",
        "    def calculate_fitness(self):\n",
        "        for i in range(self.population_size):\n",
        "            initial_money = self.initial_money\n",
        "            starting_money = initial_money\n",
        "            state = self.get_state(0)\n",
        "            inventory = []\n",
        "            \n",
        "            for t in range(0, len(self.trend) - 1, self.skip):\n",
        "                action = self.act(self.population[i], state)\n",
        "                next_state = self.get_state(t + 1)\n",
        "            \n",
        "                if action == 1 and starting_money >= self.trend[t]:\n",
        "                    inventory.append(self.trend[t])\n",
        "                    starting_money -= self.trend[t]\n",
        "\n",
        "                elif action == 2 and len(inventory):\n",
        "                    bought_price = inventory.pop(0)\n",
        "                    starting_money += self.trend[t]\n",
        "\n",
        "                state = next_state\n",
        "            invest = ((starting_money - initial_money) / initial_money) * 100\n",
        "            self.population[i].fitness = invest\n",
        "            self.population[i].last_features = self.population[i].W2.flatten()\n",
        "    \n",
        "    def evaluate(self, individual, backlog, pop, k = 4):\n",
        "        score = 0\n",
        "        if len(backlog):\n",
        "            x = np.array(backlog)\n",
        "            nn = NearestNeighbors(n_neighbors = k, metric = 'euclidean').fit(np.array(backlog))\n",
        "            d, _ = nn.kneighbors([individual])\n",
        "            score += np.mean(d)\n",
        "        \n",
        "        if len(pop):\n",
        "            nn = NearestNeighbors(n_neighbors = k, metric = 'euclidean').fit(np.array(pop))\n",
        "            d, _ = nn.kneighbors([individual])\n",
        "            score += np.mean(d)\n",
        "        \n",
        "        return score\n",
        "    \n",
        "    def evolve(self, generations=20, checkpoint= 5):\n",
        "        self._initialize_population()\n",
        "        n_winners = int(self.population_size * 0.4)\n",
        "        n_parents = self.population_size - n_winners\n",
        "        for epoch in range(generations):\n",
        "            self.calculate_fitness()\n",
        "            scores = [self.evaluate(p.last_features, self.novel_backlog, self.novel_pop) for p in self.population]\n",
        "            sort_fitness = np.argsort(scores)[::-1]\n",
        "            self.population = [self.population[i] for i in sort_fitness]\n",
        "            fittest_individual = self.population[0]\n",
        "            if (epoch+1) % checkpoint == 0:\n",
        "                print('epoch %d, fittest individual %d with accuracy %f'%(epoch+1, sort_fitness[0], \n",
        "                                                                          fittest_individual.fitness))\n",
        "            next_population = [self.population[i] for i in range(n_winners)]\n",
        "            total_fitness = np.sum([np.abs(i.fitness) for i in self.population])\n",
        "            parent_probabilities = [np.abs(i.fitness / total_fitness) for i in self.population]\n",
        "            parents = np.random.choice(self.population, size=n_parents, p=parent_probabilities, replace=False)\n",
        "            \n",
        "            for p in next_population:\n",
        "                if p.last_features is not None:\n",
        "                    self._memorize(self.novel_pop, p.last_features, backlog_maxsize)\n",
        "                    if np.random.randint(0,10) < novelty_search_threshold:\n",
        "                        self._memorize(self.novel_backlog, p.last_features, novelty_log_maxlen)\n",
        "                        \n",
        "            for i in np.arange(0, len(parents), 2):\n",
        "                child1, child2 = self.crossover(parents[i], parents[i+1])\n",
        "                next_population += [self.mutate(child1), self.mutate(child2)]\n",
        "            self.population = next_population\n",
        "            \n",
        "            if np.random.randint(0,10) < novelty_search_threshold:\n",
        "                pop_sorted = sorted(self.population, key=lambda p: p.fitness, reverse=True)\n",
        "                self.generation_backlog.append(pop_sorted[0])\n",
        "                print('novel add fittest, score: %f, backlog size: %d'%(pop_sorted[0].fitness, \n",
        "                                                                        len(self.generation_backlog)))\n",
        "                generation_backlog_temp = self.generation_backlog\n",
        "                if len(self.generation_backlog) > backlog_maxsize:\n",
        "                    generation_backlog_temp = random.sample(generation_backlog, backlog_maxsize)\n",
        "                for p in generation_backlog_temp:\n",
        "                    if p.last_features is not None:\n",
        "                        self._memorize(self.novel_backlog, p.last_features, novelty_log_maxlen)\n",
        "                        \n",
        "        return fittest_individual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dn0_YeOY6Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "population_size = 100\n",
        "generations = 100\n",
        "mutation_rate = 0.1\n",
        "neural_evolve = NeuroEvolution(population_size, mutation_rate, neuralnetwork,\n",
        "                              window_size, window_size, close, skip, initial_money)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGmI3IKcY6Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fittest_nets = neural_evolve.evolve(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKUdAfxpY6Nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest = neural_evolve.buy(fittest_nets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpZ5TannY6N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6xxeNRm9QZ",
        "colab_type": "text"
      },
      "source": [
        "## ABCD strategy agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnsxrP9tY6OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'ABCD strategy agent'\n",
        "\n",
        "def abcd(trend, skip_loop = 4, ma = 7):\n",
        "    ma = pd.Series(trend).rolling(ma).mean().values\n",
        "    x = []\n",
        "    for a in range(ma.shape[0]):\n",
        "        for b in range(a, ma.shape[0], skip_loop):\n",
        "            for c in range(b, ma.shape[0], skip_loop):\n",
        "                for d in range(c, ma.shape[0], skip_loop):\n",
        "                    if ma[b] > ma[a] and \\\n",
        "                    (ma[c] < ma[b] and ma[c] > ma[a]) \\\n",
        "                    and ma[d] > ma[b]:\n",
        "                        x.append([a,b,c,d])\n",
        "    x_np = np.array(x)\n",
        "    ac = x_np[:,0].tolist() + x_np[:,2].tolist()\n",
        "    bd = x_np[:,1].tolist() + x_np[:,3].tolist()\n",
        "    ac_set = set(ac)\n",
        "    bd_set = set(bd)\n",
        "    signal = np.zeros(len(trend))\n",
        "    buy = list(ac_set - bd_set)\n",
        "    sell = list(list(bd_set - ac_set))\n",
        "    signal[buy] = 1.0\n",
        "    signal[sell] = -1.0\n",
        "    return signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZL7mRMfY6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "signal = abcd(df['Close'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "847wzmd9Y6OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buy_stock(\n",
        "    real_movement,\n",
        "    signal,\n",
        "    initial_money = 10000,\n",
        "    max_buy = 1,\n",
        "    max_sell = 1,\n",
        "):\n",
        "    \"\"\"\n",
        "    real_movement = actual movement in the real world\n",
        "    delay = how much interval you want to delay to change our decision from buy to sell, vice versa\n",
        "    initial_state = 1 is buy, 0 is sell\n",
        "    initial_money = 10000, ignore what kind of currency\n",
        "    max_buy = max quantity for share to buy\n",
        "    max_sell = max quantity for share to sell\n",
        "    \"\"\"\n",
        "    starting_money = initial_money\n",
        "    states_sell = []\n",
        "    states_buy = []\n",
        "    states_money = []\n",
        "    current_inventory = 0\n",
        "    \n",
        "    def buy(i, initial_money, current_inventory):\n",
        "        shares = initial_money // real_movement[i]\n",
        "        if shares < 1:\n",
        "            print(\n",
        "                'day %d: total balances %f, not enough money to buy a unit price %f'\n",
        "                % (i, initial_money, real_movement[i])\n",
        "            )\n",
        "        else:\n",
        "            if shares > max_buy:\n",
        "                buy_units = max_buy\n",
        "            else:\n",
        "                buy_units = shares\n",
        "            initial_money -= buy_units * real_movement[i]\n",
        "            current_inventory += buy_units\n",
        "            print(\n",
        "                'day %d: buy %d units at price %f, total balance %f'\n",
        "                % (i, buy_units, buy_units * real_movement[i], initial_money)\n",
        "            )\n",
        "            states_buy.append(0)\n",
        "        return initial_money, current_inventory\n",
        "    \n",
        "    for i in range(real_movement.shape[0]):\n",
        "        state = signal[i]\n",
        "        if state == 1:\n",
        "            initial_money, current_inventory = buy(\n",
        "                i, initial_money, current_inventory\n",
        "            )\n",
        "            states_buy.append(i)\n",
        "        elif state == -1:\n",
        "            if current_inventory == 0:\n",
        "                    print('day %d: cannot sell anything, inventory 0' % (i))\n",
        "            else:\n",
        "                if current_inventory > max_sell:\n",
        "                    sell_units = max_sell\n",
        "                else:\n",
        "                    sell_units = current_inventory\n",
        "                current_inventory -= sell_units\n",
        "                total_sell = sell_units * real_movement[i]\n",
        "                initial_money += total_sell\n",
        "                try:\n",
        "                    invest = (\n",
        "                        (real_movement[i] - real_movement[states_buy[-1]])\n",
        "                        / real_movement[states_buy[-1]]\n",
        "                    ) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell %d units at price %f, investment %f %%, total balance %f,'\n",
        "                    % (i, sell_units, total_sell, invest, initial_money)\n",
        "                )\n",
        "            states_sell.append(i)\n",
        "        states_money.append(initial_money)\n",
        "            \n",
        "    invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "    total_gains = initial_money - starting_money\n",
        "    return states_buy, states_sell, total_gains, invest, states_money"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhNWyPhvY6OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_buy, states_sell, total_gains, invest, states_money = buy_stock(df.Close, signal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBM2FT84Y6OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df['Close']\n",
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(close, color='r', lw=2.)\n",
        "plt.plot(close, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(close, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.title('total gains %f, total investment %f%%'%(total_gains, invest))\n",
        "plt.legend()\n",
        "plt.savefig('output/'+name+'.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Go4ElExY6OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (15,5))\n",
        "plt.plot(states_money, color='r', lw=2.)\n",
        "plt.plot(states_money, '^', markersize=10, color='m', label = 'buying signal', markevery = states_buy)\n",
        "plt.plot(states_money, 'v', markersize=10, color='k', label = 'selling signal', markevery = states_sell)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUZcLPJAnEAT",
        "colab_type": "text"
      },
      "source": [
        "## Deep Evolution Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "I1zgebD9Y6Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "bins = np.linspace(-10, 10, 100)\n",
        "\n",
        "solution = np.random.randn(100)\n",
        "w = np.random.randn(100)\n",
        "\n",
        "plt.hist(solution, bins, alpha = 0.5, label = 'solution', color = 'r')\n",
        "plt.hist(w, bins, alpha = 0.5, label = 'random', color = 'y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "FQBVBghEY6Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(w):\n",
        "    return -np.sum(np.square(solution - w))\n",
        "\n",
        "\n",
        "npop = 50\n",
        "sigma = 0.1\n",
        "alpha = 0.001\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(\n",
        "            'iter %d. w: %s, solution: %s, reward: %f'\n",
        "            % (i + 1, str(w[-1]), str(solution[-1]), f(w))\n",
        "        )\n",
        "    N = np.random.randn(npop, 100)\n",
        "    R = np.zeros(npop)\n",
        "    for j in range(npop):\n",
        "        w_try = w + sigma * N[j]\n",
        "        R[j] = f(w_try)\n",
        "\n",
        "    A = (R - np.mean(R)) / np.std(R)\n",
        "    w = w + alpha / (npop * sigma) * np.dot(N.T, A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "ibvV_xz3Y6On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "I want to compare my first two individuals with my real w\n",
        "'''\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "sigma = 0.1\n",
        "N = np.random.randn(npop, 100)\n",
        "individuals = []\n",
        "for j in range(2):\n",
        "    individuals.append(w + sigma * N[j])\n",
        "    \n",
        "    \n",
        "plt.hist(w, bins, alpha=0.5, label='w',color='r')\n",
        "plt.hist(individuals[0], bins, alpha=0.5, label='individual 1')\n",
        "plt.hist(individuals[1], bins, alpha=0.5, label='individual 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "Hnsc0zkZY6Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df= df_full.copy()\n",
        "name = 'Deep Evolution Strategy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "xJW7MsVNY6Ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_state(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d : t + 1] if d >= 0 else -d * [data[0]] + data[: t + 1]\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(block[i + 1] - block[i])\n",
        "    return np.array([res])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "aE-aZllEY6Ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close = df.Close.values.tolist()\n",
        "get_state(close, 0, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "JzHbWexVY6Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_state(close, 1, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "e-6UXQhcY6O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_state(close, 2, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "Ki7JATTAY6O8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep_Evolution_Strategy:\n",
        "    def __init__(\n",
        "        self, weights, reward_function, population_size, sigma, learning_rate\n",
        "    ):\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_function\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _get_weight_from_population(self, weights, population):\n",
        "        weights_population = []\n",
        "        for index, i in enumerate(population):\n",
        "            jittered = self.sigma * i\n",
        "            weights_population.append(weights[index] + jittered)\n",
        "        return weights_population\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def train(self, epoch = 100, print_every = 1):\n",
        "        lasttime = time.time()\n",
        "        for i in range(epoch):\n",
        "            population = []\n",
        "            rewards = np.zeros(self.population_size)\n",
        "            for k in range(self.population_size):\n",
        "                x = []\n",
        "                for w in self.weights:\n",
        "                    x.append(np.random.randn(*w.shape))\n",
        "                population.append(x)\n",
        "            for k in range(self.population_size):\n",
        "                weights_population = self._get_weight_from_population(\n",
        "                    self.weights, population[k]\n",
        "                )\n",
        "                rewards[k] = self.reward_function(weights_population)\n",
        "            rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
        "            for index, w in enumerate(self.weights):\n",
        "                A = np.array([p[index] for p in population])\n",
        "                self.weights[index] = (\n",
        "                    w\n",
        "                    + self.learning_rate\n",
        "                    / (self.population_size * self.sigma)\n",
        "                    * np.dot(A.T, rewards).T\n",
        "                )\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\n",
        "                    'iter %d. reward: %f'\n",
        "                    % (i + 1, self.reward_function(self.weights))\n",
        "                )\n",
        "        print('time taken to train:', time.time() - lasttime, 'seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "CsVkkgQ4Y6O9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, input_size, layer_size, output_size):\n",
        "        self.weights = [\n",
        "            np.random.randn(input_size, layer_size),\n",
        "            np.random.randn(layer_size, output_size),\n",
        "            np.random.randn(layer_size, 1),\n",
        "            np.random.randn(1, layer_size),\n",
        "        ]\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\n",
        "        decision = np.dot(feed, self.weights[1])\n",
        "        buy = np.dot(feed, self.weights[2])\n",
        "        return decision, buy\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "VOVmYYdzY6O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 30\n",
        "model = Model(window_size, 500, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "gMd6-sNiY6PB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_money = 10000\n",
        "starting_money = initial_money\n",
        "len_close = len(close) - 1\n",
        "weight = model\n",
        "skip = 1\n",
        "\n",
        "state = get_state(close, 0, window_size + 1)\n",
        "inventory = []\n",
        "quantity = 0\n",
        "\n",
        "max_buy = 5\n",
        "max_sell = 5\n",
        "\n",
        "\n",
        "def act(model, sequence):\n",
        "    decision, buy = model.predict(np.array(sequence))\n",
        "    return np.argmax(decision[0]), int(buy[0])\n",
        "\n",
        "\n",
        "for t in range(0, len_close, skip):\n",
        "    action, buy = act(weight, state)\n",
        "    next_state = get_state(close, t + 1, window_size + 1)\n",
        "    if action == 1 and initial_money >= close[t]:\n",
        "        if buy < 0:\n",
        "            buy = 1\n",
        "        if buy > max_buy:\n",
        "            buy_units = max_buy\n",
        "        else:\n",
        "            buy_units = buy\n",
        "        total_buy = buy_units * close[t]\n",
        "        initial_money -= total_buy\n",
        "        inventory.append(total_buy)\n",
        "        quantity += buy_units\n",
        "    elif action == 2 and len(inventory) > 0:\n",
        "        if quantity > max_sell:\n",
        "            sell_units = max_sell\n",
        "        else:\n",
        "            sell_units = quantity\n",
        "        quantity -= sell_units\n",
        "        total_sell = sell_units * close[t]\n",
        "        initial_money += total_sell\n",
        "\n",
        "    state = next_state\n",
        "((initial_money - starting_money) / starting_money) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "nxi54GUrY6PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    POPULATION_SIZE = 15\n",
        "    SIGMA = 0.1\n",
        "    LEARNING_RATE = 0.03\n",
        "\n",
        "    def __init__(\n",
        "        self, model, money, max_buy, max_sell, close, window_size, skip\n",
        "    ):\n",
        "        self.window_size = window_size\n",
        "        self.skip = skip\n",
        "        self.close = close\n",
        "        self.model = model\n",
        "        self.initial_money = money\n",
        "        self.max_buy = max_buy\n",
        "        self.max_sell = max_sell\n",
        "        self.es = Deep_Evolution_Strategy(\n",
        "            self.model.get_weights(),\n",
        "            self.get_reward,\n",
        "            self.POPULATION_SIZE,\n",
        "            self.SIGMA,\n",
        "            self.LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "    def act(self, sequence):\n",
        "        decision, buy = self.model.predict(np.array(sequence))\n",
        "        return np.argmax(decision[0]), int(buy[0])\n",
        "\n",
        "    def get_reward(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        len_close = len(self.close) - 1\n",
        "\n",
        "        self.model.weights = weights\n",
        "        state = get_state(self.close, 0, self.window_size + 1)\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        for t in range(0, len_close, self.skip):\n",
        "            action, buy = self.act(state)\n",
        "            next_state = get_state(self.close, t + 1, self.window_size + 1)\n",
        "            if action == 1 and initial_money >= self.close[t]:\n",
        "                if buy < 0:\n",
        "                    buy = 1\n",
        "                if buy > self.max_buy:\n",
        "                    buy_units = self.max_buy\n",
        "                else:\n",
        "                    buy_units = buy\n",
        "                total_buy = buy_units * self.close[t]\n",
        "                initial_money -= total_buy\n",
        "                inventory.append(total_buy)\n",
        "                quantity += buy_units\n",
        "            elif action == 2 and len(inventory) > 0:\n",
        "                if quantity > self.max_sell:\n",
        "                    sell_units = self.max_sell\n",
        "                else:\n",
        "                    sell_units = quantity\n",
        "                quantity -= sell_units\n",
        "                total_sell = sell_units * self.close[t]\n",
        "                initial_money += total_sell\n",
        "\n",
        "            state = next_state\n",
        "        return ((initial_money - starting_money) / starting_money) * 100\n",
        "\n",
        "    def fit(self, iterations, checkpoint):\n",
        "        self.es.train(iterations, print_every = checkpoint)\n",
        "\n",
        "    def buy(self):\n",
        "        initial_money = self.initial_money\n",
        "        len_close = len(self.close) - 1\n",
        "        state = get_state(self.close, 0, self.window_size + 1)\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        for t in range(0, len_close, self.skip):\n",
        "            action, buy = self.act(state)\n",
        "            next_state = get_state(self.close, t + 1, self.window_size + 1)\n",
        "            if action == 1 and initial_money >= self.close[t]:\n",
        "                if buy < 0:\n",
        "                    buy = 1\n",
        "                if buy > self.max_buy:\n",
        "                    buy_units = self.max_buy\n",
        "                else:\n",
        "                    buy_units = buy\n",
        "                total_buy = buy_units * self.close[t]\n",
        "                initial_money -= total_buy\n",
        "                inventory.append(total_buy)\n",
        "                quantity += buy_units\n",
        "                states_buy.append(t)\n",
        "                print(\n",
        "                    'day %d: buy %d units at price %f, total balance %f'\n",
        "                    % (t, buy_units, total_buy, initial_money)\n",
        "                )\n",
        "            elif action == 2 and len(inventory) > 0:\n",
        "                bought_price = inventory.pop(0)\n",
        "                if quantity > self.max_sell:\n",
        "                    sell_units = self.max_sell\n",
        "                else:\n",
        "                    sell_units = quantity\n",
        "                if sell_units < 1:\n",
        "                    continue\n",
        "                quantity -= sell_units\n",
        "                total_sell = sell_units * self.close[t]\n",
        "                initial_money += total_sell\n",
        "                states_sell.append(t)\n",
        "                try:\n",
        "                    invest = ((total_sell - bought_price) / bought_price) * 100\n",
        "                except:\n",
        "                    invest = 0\n",
        "                print(\n",
        "                    'day %d, sell %d units at price %f, investment %f %%, total balance %f,'\n",
        "                    % (t, sell_units, total_sell, invest, initial_money)\n",
        "                )\n",
        "            state = next_state\n",
        "\n",
        "        invest = ((initial_money - starting_money) / starting_money) * 100\n",
        "        print(\n",
        "            '\\ntotal gained %f, total investment %f %%'\n",
        "            % (initial_money - starting_money, invest)\n",
        "        )\n",
        "        plt.figure(figsize = (20, 10))\n",
        "        plt.plot(close, label = 'true close', c = 'g')\n",
        "        plt.plot(\n",
        "            close, 'X', label = 'predict buy', markevery = states_buy, c = 'b'\n",
        "        )\n",
        "        plt.plot(\n",
        "            close, 'o', label = 'predict sell', markevery = states_sell, c = 'r'\n",
        "        )\n",
        "        plt.legend()\n",
        "        plt.savefig('output/'+name+'.png')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "us3WXRRiY6PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(input_size = window_size, layer_size = 500, output_size = 3)\n",
        "agent = Agent(\n",
        "    model = model,\n",
        "    money = 10000,\n",
        "    max_buy = 5,\n",
        "    max_sell = 5,\n",
        "    close = close,\n",
        "    window_size = window_size,\n",
        "    skip = 1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "BmU9TixCY6PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.fit(iterations = 500, checkpoint = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "outputHidden": false,
        "id": "JJ-HsHHKY6Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.buy()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}