{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwMZMgLZ9QL6"
   },
   "outputs": [],
   "source": [
    "data={\"intents\":[\n",
    "{\"tag\": \"welcome\",\n",
    "\"patterns\":[\"Hi\",\"Hello\",\"How are you\",\"whats up\", \"How do you do\",\"How’s the day?\",\"Hey are you there\",\"How is everything going\",\"glad to meet you\"],\n",
    "\"responses\":[\"Hello\",\"Glad to see you again\", \"Hi there, How can I help  you\", \"Hello! what you are looking for?\", \"Hello, how may I help you\"],\n",
    "},\n",
    "{\"tag\": \"endingnote\",\n",
    "\"patterns\":[\"see you. bye\",\"Good Bye\",\"ok then bye\",\"That’ s enough for me\",\" Im leaving\",\"See u later! Goodbye\",\"nice to meet you\" , \"The bargaining session was fun!I’m happy that I got it at cheap price, see u again\",\"Enjoyed the experience, good bye\"],\n",
    "\"responses\":[\"Hey, Thank you for visiting\",\"Hope to see you again, it was nice talking with you\", \" wish you a good day! Hope to have a talk with you later\",\"Thanks for your time with us.Hope you enjoyed it and satisfied. See you later\", \"Sad to see you go. Come whenever you need to bye from us. Stay intouch.\"],\n",
    "},\n",
    "{\"tag\": \"name\",\n",
    "\"patterns\":[\"whats you name?\",\"who are you?\",\"what can I call you\",\"tell me your name\",\"Are u a robot\",\"Are you a chatbot\"],\n",
    "\"responses\":[\"It’s FinBot here, a digital+financial assistant to help you out\",\"You can call me FinBot.Im a chatRobot/chatbot, tell me what you want\", \"I’m FinBot,a digital+financial assistant, I can guide you in our ecommerce website.\", \"Its FinBot Here,a digital +financial assistant. How may I help you?\"],\n",
    "},\n",
    "{\"tag\": \"shopping\",\n",
    "\"patterns\":[\"I would like to buy something here\",\"can I buy something\",\"What all things are there\",\"can you recommend me something from here\",\"is there any thing cheaper here\",\"Hey, I want a black shoe\"],\n",
    "\"responses\":[\"Yes please, we have a lot of new collections for you\",\"Ofcourse, there are new offers for you, fantastic selections, choose what interests you!\",\"Well casual shoes at Rs 220, this offer won’t stay much\",\"there are good varieties of coloured shoes too also there are black formal shoes. Take one\"],\n",
    "},\n",
    "{\"tag\": \"time period\",\n",
    "\"patterns\":[\"till what time this be offered?\",\"When will the shop close?\",\"when will the shop open\",\"How much time it will take for delivery\",\"When will this site close?\",\"When are you people open\",\"When will this store be open?\", \"Is this store will be open at Sunday?\"],\n",
    "\"responses\":[\"Our shop centre will be open from 9 am-9pm in all the days\",\"Yes, we are operating on all the weekdays from 9 am to 9pm\", \"We are there to provide services from Monday-sunday from 9 am-9pm\",\"We are operating from 9 am-9 pm every day\"],\n",
    "},\n",
    "{\"tag\": \"location\",\n",
    "\"patterns\": [\"What is your location?\", \"Where are you located?\", \"What is your address?\", \"Where is your restaurant situated?\" ],\n",
    "\"responses\": [\"We are on the intersection of state Alley and Techno Avenue, Delhi, India\", \"We are situated at the intersection of  state Alley and Techno Avenue in Delhi, India\", \"Our Address is: 1000 Techno Avenue, Delhi, India\"],\n",
    "},\n",
    "{\"tag\": \"payments\",\n",
    "\"patterns\": [\"Do you take credit cards?\", \"Do you accept Mastercard?\", \"Are you cash only?\",\"will you accept credit card\", \"can i pay by credit card\" ],\n",
    "\"responses\": [\"We accept VISA, Mastercard and AMEX\", \"We accept most major credit cards\"],\n",
    "},\n",
    "{\"tag\": \"todaysOffers\",\n",
    "\"patterns\": [\"What is the new collections for today?\", \"What new collections you have today?\", \"Tell me today's collection\"],\n",
    "\"responses\": [\"We have buy one get one offer for kurtis and shirts. Also new colour selections of traditional wears\", \"Our speciality for today is trendy new collections of traditional wears and also buy one get one offer for some things\"],\n",
    "},\n",
    "{\"tag\": \"deliveryoption\",\n",
    "\"patterns\": [\"Do you provide delivery to home?\", \"Do you deliver the clothes?\", \"What are the home delivery options?\",\"Tell me the delivery options you have\" ],\n",
    "\"responses\": [\"Yes, we provide home delivery of the proiducts through our agents?\", \"We have home delivery options through our agents\"],\n",
    "},\n",
    "{\"tag\": \"price of formal wear\",\n",
    "\"patterns\":[\"what is the price for a formal wear\",\"How much will it cost to buy a formal wear\",\"Will it be much expensive to buy a branded suit, how much is the average cost\"],\n",
    "\"responses\":[\"For  a formal wear it will cost in a range of 750-2500 Rs according to your selection\",\"There are a number of varieties of suits or formal wears ranging from 500-1500\",\"Its not much expensive to have the same. There are several discounts available\", \"it has an average cost of 1200 Rs\"],\n",
    "},\n",
    "{\"tag\": \"price of a saree\",\n",
    "\"patterns\": [\"what is the price for a saree\",\"How much will it cost to buy a saree\",\"What is the cost of a saree\",\"Cost of a saree\",\"Price of a saree\"],\n",
    "\"responses\": [\"The price of saree depends upon the colour and design of the same \", \"We have collections of trending sarees ranging from 500Rs to 10000 Rs\"],\n",
    "\"context_set\": \"\"\n",
    "},\n",
    "{\"tag\": \"Thank you\",\n",
    "\"patterns\":[\"Thank you\", \"Thank you so much for your help\"],\n",
    "\"responses\":[\"Its my pleasure.\",\"You are welcome\",\"No problem. Its my pleasure to help you\"],\n",
    "},\n",
    "{\"tag\": \"Bargain offer\",\n",
    "\"patterns\":[\"Can i bargain for any product today\",\"For what product do you have bargaining offer today\",\"can you bargain today\",\"can I bargain today\", \"Which product can I bargain today\",\"Tell me is there a product to bargain\"],\n",
    "\"responses\":[\"Yes, we have a bargaining offer for ribbon today. Its rated price is 30 Rs\",\"yes, today you can bargaining for a ribbon. its rated price is 30 Rs\"],\n",
    "},\n",
    "{\"tag\": \"bitcoin\",\n",
    "\"patterns\":[\"tell me the value of a bitcoin\",\"what is bitcoin\",\"Can you tell more about bitcoin?\",\"What can be bitcoin\",\"what do you mean by bitcoin\"],\n",
    "\"responses\":[\"1 Bitcoin equals 14,14,998.64 Indian Rupee\",\"Bitcoin is a digital or virtual currency created in 2009 that uses peer-to-peer technology to facilitate instant payments\",\"Bitcoin is a cryptocurrency created in 2009. Marketplaces called “bitcoin exchanges” allow people to buy or sell bitcoins using different currencies\"],\n",
    "},\n",
    "{\"tag\": \"AsymmetricInformation\",\n",
    "\"patterns\":[\"what is Asymmetric Information\",\"Tell me about Asymmetric information in finance markets\",\"Do you know about Asymmetric information\",\"What can be Asymmetric information\",\"what do you mean by Asymmetric information\"],\n",
    "\"responses\":[\"It is a type of market failure exists when one individual or party has much more information than another individual or party, and uses that advantage to exploit the other party.\",\"Finance is a market in information – often a potential borrower (such as a small business) has better information on the likelihood that they will be able to repay a loan than the lender.\"],\n",
    "},\n",
    "{\"tag\": \"bail-out\",\n",
    "\"patterns\":[\"what is bail-out\",\"Do you know about bail-out in financial markets\",\"Tell me about bailout\",\"What does critics and supporter say about bail-out\",\"bail-out\"],\n",
    "\"responses\":[\"A bail-out happens when a government buys an equity stake in a bank or some other form of financial support to prevent it from failing\",\"Critics of bail-outs argue that it can increase the burden facing taxpayers and also increase the risk of moral hazard.Supporters of bail-outs claim that they are sometimes necessary during a financial crisis to help reduce systemic risk.\",\" Bail outs have become prominent once again because of the economic and social consequences of the coronavirus crisis.\"],\n",
    "},\n",
    "{\"tag\": \"Balance sheet\",\n",
    "\"patterns\":[\"what is a balance sheet\",\"Tell me about Balance sheet\",\"Do you know Balance sheet?\",\"Balance sheet\",\"What can be Balance sheet\",\"what do you mean by Balance sheet\"],\n",
    "\"responses\":[\"A record of the assets, liabilities, and net worth of an economic actor such as a household, commercial or central bank, firm, or government\",\"A balance sheet is a financial statement that reports a company's assets, liabilities and shareholders' equity.\",\"The balance sheet is one of the three (income statement and statement of cash flows being the other two) core financial statements used to evaluate a business.\"],\n",
    "},\n",
    "{\"tag\": \"bank\",\n",
    "\"patterns\":[\"what is a bank\",\"What exactly bank does\",\"can you tell me about bank\",\"bank\",\"What can be bank\",\"what do you mean by bank\"],\n",
    "\"responses\":[\"A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\"],\n",
    "},\n",
    "{\"tag\": \"bank assests\",\n",
    "\"patterns\":[\"what are bank assets\",\"Tell me about bank assets\",\"what is bank asset\",\"bank assets\",\"what is mean by bank assets\",\"bank assets\",\"What can be bank assets\",\"what do you mean by bank assets\"],\n",
    "\"responses\":[\"Assets are “owned” by the bank e.g. cash, balances at Bank of England, loans (Advances), securities (e.g. Bonds) and fixed assets.\"],\n",
    "},\n",
    "{\"tag\": \"Bank capital\",\n",
    "\"patterns\":[\"what is mean by bank capital?\", \"do you know bank capital?\",\"what is bank capital\",\"bank capital\",\"Tell me about Bank capital\",\"What can be Bank capital\",\"what do you mean by Bank capital\"],\n",
    "\"responses\":[\"Bank capital is the value of the bank's assets minus its liabilities, or debts.\"],\n",
    "},\n",
    "{\"tag\": \"Bond market\",\n",
    "\"patterns\":[\"what is mean by bond market?\", \"do you know bond market?\",\"what is bond market?\",\"Bond Market\",\"What can be Bond Market\",\"what do you mean by Bank Market\"],\n",
    "\"responses\":[\"The bond market broadly describes a marketplace where investors buy debt securities that are brought to the market by either governmental entities or publicly-traded corporations\",\"The market for interest-bearing securities (with either a fixed or a floating rate and with a maturity of at least one year) that companies and governments issue to raise capital for investment.\"],\n",
    "},\n",
    "{\"tag\": \"Bank Run\",\n",
    "\"patterns\":[\"what is mean by Bank Run?\", \"What is bank run and how it occurs?\",\"Do you know about bank run?\",\"Bank Run\",\"What can be bank run\",\"what do you mean by bank run\"],\n",
    "\"responses\":[\"A situation in which depositors withdraw funds from a commercial bank because they fear that it may go bankrupt and not honour its liabilities such as the deposits of the bank's savers.\",\"A bank run occurs when a large number of customers of a bank or other financial institution withdraw their deposits simultaneously over concerns of the bank's solvency.\"],\n",
    "},\n",
    "{\"tag\": \"Bank reserves\",\n",
    "\"patterns\":[\"what is mean by bank reserves?\", \"do you know bank reserves?\",\"Bank reserves and how it works\",\"what is bank reserves\",\"Bank reserves\",\"What can be Bank reserves\",\"what do you mean by Bank reserves\"],\n",
    "\"responses\":[\"Money and liquid assets (such as securities that can be sold quickly) held by banks in order to meet withdrawals by customers.\",\"Bank reserves are the cash minimums that must be kept on hand by financial institutions in order to meet central bank requirements\"],\n",
    "},\n",
    "{\"tag\": \"Capital market\",\n",
    "\"patterns\":[\"what is mean by a capital market?\", \"capital market?\", \"do you know Capital market?\",\"Tell me about capital market\",\"capital market\",\"What can be capital market\",\"what do you mean by capital market\"],\n",
    "\"responses\":[\"Capital markets are the markets where securities such as shares and bonds are issued to raise medium to long-term financing.\"],\n",
    "},\n",
    "{\"tag\": \"Capital ratio\",\n",
    "\"patterns\":[\"what is mean by capital ratio?\", \"do you know Capital ratio?\",\"Tell me about capital ratio\",\"capital ratio\",\"What can be capital ratio\",\"what do you mean by capital ratio\"],\n",
    "\"responses\":[\"A commercial bank's capital ratio measures the funds it has in reserve against the riskier assets it holds that could be vulnerable in the event of a crisis.\",\"the extent to which a financial institution finances its operations by issuing shares and retaining profits, expressed as a percentage of its assets.\"],\n",
    "},\n",
    "{\"tag\": \"Credit risk\",\n",
    "\"patterns\":[\"what is mean by credit risk?\", \"do you know credit risk?\",\"Tell me about credit risk\",\"what is credit risk\",\"credit risk\",\"What can be credit risk\",\"what do you mean by credit risk\"],\n",
    "\"responses\":[\"Credit risk is the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations\",\"This is the risk to the commercial bank of lending to borrowers who turn out to be unable to repay their loans.\"],\n",
    "},\n",
    "{\"tag\": \"Crowdfunding\",\n",
    "\"patterns\":[\"what is mean by crowdfunding?\", \"do you know crowdfunding?\",\"Tell me about crowdfunding\",\"what is crowdfunding?\",\"crowdfunding\",\"What can be crowdfunding\",\"what do you mean by crowdfunding\"],\n",
    "\"responses\":[\"Crowdfunding is a form of equity finance that has grown rapidly in the USA and the UK in particular.\",\"Crowdfunding involves the collective effort of a large number of individuals who network and pool small amounts of their capital to finance a new or existing business venture. Social causes remain the most active source of crowdfunding activity.\",\" Crowdfunding is the use of small amounts of capital from a large number of individuals to finance a new business venture.\"],\n",
    "},\n",
    "{\"tag\": \"cryptocurrency\",\n",
    "\"patterns\":[\"what is mean by cryptocurrency?\", \"do you know cryptocurrency?\",\"what is cryptocurrency\",\"Tell me about cryptocurrency\",\"cryptocurrency\",\"What can be cryptocurrency\",\"what do you mean by cryptocurrency\"],\n",
    "\"responses\":[\"A cryptocurrency is a digital or virtual currency that uses cryptography and is difficult to counterfeit because of this security feature.\",\"A cryptocurrency is a digital or virtual currency designed to work as a medium of exchange\"],\n",
    "},\n",
    "{\"tag\": \"Demonetisation\",\n",
    "\"patterns\":[\"what is mean by demonetisation?\", \"do you know about demonetization that happened in our India in 2016?\",\"what is demonetisation\",\"What can be demonetisation\",\"what do you mean by demonetisation\",\"Tell me about demonetisation\",\"Demonetisation in India\",\"Demonetisation\"],\n",
    "\"responses\":[\"Yes,Demonetization is the act of stripping a currency unit of its status as legal tender.\",\"Demonetisation is a process of removing a unit of currency from circulation by stripping it of its legal tender status\",\"Yes, Demonetization occurs when a governing body cancels the legal tender status of a currency unit in circulation.\"],\n",
    "},\n",
    "{\"tag\": \"Financial crisis\",\n",
    "\"patterns\":[\"what is mean by financial crisis in a country?\", \"do you know bank capital?\",\"Tell me about bank capital\",\"bank capital\",\"What can be bank capital\",\"what do you mean by bank capital\"],\n",
    "\"responses\":[\"A disturbance to financial markets, associated typically with falling asset prices and insolvency amongst debtors and intermediaries, which ramifies through the financial system, disrupting the market’s capacity to allocate capital\",\"A financial crisis is a situation where the value of assets drop rapidly and is often triggered by a panic or a run on banks.\"],\n",
    "},\n",
    "{\"tag\": \"liquidity\",\n",
    "\"patterns\":[\"what is mean by liquidity?\", \"do you know liquidity in financial markets?\",\"what is liquidity?\",\"liquidity\",\"What can be liquidity\",\"what do you mean by liquidity\"],\n",
    "\"responses\":[\"Liquidity means the ease and cost with which assets can be turned into cash and used immediately as a means of exchange. Cash is very liquid whereas a life assurance policy is less so.\",\"It is the the availability of liquid assets such as cash to a market or company.\"],\n",
    "},\n",
    "{\"tag\": \"leverage\",\n",
    "\"patterns\":[\"what is mean by leverage?\", \"do you know about leverage?\",\"do you know leverage?\",\"Tell me about leverage\",\"leverage\",\"What can be leverage\",\"what do you mean by leverage\"],\n",
    "\"responses\":[\"Leverage is the use of borrowed funds to increase profitability. One measure of leverage is the amount of long term debt relative to equity\",\"Leverage is an investment strategy of using borrowed money—specifically, the use of various financial instruments or borrowed capital—to increase the potential return of an investment\",\"Financial leverage which is also known as leverage or trading on equity, refers to the use of debt to acquire additional assets.\"],\n",
    "},\n",
    "{\"tag\": \"liquidity trap\",\n",
    "\"patterns\":[\"what is mean by liquidity trap?\", \"do you know liquidity trap?\",\"Hey tell me what is liquidity trap\",\"what is liquidity trap?\",\"liquidity trap\",\"What can be liquidity trap\",\"what do you mean by liquidity trap\"],\n",
    "\"responses\":[\"A liquidity trap occurs when low interest rates and a high amount of cash balances in the economy fail to stimulate aggregate demand partly through a lack of confidence.\"],\n",
    "},\n",
    "{\"tag\": \"Monetary stability\",\n",
    "\"patterns\":[\"what is mean by monetary stability?\", \"do you know about monetary stability?\",\"Tell me about what exactly monetary stability is.\",\"what is monetary stability\",\"Monetary Stability\",\"What can be Monitary Stability\",\"what do you mean by Monitary stability\"],\n",
    "\"responses\":[\"Monetary stability means stable prices and confidence in the currency. \",\"Monetary stability is a synonym for price stability. Price stability refers to a stable price level or a low level of inflation and not to stable individual prices\"],\n",
    "},\n",
    "{\"tag\": \"Moral hazard\",\n",
    "\"patterns\":[\"what is moral hazard?\",\"what is mean by moral hazard?\", \"tell me an example of moral hazard?\",\"do you know moral hazard\",\"moral hazard\",\"What can be moral hazard\",\"what do you mean by moral hazard\"],\n",
    "\"responses\":[\"When the party with superior information alters his/her behaviour in such a way that benefits himself while imposing costs on those with inferior information.\",\"For example, moral hazard occurs when insured consumers are likely to take greater risks, knowing that a claim will be paid for by their cover. The consumer knows more about his/her intended actions than the producer (e.g. the insurer).\"],\n",
    "},\n",
    "{\"tag\": \"nominal interest rate\",\n",
    "\"patterns\":[\"what is mean by nominal interest rate?\", \"do you know nominal interest rate?\",\"what is nominal interest rate?\",\"Tell me about nominal interest rate\",\"nominal interest rate\",\"What can be nominal interest rate\",\"what do you mean by nominal interest rate\"],\n",
    "\"responses\":[\"The nominal interest rate is the interest rate on a loan or on savings deposits unadjusted for the rate of inflation.\",\"In short, it is the sum of real interest rate and inflation\"],\n",
    "},\n",
    "{\"tag\": \"real interest rate\",\n",
    "\"patterns\":[\"what is mean by real interest rate?\", \"do you know real interst rate?\",\"what is real interest rate\",\"Tell me about real interest rate\",\"real interest rate\",\"what can be real interest rate?\",\"What do you mean by real interest rate\"],\n",
    "\"responses\":[\"A real interest rate is an interest rate that has been adjusted to remove the effects of inflation to reflect the real cost of funds to the borrower and the real yield to the lender or to an investor.\",\"The real interest rate reflects the rate of time-preference for current goods over future goods.\"],\n",
    "},\n",
    "{\"tag\": \"narrow money\",\n",
    "\"patterns\":[\"what is mean by narrow money?\", \"do you know narrow money in financial markets?\",\"what is narrow money\",\"Tell me about Narrow money\",\"Narrow money\",\"what can be Narrow money\",\"What do you mean by Narrow money\"],\n",
    "\"responses\":[\"Narrow money refers to a category of money supply that includes all the real money held by the central bank. It includes coins and currency, demand deposits, and other liquid assets.\",\"The narrow money definition of the money supply is a measure of the value coins and notes in circulation and other money equivalents that are easily convertible into cash such as short term deposits in the banking system.\"],\n",
    "},\n",
    "{\"tag\": \"passporting\",\n",
    "\"patterns\":[\"what is mean by pass porting?\", \"do you know passporting?\",\"what is passporting\",\"Tell me about passporting\",\"passporting\",\"What can be passporting\",\"what do you mean by passporting\"],\n",
    "\"responses\":[\"Passporting allows a firm registered in the European Economic Area (EEA) to do business in any other EEA state without the need for further authorization from each country.\",\"EU legislation gives UK banks, insurers and investment firms ‘passporting’ rights to provide a range of financial services to clients across the EU – either cross-border or through local branches – while remaining regulated solely by their ‘home’ state.\"],\n",
    "},\n",
    "{\"tag\": \"PRA\",\n",
    "\"patterns\":[\"what is mean by Prudential Regulation Authority or PRA?\", \"do you know PRA?\",\"what is PRA\",\"PRA\",\"what can be PRA\"],\n",
    "\"responses\":[\"The PRA is part of the Bank of England and is responsible for the prudential regulation and supervision of around 1,700 banks, building societies, credit unions, insurers and major investment firms.\",\"The PRA has a particular focus on the solvency of specific financial markets such as: Insurance providers, Buy-to-let mortgage lenders, Credit unions and other specialist lenders.\"],\n",
    "},\n",
    "{\"tag\": \"Ring Fence\",\n",
    "\"patterns\":[\"what is mean by ring-fence?\", \"do you know ring fence?\",\"Ring fencing?\",\"what is ring fence\",\"Tell me about ring fence\",\"Ring Fence\"],\n",
    "\"responses\":[\"A ring-fence is a virtual barrier that segregates a portion of an individual's or company's financial assets from the rest.\",\"Ringfencing is when a regulated public utility business financially separates itself from a parent company that engages in non-regulated business.\"],\n",
    "},\n",
    "{\"tag\": \"VAR\",\n",
    "\"patterns\":[\"what is mean by Value at risk?\", \"do you know VAR?\",\"what is exactly VAR?\",\"what is VAR\",\"Tell me about VAR\",\"Do you know Value at Risk\",\"VAR\",\"Value at Risk\"],\n",
    "\"responses\":[\"Value at risk is a measure of the maximum loss expected on an investment.\",\"For example, a 2% value at risk of a £3 million investment means that there is a 2% chance that a portfolio will lose £3 million in a given year.\"],\n",
    "},\n",
    "{\"tag\": \"Compound interest\",\n",
    "\"patterns\":[\"What is mean by Compound interest\",\"Compound interest\",\"what is compound interest\",\"Tell me about compound interest\",\"Do you know compound interest?\",\"What can be compound interest\",\"What do you mean by compound interest\"],\n",
    "\"responses\":[\"It is the interest on the amount of money you have deposited or borrowed\",\"When you are investing or saving, compound interest is charged on the original amount you have accumulated over time\"],\n",
    "},\n",
    "{\"tag\": \"FICO score\",\n",
    "\"patterns\":[\"What is Fico Score\",\"What is mean by Fico Score\",\"Fico Score means?\",\"Fico score\",\"Do you know Fico Score\",\"Tell me about Fico Score\",\"What can be Fico score\"],\n",
    "\"responses\":[\"Your Fico score is based on several factors including payment history, the length of your credit history and total amount owed\",\"Fico Score Reanges from 300 to 850, and the higher the score,the better the terms you may receive on your next loan or credit card.\",\"People with Fico scores below 620 may have a harder time securing credit at a favourable interest rate\"],\n",
    "},\n",
    "{\"tag\": \"Net Worth\",\n",
    "\"patterns\":[\"What is Net Worth\",\"What is mean by Net Worth\",\"net worth means?\",\"Net worth\",\"Do you know Net Worth\",\"Tell me about Net worth\",\"What does Net worth do?\",\"What do you mean by Net worth\",\"what can be networth\"],\n",
    "\"responses\":[\"Your networth is simply the difference between your assets\"],\n",
    "},\n",
    "{\"tag\": \"Asset allocation\",\n",
    "\"patterns\":[\"What is Asset allocation\",\"What is mean by Asset allocation\",\"Asset allocation means?\",\"Asset allocation\",\"Do you know Asset allocation\",\"Tell me about Asset allocation\",\"What does Asset allocation do?\",\"What `do you mean by Asset allocation\",\"what can be Asset allocation\"],\n",
    "\"responses\":[\"Asset allocation is where you choose to put your money\",\"The three major asset classes are stocks, bonds, and cash\"],\n",
    "},\n",
    "{\"tag\": \"Term life insurance\",\n",
    "\"patterns\":[\"What is term life insurance\", \"Do you know term life insurance\",\"what is mean by term life insurance\",\"Tell me about term life insurance\"],\n",
    "\"responses\":[\"Term life insurance provide coverage over a set period, generally anywhere from five to 30 years\",\"If you die within the set term, your beneficiaries receive a payout. If you don't,the policy expires with no value.\"],\n",
    "},\n",
    "{\"tag\": \"Thank you\",\n",
    "\"patterns\":[\"Thank you\", \"Thank you so much for your help\"],\n",
    "\"responses\":[\"Its my pleasure.\",\"You are welcome\",\"No problem. Its my pleasure to help you\"],\n",
    "},\n",
    "\n",
    "]}\n",
    "import json\n",
    "def writeTOJsonFile(path,data1):\n",
    "  filepathname='./'+path+'/'+'1ECONO_GopikaSR_18412_DATA'+'.json'\n",
    "  with open(filepathname,'w') as fp:\n",
    "    json.dump(data,fp)\n",
    "writeTOJsonFile('./',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1O6x_SKlkl4",
    "outputId": "592c5a72-4940-428d-a746-9753b60a5f76"
   },
   "outputs": [],
   "source": [
    "with open('1ECONO_GopikaSR_18412_DATA.json','r') as f:\n",
    "    intents = json.load(f)\n",
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3FsnL_ZPTgh",
    "outputId": "4560faae-3dd7-4ac7-c91a-b23a7f565ecc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\pande\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#importing nltk and necessary downloads\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#Im using porter stemmer here\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93fjiU1fLwZe",
    "outputId": "f26cec99-e1de-42a4-9786-038ae661072c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 47.1/47.1 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: graphviz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhn4xK-rLwvr",
    "outputId": "e24143e8-4869-4cf3-d085-45c37e755341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\python310\\lib\\site-packages (from torchviz) (2.1.0)\n",
      "Requirement already satisfied: graphviz in c:\\python310\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch->torchviz) (3.8.0)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch->torchviz) (3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\python310\\lib\\site-packages (from torch->torchviz) (4.4.0)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch->torchviz) (2023.9.2)\n",
      "Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch->torchviz) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch->torchviz) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch->torchviz) (1.3.0)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py): started\n",
      "  Building wheel for torchviz (setup.py): finished with status 'done'\n",
      "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=2d3de6cb86275fd5ee8d1bc5d10daa6b0f6ce38033b4170697ff25863f097f77\n",
      "  Stored in directory: c:\\users\\pande\\appdata\\local\\pip\\cache\\wheels\\4c\\97\\88\\a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
      "Successfully built torchviz\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RJBy_313NdJP"
   },
   "outputs": [],
   "source": [
    "from torchviz import make_dot,make_dot_from_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gXftJ-V2kgO"
   },
   "source": [
    "# Lets define some functions for ease while working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNNEApKN23vG"
   },
   "source": [
    "So, first we need to tokenize the input sequence.\n",
    "Tokenizing helps us to break down our big sentence to small tokens/individual meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IV9BRIhy2xk8"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvlTGvsL3Q6s"
   },
   "source": [
    "Next is making the words into lower case and then stemming . Stemming means to find the root word. suppose we have 2 seperate words \"eating\" and \"eaten\", then after stemming all of them is just \"eat\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uhYHlYe39QPR"
   },
   "outputs": [],
   "source": [
    "#using porterstemmer here\n",
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEkJIdqT5ESv"
   },
   "source": [
    "Bag of Words(BOW) method is below.\n",
    "It is a famous method in nlp for simplifying the given sentence. By this defined function below a sentence is being represented as the bag of its words. Here, grammer and word order won't be taking into account.\n",
    "\n",
    "So, BOW is a collection/group of words to represent a paretcular sentence with word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2w5hAtIe5Ehp"
   },
   "outputs": [],
   "source": [
    "def BagOfWords(tokenized_sentence, words):\n",
    "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
    "    Bag = np.zeros(len(words), dtype=np.float32)\n",
    "    for idx, w in enumerate(words):\n",
    "        if w in sentence_words: \n",
    "            Bag[idx] = 1\n",
    "    return Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0Fe3OMInKgE",
    "outputId": "ddd4ba13-3082-4ae6-ae15-bd5af5f63223"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking whether this is working or not\n",
    "sentence=[\"hi\",\"im\",\"gopika\",\"hello\"]\n",
    "words=[\"hi\",\"how\",\"are\",\"you\"]\n",
    "BagOfWords(sentence,words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEyDqGyiV-Em"
   },
   "source": [
    "Making the list of all the words, tags and pairs(with contain patterns and their tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_ufG0fu3_No",
    "outputId": "5a34deee-5c27-4c50-9b9d-07bcc20a1fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 patterns\n",
      "47 tags: ['Asset allocation', 'AsymmetricInformation', 'Balance sheet', 'Bank Run', 'Bank capital', 'Bank reserves', 'Bargain offer', 'Bond market', 'Capital market', 'Capital ratio', 'Compound interest', 'Credit risk', 'Crowdfunding', 'Demonetisation', 'FICO score', 'Financial crisis', 'Monetary stability', 'Moral hazard', 'Net Worth', 'PRA', 'Ring Fence', 'Term life insurance', 'Thank you', 'VAR', 'bail-out', 'bank', 'bank assests', 'bitcoin', 'cryptocurrency', 'deliveryoption', 'endingnote', 'leverage', 'liquidity', 'liquidity trap', 'location', 'name', 'narrow money', 'nominal interest rate', 'passporting', 'payments', 'price of a saree', 'price of formal wear', 'real interest rate', 'shopping', 'time period', 'todaysOffers', 'welcome']\n",
      "209 unique stemmed words: [\"'s\", '2016', '`', 'a', 'about', 'accept', 'address', 'again', 'all', 'alloc', 'an', 'and', 'ani', 'are', 'asset', 'asymmetr', 'at', 'author', 'averag', 'bail-out', 'bailout', 'balanc', 'bank', 'bargain', 'be', 'bitcoin', 'black', 'bond', 'brand', 'buy', 'by', 'bye', 'call', 'can', 'capit', 'card', 'cash', 'chatbot', 'cheap', 'cheaper', 'close', 'cloth', 'collect', 'compound', 'cost', 'countri', 'credit', 'crisi', 'critic', 'crowdfund', 'cryptocurr', 'day', 'deliv', 'deliveri', 'demonet', 'demonetis', 'do', 'doe', 'enjoy', 'enough', 'everyth', 'exactli', 'exampl', 'expens', 'experi', 'fenc', 'fico', 'financ', 'financi', 'for', 'formal', 'from', 'fun', 'glad', 'go', 'good', 'goodby', 'got', 'happen', 'happi', 'have', 'hazard', 'hello', 'help', 'here', 'hey', 'hi', 'home', 'how', 'i', 'im', 'in', 'india', 'inform', 'insur', 'interest', 'interst', 'is', 'it', 'know', 'later', 'leav', 'leverag', 'life', 'like', 'liquid', 'locat', 'm', 'market', 'mastercard', 'me', 'mean', 'meet', 'monetari', 'money', 'monitari', 'moral', 'more', 'much', 'name', 'narrow', 'net', 'networth', 'new', 'nice', 'nomin', 'occur', 'of', 'offer', 'ok', 'onli', 'open', 'option', 'or', 'our', 'pass', 'passport', 'pay', 'peopl', 'port', 'pra', 'price', 'product', 'provid', 'prudenti', 'rate', 'ratio', 'real', 'recommend', 'regul', 'reserv', 'restaur', 'ring', 'ring-fenc', 'risk', 'robot', 'run', 's', 'sare', 'say', 'score', 'see', 'session', 'sheet', 'shoe', 'shop', 'site', 'situat', 'so', 'someth', 'stabil', 'store', 'suit', 'sunday', 'support', 'take', 'tell', 'term', 'thank', 'that', 'the', 'then', 'there', 'thi', 'thing', 'till', 'time', 'to', 'today', 'trap', 'u', 'up', 'valu', 'var', 'wa', 'want', 'wear', 'what', 'when', 'where', 'which', 'who', 'will', 'work', 'worth', 'would', 'you', 'your', '’']\n"
     ]
    }
   ],
   "source": [
    "#for collecting all the words, lets create an empty list\n",
    "all_the_words = []\n",
    "#for collecting all the tags\n",
    "tags = []\n",
    "#the below pair list which will be  made filled with both patterns and their tags\n",
    "pair = []\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    # adding this to tag list\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenizing each word in the sentence\n",
    "        w = tokenize(pattern)\n",
    "        # addding to our words list(since w is an array we need to use extend for adding the elements)\n",
    "        all_the_words.extend(w)\n",
    "        # add to pair\n",
    "        pair.append((w, tag))\n",
    "\n",
    "# stem and lower each word. So, first excluding punctuation marks\n",
    "ignore_words = ['?', '.', '!',',']\n",
    "all_the_words = [stem(w) for w in all_the_words if w not in ignore_words]\n",
    "# remove duplicates and sort\n",
    "all_the_words = sorted(set(all_the_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(pair), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_the_words), \"unique stemmed words:\", all_the_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uS2HDWMAqKW4",
    "outputId": "dea6b52b-cc7d-4e6d-c296-b34ddbc352c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputsize= 209\n",
      "outputsize= 47\n"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "#using a tuple to run through the pair\n",
    "for (pattern_sentence, tag) in pair:\n",
    "    # X-->is the bag of words for each pattern_sentence\n",
    "    bag = BagOfWords(pattern_sentence, all_the_words)\n",
    "    X_train.append(bag)\n",
    "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot.so defining label\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "#array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Hyper-parameters are--->\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "#len(X_train[0]) means the length of 1st bag of words because they all have the same size. \n",
    "#if we want we can just print and check. But its clear here.\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "#lets print the values\n",
    "print(\"inputsize=\",input_size)\n",
    "print(\"outputsize=\",output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oFk62qe1X1_"
   },
   "source": [
    "Yes they are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oW1PW0tsqNEL"
   },
   "outputs": [],
   "source": [
    "#traning dataset\n",
    "class ChatDataset(Dataset):\n",
    "#implementing init function\n",
    "    def __init__(self):\n",
    "      #storing the values\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Or-oNjeSU8eC"
   },
   "source": [
    "Following is the whole set up for model buiding.\n",
    "I'm using pytorch.\n",
    "\n",
    "I'm also using Batch Normalization also for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WQIZy6Z7G6wD"
   },
   "outputs": [],
   "source": [
    "#creating a new class for our neural network\n",
    "class NeuralNetModel(nn.Module):\n",
    "  #This will be a feed forward neural network with two hidden layer \n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetModel, self).__init__()\n",
    "        #creating the firstlinear layer. This gets the input size and then the connected hiddenlayer\n",
    "        self.linearlayer1 = nn.Linear(input_size, hidden_size)\n",
    "        #applying batch normalization\n",
    "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
    "        #creating the 1st hidden layer with input size as hiddensize and output size as the hidden size\n",
    "        self.linearlayer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2= nn.BatchNorm1d(hidden_size) \n",
    "        #creating the 2nd hidden layer with input size as hiddensize and output size as the num classes\n",
    "        self.linearlayer3 = nn.Linear(hidden_size, num_classes)\n",
    "        #using relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    #implementing the forward pass\n",
    "    def forward(self, x):\n",
    "      #apply our first linear layer which gets x as input and then gives out the output\n",
    "        output = self.linearlayer1(x)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "      #apply our first linear layer which gets output as input and then gives out the next output\n",
    "        output = self.linearlayer2(output)\n",
    "        #activation function\n",
    "        output = self.relu(output)\n",
    "        output = self.linearlayer3(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d__BSgUYOBAa"
   },
   "source": [
    "# Lets garphically visualize our model...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Graphviz in c:\\python310\\lib\\site-packages (0.20.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'D:\\windows_10_msbuild_Release_graphviz-10.0.1-win32\\Graphviz\\bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O6IrwfUEMxWV"
   },
   "outputs": [],
   "source": [
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "import graphviz\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "z9tJLGysMRk3",
    "outputId": "a403b3a9-05d2-4b42-8302-2019b9d8068a"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"483pt\" height=\"616pt\"\n",
       " viewBox=\"0.00 0.00 483.00 616.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 612)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-612 479,-612 479,4 -4,4\"/>\n",
       "<!-- 2327983708240 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2327983708240</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"239,-32 185,-32 185,0 239,0 239,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 2327949709696 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2327949709696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-88 165,-88 165,-68 259,-68 259,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 2327949709696&#45;&gt;2327983708240 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>2327949709696&#45;&gt;2327983708240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212,-67.62C212,-61.1 212,-52.05 212,-43.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.5,-43.65 212,-33.65 208.5,-43.65 215.5,-43.65\"/>\n",
       "</g>\n",
       "<!-- 2327972948160 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2327972948160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-144 162,-144 162,-124 262,-124 262,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972948160&#45;&gt;2327949709696 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2327972948160&#45;&gt;2327949709696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212,-123.59C212,-117.01 212,-107.96 212,-99.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.5,-99.81 212,-89.81 208.5,-99.81 215.5,-99.81\"/>\n",
       "</g>\n",
       "<!-- 2327972946624 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2327972946624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"141,-200 41,-200 41,-180 141,-180 141,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"91\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972946624&#45;&gt;2327972948160 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2327972946624&#45;&gt;2327972948160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.08,-179.59C130.96,-171.17 158.92,-158.69 180.46,-149.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.67,-152.37 189.38,-145.1 178.82,-145.98 181.67,-152.37\"/>\n",
       "</g>\n",
       "<!-- 2327983715520 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2327983715520</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"144,-268 26,-268 26,-236 144,-236 144,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"85\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"85\" y=\"-242.5\" font-family=\"monospace\" font-size=\"10.00\"> (47)</text>\n",
       "</g>\n",
       "<!-- 2327983715520&#45;&gt;2327972946624 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2327983715520&#45;&gt;2327972946624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.55,-235.55C87.27,-228.26 88.15,-219.45 88.94,-211.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.4,-212.21 89.91,-201.91 85.43,-211.52 92.4,-212.21\"/>\n",
       "</g>\n",
       "<!-- 2327972948880 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2327972948880</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"259,-200 165,-200 165,-180 259,-180 259,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972948880&#45;&gt;2327972948160 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2327972948880&#45;&gt;2327972948160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212,-179.59C212,-173.01 212,-163.96 212,-155.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.5,-155.81 212,-145.81 208.5,-155.81 215.5,-155.81\"/>\n",
       "</g>\n",
       "<!-- 2327972951328 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2327972951328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-262 162,-262 162,-242 262,-242 262,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-248.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972951328&#45;&gt;2327972948880 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2327972951328&#45;&gt;2327972948880</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M212,-241.62C212,-233.56 212,-221.65 212,-211.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.5,-211.63 212,-201.63 208.5,-211.63 215.5,-211.63\"/>\n",
       "</g>\n",
       "<!-- 2327972948400 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2327972948400</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"115,-330 15,-330 15,-310 115,-310 115,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972948400&#45;&gt;2327972951328 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2327972948400&#45;&gt;2327972951328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.06,-309.54C110.84,-298.42 152.21,-279.84 180.74,-267.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.91,-270.35 189.6,-263.06 179.04,-263.96 181.91,-270.35\"/>\n",
       "</g>\n",
       "<!-- 2327983715760 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2327983715760</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"118,-404 0,-404 0,-372 118,-372 118,-404\"/>\n",
       "<text text-anchor=\"middle\" x=\"59\" y=\"-390.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"59\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n",
       "</g>\n",
       "<!-- 2327983715760&#45;&gt;2327972948400 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2327983715760&#45;&gt;2327972948400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.39,-371.69C61.2,-362.8 62.23,-351.46 63.11,-341.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.6,-342.1 64.02,-331.82 59.62,-341.47 66.6,-342.1\"/>\n",
       "</g>\n",
       "<!-- 2327972952048 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2327972952048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"233,-330 139,-330 139,-310 233,-310 233,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972952048&#45;&gt;2327972951328 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2327972952048&#45;&gt;2327972951328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.73,-309.54C193.47,-300.04 199.36,-285.09 204.14,-272.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.32,-274.43 207.73,-263.85 200.81,-271.87 207.32,-274.43\"/>\n",
       "</g>\n",
       "<!-- 2327972951136 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2327972951136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"236,-398 136,-398 136,-378 236,-378 236,-398\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-384.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972951136&#45;&gt;2327972952048 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2327972951136&#45;&gt;2327972952048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-377.54C186,-368.23 186,-353.7 186,-341.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-341.95 186,-331.95 182.5,-341.95 189.5,-341.95\"/>\n",
       "</g>\n",
       "<!-- 2327972949456 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2327972949456</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"120,-466 20,-466 20,-446 120,-446 120,-466\"/>\n",
       "<text text-anchor=\"middle\" x=\"70\" y=\"-452.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972949456&#45;&gt;2327972951136 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2327972949456&#45;&gt;2327972951136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.62,-445.54C105.66,-434.71 137.1,-416.82 159.52,-404.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161.13,-407.17 168.1,-399.19 157.67,-401.09 161.13,-407.17\"/>\n",
       "</g>\n",
       "<!-- 2327983330688 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>2327983330688</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"124,-540 6,-540 6,-508 124,-508 124,-540\"/>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-526.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"65\" y=\"-514.5\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n",
       "</g>\n",
       "<!-- 2327983330688&#45;&gt;2327972949456 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>2327983330688&#45;&gt;2327972949456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.16,-507.69C66.83,-498.8 67.69,-487.46 68.43,-477.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.91,-478.06 69.18,-467.83 64.93,-477.53 71.91,-478.06\"/>\n",
       "</g>\n",
       "<!-- 2327972949840 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>2327972949840</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"224,-466 148,-466 148,-446 224,-446 224,-466\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-452.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972949840&#45;&gt;2327972951136 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2327972949840&#45;&gt;2327972951136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186,-445.54C186,-436.23 186,-421.7 186,-409.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.5,-409.95 186,-399.95 182.5,-409.95 189.5,-409.95\"/>\n",
       "</g>\n",
       "<!-- 2327972949312 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>2327972949312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"242,-534 142,-534 142,-514 242,-514 242,-534\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-520.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972949312&#45;&gt;2327972949840 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>2327972949312&#45;&gt;2327972949840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.14,-513.54C190.29,-504.23 188.97,-489.7 187.88,-477.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.39,-477.59 187,-467.95 184.41,-478.22 191.39,-477.59\"/>\n",
       "</g>\n",
       "<!-- 2327983333728 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>2327983333728</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"257,-608 127,-608 127,-576 257,-576 257,-608\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-594.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-582.5\" font-family=\"monospace\" font-size=\"10.00\"> (8, 209)</text>\n",
       "</g>\n",
       "<!-- 2327983333728&#45;&gt;2327972949312 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>2327983333728&#45;&gt;2327972949312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192,-575.69C192,-566.8 192,-555.46 192,-545.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.5,-545.83 192,-535.83 188.5,-545.83 195.5,-545.83\"/>\n",
       "</g>\n",
       "<!-- 2327972943264 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>2327972943264</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"327,-330 251,-330 251,-310 327,-310 327,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"289\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972943264&#45;&gt;2327972951328 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>2327972943264&#45;&gt;2327972951328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.97,-309.54C265.85,-299.15 246.14,-282.27 231.42,-269.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234.03,-267.27 224.16,-263.42 229.48,-272.59 234.03,-267.27\"/>\n",
       "</g>\n",
       "<!-- 2327972952000 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>2327972952000</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"355,-398 255,-398 255,-378 355,-378 355,-398\"/>\n",
       "<text text-anchor=\"middle\" x=\"305\" y=\"-384.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972952000&#45;&gt;2327972943264 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>2327972952000&#45;&gt;2327972943264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.71,-377.54C300.43,-368.13 296.85,-353.39 293.93,-341.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"297.4,-340.8 291.64,-331.91 290.6,-342.45 297.4,-340.8\"/>\n",
       "</g>\n",
       "<!-- 2327983717120 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>2327983717120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"372,-472 242,-472 242,-440 372,-440 372,-472\"/>\n",
       "<text text-anchor=\"middle\" x=\"307\" y=\"-458.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"307\" y=\"-446.5\" font-family=\"monospace\" font-size=\"10.00\"> (8, 8)</text>\n",
       "</g>\n",
       "<!-- 2327983717120&#45;&gt;2327972952000 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>2327983717120&#45;&gt;2327972952000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M306.54,-439.69C306.27,-430.8 305.92,-419.46 305.63,-409.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.13,-409.72 305.33,-399.83 302.13,-409.93 309.13,-409.72\"/>\n",
       "</g>\n",
       "<!-- 2327972951040 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>2327972951040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"387,-200 311,-200 311,-180 387,-180 387,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"349\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2327972951040&#45;&gt;2327972948160 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2327972951040&#45;&gt;2327972948160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M325.14,-179.59C303.46,-171.05 271.23,-158.35 246.71,-148.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"248.04,-145.45 237.46,-145.03 245.48,-151.96 248.04,-145.45\"/>\n",
       "</g>\n",
       "<!-- 2327972953152 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>2327972953152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"433,-262 333,-262 333,-242 433,-242 433,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"383\" y=\"-248.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2327972953152&#45;&gt;2327972951040 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>2327972953152&#45;&gt;2327972951040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.69,-241.62C372.92,-233.21 365.77,-220.6 359.81,-210.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.89,-208.4 354.91,-201.43 356.8,-211.86 362.89,-208.4\"/>\n",
       "</g>\n",
       "<!-- 2327983709440 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>2327983709440</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"475,-336 345,-336 345,-304 475,-304 475,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"410\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"410\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\"> (47, 8)</text>\n",
       "</g>\n",
       "<!-- 2327983709440&#45;&gt;2327972953152 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>2327983709440&#45;&gt;2327972953152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M403.74,-303.69C399.98,-294.5 395.15,-282.7 391.1,-272.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"394.41,-271.65 387.38,-263.72 387.93,-274.3 394.41,-271.65\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x21e049dab30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x=torch.randn(47,209)\n",
    "y=model(x)\n",
    "make_dot(y.mean(),params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ygRMndsPEq1"
   },
   "source": [
    "#counting the no of parameters the model have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rhAX4SJ_2rI",
    "outputId": "774d82ef-109e-455e-c7e8-8a18c8dc469b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2207"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the no of parameters it have. \n",
    "def count_parameters(model):\n",
    "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-6RPWmn262p"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP5ESzzIGpIl",
    "outputId": "f259db11-e6d1-40c4-f99c-7c2d80db1f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 3.8265 And Got 173 / 2980 with accuracy 5.81\n",
      "Epoch [20/1000], Loss: 2.2239 And Got 1130 / 5960 with accuracy 18.96\n",
      "Epoch [30/1000], Loss: 0.7408 And Got 3204 / 8940 with accuracy 35.84\n",
      "Epoch [40/1000], Loss: 1.1125 And Got 5773 / 11920 with accuracy 48.43\n",
      "Epoch [50/1000], Loss: 0.0506 And Got 8607 / 14900 with accuracy 57.77\n",
      "Epoch [60/1000], Loss: 0.3048 And Got 11499 / 17880 with accuracy 64.31\n",
      "Epoch [70/1000], Loss: 0.0177 And Got 14415 / 20860 with accuracy 69.10\n",
      "Epoch [80/1000], Loss: 0.0758 And Got 17341 / 23840 with accuracy 72.74\n",
      "Epoch [90/1000], Loss: 0.0438 And Got 20267 / 26820 with accuracy 75.57\n",
      "Epoch [100/1000], Loss: 0.4815 And Got 23189 / 29800 with accuracy 77.82\n",
      "Epoch [110/1000], Loss: 0.0442 And Got 26115 / 32780 with accuracy 79.67\n",
      "Epoch [120/1000], Loss: 0.0469 And Got 29042 / 35760 with accuracy 81.21\n",
      "Epoch [130/1000], Loss: 0.0124 And Got 31966 / 38740 with accuracy 82.51\n",
      "Epoch [140/1000], Loss: 0.0166 And Got 34888 / 41720 with accuracy 83.62\n",
      "Epoch [150/1000], Loss: 0.0009 And Got 37810 / 44700 with accuracy 84.59\n",
      "Epoch [160/1000], Loss: 0.0021 And Got 40734 / 47680 with accuracy 85.43\n",
      "Epoch [170/1000], Loss: 0.0011 And Got 43652 / 50660 with accuracy 86.17\n",
      "Epoch [180/1000], Loss: 0.0023 And Got 46576 / 53640 with accuracy 86.83\n",
      "Epoch [190/1000], Loss: 0.0179 And Got 49499 / 56620 with accuracy 87.42\n",
      "Epoch [200/1000], Loss: 0.0033 And Got 52424 / 59600 with accuracy 87.96\n",
      "Epoch [210/1000], Loss: 0.0051 And Got 55343 / 62580 with accuracy 88.44\n",
      "Epoch [220/1000], Loss: 0.0005 And Got 58270 / 65560 with accuracy 88.88\n",
      "Epoch [230/1000], Loss: 0.0009 And Got 61195 / 68540 with accuracy 89.28\n",
      "Epoch [240/1000], Loss: 0.0007 And Got 64120 / 71520 with accuracy 89.65\n",
      "Epoch [250/1000], Loss: 0.0016 And Got 67040 / 74500 with accuracy 89.99\n",
      "Epoch [260/1000], Loss: 0.0006 And Got 69966 / 77480 with accuracy 90.30\n",
      "Epoch [270/1000], Loss: 0.0010 And Got 72888 / 80460 with accuracy 90.59\n",
      "Epoch [280/1000], Loss: 0.0001 And Got 75808 / 83440 with accuracy 90.85\n",
      "Epoch [290/1000], Loss: 0.4844 And Got 78734 / 86420 with accuracy 91.11\n",
      "Epoch [300/1000], Loss: 0.0001 And Got 81659 / 89400 with accuracy 91.34\n",
      "Epoch [310/1000], Loss: 0.0001 And Got 84584 / 92380 with accuracy 91.56\n",
      "Epoch [320/1000], Loss: 0.0001 And Got 87508 / 95360 with accuracy 91.77\n",
      "Epoch [330/1000], Loss: 0.0001 And Got 90429 / 98340 with accuracy 91.96\n",
      "Epoch [340/1000], Loss: 0.0004 And Got 93353 / 101320 with accuracy 92.14\n",
      "Epoch [350/1000], Loss: 0.0000 And Got 96269 / 104300 with accuracy 92.30\n",
      "Epoch [360/1000], Loss: 0.0006 And Got 99191 / 107280 with accuracy 92.46\n",
      "Epoch [370/1000], Loss: 0.0001 And Got 102109 / 110260 with accuracy 92.61\n",
      "Epoch [380/1000], Loss: 0.0001 And Got 105028 / 113240 with accuracy 92.75\n",
      "Epoch [390/1000], Loss: 0.0002 And Got 107950 / 116220 with accuracy 92.88\n",
      "Epoch [400/1000], Loss: 0.0001 And Got 110867 / 119200 with accuracy 93.01\n",
      "Epoch [410/1000], Loss: 0.0001 And Got 113783 / 122180 with accuracy 93.13\n",
      "Epoch [420/1000], Loss: 0.0001 And Got 116710 / 125160 with accuracy 93.25\n",
      "Epoch [430/1000], Loss: 0.0000 And Got 119631 / 128140 with accuracy 93.36\n",
      "Epoch [440/1000], Loss: 0.0000 And Got 122549 / 131120 with accuracy 93.46\n",
      "Epoch [450/1000], Loss: 0.0000 And Got 125466 / 134100 with accuracy 93.56\n",
      "Epoch [460/1000], Loss: 0.0000 And Got 128385 / 137080 with accuracy 93.66\n",
      "Epoch [470/1000], Loss: 0.0001 And Got 131306 / 140060 with accuracy 93.75\n",
      "Epoch [480/1000], Loss: 0.0000 And Got 134225 / 143040 with accuracy 93.84\n",
      "Epoch [490/1000], Loss: 0.0000 And Got 137140 / 146020 with accuracy 93.92\n",
      "Epoch [500/1000], Loss: 0.0000 And Got 140062 / 149000 with accuracy 94.00\n",
      "Epoch [510/1000], Loss: 0.0000 And Got 142980 / 151980 with accuracy 94.08\n",
      "Epoch [520/1000], Loss: 0.0000 And Got 145902 / 154960 with accuracy 94.15\n",
      "Epoch [530/1000], Loss: 0.0000 And Got 148825 / 157940 with accuracy 94.23\n",
      "Epoch [540/1000], Loss: 0.0000 And Got 151745 / 160920 with accuracy 94.30\n",
      "Epoch [550/1000], Loss: 0.0000 And Got 154665 / 163900 with accuracy 94.37\n",
      "Epoch [560/1000], Loss: 0.0000 And Got 157586 / 166880 with accuracy 94.43\n",
      "Epoch [570/1000], Loss: 0.0000 And Got 160512 / 169860 with accuracy 94.50\n",
      "Epoch [580/1000], Loss: 0.0000 And Got 163431 / 172840 with accuracy 94.56\n",
      "Epoch [590/1000], Loss: 0.0000 And Got 166348 / 175820 with accuracy 94.61\n",
      "Epoch [600/1000], Loss: 0.0000 And Got 169269 / 178800 with accuracy 94.67\n",
      "Epoch [610/1000], Loss: 0.0000 And Got 172197 / 181780 with accuracy 94.73\n",
      "Epoch [620/1000], Loss: 0.0000 And Got 175123 / 184760 with accuracy 94.78\n",
      "Epoch [630/1000], Loss: 0.0000 And Got 178047 / 187740 with accuracy 94.84\n",
      "Epoch [640/1000], Loss: 0.0000 And Got 180966 / 190720 with accuracy 94.89\n",
      "Epoch [650/1000], Loss: 0.0004 And Got 183888 / 193700 with accuracy 94.93\n",
      "Epoch [660/1000], Loss: 0.0000 And Got 186811 / 196680 with accuracy 94.98\n",
      "Epoch [670/1000], Loss: 0.0000 And Got 189736 / 199660 with accuracy 95.03\n",
      "Epoch [680/1000], Loss: 0.0000 And Got 192660 / 202640 with accuracy 95.08\n",
      "Epoch [690/1000], Loss: 0.0000 And Got 195584 / 205620 with accuracy 95.12\n",
      "Epoch [700/1000], Loss: 0.0000 And Got 198508 / 208600 with accuracy 95.16\n",
      "Epoch [710/1000], Loss: 0.4458 And Got 201429 / 211580 with accuracy 95.20\n",
      "Epoch [720/1000], Loss: 0.0000 And Got 204350 / 214560 with accuracy 95.24\n",
      "Epoch [730/1000], Loss: 0.0000 And Got 207275 / 217540 with accuracy 95.28\n",
      "Epoch [740/1000], Loss: 0.0000 And Got 210198 / 220520 with accuracy 95.32\n",
      "Epoch [750/1000], Loss: 0.0000 And Got 213122 / 223500 with accuracy 95.36\n",
      "Epoch [760/1000], Loss: 0.0000 And Got 216046 / 226480 with accuracy 95.39\n",
      "Epoch [770/1000], Loss: 0.0000 And Got 218966 / 229460 with accuracy 95.43\n",
      "Epoch [780/1000], Loss: 0.0000 And Got 221890 / 232440 with accuracy 95.46\n",
      "Epoch [790/1000], Loss: 0.0000 And Got 224813 / 235420 with accuracy 95.49\n",
      "Epoch [800/1000], Loss: 0.0000 And Got 227734 / 238400 with accuracy 95.53\n",
      "Epoch [810/1000], Loss: 0.0000 And Got 230662 / 241380 with accuracy 95.56\n",
      "Epoch [820/1000], Loss: 0.0000 And Got 233585 / 244360 with accuracy 95.59\n",
      "Epoch [830/1000], Loss: 0.0000 And Got 236503 / 247340 with accuracy 95.62\n",
      "Epoch [840/1000], Loss: 0.0000 And Got 239428 / 250320 with accuracy 95.65\n",
      "Epoch [850/1000], Loss: 0.0000 And Got 242351 / 253300 with accuracy 95.68\n",
      "Epoch [860/1000], Loss: 0.0000 And Got 245273 / 256280 with accuracy 95.71\n",
      "Epoch [870/1000], Loss: 0.0000 And Got 248196 / 259260 with accuracy 95.73\n",
      "Epoch [880/1000], Loss: 0.0000 And Got 251116 / 262240 with accuracy 95.76\n",
      "Epoch [890/1000], Loss: 0.0000 And Got 254037 / 265220 with accuracy 95.78\n",
      "Epoch [900/1000], Loss: 0.0000 And Got 256957 / 268200 with accuracy 95.81\n",
      "Epoch [910/1000], Loss: 0.0000 And Got 259871 / 271180 with accuracy 95.83\n",
      "Epoch [920/1000], Loss: 0.0000 And Got 262792 / 274160 with accuracy 95.85\n",
      "Epoch [930/1000], Loss: 0.3220 And Got 265712 / 277140 with accuracy 95.88\n",
      "Epoch [940/1000], Loss: 0.0000 And Got 268635 / 280120 with accuracy 95.90\n",
      "Epoch [950/1000], Loss: 0.0000 And Got 271561 / 283100 with accuracy 95.92\n",
      "Epoch [960/1000], Loss: 0.0000 And Got 274481 / 286080 with accuracy 95.95\n",
      "Epoch [970/1000], Loss: 0.0000 And Got 277405 / 289060 with accuracy 95.97\n",
      "Epoch [980/1000], Loss: 0.0000 And Got 280331 / 292040 with accuracy 95.99\n",
      "Epoch [990/1000], Loss: 0.0000 And Got 283252 / 295020 with accuracy 96.01\n",
      "Epoch [1000/1000], Loss: 0.0000 And Got 286170 / 298000 with accuracy 96.03\n",
      "final Accuracy-----> Got 286170 / 298000 with accuracy 96.03\n",
      "final loss: 0.0000\n",
      "training complete. file saved to DATA.pth\n"
     ]
    }
   ],
   "source": [
    "#the training dataset, we have defined earlier.\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Loss is CrossEntropyLoss and optimizer is Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "total=0\n",
    "correct=0\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(words)\n",
    "        # if y would be one-hot, we must apply\n",
    "        # labels = torch.max(labels, 1)[1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scores = model(words)\n",
    "        _, pred = scores.max(1)\n",
    "        total += len(words)\n",
    "        correct += (pred==labels).sum()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f} And Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}')\n",
    "print(f'final Accuracy-----> Got {correct} / {total} with accuracy {float(correct)/float(total)*100:.2f}') \n",
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_the_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "#storing these  in a file, this will serialise it and save it to that file named DATA.pth.\n",
    "FILE = \"DATA.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8kXca3vy82S"
   },
   "source": [
    "loss is decreasing and accuracy is increasing in every epoch...\n",
    "\n",
    "FInal accuracy is 95.03 and final loss=0\n",
    "\n",
    "So, our model is pretty good for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFbO0eaDv3AS",
    "outputId": "94f433b8-d40b-45d7-d42a-be9861de4c56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetModel(\n",
       "  (linearlayer1): Linear(in_features=209, out_features=8, bias=True)\n",
       "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linearlayer3): Linear(in_features=8, out_features=47, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the data we saved before\n",
    "FILE = \"DATA.pth\" \n",
    "data = torch.load(FILE)\n",
    "\n",
    "#defining values\n",
    "input_size = data[\"input_size\"] \n",
    "hidden_size = data[\"hidden_size\"] \n",
    "output_size = data[\"output_size\"] \n",
    "all_the_words = data['all_words'] \n",
    "tags = data['tags'] \n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "#lets print the model now.\n",
    "model = NeuralNetModel(input_size, hidden_size, output_size).to(device) \n",
    "model.load_state_dict(model_state) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VubKW2BlJCEM"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def bargain(value):\n",
    "    # tokenize a sentence to tokens\n",
    "    tokens = nltk.word_tokenize(value)\n",
    "    #tagging will help top understand at what category it belong to.\n",
    "    #for eg: walk is a verb phrase,one is a cardinal digit \n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    bot_name=\"FinBot\"\n",
    "    # tree representation\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "#defining the numbers to the str values\n",
    "    numbers = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"10\" : 10,\"11\": 11,\"12\": 12,\"13\": 13,\"14\": 14,\"15\": 15,\"16\": 16,\"17\": 17,\"18\": 18,\"19\": 19,\"20\": 20,\"21\": 21,\"22\": 22,\"23\": 23,\"24\": 24,\"25\": 25,\"26\": 26,\"27\": 27,\"28\": 28,\"29\":29,\"30\": 30}\n",
    "    OptionsToReply=[\"Sorry, this is of latest fashion, Can you raise the amount a little bit\",\"This is a very special thing, we can't give you at this much less cost\",\"Oh no sorry. Please raise a little bit\"]\n",
    "    for word, wordType in entities:\n",
    "        word = stemmer.stem(word)\n",
    "#             print (word, wordType)\n",
    "        if (wordType in ['CD'] and word in numbers):\n",
    "               if numbers[word] >20:\n",
    "                 print(\"FinBot:Yes agreed! Now,you can buy the ribbon at this price\")\n",
    "               elif numbers[word] <=20:\n",
    "                 print(f\"{bot_name}: {random.choice(OptionsToReply)}\")\n",
    "               else:\n",
    "                  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2QAaZJfKiBn"
   },
   "source": [
    "checking the bargaining model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FngKg-OPsbKq",
    "outputId": "c31e553e-e681-4858-fea5-5afee3cf63ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBot: Oh no sorry. Please raise a little bit\n"
     ]
    }
   ],
   "source": [
    "bargain('15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lab_2PPcsluz",
    "outputId": "5d17a48a-d50b-46be-9270-1099cabd9d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBot: Sorry, this is of latest fashion, Can you raise the amount a little bit\n"
     ]
    }
   ],
   "source": [
    "bargain('19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43wMKbKVsp-m",
    "outputId": "12e90436-28ec-4a36-ab61-f73eaf9a33e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBot: Oh no sorry. Please raise a little bit\n"
     ]
    }
   ],
   "source": [
    "bargain('20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Y1n5TUKstk0",
    "outputId": "3b5d6428-ebde-4635-f255-ecce168b135a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBot:Yes agreed! Now,you can buy the ribbon at this price\n"
     ]
    }
   ],
   "source": [
    "bargain('25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wS0JhRA3s0qp",
    "outputId": "2acd29d4-cbfe-48d0-85eb-b1492e122123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBot:Yes agreed! Now,you can buy the ribbon at this price\n"
     ]
    }
   ],
   "source": [
    "bargain('29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTRBCsBxqvBH"
   },
   "source": [
    "# **Let's chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKcvh6hfGpL4",
    "outputId": "45b66e88-1eb8-4f71-ba43-cc819ee9d88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Name: Arptshivam Pandey\n",
      "FinBot:Hey, Let's chat! (type 'quit' to exit)Also when you start bargaining give digits\n",
      "Arptshivam Pandey:50000\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
      "Arptshivam Pandey:2000\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
      "Arptshivam Pandey:5000\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
      "Arptshivam Pandey:500000\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
      "Arptshivam Pandey:2\n",
      "FinBot: This is a very special thing, we can't give you at this much less cost\n",
      "Arptshivam Pandey:200\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
      "Arptshivam Pandey:-500\n",
      "FinBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n"
     ]
    }
   ],
   "source": [
    "bot_name = \"FinBot\"\n",
    "#for taking the input\n",
    "name=input(\"Enter Your Name: \")\n",
    "print(\"FinBot:Hey, Let's chat! (type 'quit' to exit)Also when you start bargaining give digits\")\n",
    "while True:\n",
    "  #once the person types his name, then from the next chat onwards the name will be shown\n",
    "    sent=input(name+':')\n",
    "    if sent == \"quit\":\n",
    "        break\n",
    "    #since im going to offer bargaining offer for only one product and its rated price is 30 Rs.\n",
    "    #However the person will tell only numbers ranging from 1 to 30\n",
    "    if sent in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"]:\n",
    "       bargain(sent)\n",
    "    else:\n",
    "        sent = tokenize(sent)\n",
    "        X = BagOfWords(sent, all_the_words)\n",
    "        X = X.reshape(1, X.shape[0])\n",
    "        X = torch.from_numpy(X).to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        tag = tags[predicted.item()]\n",
    "\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        prob = probs[0][predicted.item()]\n",
    "        if prob.item() > 0.75:\n",
    "            for intent in intents['intents']:\n",
    "                if tag == intent[\"tag\"]:\n",
    "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "        else:\n",
    "            print(f\"{bot_name}: I do not understand...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CodeChatBot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
