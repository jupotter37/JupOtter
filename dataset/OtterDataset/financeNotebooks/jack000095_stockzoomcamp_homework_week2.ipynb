{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2 Homework\n",
    "\n",
    "In this homework, we're going to combine data from various sources to process it in Pandas and generate additional fields.\n",
    "\n",
    "If not stated otherwise, please use the [Colab](https://github.com/DataTalksClub/stock-markets-analytics-zoomcamp/blob/main/02-dataframe-analysis/Module2_Colab_Working_with_the_data.ipynb) covered at the livestream to re-use the code snippets.\n",
    "\n",
    "---\n",
    "### Question 1: IPO Filings Web Scraping and Data Processing\n",
    "\n",
    "**What's the total sum ($m) of 2023 filings that happenned of Fridays?**\n",
    "\n",
    "Re-use the [Code Snippet 1] example to get the data from web for this endpoint: https://stockanalysis.com/ipos/filings/\n",
    "Convert the 'Filing Date' to datetime(), 'Shares Offered' to float64 (if '-' is encountered, populate with NaNs).\n",
    "Define a new field 'Avg_price' based on the \"Price Range\", which equals to NaN if no price is specified, to the price (if only one number is provided), or to the average of 2 prices (if a range is given).\n",
    "You may be inspired by the function `extract_numbers()` in [Code Snippet 4], or you can write your own function to \"parse\" a string.\n",
    "Define a column \"Shares_offered_value\", which equals to \"Shares Offered\" * \"Avg_price\" (when both columns are defined; otherwise, it's NaN)\n",
    "\n",
    "Find the total sum in $m (millions of USD, closest INTEGER number) for all fillings during 2023, which happened on Fridays (`Date.dt.dayofweek()==4`). You should see 32 records in total, 24 of it is not null.\n",
    "\n",
    "(additional: you can read about [S-1 IPO filing](https://www.dfinsolutions.com/knowledge-hub/thought-leadership/knowledge-resources/what-s-1-ipo-filing) to understand the context)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://stockanalysis.com/ipos/filings/\")\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 假設數據在一個表格內，找到該表格並提取數據\n",
    "table = soup.find('table')\n",
    "rows = table.find_all('tr')\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    data.append([ele.text.strip() for ele in cols if ele])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['filing Date', 'Symbol', 'Company Name', 'Price Range', 'Shares Offered' ])\n",
    "df = df.iloc[1:]  # 根據表格的實際布局調整索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the 'Shares Offeered' column comma to empty\n",
    "df['Shares Offered'] = df['Shares Offered'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Price Range</th>\n",
       "      <th>Shares Offered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>May 3, 2024</td>\n",
       "      <td>TBN</td>\n",
       "      <td>Tamboran Resources Corporation</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apr 29, 2024</td>\n",
       "      <td>HWEC</td>\n",
       "      <td>HW Electro Co., Ltd.</td>\n",
       "      <td>$3.00</td>\n",
       "      <td>3750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apr 29, 2024</td>\n",
       "      <td>DTSQ</td>\n",
       "      <td>DT Cloud Star Acquisition Corporation</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>6000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apr 26, 2024</td>\n",
       "      <td>EURK</td>\n",
       "      <td>Eureka Acquisition Corp</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>5000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apr 26, 2024</td>\n",
       "      <td>HDL</td>\n",
       "      <td>Super Hi International Holding Ltd.</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filing Date Symbol                           Company Name Price Range  \\\n",
       "1   May 3, 2024    TBN         Tamboran Resources Corporation           -   \n",
       "2  Apr 29, 2024   HWEC                   HW Electro Co., Ltd.       $3.00   \n",
       "3  Apr 29, 2024   DTSQ  DT Cloud Star Acquisition Corporation      $10.00   \n",
       "4  Apr 26, 2024   EURK                Eureka Acquisition Corp      $10.00   \n",
       "5  Apr 26, 2024    HDL    Super Hi International Holding Ltd.           -   \n",
       "\n",
       "  Shares Offered  \n",
       "1              -  \n",
       "2        3750000  \n",
       "3        6000000  \n",
       "4        5000000  \n",
       "5              -  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace df['Shares Offered'] non-numeric value to np.nan\n",
    "df['Shares Offered'] = pd.to_numeric(df['Shares Offered'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['filing Date'] = pd.to_datetime(df['filing Date'])\n",
    "df['filing Date'] = pd.to_datetime(df['filing Date'], format='%b %d, %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ipo_filings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_avg_price(price_str):\n",
    "    # Remove the dollar sign and split by '-'\n",
    "    prices = price_str.replace('$', '').split('-')\n",
    "    # Filter out empty strings and convert the remaining to float\n",
    "    prices = [float(price) for price in prices if price.strip()]\n",
    "    # Calculate the average if it's a range, return the single value, or None if no valid prices\n",
    "    if len(prices) == 2:\n",
    "        return sum(prices) / 2\n",
    "    elif len(prices) == 1:\n",
    "        return prices[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to the 'Price Range' column\n",
    "df['Avg_price'] = df['Price Range'].apply(extract_avg_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Price Range</th>\n",
       "      <th>Shares Offered</th>\n",
       "      <th>Avg_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>TBN</td>\n",
       "      <td>Tamboran Resources Corporation</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>HWEC</td>\n",
       "      <td>HW Electro Co., Ltd.</td>\n",
       "      <td>$3.00</td>\n",
       "      <td>3750000.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>DTSQ</td>\n",
       "      <td>DT Cloud Star Acquisition Corporation</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>6000000.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>EURK</td>\n",
       "      <td>Eureka Acquisition Corp</td>\n",
       "      <td>$10.00</td>\n",
       "      <td>5000000.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-04-26</td>\n",
       "      <td>HDL</td>\n",
       "      <td>Super Hi International Holding Ltd.</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filing Date Symbol                           Company Name Price Range  \\\n",
       "1  2024-05-03    TBN         Tamboran Resources Corporation           -   \n",
       "2  2024-04-29   HWEC                   HW Electro Co., Ltd.       $3.00   \n",
       "3  2024-04-29   DTSQ  DT Cloud Star Acquisition Corporation      $10.00   \n",
       "4  2024-04-26   EURK                Eureka Acquisition Corp      $10.00   \n",
       "5  2024-04-26    HDL    Super Hi International Holding Ltd.           -   \n",
       "\n",
       "   Shares Offered  Avg_price  \n",
       "1             NaN        NaN  \n",
       "2       3750000.0        3.0  \n",
       "3       6000000.0       10.0  \n",
       "4       5000000.0       10.0  \n",
       "5             NaN        NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./ipo_filings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if df['Avg_price'] and df['Shares Offered'] both are not null or np.nan, then calculate the 'Shares_offered_value'\n",
    "import numpy as np\n",
    "\n",
    "# Calculate 'Shares_offered_value' only where both 'Avg_price' and 'Shares Offered' are not NaN\n",
    "df['Shares_offered_value'] = np.where(df['Avg_price'].notna() & df['Shares Offered'].notna(),\n",
    "                                      df['Avg_price'] * df['Shares Offered'],\n",
    "                                      np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./ipo_filings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for entries from 2023 and on Fridays\n",
    "friday_filings_2023 = df[(df['filing Date'].dt.year == 2023) & (df['filing Date'].dt.dayofweek == 4)]\n",
    "\n",
    "# Calculate the sum of 'Shares_offered_value', convert to millions and round to the nearest integer\n",
    "total_sum_millions = int(round(friday_filings_2023['Shares_offered_value'].sum() / 1e6))\n",
    "\n",
    "total_sum_millions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:  IPOs \"Fixed days hold\" strategy\n",
    "\n",
    "\n",
    "**Find the optimal number of days X (between 1 and 30), where 75% quantile growth is the highest?**\n",
    "\n",
    "\n",
    "Reuse [Code Snippet 1] to retrieve the list of IPOs from 2023 and 2024 (from URLs: https://stockanalysis.com/ipos/2023/ and https://stockanalysis.com/ipos/2024/). Get all OHLCV daily prices for all stocks with an \"IPO date\" before March 1, 2024 (\"< 2024-03-01\") - 185 tickers. Sometimes you may need to adjust the symbol name (e.g., 'IBAC' on stockanalysis.com -> 'IBACU' on Yahoo Finance) to locate OHLCV prices for all stocks.\n",
    "\n",
    "Let's assume you managed to buy a new stock (listed on IPO) on the first day at the [Adj Close] price]. Your strategy is to hold for exactly X full days (where X is between 1 and 30) and sell at the \"Adj. Close\" price in X days (e.g., if X=1, you sell on the next day).\n",
    "Find X, when the 75% quantile growth (among 185 investments) is the highest. \n",
    "\n",
    "HINTs:\n",
    "* You can generate 30 additional columns: growth_future_1d ... growth_future_30d, join that with the table of min_dates (first day when each stock has data on Yahoo Finance), and perform vector operations on the resulting dataset.\n",
    "* You can use the `DataFrame.describe()` function to get mean, min, max, 25-50-75% quantiles.\n",
    "\n",
    "\n",
    "Addtional: \n",
    "* You can also ensure that the mean and 50th percentile (median) investment returns are negative for most X values, implying a wager for a \"lucky\" investor who might be in the top 25%.\n",
    "* What's your recommendation: Do you suggest pursuing this strategy for an optimal X?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def clawer(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 假設數據在一個表格內，找到該表格並提取數據\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        data.append([ele.text.strip() for ele in cols if ele])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['IPO Date', 'Symbol', 'Company Name', 'IPO Price', 'Current','Return'])\n",
    "    df = df.iloc[1:]  # 根據表格的實際布局調整索引\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://stockanalysis.com/ipos/2023/'\n",
    "df1 = clawer(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://stockanalysis.com/ipos/2024/'\n",
    "df2 = clawer(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate df1 and df2\n",
    "df = pd.concat([df1, df2])\n",
    "df['IPO Date'] = pd.to_datetime(df['IPO Date'])\n",
    "filtered_symbol_data = df[df['IPO Date'] < '2024-03-01']\n",
    "# df.to_csv('symbols.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_symbol_data.sort_values(by='IPO Date', ascending=True)\n",
    "filtered_symbol_data.to_csv('symbols.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "# 假設已經有了從網站上抓取的 IPO 數據和股票代碼的 DataFrame\n",
    "# 需要有以下列：'Ticker', 'IPO Date'\n",
    "symbol_data = pd.read_csv('symbols.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾 IPO 日期在 2024-03-01 之前的股票\n",
    "symbol_data['IPO Date'] = pd.to_datetime(symbol_data['IPO Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_symbol_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the row['Symbol'] == 'RYZB'\n",
    "filtered_symbol_data = filtered_symbol_data[filtered_symbol_data['Symbol'] != 'RYZB']\n",
    "# modify the row['Symbol'] == 'PTHR' to 'PTHRU'\n",
    "filtered_symbol_data['Symbol'] = filtered_symbol_data['Symbol'].replace('PTHR', 'HOVR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat symbol list\n",
    "symbol_list = list(filtered_symbol_data['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(symbol_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "# create download_ipo_data to download stock data\n",
    "def download_ipo_data(symbol_list):\n",
    "    dataframe = {}\n",
    "    for symbol in symbol_list:\n",
    "        symbol = symbol.upper()\n",
    "        df = yf.download(symbol, period='max',interval='1d')\n",
    "        if not df.empty:\n",
    "            dataframe[f'df_{symbol}'] = df\n",
    "        else:\n",
    "            print(f'no data found for {symbol}')\n",
    "    return dataframe\n",
    "\n",
    "dfs = download_ipo_data(symbol_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from filtered_symbol_data get IPO Date and Symbol, if i hold from IPO Date to IPO Date + 30 days, get the df_Symbol['Adj Close']   \n",
    "# calculate the 75% quantile growth\n",
    "# write a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_symbol_data = filtered_symbol_data.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_growth(filtered_symbol_data, dfs):\n",
    "    results = []\n",
    "    for index, row in filtered_symbol_data.iterrows():\n",
    "        symbol = row['Symbol']\n",
    "        ipo_date = pd.to_datetime(row['IPO Date'])  # Ensure ipo_date is a datetime object\n",
    "\n",
    "        df_symbol = dfs.get(f'df_{symbol}', pd.DataFrame())  # Get the dataframe for the symbol\n",
    "        if not df_symbol.empty:\n",
    "            # Ensure the DataFrame index is a DatetimeIndex\n",
    "            if not isinstance(df_symbol.index, pd.DatetimeIndex):\n",
    "                df_symbol.index = pd.to_datetime(df_symbol.index)\n",
    "\n",
    "            if ipo_date in df_symbol.index:\n",
    "                ipo_price = df_symbol.at[ipo_date, 'Adj Close']  # Use 'at' for scalar value\n",
    "                ipo_index = df_symbol.index.get_loc(ipo_date)\n",
    "            else:\n",
    "                # If IPO date is not in the index, use the first available date in the DataFrame\n",
    "                ipo_date = df_symbol.index.min()\n",
    "                if pd.isnull(ipo_date):\n",
    "                    print(f\"No initial trading data available for symbol {symbol}\")\n",
    "                    continue\n",
    "                ipo_price = df_symbol.at[ipo_date, 'Adj Close']\n",
    "                ipo_index = df_symbol.index.get_loc(ipo_date)\n",
    "\n",
    "            # Iterate over the next 30 trading days\n",
    "            for x in range(1, 31):  # Days from 1 to 30\n",
    "                future_index = ipo_index + x\n",
    "                if future_index < len(df_symbol.index):\n",
    "                    future_date = df_symbol.index[future_index]\n",
    "                    future_price = df_symbol.at[future_date, 'Adj Close']\n",
    "                    if not np.isnan(ipo_price) and not np.isnan(future_price):\n",
    "                        growth = (future_price - ipo_price) / ipo_price\n",
    "                        results.append({'Symbol': symbol, 'Days': x, 'Growth': growth})\n",
    "        else:\n",
    "            print(f\"No data found for symbol {symbol}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = calculate_growth(filtered_symbol_data, dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert results to dataframe\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Days</th>\n",
       "      <th>Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IROH</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IROH</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IROH</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IROH</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IROH</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>ROMA</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.506897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>ROMA</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.562069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>ROMA</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>ROMA</td>\n",
       "      <td>29</td>\n",
       "      <td>-0.472414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>ROMA</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.368966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5505 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol  Days    Growth\n",
       "0      IROH     1  0.000999\n",
       "1      IROH     2  0.000500\n",
       "2      IROH     3  0.000999\n",
       "3      IROH     4  0.000000\n",
       "4      IROH     5  0.000999\n",
       "...     ...   ...       ...\n",
       "5500   ROMA    26 -0.506897\n",
       "5501   ROMA    27 -0.562069\n",
       "5502   ROMA    28 -0.517241\n",
       "5503   ROMA    29 -0.472414\n",
       "5504   ROMA    30 -0.368966\n",
       "\n",
       "[5505 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 75% quantile growth for each day\n",
    "quantile_75_growth = results_df.groupby('Days')['Growth'].quantile(0.75).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days</th>\n",
       "      <th>Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.019670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.011578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.013020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.008295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.006871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.006941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.011545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.007391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.011155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.016016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.018676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.016284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.014190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.011330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.015276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.018163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.015145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.014209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.035524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.028015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.035345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.026972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.028256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.036537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.039693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.026362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.023718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Days    Growth\n",
       "0      1  0.017014\n",
       "1      2  0.019670\n",
       "2      3  0.011578\n",
       "3      4  0.013020\n",
       "4      5  0.008295\n",
       "5      6  0.006900\n",
       "6      7  0.006871\n",
       "7      8  0.006941\n",
       "8      9  0.011545\n",
       "9     10  0.007391\n",
       "10    11  0.011155\n",
       "11    12  0.016016\n",
       "12    13  0.018676\n",
       "13    14  0.019706\n",
       "14    15  0.016284\n",
       "15    16  0.014190\n",
       "16    17  0.011330\n",
       "17    18  0.015276\n",
       "18    19  0.018163\n",
       "19    20  0.015145\n",
       "20    21  0.014209\n",
       "21    22  0.035524\n",
       "22    23  0.028015\n",
       "23    24  0.035345\n",
       "24    25  0.026972\n",
       "25    26  0.028256\n",
       "26    27  0.036537\n",
       "27    28  0.039693\n",
       "28    29  0.026362\n",
       "29    30  0.023718"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_75_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of days X: 28.0, with 75% quantile growth: 0.03969330546173397\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Find the day with the highest 75% quantile growth\n",
    "optimal_day = quantile_75_growth.loc[quantile_75_growth['Growth'].idxmax()]\n",
    "\n",
    "print(f\"Optimal number of days X: {optimal_day['Days']}, with 75% quantile growth: {optimal_day['Growth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Is Growth Concentrated in the Largest Stocks?\n",
    "\n",
    "**Get the share of days (percentage as int) when Large Stocks outperform (growth_7d - growth over 7 periods back) the Largest stocks?**\n",
    "\n",
    "\n",
    "Reuse [Code Snippet 5] to obtain OHLCV stats for 33 stocks \n",
    "for 10 full years of data (2014-01-01 to 2023-12-31):\n",
    "\n",
    "`US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO','V', 'JPM']`\n",
    "\n",
    "`EU_STOCKS = ['NVO','MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE','IDEXY','CDI.PA']`\n",
    "\n",
    "`INDIA_STOCKS = ['RELIANCE.NS','TCS.NS','HDB','BHARTIARTL.NS','IBN','SBIN.NS','LICI.NS','INFY','ITC.NS','HINDUNILVR.NS','LT.NS']`\n",
    "\n",
    "`LARGEST_STOCKS = US_STOCKS + EU_STOCKS + INDIA_STOCKS`\n",
    "<br/>\n",
    "\n",
    "Now let's add the top 12-22 stocks (as of end-April 2024):\n",
    "<br/>\n",
    "\n",
    "`NEW_US = ['TSLA','WMT','XOM','UNH','MA','PG','JNJ','MRK','HD','COST','ORCL']`\n",
    "\n",
    "`NEW_EU = ['PRX.AS','CDI.PA','AIR.PA','SU.PA','ETN','SNY','BUD','DTE.DE','ALV.DE','MDT','AI.PA','EL.PA']`\n",
    "\n",
    "`NEW_INDIA = ['BAJFINANCE.NS','MARUTI.NS','HCLTECH.NS','TATAMOTORS.NS','SUNPHARMA.NS','ONGC.NS','ADANIENT.NS','ADANIENT.NS','NTPC.NS','KOTAKBANK.NS','TITAN.NS']`\n",
    "\n",
    "`LARGE_STOCKS = NEW_EU + NEW_US + NEW_INDIA`\n",
    "\n",
    "You should be able to obtain stats for 33 LARGEST STOCKS and 32 LARGE STOCKS.\n",
    "\n",
    "Calculate  `growth_7d` for every stock and every day.\n",
    "Get the average daily `growth_7d` for the LARGEST_STOCKS group vs. the LARGE_STOCKS group.\n",
    "\n",
    "For example, for the first of data you should have:\n",
    "| Date   |      ticker_category      |  growth_7d |\n",
    "|----------|:-------------:|------:|\n",
    "| 2014-01-01 |  LARGE | 1.011684 |\n",
    "| 2014-01-01 |   LARGEST   |   1.011797 |\n",
    "\n",
    "On that day, the LARGEST group was growing faster than LARGE one (new stocks).\n",
    "\n",
    "Calculate the number of days when the LARGE GROUP (new smaller stocks) outperforms the LARGEST GROUP, divide it by the total number of trading days (which should be 2595 days),\n",
    "and convert it to a percentage (closest INTEGER value). For example, \n",
    "if you find that 1700 out of 2595 days meet this condition, \n",
    "it means that 1700/2595 = 0.655, or approximately 66% of days, \n",
    "the LARGE stocks were growing faster than the LARGEST ones. \n",
    "This suggests that you should consider extending your dataset with more stocks to seek higher growth.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of days when LARGE stocks outperformed LARGEST stocks: 46.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/n3/qnzrd2254zxfjlr0q6_j679m0000gn/T/ipykernel_20404/3296506422.py:22: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  return data.pct_change(7)  # 7天的百分比變化\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 定義股票列表\n",
    "largest_stocks = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO', 'V', 'JPM',\n",
    "                  'NVO', 'MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE', 'IDEXY', 'CDI.PA',\n",
    "                  'RELIANCE.NS', 'TCS.NS', 'HDB', 'BHARTIARTL.NS', 'IBN', 'SBIN.NS', 'LICI.NS', 'INFY', 'ITC.NS', 'HINDUNILVR.NS', 'LT.NS']\n",
    "large_stocks = ['TSLA', 'WMT', 'XOM', 'UNH', 'MA', 'PG', 'JNJ', 'MRK', 'HD', 'COST', 'ORCL',\n",
    "                'PRX.AS', 'CDI.PA', 'AIR.PA', 'SU.PA', 'ETN', 'SNY', 'BUD', 'DTE.DE', 'ALV.DE', 'MDT', 'AI.PA', 'EL.PA',\n",
    "                'BAJFINANCE.NS', 'MARUTI.NS', 'HCLTECH.NS', 'TATAMOTORS.NS', 'SUNPHARMA.NS', 'ONGC.NS', 'ADANIENT.NS', 'NTPC.NS', 'KOTAKBANK.NS', 'TITAN.NS']\n",
    "\n",
    "# 下載數據\n",
    "def download_data(stocks):\n",
    "    all_data = {}\n",
    "    for stock in stocks:\n",
    "        all_data[stock] = yf.download(stock, start='2014-01-01', end='2023-12-31')['Adj Close']\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# 計算成長率\n",
    "def calculate_growth(data):\n",
    "    return data.pct_change(7)  # 7天的百分比變化\n",
    "\n",
    "# 載入數據\n",
    "largest_data = download_data(largest_stocks)\n",
    "large_data = download_data(large_stocks)\n",
    "\n",
    "# 計算成長率\n",
    "growth_largest = calculate_growth(largest_data)\n",
    "growth_large = calculate_growth(large_data)\n",
    "\n",
    "# 計算每日平均成長率\n",
    "daily_growth_largest = growth_largest.mean(axis=1)\n",
    "daily_growth_large = growth_large.mean(axis=1)\n",
    "\n",
    "# 計算大型股票組超過最大股票組的天數\n",
    "outperform_days = (daily_growth_large > daily_growth_largest).sum()\n",
    "\n",
    "# 計算百分比\n",
    "total_days = len(daily_growth_large.dropna())  # 去除 NaN 值\n",
    "percentage = round((outperform_days / total_days) * 100,2)\n",
    "\n",
    "print(f\"Percentage of days when LARGE stocks outperformed LARGEST stocks: {percentage}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Trying Another Technical Indicators strategy\n",
    "\n",
    "**What's the total gross profit (in THOUSANDS of $) you'll get from trading on CCI (no fees assumption)?**\n",
    "\n",
    "\n",
    "First, run the entire Colab to obtain the full DataFrame of data (after [Code Snippet 9]), and truncate it to the last full 10 years of data (2014-01-01 to 2023-12-31).\n",
    "If you encounter any difficulties running the Colab - you can download it using this [link](https://drive.google.com/file/d/1m3Qisfs2XfWk6Sw_Uk5kHLWqwQ0q8SKb/view?usp=sharing).\n",
    "\n",
    "Let's assume you've learned about the awesome **CCI indicator** ([Commodity Channel Index](https://www.investopedia.com/terms/c/commoditychannelindex.asp)), and decided to use only it for your operations.\n",
    "\n",
    "You defined the \"defensive\" value of a high threshould of 200, and you trade only on Fridays (`Date.dt.dayofweek()==4`).\n",
    "\n",
    "That is, every time you see that CCI is >200 for any stock (out of those 33), you'll invest $1000 (each record when CCI>200) at Adj.Close price and hold it for 1 week (5 trading days) in order to sell at the Adj. Close price.\n",
    "\n",
    "What's the expected gross profit (no fees) that you get in THOUSANDS $ (closest integer value) over many operations in 10 years?\n",
    "One operation calculations: if you invested $1000 and received $1010 in 5 days - you add $10 to gross profit, if you received $980 - add -$20 to gross profit.\n",
    "You need to sum these results over all trades (460 times in 10 years).\n",
    "\n",
    "Additional:\n",
    "  * Add an approximate fees calculation over the 460 trades from this calculator https://www.degiro.ie/fees/calculator (Product:\"Shares, USA and Canada;\" Amount per transaction: \"1000 EUR\"; Transactions per year: \"460\")\n",
    "  * are you still profitable on those trades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# table = pq.read_table('stocks_df_combined_trunc_2014_2023.parquet.brotli')\n",
    "# df = table.to_pandas()\n",
    "# df.to_csv(r'./stocks_df_combined_trunc_2014_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cci(data, n=20):\n",
    "    data = data.copy()\n",
    "    # Convert columns to numeric, ignoring errors\n",
    "    data['High'] = pd.to_numeric(data['High'], errors='coerce')\n",
    "    data['Low'] = pd.to_numeric(data['Low'], errors='coerce')\n",
    "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "    # data = data[(data.index >= '2014-01-01') & (data.index <= '2023-12-31')]\n",
    "\n",
    "    # Proceed with the CCI calculation as above\n",
    "    data['TP'] = (data['High'] + data['Low'] + data['Close']) / 3\n",
    "    data['MA'] = data['TP'].rolling(window=n).mean()\n",
    "    data['MD'] = (data['TP'] - data['MA']).abs().rolling(window=n).mean()\n",
    "\n",
    "\n",
    "    data['CCI'] = (data['TP'] - data['MA']).div(0.015 * data['MD'])\n",
    "    data.fillna(0,inplace=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "trades = []\n",
    "def simulate_trades(cci_df):\n",
    "    # Check if 'Date' is a column and convert it to datetime and set as index\n",
    "    if 'Date' in cci_df.columns:\n",
    "        cci_df['Date'] = pd.to_datetime(cci_df['Date'])\n",
    "        cci_df.set_index('Date', inplace=True)\n",
    "    elif cci_df.index.name != 'Date' or not pd.api.types.is_datetime64_any_dtype(cci_df.index):\n",
    "        # If 'Date' is not the index or the index is not datetime, raise an error\n",
    "        raise ValueError(\"DataFrame index must be 'Date' of datetime type\")\n",
    "    \n",
    "    # Rename column Adj Close_x to Adj Close if necessary\n",
    "    if 'Adj Close_x' in cci_df.columns:\n",
    "        cci_df.rename(columns={'Adj Close_x': 'Adj Close'}, inplace=True)\n",
    "    \n",
    "    # Iterate over the DataFrame rows\n",
    "    for index, row in cci_df.iterrows():\n",
    "        if index.weekday() == 4 and row['CCI'] > 200:  # Only consider trades on Fridays\n",
    "            buy_price = row['Adj Close']\n",
    "            # Calculate sell_date as 5 trading days later\n",
    "            # use the following 5 rows to calculate the sell_date\n",
    "            sell_date = cci_df.loc[index:index + pd.Timedelta(days=5)].index[-1]\n",
    "            if sell_date in cci_df.index:\n",
    "                sell_price = cci_df.loc[sell_date, 'Adj Close']\n",
    "                profit = (sell_price - buy_price) * 1000 / buy_price\n",
    "                trades.append(profit)\n",
    "    return trades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815.0775047679762\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'./stocks_df_combined_trunc_2014_2023.csv')\n",
    "df = df[['Ticker','Date','High','Low','Close','Adj Close_x']]\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')\n",
    "# tickers = ['MSFT', 'AAPL', 'GOOG']  # test\n",
    "US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO','V', 'JPM']\n",
    "EU_STOCKS = ['NVO','MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE','IDEXY','CDI.PA']\n",
    "INDIA_STOCKS = ['RELIANCE.NS','TCS.NS','HDB','BHARTIARTL.NS','IBN','SBIN.NS','LICI.NS','INFY','ITC.NS','HINDUNILVR.NS','LT.NS']\n",
    "tickers = US_STOCKS + EU_STOCKS + INDIA_STOCKS\n",
    "\n",
    "for ticker in tickers:\n",
    "    data = df[df['Ticker'] == f'{ticker}']\n",
    "    cci_df = calculate_cci(data, n=14)\n",
    "    simulate_trades(cci_df)\n",
    "print(sum(trades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "print(len(trades))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [EXPLORATORY] Question 5: Finding Your Strategy for IPOs\n",
    "\n",
    "You've seen in the first questions that the median and average investments are negative in IPOs, and you can't blindly invest in all deals.\n",
    "\n",
    "How would you correct/refine the approach? Briefly describe the steps and the data you'll try to get (it should be generally feasible to do it from public sources - no access to internal data of companies)?\n",
    "\n",
    "E.g. (some ideas) Do you want to focus on the specific vertical? Do you want to build a smart comparison vs. existing stocks on the market? Or you just will want to get some features (which features?) like total number of people in a company to find a segment of \"successful\" IPOs?\n",
    "\n",
    "---\n",
    "## Submitting the solutions\n",
    "\n",
    "Form for submitting: https://courses.datatalks.club/sma-zoomcamp-2024/homework/hw02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry, I don't have enough time or experience to complete the question. Perhaps I'll consider a different strategy for trading large companies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
