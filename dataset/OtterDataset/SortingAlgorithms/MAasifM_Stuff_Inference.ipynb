{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, Pipeline, PreTrainedTokenizer\n",
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "\n",
    "class ModelPipelineLoader:\n",
    "    def __init__(self, model_id: str) -> None:\n",
    "        \"\"\"\"\"\"\n",
    "        self._tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id)\n",
    "        self._model: Any = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, load_in_8bit=True)\n",
    "        self._pipeline: Pipeline | None = None\n",
    "\n",
    "        self._parameters: dict = {\n",
    "            'torch_dtype': 'torch.bfloat16',\n",
    "            'device_map': 'auto',\n",
    "            'max_new_tokens': 8192,\n",
    "        }\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.model}, {self.tokenizer}'\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def pipeline(self):\n",
    "        return self._pipeline\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "\n",
    "    def get_pipeline(self) -> Pipeline:\n",
    "        \"\"\"\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self._pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                torch_dtype=self.parameters['torch_dtype'],\n",
    "                device_map=self.parameters['device_map'],\n",
    "                max_new_tokens=self.parameters['max_new_tokens'],\n",
    "                eos_token_id=self._tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        return self.pipeline\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            del self._tokenizer\n",
    "            del self._model\n",
    "            torch.cuda.empty_cache()\n",
    "            del self\n",
    "        except:\n",
    "            print(\"Already destroyed.\")\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            del self._tokenizer\n",
    "            del self._model\n",
    "            torch.cuda.empty_cache()\n",
    "            del self\n",
    "        except:\n",
    "            print(\"Already destroyed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dd9056ae0e45069ad68ef14138e35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = '/mnt/sda/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/'\n",
    "\n",
    "pipe = ModelPipelineLoader(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<<SYS>>\\n\\n\"\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in Python programming. Use the provided context and your knowledge to assist users with Python-related queries.\n",
    "Read the context carefully before providing answers and guide users through Python coding steps.\n",
    "If you cannot answer a question solely based on the provided context, inform the user accordingly.\n",
    "Your responses should contain Python code and explanations tailored to the given context.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=system_prompt):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe.get_pipeline(),\n",
    "                          model_kwargs={'temperature': 0})\n",
    "instruction = \"{text}\"\n",
    "\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "\n",
      "You are an expert in Python programming. Use the provided context and your knowledge to assist users with Python-related queries.\n",
      "Read the context carefully before providing answers and guide users through Python coding steps.\n",
      "If you cannot answer a question solely based on the provided context, inform the user accordingly.\n",
      "Your responses should contain Python code and explanations tailored to the given context.\n",
      "\n",
      "<<SYS>>\n",
      "\n",
      "Write a code to for insertion sort algorithm.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"  Sure, I'd be happy to help you with the insertion sort algorithm in Python!\\n\\nInsertion sort is a simple sorting algorithm that works by iterating through the list of items to be sorted, and inserting each item into its proper position in the sorted list. Here is an example of how you could implement insertion sort in Python:\\n```\\ndef insertion_sort(arr):\\n    n = len(arr)\\n    for i in range(1, n):\\n        smallest_index = i\\n        for j in range(i):\\n            if arr[j] > arr[smallest_index]:\\n                smallest_index = j\\n        arr[smallest_index] = arr[i]\\n    return arr\\n```\\nThis function takes an unsorted list of items as input, and returns the sorted list.\\n\\nTo use this function, you can call it with a list of items to be sorted, like this:\\n```\\narr = [5, 2, 8, 3, 1, 6]\\nsorted_arr = insertion_sort(arr)\\nprint(sorted_arr)  # [1, 2, 3, 5, 6, 8]\\n```\\nI hope this helps! Let me know if you have any questions.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"Write a code to for insertion sort algorithm.\"\n",
    "llm_response = llm_chain({\"text\": user_prompt})\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_string(text):\n",
    "    # Find the index of the first occurrence of triple backticks\n",
    "    start_index = text.find(\"```\")\n",
    "    # Find the second occurrence\n",
    "    end_index = text.find(\"```\", start_index + 3) if start_index != -1 else -1\n",
    "\n",
    "    # Extract the code snippet between the triple backticks\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        code_snippet = text[start_index + 3:end_index].strip()\n",
    "        return code_snippet\n",
    "    else:\n",
    "        return \"Code snippet not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def insertion_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(1, n):\n",
      "        smallest_index = i\n",
      "        for j in range(i):\n",
      "            if arr[j] > arr[smallest_index]:\n",
      "                smallest_index = j\n",
      "        arr[smallest_index] = arr[i]\n",
      "    return arr\n"
     ]
    }
   ],
   "source": [
    "response = extract_code_from_string(llm_response['text'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistralenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
