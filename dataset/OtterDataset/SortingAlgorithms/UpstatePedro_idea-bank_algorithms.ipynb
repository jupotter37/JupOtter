{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures & Algorithms\n",
    "\n",
    "Notes from reading *Grokking Algorithms* by Aditya Bhargava"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays & Linked Lists\n",
    "\n",
    "When working with data, we often need to work with (ordered) sequences of objects. There is more than one approach structuring this data. Two common approaches are:\n",
    "1. Arrays\n",
    "2. Linked lists\n",
    "\n",
    "### Arrays\n",
    "\n",
    "When a new array is created, we reserve a contiguous block of memory to house the contents. Each address in the block stores a reference to the object that is placed at the corresponding element of the array.\n",
    "\n",
    "Because we have a pre-defined block of memory, we always know which address we need to access to retrieve an element of the array. We therefore get fast reads of the contents of the array, even when we're asking for arbitrary elements eg. `elem = l[999]`.\n",
    "\n",
    "However, over time, we may wish to change the contents of our array: for example, as we receive new values in a time series, we may need to add new elements to the end of our array. The memory block that we had reserved may now be full and we therefore need to reserve additional addresses to house all of the elements. If neighbouring addresses are already being used by other programmes, then we will need to provision a new contiguous memory block and relocate the entire array in order to keep all the elements together. \n",
    "\n",
    "We can mitigate the cost of having to relocate the array when its size changes by reserving some buffer in the blocks that we request - this way we can add new elements to the array without having to move the whole array.\n",
    "BUT this redundancy is also a drain on memory (noone else can use it) and doesn't completely remove the problem - we can still use up the buffer and find ourselves needing to relocate the array.\n",
    "\n",
    "### Linked lists\n",
    "\n",
    "Linked lists allow us to avoid this need for relocating arrays when we add new elements. The elements of a linked list are not stored in a contiguous block - they can each be stored in arbitrary positions on the disk.\n",
    "In order to keep track of all these positions, each element is responsible for storing the location of the next element in the LL. In this way, adding a new element is easy: you store it anywhere in memory, and then store it's location with the previous element. As a result, we never need to worry about having to relocate the whole LL.\n",
    "\n",
    "The downside with LLs is that in order to retrieve an element from an arbitrary position in the LL, we still have to start at the beginning and work our way through each element to get there (because each element holds the only record of the address of the next element).\n",
    "\n",
    "Thus, LLs are great for scenarios where we want to retrieve each and every element in order. But they're far from ideal if we are more likely to be requesting individual elements from arbitrary positions in the collection.\n",
    "\n",
    "\n",
    "### Performance\n",
    "\n",
    "| Operation | Array    | Linked list |\n",
    "|:-----     |:-----:   |:-----:      |\n",
    "| Insert    | $O(1)$   | $O(n)$      |\n",
    "| Search    | $O(n)$   | $O(1)$      |\n",
    "| Delete    | $O(n)$   | $O(1)$      |\n",
    "\n",
    "\n",
    "- Prefer **arrays** when the contents don't change size frequently\n",
    "- Prefer **arrays** when we often won't read from the sequence in order, when we'll typically want to retrieve values from arbitrary slices / indices\n",
    "- Prefer **linked lists** when we typically read (all) elements, in order\n",
    "- Prefer **linked lists** when we regularly change the size of the collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash tables\n",
    "\n",
    "> AKA hash map, map, dictionary & associative array\n",
    "\n",
    "- implementation\n",
    "- collisions\n",
    "- hash functions\n",
    "\n",
    "Hash tables map inputs to outputs as key, value pairs. They are composed of:\n",
    "1. Hash function\n",
    "2. Array (for storage of values)\n",
    "\n",
    "\n",
    "The input is a sequence of bytes that acts as a key to access the value, and the hash table returns the value each time that key is provided.\n",
    "A hash function takes a key as input and returns the index of the array at which the corresponding value is stored. (ie. it doesn't return the value directly, but it tells us where to store / find it).\n",
    "\n",
    "No duplicate keys allowed.\n",
    "\n",
    "### Collisions\n",
    "\n",
    "Sometimes a hash function will assign two different keys to the same bucket. This will overwrite values and could cause the hash table to return the wrong value for a key.\n",
    "\n",
    "One way around this is to create a linked-list at a slot whenever multiple keys map to it.\n",
    "But this can make hash tables inefficient if the collisions are not distributed evenly through our data (we could end up with many data points in a single linked list, and few values in the remaining array slots).\n",
    "\n",
    "### Performance\n",
    "\n",
    "| Operation | Avg | Worst |\n",
    "|:-----|:-----|:-----|\n",
    "| Insert | $O(1)$ | $O(n)$ |\n",
    "| Search | $O(1)$ | $O(n)$ |\n",
    "| Delete | $O(1)$ | $O(n)$ |\n",
    "\n",
    "\n",
    "Eventually, the table will grow so large that collisions will occur, and the performance rules will break-down.\n",
    "\n",
    "### Load factors & resizing\n",
    "\n",
    "The likelihood of collisions is measured with the **load factor**:\n",
    "\n",
    "load factor = (Num. items in the hash table) / (Num. slots available)\n",
    "\n",
    "As the load factor increases, we may need to add slots to the values array; this is called \"resizing\".\n",
    "\n",
    "Resizing is often triggered once the load factor exceeds 0.7.\n",
    "\n",
    "To resize the values array:\n",
    "- Create a new array (often double the size)\n",
    "- Re-assign all exisiting items to the new array using the hash function\n",
    "\n",
    "Resizing is expensive, but reading hash tables is still $O(1)$ once averaged-out\n",
    "\n",
    "### Hash functions\n",
    "\n",
    "A good hash function distributes values evenly across array slots - this reduces the likelihood of collisions\n",
    "\n",
    "An example of a hash function: SHA\n",
    "\n",
    "Hash functions must exhibit two properties:\n",
    "1. Consistency: Each time the same key is provided, it should unlock the same value\n",
    "2. Divergence: Each time a different key is provided, it should return a different array index\n",
    "\n",
    "> Common hash functions & how they work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an algorithm?\n",
    "\n",
    "An algorithm is a set of instructions for accomplishing a task.\n",
    "\n",
    "There are often many ways to accomplish a task, and so we need to understand the trade-offs in order to know how best to accomplish a task in a given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A motivating example: binary search\n",
    "\n",
    "Imagine that we need to find an element in a sorted list. More specifically, we want to return the position of the desired element in that list, or `null` if it cannot be found.\n",
    "\n",
    "**Simple search** might be the most obvious & basic attempt to solve that task. It starts at the first element, and moves through them one-by-one, in sequence, until it finds the right answer. If our desired value happens to be the first element, then we're in luck. If it isn't, and our list is long, then we might have to check a lot of values before we get our answer...\n",
    "\n",
    "**Binary search** dramatically reduces the number of values we have to check before we find the position of our desired element. At each step it selects the mid-point of the remaining elements and establishes whether that location is too low or too high in order to determine which elements it can remove from consideration, and which elements are still contenders (remember, it's a sorted list). This process is repeated with each step until we find the target value (or exhaust the possibilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def simple_search(sorted_elements: list, target_value: int) -> Union[int, None]:\n",
    "    for index, element in enumerate(sorted_elements):\n",
    "        if element == target_value:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "6\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sorted_list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "print(simple_search(sorted_list, 3))\n",
    "print(simple_search(sorted_list, 6))\n",
    "print(simple_search(sorted_list, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 guesses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_search(sorted_elements: list, target_value: int) -> Union[int, None]:\n",
    "    search_boundaries = {'low': 0, 'high': len(sorted_elements) - 1}\n",
    "    guesses = 0\n",
    "    while search_boundaries['low'] <= search_boundaries['high']:\n",
    "        mid_point = int((search_boundaries['low'] + search_boundaries['high']) / 2)\n",
    "        midpoint_value = sorted_elements[mid_point]\n",
    "        guesses += 1\n",
    "        if midpoint_value == target_value:\n",
    "            print(f\"{guesses} guesses\")\n",
    "            return mid_point\n",
    "        if midpoint_value > target_value:\n",
    "            search_boundaries['high'] = mid_point - 1\n",
    "        else:\n",
    "            search_boundaries['low'] = mid_point + 1\n",
    "    return\n",
    "\n",
    "binary_search([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Sort\\*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion\n",
    "\n",
    "A recursive recipe must contain three ingredients:\n",
    "\n",
    "1. Stopping criteria\n",
    "2. A first step (gets the ball rolling)\n",
    "3. A repetitive component (that will lead us to the stopping criteria) ie. a function that calls itself.\n",
    "\n",
    "Recursion does not improve computational performance, but may be faster to write. It should be used when it helps to make the solution easier to intuit.\n",
    "\n",
    "### Structure\n",
    "\n",
    "In order to avoid infinite loops of recursion, each recursive method needs to have two components:\n",
    "1. A base case: catches the stopping criteria & breaks the loop\n",
    "2. A recursive case: takes the recursion a layer deeper\n",
    "\n",
    "### The call stack\n",
    "\n",
    "The *stack* data structure acts like a LIFO inventory: new items go to the top of the pile as they're added and we remove those most recent items from the pile first as we work through it.\n",
    "\n",
    "The **call stack** is a stack data structure that our computers use to keep track of function calls that are WIP:\n",
    "- New function calls are added to the stack (memory is allocated to that call, and variables in that scope are saved to that memory)\n",
    "- Those function calls are removed from the stack as they return their results\n",
    "\n",
    "Because recursion involves calling the same function many times, with a chain of dependency between each call, it can lead to a lot of function calls being added to the call stack. With enough calls, or large state stored for each call, recursion can exhaust a machine's memory.\n",
    "\n",
    "When this occurs, there are two options:\n",
    "1. Use a loop instead\n",
    "2. Use tail recursion (which only some languages support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide & Conquer\n",
    "\n",
    "**Divide & Conquer** is a general technique for solving problems that uses recursion.\n",
    "\n",
    "1. Identify simple conditions in which the problem can be solved.\n",
    "2. Break the problem down so that you are only trying to solve it for increasingly small / simple inputs; continue until you reach a situation where you only need to deal with the simple conditions above. \n",
    "\n",
    "> NB. When working with recursion on problems involving arrays, the base / simple case is often a array of length 0 or 1.\n",
    "\n",
    "Preffering the recursive approach over loops is typical of **functional programming** - Haskell doesn't even have loops!\n",
    "\n",
    "The Binary Search algorithm that we saw earlier is also a type of D&C solution which can be coded using recursion. See the re-write of our earlier function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_binary_search(arr: list, target: int, low_idx: int, high_idx: int, guesses: int = 0) -> Union[int, None]:\n",
    "    if not low_idx < high_idx:\n",
    "        return None\n",
    "    guesses += 1\n",
    "    # Base case\n",
    "    mid_point = int(low_idx + (high_idx - low_idx) / 2)\n",
    "    print('mid: ', mid_point)\n",
    "    midpoint_value = arr[mid_point]\n",
    "    if midpoint_value == target:\n",
    "        print(f\"{guesses} guesses\")\n",
    "        return mid_point\n",
    "    elif midpoint_value > target:\n",
    "        return recursive_binary_search(arr, target, low_idx, mid_point-1, guesses=guesses)\n",
    "    else:\n",
    "        return recursive_binary_search(arr, target, mid_point+1, high_idx, guesses=guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid:  10\n",
      "mid:  4\n",
      "2 guesses\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "recursive_binary_search(arr, 4, 0, len(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quicksort\n",
    "\n",
    "Quicksort - as the name suggests - is a sorting algorithm that uses D&C.\n",
    "\n",
    "We'll work through it using the example of sorting an array:\n",
    "\n",
    "**1. Identify simple conditions in which the problem can be solved.**\n",
    "\n",
    "The simplest situation we can hope to work with is a case where the array is either empty, or only has one element: in this scenario, we can just return the array as it is because there's no sorting to be done!\n",
    "\n",
    "This could look something like:\n",
    "```python\n",
    "def quicksort(arr):\n",
    "    if len(arr) < 2:\n",
    "        return arr\n",
    "```\n",
    "\n",
    "The next simplest situation we can have is an array with 2 elements. In this scenario, if the first element is larger than the second, then swap them around and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quicksort(arr):\n",
    "    if len(arr) < 2:\n",
    "        return arr\n",
    "    if len(arr) == 2:\n",
    "        return arr if arr[0] < arr[1] else arr[::-1]\n",
    "    \n",
    "quicksort([2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Then break the problem down until you reach those conditions.**\n",
    "\n",
    "And now we need to think through how we'd approach longer arrays. With 3 elements, we can no longer do the direct comparison above. We need to Divide & Conquer... How could we break up the process so that we end up only having to deal with many instances of the base case?\n",
    "\n",
    "One approach could be to:\n",
    "- Select an element; we'll call our selected element the ***pivot***\n",
    "- Work through & throw all other elements into one of two buckets (leaving three in total, once the pivot is included): those less than the pivot, and those greater than the pivot\n",
    "- For each of the non-pivot buckets, choose a new pivot and repeat\n",
    "- Eventually, each non-pivot bucket will simplify to an array of 2 or fewer elements, and from these we can build a complete array of sorted elements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1]\n",
      "[5, 4]\n"
     ]
    }
   ],
   "source": [
    "lesser = []\n",
    "greater = []\n",
    "arr = [5,4,3,2,1]\n",
    "pivot = 3\n",
    "\n",
    "for elem in arr:\n",
    "    lesser.append(elem) if elem <= pivot else greater.append(elem)\n",
    "    \n",
    "print(lesser)\n",
    "print(greater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort(arr: list):\n",
    "    if len(arr) < 2:\n",
    "        return arr\n",
    "    if len(arr) == 2:\n",
    "        return arr if arr[0] < arr[1] else arr[::-1]\n",
    "    \n",
    "    lesser = []\n",
    "    greater = []\n",
    "    pivot = arr.pop()\n",
    "    for elem in arr:\n",
    "        lesser.append(elem) if elem <= pivot else greater.append(elem)\n",
    "    \n",
    "    return quicksort(lesser) + [pivot] + quicksort(greater)\n",
    "    \n",
    "\n",
    "assert quicksort([1, 2]) == [1, 2]\n",
    "assert quicksort([1, 2, 3]) == [1, 2, 3], f\"returns {quicksort([1,2,3])}\"\n",
    "assert quicksort([7, 6, 5, 4, 2, 3, 1]) == [1, 2, 3, 4, 5, 6, 7], f\"returns {quicksort([1,2,3,4,5,6,7])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on complexity\n",
    "\n",
    "The speed with which quicksort sorts an array depends on which pivots we choose:\n",
    "- In the worst case, the complexity of running quicksort is $O(n^{2})$ (ie. as bad as **selection sort**)\n",
    "- In the average case, the complexity of running quicksort is $O(nlog(n))$\n",
    "\n",
    "The worst case occurs if we manage to always select our pivot such that all remaining elements lie in just one partition (ie. all are lesser than, or greater than, the pivot). This results in the worst case, because we're effectively removing one item from the array at a time, rather than breaking it into many chunks. This would occur when the array passed in is already sorted and we select the first element as pivot on each split.\n",
    "\n",
    "To avoid the worst case, and aim for the average case, we usually select the pivot element by random each split.\n",
    "\n",
    "In contrast, the best case occurs if each pivot is bang in the middle of the array, so the partitions are split equally - which speeds up how quickly we break all branches down to a base case scenario. In this case, the runtime complexity is $O(log(n))$.\n",
    "\n",
    "The average case complexity is equal to the best case complexity! (the runtime will be slower because we're unlikely to get exactly the ideal splits every time, but it scales with $n$ in the same order of complexity).\n",
    "\n",
    "### Parallelism?\n",
    "\n",
    "Looking at the shape of the quicksort method we've written above, it's interesting to see that not only are we using recursion, we're using recursion twice (in that last line).\n",
    "The fact that we're breaking the problem down into many branching pieces more quickly (by calling it twice, rather than having a single chain of many dependent calls) suggests that there may be more opportunity for parallel processing to help us here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sort\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg-case Vs Worst case\n",
    "\n",
    "In all the complexity notation, there's a hidden constant; figuratively and literally...\n",
    "\n",
    "In all cases, the runtime of an algorithm depends not just on $n$, but also $c$: the time taken to compute the required operations for each element of $n$.\n",
    "For some algorithms, the order of complexity may be higher than others, but $c$ may be small enough that it's still faster to use for the size of $n$ you're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "\n",
    "| Algorithm     | Worst-case runtime complexity      |\n",
    "|:--------------|:-------------:|\n",
    "| Binary search | $O(log(n))$ |\n",
    "| Selection sort| $O(n^{2})$   |\n",
    "| Quicksort     |          |\n",
    "| Merge sort    |          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs & graph-search\n",
    "\n",
    "### Graphs\n",
    "\n",
    "Graphs are composed of the following element types:\n",
    "- Nodes\n",
    "- Edges\n",
    "- (Directions)\n",
    "\n",
    "Neighbours are nodes which are directly connected by an edge. These are \"first-degree\" connections.\n",
    "Nodes that are connected to our first-degree connections (but not to us), are \"second-degree\" connections.\n",
    "\n",
    "### Shortest path problems & Breadth-first search (BFS)\n",
    "\n",
    "Shortest path problems involve finding the shortest route between two nodes in a graph.\n",
    "\n",
    "BFS is the algorithm most commonly used to solve these types of problems.\n",
    "\n",
    "\"Breadth-first\" refers to the fact that we exhaustively search each degree of connections before searching any connections with a higher degree.\n",
    "In BFS, the nodes are not weighted.\n",
    "\n",
    "To implement this kind of search, we can use a Queue data structure:\n",
    "- **Queues** are similar to **stack** data structures; the difference is in the order with which items are removed from the list\n",
    "- **Stacks** provide the most recently added item when removing items: LIFO\n",
    "- **Queues** provide the earliest-added item when removing items: FIFO\n",
    "\n",
    "> When we queue for the till whilst shopping, the **f**irst person to join is the **f**irst person to get served\n",
    "\n",
    "> Whereas, if I make a **stack** of books, I can only (easily) remove the book at the top - the **l**ast one to be added to the stack is the **f**irst one to be used\n",
    "\n",
    "Queues support two operations:\n",
    "1. `enqueue`: add an item to the queue\n",
    "2. `dequeue`: remove an item from the queue\n",
    "\n",
    "\n",
    "### Representing graphs in data structures\n",
    "\n",
    "If we're working with directed graphs without weights, then our data structure only needs to accommodate nodes & edges (connections). We can do this with a hash table:\n",
    "- Keys: one for each node\n",
    "- Values: a list of nodes that each key-node has a directed connection to\n",
    "\n",
    "Imagine we have a collection of bus stops, connected by sections of several bus routes.\n",
    "- Each bus stop is a node,\n",
    "- Each section of a route between two stops is an edge\n",
    "\n",
    "<diagram>\n",
    "\n",
    "If we start at stop 1, is there a route to stop 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_stop_graph = {\n",
    "    1: [2,3],\n",
    "    2: [3,4],\n",
    "    3: [4,5],\n",
    "    4: [5],\n",
    "    5: []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def search_routes(start, destination):\n",
    "    # create a double-ended queue (FIFO)\n",
    "    search_roster = deque()\n",
    "    # add the first degree of route sections to the stack\n",
    "    search_roster += bus_stop_graph[start]\n",
    "    # Keep track of stops we've already addded to the stack (avoid duplicates / inf. loops)\n",
    "    searched_stops = []\n",
    "    \n",
    "    while search_roster: # keep searching so long as there are still items in the search roster\n",
    "        next_stop = search_roster.popleft() # take the first/earliest-added item from the stack\n",
    "        if next_stop not in searched_stops:\n",
    "            # is it our destination?\n",
    "            if next_stop == destination:\n",
    "                return True\n",
    "            else:\n",
    "                # otherwise, add its route sections to our search list\n",
    "                search_roster += bus_stop_graph[next_stop]\n",
    "                # and add it to searched_stops\n",
    "                searched_stops.append(next_stop)\n",
    "    return False # If we exhaust the stack without finding the destination, then there was no route from 'start' to 'destination'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert search_routes(start=1, destination=5)\n",
    "assert search_routes(start=2, destination=5)\n",
    "assert not search_routes(start=5, destination=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "In the worst case, we'll end up searching through all connections / edges $O(Edges)$\n",
    "And we'll need to add each to add every node to the search list (each add op is $O(1)$, so this works out to $O(Nodes)$\n",
    "Combined then, we're facing $O(Nodes + Edges)$\n",
    "\n",
    "\\* On that implementation, the BFS confirms whether or not there is a path from start to destination, and it finds it efficiently. But it doesn't tell me whih path to take. What's the best way of tracking that!?\n",
    "\n",
    "### Trees are graphs\n",
    "\n",
    "As a side-note, ***Trees*** are Directed graphs that don't point back [and don't share connections across branches?]\n",
    "\n",
    "## Dijkstra's Algorithm\n",
    "\n",
    "Working with the Breadth-First Search algo was appropriate when:\n",
    "1. We want to find the path with the smallest number of edges between two nodes\n",
    "2. We are **NOT** considering weights on the edges\n",
    "\n",
    "> If we want to consider weights (ie. find the smallest weighted 'distance' between two nodes) then we should use Dijkstra's algorithm.\n",
    "\n",
    "Dijkstra's algorithm makes the assumption that there is no 'cheaper' route to the cheapest first-degree connection from a given node. This is because the first edge you'd have to travel along on any other path would already have a higher cost than the cheapest - so long as ALL weights are positive. If weights can be negative, then a 2nd-degree edge could more than offset the higher cost of the first, and thereby offer a cheaper path.\n",
    "\n",
    "\\* Bellman-Ford Vs Dijikstra?\n",
    "\n",
    "\\* What to do if multiple edges have the same (cheapest) cost?\n",
    "\n",
    "Dijikstra's algorithm fails if:\n",
    "- The graph is cyclical (undirected graphs are implicitly cyclical)\n",
    "- Weights can be negative\n",
    "\n",
    "==> Only use it for Directed Acyclical Graphs (DAGs) where weights are non-negative\n",
    "\n",
    "\n",
    "## Summary of graph search\n",
    "\n",
    "| Algorithm     | Directed? | Cyclical? | Weighted? |\n",
    "|:--------------|:---------:|:---------:|:---------:|\n",
    "| Breadth-first search | Directed | Acyclical | Unweighted |\n",
    "| Dijkstra's | Directed | Acyclical | Weighted (+ve only) |\n",
    "| Bellman-Ford | Directed | Acyclical | Weighted |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[Grokking Algorithms; Aditya Bhargava](https://www.manning.com/books/grokking-algorithms)\n",
    "\n",
    "[Eliana Lopez's crib sheet](https://github.com/elianalopez/Data-Structures-and-Algorithms-Notes-with-Python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
