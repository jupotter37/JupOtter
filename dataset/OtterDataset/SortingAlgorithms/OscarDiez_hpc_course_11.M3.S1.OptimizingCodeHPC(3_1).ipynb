{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Code Optimization\n",
        "\n",
        "In high-performance computing (HPC), code optimization is critical for maximizing system performance. The goal is to make applications run faster by improving the way they utilize hardware resources, such as CPUs, memory, and interconnects. This session will guide you through optimizing code for HPC architectures, focusing on techniques that improve computational performance and memory efficiency. Understanding these concepts is essential in fields like scientific computing, where efficient code can significantly reduce computation time.\n",
        "\n",
        "We will cover:\n",
        "- The importance of code optimization in HPC.\n",
        "- An overview of HPC architectures and how their optimization requirements differ.\n",
        "- How algorithm efficiency relates to computational performance.\n",
        "\n",
        "Let's start by exploring code profiling techniques, a vital step in identifying performance bottlenecks.\n"
      ],
      "metadata": {
        "id": "36EiNa1Lvva6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Profiling Techniques\n",
        "\n",
        "Code profiling is an essential process for optimizing applications in high-performance computing (HPC). It involves measuring different aspects of a program's execution, such as CPU usage, memory access patterns, and I/O operations, to identify bottlenecks and improve performance. Profiling allows developers to pinpoint performance hotspots—specific sections of the code where most execution time is spent—and focus their optimization efforts where they will have the most significant impact.\n",
        "\n",
        "In the HPC context, profiling is even more critical, as the performance bottlenecks in parallel applications can be complex, involving synchronization, communication, and load balancing issues. Profiling also helps you understand how well the code scales across multiple nodes and cores, ensuring that optimizations lead to better parallel performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzHwowX0v6S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithmic Choices for HPC\n",
        "\n",
        "In High-Performance Computing (HPC), selecting the right algorithms is critical for maximizing performance and scalability across multiple processors. Unlike traditional serial algorithms, parallel algorithms aim to divide computational tasks into smaller independent pieces that can be processed simultaneously. This ability to decompose a problem into independent tasks and distribute it across many processors can greatly reduce computation time and improve resource utilization.\n",
        "\n",
        "In this section, we will explore parallel algorithms, particularly focusing on sorting with parallel quicksort. We will also touch on complexity analysis in parallel computing, emphasizing how the performance of an algorithm scales with an increasing number of processors.\n",
        "\n",
        "### Objectives:\n",
        "1. Understand the principles of parallel algorithms.\n",
        "2. Learn how to implement and run a parallel quicksort using MPI.\n",
        "3. Analyze the performance of parallel algorithms with respect to complexity.\n",
        "   \n",
        "### Example:\n",
        "We will implement parallel quicksort using MPI, where a dataset is divided among processors, sorted independently, and then merged. Parallel quicksort can potentially reduce sorting time from O(n log n) to O(log^2 n) with optimal parallelization and minimal communication overhead.\n"
      ],
      "metadata": {
        "id": "CIfKCDqVv9BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n"
      ],
      "metadata": {
        "id": "g8_yxKCz0uap"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for parallel quicksort with MPI to a file\n",
        "code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to swap elements in the array\n",
        "void swap(int* a, int* b) {\n",
        "    int temp = *a;\n",
        "    *a = *b;\n",
        "    *b = temp;\n",
        "}\n",
        "\n",
        "// Standard quicksort partition function\n",
        "int partition(int arr[], int low, int high) {\n",
        "    int pivot = arr[high];\n",
        "    int i = low - 1;\n",
        "    for (int j = low; j < high; j++) {\n",
        "        if (arr[j] <= pivot) {\n",
        "            i++;\n",
        "            swap(&arr[i], &arr[j]);\n",
        "        }\n",
        "    }\n",
        "    swap(&arr[i + 1], &arr[high]);\n",
        "    return i + 1;\n",
        "}\n",
        "\n",
        "// Quicksort algorithm\n",
        "void quicksort(int arr[], int low, int high) {\n",
        "    if (low < high) {\n",
        "        int pi = partition(arr, low, high);\n",
        "        quicksort(arr, low, pi - 1);\n",
        "        quicksort(arr, pi + 1, high);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Merge function to combine sorted arrays\n",
        "void merge(int* arr1, int size1, int* arr2, int size2, int* result) {\n",
        "    int i = 0, j = 0, k = 0;\n",
        "    while (i < size1 && j < size2) {\n",
        "        if (arr1[i] < arr2[j]) {\n",
        "            result[k++] = arr1[i++];\n",
        "        } else {\n",
        "            result[k++] = arr2[j++];\n",
        "        }\n",
        "    }\n",
        "    while (i < size1) result[k++] = arr1[i++];\n",
        "    while (j < size2) result[k++] = arr2[j++];\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int size, rank;\n",
        "\n",
        "    MPI_Init(&argc, &argv); // Initialize MPI\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size); // Get number of processes\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Get the rank of the process\n",
        "\n",
        "    int n = 16; // Number of elements in the array\n",
        "    int* arr = NULL;\n",
        "    int* local_arr;\n",
        "    int local_n;\n",
        "\n",
        "    // The master process initializes the array and scatters it\n",
        "    if (rank == 0) {\n",
        "        arr = (int*)malloc(n * sizeof(int));\n",
        "        printf(\"Original array: \");\n",
        "        for (int i = 0; i < n; i++) {\n",
        "            arr[i] = rand() % 100; // Random numbers between 0 and 99\n",
        "            printf(\"%d \", arr[i]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Divide the array into chunks for each process\n",
        "    local_n = n / size;\n",
        "    local_arr = (int*)malloc(local_n * sizeof(int));\n",
        "\n",
        "    // Scatter the array to all processes\n",
        "    MPI_Scatter(arr, local_n, MPI_INT, local_arr, local_n, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Each process sorts its part\n",
        "    quicksort(local_arr, 0, local_n - 1);\n",
        "\n",
        "    // Gather the sorted parts back into the original array on the root process\n",
        "    MPI_Gather(local_arr, local_n, MPI_INT, arr, local_n, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // The root process merges all sorted parts\n",
        "    if (rank == 0) {\n",
        "        int* sorted_arr = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "        // Initial merge of the first two parts\n",
        "        merge(arr, local_n, arr + local_n, local_n, sorted_arr);\n",
        "\n",
        "        // Now merge any remaining sorted parts (if more than two processes)\n",
        "        for (int i = 2 * local_n; i < n; i += local_n) {\n",
        "            int* temp_arr = (int*)malloc(n * sizeof(int));\n",
        "            merge(sorted_arr, i, arr + i, local_n, temp_arr);\n",
        "            for (int j = 0; j < i + local_n; j++) {\n",
        "                sorted_arr[j] = temp_arr[j];\n",
        "            }\n",
        "            free(temp_arr);\n",
        "        }\n",
        "\n",
        "        printf(\"Sorted array: \");\n",
        "        for (int i = 0; i < n; i++) {\n",
        "            printf(\"%d \", sorted_arr[i]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "\n",
        "        free(sorted_arr);\n",
        "        free(arr);\n",
        "    }\n",
        "\n",
        "    free(local_arr);\n",
        "    MPI_Finalize(); // Finalize MPI\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"parallel_quicksort.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with mpicc (MPI compiler)\n",
        "!mpicc -o parallel_quicksort parallel_quicksort.c\n",
        "\n",
        "# Run the compiled program with mpirun using --oversubscribe (4 processes)\n",
        "!mpirun --oversubscribe -np 4 ./parallel_quicksort\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-DgeFuev_gC",
        "outputId": "7b2e0267-1908-4213-d188-e7ac0dfff077"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array: 83 86 77 15 93 35 86 92 49 21 62 27 90 59 63 26 \n",
            "Sorted array: 15 21 26 27 35 49 59 62 63 77 83 86 86 90 92 93 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Parallel Quicksort with MPI\n",
        "\n",
        "### Code Breakdown:\n",
        "1. **MPI Initialization**: The program begins by initializing the MPI environment with `MPI_Init()`, retrieving the number of processes (`MPI_Comm_size()`) and the rank of each process (`MPI_Comm_rank()`).\n",
        "\n",
        "2. **Array Initialization**: On the root process (`rank == 0`), an array of 16 random integers is created. This array will be scattered to all processes for sorting.\n",
        "\n",
        "3. **Quicksort Algorithm**: Each process receives a chunk of the array and applies the standard quicksort algorithm locally. Quicksort is a divide-and-conquer algorithm that recursively partitions the array around a pivot element and sorts the partitions independently.\n",
        "\n",
        "4. **Scatter and Gather**: MPI’s `MPI_Scatter()` is used to distribute chunks of the array to each process, and `MPI_Gather()` is used to collect the sorted chunks back into the root process.\n",
        "\n",
        "5. **Merging Sorted Chunks**: Once each process has sorted its portion of the array, the root process merges the sorted parts into a single sorted array using the `merge()` function.\n",
        "\n",
        "6. **Parallel Execution**: This program runs with 4 processes (as specified by `mpirun -np 4`), allowing the sorting of different sections of the array in parallel.\n",
        "\n",
        "### Benefits of Parallel Quicksort:\n",
        "Parallel quicksort leverages multiple processors to independently sort portions of the data. This reduces the overall sorting time by taking advantage of concurrency. Although sorting individually on each process introduces complexity during the merging phase, the total execution time can be significantly reduced compared to a serial quicksort for large datasets.\n",
        "\n",
        "By distributing the work across processors and minimizing inter-process communication during sorting, this implementation optimizes for both computation and communication, making it a good example of parallel algorithm design.\n"
      ],
      "metadata": {
        "id": "h2CB6GlM1MnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complexity Analysis in Parallel Computing\n",
        "\n",
        "Complexity analysis in parallel computing extends beyond the traditional measures of algorithm complexity, such as time and space complexity. It also considers how well an algorithm scales with the number of processors, and how it balances computation with communication overhead.\n",
        "\n",
        "In parallel computing, an ideal algorithm would reduce the time complexity compared to its serial counterpart. For instance, a task that takes O(n) time on one processor could be reduced to O(n/p) time using `p` processors, assuming perfect division of labor and no communication delays. However, in reality, communication and synchronization between processors can affect the efficiency of parallel algorithms, and this overhead needs to be factored into the complexity analysis.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Serial vs. Parallel Time Complexity**: The goal in parallel computing is to distribute tasks across multiple processors to reduce overall execution time. The effectiveness of this distribution is reflected in how the time complexity changes when moving from serial to parallel computation.\n",
        "   \n",
        "2. **Computation and Communication Trade-offs**: In parallel computing, reducing the computational load by dividing the work among processors must be balanced with the communication overhead between those processors. Algorithms with low communication demands scale better with increasing numbers of processors.\n",
        "\n",
        "3. **Speedup and Efficiency**: Speedup refers to how much faster an algorithm runs in parallel compared to serial execution. Ideally, an algorithm running on `p` processors would be `p` times faster than its serial counterpart. Efficiency measures how well an algorithm utilizes the available processors.\n",
        "\n",
        "In this section, we will explore examples of common algorithms (e.g., sorting, searching, matrix multiplication) and their parallel performance in terms of time complexity.\n",
        "\n",
        "### Objectives:\n",
        "1. Understand the difference between serial and parallel time complexity.\n",
        "2. Explore the trade-offs between computation and communication in parallel algorithms.\n",
        "3. Analyze the speedup and efficiency of parallel algorithms using complexity analysis.\n"
      ],
      "metadata": {
        "id": "5g1C2Rwj1zTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for summing an array in parallel using MPI\n",
        "code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to sum elements of an array\n",
        "double sumArray(double* array, int size) {\n",
        "    double sum = 0.0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sum += array[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int rank, numProcs;\n",
        "    const int arraySize = 1600000;  // Large array for timing comparison\n",
        "    double globalSum = 0.0;\n",
        "\n",
        "    MPI_Init(&argc, &argv);  // Initialize MPI\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);  // Get process rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);  // Get number of processes\n",
        "\n",
        "    // Determine chunk size for each process\n",
        "    int chunkSize = arraySize / numProcs;\n",
        "    double* array = NULL;\n",
        "    double* localArray = (double*)malloc(chunkSize * sizeof(double));\n",
        "\n",
        "    // Master process initializes the array\n",
        "    if (rank == 0) {\n",
        "        array = (double*)malloc(arraySize * sizeof(double));\n",
        "        for (int i = 0; i < arraySize; i++) {\n",
        "            array[i] = (double)(i + 1);  // Fill array with values 1, 2, 3, ..., arraySize\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Start timing the parallel computation\n",
        "    double startTime = MPI_Wtime();\n",
        "\n",
        "    // Scatter the array to all processes\n",
        "    MPI_Scatter(array, chunkSize, MPI_DOUBLE, localArray, chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Each process computes its local sum\n",
        "    double localSum = sumArray(localArray, chunkSize);\n",
        "\n",
        "    // Reduce local sums into the global sum\n",
        "    MPI_Reduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // End timing the parallel computation\n",
        "    double endTime = MPI_Wtime();\n",
        "\n",
        "    // The master process prints the result and timing\n",
        "    if (rank == 0) {\n",
        "        printf(\"Global sum: %.2f\\\\n\", globalSum);\n",
        "        printf(\"Time taken with %d processors: %.6f seconds\\\\n\", numProcs, endTime - startTime);\n",
        "    }\n",
        "\n",
        "    // Clean up\n",
        "    free(localArray);\n",
        "    if (rank == 0) {\n",
        "        free(array);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();  // Finalize MPI\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"parallel_sum.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with mpicc (MPI compiler)\n",
        "!mpicc -o parallel_sum parallel_sum.c\n",
        "\n",
        "# Run the compiled program with mpirun using --oversubscribe (4 processes)\n",
        "!mpirun --oversubscribe -np 4 ./parallel_sum\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UE-77l51zsk",
        "outputId": "73f42dcc-4ca7-49d0-a8c5-e7f91eee6bb0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global sum: 1280000800000.00\n",
            "Time taken with 4 processors: 0.023640 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of MPI Program for Parallel Sum\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Array Initialization**:\n",
        "   - In the master process (rank 0), a large array of size 1,600,000 is initialized with values ranging from 1 to the array size. This array is distributed across multiple processors.\n",
        "\n",
        "2. **Scatter Operation**:\n",
        "   - The `MPI_Scatter()` function divides the array into chunks and sends each chunk to a different process. Each process receives a portion of the array and computes a local sum of its chunk.\n",
        "\n",
        "3. **Parallel Summation**:\n",
        "   - Each process calls the `sumArray()` function to compute the sum of the elements in its chunk. These local sums are then combined using `MPI_Reduce()`, which reduces all local sums into a global sum on the master process.\n",
        "\n",
        "4. **Timing**:\n",
        "   - The program uses `MPI_Wtime()` to measure the time taken for the parallel computation. The master process (rank 0) prints the global sum and the total time taken for the parallel sum operation.\n",
        "\n",
        "### Complexity Analysis:\n",
        "\n",
        "- **Serial Time Complexity**:\n",
        "   - In a serial implementation, summing an array of size `n` would take O(n) time.\n",
        "   \n",
        "- **Parallel Time Complexity**:\n",
        "   - With `p` processors, the time complexity for computation is O(n/p), since each processor sums only `n/p` elements. However, there is also communication overhead for scattering the array and reducing the local sums.\n",
        "   - The total parallel time complexity is O(n/p) + O(log p), where O(log p) accounts for the communication time.\n",
        "\n",
        "### Speedup and Efficiency:\n",
        "\n",
        "- **Speedup**:\n",
        "   - Speedup measures how much faster the parallel algorithm runs compared to the serial version. Ideally, with `p` processors, we would expect a speedup of `p` (i.e., the parallel version should be `p` times faster). However, communication overhead can reduce this speedup.\n",
        "\n",
        "- **Efficiency**:\n",
        "   - Efficiency is the ratio of the speedup to the number of processors. It indicates how well the parallel algorithm scales. An efficiency close to 1 means the algorithm scales well with the number of processors.\n",
        "\n",
        "### Trade-offs Between Computation and Communication:\n",
        "\n",
        "- As the number of processors increases, the computational load on each processor decreases, improving computation time. However, communication overhead (for scattering and reducing the data) can become more significant, limiting the speedup and reducing the efficiency of the parallel algorithm.\n",
        "\n",
        "- For an optimal parallel algorithm, the communication overhead should be minimized to achieve the best possible speedup.\n"
      ],
      "metadata": {
        "id": "rVxPEoI05eqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Complexity Differences\n",
        "\n",
        "Understanding the differences in time complexity is crucial for analyzing the performance of algorithms. Time complexity describes how the runtime of an algorithm scales as the size of the input (n) increases. Different classes of time complexity—such as O(1), O(log n), O(n), O(n log n), O(n^2), and O(2^n)—grow at different rates, which can drastically affect the performance of algorithms on large datasets.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **O(1)**: Constant time, where the algorithm's runtime is independent of the input size.\n",
        "2. **O(log n)**: Logarithmic time, typically seen in divide-and-conquer algorithms like binary search.\n",
        "3. **O(n)**: Linear time, where the runtime grows proportionally to the input size.\n",
        "4. **O(n log n)**: Seen in efficient sorting algorithms like merge sort and quicksort.\n",
        "5. **O(n^2)**: Quadratic time, typically resulting from nested loops, where each iteration depends on the entire input size.\n",
        "6. **O(2^n)**: Exponential time, where the runtime doubles with each additional element, as seen in recursive algorithms like naive Fibonacci calculation.\n",
        "\n",
        "In this section, we will implement these complexities and measure their performance for increasing input sizes.\n"
      ],
      "metadata": {
        "id": "4YD8GjIH5LOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Function to measure time taken by a function\n",
        "def measure_time(func, *args):\n",
        "    start = time.time()\n",
        "    result = func(*args)\n",
        "    end = time.time()\n",
        "    return result, end - start\n",
        "\n",
        "# O(1) - Constant time operation\n",
        "def constant_time_operation():\n",
        "    return 42  # This function always returns 42 in constant time.\n",
        "\n",
        "# O(log n) - Binary Search (logarithmic time)\n",
        "def binary_search(arr, target):\n",
        "    low, high = 0, len(arr) - 1\n",
        "    while low <= high:\n",
        "        mid = (low + high) // 2\n",
        "        if arr[mid] == target:\n",
        "            return True\n",
        "        elif arr[mid] < target:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid - 1\n",
        "    return False\n",
        "\n",
        "# O(n) - Summing an array (linear time)\n",
        "def sum_array(arr):\n",
        "    return sum(arr)\n",
        "\n",
        "# O(n log n) - Merge Sort (log-linear time)\n",
        "def merge_sort(arr):\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "    mid = len(arr) // 2\n",
        "    left = merge_sort(arr[:mid])\n",
        "    right = merge_sort(arr[mid:])\n",
        "    return merge(left, right)\n",
        "\n",
        "def merge(left, right):\n",
        "    result = []\n",
        "    i = j = 0\n",
        "    while i < len(left) and j < len(right):\n",
        "        if left[i] < right[j]:\n",
        "            result.append(left[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            result.append(right[j])\n",
        "            j += 1\n",
        "    result.extend(left[i:])\n",
        "    result.extend(right[j:])\n",
        "    return result\n",
        "\n",
        "# O(n^2) - Pairwise comparison (quadratic time)\n",
        "def quadratic_operation(arr):\n",
        "    count = 0\n",
        "    for i in range(len(arr)):\n",
        "        for j in range(len(arr)):\n",
        "            if arr[i] < arr[j]:\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "# O(2^n) - Naive recursive Fibonacci (exponential time)\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
        "\n",
        "# Test input sizes\n",
        "input_sizes = [10, 100, 1000, 10000, 100000]\n",
        "\n",
        "# Results for different time complexities\n",
        "for n in input_sizes:\n",
        "    arr = np.random.randint(1, 100, size=n)  # Generate a random array of size n\n",
        "\n",
        "    # O(1) - Constant time\n",
        "    _, time_taken = measure_time(constant_time_operation)\n",
        "    print(f\"O(1) - Constant time with input size {n}: {time_taken:.6f} seconds\")\n",
        "\n",
        "    # O(log n) - Binary Search\n",
        "    sorted_arr = sorted(arr)\n",
        "    target = random.choice(sorted_arr)\n",
        "    _, time_taken = measure_time(binary_search, sorted_arr, target)\n",
        "    print(f\"O(log n) - Binary search with input size {n}: {time_taken:.6f} seconds\")\n",
        "\n",
        "    # O(n) - Summing an array\n",
        "    _, time_taken = measure_time(sum_array, arr)\n",
        "    print(f\"O(n) - Summing an array with input size {n}: {time_taken:.6f} seconds\")\n",
        "\n",
        "    # O(n log n) - Merge Sort\n",
        "    _, time_taken = measure_time(merge_sort, arr)\n",
        "    print(f\"O(n log n) - Merge sort with input size {n}: {time_taken:.6f} seconds\")\n",
        "\n",
        "    # O(n^2) - Quadratic operation\n",
        "    if n <= 1000:  # Limit n for quadratic to avoid long computation times\n",
        "        _, time_taken = measure_time(quadratic_operation, arr)\n",
        "        print(f\"O(n^2) - Quadratic operation with input size {n}: {time_taken:.6f} seconds\")\n",
        "\n",
        "# O(2^n) - Fibonacci (Note: Limiting n due to exponential growth)\n",
        "fibonacci_sizes = [10, 20, 30]\n",
        "for n in fibonacci_sizes:\n",
        "    _, time_taken = measure_time(fibonacci, n)\n",
        "    print(f\"O(2^n) - Fibonacci with input size {n}: {time_taken:.6f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HFhg-wx5hb2",
        "outputId": "e7ca4453-3fed-4f61-9357-b0bf02676ac0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O(1) - Constant time with input size 10: 0.000003 seconds\n",
            "O(log n) - Binary search with input size 10: 0.000004 seconds\n",
            "O(n) - Summing an array with input size 10: 0.000021 seconds\n",
            "O(n log n) - Merge sort with input size 10: 0.000087 seconds\n",
            "O(n^2) - Quadratic operation with input size 10: 0.000059 seconds\n",
            "O(1) - Constant time with input size 100: 0.000001 seconds\n",
            "O(log n) - Binary search with input size 100: 0.000006 seconds\n",
            "O(n) - Summing an array with input size 100: 0.000019 seconds\n",
            "O(n log n) - Merge sort with input size 100: 0.000537 seconds\n",
            "O(n^2) - Quadratic operation with input size 100: 0.002758 seconds\n",
            "O(1) - Constant time with input size 1000: 0.000001 seconds\n",
            "O(log n) - Binary search with input size 1000: 0.000005 seconds\n",
            "O(n) - Summing an array with input size 1000: 0.000096 seconds\n",
            "O(n log n) - Merge sort with input size 1000: 0.008324 seconds\n",
            "O(n^2) - Quadratic operation with input size 1000: 0.296539 seconds\n",
            "O(1) - Constant time with input size 10000: 0.000002 seconds\n",
            "O(log n) - Binary search with input size 10000: 0.000009 seconds\n",
            "O(n) - Summing an array with input size 10000: 0.001460 seconds\n",
            "O(n log n) - Merge sort with input size 10000: 0.062849 seconds\n",
            "O(1) - Constant time with input size 100000: 0.000002 seconds\n",
            "O(log n) - Binary search with input size 100000: 0.000011 seconds\n",
            "O(n) - Summing an array with input size 100000: 0.008137 seconds\n",
            "O(n log n) - Merge sort with input size 100000: 0.767925 seconds\n",
            "O(2^n) - Fibonacci with input size 10: 0.000027 seconds\n",
            "O(2^n) - Fibonacci with input size 20: 0.002666 seconds\n",
            "O(2^n) - Fibonacci with input size 30: 0.351296 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Time Complexity Comparison Code\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **O(1) - Constant Time**:\n",
        "   - The constant time function (`constant_time_operation`) always returns a constant value (42). This is an example of an operation that takes the same amount of time regardless of the input size.\n",
        "\n",
        "2. **O(log n) - Binary Search**:\n",
        "   - Binary search divides the input array in half during each iteration, resulting in logarithmic time complexity. For each input size, we search for a random target in a sorted array and measure the time it takes.\n",
        "\n",
        "3. **O(n) - Summing an Array**:\n",
        "   - Summing an array is a linear operation, where each element of the array is processed exactly once. The time taken increases proportionally with the input size.\n",
        "\n",
        "4. **O(n log n) -\n"
      ],
      "metadata": {
        "id": "PvzUGfYg5hGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structure Considerations for MPI\n",
        "\n",
        "When working with MPI (Message Passing Interface), it’s essential to carefully select data structures that minimize communication overhead and maximize performance across distributed processes. The choice of data structures directly influences how data is partitioned and transmitted between processes running on different cores or machines.\n",
        "\n",
        "### Key Considerations:\n",
        "1. **Contiguous Data Structures**: Arrays and matrices stored in a contiguous manner (e.g., row-major or column-major order) are advantageous in MPI. Functions like `MPI_Send` and `MPI_Recv` are optimized for handling contiguous blocks of data, allowing efficient communication of large datasets with minimal overhead.\n",
        "   \n",
        "2. **Non-Contiguous Data Structures**: Linked lists and other non-contiguous data structures pose challenges in MPI, as their scattered memory layout complicates the communication process. This often requires the use of advanced MPI features like derived data types or manual packing and unpacking, which introduces complexity and potential inefficiencies.\n",
        "\n",
        "3. **Collective Operations**: MPI provides collective operations such as `MPI_Bcast`, `MPI_Scatter`, `MPI_Gather`, and `MPI_Allreduce` that benefit significantly from optimized data structures. Arrays and matrices, due to their contiguous memory layout, tend to work well with these operations. For instance, `MPI_Allreduce` can efficiently sum elements across multiple processes.\n",
        "\n",
        "In this section, we will demonstrate how contiguous arrays are handled in MPI, and how to use `MPI_Allreduce` to efficiently compute the sum of array elements across processes.\n",
        "\n",
        "### Objectives:\n",
        "1. Learn how to distribute contiguous data (e.g., arrays) across processes using MPI.\n",
        "2. Implement a reduction operation (`MPI_Allreduce`) to sum array elements from different processes.\n",
        "3. Compare the efficiency of using contiguous vs. non-contiguous data structures in an MPI context.\n"
      ],
      "metadata": {
        "id": "SF3-d1fDv_0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for comparing array and linked list performance with MPI\n",
        "code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// Linked list node structure\n",
        "struct Node {\n",
        "    double data;\n",
        "    struct Node* next;\n",
        "};\n",
        "\n",
        "// Function to create a linked list of a given size\n",
        "struct Node* createLinkedList(int size) {\n",
        "    struct Node* head = (struct Node*)malloc(sizeof(struct Node));\n",
        "    struct Node* current = head;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        current->data = (double)(i + 1);\n",
        "        if (i == size - 1) {\n",
        "            current->next = NULL;\n",
        "        } else {\n",
        "            current->next = (struct Node*)malloc(sizeof(struct Node));\n",
        "            current = current->next;\n",
        "        }\n",
        "    }\n",
        "    return head;\n",
        "}\n",
        "\n",
        "// Function to convert a linked list to an array (packing)\n",
        "void linkedListToArray(struct Node* head, double* array, int size) {\n",
        "    struct Node* current = head;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = current->data;\n",
        "        current = current->next;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to convert an array back to a linked list (unpacking)\n",
        "void arrayToLinkedList(double* array, struct Node* head, int size) {\n",
        "    struct Node* current = head;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        current->data = array[i];\n",
        "        current = current->next;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to sum elements of a linked list\n",
        "double sumLinkedList(struct Node* head, int size) {\n",
        "    double sum = 0.0;\n",
        "    struct Node* current = head;\n",
        "    while (current != NULL) {\n",
        "        sum += current->data;\n",
        "        current = current->next;\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "// Function to sum elements of an array\n",
        "double sumArray(double* array, int size) {\n",
        "    double sum = 0.0;\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        sum += array[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int rank, numProcs;\n",
        "    const int listSize = 160000;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "\n",
        "    int chunkSize = listSize / numProcs;\n",
        "    double globalSum = 0.0;\n",
        "\n",
        "    // Timers for comparison\n",
        "    double arrayStart, arrayEnd, linkedListStart, linkedListEnd;\n",
        "\n",
        "    // Array example\n",
        "    double* array = (double*)malloc(listSize * sizeof(double));\n",
        "    double* localArray = (double*)malloc(chunkSize * sizeof(double));\n",
        "\n",
        "    if (rank == 0) {\n",
        "        for (int i = 0; i < listSize; i++) {\n",
        "            array[i] = (double)(i + 1);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Time the array-based communication and summation\n",
        "    arrayStart = MPI_Wtime();\n",
        "    MPI_Scatter(array, chunkSize, MPI_DOUBLE, localArray, chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "    double localArraySum = sumArray(localArray, chunkSize);\n",
        "    MPI_Allreduce(&localArraySum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n",
        "    arrayEnd = MPI_Wtime();\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"Global sum using array: %.2f\\\\n\", globalSum);\n",
        "        printf(\"Time taken using array: %.6f seconds\\\\n\", arrayEnd - arrayStart);\n",
        "    }\n",
        "\n",
        "    // Linked list example\n",
        "    struct Node* list = NULL;\n",
        "    struct Node* localList = NULL;\n",
        "    double* packedArray = (double*)malloc(chunkSize * sizeof(double));\n",
        "\n",
        "    if (rank == 0) {\n",
        "        list = createLinkedList(listSize); // Create a linked list on the root process\n",
        "    }\n",
        "\n",
        "    // Time the linked list-based communication and summation\n",
        "    linkedListStart = MPI_Wtime();\n",
        "\n",
        "    if (rank == 0) {\n",
        "        linkedListToArray(list, array, listSize); // Pack linked list into array\n",
        "    }\n",
        "\n",
        "    MPI_Scatter(array, chunkSize, MPI_DOUBLE, packedArray, chunkSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    // Convert the packed array back into a linked list in each process\n",
        "    localList = createLinkedList(chunkSize);\n",
        "    arrayToLinkedList(packedArray, localList, chunkSize);\n",
        "\n",
        "    double localLinkedListSum = sumLinkedList(localList, chunkSize);\n",
        "    MPI_Allreduce(&localLinkedListSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n",
        "\n",
        "    linkedListEnd = MPI_Wtime();\n",
        "\n",
        "    if (rank == 0) {\n",
        "        printf(\"Global sum using linked list: %.2f\\\\n\", globalSum);\n",
        "        printf(\"Time taken using linked list: %.6f seconds\\\\n\", linkedListEnd - linkedListStart);\n",
        "    }\n",
        "\n",
        "    // Clean up\n",
        "    free(array);\n",
        "    free(localArray);\n",
        "    free(packedArray);\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"mpi_linkedlist_vs_array.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with mpicc (MPI compiler)\n",
        "!mpicc -o mpi_linkedlist_vs_array mpi_linkedlist_vs_array.c\n",
        "\n",
        "# Run the compiled program with mpirun using --oversubscribe (4 processes)\n",
        "!mpirun --oversubscribe -np 4 ./mpi_linkedlist_vs_array\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuo6peCcwtG6",
        "outputId": "09a18c38-f508-4396-82d9-f2a1863086d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global sum using array: 12800080000.00\n",
            "Time taken using array: 0.001410 seconds\n",
            "Global sum using linked list: 12800080000.00\n",
            "Time taken using linked list: 0.010140 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of MPI Program: Linked List vs Array for Communication\n",
        "\n",
        "### Code Breakdown:\n",
        "\n",
        "1. **Data Structures**:\n",
        "   - We use both an **array** and a **linked list** to demonstrate the differences in performance when sending and receiving data in MPI.\n",
        "   - The linked list is a non-contiguous data structure, while the array is contiguous, making communication more efficient for the array.\n",
        "\n",
        "2. **Array Communication**:\n",
        "   - The array is directly scattered using `MPI_Scatter()` and summed using `MPI_Allreduce()`. This is the standard, efficient way to communicate data in MPI.\n",
        "   \n",
        "3. **Linked List Communication**:\n",
        "   - Since MPI cannot directly handle non-contiguous data structures like linked lists, we **pack** the linked list into an array before sending it.\n",
        "   - After receiving the array on each process, we **unpack** it back into a linked list.\n",
        "   - This packing and unpacking introduces extra overhead, making the communication of linked lists less efficient compared to arrays.\n",
        "\n",
        "4. **Summation**:\n",
        "   - For both the array and the linked list, each process sums its portion of the data and uses `MPI_Allreduce()` to compute the global sum across all processes.\n",
        "   \n",
        "5. **Timing**:\n",
        "   - We use `MPI_Wtime()` to measure the time taken for the communication and summation for both the array and the linked list.\n",
        "   - The program prints the global sum and the time taken for both data structures, allowing us to compare their performance.\n",
        "\n",
        "### Performance Differences:\n",
        "\n",
        "- **Array**: Since arrays are stored contiguously in memory, MPI functions like `MPI_Scatter` and `MPI_Allreduce` can operate on them efficiently, resulting in lower communication and computation times.\n",
        "- **Linked List**: Linked lists require packing and unpacking into arrays before communication, which adds significant overhead to both communication and computation. This makes linked lists less suitable for MPI-based parallel computing.\n",
        "\n",
        "By comparing the times for both data structures, you will see the performance impact of using non-contiguous data structures in MPI.\n"
      ],
      "metadata": {
        "id": "6wZCuMYJ4qHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop Optimization Techniques\n",
        "\n",
        "Loop optimization is a critical strategy for improving performance in HPC applications. Techniques like loop unrolling, fusion, and tiling can dramatically increase cache utilization and reduce overhead.\n",
        "\n",
        "We will now explore loop tiling for matrix multiplication, a common technique to enhance performance by improving data locality.\n"
      ],
      "metadata": {
        "id": "KHV-lqEvwvBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loop Optimization Techniques in HPC\n",
        "\n",
        "In High-Performance Computing (HPC), loops often represent the core of computation in scientific and engineering applications. Optimizing loop structures can significantly enhance the performance of these applications. Several key loop optimization techniques are widely used to improve efficiency, reduce execution time, and optimize memory usage. This section focuses on:\n",
        "\n",
        "### Key Loop Optimization Techniques:\n",
        "1. **Loop Unrolling**: Reduces the overhead of loop control and increases instruction-level parallelism (ILP) by replicating loop iterations.\n",
        "2. **Loop Fusion and Fission**: Combines (fusion) or splits (fission) loops to improve cache utilization and reduce memory access latency.\n",
        "3. **Loop Tiling**: Reorganizes loop computation into smaller blocks, optimizing cache use and reducing the number of cache misses in memory-intensive tasks like matrix operations.\n",
        "\n",
        "Each technique is aimed at improving specific aspects of loop execution, such as minimizing loop overhead, improving data locality, or enhancing cache performance. Let's explore these techniques in more detail with practical examples.\n"
      ],
      "metadata": {
        "id": "PxKKQUgL6Gcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for loop optimization techniques to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// Example data size\n",
        "#define N 580\n",
        "\n",
        "// Function to initialize arrays\n",
        "void initialize_arrays(double A[], double B[], double C[], int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        A[i] = (double)(rand() % 1000);\n",
        "        B[i] = (double)(rand() % 1000);\n",
        "        C[i] = 0.0;\n",
        "    }\n",
        "}\n",
        "\n",
        "// O(n) Loop Unrolling Example\n",
        "void loop_unrolling(double A[], double B[], double C[], int n) {\n",
        "    for (int i = 0; i < n; i += 4) {\n",
        "        C[i] = A[i] + B[i];\n",
        "        C[i + 1] = A[i + 1] + B[i + 1];\n",
        "        C[i + 2] = A[i + 2] + B[i + 2];\n",
        "        C[i + 3] = A[i + 3] + B[i + 3];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Loop Fusion Example\n",
        "void loop_fusion(double A[], double B[], double C[], double D[], int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        A[i] = A[i] + B[i];\n",
        "        D[i] = C[i] * A[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Loop Fission Example\n",
        "void loop_fission(double A[], double B[], double C[], double D[], int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        A[i] = A[i] + B[i];\n",
        "    }\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        D[i] = C[i] * A[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Loop Tiling Example for Matrix Multiplication\n",
        "void matrix_multiplication_tiling(double A[N][N], double B[N][N], double C[N][N], int n, int blockSize) {\n",
        "    for (int i = 0; i < n; i += blockSize) {\n",
        "        for (int j = 0; j < n; j += blockSize) {\n",
        "            for (int k = 0; k < n; k += blockSize) {\n",
        "                for (int ii = i; ii < i + blockSize && ii < n; ii++) {\n",
        "                    for (int jj = j; jj < j + blockSize && jj < n; jj++) {\n",
        "                        for (int kk = k; kk < k + blockSize && kk < n; kk++) {\n",
        "                            C[ii][jj] += A[ii][kk] * B[kk][jj];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Array initialization for loop unrolling, fusion, and fission\n",
        "    double A[N], B[N], C[N], D[N];\n",
        "    initialize_arrays(A, B, C, N);\n",
        "\n",
        "    // Timing loop unrolling\n",
        "    clock_t start = clock();\n",
        "    loop_unrolling(A, B, C, N);\n",
        "    clock_t end = clock();\n",
        "    printf(\"Loop Unrolling Time: %.6f seconds\\\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n",
        "\n",
        "    // Timing loop fusion\n",
        "    start = clock();\n",
        "    loop_fusion(A, B, C, D, N);\n",
        "    end = clock();\n",
        "    printf(\"Loop Fusion Time: %.6f seconds\\\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n",
        "\n",
        "    // Timing loop fission\n",
        "    start = clock();\n",
        "    loop_fission(A, B, C, D, N);\n",
        "    end = clock();\n",
        "    printf(\"Loop Fission Time: %.6f seconds\\\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n",
        "\n",
        "    // Matrix multiplication with loop tiling\n",
        "    double M[N][N], N1[N][N], N2[N][N];\n",
        "    start = clock();\n",
        "    matrix_multiplication_tiling(M, N1, N2, N, 64);\n",
        "    end = clock();\n",
        "    printf(\"Loop Tiling (Matrix Multiplication) Time: %.6f seconds\\\\n\", (double)(end - start) / CLOCKS_PER_SEC);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"loop_optimization.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc (C compiler)\n",
        "!gcc -o loop_optimization loop_optimization.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./loop_optimization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX8b_z5Ewuui",
        "outputId": "0af8b27a-917b-4fd9-d9de-80a734b1bb45"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loop Unrolling Time: 0.000003 seconds\n",
            "Loop Fusion Time: 0.000005 seconds\n",
            "Loop Fission Time: 0.000006 seconds\n",
            "Loop Tiling (Matrix Multiplication) Time: 1.123824 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Loop Optimization Techniques in C\n",
        "\n",
        "### Loop Unrolling:\n",
        "- **Concept**: In this example, we unroll a loop that adds elements of two arrays. By unrolling, we reduce the loop control overhead and allow multiple instructions to be executed in parallel, which can significantly improve performance on modern processors with multiple ALUs.\n",
        "- **Example**: The original loop iterated over one element at a time, but in the unrolled version, four elements are processed in each iteration.\n",
        "\n",
        "### Loop Fusion:\n",
        "- **Concept**: Loop fusion combines two loops that operate over the same range into a single loop. This reduces loop overhead and improves cache locality since the data loaded in the first part of the loop can be used in the second part without being reloaded.\n",
        "- **Example**: Two loops—one adding elements of arrays `A` and `B` and another updating `D`—are fused into one loop.\n",
        "\n",
        "### Loop Fission:\n",
        "- **Concept**: Loop fission, or loop splitting, breaks a single loop into multiple loops to avoid cache conflicts or reduce the working set size. This can be particularly helpful when handling large data sets that do not fit in cache.\n",
        "- **Example**: A single loop that both updates `A` and computes values for `D` is split into two separate loops. This can improve cache efficiency when large arrays are involved.\n",
        "\n",
        "### Loop Tiling:\n",
        "- **Concept**: Loop tiling, also known as loop blocking, is used to break down large computations (such as matrix multiplication) into smaller blocks or tiles. This improves cache performance by ensuring that smaller portions of data are repeatedly used while they remain in the cache, reducing memory traffic between RAM and the cache.\n",
        "- **Example**: The matrix multiplication example demonstrates how loop tiling is applied to process blocks of a matrix, ensuring that the working data fits in the cache, thereby reducing cache misses.\n",
        "\n",
        "### Performance Improvements:\n",
        "- **Timing Results**: The code measures the execution time of each optimization technique. Students can compare these results to understand the impact of each technique on performance.\n",
        "- **Cache and Loop Control**: By optimizing loops, we reduce unnecessary memory access, minimize cache misses, and reduce the overhead of loop control, leading to improved computational performance.\n"
      ],
      "metadata": {
        "id": "qgKrvgcF6e8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization in HPC\n",
        "\n",
        "Vectorization is an optimization technique that enables a single instruction to process multiple data points simultaneously using SIMD (Single Instruction, Multiple Data) capabilities. Modern CPUs include SIMD extensions like Intel's AVX and ARM's NEON, which allow operations such as addition and multiplication to be performed on multiple data elements at once. By leveraging these extensions, developers can achieve greater data parallelism, improving computational throughput.\n",
        "\n",
        "### Key Aspects of Vectorization:\n",
        "1. **SIMD**: Vectorized operations execute the same instruction across multiple data points in parallel.\n",
        "2. **Data Alignment**: Ensuring that data is aligned correctly is crucial for efficient vectorization.\n",
        "3. **Compiler Directives and Intrinsics**: Compilers like GCC and Intel provide auto-vectorization and intrinsics to help optimize performance.\n",
        "\n",
        "### Benefits of Vectorization:\n",
        "- Increased throughput by processing multiple data elements in parallel.\n",
        "- Enhanced performance for operations like matrix multiplication, dot products, and other numerical computations.\n",
        "- Reduced overhead and improved memory efficiency by utilizing SIMD registers to hold multiple values and process them simultaneously.\n",
        "\n",
        "This section will demonstrate how to apply vectorization using compiler directives, SIMD intrinsics, and practical examples of vectorized operations.\n"
      ],
      "metadata": {
        "id": "9QUs4_8P6g5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for vectorization examples using AVX but without FMA to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <immintrin.h>  // For AVX intrinsics\n",
        "#include <omp.h>        // For OpenMP\n",
        "\n",
        "// Example size\n",
        "#define N 1000000\n",
        "\n",
        "// Vector addition using OpenMP SIMD directive (auto-vectorization)\n",
        "void vector_add_openmp(float* x, float* y, float* z, int n, float scalar) {\n",
        "    #pragma omp simd\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        z[i] = x[i] * scalar + y[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Vector addition using AVX intrinsics (without FMA)\n",
        "void vector_add_intrinsics(float* x, float* y, float* z, int n, float scalar) {\n",
        "    __m256 scalar_vec = _mm256_set1_ps(scalar);  // Load scalar into AVX register\n",
        "    for (int i = 0; i < n; i += 8) {\n",
        "        __m256 x_vec = _mm256_load_ps(&x[i]);    // Load 8 elements from x\n",
        "        __m256 y_vec = _mm256_load_ps(&y[i]);    // Load 8 elements from y\n",
        "        __m256 mul_vec = _mm256_mul_ps(x_vec, scalar_vec);  // Multiply x[i] * scalar\n",
        "        __m256 z_vec = _mm256_add_ps(mul_vec, y_vec);       // Add the result to y[i]\n",
        "        _mm256_store_ps(&z[i], z_vec);           // Store result in z\n",
        "    }\n",
        "}\n",
        "\n",
        "// Dot product using AVX intrinsics\n",
        "double dot_product_intrinsics(const double* a, const double* b, int n) {\n",
        "    __m256d sum = _mm256_setzero_pd();  // Initialize the sum\n",
        "    for (int i = 0; i < n; i += 4) {\n",
        "        __m256d va = _mm256_load_pd(&a[i]);  // Load 4 elements from a\n",
        "        __m256d vb = _mm256_load_pd(&b[i]);  // Load 4 elements from b\n",
        "        __m256d prod = _mm256_mul_pd(va, vb); // Multiply a[i] and b[i]\n",
        "        sum = _mm256_add_pd(sum, prod);      // Add product to sum\n",
        "    }\n",
        "    double buffer[4];\n",
        "    _mm256_store_pd(buffer, sum);            // Store the result\n",
        "    return buffer[0] + buffer[1] + buffer[2] + buffer[3];  // Sum the buffer\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Allocate arrays for vector operations\n",
        "    float *x = (float*)_mm_malloc(N * sizeof(float), 32);\n",
        "    float *y = (float*)_mm_malloc(N * sizeof(float), 32);\n",
        "    float *z = (float*)_mm_malloc(N * sizeof(float), 32);\n",
        "\n",
        "    // Initialize arrays\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        x[i] = (float)(i + 1);\n",
        "        y[i] = (float)(i + 2);\n",
        "        z[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Scalar for vector addition\n",
        "    float scalar = 2.0f;\n",
        "\n",
        "    // Timing vector addition with OpenMP SIMD\n",
        "    double start = omp_get_wtime();\n",
        "    vector_add_openmp(x, y, z, N, scalar);\n",
        "    double end = omp_get_wtime();\n",
        "    printf(\"OpenMP SIMD Vector Addition Time: %.6f seconds\\\\n\", end - start);\n",
        "\n",
        "    // Timing vector addition with AVX intrinsics\n",
        "    start = omp_get_wtime();\n",
        "    vector_add_intrinsics(x, y, z, N, scalar);\n",
        "    end = omp_get_wtime();\n",
        "    printf(\"AVX Intrinsics Vector Addition Time: %.6f seconds\\\\n\", end - start);\n",
        "\n",
        "    // Timing dot product with AVX intrinsics\n",
        "    double *a = (double*)_mm_malloc(N * sizeof(double), 32);\n",
        "    double *b = (double*)_mm_malloc(N * sizeof(double), 32);\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        a[i] = (double)(i + 1);\n",
        "        b[i] = (double)(i + 2);\n",
        "    }\n",
        "\n",
        "    start = omp_get_wtime();\n",
        "    double dot_result = dot_product_intrinsics(a, b, N);\n",
        "    end = omp_get_wtime();\n",
        "    printf(\"AVX Intrinsics Dot Product Result: %.2f\\\\n\", dot_result);\n",
        "    printf(\"AVX Intrinsics Dot Product Time: %.6f seconds\\\\n\", end - start);\n",
        "\n",
        "    // Clean up\n",
        "    _mm_free(x);\n",
        "    _mm_free(y);\n",
        "    _mm_free(z);\n",
        "    _mm_free(a);\n",
        "    _mm_free(b);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"vectorization_examples_no_fma.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc (C compiler) without FMA, but with AVX\n",
        "!gcc -o vectorization_examples_no_fma vectorization_examples_no_fma.c -fopenmp -mavx -lm\n",
        "\n",
        "# Run the compiled program\n",
        "!./vectorization_examples_no_fma\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8eWbeus6hfP",
        "outputId": "a0b8cdfc-0520-48cc-ba3f-2f0e61c9d900"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenMP SIMD Vector Addition Time: 0.004063 seconds\n",
            "AVX Intrinsics Vector Addition Time: 0.001718 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Vectorization Examples (Without FMA)\n",
        "\n",
        "### Vector Addition Using AVX Intrinsics (Without FMA)\n",
        "- **Modified Code**: In this example, we avoid using the fused multiply-add (FMA) instruction and instead use separate AVX instructions for multiplication (`_mm256_mul_ps`) and addition (`_mm256_add_ps`).\n",
        "- **Impact**: While FMA can be more efficient by combining multiplication and addition into a single instruction, not all systems or compilers support it. By using regular AVX instructions, we maintain compatibility across more systems.\n",
        "\n",
        "### Performance Comparison:\n",
        "- **OpenMP SIMD vs. AVX Intrinsics**: The program compares the performance of vector addition using OpenMP's auto-vectorization capabilities (`#pragma omp simd`) with AVX intrinsics. OpenMP provides a high-level abstraction that is easier to implement, while AVX intrinsics offer more control and potentially higher performance.\n",
        "- **Dot Product Using AVX**: The dot product example shows how AVX intrinsics can be used to process four double-precision values simultaneously. By processing multiple elements at once, the number of iterations is reduced, resulting in faster execution compared to a scalar approach.\n",
        "\n",
        "### Memory Alignment:\n",
        "- **Memory Alignment**: The program uses `_mm_malloc` to allocate memory aligned to 32-byte boundaries, which is required for efficient use of AVX intrinsics. Proper alignment ensures that data is loaded efficiently into SIMD registers without penalties.\n",
        "\n",
        "By avoiding the use of FMA and sticking to regular AVX instructions, we ensure that the program can run on a wider range of systems while still benefiting from vectorization and SIMD parallelism.\n"
      ],
      "metadata": {
        "id": "AyECdyGX6yif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Access Patterns in HPC\n",
        "\n",
        "Optimizing memory access patterns is a key technique for reducing latency and increasing throughput in High-Performance Computing (HPC) applications. In many systems, memory access times can become a bottleneck due to the disparity between the speed of the processor and the speed of the memory. Understanding cache utilization and memory bandwidth is crucial to improving memory access efficiency.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Cache Utilization**: Cache memory is faster than main memory but much smaller. Efficiently using the cache ensures that data needed for computation is readily available, reducing costly accesses to slower main memory. Techniques like loop tiling (as discussed previously) and **prefetching** can significantly improve cache performance.\n",
        "    - **Prefetching**: This technique involves loading data into the cache before it is needed by the processor, reducing idle cycles. Prefetching can be controlled by the compiler or manually programmed.\n",
        "\n",
        "2. **Memory Bandwidth**: This measures the rate at which data is read from or written to memory. Maximizing memory bandwidth ensures that the processor receives data quickly and minimizes idle time. Optimizing data locality (organizing data to stay closer to the processor) and reducing unnecessary data movement are critical strategies for improving memory bandwidth.\n",
        "\n",
        "This section will explore techniques for improving cache utilization and memory bandwidth, including an example of prefetching and an in-place matrix transpose operation, which reduces memory bandwidth requirements by eliminating the need for extra memory allocations.\n"
      ],
      "metadata": {
        "id": "6SwAmqQC8qP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for prefetching example to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "\n",
        "// Function to demonstrate prefetching\n",
        "void prefetching_example(float* data, int n, int prefetch_distance) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        // Prefetch data before processing\n",
        "        __builtin_prefetch(&data[i + prefetch_distance], 0, 1);\n",
        "        // Process data\n",
        "        data[i] = data[i] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 100000;  // Example size\n",
        "    int prefetch_distance = 64;  // Prefetch distance\n",
        "    float data[n];\n",
        "\n",
        "    // Initialize array\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        data[i] = (float)i;\n",
        "    }\n",
        "\n",
        "    // Call prefetching example\n",
        "    prefetching_example(data, n, prefetch_distance);\n",
        "\n",
        "    // Print some of the processed results\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%f \", data[i]);\n",
        "    }\n",
        "    printf(\"\\\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"prefetching_example.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc\n",
        "!gcc -o prefetching_example prefetching_example.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./prefetching_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-7vojH28qqx",
        "outputId": "44d2c6f2-8325-4fb3-f435-92f598ea0d96"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000000 2.000000 4.000000 6.000000 8.000000 10.000000 12.000000 14.000000 16.000000 18.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In-Place Matrix Transpose to Optimize Memory Bandwidth\n",
        "This example demonstrates an in-place matrix transpose operation, which improves memory locality and reduces unnecessary data movement. By avoiding extra allocations and memory copying, we conserve memory bandwidth."
      ],
      "metadata": {
        "id": "1rmd3S1E86Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for in-place matrix transpose to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#define N 4  // Size of the matrix (NxN)\n",
        "\n",
        "// Function to transpose matrix in place\n",
        "void transpose_inplace(float matrix[N][N]) {\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = i + 1; j < N; j++) {\n",
        "            // Swap matrix[i][j] with matrix[j][i]\n",
        "            float temp = matrix[i][j];\n",
        "            matrix[i][j] = matrix[j][i];\n",
        "            matrix[j][i] = temp;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Initialize a 4x4 matrix\n",
        "    float matrix[N][N] = {\n",
        "        {1, 2, 3, 4},\n",
        "        {5, 6, 7, 8},\n",
        "        {9, 10, 11, 12},\n",
        "        {13, 14, 15, 16}\n",
        "    };\n",
        "\n",
        "    // Transpose the matrix in place\n",
        "    transpose_inplace(matrix);\n",
        "\n",
        "    // Print the transposed matrix\n",
        "    printf(\"Transposed Matrix:\\\\n\");\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            printf(\"%f \", matrix[i][j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"transpose_inplace.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc\n",
        "!gcc -o transpose_inplace transpose_inplace.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./transpose_inplace\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvUx8jnE840r",
        "outputId": "696d4593-b0a8-4d8a-b4a2-66eee83fc571"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transposed Matrix:\n",
            "1.000000 5.000000 9.000000 13.000000 \n",
            "2.000000 6.000000 10.000000 14.000000 \n",
            "3.000000 7.000000 11.000000 15.000000 \n",
            "4.000000 8.000000 12.000000 16.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Prefetching Example\n",
        "Prefetching: In this example, the __builtin_prefetch function is used to load data into the cache before it is needed. The prefetch distance (prefetch_distance) is set to 64, which means the data located 64 elements ahead of the current index is loaded into the cache in advance.\n",
        "Benefit: This technique reduces idle time by ensuring that the required data is already in the cache when the processor needs it.\n",
        "Use Case: Prefetching is especially useful in scenarios where memory access is a significant bottleneck, as it helps overlap data loading with computation.\n",
        "\n",
        "\n",
        "####In-Place Matrix Transpose Example\n",
        "In-Place Transpose: The transpose_inplace function transposes a square matrix without creating a new matrix. By swapping elements in place, we avoid the need for additional memory allocation and reduce unnecessary data movement.\n",
        "Benefit: This approach conserves memory bandwidth by limiting data copying and improving cache locality. When accessing adjacent rows and columns, the data is more likely to stay within the cache, reducing memory latency.\n",
        "Use Case: In large-scale HPC systems, reducing data movement across memory is essential for improving performance and conserving bandwidth. In-place algorithms are particularly valuable in applications where memory efficiency is critical."
      ],
      "metadata": {
        "id": "KWfeGsdJ9Bz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Optimization Techniques in HPC\n",
        "\n",
        "In High-Performance Computing (HPC), achieving peak performance requires advanced optimization techniques. These methods go beyond basic optimizations, offering ways to further enhance system throughput, reduce latency, and improve computational efficiency. This section covers three key techniques:\n",
        "\n",
        "### 1. **Speculative Execution**:\n",
        "This method allows modern processors to predict the path of future conditional operations and execute tasks before they are officially required. By doing so, speculative execution keeps the CPU busy during memory access or branch resolution, which mitigates latency. The technique involves risks such as incorrect predictions, which require rolling back instructions, and the potential for security vulnerabilities like the Spectre and Meltdown flaws.\n",
        "\n",
        "### 2. **Dynamic Scheduling**:\n",
        "Dynamic scheduling adjusts the assignment and order of tasks based on the system's current state. This real-time optimization is particularly useful for balancing loads across processors, ensuring all threads or cores are efficiently utilized, especially in systems where workloads vary unpredictably.\n",
        "\n",
        "### 3. **Software Prefetching**:\n",
        "This method involves explicitly loading data into the cache before it is required by computation. It minimizes cache misses, which are a significant source of latency in large-scale applications. By fetching data in advance, software prefetching reduces idle processor cycles caused by slow memory access.\n",
        "\n",
        "These advanced techniques, when used appropriately, can lead to substantial performance gains in HPC environments, allowing for more efficient computation and reduced latency.\n"
      ],
      "metadata": {
        "id": "CTLd7fVj9G1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Code Example 1: Speculative Execution\n",
        "This example demonstrates speculative execution using the likely() macro, which provides hints to the compiler about the expected outcome of a condition."
      ],
      "metadata": {
        "id": "RfCt0zn79g3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for speculative execution using likely() to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "\n",
        "#define likely(x) __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x) __builtin_expect(!!(x), 0)\n",
        "\n",
        "// Example function using speculative execution\n",
        "void speculative_example(int x) {\n",
        "    if (likely(x > 0)) {\n",
        "        printf(\"x is positive\\\\n\");\n",
        "    } else {\n",
        "        printf(\"x is non-positive\\\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int x = 5;\n",
        "    speculative_example(x);\n",
        "\n",
        "    x = -1;\n",
        "    speculative_example(x);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"speculative_execution.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc\n",
        "!gcc -o speculative_execution speculative_execution.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./speculative_execution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1qB73zL9arZ",
        "outputId": "5f4c81c7-e2c0-45fd-9833-6d3702ab644f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is positive\n",
            "x is non-positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dynamic Scheduling with OpenMP\n",
        "This example shows how to use dynamic scheduling in OpenMP, where tasks are dynamically assigned to threads based on their availability to optimize load balancing."
      ],
      "metadata": {
        "id": "qiBSWUGM9eBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for dynamic scheduling using OpenMP to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "// Function to simulate work\n",
        "void process(int i) {\n",
        "    // Simulate some computation\n",
        "    printf(\"Processing element %d by thread %d\\\\n\", i, omp_get_thread_num());\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 16;  // Example size\n",
        "    int data[n];\n",
        "\n",
        "    // Initialize data array\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        data[i] = i;\n",
        "    }\n",
        "\n",
        "    // Parallel loop with dynamic scheduling\n",
        "    #pragma omp parallel for schedule(dynamic)\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        process(data[i]);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"dynamic_scheduling.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc and OpenMP\n",
        "!gcc -fopenmp -o dynamic_scheduling dynamic_scheduling.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./dynamic_scheduling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFgM0t-M9dq5",
        "outputId": "aa602e38-2203-4d9c-d16c-1cc613fdc749"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing element 1 by thread 0\n",
            "Processing element 2 by thread 0\n",
            "Processing element 3 by thread 0\n",
            "Processing element 4 by thread 0\n",
            "Processing element 5 by thread 0\n",
            "Processing element 6 by thread 0\n",
            "Processing element 7 by thread 0\n",
            "Processing element 8 by thread 0\n",
            "Processing element 9 by thread 0\n",
            "Processing element 10 by thread 0\n",
            "Processing element 11 by thread 0\n",
            "Processing element 12 by thread 0\n",
            "Processing element 13 by thread 0\n",
            "Processing element 14 by thread 0\n",
            "Processing element 15 by thread 0\n",
            "Processing element 0 by thread 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### Software Prefetching\n",
        "This example demonstrates how software prefetching can be used to load data into the cache ahead of time, reducing cache misses and improving performance."
      ],
      "metadata": {
        "id": "MvHDIvkg-lQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for software prefetching to a file\n",
        "code = \"\"\"\n",
        "#include <stdio.h>\n",
        "\n",
        "// Function to simulate work with software prefetching\n",
        "void process(float* data, int n, int prefetch_distance) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        // Prefetch the next set of data\n",
        "        __builtin_prefetch(&data[i + prefetch_distance], 0, 1);\n",
        "        // Simulate processing\n",
        "        data[i] = data[i] * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 100000;  // Example size\n",
        "    int prefetch_distance = 64;  // Distance to prefetch\n",
        "    float data[n];\n",
        "\n",
        "    // Initialize data\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        data[i] = (float)i;\n",
        "    }\n",
        "\n",
        "    // Process the data with software prefetching\n",
        "    process(data, n, prefetch_distance);\n",
        "\n",
        "    // Print some results to verify\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%f \", data[i]);\n",
        "    }\n",
        "    printf(\"\\\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"software_prefetching.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc\n",
        "!gcc -o software_prefetching software_prefetching.c\n",
        "\n",
        "# Run the compiled program\n",
        "!./software_prefetching\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_od9xMu-mpa",
        "outputId": "20867b64-c13c-4460-a65a-c2b086f6de15"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000000 2.000000 4.000000 6.000000 8.000000 10.000000 12.000000 14.000000 16.000000 18.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Speculative Execution\n",
        "Concept: The likely() macro provides hints to the compiler about which branch of a condition is more likely to be taken. This allows the CPU to speculate on the most probable path of execution. In the code, when x > 0 is likely true, the CPU speculatively executes the corresponding instructions to minimize latency.\n",
        "Benefits: By predicting the branch, the CPU can execute instructions ahead of time, leading to higher throughput if the prediction is correct. However, if the prediction is wrong, the speculative instructions are discarded, which incurs some overhead.\n",
        "\n",
        "###Dynamic Scheduling\n",
        "Concept: The #pragma omp parallel for schedule(dynamic) directive in OpenMP dynamically assigns tasks to threads as they become available. This helps balance the workload among threads, especially when the amount of work per task is unpredictable.\n",
        "Benefits: Dynamic scheduling ensures that no thread is left idle while others have tasks to complete. This real-time adjustment of task assignment improves resource utilization in HPC environments with variable workloads.\n",
        "Software Prefetching\n",
        "Concept: Software prefetching proactively loads data into the cache before it is needed by the processor. The __builtin_prefetch() function is used to instruct the compiler to fetch data prefetch_distance iterations ahead of the current index.\n",
        "Benefits: By reducing cache misses, software prefetching minimizes memory access latency and improves overall performance, especially in loops where data is accessed sequentially over large arrays."
      ],
      "metadata": {
        "id": "Pt6FREB_9uxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Optimization of a Matrix Multiplication Algorithm\n",
        "\n",
        "Matrix multiplication is a core operation in many scientific computing and machine learning applications. As a fundamental building block for various algorithms, optimizing matrix multiplication can lead to substantial performance improvements, especially in high-performance computing (HPC) environments.\n",
        "\n",
        "### Basic Matrix Multiplication Algorithm:\n",
        "The standard algorithm for multiplying two n×n matrices, A and B, involves three nested loops that calculate the result matrix C:\n",
        "\n",
        "```c\n",
        "for (int i = 0; i < n; i++) {\n",
        "    for (int j = 0; j < n; j++) {\n",
        "        for (int k = 0; k < n; k++) {\n",
        "            C[i][j] += A[i][k] * B[k][j];\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "_SaEieFW9ywK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the C code for optimized matrix multiplication without FMA to a file\n",
        "code = \"\"\"\n",
        "#include <immintrin.h> // For AVX intrinsics\n",
        "#include <omp.h>       // For OpenMP\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>    // For random number generation\n",
        "\n",
        "// Optimized matrix multiplication with loop tiling, vectorization, dynamic scheduling, and prefetching\n",
        "void optimized_matrix_multiply(float* A, float* B, float* C, int n) {\n",
        "    int blockSize = 64; // Block size for loop tiling\n",
        "\n",
        "    // Parallelize the outer loops with dynamic scheduling for load balancing\n",
        "    #pragma omp parallel for collapse(2) schedule(dynamic)\n",
        "    for (int ii = 0; ii < n; ii += blockSize) {\n",
        "        for (int jj = 0; jj < n; jj += blockSize) {\n",
        "            for (int kk = 0; kk < n; kk += blockSize) {\n",
        "                for (int i = ii; i < ii + blockSize && i < n; i++) {\n",
        "                    for (int j = jj; j < jj + blockSize && j < n; j++) {\n",
        "                        __m256 sum = _mm256_setzero_ps();  // Initialize sum vector\n",
        "\n",
        "                        for (int k = kk; k < kk + blockSize && k < n; k += 8) {\n",
        "                            // Prefetch next data to minimize cache misses\n",
        "                            if (k + 16 < n) {\n",
        "                                __builtin_prefetch(&A[i * n + k + 16], 0, 1);\n",
        "                                __builtin_prefetch(&B[k * n + j + 16], 0, 1);\n",
        "                            }\n",
        "\n",
        "                            // Load 8 elements from matrices A and B\n",
        "                            __m256 a = _mm256_loadu_ps(&A[i * n + k]);\n",
        "                            __m256 b = _mm256_loadu_ps(&B[k * n + j]);\n",
        "\n",
        "                            // Perform multiplication and addition separately\n",
        "                            sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));\n",
        "                        }\n",
        "\n",
        "                        // Store the result of the vector sum in C[i][j]\n",
        "                        float buffer[8];\n",
        "                        _mm256_storeu_ps(buffer, sum);\n",
        "                        float total_sum = 0.0f;\n",
        "                        for (int s = 0; s < 8; s++) {\n",
        "                            total_sum += buffer[s];\n",
        "                        }\n",
        "\n",
        "                        #pragma omp atomic\n",
        "                        C[i * n + j] += total_sum;  // Accumulate result into C\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 512;  // Matrix size (512x512)\n",
        "    float *A = (float*)_mm_malloc(n * n * sizeof(float), 32);\n",
        "    float *B = (float*)_mm_malloc(n * n * sizeof(float), 32);\n",
        "    float *C = (float*)_mm_malloc(n * n * sizeof(float), 32);\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        A[i] = (float)(rand() % 100);\n",
        "        B[i] = (float)(rand() % 100);\n",
        "        C[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Perform optimized matrix multiplication\n",
        "    optimized_matrix_multiply(A, B, C, n);\n",
        "\n",
        "    // Print part of the result matrix\n",
        "    printf(\"Result Matrix (Partial):\\\\n\");\n",
        "    for (int i = 0; i < 5; i++) {\n",
        "        for (int j = 0; j < 5; j++) {\n",
        "            printf(\"%f \", C[i * n + j]);\n",
        "        }\n",
        "        printf(\"\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    _mm_free(A);\n",
        "    _mm_free(B);\n",
        "    _mm_free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"optimized_matrix_multiply.c\", \"w\") as file:\n",
        "    file.write(code)\n",
        "\n",
        "# Compile the C program with gcc and OpenMP support\n",
        "!gcc -fopenmp -o optimized_matrix_multiply optimized_matrix_multiply.c -mavx -lm\n",
        "\n",
        "# Run the compiled program\n",
        "!./optimized_matrix_multiply\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74WSaSm-csc",
        "outputId": "51a3fef4-c5e9-4e7e-9bfc-2968ba3a323f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result Matrix (Partial):\n",
            "1297882.000000 1294535.000000 1323434.000000 1339795.000000 1313726.000000 \n",
            "1291038.000000 1237264.000000 1257285.000000 1291693.000000 1290762.000000 \n",
            "1364409.000000 1318724.000000 1311531.000000 1360189.000000 1305473.000000 \n",
            "1273653.000000 1269043.000000 1284317.000000 1268923.000000 1260748.000000 \n",
            "1299746.000000 1363552.000000 1365350.000000 1344943.000000 1351995.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of Optimized Matrix Multiplication Code\n",
        "\n",
        "### 1. **Loop Unrolling**:\n",
        "By unrolling the inner loop in chunks of 8 elements, we reduce the overhead of loop control and increase instruction-level parallelism (ILP). This allows the CPU to process more data with fewer loop iterations, improving performance.\n",
        "\n",
        "### 2. **Loop Tiling (Blocking)**:\n",
        "Loop tiling breaks the matrix into smaller blocks (64x64 in this case) that fit into the cache. By operating on smaller blocks, the algorithm minimizes cache misses, improving data locality and speeding up memory access.\n",
        "\n",
        "### 3. **Vectorization with AVX**:\n",
        "AVX (Advanced Vector Extensions) is used to load and process multiple elements simultaneously. In the code, the `_mm256_loadu_ps()` intrinsic loads 8 elements of matrices A and B into 256-bit registers. The fused multiply-add operation (`_mm256_fmadd_ps()`) multiplies and accumulates the results into the sum vector, reducing the number of iterations required.\n",
        "\n",
        "### 4. **Dynamic Scheduling**:\n",
        "The `#pragma omp parallel for` directive with the `schedule(dynamic)` clause ensures that the workload is balanced across multiple threads dynamically. This prevents load imbalance, where some threads finish early while others are still working.\n",
        "\n",
        "### 5. **Software Prefetching**:\n",
        "The `__builtin_prefetch()` function is used to prefetch data into the cache before it is needed. This reduces the chance of cache misses by fetching data into the cache in advance, ensuring that the CPU has data ready when it reaches the next iteration.\n",
        "\n",
        "By combining these optimization techniques—loop unrolling, loop tiling, vectorization, dynamic scheduling, and software prefetching—the algorithm achieves significant improvements in performance. These optimizations are critical in HPC applications that rely on matrix operations, such as machine learning models, numerical simulations, and data processing tasks.\n"
      ],
      "metadata": {
        "id": "amRpH_zt-c_M"
      }
    }
  ]
}