{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import trim_messages, BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question  or check the correctness of the response based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "----\n",
    "Use latex formatting for equations. Enclose equations in $ sign.\n",
    "Use markdown formatting for everything else.\n",
    "Here's the user input: {query}\n",
    "----\n",
    "If this is a question answer the question, if this is an attempt to answer a previous question, check if the answer is correct.\n",
    "\"\"\"\n",
    "\n",
    "SUMMARIZATION_PROMPT=\"\"\"\n",
    "You are a summarizer of text tasked with summarizing the chat conversation between a student and a teaching assistant.\n",
    "Present in the summary are also thoughts of the teaching assistant on how to answer the student's queries which are\n",
    "not present in the actual conversation because the assistant wants to help the student arrive at the answer themselves instead\n",
    "of simply giving the answer.\n",
    "Here is the summary of their conversation so far, the assistant's extra thoughts and the last couple of messages exchanged:\n",
    "\n",
    "{context}\n",
    "\n",
    "\n",
    "Summarize this conversation so that another teaching assistant could read it and understand the problem the student is facing,\n",
    "how to solve the problem, and the progress of the student so far. They should be able to get all the context necessary to\n",
    "begin helping the student from your summary.\n",
    "Be concise and only include details of the actual conversation.\n",
    "\"\"\"\n",
    "\n",
    "SOCRATIC_PROMPT_TEMPLATE=\"\"\"\n",
    "You are a tutor trying to help a student learn a concept. You are helping them with a problem and want to help\n",
    "them understand the concepts by figuring out the solution themselves with only small nudges in the right direction.\n",
    "\n",
    "Here's the context that only you know that can be used to solve the problem. The summary: \n",
    "------------------------------\n",
    "\n",
    "{context}\n",
    "\n",
    "------------------------------\n",
    "Here's the student's most recent response: {query}\n",
    "\n",
    "\n",
    "Based on the solution to the question, use the socratic method to guide the student towards the answer.\n",
    "Do no answer the question, but provide hints or prompt the student to think of the next step.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SAFETY_PROMPT=\"\"\"\n",
    "You are a moderator checking if the student's queries are engaging with the data \n",
    "structures and algorithms material in a healthy and safe manner. Here's the student's \n",
    "most recent query/response:\n",
    "\n",
    "{query}\n",
    "\n",
    "Check if the query/response contains harmful content or if it is unrelated to the \n",
    "topic of data structures and algorithms. Give a binary score 'yes' or 'no' for safety.\n",
    "Score 'yes' if the query/response is safe and relevant to the topic and 'no' otherwise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "def get_vertex_embeddings():\n",
    "    vertexai.init(\n",
    "        project=os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "        location=os.environ.get(\"VERTEX_PROJECT_LOCATION\")\n",
    "        )\n",
    "\n",
    "    # Initialize the a specific Embeddings Model version\n",
    "    embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_retriever():\n",
    "    QDRANT_URL=os.environ.get(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY=os.environ.get(\"QDRANT_API_KEY\")\n",
    "    vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "        collection_name=\"dsa_notes\",\n",
    "        embedding=get_vertex_embeddings(),\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    return vectorstore.as_retriever(k=2)\n",
    "\n",
    "retriever = get_vector_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! ðŸ‘‹ How can I help you today? ðŸ˜Š \\n', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 1, 'candidates_token_count': 13, 'total_token_count': 14, 'cached_content_token_count': 0}, 'finish_reason': 'STOP'}, id='run-c8dfc5bd-d100-4f3c-9fbd-1be7377c29cd-0', usage_metadata={'input_tokens': 1, 'output_tokens': 13, 'total_tokens': 14})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "MODEL_ID=\"gemini-1.5-flash-001\"\n",
    "PROJECT_ID=\"genai-exchange-hackathon\"\n",
    "REGION=\"asia-south1\"\n",
    "llm = ChatVertexAI(model_name=MODEL_ID, location=REGION, project=PROJECT_ID)\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "llm2 = ChatOpenAI()\n",
    "\n",
    "# Router\n",
    "class CheckContext(BaseModel):\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Is the context enough to provide a response to the student's query? 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with router output\n",
    "llm_router = llm.with_structured_output(CheckContext)\n",
    "\n",
    "# Prompt\n",
    "system_router = \"\"\"\n",
    "You are a reasoning agent checking if the provided context is enough to answer a student's\n",
    "query. The query can be a question: if so you must check if the context is enough to\n",
    "answer the question. The query can also be a student's attempt at answering or taking the\n",
    "next step in answering a question: if so, you must check if the context is enough to\n",
    "check the student's response for correctness and be able to guide them towards the right\n",
    "path. Give a binary score 'yes' or 'no' to indicate whether the context is enough \n",
    "for the task. If responding to either type of query requires performing new arithematic\n",
    "not present in the context, score 'no'.\n",
    "\"\"\"\n",
    "\n",
    "prompt_router = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system_router),\n",
    "        ('human', \"Context: \\n\\n {context} \\n\\n Student query: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt_router | llm_router\n",
    "query = \"what is the quicksort?\"\n",
    "docs = retriever.invoke(query)\n",
    "doc_text = docs[1].page_content\n",
    "print(router.invoke({'context': doc_text, 'query': query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class CheckSafety(BaseModel):\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Is the query safe and relevant to data structures and algorithms? 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "llm_safety = llm2.with_structured_output(CheckSafety)\n",
    "\n",
    "prompt_safety = ChatPromptTemplate.from_template(SAFETY_PROMPT)\n",
    "\n",
    "safety = prompt_safety | llm_safety\n",
    "\n",
    "query = \"how can I use quicksort in white hacking?\"\n",
    "print(safety.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CheckSafety)\n",
    "obj = safety.invoke({\"query\": query})\n",
    "obj.binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_socratic = ChatPromptTemplate.from_template(SOCRATIC_PROMPT_TEMPLATE)\n",
    "socratic = prompt_socratic | llm | StrOutputParser()\n",
    "print(socratic.invoke({'context': doc_text, 'query': query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = ChatPromptTemplate.from_template(SUMMARIZATION_PROMPT)\n",
    "summary = prompt_summary | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The student asked how quicksort can be used in white hacking. The teaching assistant prompted the student to consider quicksort's efficiency and its potential application in analyzing or manipulating data for security purposes. The teaching assistant also asked the student to think about what white hackers do to improve security or identify vulnerabilities. The student has not yet provided a response. \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.invoke({'context': query + socratic.invoke({'context': doc_text, 'query': query})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/grpc/_channel.py:1181\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1175\u001b[0m (\n\u001b[1;32m   1176\u001b[0m     state,\n\u001b[1;32m   1177\u001b[0m     call,\n\u001b[1;32m   1178\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1179\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1180\u001b[0m )\n\u001b[0;32m-> 1181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.182.170:443 {grpc_message:\"Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\", grpc_status:8, created_time:\"2024-09-20T17:19:54.65545+05:30\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m context_of_no_consequence \u001b[38;5;241m=\u001b[39m query \u001b[38;5;241m+\u001b[39m \u001b[43msocratic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:277\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    276\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    287\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:777\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    776\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:634\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    633\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    635\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    636\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    638\u001b[0m ]\n\u001b[1;32m    639\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:846\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 846\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:1165\u001b[0m, in \u001b[0;36mChatVertexAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_gemini_model:\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_non_gemini(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_gemini\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:1322\u001b[0m, in \u001b[0;36mChatVertexAI._generate_gemini\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_gemini\u001b[39m(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1316\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m   1321\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request_gemini(messages\u001b[38;5;241m=\u001b[39mmessages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gemini_response_to_chat_result(response)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:610\u001b[0m, in \u001b[0;36m_completion_with_retry\u001b[0;34m(generation_method, max_retries, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    605\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    606\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gemini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[1;32m    609\u001b[0m )\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:603\u001b[0m, in \u001b[0;36m_completion_with_retry.<locals>._completion_with_retry_inner\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry_inner\u001b[39m(generation_method: Callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py:2275\u001b[0m, in \u001b[0;36mPredictionServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 2275\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai."
     ]
    }
   ],
   "source": [
    "context_of_no_consequence = query + socratic.invoke({'context': doc_text, 'query': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_of_no_consequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcontext_of_no_consequence\u001b[49m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context_of_no_consequence' is not defined"
     ]
    }
   ],
   "source": [
    "summary.invoke({'context': context_of_no_consequence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "import requests\n",
    "from typing import Annotated\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "@tool\n",
    "def wolfram_call(query: str) -> str:\n",
    "    \"\"\"Calls the wolfram alpha api on query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to use as a parameter in the function call. For example, \"Solve 3x=5\"\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the result of the api call on the query.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"input\": query,\n",
    "        \"appid\": os.environ.get(\"WOLFRAM_ALPHA_APPID\"),\n",
    "        \"format\": \"plaintext\",\n",
    "        \"output\": \"json\",\n",
    "    }\n",
    "    response = requests.get(\"https://api.wolframalpha.com/v2/query\", params=params)\n",
    "    full_response = response.json()\n",
    "    pods = [x[\"subpods\"] for x in full_response[\"queryresult\"][\"pods\"]]\n",
    "    return str(pods)\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value, you\n",
    "    should print it out with `print(...)`. This is visible to the student.\n",
    "\n",
    "    Args:\n",
    "        code (Annotated[str, &quot;The python code to execute to generate your chart.&quot;]): The python code to execute\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n ```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return result_str\n",
    "\n",
    "@tool\n",
    "def notes_search(query: str) -> str:\n",
    "    \"\"\"Calls the vector database of data stuctures and algorithm notes to get any definitions, theorems, axioms, proofs, examples, information for summarizing.\n",
    "    The database contain notes for . It covers the following topics:\n",
    "    limits, derivatives, derivative applications, integrals, integral applications, integration techniques, more integral applications, parametric and polar, series and sequences, vectors, 3-d space, partial derivatives, applications of partial derivatives, multiple integrals, line integrals, surface integrals, various calculus proofs, review of trignometry and functions.\n",
    "\n",
    "    Args:\n",
    "        query (str): the query sent to the vector database.\n",
    "\n",
    "    Returns:\n",
    "        str: Returns the documents retrieved from the vector database based on the query.\n",
    "    \"\"\"\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "web_search = TavilySearchResults(max_results=2)\n",
    "\n",
    "tools = [wolfram_call, python_repl, notes_search, web_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    safe: str = \"\"\n",
    "    query: str = \"\"\n",
    "    context: str = \"\"\n",
    "    solved: str = \"\"\n",
    "    response: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Quicksort is a sorting algorithm, and while it can be used in various applications, including security, it's not directly related to white hacking. White hacking involves ethical hacking to identify and fix vulnerabilities in systems. While sorting algorithms can be useful in analyzing data related to security, they aren't the primary tools used for white hacking. \\n\\nTo learn more about white hacking, you should focus on topics like:\\n\\n* **Vulnerability Assessment:** Identifying weaknesses in systems and software.\\n* **Penetration Testing:** Simulating attacks to find exploitable vulnerabilities.\\n* **Security Auditing:** Reviewing security practices and controls.\\n* **Network Security:** Understanding how networks function and how to protect them.\\n* **Cryptography:** Studying the principles and techniques of secure communication.\\n\\nThese are just some examples, and there are many other areas within white hacking that you can explore. There are also resources available online and through educational institutions that can help you learn more about the field. \\n\" response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'LOW', 'blocked': False, 'severity': 'HARM_SEVERITY_HIGH'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_MEDIUM'}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False, 'severity': 'HARM_SEVERITY_NEGLIGIBLE'}], 'usage_metadata': {'prompt_token_count': 550, 'candidates_token_count': 200, 'total_token_count': 750, 'cached_content_token_count': 0}, 'finish_reason': 'STOP'} id='run-49b0d19e-b3f6-4403-9662-28f91128608c-0' usage_metadata={'input_tokens': 550, 'output_tokens': 200, 'total_tokens': 750}\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "SOLVER_PROMPT=\"\"\"You are a teaching assistant helping a student solve problmes in data\n",
    "structures and algorithms.\n",
    "Here is a summary of your conversation with the student so far:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the student's most recent response: {query}\n",
    "\n",
    "If the student asked a new question, solve it completely. If the student responded with\n",
    "their solution, check if it is correct. You have a number of tools available for this task:\n",
    "notes_search: search through multiple books and problem sets on the topic,\n",
    "web_search: search the web for facts and information not found in the notes,\n",
    "wolfram_call: a very powerful calculator,\n",
    "python_repl: a python interpreter that can be used to run and check code, solve math problems or create graphs to help the student visualize the solution better.\n",
    "Use any of these as many times as needed to solve the problem as accurately as possible. \n",
    "Be concise in your solution.\n",
    "\"\"\"\n",
    "\n",
    "prompt_test = ChatPromptTemplate.from_template(SOLVER_PROMPT)\n",
    "solver = prompt_test | llm.bind_tools(tools)\n",
    "print(solver.invoke({'context': '', 'query': query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert chains to nodes\n",
    "def safety_node(state: AgentState):\n",
    "    if 'yes' == safety.invoke({\"query\": state[\"query\"]}).binary_score:\n",
    "        state[\"safe\"] = 'yes'\n",
    "    else:\n",
    "        state[\"safe\"] = \"no\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def router_node(state: AgentState):\n",
    "    if 'yes' ==  router.invoke({'context': state['context'], 'query': state['query']}).binary_score:\n",
    "        state[\"solved\"] = 'yes'\n",
    "    else:\n",
    "        state[\"solved\"] = 'no'\n",
    "    return state\n",
    "        \n",
    "def socratic_node(state: AgentState):\n",
    "    state[\"response\"] = socratic.invoke({'context': state['context'], 'query': state['query']})\n",
    "    return state\n",
    "\n",
    "def solver_node(state: AgentState):\n",
    "    state['context'] += solver.invoke({'context': state['context'], 'query': state['query']}).content\n",
    "    return state\n",
    "def summary_node(state: AgentState):\n",
    "    state['context'] = summary.invoke({'context': state['context'] + state['query'] + state['response']})\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "tool_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"safety\", safety_node)\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"socratic\", socratic_node)\n",
    "workflow.add_node(\"solver\", solver_node)\n",
    "# workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"summary\", summary_node)\n",
    "\n",
    "workflow.set_entry_point(\"safety\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"safety\",\n",
    "    lambda x: x[\"safe\"],\n",
    "    {\n",
    "        \"yes\": \"router\",\n",
    "        \"no\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda x: x['solved'],\n",
    "    {\n",
    "        \"yes\": \"socratic\",\n",
    "        \"no\": \"solver\",\n",
    "    },\n",
    ")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"solver\",\n",
    "#     tools_condition,\n",
    "#     {\n",
    "#         \"tools\": \"tools\",\n",
    "#         \"__end__\": \"socratic\",\n",
    "#     }\n",
    "# )\n",
    "# workflow.add_edge(\"tools\", \"solver\")\n",
    "workflow.add_edge(\"socratic\", \"summary\")\n",
    "workflow.add_edge(\"solver\", \"socratic\")\n",
    "workflow.add_edge(\"summary\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "query = \"What is hashing?\"\n",
    "unsafe_query = \"what is hacking?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'safe': 'yes', 'query': 'What is hashing?', 'context': '', 'response': ''}\n",
      "{'safe': 'yes', 'query': 'What is hashing?', 'context': '', 'solved': 'no', 'response': ''}\n",
      "{'safe': 'yes', 'query': 'What is hashing?', 'context': \"Hashing is a technique used to map keys to indices in a hash table. It allows for efficient search, insertion, and deletion of data. \\n\\nHere's how it works:\\n\\n1. **Hash Function:** A hash function takes a key as input and returns a hash value (an integer). This hash value represents the index in the hash table where the key-value pair will be stored.\\n\\n2. **Hash Table:** A hash table is a data structure that stores key-value pairs. It consists of an array of buckets, where each bucket can hold multiple key-value pairs.\\n\\n3. **Collision Handling:** When two different keys map to the same index, a collision occurs. Collision handling techniques, like chaining or open addressing, are used to resolve these collisions.\\n\\nHashing is used in various applications, including:\\n\\n* **Dictionaries:** In Python, dictionaries use hashing to store key-value pairs.\\n* **Databases:** Hashing is used for indexing and searching data in databases.\\n* **Cryptographic Applications:** Hashing is used to generate fingerprints for data security.\\n\\nLet me know if you have any more questions about hashing!\\n\", 'solved': 'no', 'response': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'safe': 'yes', 'query': 'What is hashing?', 'context': \"Hashing is a technique used to map keys to indices in a hash table. It allows for efficient search, insertion, and deletion of data. \\n\\nHere's how it works:\\n\\n1. **Hash Function:** A hash function takes a key as input and returns a hash value (an integer). This hash value represents the index in the hash table where the key-value pair will be stored.\\n\\n2. **Hash Table:** A hash table is a data structure that stores key-value pairs. It consists of an array of buckets, where each bucket can hold multiple key-value pairs.\\n\\n3. **Collision Handling:** When two different keys map to the same index, a collision occurs. Collision handling techniques, like chaining or open addressing, are used to resolve these collisions.\\n\\nHashing is used in various applications, including:\\n\\n* **Dictionaries:** In Python, dictionaries use hashing to store key-value pairs.\\n* **Databases:** Hashing is used for indexing and searching data in databases.\\n* **Cryptographic Applications:** Hashing is used to generate fingerprints for data security.\\n\\nLet me know if you have any more questions about hashing!\\n\", 'solved': 'no', 'response': \"That's a great question!  We just covered the basics of hashing.  Let's break down what we learned.  Remember what a hash function does?  Think about how that relates to a hash table and how they work together. \\n\"}\n",
      "{'safe': 'yes', 'query': 'What is hashing?', 'context': 'The student asked \"What is hashing?\". The TA provided a comprehensive explanation of hashing, covering the concepts of hash functions, hash tables, collision handling, and applications of hashing. The student has not yet responded to the TA\\'s explanation, so the TA is prompting them to think about what they have just learned by asking them to consider the relationship between hash functions and hash tables.  \\n', 'solved': 'no', 'response': \"That's a great question!  We just covered the basics of hashing.  Let's break down what we learned.  Remember what a hash function does?  Think about how that relates to a hash table and how they work together. \\n\"}\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"query\": query, \"context\": \"\", \"response\": \"\"}):\n",
    "    for value in event.values():\n",
    "        # if value[\"safe\"] == \"no\":\n",
    "        #     print(\"Sorry, I can't answer that request.\")\n",
    "        # else:\n",
    "        #     print(\"Content:\", value[\"context\"])\n",
    "        #     print(\"Response:\", value[\"response\"])\n",
    "        #     # print(\"Assistant:\", value[\"response\"].content)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "events = graph.stream({\"query\": query})\n",
    "\n",
    "for event in events:\n",
    "    for value in event.values():\n",
    "        value[\"response\"].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
