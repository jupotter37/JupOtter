{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Heaps\n",
    "\n",
    "### 12.1 Data structures overview\n",
    "\n",
    "**Point:** organize date in a way that you can access it quickly and usefully\n",
    "\n",
    "**Examples:** \n",
    "* Lists\n",
    "* Stacks\n",
    "* Queues (recursions)\n",
    "* Heaps\n",
    "* Search trees\n",
    "* Hash tables\n",
    "* Balloon filters\n",
    "* Union find structures\n",
    "\n",
    "**Why so many?** different data structures support different sets of operations and are therefore well suited for different types of tasks\n",
    "\n",
    "**Pros and cons of the basic ones**  \n",
    "* Generally speaking the fewer operations a data structure supports \n",
    "    * the faster the operations will be\n",
    "    * And the smaller the space overhead require by the data structure\n",
    "\n",
    "What are the operations that you need a data structure to export? \n",
    "Four levels of data structure knowledge that someone might have:\n",
    "* **Zero** ignorance, for someone who has never heard of a data structure,\n",
    "* **One** party level awareness. \n",
    "* **Two** someone who has solid literacy about data structures. \n",
    "    * comfortable using them as a client in their own programs, \n",
    "    * they have a good sense of which data structures are appropriate for which types of tasks. \n",
    "* **Three** hardcore programmers and computer scientists. \n",
    "    * they actually have an understanding of the guts of these data structures. \n",
    "    * How they are coded up, how they're implemented, not merely how they are used. \n",
    "    \n",
    "focus of this part of the course: on taking up to level two. \n",
    "\n",
    "### 12.2 Heaps operations and applications\n",
    "\n",
    "**Structure**\n",
    "* Heap is a container for a bunch of objects. \n",
    "* And each of these objects should have a key, like a number so that for any given objects you can compare their keys and say one key is bigger than the other key. \n",
    "\n",
    "**Operations supported**\n",
    "* insertion – add new object to a heap \n",
    "* extract min – remove an object from a heap with min key value \n",
    "    * [duplicates broken arbitrary: when you extract min from a heap they may have duplicate key values then there's no specification about which one you get]\n",
    "  \n",
    "Max heap works the same way\n",
    "* if you have only min heap at your disposal, you can negate the sign of all of the key values before you insert them, And then extract min will actually extract, the max key value\n",
    "\n",
    "**Running time**\n",
    "* insertion: O(log_2(n))\n",
    "* extract min: O(log_2(n))\n",
    "* n – number of objects in the heap\n",
    "\n",
    "**Additional operations**\n",
    "* Heapify – initialize a heap in O(n) time \n",
    "    * if you have N things and you want to put them all in a heap, \n",
    "    * obviously you could just invoke insert once per each object. \n",
    "    * If you have N objects, it seems like that would take N times log N time, log N for each of the N inserts. \n",
    "    * But there's a slick way to do them in a batch, which takes only linear time.  \n",
    "* Delete an arbitrary element from the middle of a heap in O(log(n)) time. \n",
    "    \n",
    "**Canonical use of heap**\n",
    "* your program is doing repeated minimum computations. \n",
    "    * Especially via exhaustive search, \n",
    "    \n",
    "**Examples**\n",
    "  \n",
    "1. Sorting and unsorted array  \n",
    "    * selection-sort\n",
    "        * scan through the unsorted array. \n",
    "        * find the minimum element; \n",
    "        * put that in the first position. \n",
    "        * You scan through the other N-1 elements; etc\n",
    "        * does a linear number of linear scans through this array. \n",
    "        * Quadratic time algorithm. \n",
    "    * heap sort.\n",
    "        * insert all of the elements from the array into the heap. \n",
    "        * extract the minimum one by one. \n",
    "        * What is the running time of heap sort? \n",
    "            * Well, we insert each element once and we extract each element once \n",
    "            * so that's 2n heap operations and \n",
    "            * you can count on heaps being implemented so that every operation takes logarithmic time. \n",
    "            * So we have a linear number of logarithmic time operations for running time of = O(nlog(n))\n",
    "            * this is exactly the running time we had for merge sort; \n",
    "            * this was exactly the average running time we got for randomized quick sort. \n",
    "        * Moreover, Heap Sort is a comparison based sorting algorithm: we don't use any data about the key elements. \n",
    "            * And, as some of you may have seen in an optional video, there does not exist a comparison-based sorting algorithm with running time better than N log N. \n",
    "            * So for the question, can we do better? The answer is no \n",
    "        * Is it as good as quick sort? Hm, maybe not quite but its close it's.   \n",
    "\n",
    "  \n",
    "2. Priority queue. \n",
    "    * you've been tasked with writing software that performs a simulation of the physical world. \n",
    "    * why would a heap come up in a simulation context? \n",
    "    * Well, the objects in this application are going to be events records.\n",
    "    * So when you have event records like this, there's a very natural key, which is just the timestamp, the time at which this event in the future is scheduled to occur. \n",
    "    * A problem which has to get solved over and over again is you have to figure out what's the next event that's going to occur. \n",
    "        * You have to know what other events to schedule; \n",
    "        * you have to update the screen and so on. \n",
    "    * A very silly thing you could do is just maintain an unordered list of all of the events that have ever been scheduled and do a linear path through them and compute the minimum. \n",
    "    * if you're storing these event records in a heap with the key being the time stamps then when you extract them using logarithmic time.  \n",
    "     \n",
    "  \n",
    "3. Median maintenance \n",
    "    * I'm going to pass you index cards, one at a time, where there's a number written on each index card. \n",
    "    * Your responsibility is to tell me at each time step the median of the number that I've passed you so far. \n",
    "    * So, after I've given you the first eleven numbers you should tell me as quickly as possible the sixth smallest \n",
    "    * we know how to compute the median in linear time but the last thing I want is for you to be doing a linear time computation every single time step. \n",
    "    * Algorithm to use O(log(i)) time at each step i\n",
    "    * What if you use two heaps\n",
    "        * The first heap we'll call H_low – supports extract max. \n",
    "        * Another heap H_high – supports extract min. \n",
    "        * And the key idea is to maintain the invariant that the smallest half of the numbers that you've seen so far are all in the low heap. \n",
    "        * And the largest half of the numbers that you've seen so far are all in the high heap. \n",
    "    * Now given this key idea of splitting the elements in half, according to the two heaps. You need two realizations. \n",
    "        * So first of all, you have to prove you can actually maintain this invariant with only O(logI) work in step I. \n",
    "            * So let's suppose we've already processed the first twenty numbers, \n",
    "            * and the smallest ten of them we've all worked hard to, to put only in H_low. \n",
    "            * And the biggest ten of them we've worked hard to put only in H high. \n",
    "            * Now, here's a preliminary observation.\n",
    "            * What's true, so what do we know about the maximum element in h low? \n",
    "            * Well these are the tenth smallest overall and the maximum then is the biggest of the tenth smallest. \n",
    "            * So that would be a tenth order statistic, so the tenth order overall. \n",
    "            * Now what about the high heap? What's its minimum value? \n",
    "            * Well those are the biggest ten values. \n",
    "            * So the minimum of the ten biggest values would be the eleventh order statistic. \n",
    "            * these are in fact the two medians \n",
    "            * When this new element comes in, the twenty-first element comes in, we need to know which heap to insert it into \n",
    "            * if it's smaller than the tenth order statistic then it's still gonna be in the bottom, then it's in the bottom half of the elements and needs to go in the low heap. \n",
    "            * If it's bigger than the eleventh order statistic, if it's bigger than the minimum value of the high heap then that's where it belongs, in the high heap. \n",
    "            * If it's wedged in between the tenth and eleventh order of statistics, it doesn't matter. We can put it in either one. This is the new median anyways. \n",
    "            * Now, we're not done yet with this first point, because there's a problem with potential imbalance. \n",
    "            * So imagine that the twenty-first element comes up and it's less than the maximum of the low heap, so we stick it in the low heap and now that has a population of eleven. \n",
    "            * And now imagine the twenty-second number comes up and that again is less than the maximum element in the low heap, so again we have to insert it in the low heap. \n",
    "            * Now we have twelve elements in the low heap, but we only have ten in the right heap. \n",
    "            * So we don't have a 50. 50, 50 split of the numbers but we could easily re-balance we just extract the max from the low heap and we insert it into the high heap. \n",
    "        * Second of all, you have to realize this invariant allows you to solve the desired problem. \n",
    "            * second point given that this invariant is true at each time step. \n",
    "            * How do we compute the median? \n",
    "            * it's going to be either the maximum of the low heap and/or the minimum of the high heap depending on whether I is even or odd. \n",
    "            * If it's even, both of those are medians. \n",
    "            * If I is odd, then it's just whichever heap has one more element than the other one. \n",
    "  \n",
    "  \n",
    "4. Speeding up Dijkstra's shortest path algorithm. \n",
    "* In another video\n",
    "* Correct data structures can speeв up algorithms, especially when you're doing things like minimum computations in an inner loop. \n",
    "* So Dijkstra's shortest path algorithm has a central while loop.\n",
    "    * it operates once per vertex of the graph. \n",
    "    * it seems like what each iteration of the while loop does is an exhaustive search through the edges of the graph, computing a minimum. \n",
    "    * So if we think about the work performed in this naive implementation, it's exactly in the wheel-house of a heap, right. \n",
    "    * the run time drops from this really rather large polynomial (n * m) down to something which is almost linear time.O(mlog(n)). \n",
    "    * Where m is the number of edges and n is the number of vertices. \n",
    "\n",
    "\n",
    "### 12.3 Heaps implementation details  \n",
    "  \n",
    "* If you wanna know how heaps really work, it's important to keep in mind simultaneously Two different views of a heap \n",
    "    * One, as a tree \n",
    "        * rooted\n",
    "        * binary\n",
    "        * as complete as possible      \n",
    "    * And one, as a, array. \n",
    "\n",
    "\n",
    "**Tree: Heap property** \n",
    "* imposes an ordering on how the different objects are arranged in this tree structure. \n",
    "* at every single node X of this tree the key of the object stored in X should be no more than the keys of Xs children. \n",
    "* Notice that I am allowing duplicates. \n",
    "* Another thing to realize is that while the heap property imposes useful structure on how the objects can be arranged it in no way uniquely pins down their structure.\n",
    "    * So this exact same set of seven keys could be arranged differently and it would still be a heap. \n",
    "* The important thing is that, in any heap, the root has to have a minimum value key. \n",
    "\n",
    "**Array**\n",
    "* it's much more efficient in a heap to just directly implement it as an array.\n",
    "    * We're just going to group the nodes of this tree by their level. \n",
    "    * And now we just stick these notes into the array, one level at a time. \n",
    "    * On the one hand we have this nice, logical tree structure.\n",
    "    * On the other hand we have this array implementation and we're not wasting any space on the usual pointers you would have in a tree to traverse between parents and children. \n",
    "    * So where's the free lunch coming from? \n",
    "        * Well the reason is that because we're able to keep this binary tree as balanced as possible, we don't actually need pointers to figure out who is whose parent and who is whose child. \n",
    "        * We can just read that off directly From the positions in the array. \n",
    "        * If you have a node in the i-th position (i ≠ 1), \n",
    "            * if i is even, then the parent is just the position of i/2, \n",
    "            * and if I is odd, I/2 rounded to min\n",
    "        * if we have an object in position i. \n",
    "            * children of i are gonna be at the position 2i and 2i+1. \n",
    "            * Of course those may be empty so if you have a leaf of course that doesn't have any children and then maybe one node that has only one child.\n",
    "     * So rather than traversing pointers it's very easy to just go from a node to its parent to either one of its children just by doing these appropriate trivial calculations with respects to its position. \n",
    "     \n",
    "**Advantages of heaps**    \n",
    "* (1) storage – we're storing objects directly in an array, with no extra space. \n",
    "* (2) not only do we not have to have a space for pointers But we don't even have to do any traversing. \n",
    "    * All we do are these really simple. \n",
    "    * divide by two or multiply two operations And using bit shifting tricks. \n",
    "    * Those can also be implemented extremely quickly.\n",
    "    \n",
    "**Implementation of insertion**\n",
    "* rather than give you any pseudo code, I'm just going to show you how these work by example. \n",
    "* Insertion (given a key k)\n",
    "    * So let's suppose we have an existing heap,\n",
    "    * And we're called upon to insert a new object. \n",
    "    * Let's say with a key value K. \n",
    "    * Now remember heaps are always suppose to be perfectly balanced binary trees. \n",
    "    * So if we want to maintain the property that this tree is perfectly balanced is pretty only one place we can try to put the new key K and that's as the next leaf. \n",
    "        * That is it's going to be the new right most leaf on the bottom level. \n",
    "        * Or in terms of the array implementation we just stick it in the first non empty slot in the array \n",
    "        * And if we keep track of the array size we're getting constant time of course know where to put the new key. \n",
    "    * Now whether or not we can get away with this depends on what the actual key value K \n",
    "        * if no violationo of the key heap rule – ew are done\n",
    "        * And in these lucky events insertion is even taking constant time. \n",
    "        * Really all we're doing is putting elements at the end of an array and not doing any rearranging. \n",
    "        * If violation – swap the positions of the five and the twelve,\n",
    "        * and that's something that of course can be done in constant time, \n",
    "        * untill the лун rule is restored\n",
    "    * One subtle point that you might be thinking that in addition to screwing up at the root node, that messing around with his children, maybe we could have screwed up the twill by messing around with its parent.\n",
    "        * in general, as you push up the object up the tree, there's only going to be one possible edge that could be out of order \n",
    "        * And that's between where this object currently resides and whatever its parent is. \n",
    "* swap = bubble up = sift up = heapify up\n",
    "\n",
    "So:  \n",
    "* Step 1: stick k at end of last level\n",
    "* Step 2: bubble-up k until heap property is restored (ie key of лэы parent is =< k)\n",
    "\n",
    "Check:  \n",
    "* bubbling up process must stop with heap property restored\n",
    "* running time: con\n",
    "    * because this is a perfectly balanced binary tree log_2(n) \n",
    "    * And what is the running time of this insertion procedure while you only do a constant amount of work at each level, just doing the swap and comparison and \n",
    "    * then in the worst case, you'll have to swap at every single level and there is a log_2(n) number of levels.\n",
    "    \n",
    "\n",
    "**Implementation of extraction of min**\n",
    "* do this by example and it's gonna be by repeating the bubble down procedure. \n",
    "* So the Extract main operation is responsible for removing from the heap an object with minimum key value and handing it back to the client on a silver platter. \n",
    "    * So it pretty much have to whip out the root. \n",
    "    * this removal of course leaves a gaping hole in our tree structure and that's no good. \n",
    "    * there's pretty much only one node that could fill this hole without causing other problems with the tree structure, and that is the very last node. \n",
    "    * So the rightmost leaf at the bottom level one that simple fix is to swap that up and have that take the place of the original root. \n",
    "    * now we've totally screwed up the heap property,\n",
    "    * when you're trying to push notes down to the rightful position in the tree, there is two different swaps you could do, one for the left child, one for the right child and the decision that we make matters \n",
    "    * We really want to swap it with the smaller child. \n",
    "        * Remember, every node should have a key bigger than both of its children. \n",
    "        * So if we're going to swap up either of children, one of those is going to become the parent of the other. \n",
    "        * The parent is supposed to be smaller, so evidently we should take the smaller of the two children and swap the thirteen with that. \n",
    "        * We bubble bown untill the violation is nit fixed\n",
    "        \n",
    "To check:        \n",
    "* first of all you should check that in fact this bubble down has to at some point halt And when it halts you do have a bona fide heap. The heap property is definitely restored \n",
    "* And second of all the running time is, is logarithmic. \n",
    "    * perfectly balanced is essentially the log_2(number of elements in the heap) \n",
    "    * and in bubbling down all you do is a constant amount of work per level: all you have to do is a couple comparisons and swap. \n",
    "    \n",
    "So:\n",
    "* Step 1: delete root\n",
    "* Step 2: place last element to the root node\n",
    "* Step 3: bubble it down by swaping with smallest of children\n",
    "\n",
    "\n",
    "# 13. Balanced binary search trees\n",
    "\n",
    "### 13.1 BBST operations and applications\n",
    "\n",
    "**What is it for**\n",
    "* Think about it as a dynamic version of a sorted array. \n",
    "    * you can do pretty much anything on the data that you could if it was just the static sorted array. \n",
    "    * But in addition, the data structure can accommodate insertions and deletions. \n",
    "    * Thus you can accommodate a dynamic set of data that you're storing overtime. \n",
    "  \n",
    "* let's just start with the sorted array and look at some of the things you can easily do with data.\n",
    "    * So let's think about an array that has numerical data \n",
    "    * although, generally as we've said, in data structures is usually associated other data that's what you actually care about and the numbers are just some unique identifier for each of the records. \n",
    "* (1) SEARCH: First of all, you can search and recall that searching in a sorted array is generally done using binary search so this is how we used to look up phone numbers when we have physical phone books. \n",
    "    * You'd start in the middle of the phone book, if the name you were looking for was less than the midpoint, you recurse on the left hand side, otherwise you'd recurse on the right hand side. \n",
    "    * As we discussed back in the Master Method Lectures long ago, this is going to run in logarithmic time.\n",
    "    * O(log(N))\n",
    "* (2) SELECT find i-th statistic \n",
    "    * Something else we discussed in previous lectures is the selection problem. \n",
    "    * So previously, we discussed this in much harder context of unsorted arrays. \n",
    "    * we worked very hard to get a linear time algorithm for retreiving i-statistic problem in unsorted arrays. \n",
    "    * Now, in a sorted array it's pretty easy problem, just return whatever element happens to be in the seventeenth position of the array \n",
    "    * It's already sorted constant time, you can solve the selection problem.\n",
    "    * O(1)\n",
    "    * MIN/MAX Of course, two special cases of the selection problem are finding the minimum element of the array. That's just if the order statistic problem with i = 1and the maximum element, that's just i = n. \n",
    "    * What other operations could we implement on a sorted array? \n",
    "* (3) Predecessor and Successor operations. \n",
    "    * And so the way these work is, you start with one element. \n",
    "    * So, say you start with a pointer to the 23, and you want to know where in this array is the next smallest element. \n",
    "    * That's the predecessor query and the successor operation returns the next largest element in the array. \n",
    "    * In a sorted array, these are trivial, you can return them in constant time. \n",
    "* (4) Rank operation \n",
    "    * rank = how many key stored in the data structure are less than or equal to a given key. \n",
    "    * implementing the rank operation in sorted array is really no harder than implementing search. \n",
    "    * All you do is search for the given key and wherever it is search terminates in the array you just look at the position in the array and that's the rank of that element. \n",
    "    * If you do an unsuccessful search, say you search for 21, well then you get stuck in between the 17 and the 23, and at that point you can conclude that the rank of 21 in this array is five. \n",
    "* (5) Output or print stored keys in sorted order\n",
    "    * And naturally, all you do here is a single scan from left to right through the array, outputting whatever element you see next. \n",
    "    * The time required is constant per element or linear overall.   \n",
    "  \n",
    "These are operations that operate on a static data set which is not changing overtime. But the world in general is dynamic. \n",
    "* There is one of the data structure that not only supports these kinds of operations but also, insertions and deletions. \n",
    "* Now of course it's not that it's impossible to implement insert or delete in a sorted array, it's just that they're going to run way too slow. \n",
    "* In general, you have to copy over a linear amount of stuff on an insertion or deletion if you want to maintain the sorted array property. \n",
    "* So this linear time performance when insertion and deletion is unacceptable unless you barely ever do those operations. \n",
    "* the raison d'etre of the Balanced Binary Search Tree is to implement this exact same set of operations just as rich as that's supported by a sorted array but in addition, insertions and deletions. \n",
    "  \n",
    "A few of these operations won't be quite as fast or we have to give up a little bit:\n",
    "* instead of constant time, the one in logarithmic time \n",
    "* and we still got logarithmic time for all of these operations, \n",
    "* linear time for outputting the elements in sort of order plus, \n",
    "* we'll be able to insert and delete in logarithmic time \n",
    "\n",
    "Balanced Binary Search Tree \n",
    "* will act like a sorted array \n",
    "* plus, it will have fast, meaning logarithmic time inserts and deletes. \n",
    "* (2) Select (i-th statistic) \n",
    "    * runs in constant time in a sorted array and here it's going to take logarithmic, so we'll give up a little bit on the selection problem but we'll still be able to do it quite quickly. \n",
    "    * Even on the special cases of finding the minimum or finding the maximum in our, in our data structure, we're going to need logarithmic time in general. \n",
    "* (3) Same thing for finding predecessors and successors they're not, they're no longer constant time, they go with logarithmic. \n",
    "* (4) Rank took as logarithmic time in the sorted array version and that will remain logarithmic here. \n",
    "* (5)As we'll see, we lose essentially nothing over the sorted array, if we want to output the key values in sorted order say from smallest to largest. \n",
    "\n",
    "**Comparison/Recap**\n",
    "* Static sorted array: if you have a static data set, you don't need inserts and deletes\n",
    "    * Search: O(log(n))\n",
    "    * Select (i-th stat): O(1)\n",
    "    * Min/max (select special case): O(1)\n",
    "    * Pred/succ: O(1)\n",
    "    * Rank: O(log(n))\n",
    "    * Output in sorted order: O(n)\n",
    "* Balanced binary search tree: if you want a very rich set of operations for processing your data\n",
    "    * Search: O(log(n))\n",
    "    * ! Select (i-th stat): O(log(n))\n",
    "    * ! Min/max (select special case): O(log(n))\n",
    "    * ! Pred/succ: O(log(n))\n",
    "    * Rank: O(log(n))\n",
    "    * Output in sorted order: O(n)\n",
    "    * + Insert: O(log(n))\n",
    "    * + Delete: O(log(n))\n",
    "* Heap: the benefits of a heap don't show up in the big O notation here both have logarithmic operation time but the constant factors both in space and time are going to be faster with a heap then with a Balanced Binary Search Tree\n",
    "    * Min/max : O(log(n))\n",
    "    * Insert: O(log(n))\n",
    "    * Delete: O(log(n)) \n",
    "* Hash table: if you don't actually need to remember things like minima, maxima or remember ordering information on the keys, you just have to remember what's there and what's not\n",
    "    * Search: O(log(n))\n",
    "    * Insert: O(log(n))\n",
    "    * + sometimes Delete: O(log(n)) \n",
    "    \n",
    "### 13.2 BBST basics\n",
    "* one node per key\n",
    "* each node have 3 pointers: 1 for left child, 1 for right, 1 for parent\n",
    "    * except leafs, parent node and some parent nodes of leafs\n",
    "* Search tree property: for every node in the tree\n",
    "    * If the node has some key value then \n",
    "    * all of the keys stored in the left subtree should be less than that key. \n",
    "    * all of the keys stored in the right subtree should be bigger than that key.\n",
    "    * If there are redundant values, one of those inequities become =< or >=\n",
    "* the search tree property tells you exactly where to look for some given key (very much in the spirit of binary search)\n",
    "* Note: For a given set of keys there can be many dfferent BSTs\n",
    "* Note: height (= depth = longest root-leaf path) can vary from log_2(n) in best case scenario to n\n",
    "\n",
    "**Operations**  \n",
    "* (1) Search for key k in tree T\n",
    "    * start at the root\n",
    "    * traverse left (if k < key at current node) or right (if k > key at current node) child pointers as needed\n",
    "    * return node with key k or NULL (if no such key), as apropriate\n",
    "* (2) Insert (modif of (1)) a new key k into a tree T\n",
    "    * If we don't want duplicates:\n",
    "        * search for k (unsuccessfully)\n",
    "        * rewire final NULL pointer to new node with key k\n",
    "    * In duplicates:\n",
    "        * You just need some convention for handling the case when you do in counter the key that we are about to insert. \n",
    "        * So, for example, if the current note has the key equal to the one you're inserting, you could have the convention that you always continue on the left subtree and then you continue the search as usual again, eventually terminating at a null pointer and you stick the new inserted node you rewire to null pointer to point to it.\n",
    "        * Should preserve the BST property! Not understood how to deal with insertions in the middle\n",
    "* (3) To compute Minimum of a tree\n",
    "    * [NB] One way to think of it is that you do a search for negative infinity in the search tree\n",
    "    * Start at root\n",
    "    * Keep following the left child pointers until you can't anymore\n",
    "    * Retun last key found\n",
    "* (4) To compute Maximum of a tree\n",
    "    * [NB] One way to think of it is that you do a search for positive infinity in the search tree\n",
    "    * Start at root\n",
    "    * Keep following the right child pointers until you can't anymore\n",
    "    * Retun last key found \n",
    "* (5) Compte the predessesor of key k\n",
    "    * The easy case:\n",
    "        * The node of the key k has a nonempty left subtree\n",
    "        * In this case we just want the biggest element of this subtree (whick is gettable by following right pointer of the subtree\n",
    "    * Otherwise:\n",
    "        * We look at parent pointer until we get to a node which is smaller than k, and that is guaranteed to be a predesessor\n",
    "            * you'll get to a key smaller than yours, the very first time you take a left turn.\n",
    "            * So the very first time that you go from a right child to it's parent. \n",
    "    * if you start from the unique node that has no predecessor at all, you'll never going to trigger this terminating condition \n",
    "    * if you wanted to be the successor of the key instead of the predecessor, obviously you just flip left and right through out this entire description. \n",
    "* (6) In-order traversal: print out keys in increasing order \n",
    "    * let r = root of search tree with subtrees T_l, T_r\n",
    "    * recurse on T_l [by recursion/induction prints out keys of T_l in increasing order]\n",
    "    * print out r's key\n",
    "    * recurse on T_r [by recursion/induction prints out keys of T_r in increasing order]\n",
    "    * running time O(n): \n",
    "        * the reason is = one recursive call for one node, it prints one node only once O(1)     \n",
    "* (7) Deletion of a k form a search tree (3 cases)\n",
    "    * search for k\n",
    "    * easy case: k has no children\n",
    "        * just delete k's node form the tree\n",
    "    * medium case: k has one child\n",
    "        * just splice out k's node: unique hild assumes position previusly held by k's node)\n",
    "    * difficult case: k has 2 children\n",
    "        * compute k's predessesor l \n",
    "            * case 1 for predecessor computation (if NULL left subtree):is not relevant, because we land in a medium case of deletion (no left subtree => at most one child node of k)\n",
    "            * case 2 for predecessor computation (non-NULL): traverse k's  left child pointer, then right child pointers untill no long possible\n",
    "        * you swap k and l\n",
    "            * for case 2: it means that k lands in a position where it has at most 1 child (right subtree is null) => we are in the easy or medium cases (this deletion operation retains the search tree property)\n",
    "            * running time: O(height)\n",
    "* (8) Select ( I'll give you an order statistic like seventeen and I want you to return the seventeenth smallest key in the tree\n",
    "    * Idea: store a little bit of extra info at each tree node (about tree itself) (not about the data) = data structure augmentation\n",
    "        * most canonical augmentation of the search tree like these is to keep track in each node, not just to the key value but also over the population of tree nodes in the sub tree that is rooted there\n",
    "        * size(x) = number of tree nodes in subtree rooted in x (including root)\n",
    "        * note: if x has children y in z, then size(x) = size(y) + size(z) + 1\n",
    "        * The maintainance of extra data in the data structure requires additional computational resources\n",
    "        * In case of Insertion and deletion it's easy to keep sizes up to date\n",
    "    * Selection procedure (i-th order stat)\n",
    "        * start at root x, with children y (left) and z (right)\n",
    "        * [all y nodes are smaller than x, and we know the number of them, = m, so x is m+1-th order statistics]\n",
    "        * let a = size(y) [a = 0 if x has no left child]\n",
    "        * if a = i-1 return x's key\n",
    "        * if a >= i recursively compute i-th order statistics of search tree rated at y\n",
    "        * if a < i-1 recursively compute (i-a-1)-th order statistics of search tree rated at z \n",
    "        * the root is acting as a pivot element\n",
    "        * running time O(height)\n",
    "* (9) Rank (Rank is I give you a key value and I want to know how many keys in the tree are less than or equal to that value)\n",
    "    * Analogous to Select\n",
    "  \n",
    "  \n",
    "### 13.3 Red-Black Trees\n",
    "* we'll start talking about balanced binary search trees (as most effective ones)\n",
    "* Idea: ensure that height always would be log_2(n) => search/delete/Insert/min/max/ pred/succ will then run in O(log(n)) [n = number of keys]\n",
    "* there are a lot of BST, examples\n",
    "    * red-black trees \n",
    "    * AVL trees; \n",
    "    * splay trees = modify themselves even when you are doing lookups\n",
    "    * B trees, B+ trees = relevant for implementing databases (with many keys and multiple branches, not binary)\n",
    "        * => better matchup with memory hierarchy\n",
    "\n",
    "**Red-Black Invariants**\n",
    "* Same as BST, except that they always maintain a number of additional invariants:\n",
    "* (1) Each node red or black\n",
    "* (2) Root is always black\n",
    "* (3) no 2 reds in a row [red node => black children only; several blacks are possible]\n",
    "* (4) every path you might take from a root pointer to a NULL node (like in unseccessful search) has same number of black nodes \n",
    "\n",
    "**Example 1**\n",
    "* Claim: a chai of length 3 nodes cannot be a red-black tree\n",
    "* Proof: non-example chain root: 1(b)-2(r)-3(b), violation of (4)\n",
    "    * first null is when you search for 0 => 1 black node (1)\n",
    "    * seconde null is when you search for 4 => 2 black nodes (1, 3)\n",
    "\n",
    "**Example 2**\n",
    "* [3, b]-[5 root, b]-[7, b]\n",
    "* satisfied all 4 invariants\n",
    "* You should support those invariants when inserting/deleting nodes\n",
    "    * inserting 6 (on the left of 7) and cloring it red would leave invariants satisfied\n",
    "    * inserting red 8 would also preserve the invariants\n",
    "    * this is not the unique way\n",
    "        * We can recolor 6 and 8 black, and 7 – red\n",
    "\n",
    "**Claim**  \n",
    "* If you satisfy those 4 invariants in your search tree, the height is guaranteed to be minimal  \n",
    "* ie: every red-black tree with n nodes has height =< 2log_2(n+1)\n",
    "* Proof:\n",
    "    * Observation: if every root-NULL path >= k nodes, then tree includes (at the top) a perfectly balanced tree of depth k-1\n",
    "        * Proof by contradiction:  If you were missing some nodes in any of these top k levels. We'll that would give you a way of hitting a null pointer seeing less then k nodes)\n",
    "        * This gives us a lowerbound of a population of a search tree as a function of lengths of its root-NULL paths: \n",
    "        * ie the size n of the tree must include at least number of nodes in a perfectly balanced tree of depth k-1 (which = 2^(k)-1)\n",
    "        * Recap: size n >= 2^(k) - 1 (k = minimum number of nodes on root-NULL path)\n",
    "        * => k =< log_2(n+1)\n",
    "    * Red-black tree with n node: in the best case all of them are black, so there is a root-NULL path with at most log_2(n+1) black nodes\n",
    "    * By 4-th invariant: every root-NULL path has =<  log_2(n+1) nodes\n",
    "    * By 3-d invariant: \n",
    "        * black are the majority of the nods (since there are no 2 reds in a row), so if we know that number of black nodes is small, the total number is at most twice as large\n",
    "        * At the worst case, the number of red nodes is equal to the number of black nodes, which doubles the length of the path once you start counting the red nodes as well. And this is exactly what it means for a tree to have a logarithmic depth. So, this, in fact, proves the claim\n",
    "        * ie every root-NULL path has =< 2log_2(n+1) total nodes \n",
    "    *  if the search trees satisfies the invariants 1-4, then, knowing nothing else about this search tree, it's got to be almost balanced. It's perfectly balanced up to a factor of two.\n",
    "    * => operations run in logarithmic time    \n",
    "\n",
    "\n",
    "### 13.4 Rotations\n",
    "* Implementations of balanced binary search trees\n",
    "* Focus on key primitive **rotation** which is common to All BBST implementations: red-black trees, EVL trees, B or B+ trees.\n",
    "* Idea: locally rebalance subtrees at a node in O(1) time \n",
    "    * You invoke rotation in a parant-child pair of a serach tree\n",
    "        * If right child of the parent – you use left rotation. And vise versa\n",
    "* Example\n",
    "    * x (can have a parent p)\n",
    "    * his chldren:\n",
    "        * subtree A\n",
    "        * node y\n",
    "            * his children: subtrees B, and C\n",
    "* Consequences of this structure\n",
    "    * y > x\n",
    "    * all in C > y\n",
    "    * all in A < x\n",
    "    * all in B < y\n",
    "     * => x < B < y  \n",
    " * Fundamentally the goal is to invert the relationship иetween the nodes x and y. \n",
    "     * Currently x is the parent and y is the child.\n",
    "     * We want to rewire a few pointers so that y is now the parent and x is the child.\n",
    "     * There is prety much a unique way to do that\n",
    "     * y > x => after inversion x should be the left child of y\n",
    "     * what happens to the parent of x?\n",
    "         * y inherits x-s old parent p\n",
    "     * A < x < y => A is a left child of x\n",
    "     * C > y > x => C is a right child of y\n",
    "     * x < B < y  => B is a right child of x\n",
    "* The right rotation is the inverse opertion: child is the left child of the parent \n",
    "* Properties of rotation\n",
    "    * Implemented in constant time: you rewire a constant number of pointers\n",
    "    * Preservation of search tree property = primitive\n",
    "\n",
    "\n",
    "### 13.5 Insertion Implementation for Re-Black tree\n",
    "* amongst all of the supported operations there are only two that modify the data structure: insertion and deletions\n",
    "* High level plan:\n",
    "    * we try to implement those as for normal BST, as if there were no invariants\n",
    "    * if an invariant is broken we try to fix it by 2 tools\n",
    "        * recoloring\n",
    "        * rotation\n",
    "* Deletion is not trevial, so we discuss only insert\n",
    "* Insert(x)\n",
    "    * first just forget about the invariance, and we insert it as usual\n",
    "        * we follow left and right shot pointers, \n",
    "        * until we get to a null pointer, \n",
    "        * and we install this new node with key x, where we fell off the tree. \n",
    "        * That makes x a leaf in this binary search tree. \n",
    "    * Let's let y denote x's parent, after it gets inserted. \n",
    "    * Now in a red-black tree every node has a color. We should decide\n",
    "        * whichever color we make it we have the potential of destroying one of the invariants. \n",
    "        * suppose we color it red. \n",
    "            * third invariant says, you cannot have two reds in a row. \n",
    "            * So if Y, X's new parent is already red, then when we color X red, we have 2 reds in a row. \n",
    "            * And we've broken invariant number 3. \n",
    "        * On the other hand, if we color this new node, X, black, we've introduced a new black node to certain root null paths in this tree. \n",
    "            * And remember, the 4th invariant insists, that all the root null paths have exactly the same number of black. \n",
    "        * So we're going to choose the lesser of two evils: color x red. \n",
    "            * intuitively, it's because this invariant violation is local. \n",
    "            * if we coated x black then we violated this much more global type of property involving all of the route in all paths and that's a much more intimidating violation to try to fix.\n",
    "        * if we are lucky, the parent is black and invariant 3 is not violated and we can stop\n",
    "        * if parent is red:\n",
    "            * before we inserted x, we had a red black tree, all 4 of the invariants were satisfied. \n",
    "            * So Y it could not have been the root. \n",
    "            * It has to have a parent. \n",
    "            * Let's call that parent W. \n",
    "            * Moreover by the third invariant there was no double red in this tree before we inserted X so by virtue of Y being red, it's parent W must have been black. \n",
    "            * So, now the insertion operation branches into 2 different cases and it depends on the color, on the status of w's other child. \n",
    "                * Case 1: we assume that w's other child (Z) exists in its colored red. \n",
    "                * Case 2: w either doesn't have a second child (Y is its only child) or when its other child (Z) is colored black. \n",
    "                * For **Case 1** we'll do recoloring: Z, Y to black; w to red. This recoloring does not break the fourth invariant\n",
    "                * What is the color of w's parent? If black, we are done (invariant 3 restored) if not, we propagate double red upword. The number of times this propagation can happen is =< 2log_2(n) => time O(logn)\n",
    "                * If we are dealing with the root of the tree, we may just restore the 2 invariant by coloring it black (this recoloring preservs the 4th invariant also)\n",
    "                * For **Case 2** can be encountered in the beginning, when x is a leaf, of once case 1 get propagating into the tree by recoloring. Either way it triggers case 2\n",
    "                * case 2 is great in the sense that, with nearly constant work, you can restore in variant number 3 and get rid of the double red without breaking any of the other invariants. You do have to put to use both recolorings and rotations \n",
    "                * There are unfortunately a couple of sub cases of case 2 depending on exactly the relationships between x, y, z, and w, but two to three rotations plus some recolorings is always sufficient in case two to restore all of the In variance (ie O(1) time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 7\n",
    "\n",
    "**1. Suppose you implement the functionality of a priority queue using a sorted array (e.g., from biggest to smallest). What is the worst-case running time of Insert and Extract-Min, respectively? (Assume that you have a large enough array to accommodate the Insertions that you face.)**\n",
    "  \n",
    "Answer: Theta(n) and Theta(1)\n",
    "\n",
    "**2. Suppose you implement the functionality of a priority queue using an unsorted array. What is the worst-case running time of Insert and Extract-Min, respectively? (Assume that you have a large enough array to accommodate the Insertions that you face.)**\n",
    "  \n",
    "Answer: Theta(1) and Theta(n)\n",
    "\n",
    "**3. You are given a heap with n elements that supports Insert and Extract-Min. Which of the following tasks can you achieve in O(log(n)) time?**\n",
    "   \n",
    "Answer: Find the fifth-smallest element stored in the heap.\n",
    "\n",
    "**4. You are given a binary tree (via a pointer to its root) with n nodes. As in lecture, let size(x) denote the number of nodes in the subtree rooted at the node x. How much time is necessary and sufficient to compute size(x) for every node x of the tree?**\n",
    "\n",
    "Answer: Theta(n) (we compute n distinct sizes)\n",
    "\n",
    "**5. Suppose we relax the third invariant of red-black trees to the property that there are no three reds in a row. That is, if a node and its parent are both red, then both of its children must be black. Call these relaxed red-black trees. Which of the following statements is not true?**\n",
    "  \n",
    "Answer: Every binary search tree can be turned into a relaxed red-black tree (via some coloring of the nodes as black or red)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming assignment 7\n",
    "\n",
    "**The goal of this problem is to implement the \"Median Maintenance\" algorithm (covered in the Week 3 lecture on heap applications). The text file contains a list of the integers from 1 to 10000 in unsorted order; you should treat this as a stream of numbers, arriving one by one. Letting:** \n",
    "* **x_i denote the i-th number of the file,** \n",
    "* **the k-th median m_k is defined as the median of the numbers x_1, ..., x_k**\n",
    "    * **So, if k is odd, then m_k is ((k+1)/2)-th smallest number among x_1, ..., x_k** \n",
    "    * **if k is even, then m_k is the (k/2)-th smallest number among x_1, ..., x_k** \n",
    "  \n",
    "**In the box below you should type the sum of these 10000 medians, modulo 10000 (i.e., only the last 4 digits). That is, you should compute (m_1+m_2+m_3 + ... + m_10000) mod 10000**\n",
    "\n",
    "**OPTIONAL EXERCISE: Compare the performance achieved by heap-based and search-tree-based implementations of the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArray():\n",
    "    '''\n",
    "    load txt (one number per line) into an array\n",
    "    each line is a number \n",
    "    '''\n",
    "    file = open(\"../algorithms_course_code/data/pg_asmt_7_heaps.txt\", 'r')\n",
    "    array = []\n",
    "    for line in file:\n",
    "        line_strip = line.rstrip(\"\\n\")\n",
    "        array.append(int(line_strip))\n",
    "    file.close()    \n",
    "    return array\n",
    "\n",
    "def loadTestArray():\n",
    "    '''\n",
    "    load txt (one number per line) into an array\n",
    "    each line is a number \n",
    "    '''\n",
    "    file = open(\"../algorithms_course_code/data/pg_asmt_7_heaps_test_cases.txt\", 'r')\n",
    "    array = []\n",
    "    for line in file:\n",
    "        line_strip = line.rstrip('\\n')\n",
    "        if line_strip != '':\n",
    "            array.append(int(line_strip))\n",
    "    file.close()    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input array length 10000\n"
     ]
    }
   ],
   "source": [
    "input_array = loadArray()\n",
    "test_array = loadTestArray()\n",
    "print(\"input array length\", len(input_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are there redundant values in the array?\n",
    "import numpy as np\n",
    "len(np.unique(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert and Remove functions on Heap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertToMinHeap(array, element):  \n",
    "    array.append(element)    \n",
    "    i = len(array)  \n",
    "    \n",
    "    while (i != 1): \n",
    "        if i%2 != 0:\n",
    "            if array[i-1] < array[(i//2)-1]:\n",
    "                array[i-1], array[(i//2)-1] = array[(i//2)-1], array[i-1]\n",
    "                i = (i//2) \n",
    "            else:\n",
    "                return array\n",
    "        \n",
    "        else:    \n",
    "            if array[i-1] < array[(i-1)//2]:\n",
    "                array[i-1], array[(i-1)//2] = array[(i-1)//2], array[i-1]\n",
    "                i = (i-1)//2 + 1\n",
    "            else:\n",
    "                return array\n",
    "\n",
    "\n",
    "def insertToMaxHeap(array, element): \n",
    "    array.append(element)  \n",
    "    i = len(array) \n",
    "    \n",
    "    while (i != 1): \n",
    "        if i%2 != 0: \n",
    "            if array[i-1] > array[(i//2)-1]: \n",
    "                array[i-1], array[(i//2)-1] = array[(i//2)-1], array[i-1]\n",
    "                i = (i//2) \n",
    "            else:\n",
    "                return array\n",
    "        \n",
    "        else:   \n",
    "            if array[i-1] > array[(i-1)//2]: \n",
    "                array[i-1], array[(i-1)//2] = array[(i-1)//2], array[i-1] \n",
    "                i = (i-1)//2 + 1 # = 2, 1   \n",
    "            else:\n",
    "                return array\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMinFromHeap(array):\n",
    "    import math\n",
    "    return_val = array[0] \n",
    "    length = len(array) \n",
    "    layers_no = math.trunc(math.log2(length)) + 1\n",
    "    number_of_nodes_having_children = (2**(layers_no-1))-1\n",
    "    array[0] = array.pop(length-1) \n",
    "    \n",
    "    i = 1\n",
    "    while i < number_of_nodes_having_children + 1:\n",
    "        try:\n",
    "            array[2*i - 1]\n",
    "            try: \n",
    "                array[2*i]\n",
    "                if array[i-1] > min(array[2*i - 1], array[2*i]):\n",
    "                    if min(array[2*i - 1], array[2*i]) == array[2*i - 1]:\n",
    "                        next_index = 2*i - 1\n",
    "                    else:\n",
    "                        next_index = 2*i\n",
    "                else:\n",
    "                    return return_val\n",
    "                \n",
    "            except IndexError:         \n",
    "                \n",
    "                if array[i-1] > array[2*i - 1]:\n",
    "                    next_index = 2*i - 1 \n",
    "                else:\n",
    "                    return return_val                               \n",
    "            \n",
    "            array[i-1], array[next_index] = array[next_index], array[i-1]\n",
    "            i = next_index + 1\n",
    "        \n",
    "        except IndexError: \n",
    "            return return_val\n",
    "    \n",
    "    return return_val\n",
    "\n",
    "def removeMaxFromHeap(array):  \n",
    "    import math\n",
    "    return_val = array[0] \n",
    "    length = len(array) \n",
    "    array[0] = array.pop(length-1) \n",
    "    length -= 1 \n",
    "    layers_no = math.trunc(math.log2(length)) + 1 \n",
    "    number_of_nodes_having_children = (2**(layers_no-1))-1 \n",
    "    \n",
    "    \n",
    "    i = 1\n",
    "    while i < number_of_nodes_having_children + 1: \n",
    "        try:\n",
    "            array[2*i - 1] \n",
    "\n",
    "            try: \n",
    "                array[2*i] \n",
    "\n",
    "                if array[i-1] < max(array[2*i - 1], array[2*i]): \n",
    "                    if max(array[2*i - 1], array[2*i]) == array[2*i - 1]: \n",
    "                        next_index = 2*i - 1 \n",
    "                    else:\n",
    "                        next_index = 2*i    \n",
    "                else:\n",
    "                    return return_val \n",
    "            \n",
    "            except IndexError:\n",
    "                if array[i-1] < array[2*i - 1]:\n",
    "                    next_index = 2*i - 1 \n",
    "                else:\n",
    "                    return return_val \n",
    "                    \n",
    "            array[i-1], array[next_index] = array[next_index], array[i-1] \n",
    "            i = next_index + 1 \n",
    "        \n",
    "        except IndexError:\n",
    "            return return_val\n",
    "    \n",
    "    return return_val     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintainMedian(array):\n",
    "  \n",
    "    heap_low = [array[0]]\n",
    "    heap_high = []\n",
    "    \n",
    "    medians = [array[0]]\n",
    "    med_sum = array[0]\n",
    "    \n",
    "    for element in array[1:]:\n",
    "        if element <= heap_low[0]:\n",
    "            insertToMaxHeap(heap_low, element)\n",
    "        else:\n",
    "            insertToMinHeap(heap_high, element)\n",
    "            \n",
    "        if len(heap_low) > len(heap_high) + 1:\n",
    "            moving_element = removeMaxFromHeap(heap_low)\n",
    "            insertToMinHeap(heap_high, moving_element)\n",
    "        \n",
    "        elif len(heap_high) > len(heap_low):    \n",
    "            moving_element = removeMinFromHeap(heap_high)\n",
    "            insertToMaxHeap(heap_low, moving_element)\n",
    "            \n",
    "        median = heap_low[0]\n",
    "        medians.append(median)\n",
    "        med_sum +=  median\n",
    "    \n",
    "    \n",
    "    # print(\"medians: \", medians, \"\\n\", \"med_sum: \", med_sum, \"modulo: \", med_sum%10000) \n",
    "    return med_sum%10000\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run function on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6331, 2793, 1640, 9290, 225, 625, 6195, 2303, 5685, 1354]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medians:  [6331, 2793, 2793, 2793, 2793, 1640, 2793, 2303, 2793, 2303] \n",
      " med_sum:  29335 modulo:  9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data\n",
    "maintainMedian(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array\n",
    "maintainMedian(input_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Hashing: the basics\n",
    "\n",
    "### 14.1. Hash tables: operations and applications\n",
    "* Technically HT = array\n",
    "* Arrays support immediate random access to values by an index\n",
    "* mapping between the keys which make sence (e.g. names of friends) and numerical position of some array (containing phones for example) is done by hash function\n",
    "  \n",
    "**Purpose** maintain a possibly evolving set of stuff (transactions, people+associated daya. ip addresses, etc)\n",
    "**Operations** all these by using a \"key\"; run in O(1)\n",
    "* Insert: add new record\n",
    "* Delete: existing record\n",
    "* Lookup: check for a particular record\n",
    "\n",
    "But\n",
    "* (1) run in O(1) time *for proper implementation*!\n",
    "    * ie right number of buckets\n",
    "    * good hash functions\n",
    "* (2) hash tables don't enjoy worst case guarantees. \n",
    "    * You cannot say for a given hash table that for every possible data set you're gonna get O(1)\n",
    "    * Only for non-pathalogical data!\n",
    "  \n",
    "Sometimes hear people refer to data structures that support these operations as a dictionary\n",
    "* This is misleading\n",
    "* Most of dicts are in alphabetical order, so supported by binary search trees\n",
    "* What hash tables do not do is maintaining ordering of the elements it supports (no min/max operations here!)\n",
    "\n",
    "**Applications**\n",
    "* (1) De-dupliation \n",
    "    * Input is a \"stream\" of objects \n",
    "        * You do a pass of a huge file (for loop)\n",
    "        * New data arriving in real time\n",
    "    * Goal: remove duplicates (ie keep track of unique objects)\n",
    "        * report unique visitors of web site\n",
    "        * avoid duplicates in search results\n",
    "    * Solution:\n",
    "        * when new obect x arrives\n",
    "        * lookup x in a hash table H (in cst time)\n",
    "        * if not found Insert x into H\n",
    "* (2) 2-SUM Problem\n",
    "    * Input: \n",
    "        * Unsorted array A of n integers. \n",
    "        * Target sum t\n",
    "    * Goal: determine whether or not there are 2 numbers (x, y) in A, such that x + y = t\n",
    "    * Solutions:\n",
    "        * (1) Naive: double linear scan O(n^2)\n",
    "        * (2) Sort A upfront (nlog(n))\n",
    "            * (a) (nlog(n)) by mergesort for example\n",
    "            * (b) for each x in A, look for t-x in A via binary search (log(n)) => for all searches O(nlog(n)) also\n",
    "        * (3) The best solution: hash table (O(n))\n",
    "            * What's the clue for using hash table here? All the work on step 2 of sorted solution is from speed lookups, paying log(n) time. Hash can do in O(1)\n",
    "            * (a) insert elements of A into a hash table H (O(n))\n",
    "            * (b) for each x in A Lookup t-x in H (O(n))\n",
    "\n",
    "* (3) Historical applicaions\n",
    "    * symbol tables in compilers\n",
    "    * blocking network traffic by ip addresses in black list\n",
    "    * speeding up search algorithms (eg game tree exploration, which is too huge to be stored in a graph)\n",
    "        * use hash table to avoid exploring an configuration more than once (eg arrangement of chess pieces)\n",
    "    * etc\n",
    "    \n",
    " \n",
    "### 14.2. Hash tables: Implementation\n",
    "\n",
    "**High level idea**\n",
    "* Setup: identify the universe (all ip addresses = 2^32, all chess board configurations, all names = 26^30 – really huge)\n",
    "* Goal: want to maintain a evolving subset S of this universe U (S of reasonable size)\n",
    "* Naive solutions:\n",
    "    * (1) array-based solution indexed by U \n",
    "        * O(1) operations\n",
    "        * but O(|U|) space\n",
    "    * (2) list based solution\n",
    "        * O(|S|) space\n",
    "        * but O(|S|) time lookup\n",
    "* Optimal solution:\n",
    "    * (1) pick n (length of the array) = # of buckets with n ~~ |S| (around double of S)\n",
    "        * S is dynamic but we assume that the size of S doesn't fluctuate too much\n",
    "        * Otherwise \n",
    "            *  you can just keep track of how many elements are in your hash table. \n",
    "            * And when it exceeds a certain threshold, so when it's too big relative to the size of your array, you just double the array. \n",
    "            * And then you reinsert all of the elements into this new doubled array. \n",
    "            * Similarly, if you want to, if the set shrinks, you can have tricks for shrinking the array dynamically as well.\n",
    "    * (2) choose a hash function h: U -> [0, ..., n-1] \n",
    "        * function takes as input some key (ip, name, ...) \n",
    "        * and spit up a position in this array\n",
    "        * function examples later\n",
    "    * (3) use array A of length n to store x in A[h(x)]\n",
    "   \n",
    " * Serious issue: collision: If several different keys point to the same bucket number? \n",
    "     * Birthday paradox: it suffises to have 23 people in the room to get 50% chance there is at least one pair born on the same day\n",
    "         * number of comps = (n-1)n/2\n",
    "         * probability that 2 people bord on dfferent days = 364/365\n",
    "         * probability that all people born on different days = (364/365)^(n-1)n/2\n",
    "         * and that should be < 1/2\n",
    "         * => n = 23\n",
    "     * We see that even for a tiny dataset you start getting collisions with the hash function\n",
    "\n",
    "**Resolving collisions**\n",
    "* Collision: distinct x, y belongng to U such that h(x) = h(y)\n",
    "* Solution 1 (\"Chaining\"): \n",
    "    * keep linked list in each bucket\n",
    "    * given a key (= object x), perform Insert/Delete/Lookup in the list in A[h(x)] (= bucket of x)\n",
    "* Solution 2 (\"Open addressing\")\n",
    "    * only one object per bucket in the array\n",
    "    * hash function is gonna be replaced by hash sequence (= probe sequence)\n",
    "        * if the bucket is occupied\n",
    "        * it gives you next bucket number to try\n",
    "        * keep trying till find open slot\n",
    "    * examples \n",
    "        * linear probing = you try buckets consecutevily by increasing numbers\n",
    "        * double hashing: 2 hash functions\n",
    "            * first hash specifies the position of the array\n",
    "            * second one specifies the shift to be added to the first index = offset\n",
    "            * if you try to insert another name, second function will give you another offset\n",
    "* How to choose between solutions?\n",
    "    * If you consider space economy: 2 solution\n",
    "    * Deletion is trickier with open addressing (2 solution)\n",
    "    \n",
    "**What makes hash function good?**\n",
    "* Note: \n",
    "    * in hash table with **chaining**, Insertion is O(1)\n",
    "        * You should insert new object x in front of the list in this bucket A[h(x)]\n",
    "        * Delete/Insert = O(len list) (exaustive search for x)\n",
    "    * If you have 100 objects and 100 buckets we can put those objects differently depending on hash function\n",
    "        * => list length could be from m/n (equal list length) to m (all objects in one bucket) for m objects and n buckets =>\n",
    "        * Point: performance depends on the choice of hash function!\n",
    "    * Analogous for **open addressing**: running time will depend on the length of probe sequence, which depends on hash function also\n",
    "        * For really good hash functions in some sense, stuff that spreads data out evenly, you expect probe sequences to be not too bad. \n",
    "        * And for say the constant function you are going to expect these probe sequences to grow linearly with the numbers of object you insert into the table. \n",
    "* Propertes of a \"Good\" hash funcion (for chaining)\n",
    "    * (1) should leed to good performance => ie should spread the data out \n",
    "        * gold standard = complitely random hashing\n",
    "        * it also should not work too hard: every time we insert/del/lookup we apply hash function, so to run those opertations in O(1), hash function should also run in O(1)\n",
    "    * (2) should be eashy to store (very fast to evaluate) (= we should remember, where the key is stored and easily get it. So we should somwhere store this information)\n",
    "    \n",
    "    \n",
    "**Bad hash functions** \n",
    "* It's really easy to design bad hash functions!\n",
    "* The design is much more of art\n",
    "* Example 1: \n",
    "    * keys = phone numbers (10-digits)\n",
    "    * the universe size 10^10\n",
    "    * I keep track of 500 numbers\n",
    "    * => Choose number of boxes n = 1000\n",
    "    * hash function takes a phone number and spits out some number from 0 to 99\n",
    "    * (1) terrible: if we just project digits on box numbers: use some significant digits from a phone number: h(x) = first 3 digits of x\n",
    "        * this is terrible choice: numbers with same codes will all fall into one bucket\n",
    "        * besides that not all three digit numbers are area codes in the United States. So there'll be buckets of your hash table which are totally guaranteed to be empty. So you waste a ton of space in your hash table,\n",
    "    * (2) mediocre: h(x) = last 3 digits of x\n",
    "        * no evidence that these 3 last digits are equally distributed among 1000 possibilities \n",
    "* Example 2:\n",
    "    * you are keeping track of objects just based by where they are laid out in memory.\n",
    "    * the key for an object = its memory location \n",
    "    * and if these things are in bytes => every memory location will be a multiple of four. \n",
    "    * Universe where the possible keys are the possible memory locations, \n",
    "    * So here you're just associating objects with where they're laid in memory, \n",
    "    * and a hash function is responsible for taking in as input some memory location of some object and spitting out some bucket number\n",
    "    * memory locations = 2^k (even)\n",
    "    * let hash table with 1000 buckets\n",
    "    * hash function going to take this big number, which is the memory location, and squeeze it down to a small number\n",
    "    * bad choice: h(x) = x mod 1000 (3 last digits)\n",
    "        * all odd buckets guaranteed to be empty\n",
    "\n",
    "**Quick and Dirty hash functions**\n",
    "* [object from U of any kind, incl non-numeric] \n",
    "* =/step 1: hash code/=> \n",
    "* [integers] \n",
    "* =/step 2: compression function/=> \n",
    "* [buckets]\n",
    "\n",
    "* step 1 \n",
    "    * sometimes you can skip the first step if objects are numeric\n",
    "    * otherwise you just convert strings to integers, for example in a following way\n",
    "        * take ASCII code of each character\n",
    "        * aggregate all of the different numbers, one number per character into some overall number \n",
    "        * iterate over the characters one at a time. \n",
    "        * You can keep a running sum. \n",
    "        * And with each character, you can multiply the running sum by some constant, \n",
    "        * and then add the new letter to it, \n",
    "        * and then, if you need to, take a modulus to prevent overflow\n",
    "* step 2\n",
    "    * how to design this function?\n",
    "    * mod function (x%y =used to get the remainer from the x/y operation) \n",
    "        * easy to call\n",
    "        * simple to store\n",
    "        * but it doesn't spread the data equally\n",
    "        * but there is a number of rules on how to pick **n** for mod to work in a descent way\n",
    "            * (1) if the data is the multiple of a number, and the number of buckets also – big problem, some buckets will remain empty. So choose n to be a prime number (within constant factor of # of objects in table) \n",
    "            * (2) you want the prime to be not too close to patterns in your data. \n",
    "                * not too close to power of 2\n",
    "                * not too close to power of 10\n",
    "        \n",
    "# 15. Universal hashing    \n",
    "\n",
    "### 15.1. Pathological Data Sets and Universal Hashing Motivation\n",
    "* Every hash function has its own Kyptonite. A pathological data set for it which then motivates the need to tread carefully with mathematics\n",
    "\n",
    "**The Load of a Hash table**\n",
    "* Definition: the load factor of a hash table is alpha = (# of objects in hash table)/(# of buckets in the hash table)\n",
    "* Note:\n",
    "    * (1) alpha = O(1) is necessary to condition for operations to run in constant time \n",
    "    * (2) with open addressing need alpha << 1\n",
    "* Upshot: for good HT performance need to control load\n",
    "* If alpha exceeeds some number (0.75 for example), we can double the number of buckets\n",
    "    * new hash table\n",
    "    * new hash function with double range\n",
    "    * double denominator of alpha\n",
    "* The same logic for bucket shrinks\n",
    "\n",
    "**Pathological Data Sets**\n",
    "* for good HT performance need good hash function\n",
    "    * spreading the data evenly across buckets\n",
    "    * \n",
    "* Ideally: a hash function that works well independent of the data \n",
    "* It doesn't exist! \n",
    "    * For every hash function there is a pathological data set\n",
    "* Reason: fix any hush function h: U -> {0, ... , n-1} (where |U| >> n)\n",
    "    * => via pigeonhole principle: there exist a bucket i  such tat at least |U|/n elements of U hash to i under h\n",
    "    * => if data set is taken from only these values, everything collides\n",
    "\n",
    "**Pathological data int the real world**\n",
    "* So if you use hash tables, you depend on data (unlikely in sorting, BST, heaps, etc)\n",
    "* Reference: Crosby and Wallach, USENIX 2003\n",
    "    * Main Point: can paralyze several real-world systems (eg network intrusion detection) by exploiting badly designed hash functions (devoted the performance to linear)\n",
    "    * Those systems are:\n",
    "        * open source\n",
    "        * use overly simplistic hash function\n",
    "        * => easy to reverse engineer a pathological data set\n",
    "\n",
    "**Solutions**\n",
    "* (1) use a cryptographic hash functions (eg SHA-2)\n",
    "    * they do have pathological data sets\n",
    "    * but its impossible to figure out what are those (infeasible to reverse engineer)\n",
    "* (2) use randomization\n",
    "    * design a family of hash functions H such that for any data sets S, \"almost all\" functions h belonging to H spread S out \"pretty evenly\" (compare to Quick Sort guarantee)\n",
    "    * <=> for each fixed data set a random choice of a hash function is going to do well on average on that data set \n",
    "    \n",
    "### 15.2. Universal Hashing: Definition and Example\n",
    "\n",
    "**Definition**\n",
    "* The universe is fixed: U\n",
    "* We decided on the number of buckets n\n",
    "* Let H be a set of hash functions from U to {0, ... , n-1}. \n",
    "* H is universal <=> for all x,y belonging to U (with x ≠ y), Pr[x, y collide, ie h(x) = h(y), for any h in H] =< 1/n, when h is chosen uniformily at random from H\n",
    "* (ie collision probability as small as with \"gold standard\" of perfectly random hashing)\n",
    "\n",
    "**Question**\n",
    "* Consider a hash function family H, where each hash function of H maps elements from a universe U to one of n buckets. Suppose H has the following property: for every bucket i and key k, a 1/n fraction of the hash functions in H map k to i. Is H universal?\n",
    "* Depends on H:\n",
    "    * Example 1 of universal H that satisfies those conditions:\n",
    "        * take H to be the set of all functions from mapping the universe to the number of buckets. \n",
    "        * that's a huge set, but it's a set nonetheless. \n",
    "        * By symmetry of having all of the functions, it both satisfies the property of this slide. \n",
    "        * It is indeed true that exactly a one on one refraction of all functions, map arbitrary key k to arbitrary bucket i. \n",
    "        * And by the same reasoning, by the same symmetry properties, this is universal. \n",
    "        * So really, if you think about it choosing a function at random function from H, is now just choosing a completely random function. \n",
    "        * that would indeed have a collision probability of exactly 1/n for each pair of distinct keys. So, this shows sometimes you can have both this property and be universal.\n",
    "    * Example 2 non-example: \n",
    "        * Take H = the set of n functions, each of which is a constant function\n",
    "        * Each maps any object x to exactly one bucket (no other function maps to this bucket)\n",
    "        * this set H does indeed satisfy this very reasonable looking property on this slide. \n",
    "        * Fix any key, fix any bucket, you know say bucket number 31 \n",
    "        * What is the probability that you pick a hash function that maps this key to bucket number 31? \n",
    "        * Independent of what the key is, it's going to be the probability that you pick the constant hash function whose output is always 31. \n",
    "        * Since there's n different constant functions, there's a 1/n probability.\n",
    "        * And Pr[hi(xk) = hi(xm) for any i] = 1 => not universal\n",
    "\n",
    "**Example: Hashing IP Addresses**\n",
    "Construction of a useful hash function that meet the difinition\n",
    "* Let U = IP addresses of the form (x1, x2, x3, x4) where each xi belongs to {0, 1, 2, ... 255}\n",
    "* hash function would be modulus with respect to a prime number of buckets\n",
    "* [the number of buckets n should be at least as large as the maximum hash coeffcient value xi, yi etc: see below]\n",
    "* the only **difference** is that we gonna multiply these x's by a random set of coefficients\n",
    "* we choose the number of buckets = double of number of objects you gonna store (let's say 500) \n",
    "* => prime around 1000 = 997\n",
    "* Construction of hash functions:\n",
    "    * One hash function h_a = [a1, a2, a3, a4]\n",
    "    * every ai = integer between 0 and n-1 (ie 0-996)\n",
    "    * There are n^4 such functions\n",
    "    * But how actually evaluate one of these functions\n",
    "* Define: \n",
    "    * h_a: IP address -> buckets\n",
    "    * by dot product: h_a . (x1, x2, x3, x4) = a1x1+a2x2+a3x3+a4x4 \n",
    "    * to get back in the range of what the buckets are actually indexed we take the modulus the number of buckets = mod 997\n",
    "    * => the output would be from 0 to n-1\n",
    "* So that's a set of n^4 hash functions \n",
    "    * for each hash function all we need to remember are the coefficients = cst space to store\n",
    "    * to get a bucket we do a constant amount of time (multiplication and mod) = cst time\n",
    "    * that's enough to meet the def of universal hash function\n",
    "* **Theorem** This family is universal\n",
    "* **Proof**\n",
    "    * Consider distinct IP addresses(x1,x2,x3,x4) and (y1,y2,y3,y4)\n",
    "    * Assume x4≠y4 (it should not matter where do the addresses differ actually)\n",
    "    * Question: collision probability? (Pr[ha(x1,x2,x3,x4) = ha(y1,y2,y3,y4)] ha belonging to H)\n",
    "        * what fraction of hash functions are cause them to collide?\n",
    "    * Note: collision \n",
    "        * <=> (a1x1+a2x2+a3x3+a4x4)(mod n) = (a1y1+a2y2+a3y3+a4y4)(mod n)\n",
    "        * <=> a4(x4-y4) (mod n) = (Sum_1_3(ai(yi-xi))) (mod n)\n",
    "    * Next: condition on random choises of a1, a2, a3; a4 still random\n",
    "        * with a1, a2, a3 fixed arbitrary, how many choices of a4 satisfy a4(x4-y4) (mod n) = (Sum_1_3(ai(yi-xi))) (mod n)? <=> choice of a4 causes collision\n",
    "        * by fixing a1, a2, a3, the right sum is just some fixed number from 0 to n-1\n",
    "        * a4 still random\n",
    "    * Key claim: left-hand side is equally likely to be any of {0, ..., n-1}\n",
    "    * Reason: \n",
    "        * (1) x4≠y4 => x4-y4 ≠ 0\n",
    "        * the number of buckets n should be at least as large as the maximum coeffcient value (n > x4 and y4)\n",
    "            * otherwise you could have x4 and y4 being different from each other, \n",
    "            * but the difference still winds up being zero mod-n. \n",
    "            * So for example, suppose n = 4, x4 = 6, y4 =10. \n",
    "            * x4-y4 = 4 that's actually zero mod-4. \n",
    "        * With IP = 255 is the max and n = 997, we are fine\n",
    "            *  if you only wanted to use say, 100 buckets, you didn't want to use 1000,\n",
    "            * you could just use smaller coefficients just by breaking up the IP address, \n",
    "            * instead of into 8-bit chunks,into 6-bit chunks, or 4-bit chunks, \n",
    "            * and that would keep the coefficient size smaller than the number of buckets,\n",
    "        * (2) n is prime\n",
    "        * (3) a4 uniform at random\n",
    "    * Proof by example (for formal see numbers theory): \n",
    "        * n =7, x4-y4 = 2 (or 3, or any other < than n) mod n \n",
    "        * I want to step through the seven possible choices of a4, and look at what we get in the left side of equation\n",
    "        * a4 = 0 => 0 * 2 mod n = 0\n",
    "        * a4 = 1 => 1 * 2 mod n = 2\n",
    "        * a4 = 2 => 2 * 2 mod n = 4\n",
    "        * a4 = 3 => 3 * 2 mod n = 6        \n",
    "        * a4 = 4 => 4 * 2 mod n = 1        \n",
    "        * a4 = 5 => 5 * 2 mod n = 3        \n",
    "        * a4 = 6 => 6 * 2 mod n = 5\n",
    "        * => all possible values of n \n",
    "    * => right side has 1/n chance to be equal to any particular bucket value\n",
    "    * QED\n",
    "\n",
    "\n",
    "### 15.3. Universal Hashing: Analysis of Chaining\n",
    "Performance of universal hash functions\n",
    "* Scenario:\n",
    "    * hash table implemented with chaning\n",
    "    * hash function h chosen uniformly at random from universal family H\n",
    "* Theorem [1979]\n",
    "    * All operations run in O(1) time (for every dataset S!)\n",
    "* Caveats:\n",
    "    * (1) in expectation over random choice of the hash function h\n",
    "    * (2) assumes |S| = O(n) [ie load alpha = |S|/n = O(1)]\n",
    "    * (3) assumes takes O(1) time to evaluate hash function\n",
    "* Proof: for Unsuccessful Lookup\n",
    "    * If we determine the upper bound of an unsuccessful operation, then it will be sufficient for bounding all of operations\n",
    "        * in hash table with chaining, you first hash the appropriate bucket and then you do the appropriate Insert, Delete or Lookup in the link list in that bucket. \n",
    "        * So, the worst case as far as traversing though this length list is if you're looking for something but it's not there \n",
    "        * you have to look at every single element in the list and followup into the list before you can conclude that the lookup has failed. \n",
    "        * insertion is always constant time, \n",
    "        * deletion and successful searches, you might get lucky, and stop early before you hit the end of the list. \n",
    "        * if we bother to analyze unsuccessful lookups that will carry over to all of the other operations.\n",
    "    * Let S = data set with |S| = O(n), n = number of buckets\n",
    "    * Consider lookup for x not belonging to S\n",
    "    * Running time\n",
    "        * Evaluation the hash function = computing h(x) = O(1)\n",
    "        * O(list length in A[h(x)]) = tranverse list\n",
    "    * Let L = list length in A[h(x)] \n",
    "        * L is a random variable which depends on choice of        \n",
    "    * We've reduced what we care about – expected time for lookup – to understanding the expected value of this random variable Capital L\n",
    "    * Decomposition principle:\n",
    "        * Identify a random variable (it's our L)\n",
    "        * Express it as sum of indicator (0/1) random variables L = Sum_1_n[Xi]\n",
    "        * Apply linearity of expectations: E[L] = sum_1_n[Pr[Xi = 1]]\n",
    "    * Indicator: \n",
    "        * For y belonging to S (y≠x) define Zy = 1 if h(y) = h(x), and 0 otherwise\n",
    "        * Note: L = sum_y_in_S [Zy] (counting the number of objects in the dataset S that will land in a bucket)\n",
    "    * Average list length = E[L] \n",
    "        * = Sum_y_in_S[E[Zy]]\n",
    "        * = Sum_y_in_S[Pr[h(x)=h(y)]] (because E[Zy] = 0 * Pr[Zy = 0] + 1 Pr[Zy = 1] = Pr[Pr[h(x)=h(y)])\n",
    "        * =< Sum_y_in_S[1/n] (by universality)\n",
    "        * = |S|/n\n",
    "        * = laod (alpha) = O(1) (by caveat)\n",
    "        \n",
    "        \n",
    "### 15.4 Hash table performance with open adderssing\n",
    "* Recall\n",
    "    * One object per slot\n",
    "    * load should be less then 1\n",
    "    * hash function produces a probe sequence for each possible key x\n",
    "* Fact: difficult to analyze rigorously\n",
    "* Heuristic assumption (which is not true! only for quick and dirty idealized analysis only): all n! probe sequences equally likely (it gives in some way a best case reference scenario)\n",
    "* Heuristic analysis\n",
    "    * Observation: under heuristic assumption the expected amount of time for insertion: 1/(1-alpha), alpha = load\n",
    "        * if alpha is small, we are fine and close to constant time\n",
    "    * Proof: coin flipping experiment:\n",
    "        * If alpha = load\n",
    "        * Doing random probes, looking for an empty slot, is = to flipping a coin with the probability of heads 1-alpha,\n",
    "        * And the number of probes you need until you successfully insert is just the number of times you flip this last coin until you see a head \n",
    "        * In fact this biased coin flipping experiment slightly overestimates the expected time for insertions under heuristics assumptions \n",
    "        * and that's because in the insertion time whenever we're never going to try the same slot twice. \n",
    "        * We're going to try all end buckets in some order with each of the n! orderings equally likely\n",
    "    * Quiz: Let N denote the number of coin flips needed to get \"heads,\" with a coin whose probability of \"heads\" is 1−α. What is E[N]?\n",
    "        * Answer: 1/(1-alpha)\n",
    "        * E[N] = 1P[h] + 2P[t]P[h] + 3(P[t]^2) P[h] + ... + N(P[t]^(N-1))P[h]\n",
    "            * = P[h] (1 + 2P[t] + ... + N(P[t]^(N-1)) (1)\n",
    "        * multiply (1) by P[t] => (2)\n",
    "        * (1)-(2) =>\n",
    "        * E[N] = 1 + P[t] + ... + P[t]^N\n",
    "            * = 1/(1-P[t]) \n",
    "            * 1/(1-alpha)\n",
    "        * as long as your load, alpha, is well bounded below one, you're good.\n",
    "    * Scepticisim: heuristic assumption is wrong, in reality all depends on which function you are using, depends on \n",
    "        * If you're Using double hashing and you have non-pathological data, I would go ahead and look for this 1/1-alpha bound in practice\n",
    "            * In linear probing heuristic assumption is completely wrong\n",
    "            * However, you're going to see something worse, but still in idealized situations quite reasonable\n",
    "  \n",
    "**Linear probing**\n",
    "* Heuristic assumption is really false: \n",
    "    * heuristic assumption, say that all in factorial probe sequences are equally likely\n",
    "    * and in linear probing the previous choice defines next \n",
    "* Assume instead: initial probe uniformly random, independent for different keys\n",
    "    * This is way stronger than assuming you ha ve a universal family of hash functions. \n",
    "    * This assumption is not satisfied in practice, but Performance guarantees we can derive under this assumption are typically satisfied in practice by well implemented hash tables that use linear probing.\n",
    "* Theorem: under this assumption expected Insertion time is around 1/(1-alpha)^2\n",
    "    *  It is still a function of the load alpha only and not a function of the number of objects in the hash table. \n",
    "    * That is with linear programming you will not get as good a performance guarantee, \n",
    "    * but it is still the case that if you keep the load factor bounded away from one. \n",
    "    * If you make sure the hash table doesn't get too full you will enjoy constant time operations on average\n",
    "* You might well wonder if it's ever worth implementing linear probing, given that it has the worst performance curve. \n",
    "    * The performance curve you'd hope from something like double hashing, 1/(1-alpha). \n",
    "    * It's a tricky cost benefit analysis between linear probing and more complicated but better performing strategies. \n",
    "    * That really depends on the application\n",
    "    * For example it interacts very well with memory hierarchies\n",
    "    \n",
    "    \n",
    "# 16. Bloom Filters\n",
    "\n",
    "### 16.1 The basics\n",
    "* Variant of hash tables\n",
    "* Used just to store the knowledge whether we've seen an object or not (we don't want to retreive it)\n",
    "  \n",
    "**Operations**\n",
    "* Insert\n",
    "* Lookup\n",
    "* Superfast\n",
    "\n",
    "**Comparaison**\n",
    "* Pros:\n",
    "    * More space efficient\n",
    "* Cons:\n",
    "    * Can't store an associated object \n",
    "    * Deletions are not allowed (at least it requires too much work)\n",
    "    * Small false positive probability \n",
    "    \n",
    "**Applications**\n",
    "* Original: early spellchecker\n",
    "    * You inserted the dict word by word into a bloom filter\n",
    "    * For the document, you go word by word and check whether it is in the bloom filter\n",
    "    * mark \"correct\" if yes\n",
    "* Canonical: keep track of the list of forbidden passwords\n",
    "* Modern: network routers packets treatment\n",
    "    * you have a budget on space\n",
    "    * you need super fast packets treatment\n",
    "\n",
    "**Implementation**\n",
    "* Ingredients\n",
    "    * Array, taking to each bucket a single bit (so array of n bits)\n",
    "        * So b = n/|S| – number of bits you are using per object in data set S)\n",
    "        * You can think for now of that number = 8 bits (per object)\n",
    "    * k hash functions h1, ... hk (k = small constant = 3, 4, 5, ...)\n",
    "* Insert(x): \n",
    "    * for i = 1, ..., k set [A[Hi(x)]] = 1 (whether or not bit already set to 1)\n",
    "* Lookup(x)\n",
    "    * return True <=> A[hi(x)] = 1 for every i = 1, 2, ... k\n",
    "* Note: \n",
    "    * no false negatives (if x was inserted lookup(x) guaranteed to succeed), \n",
    "    * but there could be false positives if all hi(x)'s already set to 1 by other insertions\n",
    "        * suppose there is an ip address\n",
    "        * suppose k = equals to 3\n",
    "        * for this ip we get (23, 51, 6) as (h1(x), h2(x), h3(x))\n",
    "        * those bits could be set to one by other ip addresses => we ge a false positive\n",
    "* The idea is usefull only if the error probability is quite small while space is also remaining small\n",
    "\n",
    "\n",
    "### 16.2 Heuristic analysis \n",
    "* Intuition: There will be some trade off by the space and the probability of errors (false positives)\n",
    "* Heuristic assumption [not justified]: all hi(x)'s uniformily random and independent (across different i's and x's)\n",
    "* Setup: we have n bits array, insert dataset S into a bloom filter\n",
    "* Note: for each bit of A, ther probability it's been set to 1 is (under assumption) 1 - (1-1/n)^(k|S|)\n",
    "    * The probability that it is zero: (1-1/n)^(k|S|)\n",
    "        * the probability, that this bit won't be hit by a certain \"dart\": 1 - 1/n\n",
    "        * the number of \"darts\" = number of each hash function triggered by each object: k|S|\n",
    "* Recall: function 1 + x is below e^x except the point 0\n",
    "    * => 1 - (1-1/n)^(k|S|) =< 1 - e^(-k|S|/n) = 1 - e^(-k/b)\n",
    "    * b = number of bits per object\n",
    "    * => the larger b, the lower probability of error\n",
    "* So under assumption: for x not in S, false positive probability is =< [1-e^(-k/b]]^k  = called Epsilon = error rate\n",
    "* **How do we set k?** for fixed b, Epsilon minimized by setting k around (ln 2) * b \n",
    "* Plugging back in: Epsilon = (1/2^(b ln2))\n",
    "    * If you double the number of bits that you're allocating per object, you're squaring the error rate and for small error rates, squaring it makes it much, much, much smaller\n",
    "    * Or b around 1.44 log_2 (1/Epsilon)\n",
    "* Example: With b = 8 bits, choose k = 5 or 6, error would be around 2% which is often good enough\n",
    "* **NB!** Remember, this is an idealized analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem set 8\n",
    "\n",
    "**1. Which of the following is not a property that you expect a well-designed hash function to have?**  \n",
    "\n",
    "Answer:  The hash function should \"spread out\" **every** data set (across the buckets/slots of the hash table).\n",
    "\n",
    "**2. Suppose we use a hash function h to hash n distinct keys into an array T of length m. Assuming simple uniform hashing --- that is, with each key mapped independently and uniformly to a random bucket --- what is the expected number of keys that get mapped to the first bucket? More precisely, what is the expected cardinality of the set \\{k:h(k)=1}.**  \n",
    "\n",
    "Answer: n/m (because it should be the avarege number of keys)\n",
    "\n",
    "**3. Suppose we use a hash function h to hash n distinct keys into an array T of length m. Say that two distinct keys x,y collide under h if h(x)=h(y). Assuming simple uniform hashing --- that is, with each key mapped independently and uniformly to a random bucket --- what is the probability that a given pair x,y of distinct keys collide?**  \n",
    "\n",
    "Answer: 1/m (the probability that key2 gets to the same bucket that key1)\n",
    "\n",
    "**4. Suppose we use a hash function h to hash n distinct keys into an array T of length m. Assuming simple uniform hashing --- that is, with each key mapped independently and uniformly to a random bucket --- what is the expected number of pairs of distinct keys that collide? (As above, distinct keys x,y are said to collide if h(x)=h(y).)** \n",
    "\n",
    "Answer: \n",
    "* E(there would be N such pairs) = Pr[h(x1) = h(x2)] + Pr[h(x1) = h(x3)] + ... + Pr[h(xn-1) = h(xn)] \n",
    "* = [(n-1)n/2] * 1/m\n",
    "\n",
    "**5. To interpret our heuristic analysis of bloom filters in lecture, we considered the case where we were willing to use 8 bits of space per object in the bloom filter. Suppose we were willing to use twice as much space (16 bits per object). What can you say about the corresponding false positive rate, according to our heuristic analysis (assuming that the number k of hash tables is set optimally)? [Choose the strongest true statement.]**\n",
    "\n",
    "Answer: \n",
    "* According to the analysis in the lecture for b = 8 Epsilon 1 = 2% = ie 0.02\n",
    "* Epsilon = (1/2^(b ln2)) => Epsillon 2 = Epsillon 1 ^ 2 => 0.0004 = 0.04%\n",
    "* wich is Less than 0.1%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional problems 8\n",
    "\n",
    "* Recall that a set H of hash functions (mapping the elements of a universe U to the buckets {0,1,2,…,n−1}) is universal \n",
    "* if for every distinct x,y ∈ U, the probability Prob[h(x)=h(y)] that x and y collide, assuming that the hash function h is chosen uniformly at random from H, is at most 1/n. \n",
    "* In this problem you will prove that a collision probability of 1/n is essentially the best possible. \n",
    "* Precisely, suppose that H is a family of hash functions mapping U to {0,1,2,…,n−1}, as above. \n",
    "    * Show that there must be a pair x,y∈U of distinct elements such that, \n",
    "    * if h is chosen uniformly at random from H, \n",
    "    * then Prob[h(x) = h(y)] ≥ 1/n - 1/|U|\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Exam 2\n",
    "\n",
    "**1. Consider a directed graph G=(V,E) with non-negative edge lengths and two distinct vertices s and t of V. Let P denote a shortest path from s to t in G. If we add 10 to the length of every edge in the graph, then: [Check all that apply.]**  \n",
    "\n",
    "**(a) P might or might not remain a shortest s−t path (depending on the graph).**\n",
    "**(b) If P has only one edge, then P definitely remains a shortest s−t path.**\n",
    "**(c) P definitely remains a shortest s−t path.**\n",
    "**(d) P definitely does not remain a shortest s−t path.**  \n",
    "\n",
    "(!) Answer: a, **b**   \n",
    "\n",
    "**2. What is the running time of depth-ﬁrst search, as a function of n and m, if the input graph G=(V,E) is represented by an adjacency matrix (i.e., NOT an adjacency list), where as usual n=∣V∣ and m=∣E∣?**  \n",
    "\n",
    "Answer: O(n^2)  \n",
    " \n",
    "**3. What is the asymptotic running time of the Insert and Extract-Min operations, respectively, for a heap with n objects?**  \n",
    "\n",
    "Answer: O(log_2(n)) and O(log_2(n))   \n",
    "\n",
    "**4. On adding one extra edge to a directed graph G, the number of strongly connected components...?**  \n",
    "\n",
    "Answer: might or might not remain the same (depending on the graph)  \n",
    "\n",
    "**5.Which of the following statements hold? (As usual n and m denote the number of vertices and edges, respectively, of a graph.) [Check all that apply.]**\n",
    "\n",
    "**(a) Breadth-first search can be used to compute shortest paths in O(m+n) time (when every edge has unit length).** \n",
    "**(b) Breadth-first search can be used to compute the connected components of an undirected graph in O(m+n) time.**\n",
    "**(c) Depth-first search can be used to compute a topological ordering of a directed acyclic graph in O(m+n) time.**\n",
    "**(d) Depth-first search can be used to compute the strongly connected components of a directed graph in O(m+n) time.**\n",
    "\n",
    "Answer: a, b, c, d    \n",
    "\n",
    "**6. When does a directed graph have a unique topological ordering?**\n",
    "\n",
    "Answer: None of the options  \n",
    "\n",
    "**7.Suppose you implement the operations Insert and Extract-Min using a sorted array (from biggest to smallest). What is the worst-case running time of Insert and Extract-Min, respectively? (Assume that you have a large enough array to accommodate the Insertions that you face.)**\n",
    "\n",
    "(!) Answer: Θ(n) and Θ(1) \n",
    "\n",
    "**8. Which of the following patterns in a computer program suggests that a heap data structure could provide a significant speed-up (check all that apply)?**\n",
    "  \n",
    "**(a) None of the other options**  \n",
    "**(b) Repeated minimum computations**  \n",
    "**(c) Repeated maximum computations**  \n",
    "**(d) Repeated lookups**  \n",
    "\n",
    "Answer: b, c\n",
    "\n",
    "**9. Which of the following patterns in a computer program suggests that a hash table could provide a significant speed-up (check all that apply)?**\n",
    "\n",
    "**(a) Repeated minimum computations**  \n",
    "**(b) Repeated lookups**  \n",
    "**(c) None of the other options**  \n",
    "**(d) Repeated maximum computations**  \n",
    "\n",
    "Answer: b\n",
    "\n",
    "**10. Which of the following statements about Dijkstra's shortest-path algorithm are true for input graphs that might have some negative edge lengths? [Check all that apply.]**\n",
    "\n",
    "**(a) It is guaranteed to terminate.**  \n",
    "**(b) It may or may not terminate (depending on the graph).**  \n",
    "**(c) It is guaranteed to correctly compute shortest-path distances (from a given source vertex to all other vertices).**  \n",
    "**(d) It may or may not correctly compute shortest-path distances (from a given source vertex to all other vertices), depending on the graph.**  \n",
    "\n",
    "Answer: a, d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programing assignment 8\n",
    "\n",
    "The goal of this problem is to implement a variant of the 2-SUM algorithm covered in this week's lectures.\n",
    "\n",
    "The file contains 1 million integers, both positive and negative (there might be some repetitions!). This is your array of integers, with the i-th row of the file specifying the i-th entry of the array.\n",
    "\n",
    "Your task is to compute the number of target values t in the interval [-10000,10000] (inclusive) such that there are distinct numbers x,y in the input file that satisfy x+y=t. (NOTE: ensuring distinctness requires a one-line addition to the algorithm from lecture.)\n",
    "\n",
    "Write your numeric answer (an integer between 0 and 20001) in the space provided.\n",
    "\n",
    "OPTIONAL CHALLENGE: If this problem is too easy for you, try implementing your own hash table for it. For example, you could compare performance under the chaining and open addressing approaches to resolving collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArray():\n",
    "    '''\n",
    "    load txt (one number per line) into an array\n",
    "    each line is a number \n",
    "    '''\n",
    "    file = open(\"../algorithms_course_code/data/pg_asmt_8_hash.txt\", 'r')\n",
    "    array = []\n",
    "    for line in file:\n",
    "        line_strip = line.rstrip(\"\\n\")\n",
    "        array.append(int(line_strip))\n",
    "    file.close()    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[68037543430, -21123414637, 56619844751, 59688006695, 82329471587]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadArray()\n",
    "print(len(data))\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999752"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique values:\n",
    "import numpy as np\n",
    "len(np.unique(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: mod and remainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: -99999887310 max: 99999662302\n"
     ]
    }
   ],
   "source": [
    "print(\"min:\", min(data), \"max:\", max(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min mod 10000: -9999989 , max mod 10000: 9999966\n"
     ]
    }
   ],
   "source": [
    "print(\"min mod 10000:\", min(data)//10000, \", max mod 10000:\", max(data)//10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n"
     ]
    }
   ],
   "source": [
    "# creating a dictionary \n",
    "# key = the mod 10000 of each element of the array (will span from -9999989 to 9999966)\n",
    "# value = the array to store array elements with corresponding mod 10000\n",
    "h = {}\n",
    "delta_mods = 9999999\n",
    "\n",
    "for i in range(-delta_mods, delta_mods+1):\n",
    "    h[i] = []\n",
    "\n",
    "# transform array to a set (removes duplicates)    \n",
    "data_set = list(set(data.copy()))\n",
    "\n",
    "# fill the dictionary with array elements whith mod 10000 = key\n",
    "for i in data_set:\n",
    "    h[i//10000] += [i]\n",
    "\n",
    "# create a list for sums    \n",
    "t = []\n",
    "\n",
    "for i in range(-delta_mods, delta_mods+1):\n",
    "    if len(h[i]) > 0:\n",
    "        find = h[-i-2]+h[-i-1]+h[-i]+h[-i+1]\n",
    "        for x in h[i]:\n",
    "            for y in find:\n",
    "                if x != y and abs(x+y) <= 10000 and x+y not in t:\n",
    "                    t += [x+y]\n",
    "\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: birary tree search (straigtforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hashtable with input values as keys and 1 as values\n",
    "hashtable = {}\n",
    "for num in data:\n",
    "    hashtable[num] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from tqdm import tqdm # package to visualize for progress bar \n",
    "\n",
    "def two_sum(nums, low, high):\n",
    "    \"\"\" \n",
    "    find the counts of a target in [low, high] that \n",
    "    satisfies sum of two distinct elements in nums equal to target\n",
    "    TODO: finishing in about 3 hours, need to be optimized\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for target in tqdm(range(low, high + 1)):\n",
    "        tmp_dict = hashtable.copy()\n",
    "        for num in nums:\n",
    "            if num in hashtable and target - num in tmp_dict and num != target - num:\n",
    "                count += 1\n",
    "                break\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = two_sum(nums, -10000, 10000)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: sort A, for each x find subarray of A where y's might reside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Sort the input array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-99999887310, -99999653362, -99999563583, -99999468029, -99999376014]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sorted = data.copy()\n",
    "data_sorted.sort()\n",
    "data_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68037543430, -21123414637, 56619844751, 59688006695, 82329471587]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Create a copy of an input array and convert it to balanced binary search tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbst_array = data_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to convert a sorted array \n",
    "# to a balanced Binary Search Tree \n",
    "\n",
    "\n",
    "# Python3 implementation of above approach \n",
    "  \n",
    "# Link list node  \n",
    "class LNode : \n",
    "    def __init__(self): \n",
    "        self.data = None\n",
    "        self.next = None\n",
    "\n",
    "# A Binary Tree node  \n",
    "class TNode : \n",
    "    def __init__(self): \n",
    "        self.data = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "head = None\n",
    "  \n",
    "# This function counts the number of  \n",
    "# nodes in Linked List and then calls  \n",
    "# sortedListToBSTRecur() to construct BST  \n",
    "def sortedListToBST():  \n",
    "    global head \n",
    "      \n",
    "    # Count the number of nodes in Linked List  \n",
    "    n = countLNodes(head)  \n",
    "  \n",
    "    # Construct BST  \n",
    "    return sortedListToBSTRecur(n)  \n",
    "  \n",
    "# The main function that constructs  \n",
    "# balanced BST and returns root of it.  \n",
    "# head -. Pointer to pointer to  \n",
    "# head node of linked list n -. No. \n",
    "# of nodes in Linked List  \n",
    "def sortedListToBSTRecur( n) : \n",
    "    global head \n",
    "      \n",
    "    # Base Case  \n",
    "    if (n <= 0) : \n",
    "        return None\n",
    "  \n",
    "    # Recursively construct the left subtree  \n",
    "    left = sortedListToBSTRecur( int(n/2))  \n",
    "  \n",
    "    # Allocate memory for root, and  \n",
    "    # link the above constructed left  \n",
    "    # subtree with root  \n",
    "    root = newNode((head).data)  \n",
    "    root.left = left  \n",
    "  \n",
    "    # Change head pointer of Linked List \n",
    "    # for parent recursive calls  \n",
    "    head = (head).next\n",
    "  \n",
    "    # Recursively construct the right  \n",
    "    # subtree and link it with root  \n",
    "    # The number of nodes in right subtree \n",
    "    # is total nodes - nodes in  \n",
    "    # left subtree - 1 (for root) which is n-n/2-1 \n",
    "    root.right = sortedListToBSTRecur( n - int(n/2) - 1)  \n",
    "  \n",
    "    return root  \n",
    "  \n",
    "# UTILITY FUNCTIONS  \n",
    "  \n",
    "# A utility function that returns  \n",
    "# count of nodes in a given Linked List  \n",
    "def countLNodes(head) : \n",
    "  \n",
    "    count = 0\n",
    "    temp = head  \n",
    "    while(temp != None):  \n",
    "      \n",
    "        temp = temp.next\n",
    "        count = count + 1\n",
    "      \n",
    "    return count  \n",
    "  \n",
    "# Function to insert a node  \n",
    "#at the beginging of the linked list  \n",
    "def push(head, new_data) : \n",
    "  \n",
    "    # allocate node  \n",
    "    new_node = LNode() \n",
    "      \n",
    "    # put in the data  \n",
    "    new_node.data = new_data  \n",
    "  \n",
    "    # link the old list off the new node  \n",
    "    new_node.next = (head)  \n",
    "  \n",
    "    # move the head to point to the new node  \n",
    "    (head) = new_node  \n",
    "    return head \n",
    "\n",
    "\n",
    "# Function to print nodes in a given linked list  \n",
    "def printList(node):  \n",
    "  \n",
    "    while(node != None):  \n",
    "      \n",
    "        print( node.data ,end= \" \")  \n",
    "        node = node.next\n",
    "        \n",
    "    \n",
    "# Helper function that allocates a new node with the  \n",
    "# given data and None left and right pointers.  \n",
    "def newNode(data) : \n",
    "  \n",
    "    node = TNode() \n",
    "    node.data = data  \n",
    "    node.left = None\n",
    "    node.right = None\n",
    "  \n",
    "    return node  \n",
    "  \n",
    "# A utility function to  \n",
    "# print preorder traversal of BST  \n",
    "def preOrder(node) : \n",
    "  \n",
    "    if (node == None) : \n",
    "        return\n",
    "    print(node.data, end = \" \" ) \n",
    "    preOrder(node.left)  \n",
    "    preOrder(node.right) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TreeNode' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-594-69971799a231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_array_to_bst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-593-ce0e2896833b>\u001b[0m in \u001b[0;36mpreOrder\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mpreOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mpreOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TreeNode' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "result = sorted_array_to_bst([1, 2, 3, 4, 5, 6, 7])\n",
    "preOrder(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TreeNode at 0x10f924b70>"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
