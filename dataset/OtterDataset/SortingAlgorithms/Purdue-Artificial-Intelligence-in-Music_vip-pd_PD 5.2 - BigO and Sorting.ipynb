{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V9h0Dy5_cVQ"
   },
   "source": [
    "# **PD 5.2 - Big-O and Sorting**\n",
    "\n",
    "Written by William Jiang and Tim Nadolsky\n",
    "\n",
    "Welcome to the 5th PD in the Purdue AIM VIP PD series! This tutorial will introduce a few sorting algorithms and Big-O analysis. It is recommended to complete PD 5.1 - Recursion and Induction before this assignment as proof techniques from that assignment will be used here.\n",
    "\n",
    "At the end of this notebook, you should have learned the following:\n",
    "- How Big-O allows us to talk about program runtimes in a simple manner\n",
    "- The difference between Big-O, Big-$\\Omega$ and Big-$\\Theta$\n",
    "- How to convert recurrences to Big-O expressions\n",
    "- Some basic sorting algorithms\n",
    "- How to prove basic sorting algorithms work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "# This function runs the function called \"function\" (input argument), and measures/prints out the time it takes to run.\n",
    "def time_function(function: Callable):\n",
    "    start_time = time.time()           # Grab the current time\n",
    "    function()                         # Call the function\n",
    "    end_time = time.time()             # Grab the current time\n",
    "    duration = end_time - start_time   # Calculate the difference in times to find the program run time\n",
    "    print(\"The operation took %.4f seconds\" % duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Big-O\n",
    "\n",
    "Consider the following situation: Andy and Bob each have their own version of a program which runs on a test dataset provided by their professor. Andy's program takes 0.51 seconds on the test dataset while Bob's program takes 0.49 seconds. Pretty similar, right?\n",
    "\n",
    "However, one day their prof announces that since they have more funding, they have a new dataset which is 3 times the size of the original, and they also got new computers which are much faster than the old ones. When Andy and Bob rerun their code on the new machines, Andy's code takes 0.07 seconds on the old dataset and 0.64 seconds on the new one, while Bob's code takes 0.06 seconds on the old dataset but 1.62 seconds on the new one! What gives? I thought Bob's program was faster too!\n",
    "\n",
    "Hopefully, the above scenario illustrates the need for a program runtime analysis technique that:\n",
    "- Gives us a good idea of how fast a given piece of code is when we change the input data size.\n",
    "- Doesn't make us deal with empirical/measured values.\n",
    "- Given two pieces of code, if one is faster than the other according to our metric, this metric shouldn't change regardless of what machine we run our code on.\n",
    "\n",
    "To solve this problem, computer scientists started using what's called Big-O notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big O definition\n",
    "\n",
    "Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, it is frequently used to analyze the worst-case runtime and memory efficiency of algorithms for problems involving large amounts of data.\n",
    "\n",
    "In simple terms, you can think of Big O like this: we have two functions $f(n)$ and $g(n)$. We say $f(n)$ is in the set $O(g(n))$ if when our input size is really large, $f(n)$'s runtime is at most proportional to $g(n)$'s. Essentially, $g(n)$ is somewhat like an upper bound on the runtime of $f(n)$.\n",
    "\n",
    "Mathematically, given two functions $f(n)$ and $g(n)$, we say that $f(n)$ is in ($\\in$) $O(g(n))$ if there exist constants $c > 0$ and $n_0 \\ge 0$ such that $f(n) \\le c\\cdot g(n)$ for all $n \\ge n_0$.\n",
    "\n",
    "When measuring runtimes, we usually discard actual measured runtimes and instead rely on abstract \"operations\", where 1 addition/multiplication/variable set/get/etc. are all counted as 1 \"operation\". This relies on the assumption that all these operations take roughly the same time at the hardware level, which is usually good enough in languages like Python or even C, but not necessarily valid in embedded systems/microcode/etc.\n",
    "\n",
    "Let's start with an analysis of very simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "vUPfDcVAXzYD"
   },
   "outputs": [],
   "source": [
    "# Python program to illustrate time complexity for a single for-loop\n",
    "# This loop runs for O(n)time\n",
    "def simple_for_loop():\n",
    "    a = 0\n",
    "    n = 1000000\n",
    "    for i in range(n)\n",
    "        a = a + 1\n",
    "        \n",
    "time_function(simple_for_loop)  # Sneaky use of Python function pointers - scroll up and check the function definition to see\n",
    "                                # how they work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evrM8ZZIBLpw"
   },
   "source": [
    "#### \"Absolute\" run time analysis\n",
    "\n",
    "``a = a + 1`` is one operation as defined above - let's say it takes $c_0$ time. Our ``for`` loop runs $n$ times, so our total \"absolute\" runtime is $c_0 n$ operations. (\"Absolute\" is in quotes here because the value of $c_0$ depends on our hardware.)\n",
    "\n",
    "#### Converting to Big-O\n",
    "\n",
    "As a toy example, let's convert our runtime $f(n)=c_0 n$ to a Big-O recurrence. $g(n)=n$ looks like a good candidate. Applying the definition, we get that for $c=c_0$ and $n_0=0$, we have $f(n)\\leq c\\cdot g(n)$. Thus, our program ($f(n)$) is in the runtime set $O(n)$.\n",
    "\n",
    "As for what this means practically, we should expect our function runtime to grow linearly - that is, doubling the input size will also double the time the function takes to run. You can plot some runtime values in Excel to verify this trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY6j_rtoHxO8"
   },
   "source": [
    "#### More examples of runtime recurrence conversions:\n",
    "\n",
    "$f(n)=n^3+5n^2-30n+1$: \n",
    "\n",
    "$\\leq n^3 + 5n^3 + 30n^3 + n^3$ (replace all $-$ with $+$ and all polynomial terms with the largest degree exponential)\n",
    "\n",
    "$\\leq 37n^3$.\n",
    "\n",
    "Choose $g(n)=n^3$, $c=37$, $n_0 = 1$. Thus for all $n\\geq n_0$, $f(n)\\leq cg(n)$ and thus $f(n)\\in O(n^3)$.\n",
    "\n",
    "In general, if $f(n)$ is a polynomial with degree $d$, $f(n)\\in O(n^d)$ by a similar proof to the one above.\n",
    "\n",
    "$f(n) = 3^n + n^3$:\n",
    "\n",
    "$\\leq 3^n + (3^{\\log_3{n}})^3$\n",
    "\n",
    "$\\leq 3^n + 3^{3\\log_3{n}}$\n",
    "\n",
    "$\\leq 3^n (1 + 3^{(3\\log_3{n}) - n})$\n",
    "\n",
    "$\\leq 2\\cdot3^{n}$\n",
    "\n",
    "Choose $g(n)=3^n$, $c=2$, $n_0 = 1$. Thus for all $n\\geq n_0$, $f(n)\\leq cg(n)$ and thus $f(n)\\in O(3^n)$.\n",
    "\n",
    "In general, if $f(n)=x(n)+y(n)$ where $x(n)\\in O(a(n))$ and $y(n) \\in O(b(n))$, then $f(n)$ is in $O(\\max{(a(n),b(n))})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An interesting quirk of Big-O\n",
    "\n",
    "Let's go back to $f(n)=c_0 n$. Instead of $g(n)=n$, let's choose $g(n) = n^2$. You can replicate the rest of the proof exactly the same, so $f(n)\\in O(n^2)$. What gives? \n",
    "\n",
    "This is correct - almost every function $f(n)$ is $O(g(n))$ if $g(n)$ grows sufficiently fast. Thus, as computer scientists - we are only interested in **tight** Big-O bounds - bounds where we cannot define another reasonably common function to be a better \"fit\" to the function's growth than the one we currently have in the bound.\n",
    "\n",
    "For example, $O(n)$ is a tight bound for $c_0 n$ but $O(n^2)$ is not.\n",
    "\n",
    "#### A table of functions and their Big-O order\n",
    "\n",
    "Here is a list of common functions and their Big-O order.\n",
    "\n",
    "$O(1)$ (constant time)\n",
    "\n",
    "$\\in O(\\log \\log n)$\n",
    "\n",
    "$\\in O(\\log n)$ (logarithmic time)\n",
    "\n",
    "$\\in O(n^p)$ where $0<p\\leq 1$\n",
    "\n",
    "$\\in O(n)$ (linear time)\n",
    "\n",
    "$\\in O(n \\log n)$ (linearithmic time)\n",
    "\n",
    "$\\in O(n^2)$ (quadratic time)\n",
    "\n",
    "$\\in O(n^2 \\log n)$\n",
    "\n",
    "$\\in O(n^3)$ (cubic time)\n",
    "\n",
    "...\n",
    "\n",
    "$\\in O(n^p)$ where $p$ is large (polynomial time)\n",
    "\n",
    "$\\in O(b^n)$ where $b>1$ (exponential time)\n",
    "\n",
    "$\\in O(n!)$ (factorial time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0BToqUmIE83"
   },
   "source": [
    "## 5.2.2 Big-Ω and Big-Θ\n",
    "\n",
    "Big-$\\Omega$ (omega) is similar to big-O in that it represents an asymptotic lower bound on the runtime of an algorithm. Put more rigorously,\n",
    "\n",
    "Given two functions  $f(n)$  and  $g(n)$ , we say that $f(n)$ is  $\\Omega(g(n))$  if there exist constants $c>0$ and  $n_0\\ge 0$ such that $f(n)\\ge c \\cdot g(n)$  for all $n \\ge n_0$.\n",
    "\n",
    "The proofs look almost exactly the same as Big-O proofs; just the direction of the inequality is reversed.\n",
    "\n",
    "For example, $f(n)=n$ is $\\Omega(\\log n)$ and $f(n^3)$ is $\\Omega(n)$.\n",
    "\n",
    "From this definition, we know that if $f(n) \\in O(g(n))$, $g(n) \\in \\Omega(f(n))$ by choosing $c'=\\frac{1}{c}$ with the same $n_0$.\n",
    "\n",
    "Finally, we can use Big-$\\Theta$ (theta) notation to describe an asymptotically tight bound for a function. Given two functions  $f(n)$  and  $g(n)$  , we say that $f(n)$ is  $Θ(g(n))$  if there exist constants  $c_1,c_2>0$  and  $n_0\\ge0$  such that  $c_1\\cdot g(n) \\le f(n)\\le c_2\\cdot g(n)$  for all  $n \\ge n_0$ .\n",
    "\n",
    "For example, consider the function $f(n)=n^2+3n+5$. Since for all $n \\ge 1$ we have $n^2\\le f(n)\\le 9n^2$, we know that $f(n)$ is $\\Theta(n^2)$.\n",
    "\n",
    "From the definition of Big-Θ, we can easily show that $f(n)$ is  $Θ(g(n))$ if $f(n)=O(g(n))$ and $f(n) = Ω(g(n))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iuVKeoCX5HN"
   },
   "source": [
    "\n",
    "## 5.2.3 Sorting\n",
    "\n",
    "As an application of recursion, induction, and Big-O analysis, we will look at some sorting algorithms. In computer science, we often use sorting to rearrange an array or list of elements in ascending or descending order. There are many algorithms for sorting, but for the purposes of this module we will cover two types of sorting.\n",
    "\n",
    "Specifically, we consider a sorting algorithm to be a function which takes in an array $A$ and outputs a copy of $A$ with the elements arranged in descending order.\n",
    "\n",
    "#### Human Sort\n",
    "\n",
    "This isn't a real sort according to some people, but most people seem to intuitively know how this works so we consider it here.\n",
    "\n",
    "The pseudocode for Human Sort is as follows:\n",
    "1. Make a new array B that's the same size as A.\n",
    "2. Find the minimum element in A.\n",
    "3. Remove this minimum element from A and place it at the end of B.\n",
    "4. Repeat steps 2-3 until all elements have been moved to B.\n",
    "\n",
    "Here is Python code for Human Sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_sort(A: list(float)):\n",
    "    B = []\n",
    "    while len(A) != 0:\n",
    "        min = float('inf')  # *1 start\n",
    "        for i in A:\n",
    "            if i < min:\n",
    "                min = i     # *1 end\n",
    "        B.append(min)       # *2\n",
    "        A.remove(min)       \n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a rough proof this works:\n",
    "\n",
    "Claim: We claim that after the first $i$ elements have been moved from $A$ to $B$, $B$ is sorted.\n",
    "\n",
    "Base case: After $i=0$ elements have been moved, $B$ is empty, and thus, it is sorted.\n",
    "\n",
    "Inductive step: Assume after $k-1$ elements have been moved, $B$ is sorted.\n",
    "\n",
    "The code between the lines ``*1 start`` and ``*1 end`` finds the minimum element in $A$. (Proof is exercise to reader.) After this code runs, ``min`` contains the minimum element in $A$. ``min`` is also greater than any other element in $B$; if it isn't, it would have been moved in a previous step (there must be at least one element in $B$ greater than $A$, which should have been moved after ``min``).\n",
    "\n",
    "We then remove ``min`` from $A$ and place it at the end of $B$ around line ``*2``. ``min`` is after every element in $B$ and is greater than every element in $B$; thus, it is in the correct position in $B$. The first $k-1$ elements in $B$ haven't been touched and are therefore still sorted with respect to each other (by IH). Thus, $B$ is sorted after the first $k$ elements have been moved.\n",
    "\n",
    "Thus, by induction, we prove that after the first $i$ elements have been moved from $A$ to $B$, $B$ is sorted. Finally, after all the elements have been moved from $A$ to $B$, $B$ is sorted. QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of this are left to the reader to verify, but this algorithm takes $O(n^2)$ time to run and $O(n)$ auxillary space to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Sort\n",
    "Merge sort uses a recursive approach (called divide-and-conquer) to sort an array of elements. The three main steps are:\n",
    "\n",
    "1.  Divide the list or array recursively into two halves until it can no more be divided.\n",
    "\n",
    "2.  Each subarray is sorted individually using the merge sort algorithm.\n",
    "\n",
    "3.  The sorted subarrays are merged back together in sorted order. The process continues until all elements from both subarrays have been merged.\n",
    "\n",
    "Here is pseudocode that can be used for one implementation of mergesort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdMGOG7NLQBE"
   },
   "outputs": [],
   "source": [
    "# This is pseudocode, please do not run\n",
    "function mergesort(A) # A is a list of integers.\n",
    "  if len(A) > 1\n",
    "    # Find the middle index of the array\n",
    "    m = len(A)/2\n",
    "\n",
    "    # Split the array into left (L) and right (R) subarrays\n",
    "    L = A[0,...,m-1] # left subarray\n",
    "    R = A[m,...,len(A)-1] # right subarray\n",
    "    L = mergesort(L) # call mergesort again on left subarray\n",
    "    R = mergesort(R) # call mergesort again on right subarray\n",
    "\n",
    "    # Merge L and R back on top of A\n",
    "    Lindex = 0\n",
    "    Rindex = 0\n",
    "    Aindex = 0\n",
    "    while Lindex < len(L) and Rindex < len(R)\n",
    "      if L[Lindex] <= R[Rindex]: # If the current element in L is less than or equal to the current element in R\n",
    "        A[Aindex] = L[Lindex] # Copy the element from L to A\n",
    "        Lindex ++ # Move to the next element in L\n",
    "        Aindex ++ # Move to the next element in A\n",
    "      else:\n",
    "        A[Aindex] = R[Rindex] # Otherwise copy the element from R to A\n",
    "        Rindex ++ # Move to the next element in R\n",
    "        Aindex ++ # Move to the next element in A\n",
    "    end while\n",
    "\n",
    "    # Copy any remaining elements in L to A\n",
    "    while Lindex < len(L)\n",
    "      A[Aindex] = L[Lindex]\n",
    "      Lindex ++\n",
    "      Aindex ++\n",
    "    end while\n",
    "    # Copy any remaining elements in R to A\n",
    "    while Rindex < len(R)\n",
    "      A[Aindex] = R[Rindex]\n",
    "      Rindex ++\n",
    "      Aindex ++\n",
    "    end while\n",
    "  end if\n",
    "  return A # return original array, which is now sorted\n",
    "end algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNhwgMbBPpoD"
   },
   "source": [
    "For a great illustration on how merge sort works, see the following demo: https://www.hackerearth.com/practice/algorithms/sorting/merge-sort/visualize/\n",
    "\n",
    "It is recommended to run the demo without any preset array values. This causes the simulation to fill the array with random values and can give a better understanding of the sorting algorithm.\n",
    "\n",
    "How can we use the given code to analyze the time and space complexity of merge sort? Observe that other than the two recursive calls to mergesort there are constant time calculations and three while loops.\n",
    "\n",
    "However, observe that the three while loops together result in a total of $n$ iterations because together they just merge $L$ and $R$ back together and since $L$ and $R$ together form $A$, the claim follows.\n",
    "Together then, other than the two recursive calls, $Θ(n)$ time is required. It follows that the time complexity $T(n)$ on an input of size $n$ therefore satisfies the recurrence relation:\n",
    "$T(n) = 2T(n/2) + f(n)$ with $T(1)$ constant and $f(n) = Θ(n)$\n",
    "\n",
    "From here, try to figure out why $T(n)=Θ(n\\log n)$. Hint: try drawing a tree using the recurrence relation. What is the sum of the cost of each levels? How many levels are there?\n",
    "\n",
    "We can find the space complexity of merge sort in a similar manner. After finding a recurrence from analyzing the code, we can solve to find $Θ(n)$ space complexity.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
