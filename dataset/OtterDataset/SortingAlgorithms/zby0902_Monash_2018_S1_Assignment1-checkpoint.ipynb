{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Assignment 1 - Solution Workbook\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the assignment instruction carefully and implement the algorithms in this workbook. \n",
    "- You can use the datasets fireData and climateData (provided below) if you are aiming for Credit Task.\n",
    "- For Distinction and High Distinction tasks, you are required to read the files FireData.csv and ClimateData.CSV provided with the assignment programatically and prepare the data in the correct format so that it can be used in your algorithm. \n",
    "- You can introduce new cells as necessary.\n",
    "\n",
    "**Your details**\n",
    "- Name: Boyu Zhang\n",
    "- Student ID:28491300 \n",
    "\n",
    "- Name: Yining Ye\n",
    "- Student ID:27517517\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firePath = './data/FireData.csv'\n",
    "climatePath = './data/ClimateData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_list(path):\n",
    "    with open(path,'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climateData = read_to_list(climatePath)[1:]\n",
    "fireData = read_to_list(firePath)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a glance on the data of climate\n",
    "climateData[0],fireData[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Parallel Search\n",
    "### 1. \n",
    "Write an algorithm to search climate data for the records on ​15th December 2017​. \n",
    "Justify your choice of the data partition technique and search technique you have used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if climate data is sorted by date\n",
    "climateData == sorted(climateData,key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:\n",
    "\n",
    "Binary search + roud-robin partition\n",
    "\n",
    "From the above exploration of the data in the climateData list, we can find out that all data are already sorted in term of the data column which is is excatly our search key.\n",
    "\n",
    "Given this, we can just pick a simplest partition method such as round-robin to partition the dataset evenly which maintains the balance of load without compromising on efficiency.\n",
    "\n",
    "The binary search is obviously the desirable option when the source data is already sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here starts the first half of task1\n",
    "#first pick a partition method:\n",
    "def rr_partition(data,n):\n",
    "    \"\"\"\n",
    "    Perform a simple round robin partition on the given data set\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be partitioned, which is a list\n",
    "    n: the number of groups that the dataset will be divided into\n",
    "    \n",
    "    Return:\n",
    "    result: the partitioned subset of the dataset \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in  range(n):\n",
    "        result.append([])\n",
    "    for index,element in enumerate(data):\n",
    "        index_bin = index%n\n",
    "        result[index_bin].append(element)\n",
    "    return result\n",
    "    \n",
    "#then pick a search method:\n",
    "def binary_search(data,key):\n",
    "    \"\"\"\n",
    "    Perform binary search given certain key\n",
    "    \n",
    "    Parameters:\n",
    "    data: the input dataset which is a list\n",
    "    key: an query record\n",
    "    \n",
    "    Return:\n",
    "    found: the mathced record and its position in a tuple, return (-1,None) if not found \n",
    "    \"\"\"\n",
    "    position = -1\n",
    "    found = None\n",
    "    upper = len(data) - 1\n",
    "    lower = 0\n",
    "    \n",
    "    while lower <= upper and not found:\n",
    "        mid = (upper + lower)//2\n",
    "        if data[mid][1] == key:\n",
    "            found = data[mid]\n",
    "            position = mid\n",
    "        elif data[mid][1] < key:\n",
    "            lower = mid + 1\n",
    "        else:\n",
    "            upper = mid - 1     \n",
    "    return found\n",
    "\n",
    "#the complete parrallel search:\n",
    "from multiprocessing import Pool\n",
    "def parallel_search_date(data,query,n_processor):\n",
    "    \"\"\"\n",
    "    A method doing parallel search on a given dataset ,\n",
    "    when given a search clue like a single key or a range for certain column value\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be searched, which is a list\n",
    "    query: a query record\n",
    "    n_processer: the number of processor to parallize the search job\n",
    "    \n",
    "    Return:\n",
    "    results: the list of all search results in all processors\n",
    "    \"\"\"\n",
    "    results = [read_to_list(climatePath)[0]]\n",
    "    pool = Pool(processes=n_processor)\n",
    "    datasets = rr_partition(data, n_processor)\n",
    "    for partition in datasets:\n",
    "        result = pool.apply_async(binary_search, args=(partition,query))\n",
    "        output = result.get()\n",
    "        results.append(output)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the output\n",
    "parallel_search_date(climateData,'2017-12-15',6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Write an algorithm to find the​ ``latitude``​, ​``longitude`` ​and ​``confidence`` ​when the surface\n",
    "temperature (°C) was between ​65 °C​ and​ 100 °C​. Justify your choice of the data partition\n",
    "technique and search technique you have used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:\n",
    "\n",
    "Round-robin partition + linear-search\n",
    "\n",
    "From the foregoing exploration we can see that the record of fire data is not sorted by surface temperature. THis time the query is a range, it is easy to consider range partition first, however, it appears if the partition range matches the query range, then there is totally no point search in the other partitions which is not a parallized case any more; on the other hand if the two ranges do not match, then there is no point using range patition.\n",
    "\n",
    "All the remaining partition method don't help with optimize the performance of parallel search, thus we still pick the simplest one -- round-robin which has the lowest time complexity and ensure load balance.\n",
    "\n",
    "As for the search method, binary search is not quite compatitable with a range query, which means the mechanism don't reduce search time complexity and can even lead to confusing output.Thus this time we pick just the linear seach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here starts task1 part2\n",
    "def linear_seach(data,key):\n",
    "    \"\"\"\n",
    "    Perform linear search on given dataset\n",
    "    \n",
    "    Parameters:\n",
    "    dat: the dataset to be searched\n",
    "    key: the key(can be a range) used for searching\n",
    "    \n",
    "    Return:\n",
    "    result: a tuple containing the index of the matched record and the query result\n",
    "    \"\"\"\n",
    "    position = -1\n",
    "    found = None\n",
    "    result = []\n",
    "    for record in data:\n",
    "        if int(record[-1]) in range(key[0],key[1]):\n",
    "            found = record[:2] + [record[-3]]\n",
    "            position = data.index(record)\n",
    "            result.append(found)\n",
    "    return result\n",
    "\n",
    "def parallel_search_temperature(data,query,n_processor):\n",
    "    \"\"\"\n",
    "    A method doing parallel search on a given dataset ,\n",
    "    when given a search clue like a single key or a range for certain column value\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be searched, which is a list\n",
    "    query: a query record\n",
    "    n_processer: the number of processor to parallize the search job\n",
    "    \n",
    "    Return:\n",
    "    results: the list of all search results in all processors\n",
    "    \"\"\"\n",
    "    results = [['Latitude','Longitude','Confidence']]\n",
    "    pool = Pool(processes=n_processor)\n",
    "    datasets = rr_partition(data, n_processor)\n",
    "    for partition in datasets:\n",
    "        result = pool.apply_async(linear_seach, args=(partition,query))\n",
    "        output = result.get()\n",
    "        results += output\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test function\n",
    "parallel_search_temperature(fireData,[65,100],2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Parallel Join\n",
    "### 1.\n",
    "Write an algorithm to find `surface temperature (°C)`, `air temperature (°C)`, `relative\n",
    "humidity` and `maximum wind speed`.Justify your choice of the data partition technique\n",
    "and join technique you have used.\n",
    "\n",
    "**Justification**:\n",
    "\n",
    "Round-Robin Data Partitioning + Divide and Droadcast-Based Parallel Join Algorithm (Hash-Based Join Algorithm).\n",
    "\n",
    "The number of fire records for each month is different. The total of records is 2668 but most of the records were collected on April (1434 recored) and May (673 records). Due to the serious uneven problem, round-robin data parititionning would distribute all the records evenly for each processor before parallel join process.\n",
    "\n",
    "Divide and broadcast-based parallel join algorithm would be preferable for the join process as the smaller table (climateData) would be broadcasted and the bigger one (fireData) would be divided evenly by the round-robin into several small parts. After this, each small part would be for probing and the climateData table would be for hashing, since we assume we only use 4 processors and each small part then is still larger than the climateDate (only 366 records). The reason why we skipped the disjoint partitioning-based join algorithm for this task is hash data partitioning and range data partitioning. Firstly, the hash-data partitioning would lead to large hash table because the hash-key would be the date attribute. Secondly, range data partitioning would cause serve skew problem (uneven problem) no matter how we distribute the range (based on seasons or just the month or day which can cut the whole table into two relatively even parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def H(r, index):\n",
    "    \"\"\"\n",
    "    We define a hash function 'H' that is used in the hashing process \n",
    "    by using the bulit-in MD5 Hash inside the hashlib.\n",
    "    \n",
    "    Arguments:\n",
    "    r -- a record where hashing will be applied on its attribute\n",
    "    index - the value in the index of the recored\n",
    "    \n",
    "    Return:\n",
    "    result -- the hash digest of the record r\n",
    "    \"\"\"\n",
    "    aString = r[index]\n",
    "    hash_value = hashlib.md5(aString.encode())\n",
    "    return (hash_value.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm\n",
    "    The join attribute is the date attribute in the input tables T1 & T2\n",
    "    \n",
    "    Arguments:\n",
    "    T1 & T2 --Tables to be joined\n",
    "    \n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    dic = {}\n",
    "    for s in T2:\n",
    "        s_key = H(s, 1) \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s)\n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "    for r in T1:\n",
    "        r_key = H(r, 6)\n",
    "        if r_key in dic:\n",
    "            d = dic[r_key]\n",
    "            for index, element in enumerate(d):\n",
    "                if r[6] == element[1]:\n",
    "                    result.append(\",\".join([r[7], element[2], element[3], element[5]])) \n",
    "    return result\n",
    "  \n",
    "# Divide and Broadcast-Based Parallel Join\n",
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DDP_join(T1, T2, n_processor):\n",
    "    \n",
    "    result = []\n",
    "    # use round robin data paritioninig to divide the larger table (fireData)\n",
    "    T1_subsets = rr_partition(T1, n_processor)\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    for t1 in T1_subsets:\n",
    "        result.append(pool.apply(HB_join, [t1, T2]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output\n",
    "n_processor = 4\n",
    "#for more clear output\n",
    "result = ['surf temp, air temp,realt humid, max windspeed'] + DDP_join(fireData, climateData, n_processor)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Write an algorithm to find `datetime`, `air temperature (°C)`, `surface temperature (°C)` and confidence when the `confidence ` is between 80 and 100. Justify your choice of the data\n",
    "partition technique and join technique you have used.\n",
    "\n",
    "**Justification**:\n",
    "\n",
    "Round-Robin Data Partitioning + Divide and Droadcast-Based Parallel Join Algorithm (Hash-Based Join Algorithm)\n",
    "\n",
    "Our group's justification for the date partitioning and join algorithm are the same to the task 2.1. For the search part, we simply use a linear search to perform the searching process serially based the join result. One reason is that the joint result is not sorted or not in order, which is not suitable for binary search. Another reason is performing parallel searching requires data partitioning again. Obviously, the complexity of parallel search would be increased if we partition the joint result again then perform linear search parallelly, compared to a simple serially linear search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join2(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm\n",
    "    The join attribute is the date attribute in the input tables T1 & T2\n",
    "    \n",
    "    Arguments:\n",
    "    T1 & T2 --Tables to be joined\n",
    "    \n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    dic = {}\n",
    "    #hash the climateDate table\n",
    "    for s in T2:\n",
    "        s_key = H(s, 1) \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s)\n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "            \n",
    "    # probe the small part of fireDate\n",
    "    for r in T1:\n",
    "        r_key = H(r, 6)\n",
    "        if r_key in dic:\n",
    "            d = dic[r_key]\n",
    "            for index, element in enumerate(d):\n",
    "                if r[6] == element[1]:\n",
    "                    result.append(\",\".join([r[3], element[2], r[7], r[5]])) \n",
    "    return result\n",
    "  \n",
    "# Divide and Droadcast-Based Parallel Join\n",
    "# Include this package for parallel processing\n",
    "import multiprocessing as mp\n",
    "\n",
    "def DDP_join2(T1, T2, n_processor):\n",
    "    \n",
    "    result = []\n",
    "    T1_subsets = rr_partition(T1, n_processor)\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # Apply local join for each processor\n",
    "    for t1 in T1_subsets:\n",
    "        result.append(pool.apply(HB_join2, [t1, T2]))\n",
    "     # Select records based on the condition where confidence in range [80, 100]\n",
    "    final_result = []\n",
    "    for index, element in enumerate(result):\n",
    "        for i in range(len(element)):\n",
    "            #seperate each column by comma in order to get the confidence \n",
    "            a = [x.strip() for x in element[i].split(',')]\n",
    "            b = int(a[3])\n",
    "            if (b >=80 and b <= 100):\n",
    "                final_result.append(element[i])   \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_processor = 4\n",
    "result = ['datetime, air temp, surf temp, confidence'] + DDP_join2(fireData, climateData, n_processor)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3  Parallel Sort\n",
    "Write an algorithm to sort fire data based on `surface temperature(°C)` in a ascending order. Justify your choice of the data partition technique and sorting technique you have used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:\n",
    "\n",
    "Merge sort(internal) + serial sort(based on sort-merge) + paralle-merge-all\n",
    "\n",
    "Since each record in the firedata is quite similar, which means the processing time for each of them can be regarded as the same, thus to make best use of all the processors in the parallized process, it is best to consider round-robin as a data partition method which is the one that can maintains best load-balancing as well as smallest complexity in both terms of time and space as a partition method.\n",
    "\n",
    "Here I decide to pick `merge sort` as the internal sorting algorithm. For `merge sort` is an identical comparison-based sorting algorithm whose time complexity is O(nlgn) while quick-sort is just quick in implementation but the worst case can be n^2. And both of the two sorting algorithms has the same time complexity since they all use divide and conquer and are divided in a binary way.\n",
    "\n",
    "Thus in term parallel sorting I am using the merge-sort as the internal sorting method, and the `merge-all` as the technique to merge the sorted list of all processors since the dataset is relatively small thus the burden of the final processor doing the merge-all won't be too large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is the internal sorting method --'mergesort'\n",
    "def merge(a,b):\n",
    "    \"\"\" Function to merge two arrays which has length shorter equal 1 in a sorted way\n",
    "    \"\"\"\n",
    "    c = []\n",
    "    while len(a) != 0 and len(b) != 0:\n",
    "        if int(a[0][-1]) < int(b[0][-1]):\n",
    "            c.append(a[0])\n",
    "            a.remove(a[0])\n",
    "        else:\n",
    "            c.append(b[0])\n",
    "            b.remove(b[0])\n",
    "    if len(a) == 0:\n",
    "        c += b\n",
    "    else:\n",
    "        c += a\n",
    "    return c\n",
    "\n",
    "def mergesort(x):\n",
    "    \"\"\" Function to sort an array using merge sort algorithm \"\"\"\n",
    "    if len(x) == 0 or len(x) == 1:\n",
    "        return x\n",
    "    else:\n",
    "        middle = len(x)//2\n",
    "        a = mergesort(x[:middle])\n",
    "        b = mergesort(x[middle:])\n",
    "        return merge(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first look at 'k-way merging algorithm' that will be used \n",
    "# to merge sub-record sets in our external sorting algorithm.\n",
    "import sys\n",
    "\n",
    "# Find the smallest record\n",
    "def find_min(records):    \n",
    "    \"\"\" \n",
    "    Find the smallest record\n",
    "    \n",
    "    Arguments:\n",
    "    records -- the input record set\n",
    "\n",
    "    Return:\n",
    "    result -- the smallest record's index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        m = int(records[0][-1])# records = [[],[],[]]\n",
    "    except:\n",
    "        print(records)\n",
    "    index = 0\n",
    "    for i in range(len(records)):\n",
    "        if int(records[i][-1]) < m:  \n",
    "            index = i\n",
    "            m = int(records[i][-1])\n",
    "    return index\n",
    "\n",
    "def k_way_merge(record_sets):#records_sets = [[[r1],[r2]],[[r3],[r4]],[[r5],[r6]]]\n",
    "    \"\"\" \n",
    "    K-way merging algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    record_sets -- the set of mulitple sorted sub-record sets\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted and merged record set\n",
    "    \"\"\"\n",
    "    \n",
    "    # indexes will keep the indexes of sorted records in the given buffers\n",
    "    indexes = []\n",
    "    for x in record_sets:\n",
    "        indexes.append(0) # initialisation with 0\n",
    "\n",
    "    # final result will be stored in this variable\n",
    "    result = []  \n",
    "    \n",
    "    # the merging unit (i.e. # of the given buffers)\n",
    "    sub = []\n",
    "    \n",
    "    while(True):\n",
    "        sub = [] # initialise the merging unit\n",
    "        \n",
    "        # This loop gets the current position of every buffer\n",
    "        for i in range(len(record_sets)):\n",
    "            if(indexes[i] >= len(record_sets[i])):\n",
    "                sub.append([sys.maxsize])\n",
    "            else:\n",
    "                sub.append(record_sets[i][indexes[i]])  \n",
    "                \n",
    "        # find the smallest record \n",
    "        smallest = find_min(sub)\n",
    "        # if we only have sys.maxsize on the tuple, we reached the end of every record set\n",
    "        if(sub[smallest] == [sys.maxsize]):\n",
    "            break\n",
    "\n",
    "        # This record is the next on the merged list\n",
    "        result.append(record_sets[smallest][indexes[smallest]])\n",
    "        indexes[smallest] +=1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The serial sorting method\n",
    "def serial_sorting(dataset, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a serial external sorting method based on sort-merge\n",
    "    The buffer size determines the size of eac sub-record set\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- the entire record set to be sorted\n",
    "    buffer_size -- the buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the sorted record set\n",
    "    \"\"\"\n",
    "    \n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # --- Sort Phase ---\n",
    "    sorted_set = []\n",
    "    \n",
    "    # Read buffer_size pages at a time into memory and\n",
    "    # sort them, and write out a sub-record set (i.e. variable: subset)\n",
    "    start_pos = 0\n",
    "    N = len(dataset)\n",
    "    while True:\n",
    "        if ((N - start_pos) > buffer_size):\n",
    "            # read B-records from the input, where B = buffer_size\n",
    "            subset = dataset[start_pos:start_pos + buffer_size] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = mergesort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            start_pos += buffer_size\n",
    "        else:\n",
    "            # read the last B-records from the input, where B is less than buffer_size\n",
    "            subset = dataset[start_pos:] \n",
    "            # sort the subset (using qucksort defined above)\n",
    "            sorted_subset = mergesort(subset) \n",
    "            sorted_set.append(sorted_subset)\n",
    "            break\n",
    "    \n",
    "    # --- Merge Phase ---\n",
    "    merge_buffer_size = buffer_size - 1\n",
    "    dataset = sorted_set\n",
    "    while True:\n",
    "        merged_set = []\n",
    "\n",
    "        N = len(dataset)\n",
    "        start_pos = 0\n",
    "        while True:\n",
    "            if ((N - start_pos) > merge_buffer_size): \n",
    "                # read C-record sets from the merged record sets, where C = merge_buffer_size\n",
    "                subset = dataset[start_pos:start_pos + merge_buffer_size]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                start_pos += merge_buffer_size\n",
    "            else:\n",
    "                # read C-record sets from the merged sets, where C is less than merge_buffer_size\n",
    "                subset = dataset[start_pos:]\n",
    "                merged_set.append(k_way_merge(subset)) # merge lists in subset\n",
    "                break\n",
    "\n",
    "        dataset = merged_set\n",
    "        if (len(dataset) <= 1): # if the size of merged record set is 1, then stop \n",
    "            result = merged_set\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_merge_all_sorting(dataset, n_processor, buffer_size):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge-all sorting method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be sorted\n",
    "    n_processor -- number of parallel processors\n",
    "    buffer_size -- buffer size determining the size of each sub-record set\n",
    "\n",
    "    Return:\n",
    "    result -- the merged record set\n",
    "    \"\"\"\n",
    "    if (buffer_size <= 2):\n",
    "        print(\"Error: buffer size should be greater than 2\")\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Pre-requisite: Perform data partitioning using round-robin partitioning\n",
    "    subsets = rr_partition(dataset, n_processor)\n",
    "    \n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Sort phase -----\n",
    "    sorted_set = []\n",
    "    for s in subsets:\n",
    "        # call the serial_sorting method above\n",
    "        sorted_set.append(*pool.apply(serial_sorting, [s, buffer_size]))#unpack all elements in the list to add them as single elment in the outer list\n",
    "    pool.close()\n",
    "    \n",
    "    # ---- Final merge phase ----\n",
    "    result = k_way_merge(sorted_set)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output of parallel sort\n",
    "parallel_merge_all_sorting(fireData,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Parallel Group-By\n",
    "### 1.\n",
    "\n",
    "Write an algorithm to get the number of fire in each day. You are required to only display total number of fire and the date in the output. Justify your choice of the data partition technique if any.\n",
    "\n",
    "**Justification**:\n",
    "\n",
    "Round Robin Data Partitioning + Merge-All Group By\n",
    "\n",
    "The reason why we use round robin data partitioning for this task is the same to task2.1 and task 2.2. That is good load balance (even distribution). Hash data partitioning leads to large hash table due to the date (hash key) and range data partitioning causes uneven problem (severe skew issue). \n",
    "\n",
    "For the group by algorithm, we use traditional group by method (merge all group by). This is because the group by attribute is date and redistributing local aggregation results based on date does not change a lot. If we assume four processors would be used, then the parallel time spent on redistribution would be around 25% of the time spent by one processor who performs this redistribution serially. But after that, the global aggregation still requires each processor to do the aggregation parallelly and this would take around the same time to access the redistributed data set. The above explanation about redistribution is based on the current data (fireData), which is not a large data set compared to other data sets in reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby(dataset, index_groupby):\n",
    "    \"\"\"\n",
    "     Peform a local groupby method\n",
    "     \n",
    "     Arguments:\n",
    "     dataset -- entire record set to be merged\n",
    "     index_groupby -- the index of attribute that would be used to group by\n",
    "     \n",
    "     Return:\n",
    "     return -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "  \n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[index_groupby]\n",
    "        if key not in dict:\n",
    "            dict[key] = 0\n",
    "        # the dict[key] would be used as counter\n",
    "        dict[key] += 1\n",
    "    \n",
    "    return dict\n",
    "\n",
    "def parallel_merge_all_groupby(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    # Use round robin date partition to divide the dataset \n",
    "    subsets = rr_partition(fireData,3)\n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(subsets)\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in subsets:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby, [s, 6])) #[{'date1': sum},{'date2': sum},{'date1': sum}.....]\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----e\n",
    "    result = {} #global result {'date1': sum, 'date2':sum}\n",
    "    for index, element in enumerate(local_result):\n",
    "        for key, val in element.items():\n",
    "            if key not in result:\n",
    "                result[key] = 0\n",
    "            result[key] += val    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_merge_all_groupby (fireData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Parallel Group-By Write an algorithm to find the **average** `surface temperature (°C)` for each day . You are required to only display average surface temperature (°C) and the date in the output. Justify your choice of the data partition technique if any.\n",
    "\n",
    "**Justification**:\n",
    "\n",
    "Round Robin Data Partitioning + Merge-All Group By\n",
    "\n",
    "The reasons for round robin data partionining and merge-all group by being used here are the same to the above one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_groupby2(dataset, index_groupby, index_target):\n",
    "    \"\"\"\n",
    "    Peform a local groupby method\n",
    "    \n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "    index_groupby -- the index of attribute that would be used to group by\n",
    "    index_target -- the index of attribute whose values would be used to sum\n",
    "    \n",
    "    Return:\n",
    "    return -- the aggregated record set according to the group_by attribute index\n",
    "    \"\"\"\n",
    "  \n",
    "    dict = {}\n",
    "    for index, record in enumerate(dataset):\n",
    "        key = record[index_groupby]\n",
    "        #Get the target value from the record\n",
    "        val = int(record[index_target])\n",
    "        if key not in dict:\n",
    "            dict[key] = [0,0]\n",
    "        dict[key][0] += val\n",
    "        dict[key][1] += 1\n",
    "    return dict #{'station': [sum, counter],'station': [sum, counter]}\n",
    "\n",
    "\n",
    "def parallel_merge_all_groupby2(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    final_result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    # Use round robin date partition to divide the dataset \n",
    "    subsets = rr_partition(dataset, 4)\n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(subsets)\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in subsets:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby2, [s, 6, 7])) #[{'date1': [sum, counter]},{'date2': [sum, counter]},{'date1': [sum, counter]}.....]\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    result = {} #global result #[{'date1': [sum, counter]},{'date2': [sum, counter]}.....]\n",
    "    for index, element in enumerate(local_result):\n",
    "        for key, val in element.items():\n",
    "            if key not in result:\n",
    "                result[key]= [0,0]\n",
    "            result[key][0] += val[0]\n",
    "            result[key][1] += val[1]\n",
    "\n",
    "    #----Final Avg Calculation-------\n",
    "    final_result = {} #final avg result {'date1': avg, 'date2': avg}\n",
    "    for key, val in result.items():\n",
    "        final_result[key] = val[0] / val[1]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_merge_all_groupby2(fireData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Parallel Group-By Join\n",
    "Write an algorithm to find the average `surface temperature (°C)` for each weather station.You are required to only display average surface temperature (°C) and the station in the\n",
    "output. Justify your choice of the data partition and join technique.\n",
    "\n",
    "**Justification**:\n",
    "\n",
    "Round-Robin Data Partitioning + Divide and Droadcast-Based Parallel Join Algorithm (Hash-Based Join Algorithm) + Merge-All Group By\n",
    "\n",
    "The data parition technique is the round-robin data partitioning here considering the better load balance, which is the same to the task2's explanation. \n",
    "The reason for merge-all group by technique for this task is the same to task 4.1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HB_join3(T1, T2):\n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm\n",
    "    The join attribute is the date attribute in the input tables T1 & T2\n",
    "    \n",
    "    Arguments:\n",
    "    T1 & T2 --Tables to be joined\n",
    "    \n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    dic = {}\n",
    "    for s in T2:\n",
    "        s_key = H(s, 1) \n",
    "        if s_key in dic:\n",
    "            dic[s_key].append(s)\n",
    "        else:\n",
    "            dic[s_key] = [s]\n",
    "    for r in T1:\n",
    "        r_key = H(r, 6)\n",
    "        if r_key in dic:\n",
    "            d = dic[r_key]\n",
    "            for index, element in enumerate(d):\n",
    "                if r[6] == element[1]:\n",
    "                    result.append(\",\".join([r[7], element[0]]))\n",
    "    return result\n",
    "  \n",
    "import multiprocessing as mp\n",
    "    \n",
    "def DDP_join3(T1, T2, n_processor):\n",
    "    \n",
    "    result = []\n",
    "    # Use round robin date partition to divide the T1 (larger table-fireData) \n",
    "    T1_subsets = rr_partition(T1, n_processor)\n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "    # Perform hash-based join parallely \n",
    "    for t1 in T1_subsets:\n",
    "        result.append(pool.apply(HB_join3, [t1, T2]))\n",
    "    return result\n",
    "    \n",
    "    # Apply local join for each processor\n",
    "    for index, t1 in enumerate(T1_subsets):\n",
    "        result.append(pool.apply(HB_join, [t1, T2_subsets[index]]))    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_merge_all_groupby3(dataset):\n",
    "    \"\"\"\n",
    "    Perform a parallel merge_all groupby method\n",
    "\n",
    "    Arguments:\n",
    "    dataset -- entire record set to be merged\n",
    "\n",
    "    Return:\n",
    "    final_result -- the aggregated record dictionary according to the group_by attribute index\n",
    "    \"\"\"\n",
    "     \n",
    "    # Use round robin date partition to divide the dataset \n",
    "    subsets =  rr_partition(dataset, 4)\n",
    "    # Define the number of parallel processors: the number of sub-datasets.\n",
    "    n_processor = len(subsets)\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    pool = mp.Pool(processes = n_processor)\n",
    "\n",
    "    # ----- Local aggregation step -----\n",
    "    local_result = []\n",
    "    for s in subsets:\n",
    "        # call the local aggregation method\n",
    "        local_result.append(pool.apply(local_groupby2, [s, 1, 0])) #[{'station1': [sum, counter]},{'station2': [sum, counter]},{'station1': [sum, counter]}.....]\n",
    "    pool.close()\n",
    "\n",
    "    # ---- Global aggregation step ----\n",
    "    result = {} #global result {'station1': [sum, counter], 'station2': [sum, counter]}\n",
    "    for index, element in enumerate(local_result):\n",
    "        for key, val in element.items():\n",
    "            if key not in result:\n",
    "                result[key]= [0,0]\n",
    "            result[key][0] += val[0]\n",
    "            result[key][1] += val[1]\n",
    "\n",
    "    #----Final Avg Calculation-------\n",
    "    final_result = {} #final avg result {'station1': avg, 'station2':avg}\n",
    "    for key, val in result.items():\n",
    "         final_result[key] = val[0] / val[1]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_join_result(join_result):\n",
    "    \"\"\"\n",
    "    Format the join result into lists of list\n",
    "    \n",
    "    Arguments:\n",
    "    join_result --the result would to be formated\n",
    "    \n",
    "    Return:\n",
    "    result -- the formated result\n",
    "    \"\"\"\n",
    "    formated_result = []\n",
    "    count = 0\n",
    "    for s in join_result:\n",
    "        for r in s:\n",
    "            a = [x.strip() for x in r.split(',')]\n",
    "            b = int(a[0])\n",
    "            formated_result.append([b, a[1]])\n",
    "      \n",
    "    return formated_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the join function \n",
    "join_result =  DDP_join3(fireData, climateData, 2)\n",
    "\n",
    "# Format the join result according to \"lists of list\" format\n",
    "formated_join_result = format_join_result(join_result)\n",
    "\n",
    "#Perform the parallel merge all groupby\n",
    "parallel_merge_all_groupby3(formated_join_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:5148]",
   "language": "python",
   "name": "conda-env-5148-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
