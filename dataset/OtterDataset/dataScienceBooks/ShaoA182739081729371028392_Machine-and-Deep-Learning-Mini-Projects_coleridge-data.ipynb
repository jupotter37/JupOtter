{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook illustrates how to use Masked Language Modeling for this competition.\n\nObservation: most of the dataset names consist of only words with uppercased-first-letter and some stopwords like `on`, `in`, `and` (e.g. `Early Childhood Longitudinal Study`, `Trends in International Mathematics and Science Study`). \n\nThus, one approach to find the datasets is: \n- Locate all the sequences of capitalized words (these sequences may contain some stopwords), \n- Replace each sequence with one of 2 special symbols (e.g. `$` and `#`), implying if that sequence represents a dataset name or not.\n- Have the model learn the MLM task.\n- Distil Bert -> BERT Model -> DistilRoberta -> ROBERTA","metadata":{"id":"x8D1ExGr72it"}},{"cell_type":"markdown","source":"The code below shows how to train a model for that purpose with the help of the `huggingface`.","metadata":{"id":"2BEYjgi_72iv"}},{"cell_type":"markdown","source":"# Install packages","metadata":{"id":"YgButBqP72ix"}},{"cell_type":"code","source":"!pip install fsspec==0.9.0","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:23:41.453210Z","iopub.execute_input":"2021-06-22T19:23:41.453576Z","iopub.status.idle":"2021-06-22T19:23:51.518701Z","shell.execute_reply.started":"2021-06-22T19:23:41.453540Z","shell.execute_reply":"2021-06-22T19:23:51.517838Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fsspec==0.9.0\n  Downloading fsspec-0.9.0-py3-none-any.whl (107 kB)\n\u001b[K     |████████████████████████████████| 107 kB 420 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from fsspec==0.9.0) (3.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->fsspec==0.9.0) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->fsspec==0.9.0) (3.7.4.3)\nInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 0.8.7\n    Uninstalling fsspec-0.8.7:\n      Successfully uninstalled fsspec-0.8.7\nSuccessfully installed fsspec-0.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import networkx as nx\n!pip install datasets --no-index --find-links=../input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"id":"iwNAU1ne72iy","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:23:51.520146Z","iopub.execute_input":"2021-06-22T19:23:51.520452Z","iopub.status.idle":"2021-06-22T19:24:23.342999Z","shell.execute_reply.started":"2021-06-22T19:23:51.520419Z","shell.execute_reply":"2021-06-22T19:24:23.341783Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Looking in links: ../input/coleridge-packages/packages/datasets\nProcessing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\nRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (0.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: tqdm, xxhash, huggingface-hub, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.59.0\n    Uninstalling tqdm-4.59.0:\n      Successfully uninstalled tqdm-4.59.0\nSuccessfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\nProcessing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nProcessing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\nInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.10.2\n    Uninstalling tokenizers-0.10.2:\n      Successfully uninstalled tokenizers-0.10.2\nSuccessfully installed tokenizers-0.10.1\nProcessing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.5.1\n    Uninstalling transformers-4.5.1:\n      Successfully uninstalled transformers-4.5.1\nSuccessfully installed transformers-4.5.0.dev0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{"id":"ViVZQibj72iz"}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nTFAutoModel, AutoConfig\n\nimport transformers\nimport nltk\nimport pickle\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\n  \nsns.set()\nrandom.seed(123)\nnp.random.seed(456)","metadata":{"id":"nUslQ2e272iz","outputId":"a13ae57f-809e-4562-9d57-86b135d5da3a","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:23.347241Z","iopub.execute_input":"2021-06-22T19:24:23.347579Z","iopub.status.idle":"2021-06-22T19:24:32.358613Z","shell.execute_reply.started":"2021-06-22T19:24:23.347544Z","shell.execute_reply":"2021-06-22T19:24:32.357616Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"}]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_it_all()","metadata":{"id":"v3zofVj2WaMQ","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.360093Z","iopub.execute_input":"2021-06-22T19:24:32.360359Z","iopub.status.idle":"2021-06-22T19:24:32.365457Z","shell.execute_reply.started":"2021-06-22T19:24:32.360333Z","shell.execute_reply":"2021-06-22T19:24:32.364480Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = \"roberta-large\"\nLOAD_FROM_PREV = False\nPREPROCESSED_PATH = './'\nSAVE_PATH = './'\nMAX_LEN = 512 # pad values up to 512.\nMAX_LBL = 60 # Max in lbl = 60# Longer than 300 is typically spanish or random text\nOVERLAP = 20\ntrain_corpus = None\nval_corpus = None\n\nDATASET_SYMBOL = '$' # this symbol represents a dataset name\nNONDATA_SYMBOL = '#' # this symbol represents a non-dataset name","metadata":{"id":"S0m9K2AT72i0","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.367085Z","iopub.execute_input":"2021-06-22T19:24:32.367745Z","iopub.status.idle":"2021-06-22T19:24:32.378967Z","shell.execute_reply.started":"2021-06-22T19:24:32.367701Z","shell.execute_reply":"2021-06-22T19:24:32.377954Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def freeze(layers):\n  for layer in layers:\n    for parameter in layer.parameters():\n      parameter.requires_grad = False\n","metadata":{"id":"Lc3VqMuhAMg2","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.380711Z","iopub.execute_input":"2021-06-22T19:24:32.381470Z","iopub.status.idle":"2021-06-22T19:24:32.395125Z","shell.execute_reply.started":"2021-06-22T19:24:32.381422Z","shell.execute_reply":"2021-06-22T19:24:32.393391Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{"id":"lKE6Xmbe72i0"}},{"cell_type":"markdown","source":"# Larger Sentences:\n- No duplicates of Sentences, just all tokens at once.\n- We have multi class labels, why not use them.","metadata":{"id":"VbtXX3edJ5Fe"}},{"cell_type":"code","source":"def merge_duplicates(csv):\n    titles = set(csv.pub_title)\n    df = {'pub_title': [], 'cleaned_label': []}\n    for title in tqdm(titles):\n        all_rows = csv[np.equal(csv.pub_title, title)]\n        all_labels = []\n        for row in all_rows.iterrows():\n            row = row[1]\n            labels = row.cleaned_label.split(\"|\")\n            all_labels += labels\n            pub_title = row.pub_title\n        all_labels = set(all_labels)\n        df['pub_title'] += [pub_title]\n        df['cleaned_label'] += ['|'.join(string for string in all_labels)]\n    return pd.DataFrame(df)\n        \n            ","metadata":{"id":"KecS9mzZ72i0","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.396393Z","iopub.execute_input":"2021-06-22T19:24:32.396823Z","iopub.status.idle":"2021-06-22T19:24:32.405306Z","shell.execute_reply.started":"2021-06-22T19:24:32.396783Z","shell.execute_reply":"2021-06-22T19:24:32.404329Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_all_labels(csv):\n    labels = []\n    for row in csv.iterrows():\n        row = row[1]\n        labels += row.cleaned_label.split(\"|\")\n    return labels\ndef get_labels(csv):\n    labels = []\n    for row in csv.iterrows():\n        row = row[1]\n        row_label = row.cleaned_label.split(\"|\")\n        labels += [row_label]\n    return labels\ndef create_undirected_graph(csv):\n    # Creates an Undirected Graph from the dataset\n    labels = get_labels(csv)# (N, )\n    unique_labels = compute_all_labels(csv)\n    datasets_to_pub = {label: [] for label in unique_labels}\n    \n    pub_titles = csv.pub_title # (N, )\n    # Create Mini Graph to create bigger graph\n    for i in tqdm(range(len(pub_titles))):\n        ex_label = labels[i]\n        ex_title = pub_titles[i]\n        for label in ex_label:\n            datasets_to_pub[label] += [ex_title]\n\n    graph = nx.DiGraph()\n    graph.add_nodes_from(pub_titles)\n    for label in tqdm(datasets_to_pub):\n        # Double Loop\n        titles = datasets_to_pub[label]\n        for pub1_idx in range(len(titles)):\n            for pub2_idx in range(pub1_idx, len(titles)):\n                graph.add_edge(titles[pub1_idx], titles[pub2_idx])\n    # Create Undirected Graph\n    graph = graph.to_undirected()\n    # compute Strongly Connected Components\n    components = nx.strongly_connected_components(graph.to_directed())\n    all_components = []\n    for comp in components:\n        all_components += [comp]\n    return all_components\ndef grab_from_components(csv, components):\n    # Restores a CSV from the Components\n    return csv[csv.pub_title.isin(components)]\ndef create_train_test_splits(csv, components):\n    # The Datasets are too intermingled to properly split, so we just chuck the two largest(4000 + 8000 = 12000) into train,\n    # the 3rd largest(1000) into test, and split across the tiny ones(17 components, accumulated to 100 nodes)\n    component_to_length = []\n    for comp in components:\n        component_to_length += [(comp, len(comp))]\n    # Sort\n    key_fn = lambda x: x[1]\n    component_to_length = sorted(component_to_length, key = key_fn, reverse = True, )\n    \n    train_guarenteed = [t[0] for t in component_to_length[:3]]\n    test_guarenteed = []\n    to_split = [t[0] for t in component_to_length[3:]]\n    splitter = KFold(shuffle = True, random_state = 42)\n    \n    FOLDS = []\n    for train_idx, test_idx in splitter.split(to_split):\n        train = train_guarenteed\n        test = test_guarenteed\n        for idx in train_idx:\n            train += [to_split[idx]]\n        for idx in test_idx:    \n            test += [to_split[idx]]\n        # Convert List of Sets to Single Set\n        train_set = set()\n        test_set = set()\n        \n        for comp in train:\n            train_set.update(comp)\n        for comp in test:\n            test_set.update(comp)\n        train_set = grab_from_components(csv, train_set)\n        test_set = grab_from_components(csv, test_set)\n        FOLDS += [(train_set, test_set)]\n    return FOLDS\ndef get_splits(csv):\n    # function that automates everything for you.\n    print(\"Generating SCCs...................\")\n    all_components = create_undirected_graph(csv)\n    print('generatings splits..........')\n    return create_train_test_splits(csv, all_components)","metadata":{"id":"1v_N_XDu72i1","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.407750Z","iopub.execute_input":"2021-06-22T19:24:32.408060Z","iopub.status.idle":"2021-06-22T19:24:32.428204Z","shell.execute_reply.started":"2021-06-22T19:24:32.408032Z","shell.execute_reply":"2021-06-22T19:24:32.427123Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if not LOAD_FROM_PREV:\n  \n  import copy\n  # train\n  train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\n  paper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train/'\n\n  train = pd.read_csv(train_path)\n  # Group by publication, training labels should have the same form as expected output.\n  train = train.groupby('Id').agg({\n      'pub_title': 'first',\n      'dataset_title': '|'.join,\n      'dataset_label': '|'.join,\n      'cleaned_label': '|'.join\n  }).reset_index()    \n  del train['dataset_label']\n  del train['dataset_title']\n  del train['Id']\n  train = merge_duplicates(train)\n  ORIG_FOLDS = get_splits(train)\n  FOLDS = copy.deepcopy(ORIG_FOLDS)\n  print('train size: ', len(train))\n\n  all_datasets = pd.read_csv(train_path)\n  # Group by publication, training labels should have the same form as expected output.\n  all_datasets = all_datasets.groupby('Id').agg({\n      'pub_title': 'first',\n      'dataset_title': '|'.join,\n      'dataset_label': '|'.join,\n      'cleaned_label': '|'.join\n  }).reset_index() \n\n  # Post Process Fold\n  NEW_FOLDS = []\n  for train, test in FOLDS:\n      train = train.sort_values('pub_title').reset_index(drop = True)\n      test = test.sort_values('pub_title').reset_index(drop = True)\n      train_dataset = all_datasets[np.isin(all_datasets.pub_title, train.pub_title)].sort_values('pub_title').reset_index(drop = True)\n      test_dataset = all_datasets[np.isin(all_datasets.pub_title, test.pub_title)].sort_values('pub_title').reset_index(drop = True)\n      \n      ## Add back in the ids\n      train['Id'] = train_dataset.Id\n      train['dataset_label'] = train_dataset.dataset_label\n      train['dataset_title'] = train_dataset.dataset_title\n      \n      test['Id'] = test_dataset.Id\n      test['dataset_label'] = test_dataset.dataset_label\n      test['dataset_title'] = test_dataset.dataset_title\n      \n      NEW_FOLDS += [(train, test)]\n      FOLDS = NEW_FOLDS","metadata":{"id":"Z7xXevVw72i4","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:24:32.430195Z","iopub.execute_input":"2021-06-22T19:24:32.430607Z","iopub.status.idle":"2021-06-22T19:29:32.256788Z","shell.execute_reply.started":"2021-06-22T19:24:32.430566Z","shell.execute_reply":"2021-06-22T19:29:32.255743Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=14271.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ead3dab5a443078a5772a57c2e0570"}},"metadata":{}},{"name":"stdout","text":"\nGenerating SCCs...................\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=14271.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27918c056de8417f98c4558c2ff76198"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c88def18410d490c9e1890413c6bf247"}},"metadata":{}},{"name":"stdout","text":"\ngeneratings splits..........\ntrain size:  14271\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n# Prepare data for train MLM","metadata":{"id":"BBescaHv72i7"}},{"cell_type":"markdown","source":"### Auxiliary functions","metadata":{"id":"vc9kg89m72i8"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, return_token_type_ids = True, return_attention_masks = True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T19:29:32.258147Z","iopub.execute_input":"2021-06-22T19:29:32.258465Z","iopub.status.idle":"2021-06-22T19:29:44.130970Z","shell.execute_reply.started":"2021-06-22T19:29:32.258434Z","shell.execute_reply":"2021-06-22T19:29:44.129797Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=482.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48bccaa442d5484c88694e2a4a574351"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd60082e0688458e8bba946eb755eb0b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5590f8c292364e91b6d9b93c4af187e8"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f148cef3484712aa0a028a1523212a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s).lower()).strip()\n    s = re.sub(' +', ' ', s)\n    count = 0\n    start_span = 0\n    spans_to_remove = []\n    for idx, letter in enumerate(s):\n        if letter == ' ':\n            continue\n        if letter.isdigit():\n            if count == 0:\n                start_span = idx\n            count += 1\n            \n        else:\n            if count > 10:\n                # 10 Numbers in a row, likely tabular data, remove the entire string\n                spans_to_remove += [(start_span, idx)]\n                #print(s[start_span:idx])\n            count = 0\n    # Remove the spans, starting from the back \n    spans_to_remove.reverse()\n    removed = False\n    for idx1, idx2 in spans_to_remove:\n        s = s[:idx1] + s[idx2:]\n        removed = True\n    \n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    sentences = sentences[1:-1]\n    length = MAX_LEN - 2\n    MAX_SENTS = 5000# Wayyyy more than the max anyways(200)\n    short_sentences = np.zeros((MAX_SENTS, length), dtype = np.int32)\n    cur_idx = 0\n    MIN_WORDS = 25\n    \n    words = sentences\n    if len(words) > length:\n        for p in range(0, len(words), length - OVERLAP):\n            new_words = words[p:p + length]\n            padded_words = np.ones((length), dtype = np.int32) * tokenizer.pad_token_id\n            if len(new_words) < MIN_WORDS:\n                continue\n            else:\n                padded_words[:len(new_words)] = new_words\n                short_sentences[cur_idx, :] = padded_words\n                cur_idx += 1\n\n    else:\n        padded_words = np.ones((length), dtype = np.int32) * tokenizer.pad_token_id\n        padded_words[:len(words)] = words\n        short_sentences[cur_idx, :] = padded_words\n        cur_idx += 1\n    short_sentences = short_sentences[:cur_idx]\n    \n    return short_sentences\n\ndef find_sublist(big_list, small_list):\n    \"\"\"\n    find all positions of $small_list in $big_list.\n    \"\"\"\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions\n\ndef jaccard_similarity_list(l1, l2):\n    \"\"\"\n    Return the Jaccard Similarity score of 2 lists.\n    \"\"\"\n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset'}\ndef find_negative_candidates(sentence, labels):\n    \"\"\"\n    Extract negative samples for Masked Dataset Modeling from a given $sentence.\n    A negative candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence. Lastly, the sequence must be quite different to any of the \n    ground truth labels (measured by Jaccard similarity).\n    \"\"\"\n    def candidate_qualified(words, labels):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 5 and \\\n               all(jaccard_similarity_list(words, label) < 0.75 for label in labels)\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1], labels):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1], labels):\n            candidates.append((phrase_start, phrase_end))\n    \n    return candidates","metadata":{"id":"9qRFbMjS72i8","execution":{"iopub.status.busy":"2021-06-22T19:34:07.398226Z","iopub.execute_input":"2021-06-22T19:34:07.398585Z","iopub.status.idle":"2021-06-22T19:34:07.422685Z","shell.execute_reply.started":"2021-06-22T19:34:07.398554Z","shell.execute_reply":"2021-06-22T19:34:07.421604Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"raw","source":"### Extract positive and negative samples","metadata":{"id":"uI67si0t72i8"}},{"cell_type":"code","source":"def load_corpus(train):    \n    corpus = []\n    count = 0\n    for paper_id, dataset_labels in tqdm(train[['Id', 'dataset_label']].itertuples(index=False)):\n        labels = [clean_paper_sentence(label) for label in dataset_labels.split('|')]\n        # Create a Full Dataset \n        \n        with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n        content = '. '.join(section['text'] for section in paper)\n        sentences = list(set([clean_paper_sentence(sentence) for sentence in [content]]))\n        sentences = tokenizer(sentences)['input_ids'][0]\n        \n        sentences = shorten_sentences(sentences) # make sentences short, Each of these sentences store roughly 512 tokens(Pad or truncate)\n        sentences = tokenizer.batch_decode(sentences)\n        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n        # FIND LABELS Per sentence \n        LABEL = []\n        SENTENCES = []\n        \n        ZERO_SENTENCES = []\n        ZERO_LABEL = []\n        for sentence in sentences:\n          lbl_sentence = []\n          for lbl in labels:\n            if lbl in sentence:\n              lbl_sentence += [lbl]\n          if len(lbl_sentence) == 0:\n            # check if negative sample\n            candidates = find_negative_candidates(sentence, labels)\n            ZERO_SENTENCES += [sentence]\n            ZERO_LABEL += [lbl_sentence]\n          else:\n            SENTENCES += [sentence]\n            LABEL += [lbl_sentence]\n        # Random Select 1.2 * sentences - Slightly more neg than pos(like in real dataset)\n        num_pos = int(len(SENTENCES) * 1.2)\n        try:\n            indices = np.random.randint(0, high = len(ZERO_SENTENCES) - 1, size = num_pos)\n            indices = list(set(indices.tolist()))\n            for IDX in indices:\n                SENTENCES += [ZERO_SENTENCES[IDX]]\n                LABEL += [ZERO_LABEL[IDX]]\n        except:\n            pass\n        NEW_SENTENCES = []\n        NEW_LABELS = []\n        for i in range(len(SENTENCES)):\n          lbl = LABEL[i]\n          sent = SENTENCES[i].split()\n          processed_lbl = []\n          for j, l in enumerate(lbl):\n            if j == len(lbl) - 1:\n              processed_lbl += l.split()\n            else:\n              processed_lbl += l.split() + ['|']\n          corpus += [(sent, processed_lbl)]\n          \n        count += 1\n        if count % 100 == 0:\n            print(count)\n    return corpus","metadata":{"id":"U61SpQtH72i9","execution":{"iopub.status.busy":"2021-06-22T19:34:08.319956Z","iopub.execute_input":"2021-06-22T19:34:08.320325Z","iopub.status.idle":"2021-06-22T19:34:08.335225Z","shell.execute_reply.started":"2021-06-22T19:34:08.320296Z","shell.execute_reply":"2021-06-22T19:34:08.334549Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Save data to a file","metadata":{"id":"H_Ai7Rfl72i9"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not LOAD_FROM_PREV:\n  VAL_LABELS =[]\n  FOLD_IDX = 0\n  FOLDS = NEW_FOLDS\n  FOLDS = FOLDS[FOLD_IDX]\n\n\n  train, val = FOLDS\n  # load corpus\n  if train_corpus is None:\n    train_corpus = load_corpus(train)\n  if val_corpus is None:\n    val_corpus = load_corpus(val)\n\n  with open(f'train_mlm.json', 'w') as f:\n      for idx in range(len(train_corpus)):\n          sentence, label = train_corpus[idx] \n          row_json = {'text':sentence, 'label': label}\n          json.dump(row_json, f)\n          f.write('\\n')\n  # Map Dataset to Selected Corpus\n  val_labels = []\n  with open(f'val_mlm.json', 'w') as f:\n      for idx in range(len(val_corpus)):\n          sentence, label = val_corpus[idx]\n          row_json = {'text':sentence, 'label': label}\n\n          json.dump(row_json, f)\n          f.write('\\n')\n\n  VAL_LABELS = val_labels","metadata":{"id":"u-feF5XG72i-","execution":{"iopub.status.busy":"2021-06-22T19:34:09.081521Z","iopub.execute_input":"2021-06-22T19:34:09.082034Z","iopub.status.idle":"2021-06-22T19:55:23.875555Z","shell.execute_reply.started":"2021-06-22T19:34:09.082003Z","shell.execute_reply":"2021-06-22T19:55:23.874547Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47aaa34023c9425aba7fbc6eebcb6b5e"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (102479 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\n4400\n4500\n4600\n4700\n4800\n4900\n5000\n5100\n5200\n5300\n5400\n5500\n5600\n5700\n5800\n5900\n6000\n6100\n6200\n6300\n6400\n6500\n6600\n6700\n6800\n6900\n7000\n7100\n7200\n7300\n7400\n7500\n7600\n7700\n7800\n7900\n8000\n8100\n8200\n8300\n8400\n8500\n8600\n8700\n8800\n8900\n9000\n9100\n9200\n9300\n9400\n9500\n9600\n9700\n9800\n9900\n10000\n10100\n10200\n10300\n10400\n10500\n10600\n10700\n10800\n10900\n11000\n11100\n11200\n11300\n11400\n11500\n11600\n11700\n11800\n11900\n12000\n12100\n12200\n12300\n12400\n12500\n12600\n12700\n12800\n12900\n13000\n13100\n13200\n13300\n13400\n13500\n13600\n13700\n13800\n13900\n14000\n14100\n14200\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd480b8914c4466f92fea18dd8b3030b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tune the Transformer","metadata":{"id":"pVoHjonq72i-"}},{"cell_type":"code","source":"if not LOAD_FROM_PREV:\n  datasets = load_dataset('json',\n              data_files={\n                  'train' : f'train_mlm.json'\n              },\n              keep_in_memory = True\n  )\n  val_dataset = load_dataset('json',\n    data_files = {\n        'test': f'val_mlm.json' \n    },\n    keep_in_memory = True\n  )\n  datasets[\"train\"][:5]","metadata":{"id":"owlAq9fp72i-","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:19.513679Z","iopub.execute_input":"2021-06-22T20:04:19.514031Z","iopub.status.idle":"2021-06-22T20:04:21.582169Z","shell.execute_reply.started":"2021-06-22T20:04:19.513999Z","shell.execute_reply":"2021-06-22T20:04:21.581087Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Augment the Dataset","metadata":{"id":"9BEQHJ4sWdMh"}},{"cell_type":"code","source":"def fix_ending_tokens(sentence, token_type_ids, att_mask):\n    # Find End Token\n    end_token = np.argmax(np.equal(sentence, tokenizer.eos_token_id).astype(np.int32))\n    # Replace this token with padding\n    sentence[end_token] = tokenizer.pad_token_id\n    # Find the first pad token\n    pad_token = np.argmax(np.equal(sentence, tokenizer.pad_token_id).astype(np.int32))\n    # Replace the first token with End\n    sentence[pad_token] = tokenizer.eos_token_id\n    att_mask[pad_token] = 1\n    # Rest is padding \n    sentence[pad_token + 1:] = tokenizer.pad_token_id\n    att_mask[pad_token + 1:] = 0\n\n\n    # CHECK IF LEN IS > MAXLEN(TRUNCATE)\n    if len(sentence) > MAX_LEN:\n        sentence = sentence[:MAX_LEN]\n        att_mask = att_mask[:MAX_LEN]\n        token_type_ids = token_type_ids[:MAX_LEN]\n        # check if last token is padding\n        if sentence[-1] == tokenizer.pad_token_id:\n            # Skip\n            # Nothing changes\n            pass\n        else:\n            # Replace Last token with End\n            sentence[-1] = tokenizer.eos_token_id\n            # Fix Att_mask\n            att_mask[-1] = 1\n\n    # CHECK IF LEN is < MAXLEN(PAD)\n    if len(sentence) < MAX_LEN:\n        padded_sentence = np.ones(MAX_LEN, dtype = sentence.dtype) * tokenizer.pad_token_id\n        padded_sentence[:len(sentence)] = sentence \n        sentence = padded_sentence\n        \n        padded_att_mask = np.zeros(MAX_LEN, dtype = att_mask.dtype) \n        padded_att_mask[:len(att_mask)] = att_mask\n        att_mask = padded_att_mask\n        \n        padded_token_type_ids = np.zeros(MAX_LEN, dtype = token_type_ids.dtype)\n        padded_token_type_ids[:len(token_type_ids)] = token_type_ids\n        token_type_ids = padded_token_type_ids\n    return sentence, token_type_ids, att_mask","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.583807Z","iopub.execute_input":"2021-06-22T20:04:21.584134Z","iopub.status.idle":"2021-06-22T20:04:21.594691Z","shell.execute_reply.started":"2021-06-22T20:04:21.584103Z","shell.execute_reply":"2021-06-22T20:04:21.593690Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize and collate data","metadata":{"id":"Y4q4bQx272i_"}},{"cell_type":"markdown","source":"BATCHES DON't WORK RN.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    \n    values = tokenizer([\" \".join(ex) for ex in examples['text']], return_attention_mask=True, return_token_type_ids=True)\n    input_ids = values['input_ids']\n    token_type_ids = values['token_type_ids']\n    attention_mask = values['attention_mask']\n    \n    new_input_ids = np.zeros((len(input_ids), MAX_LEN), dtype = np.int32)\n    new_token_type_ids = np.zeros((len(input_ids), MAX_LEN), dtype = np.int32)\n    new_attention_mask = np.zeros((len(input_ids), MAX_LEN), dtype = np.int32)\n    \n    for b in range(len(input_ids)):\n        i_id = np.array(input_ids[b])\n        tti = np.array(token_type_ids[b])\n        att_msk = np.array(attention_mask[b])\n        \n        i_id, tti, att_msk = fix_ending_tokens(i_id, tti, att_msk)\n        \n        new_input_ids[b, :] = i_id\n        new_token_type_ids[b, :] = tti\n        new_attention_mask[b, :] = att_msk\n        \n        \n    values['input_ids'] = new_input_ids.tolist()\n    values['token_type_ids'] = new_token_type_ids.tolist()\n    values['attention_mask'] = new_attention_mask.tolist()\n    \n    encoded = tokenizer([\" \".join(ex) for ex in examples['label']],  return_token_type_ids= False, return_attention_mask=False)\n    values['label'] = encoded['input_ids']\n    return values\nif not LOAD_FROM_PREV:\n  tokenized_train_dataset = datasets.map(tokenize_function, batched = True, num_proc = 1, remove_columns= ['text'])\n  tokenized_val_dataset = val_dataset.map(tokenize_function, batched = True, num_proc = 1, remove_columns = ['text'])","metadata":{"id":"finiK3Jm72i_","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.597065Z","iopub.execute_input":"2021-06-22T20:04:21.597376Z","iopub.status.idle":"2021-06-22T20:04:21.722092Z","shell.execute_reply.started":"2021-06-22T20:04:21.597347Z","shell.execute_reply":"2021-06-22T20:04:21.721078Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value, is_list=False):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _float_feature(value, is_list=False):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value, is_list=False):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n","metadata":{"id":"u8RpJf_dsMWB","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.723463Z","iopub.execute_input":"2021-06-22T20:04:21.723775Z","iopub.status.idle":"2021-06-22T20:04:21.733279Z","shell.execute_reply.started":"2021-06-22T20:04:21.723744Z","shell.execute_reply":"2021-06-22T20:04:21.732359Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def serialize_tokenized(attention_mask, input_ids, label, token_type_ids):\n    \"\"\"\n    Creates a tf.Example message ready to be written to a file from 4 features.\n\n    Args:\n        image (TBD): TBD\n        other: Either the image_id or the target inchi\n    \n    Returns:\n        A tf.Example Message ready to be written to file\n    \"\"\"\n    # Create a dictionary mapping the feature name to the \n    # tf.Example-compatible data type.\n    \n    # PAD THE VALUE\n    NUM_PAD = MAX_LEN - len(input_ids)\n    PAD_ID = tokenizer.pad_token_id\n    input_ids += [PAD_ID] * NUM_PAD\n    token_type_ids += [0] * NUM_PAD\n    attention_mask += [0] * NUM_PAD\n    NUM_LBL_PAD = MAX_LBL - len(label)\n    label += [PAD_ID] * NUM_LBL_PAD\n \n    feature = {\n        'attention_mask': _int64_feature(attention_mask, is_list = True),\n        'input_ids': _int64_feature(input_ids, is_list = True),\n        'label': _int64_feature(label, is_list = True),\n        'token_type_ids': _int64_feature(token_type_ids, is_list = True) \n    }\n    # Create a Features message using tf.train.Example.\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","metadata":{"id":"HtKLfdCjsNM4","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.734620Z","iopub.execute_input":"2021-06-22T20:04:21.735122Z","iopub.status.idle":"2021-06-22T20:04:21.749484Z","shell.execute_reply.started":"2021-06-22T20:04:21.735081Z","shell.execute_reply":"2021-06-22T20:04:21.748708Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_tfrecords(tokenized, save_path, shard_size = 100000):\n  try:\n    os.mkdir(f\"{PREPROCESSED_PATH}{save_path}\")\n  except:\n    pass\n  dataset_length = len(tokenized)\n  counts = []\n  for i in range((dataset_length // shard_size ) + 1):\n    start_idx = i * shard_size\n    end_idx = (i + 1) * shard_size\n    \n    file_name = f\"{PREPROCESSED_PATH}{save_path}{i}.tfrec\"\n    all_values = tokenized[start_idx: end_idx]\n    \n    attention_mask = all_values['attention_mask']\n    input_ids = all_values['input_ids']\n    label = all_values['label']\n    token_type_ids = all_values['token_type_ids']\n    with open(file_name, 'w') as file:\n      pass\n    with tf.io.TFRecordWriter(file_name) as writer:\n        count = 0\n        for idx in tqdm(range(len(input_ids))):\n          att_mask = attention_mask[idx]\n          i_id = input_ids[idx]\n          lbl = label[idx]\n          tti = token_type_ids[idx]\n            \n          ex = serialize_tokenized(att_mask, i_id, lbl, tti)\n          writer.write(ex)\n        print(count)\n  return counts","metadata":{"id":"KVQSigSZw_dw","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.750933Z","iopub.execute_input":"2021-06-22T20:04:21.751383Z","iopub.status.idle":"2021-06-22T20:04:21.762218Z","shell.execute_reply.started":"2021-06-22T20:04:21.751355Z","shell.execute_reply":"2021-06-22T20:04:21.761316Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"try:\n  os.mkdir(PREPROCESSED_PATH)\nexcept:\n  pass","metadata":{"id":"oAVyv7az0ccX","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-22T20:04:21.763348Z","iopub.execute_input":"2021-06-22T20:04:21.763857Z","iopub.status.idle":"2021-06-22T20:04:21.776394Z","shell.execute_reply.started":"2021-06-22T20:04:21.763824Z","shell.execute_reply":"2021-06-22T20:04:21.775712Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"TRAIN_COUNTS = write_tfrecords(tokenized_train_dataset['train'], 'train_tfrecords/')\nVAL_COUNTS = write_tfrecords(tokenized_val_dataset['test'], 'test_tfrecords/')","metadata":{"id":"wEl7xJiez70h","outputId":"2a610029-47e4-4a47-fe68-be0276f1bb6e","execution":{"iopub.status.busy":"2021-06-22T20:04:21.778020Z","iopub.execute_input":"2021-06-22T20:04:21.778409Z","iopub.status.idle":"2021-06-22T20:05:37.132590Z","shell.execute_reply.started":"2021-06-22T20:04:21.778381Z","shell.execute_reply":"2021-06-22T20:05:37.131566Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=56701.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebcfe852eed4114bbcd06418a64b15c"}},"metadata":{}},{"name":"stdout","text":"\n0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=167.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8406fd62285249f2860331333adfe439"}},"metadata":{}},{"name":"stdout","text":"\n0\n","output_type":"stream"}]}]}