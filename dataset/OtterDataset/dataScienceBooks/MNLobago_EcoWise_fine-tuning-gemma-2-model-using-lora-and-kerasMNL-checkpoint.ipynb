{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n",
    "8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n",
    "9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n",
    "10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "\n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gemma 2?\n",
    "\n",
    "Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n",
    "\n",
    "Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n",
    "* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n",
    "* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n",
    "* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n",
    "\n",
    "To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages\n",
    "\n",
    "We start by installing `keras-nlp` and `keras` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T14:13:38.123623Z",
     "iopub.status.busy": "2024-10-27T14:13:38.123157Z",
     "iopub.status.idle": "2024-10-27T14:14:21.265319Z",
     "shell.execute_reply": "2024-10-27T14:14:21.264120Z",
     "shell.execute_reply.started": "2024-10-27T14:13:38.123595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3\n",
    "!pip install -q -U kagglehub --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n",
    "\n",
    "Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:43.408556Z",
     "iopub.status.busy": "2024-10-27T14:17:43.407386Z",
     "iopub.status.idle": "2024-10-27T14:17:59.147958Z",
     "shell.execute_reply": "2024-10-27T14:17:59.146923Z",
     "shell.execute_reply.started": "2024-10-27T14:17:43.408514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 14:17:48.290333: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-27 14:17:48.290467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-27 14:17:48.438304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n",
    "os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "\n",
    "We use a `Config` class to group the information needed to control the fine-tuning process:\n",
    "* random seed \n",
    "* dataset path\n",
    "* preset - name of pretrained Gemma 2\n",
    "* sequence length - this is the maximum size of input sequence for training\n",
    "* batch size - size of the input batch in training, x 2 as two GPUs\n",
    "* lora rank - rank for LoRA, higher means more trainable parameters \n",
    "* learning rate used in the train\n",
    "* epochs - number of epochs for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:59.150496Z",
     "iopub.status.busy": "2024-10-27T14:17:59.149796Z",
     "iopub.status.idle": "2024-10-27T14:17:59.156214Z",
     "shell.execute_reply": "2024-10-27T14:17:59.155284Z",
     "shell.execute_reply.started": "2024-10-27T14:17:59.150451Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/kaggle/input/carbony-preprocessed\"\n",
    "    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training\n",
    "    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n",
    "    learning_rate=8e-5 # learning rate used in train\n",
    "    epochs = 1 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:59.157630Z",
     "iopub.status.busy": "2024-10-27T14:17:59.157314Z",
     "iopub.status.idle": "2024-10-27T14:17:59.167091Z",
     "shell.execute_reply": "2024-10-27T14:17:59.166295Z",
     "shell.execute_reply.started": "2024-10-27T14:17:59.157605Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "\n",
    "We load the data we will use for fine-tunining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:05.780320Z",
     "iopub.status.busy": "2024-10-27T14:18:05.779917Z",
     "iopub.status.idle": "2024-10-27T14:18:05.966861Z",
     "shell.execute_reply": "2024-10-27T14:18:05.966083Z",
     "shell.execute_reply.started": "2024-10-27T14:18:05.780291Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open(f\"{Config.dataset_path}/high_volume_carbon_footprint_qa.json\") as file:\n",
    "    content = file.read()\n",
    "    try:\n",
    "        features_list = json.loads(content)\n",
    "        for features in features_list:\n",
    "            # Check if 'question' and 'answer' keys exist\n",
    "            if 'question' in features and 'answer' in features:\n",
    "                # Update the template to use the correct keys\n",
    "                template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
    "                data.append(template.format(**features))\n",
    "            else:\n",
    "                print(f\"Missing keys in features: {features}\")\n",
    "        #data = data[:2000]  # Limit to 1000 examples\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Now you can use the 'data' list as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:08.478995Z",
     "iopub.status.busy": "2024-10-27T14:18:08.478616Z",
     "iopub.status.idle": "2024-10-27T14:18:08.485776Z",
     "shell.execute_reply": "2024-10-27T14:18:08.484831Z",
     "shell.execute_reply.started": "2024-10-27T14:18:08.478966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45863"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:10.677608Z",
     "iopub.status.busy": "2024-10-27T14:18:10.676921Z",
     "iopub.status.idle": "2024-10-27T14:18:10.683541Z",
     "shell.execute_reply": "2024-10-27T14:18:10.682601Z",
     "shell.execute_reply.started": "2024-10-27T14:18:10.677574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question:\\nWhat actions can local governments take to promote carbon emissions footprint reductions?\\n\\nAnswer:\\nLocal governments can promote carbon footprint reduction by implementing policies that enhance **shared transport**, \\nsupport **renewable energy**, and encourage community engagement.',\n",
       " 'Question:\\nWhere can I access a calculator for determining my FLIGHTS carbon emissions footprint?\\n\\nAnswer:\\nYes, you can estimate your flight pollution using this calculator: [Flight Carbon Footprint Calculator](https://www.atag.org/our-activities/environmental-initiatives/carbon-footprint-calculator.html)',\n",
       " 'Question:\\nWhat effects does plastic waste have on carbon release?\\n\\nAnswer:\\nPlastic waste significantly contributes to **carbon toxic emissions** during its production and degradation phases, further exacerbated by improper disposal methods leading to environmental pollution.',\n",
       " \"Question:\\nWhat does the abbreviation 'TNC' represent in the context of climate emergency?\\n\\nAnswer:\\nTRANSNATIONAL CORPORATION:\\nA company that operates in multiple countries, often influencing global carbon discharge.\",\n",
       " 'Question:\\nHow can we reduce carbon footprints using sustainable energy, and what technology helps reduce greenhouse gases?\\n\\nAnswer:\\nRenewable energy sources like **solar** and **wind power** do not produce carbon gas emissions during electricity generation. Transitioning to these sources can significantly reduce our **carbon footprint**.',\n",
       " 'Question:\\nHow do waste reduction strategies practices directly influence carbon emissions?\\n\\nAnswer:\\n**Waste management practices** greatly affect carbon emissions; \\nmethods such as **composting** and **reprocessing** are typically less carbon-intensive compared to **landfilling**.',\n",
       " 'Question:\\nHow do common household items vary in their carbon mark?\\n\\nAnswer:\\nCommon **household items** have differing carbon emissions footprints; \\nelectronics and appliances are notable for significant emissions from their production and disposal processes.',\n",
       " \"Question:\\nWhat is ecological footprint and how can we reduce it? How can we reduce our climate footprint in nature?\\n\\nAnswer:\\nAn **ecological footprint** measures how much **natural resources** a human being consumes against the earth's ability to regenerate them. Reducing **carbon footprint** can help heal the environment and mitigate **climatic shift**.\",\n",
       " 'Question:\\nWhat are the research topics related to climate footprint research in electrical and computer engineering?\\n\\nAnswer:\\nThere are several **research topics** related to carbon impact in electrical and computer engineering, including:\\n- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\\n- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\\n- **Management** of loading and unloading of **thermal solar energy storage systems** for electric generation plants.',\n",
       " 'Question:\\nCan we realistically save the world from global warming?\\n\\nAnswer:\\nYes, but only if we all act now and fast.\\nKeeping warming to **1.5°C** is still possible with international cooperation\\nto make deep, rapid cuts to greenhouse gas discharge.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:14.509260Z",
     "iopub.status.busy": "2024-10-27T14:18:14.508529Z",
     "iopub.status.idle": "2024-10-27T14:18:14.514337Z",
     "shell.execute_reply": "2024-10-27T14:18:14.513362Z",
     "shell.execute_reply.started": "2024-10-27T14:18:14.509228Z"
    }
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Question\", \"Answer\"], [\"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:17.901163Z",
     "iopub.status.busy": "2024-10-27T14:18:17.900461Z",
     "iopub.status.idle": "2024-10-27T14:19:18.446915Z",
     "shell.execute_reply": "2024-10-27T14:19:18.446045Z",
     "shell.execute_reply.started": "2024-10-27T14:18:17.901128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the specialized class\n",
    "\n",
    "Here we define the special class `GemmaQA`. \n",
    "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
    "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.448750Z",
     "iopub.status.busy": "2024-10-27T14:19:18.448493Z",
     "iopub.status.idle": "2024-10-27T14:19:18.454865Z",
     "shell.execute_reply": "2024-10-27T14:19:18.453940Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.448726Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                question=question,\n",
    "                answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.456252Z",
     "iopub.status.busy": "2024-10-27T14:19:18.455985Z",
     "iopub.status.idle": "2024-10-27T14:19:18.583614Z",
     "shell.execute_reply": "2024-10-27T14:19:18.582827Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.456228Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.585959Z",
     "iopub.status.busy": "2024-10-27T14:19:18.585646Z",
     "iopub.status.idle": "2024-10-27T14:19:18.592121Z",
     "shell.execute_reply": "2024-10-27T14:19:18.591229Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.585918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[     2,   9413, 235292, ...,      0,      0,      0],\n",
      "       [     2,   9413, 235292, ...,      0,      0,      0]],      dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[  9413 235292    108 ...      0      0      0]\n",
      " [  9413 235292    108 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.593603Z",
     "iopub.status.busy": "2024-10-27T14:19:18.593323Z",
     "iopub.status.idle": "2024-10-27T14:19:19.097275Z",
     "shell.execute_reply": "2024-10-27T14:19:19.096429Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.593578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n",
    "gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training sequence\n",
    "\n",
    "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
    "We compile the model, with the loss, optimizer and metric.\n",
    "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:19.098736Z",
     "iopub.status.busy": "2024-10-27T14:19:19.098440Z",
     "iopub.status.idle": "2024-10-28T00:51:45.441130Z",
     "shell.execute_reply": "2024-10-28T00:51:45.440190Z",
     "shell.execute_reply.started": "2024-10-27T14:19:19.098710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45863/45863\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37943s\u001b[0m 826ms/step - loss: 0.0733 - sparse_categorical_accuracy: 0.8514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7b80b3fe5060>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set sequence length cf. config (512)\n",
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.442525Z",
     "iopub.status.busy": "2024-10-28T00:51:45.442251Z",
     "iopub.status.idle": "2024-10-28T00:51:45.446749Z",
     "shell.execute_reply": "2024-10-28T00:51:45.445834Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.442501Z"
    }
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For start, we are testing the model with some of the data from the training set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.448282Z",
     "iopub.status.busy": "2024-10-28T00:51:45.447832Z",
     "iopub.status.idle": "2024-10-28T00:51:45.778444Z",
     "shell.execute_reply": "2024-10-28T00:51:45.777544Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.448256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What actions can local governments take to pro...</td>\n",
       "      <td>Local governments can promote carbon footprint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where can I access a calculator for determinin...</td>\n",
       "      <td>Yes, you can estimate your flight pollution us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What effects does plastic waste have on carbon...</td>\n",
       "      <td>Plastic waste significantly contributes to **c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the abbreviation 'TNC' represent in ...</td>\n",
       "      <td>TRANSNATIONAL CORPORATION:\\nA company that ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we reduce carbon footprints using sust...</td>\n",
       "      <td>Renewable energy sources like **solar** and **...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What actions can local governments take to pro...   \n",
       "1  Where can I access a calculator for determinin...   \n",
       "2  What effects does plastic waste have on carbon...   \n",
       "3  What does the abbreviation 'TNC' represent in ...   \n",
       "4  How can we reduce carbon footprints using sust...   \n",
       "\n",
       "                                              answer  \n",
       "0  Local governments can promote carbon footprint...  \n",
       "1  Yes, you can estimate your flight pollution us...  \n",
       "2  Plastic waste significantly contributes to **c...  \n",
       "3  TRANSNATIONAL CORPORATION:\\nA company that ope...  \n",
       "4  Renewable energy sources like **solar** and **...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv(\"/kaggle/input/to-query/format1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.780186Z",
     "iopub.status.busy": "2024-10-28T00:51:45.779810Z",
     "iopub.status.idle": "2024-10-28T00:51:45.787120Z",
     "shell.execute_reply": "2024-10-28T00:51:45.786216Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.780154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    What does production of **meat and dairy produ...\n",
       "answer      Livestock production contributes to climate ch...\n",
       "Name: 15, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.790818Z",
     "iopub.status.busy": "2024-10-28T00:51:45.790537Z",
     "iopub.status.idle": "2024-10-28T00:52:08.880613Z",
     "shell.execute_reply": "2024-10-28T00:52:08.879680Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.790795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What does production of **meat and dairy products** have to do with climate disruption?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Livestock production contributes to climate change through:\n",
       "- **Methane emissions**\n",
       "- **Deforestation** for animal agriculture,\n",
       "accounting for a significant percentage of **global greenhouse gas emissions**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.loc[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:08.882368Z",
     "iopub.status.busy": "2024-10-28T00:52:08.882060Z",
     "iopub.status.idle": "2024-10-28T00:52:31.257467Z",
     "shell.execute_reply": "2024-10-28T00:52:31.256487Z",
     "shell.execute_reply.started": "2024-10-28T00:52:08.882343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Other than carbon foot print name other foot print?\n",
      "\n",
      "Answer:\n",
      "Carbon footprint other than ecological footprint refers to the total greenhouse gas emissions caused by an individual, organization, event, or product, measured in CO2 equivalents.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"Other than carbon foot print name other foot print?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:31.259449Z",
     "iopub.status.busy": "2024-10-28T00:52:31.258999Z",
     "iopub.status.idle": "2024-10-28T00:52:32.347737Z",
     "shell.execute_reply": "2024-10-28T00:52:32.346774Z",
     "shell.execute_reply.started": "2024-10-28T00:52:31.259414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Who are you?\n",
      "\n",
      "Answer:\n",
      "I'm a student studying environmental science with a focus on **greenhouse gas emissions** and **energy management**.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"Who are you?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:32.349577Z",
     "iopub.status.busy": "2024-10-28T00:52:32.349267Z",
     "iopub.status.idle": "2024-10-28T00:52:33.814310Z",
     "shell.execute_reply": "2024-10-28T00:52:33.813366Z",
     "shell.execute_reply.started": "2024-10-28T00:52:32.349549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "List two types of caron foot prints?\n",
      "\n",
      "Answer:\n",
      "Yes, two types are: \n",
      "   - **Personal greenhouse gases** \n",
      "   - **Corporate greenhouse gases** , \n",
      "each with different emission profiles.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"List two types of caron foot prints?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:33.817300Z",
     "iopub.status.busy": "2024-10-28T00:52:33.815534Z",
     "iopub.status.idle": "2024-10-28T00:52:37.474231Z",
     "shell.execute_reply": "2024-10-28T00:52:37.473263Z",
     "shell.execute_reply.started": "2024-10-28T00:52:33.817270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What are the research topics related to climate footprint research in electrical and computer engineering?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "There are several **research topics** related to carbon footprint in electrical and computer engineering, including:\n",
       "- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\n",
       "- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\n",
       "- **Management** of loading and unloading of **thermal solar generation storage systems** for electric generation plants."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[8]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:37.476018Z",
     "iopub.status.busy": "2024-10-28T00:52:37.475669Z",
     "iopub.status.idle": "2024-10-28T00:52:39.558912Z",
     "shell.execute_reply": "2024-10-28T00:52:39.558022Z",
     "shell.execute_reply.started": "2024-10-28T00:52:37.475987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "Which energy resource has the smallest climate footprint?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "**Wind, nuclear, tidal, hydropower, geothermal, solar,** and **wave energy** have the lowest carbon impacts, emitting between **11 and 48 gCO2** on a life-cycle basis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:39.560350Z",
     "iopub.status.busy": "2024-10-28T00:52:39.560064Z",
     "iopub.status.idle": "2024-10-28T00:52:41.563088Z",
     "shell.execute_reply": "2024-10-28T00:52:41.562090Z",
     "shell.execute_reply.started": "2024-10-28T00:52:39.560325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What is Climate Change?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Climate change refers to the long-term alteration of temperature and typical weather patterns in a place,\n",
       "primarily driven by human activities that increase climate pollutants in the atmosphere,\n",
       "such as burning fossil fuels and deforestation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is Climate Change?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:41.564380Z",
     "iopub.status.busy": "2024-10-28T00:52:41.564116Z",
     "iopub.status.idle": "2024-10-28T00:52:44.038894Z",
     "shell.execute_reply": "2024-10-28T00:52:44.037926Z",
     "shell.execute_reply.started": "2024-10-28T00:52:41.564357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "List ways to reduce carbon foot print?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Strategies for reducing **carbon footprint** include: \n",
       "   - **using public transportation** \n",
       "   - **embracing a plant-based diet** \n",
       "   - **reducing energy consumption at home** \n",
       "   - **advocating for renewable energy sources**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"List ways to reduce carbon foot print?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:44.040578Z",
     "iopub.status.busy": "2024-10-28T00:52:44.040216Z",
     "iopub.status.idle": "2024-10-28T00:52:45.492706Z",
     "shell.execute_reply": "2024-10-28T00:52:45.491752Z",
     "shell.execute_reply.started": "2024-10-28T00:52:44.040544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A code competition is an event where developers compete by creating software applications in a given time frame,\n",
       "often with a specific theme or objective."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:45.494134Z",
     "iopub.status.busy": "2024-10-28T00:52:45.493831Z",
     "iopub.status.idle": "2024-10-28T00:52:47.771201Z",
     "shell.execute_reply": "2024-10-28T00:52:47.770302Z",
     "shell.execute_reply.started": "2024-10-28T00:52:45.494107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What are the steps to create a Kaggle dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "First, you should create a **team account** on Kaggle.\n",
       "Next, you should **upload your data** in a folder named 'raw'.\n",
       "Finally, you should **create a README file** with detailed information about your data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are the steps to create a Kaggle dataset?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:58:51.062524Z",
     "iopub.status.busy": "2024-10-28T00:58:51.062098Z",
     "iopub.status.idle": "2024-10-28T00:58:53.391249Z",
     "shell.execute_reply": "2024-10-28T00:58:53.390320Z",
     "shell.execute_reply.started": "2024-10-28T00:58:51.062486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "How can I reduce my carbon foot print as a Multimedia University of Kenya Student?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "To reduce your **carbon impact** as a student at the **Multimedia University of Kenya**, try:\n",
       "- **Using energy-efficient devices**\n",
       "- **Monitoring your energy consumption**\n",
       "- **Participating in **sustainability initiatives** on campus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can I reduce my carbon foot print as a Multimedia University of Kenya Student?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:59:30.071082Z",
     "iopub.status.busy": "2024-10-28T00:59:30.070724Z",
     "iopub.status.idle": "2024-10-28T00:59:31.141327Z",
     "shell.execute_reply": "2024-10-28T00:59:31.140343Z",
     "shell.execute_reply.started": "2024-10-28T00:59:30.071055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "How Old are you?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "I am **22 years old**, and my carbon legacy is based on that age."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How Old are you?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:47.772712Z",
     "iopub.status.busy": "2024-10-28T00:52:47.772416Z",
     "iopub.status.idle": "2024-10-28T00:53:23.227706Z",
     "shell.execute_reply": "2024-10-28T00:53:23.226867Z",
     "shell.execute_reply.started": "2024-10-28T00:52:47.772686Z"
    }
   },
   "outputs": [],
   "source": [
    "preset_dir = \".\\gemma2_2b_en_kaggle_docs\"\n",
    "gemma_causal_lm.save_to_preset(preset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Model on Kaggle as a Kaggle Model\n",
    "\n",
    "We are publishing now the saved model as a Kaggle Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:53:23.229084Z",
     "iopub.status.busy": "2024-10-28T00:53:23.228802Z",
     "iopub.status.idle": "2024-10-28T00:56:36.360074Z",
     "shell.execute_reply": "2024-10-28T00:56:36.359114Z",
     "shell.execute_reply.started": "2024-10-28T00:53:23.229058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Model https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs ...\n",
      "Model 'gemma2-kaggle-docs' does not exist or access is forbidden for user 'moanlobago'. Creating or handling Model...\n",
      "Model 'gemma2-kaggle-docs' Created.\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/preprocessor.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 1.41k/1.41k [00:00<00:00, 2.19kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/preprocessor.json (1KB)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 782/782 [00:00<00:00, 1.22kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/config.json (782B)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/task.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 2.98k/2.98k [00:00<00:00, 4.62kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/task.json (3KB)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 591/591 [00:00<00:00, 945B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/tokenizer.json (591B)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/model.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 10.5G/10.5G [03:03<00:00, 57.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/model.weights.h5 (10GB)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 143/143 [00:00<00:00, 238B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/metadata.json (143B)\n",
      "Starting upload for file .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 5.61MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_kaggle_docs/assets/tokenizer/vocabulary.spm (4MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\n"
     ]
    }
   ],
   "source": [
    "kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n",
    "\n",
    "kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-kaggle-docs/keras/gemma2_2b_en_kaggle_docs\"\n",
    "keras_nlp.upload_preset(kaggle_uri, preset_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5853991,
     "sourceId": 9596582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5958248,
     "sourceId": 9735365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5958454,
     "sourceId": 9735634,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 78150,
     "modelInstanceId": 72244,
     "sourceId": 85984,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
