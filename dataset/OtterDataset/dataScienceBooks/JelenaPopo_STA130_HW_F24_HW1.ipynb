{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d4c22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "        'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
       "        'alive', 'alone'],\n",
       "       dtype='object'),\n",
       " (891, 15))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the Titanic dataset from the provided URL and display the column names and the number of rows and columns in the dataset.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "titanic_data = pd.read_csv(url)\n",
    "\n",
    "# Get the column names and the shape (number of rows and columns)\n",
    "columns = titanic_data.columns\n",
    "shape = titanic_data.shape\n",
    "\n",
    "columns, shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce19e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Survived                   0\n",
      "Pclass                     0\n",
      "Name                       0\n",
      "Sex                        0\n",
      "Age                        0\n",
      "Siblings/Spouses Aboard    0\n",
      "Parents/Children Aboard    0\n",
      "Fare                       0\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values:\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download Titanic dataset\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = titanic_df.isnull().sum()\n",
    "\n",
    "# Print columns with missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Alternatively, you can show a summary of columns that contain missing values\n",
    "print(\"\\nColumns with missing values:\")\n",
    "print(titanic_df.columns[titanic_df.isnull().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00567ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values again\n",
    "missing_values = titanic_df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17b433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 891 rows and 12 columns.\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions of the dataset\n",
    "dataset_shape = titanic_df.shape\n",
    "\n",
    "# Print the result\n",
    "print(f\"The dataset contains {dataset_shape[0]} rows and {dataset_shape[1]} columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01684f3c",
   "metadata": {},
   "source": [
    "The term observation refers to rows of the dataset, in the context of this dataset it refers to the passengers where each row would be a different passenger\n",
    "the term variables refers to the columms of the dataset and show the charaterists of the passengers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Download Titanic dataset\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Print the shape of the dataset (number of rows and columns)\n",
    "print(f\"The dataset contains {titanic_df.shape[0]} rows and {titanic_df.shape[1]} columns.\\n\")\n",
    "\n",
    "# Print data types of each column\n",
    "print(\"Data types of each column:\\n\")\n",
    "print(titanic_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate and print a summary for numerical columns\n",
    "print(\"Summary of numerical columns:\\n\")\n",
    "print(titanic_df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Generate and print a summary for all columns, including categorical\n",
    "print(\"Summary of all columns (numerical and categorical):\\n\")\n",
    "print(titanic_df.describe(include='all'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156bad3",
   "metadata": {},
   "source": [
    "df.shape returns the dimensions of the dataset/amount of rows and columns \n",
    "df.describe() provides statistics for numerical columns\n",
    "discrepancy can occur because df.descirbe only looks at numerical columns so it may not summarize\n",
    "the same amount of columns stated by df.shape if there are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bf512",
   "metadata": {},
   "source": [
    "Attibutes and methods \n",
    "\n",
    "attributes are like stored values where its not performing any actions, it's recalling the stored value not modifing the data which means you don't need parentheses. In the case of df.shape its giving you the stored value of its dimesions \n",
    "\n",
    "Methods\n",
    "df.describe() is a method and must include parentheses. Unlike attributes it executes code and it performs a task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb45a9d",
   "metadata": {},
   "source": [
    "Post-Lecture Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08518267",
   "metadata": {},
   "source": [
    "definitions of the summmary statistics provided by the df.describe() method \n",
    "\n",
    "Count - the number of values that are not non-null or missing in each column \n",
    "mean - the average value of the daya in the column \n",
    "standard deviation - deviating values from the average\n",
    "Minimum - smallest value \n",
    "25th Percentile - the value where 25% of the value are below that value \n",
    "50th percentile - median / the middle of the data when sorted \n",
    "75th Percentile - the value where 75% of answers are greater than that value \n",
    "maximum - greatest value in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09b638",
   "metadata": {},
   "source": [
    "7.1 \n",
    "df.dropna() - will remove any row with a missing value\n",
    "del df['col'] removes the specific column stated\n",
    "\n",
    "df.dropna() would be pefered if trying to remove rows or columns based on missing values while del df['col'] is to delete a specific column. Using both together could be important if the dataset had missing values in both colunmns and rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9965744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "    PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch    Ticket  \\\n",
      "1             2         1       1  female  38.0      1      0  PC 17599   \n",
      "3             4         1       1  female  35.0      1      0    113803   \n",
      "6             7         0       1    male  54.0      0      0     17463   \n",
      "10           11         1       3  female   4.0      1      1   PP 9549   \n",
      "11           12         1       1  female  58.0      0      0    113783   \n",
      "\n",
      "       Fare Cabin Embarked  \n",
      "1   71.2833   C85        C  \n",
      "3   53.1000  C123        S  \n",
      "6   51.8625   E46        S  \n",
      "10  16.7000    G6        S  \n",
      "11  26.5500  C103        S  \n",
      "\n",
      "Missing values in each column after cleaning:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Step 2: Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "\n",
    "# Step 3: Drop rows with missing values (i.e., any row that has NaN in any column)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Step 4: Use 'del' to delete a specific column (for example, let's delete the 'Name' column)\n",
    "del df_cleaned['Name']\n",
    "\n",
    "# Step 5: Show the cleaned DataFrame after removing missing values and deleting the 'Name' column\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(df_cleaned.head())  # Display first 5 rows of the cleaned DataFrame\n",
    "print(\"\\nMissing values in each column after cleaning:\")\n",
    "print(df_cleaned.isnull().sum())  # Confirm no missing values remain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bab63",
   "metadata": {},
   "source": [
    "Summarized chat bot session \n",
    "Finding Public Datasets:\n",
    "You initially requested interesting datasets that are publicly available via a URL and contain missing values for practicing data cleaning techniques. I suggested the Titanic dataset from Stanford’s public repository, which you later downloaded.\n",
    "\n",
    "Checking for Missing Values:\n",
    "You checked for missing values in the Titanic dataset using df.isnull().sum(). The output showed no missing values, which led to the discovery that this specific version of the Titanic dataset had already been pre-processed.\n",
    "\n",
    "Understanding Dataset Size:\n",
    "We discussed understanding the size of a dataset using df.shape, which provides the number of observations (rows) and variables (columns). This concept was further clarified in the context of the Titanic dataset.\n",
    "\n",
    "Basic Summarization:\n",
    "I introduced basic summarization using df.describe() to calculate key statistics like mean, median, standard deviation, etc., for numerical columns. This method only applies to columns with numerical data.\n",
    "\n",
    "Attributes vs Methods:\n",
    "You asked about the difference between attributes and methods in Pandas. I explained that attributes (like df.shape) provide information directly, while methods (like df.describe()) require parentheses because they perform calculations or other operations on the DataFrame.\n",
    "\n",
    "Explanation of Summary Statistics:\n",
    "We discussed the meaning of each summary statistic produced by df.describe(), including count, mean, std (standard deviation), min, and quartiles (25%, 50%, 75%).\n",
    "\n",
    "df.dropna() vs del df['col']:\n",
    "I explained the difference between df.dropna() (which removes rows or columns with missing values) and del df['col'] (which deletes a specific column). The key distinction is that df.dropna() is for handling missing data, while del is for removing entire columns.\n",
    "\n",
    "Combining df.dropna() and del:\n",
    "You asked for a code example combining both techniques to clean a dataset. I provided a Python example that loads the Titanic dataset, uses df.dropna() to remove rows with missing values, and uses del to remove the Name column.\n",
    "\n",
    "Switching to the Kaggle Titanic Dataset:\n",
    "Since the original Titanic dataset had no missing values, I recommended using the Kaggle version of the Titanic dataset, which contains missing values in columns like Age, Cabin, and Embarked. I provided updated code to show how the dataset looks before and after cleaning.\n",
    "\n",
    "Final Code:\n",
    "We finalized the Python code that demonstrates loading the Kaggle Titanic dataset, inspecting missing values, dropping rows with missing values, and deleting a specific column (Name). The code included both pre-cleaning and post-cleaning outputs.\n",
    "\n",
    "link to chatbot session: https://chatgpt.com/share/7d36937d-e411-4634-897d-149369438eee\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e807f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "    PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch    Ticket  \\\n",
      "1             2         1       1  female  38.0      1      0  PC 17599   \n",
      "3             4         1       1  female  35.0      1      0    113803   \n",
      "6             7         0       1    male  54.0      0      0     17463   \n",
      "10           11         1       3  female   4.0      1      1   PP 9549   \n",
      "11           12         1       1  female  58.0      0      0    113783   \n",
      "\n",
      "       Fare Cabin Embarked  \n",
      "1   71.2833   C85        C  \n",
      "3   53.1000  C123        S  \n",
      "6   51.8625   E46        S  \n",
      "10  16.7000    G6        S  \n",
      "11  26.5500  C103        S  \n",
      "\n",
      "Missing values in each column after cleaning:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#final code produced \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "\n",
    "# Drop rows with missing values (i.e., any row that has NaN in any column)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Delete a specific column (for example, let's delete the 'Name' column)\n",
    "del df_cleaned['Name']\n",
    "\n",
    "# Show the cleaned DataFrame after removing missing values and deleting the 'Name' column\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(df_cleaned.head())  # Display first 5 rows of the cleaned DataFrame\n",
    "print(\"\\nMissing values in each column after cleaning:\")\n",
    "print(df_cleaned.isnull().sum())  # Confirm no missing values remain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3b614",
   "metadata": {},
   "source": [
    "New chatbot session started "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93bce8",
   "metadata": {},
   "source": [
    "8.1 \n",
    "df.groupby(\"col1\") - groups the values in col1 for expample if col1 is seasons you'll get groups for each season.\n",
    "[col2] - selects the column to analyse.\n",
    ".describe() - provides summary statistcis for the selected column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aec889",
   "metadata": {},
   "source": [
    "8.2 \n",
    "df.describe() provides an overview of values for every column in the data set. Count will show the number of missing values in the dataset \n",
    "df.groupby(\"col1\")[\"col2\"].describe() gives the statistics of each groups which can have different count values because of missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc8301a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#8.3.a\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset from Kaggle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url_kaggle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url_kaggle)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show the DataFrame before removing missing values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore cleaning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3.a\n",
    "\n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a717dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset from Kaggle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url_kaggle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_kaggle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show the DataFrame before removing missing values\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore cleaning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#8.3.b\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanics.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b55c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing values in each column before cleaning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Display number of missing values per column\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3.c\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "DF.groupby(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0f4aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (3382038892.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(url_kaggle\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#8.3.d\n",
    "\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "df.groupby(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff35c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_groupby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing values in each column before cleaning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Display number of missing values per column\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdf_groupby\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol1\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_groupby' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3.e\n",
    "\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "df_groupby(\"col1\")[\"col2\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361ef46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Column not found: age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m url_kaggle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url_kaggle)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[0;32m-> 1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: age'"
     ]
    }
   ],
   "source": [
    "#8.3.f\n",
    "\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "df.groupby(\"Sex\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdd79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing values in each column before cleaning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Display number of missing values per column\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msex\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sex' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3.g\n",
    "\n",
    "import pandas as pd \n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "df.groupby(sex)[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ec5bd",
   "metadata": {},
   "source": [
    "After going through mutiple different types of errors, I've found that chatbot are more efficient in fixing errors as it can locate the error and correct it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c6c20",
   "metadata": {},
   "source": [
    "chatbot session summary:\n",
    "\n",
    "1. Loading and Initial Dataset Operations\n",
    "\n",
    "We discussed loading the Titanic dataset using pandas and how to access it from the URL: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv.\n",
    "You learned how to display the first 5 rows of the DataFrame using df.head().\n",
    "We also covered how to check for missing values in the dataset using df.isnull().sum().\n",
    "2. Grouping and Descriptive Statistics\n",
    "\n",
    "I explained how to group data using pandas' groupby() function. You initially tried using df_groupby() instead of the correct df.groupby().\n",
    "We discussed how groupby(\"col1\")[\"col2\"].describe() works. You learned that the function groups the DataFrame by \"col1\", calculates descriptive statistics (like mean, standard deviation, count, etc.) for \"col2\", and returns these statistics for each group.\n",
    "Example: df.groupby(\"Pclass\")[\"Age\"].describe() groups by passengers' class (Pclass) and returns statistics for passengers' age (Age).\n",
    "3. Common Mistakes and Syntax Fixes\n",
    "\n",
    "There were some cases where columns \"col1\" and \"col2\" did not exist in the dataset. I guided you to replace them with valid columns like \"Sex\" and \"Age\".\n",
    "We also addressed Python's case-sensitivity with column names and ensured you used the correct capitalization (e.g., \"Age\" instead of \"age\").\n",
    "Example: df.groupby(\"Sex\")[\"Age\"].describe() provided a correct method to group data by the passengers' gender (Sex) and calculate descriptive statistics for their ages (Age).\n",
    "4. Code Execution and Import Errors\n",
    "\n",
    "You encountered errors like NameError: name 'pd' is not defined because you missed the import pandas as pd statement. I provided the correct way to import the pandas library at the start of your script.\n",
    "There were also some issues related to dataset URL typos, such as using \"titanics.csv\" instead of \"titanic.csv\". Once corrected, the dataset loaded correctly.\n",
    " \n",
    " chat bot link: https://chatgpt.com/share/66e394d2-6174-8006-bbff-f857de68017d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c203e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Missing values in each column before cleaning:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "\n",
      "Descriptive statistics for 'Age' by 'Sex':\n",
      "        count       mean        std   min   25%   50%   75%   max\n",
      "Sex                                                              \n",
      "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
      "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0\n"
     ]
    }
   ],
   "source": [
    "#final code \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Load the Titanic dataset from Kaggle\n",
    "url_kaggle = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url_kaggle)\n",
    "\n",
    "# Show the DataFrame before removing missing values\n",
    "print(\"Before cleaning:\")\n",
    "print(df.head())  # Display first 5 rows of the original DataFrame\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing values in each column before cleaning:\")\n",
    "print(df.isnull().sum())  # Display number of missing values per column\n",
    "\n",
    "# Grouping by 'Sex' and showing descriptive statistics for the 'Age' column\n",
    "# This will group the dataset by 'Sex' and calculate statistics for 'Age'\n",
    "age_stats_by_sex = df.groupby(\"Sex\")[\"Age\"].describe()\n",
    "\n",
    "# Print the descriptive statistics for 'Age' by 'Sex'\n",
    "print(\"\\nDescriptive statistics for 'Age' by 'Sex':\")\n",
    "print(age_stats_by_sex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aab80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
