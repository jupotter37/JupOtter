{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "# Plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.cross_validation  import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the folder\n",
    "path = 'C:/Users/prash/Downloads/ML ALGORITHMS/'\n",
    "\n",
    "# Number of clusters required\n",
    "max_depth = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Iris Dataset\n",
    "iris_dataset = pd.read_csv(path + 'DATASETS/' + 'Iris.csv')\n",
    "# Removing Index Column\n",
    "iris_dataset = iris_dataset.iloc[:,1:]\n",
    "\n",
    "# Input Dataframe\n",
    "X = iris_dataset.iloc[:,:-1]\n",
    "X = np.array(X)\n",
    "\n",
    "# Encode the Output labels\n",
    "Y = iris_dataset.iloc[:,-1]\n",
    "for i in range(len(Y.unique())):\n",
    "    Y = Y.replace(Y.unique()[i],i)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Divide into train and test datasets\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini_Impurity(num_samples_per_class,n_classes,output_size):\n",
    "    \"\"\"Compute Gini impurity of a non-empty node.\n",
    "    Gini impurity is defined as Σ p(1-p) over all classes, with p the frequency of a\n",
    "    class within the node. Since Σ p = 1, this is equivalent to 1 - Σ p^2.\n",
    "    \"\"\"    \n",
    "    gini_impurity = 1.0 - sum(((n / output_size) ** 2) for n in num_samples_per_class)\n",
    "    \n",
    "    return gini_impurity\n",
    "\n",
    "class Node:\n",
    "    \"\"\"A decision tree node.\"\"\"\n",
    "\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "        \n",
    "def best_split_criteria(X, Y, n_classes):\n",
    "    \"\"\"Find the best split for a node.\n",
    "    \"Best\" means that the average impurity of the two children, weighted by their\n",
    "    population, is the smallest possible. Additionally it must be less than the\n",
    "    impurity of the current node.\n",
    "    To find the best split, we loop through all the features, and consider all the\n",
    "    midpoints between adjacent training samples as possible thresholds. We compute\n",
    "    the Gini impurity of the split generated by that particular feature/threshold\n",
    "    pair, and return the pair with smallest impurity.\n",
    "    Returns:\n",
    "        best_idx: Index of the feature for best split, or None if no split is found.\n",
    "        best_thr: Threshold to use for the split, or None if no split is found.\n",
    "    \"\"\"\n",
    "    # Need at least two elements to split a node.\n",
    "    m = Y.size\n",
    "    if m <= 1:\n",
    "        return None, None\n",
    "\n",
    "    # Count of each class in the current node.\n",
    "    num_parent = [np.sum(Y == c) for c in range(n_classes)]\n",
    "\n",
    "    # Gini of current node.\n",
    "    best_gini = Gini_Impurity(num_parent,n_classes,m)\n",
    "    best_idx, best_thr = None, None\n",
    "\n",
    "    # Loop through all features.\n",
    "    for idx in range(n_features):\n",
    "        # Sort data along selected feature.\n",
    "        thresholds, classes = zip(*sorted(zip(X[:, idx], Y)))\n",
    "\n",
    "        # We could actually split the node according to each feature/threshold pair\n",
    "        # and count the resulting population for each class in the children, but\n",
    "        # instead we compute them in an iterative fashion, making this for loop linear rather than quadratic.\n",
    "        num_left = [0] * n_classes\n",
    "        num_right = num_parent.copy()\n",
    "        # possible split positions\n",
    "        for i in range(1, m):  \n",
    "            c = classes[i - 1]\n",
    "            \n",
    "            num_left[c] += 1\n",
    "            num_right[c] -= 1\n",
    "            \n",
    "            gini_left = Gini_Impurity(num_left,n_classes,i)\n",
    "            gini_right = Gini_Impurity(num_right,n_classes,(m-i))\n",
    "\n",
    "            # The Gini impurity of a split is the weighted average of the Gini impurity of the children\n",
    "            gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "\n",
    "            # The following condition is to make sure we don't try to split two\n",
    "            # points with identical values for that feature, as it is impossible\n",
    "            # (both have to end up on the same side of a split).\n",
    "            if thresholds[i] == thresholds[i - 1]:\n",
    "                continue\n",
    "\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_idx = idx\n",
    "                # midpoint\n",
    "                best_thr = (thresholds[i] + thresholds[i - 1]) / 2  \n",
    "\n",
    "    return best_idx, best_thr\n",
    "\n",
    "\n",
    "def grow_tree(X, Y, n_classes,max_depth,depth = 0):\n",
    "\n",
    "    num_samples_per_class = [np.sum(Y == i) for i in range(n_classes)]\n",
    "    predicted_class = np.argmax(num_samples_per_class)\n",
    "\n",
    "    node = Node(gini = Gini_Impurity(num_samples_per_class,n_classes,Y.size),\n",
    "                num_samples = Y.size,\n",
    "                num_samples_per_class = num_samples_per_class,\n",
    "                predicted_class = predicted_class)\n",
    "    \n",
    "    # Split recursively until maximum depth is reached.\n",
    "    if depth < max_depth:\n",
    "        best_idx, best_threshold = best_split_criteria(X, Y, n_classes)\n",
    "        if best_idx is not None:\n",
    "            indices_left = X[:, best_idx] < best_threshold\n",
    "            \n",
    "            X_left, Y_left = X[indices_left], Y[indices_left]\n",
    "            X_right, Y_right = X[~indices_left], Y[~indices_left]\n",
    "            \n",
    "            node.feature_index = best_idx\n",
    "            node.threshold = best_threshold\n",
    "            \n",
    "            node.left = grow_tree(X_left, Y_left, n_classes,max_depth,depth = depth+1)\n",
    "            node.right = grow_tree(X_right, Y_right, n_classes,max_depth,depth = depth+1)\n",
    "            \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(set(Y))  # classes are assumed to go from 0 to n-1\n",
    "n_features = X.shape[1]\n",
    "\n",
    "\n",
    "# Training \n",
    "tree_model = grow_tree(X, Y, n_classes,max_depth,depth = 0)\n",
    "\n",
    "# Testing\n",
    "Y_pred = []\n",
    "for test in X_test:\n",
    "    node = tree_model\n",
    "    while node.left:\n",
    "        if test[node.feature_index] < node.threshold:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    Y_pred.append(node.predicted_class)\n",
    "\n",
    "    \n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(Y_pred, Y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = max_depth)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "Y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(Y_pred, Y_test)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775 <br>\n",
    "https://medium.com/@mathanrajsharma/fundamentals-of-classification-and-regression-trees-cart-e9af0b152503"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
