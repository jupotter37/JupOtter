{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it follows a brief introduction to the Deep Fake Challenge from Kaggle. \n",
    "# <a id='0'>Contents</a>\n",
    "\n",
    "\n",
    "- <a href='#1'>DeepFake: a threat  to democracy or just a little bit of fun?</a>  \n",
    "- <a href='#2'>What is Kaggle?</a>  \n",
    "- <a href='#3'>Somebody said GAN? A dive into DeepFakes creation</a> \n",
    "- <a href='#4'>DeepFake Detection Challenge</a> \n",
    "- <a href='#5'>Preliminary data exploration</a>  \n",
    "    * Detection starter kit \n",
    "- <a href='#6'>Metadata exploration</a>  \n",
    "     * FFMPEG and FFPROBE  \n",
    "- <a href='#7'>Videos exploration</a>  \n",
    "     * Frames Extraction\n",
    "- <a href='#8'>Face detection</a>  \n",
    "- <a href='#9'>References</a>     \n",
    "\n",
    "\n",
    "\n",
    "#  <a id='1'>DeepFake: a threat to democracy or just a bit of fun?</a> \n",
    "\n",
    "*\"We are already at the point where you can't tell the difference between deepfakes and the real thing,\"* **Professor Hao Li, University of Southern California**\n",
    "\n",
    "\n",
    "**Facebook** [has announced](https://about.fb.com/news/2020/01/enforcing-against-manipulated-media/) it will remove videos modified by artificial intelligence, known as deepfakes, from its platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube https://www.youtube.com/watch?v=cQ54GDm1eL0\n",
    "YouTubeVideo('cQ54GDm1eL0', width= 600, height= 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>What is Kaggle?</a>  \n",
    "\n",
    "[**Kaggle**](https://www.kaggle.com/) is an **AirBnB for Data Scientists** – this is where they spend their nights and weekends. It’s a crowd-sourced platform to attract, nurture, train and challenge data scientists from all around the world to solve data science, machine learning and predictive analytics problems. It has over 536,000 active members from 194 countries and it receives close to 150,000 submissions per month. Started from Melbourne, Australia Kaggle moved to Silicon Valley in 2011, raised some 11 million dollars from the likes of Hal Varian (Chief Economist at Google), Max Levchin (Paypal), Index and Khosla Ventures and then ultimately been acquired by the Google in March of 2017. Kaggle is the number one stop for data science enthusiasts all around the world who compete for prizes and boost their Kaggle rankings. There are only 94 Kaggle Grandmasters in the world to this date.\n",
    "\n",
    "Do you know that most data scientists are only theorists and rarely get a chance to practice before being employed in the real-world? Kaggle solves this problem by giving data science enthusiasts a platform to interact and compete in solving real-life problems. The experience you get on Kaggle is invaluable in preparing you to understand what goes into finding feasible solutions for big data.\n",
    "\n",
    "Fine, fine, fine but what do we do on Kaggle? **We Learn**\n",
    "\n",
    "\n",
    "#  <a id='3'>A dive into DeepFakes creation: somebody said GAN? </a> \n",
    "Deepfakes are fakes generated by deep learning. So far so easy. \n",
    "\n",
    "This usually means someone used a generative model like an [*AutoEncoder*](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/) or most likely a [*Generative Adversarial Network*](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), short **GAN**. GANs are technically two networks that work against each other, illustrated below. The *artist* (**generator**) draws its inspiration from a noise sample and creates a rendering of the data you are trying to generate with said GAN. The private *investigator* (**discriminator**) randomly gets assigned real and fake data to investigate. \n",
    "\n",
    "The learning process is collaborative. The generator gets better at fooling the discriminator and the discriminator gets better at figuring out which data is real and which isn't. In mathematical terms they are learning until a [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) is reached, which means neither can learn new tricks and get better. They're a really cool concept and even used in scientific simulation at [CERN](https://indico.cern.ch/event/595059/contributions/2497383/attachments/1431666/2199445/gan_presentation_IML.pdf).\n",
    "\n",
    "You can probably guess that they can be tricky to train, due to so many moving parts. This has become a very popular area of research, warranting a [GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo) of all named GANs. Some important stuff you may want to check out if your interested are keywords like Wasserstein GANs, Gradient Penalization, Attention, and in this context Style Transfer (namely face2face).\n",
    "\n",
    "![GAN from PhD thesis.](https://dramsch.net/assets/images/GAN.PNG)\n",
    "\n",
    "<big><big>It sounds absurd, I know. [Here you can find some more practical examples](https://poloclub.github.io/ganlab/), why don't you play with them for a while?</big></big>\n",
    "\n",
    "#  <a id='4'>DeepFake Detection Challenge</a>  \n",
    "[Official Challenge on Kaggle](https://www.kaggle.com/c/deepfake-detection-challenge/)\n",
    "\n",
    "[Official Website](www.deepfakedetectionchallenge.ai)\n",
    "\n",
    "* I strongly encourage you to start first with the [official Getting Started guide here](https://www.kaggle.com/c/deepfake-detection-challenge/overview/getting-started).\n",
    "\n",
    "* What is the goal of the Deepfake Detection Challenge? According to the FAQ \"The AI technologies that power deepfakes and other tampered media are rapidly evolving, making deepfakes so hard to detect that, at times, even human evaluators can’t reliably tell the difference. The Deepfake Detection Challenge is designed to incentivize rapid progress in this area by inviting participants to compete to create new ways of detecting and preventing manipulated media.\"\n",
    "\n",
    "* In this Code Competition:    \n",
    "    *  CPU Notebook <= 9 hours run-time, GPU Notebook <= 9 hours run-time on Kaggle's P100 GPUs, **No internet access enabled**\n",
    "    * External data is allowed **up to 1 GB in size**. External data must be freely & publicly available, including pre-trained models\n",
    "    \n",
    "* This code competition's training set is not available directly on Kaggle, as its size is prohibitively large to train in Kaggle. Instead, it's strongly recommended that you train offline and load the externally trained model as an external dataset into Kaggle Notebooks to perform inference on the Test Set. Review Getting Started for more detailed information.\n",
    "\n",
    "### Scoring\n",
    "\n",
    "Submissions are scored on [log loss](http://wiki.fast.ai/index.php/Log_Loss):\n",
    "\n",
    "![logloss](http://latex2png.com/pngs/47025bcc90ef9d505b1050ab5934172f.png)\n",
    "\n",
    "where:\n",
    "\n",
    "* _n_ is the number of videos being predicted\n",
    "* _y^<sub>i</sub>_ is the predicted probability of the video being FAKE\n",
    "* _y<sub>i</sub>_ is 1 if the video is FAKE, 0 if REAL\n",
    "* _log()_ is the natural (base e) logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Implementation\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss([\"REAL\", \"FAKE\", \"FAKE\", \"REAL\"],\n",
    "         [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- We have a bunch of .mp4 files, split into compressed sets of ~10GB a piece. A metadata.json accompanies each set of .mp4 files, and contains filename, label (REAL/FAKE), original and split columns, listed below under Columns.\n",
    "- The full training set is just over 470 GB (**Yeah it's huge !**).\n",
    "\n",
    "*References: https://deepfakedetectionchallenge.ai/faqs*\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "There are 4 groups of datasets associated with this competition.\n",
    "\n",
    "**Training Set:** This dataset, containing labels for the target, is available for download for competitors to build their models. It is broken up into 50 files, for ease of access and download. Due to its large size, it must be accessed through a GCS bucket which is only made available to participants after accepting the competition’s rules. Please read the rules fully before accessing the dataset, as they contain important details about the dataset’s permitted use. *It is expected and encouraged that you train your models outside of Kaggle’s notebooks environment and submit to Kaggle by uploading the trained model as an external data source.*\n",
    "\n",
    "**Public Validation Set:** When you commit your Kaggle notebook, the submission file output that is generated will be based on the small set of 400 videos/ids contained within this Public Validation Set. This is available on the Kaggle Data page as test_videos.zip\n",
    "\n",
    "**Public Test Set:** This dataset is completely withheld and is what Kaggle’s platform computes the public leaderboard against. When you “Submit to Competition” from the “Output” file of a committed notebook that contains the competition’s dataset, your code will be re-run in the background against this Public Test Set. When the re-run is complete, the score will be posted to the public leaderboard. If the re-run fails, you will see an error reflected in your “My Submissions” page. Unfortunately, we are unable to surface any details about your error, so as to prevent error-probing. *You are limited to 2 submissions per day, including submissions with errors.*\n",
    "\n",
    "**Private Test Set:** This dataset is privately held outside of Kaggle’s platform, and is used to compute the private leaderboard. It contains videos with a similar format and nature as the Training and Public Validation/Test Sets, but are real, organic videos with and without deepfakes. After the competition deadline, Kaggle transfers your 2 final selected submissions’ code to the host. They will re-run your code against this private dataset and return prediction submissions back to Kaggle for computing your final private leaderboard scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, shutil\n",
    "import timeit, os, gc\n",
    "import subprocess as sp\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Data Files Accessible within kernel\n",
    "\n",
    "### Files\n",
    "- **train_sample_videos.zip** - a ZIP file containing a sample set of training videos and a metadata.json with labels. the full set of training videos is available through the links provided above.\n",
    "- **sample_submission.csv** - a sample submission file in the correct format.\n",
    "- **test_videos.zip** - a zip file containing a small set of videos to be used as a public validation set.\n",
    "To understand the datasets available for this competition, review the Getting Started information.\n",
    "\n",
    "### Metadata Columns\n",
    "- **filename** - the filename of the video\n",
    "- **label** - whether the video is REAL or FAKE\n",
    "- **original** - in the case that a train set video is FAKE, the original video is listed here\n",
    "- **split** - this is always equal to \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_metadata = pd.read_json('/Volumes/DFChallenge/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage\n",
    "norm_constant = train_sample_metadata.groupby('label')['label'].count()[0] + train_sample_metadata.groupby('label')['label'].count()[1]\n",
    "(train_sample_metadata.groupby('label')['label'].count()/norm_constant).plot(figsize=(15, 5), kind='bar', title='Distribution of Labels in the Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <big><a id='5'>Preliminary data exploration</a></big>\n",
    "\n",
    "## Detection Starter Kit \n",
    "\n",
    "A **quickstart guide** on DeepFakes: [\"DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection](https://arxiv.org/abs/2001.00179)\n",
    "\n",
    "This CPU-only kernel is a Deep Fakes video EDA. It relies on [static FFMPEG](https://www.ffmpeg.org/download.html) to read/extract data from videos.\n",
    "\n",
    "- It extracts meta-data. They help us to know frame rate, dimensions and audio format (we can forget leak of \"display_ratio\" as it will be fixed).\n",
    "- It extracts frames of videos as PNG.\n",
    "- It extracts audio track as AAC (disabled).\n",
    "- It compares a few face detectors (OpenCV HaarCascade, MTCNN). More to come (Yolo, BlazeFace, DLib, Faced, ...).\n",
    "- It provides basic statistics on faces per video, face width/height and face detection confidence. It computes an average face width/height.\n",
    "\n",
    "We notice that face detection (with OpenCV currently) is far from being perfect. An additional stage to clean-up detected faces is required before training a model! \n",
    "Maybe some kind of votes/ensemble with different detectors would help.\n",
    "\n",
    "In this kernel you will see also some interesting edge cases of face detection:\n",
    "- Face detected on a t-shirt.\n",
    "- Face detected on a background board.\n",
    "- Face detected inside a face.\n",
    "\n",
    "\n",
    "#  <a id='6'>Metadata Exploration</a> \n",
    "\n",
    "### FFMPEG and FFPROBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = \"./\"\n",
    "FFMPEG = \"/usr/local/Cellar/ffmpeg/4.1.3_1/bin\"\n",
    "FFMPEG_PATH = FFMPEG\n",
    "DATA_FOLDER = \"/Volumes/DFChallenge/deepfake-detection-challenge\"\n",
    "TMP_FOLDER = DATA_FOLDER\n",
    "DATA_FOLDER_TRAIN = DATA_FOLDER\n",
    "VIDEOS_FOLDER_TRAIN = DATA_FOLDER_TRAIN + \"/train_sample_videos\"\n",
    "IMAGES_FOLDER_TRAIN = TMP_FOLDER + \"/images\"\n",
    "AUDIOS_FOLDER_TRAIN = TMP_FOLDER + \"/audios\"\n",
    "EXTRACT_META = True # False\n",
    "EXTRACT_CONTENT = True # False\n",
    "EXTRACT_FACES = True # False\n",
    "FRAME_RATE = 0.2 # Frame per second to extract (max is 30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is [ffprobe](https://ffmpeg.org/ffprobe.html) indeed? Basically, ffprobe gathers information from multimedia streams and prints it in human - and machine - readable fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(*popenargs, **kwargs):\n",
    "    closeNULL = 0\n",
    "    try:\n",
    "        from subprocess import DEVNULL\n",
    "        closeNULL = 0\n",
    "    except ImportError:\n",
    "        import os\n",
    "        DEVNULL = open(os.devnull, 'wb')\n",
    "        closeNULL = 1\n",
    "\n",
    "    process = sp.Popen(stdout=sp.PIPE, stderr=DEVNULL, *popenargs, **kwargs)\n",
    "    output, unused_err = process.communicate()\n",
    "    retcode = process.poll()\n",
    "\n",
    "    if closeNULL:\n",
    "        DEVNULL.close()\n",
    "\n",
    "    if retcode:\n",
    "        cmd = kwargs.get(\"args\")\n",
    "        if cmd is None:\n",
    "            cmd = popenargs[0]\n",
    "        error = sp.CalledProcessError(retcode, cmd)\n",
    "        error.output = output\n",
    "        raise error\n",
    "        \n",
    "    return output\n",
    "\n",
    "def ffprobe(filename, options = [\"-show_error\", \"-show_format\", \"-show_streams\", \"-show_programs\", \"-show_chapters\", \"-show_private_data\"]):\n",
    "    ret = {}\n",
    "    command = [FFMPEG_PATH + \"/ffprobe\", \"-v\", \"error\", *options, \"-print_format\", \"json\", filename]\n",
    "    ret = run_command(command)\n",
    "    if ret:\n",
    "        ret = json.loads(ret)\n",
    "    return ret\n",
    "\n",
    "# ffmpeg -i input.mov -r 0.25 output_%04d.png\n",
    "def ffextract_frames(filename, output_folder, rate = 0.25):\n",
    "    command = [FFMPEG_PATH + \"/ffmpeg\", \"-i\", filename, \"-r\", str(rate), \"-y\", output_folder + \"/output_%04d.png\"]\n",
    "    ret = run_command(command)\n",
    "    return ret\n",
    "\n",
    "# ffmpeg -i input-video.mp4 output-audio.mp3\n",
    "def ffextract_audio(filename, output_path):\n",
    "    command = [FFMPEG_PATH + \"/ffmpeg\", \"-i\", filename, \"-vn\", \"-ac\", \"1\", \"-acodec\", \"copy\", \"-y\", output_path]\n",
    "    ret = run_command(command)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "js = ffprobe(VIDEOS_FOLDER_TRAIN + \"/\"+ \"bqdjzqhcft.mp4\")\n",
    "print(json.dumps(js, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some meta-data\n",
    "if EXTRACT_META == True:\n",
    "    results = []\n",
    "    subfolder = VIDEOS_FOLDER_TRAIN\n",
    "    filepaths = glob.glob(subfolder + \"/*.mp4\")\n",
    "    for filepath in tqdm(filepaths):\n",
    "        js = ffprobe(filepath)\n",
    "        if js:\n",
    "            results.append(\n",
    "                (js.get(\"format\", {}).get(\"filename\")[len(subfolder) + 1:],\n",
    "                js.get(\"format\", {}).get(\"format_long_name\"),\n",
    "                # Video \n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"codec_name\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"height\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"width\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"nb_frames\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"bit_rate\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"duration\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"start_time\"),\n",
    "                js.get(\"streams\", [{}, {}])[0].get(\"avg_frame_rate\"),\n",
    "                # Audio\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"codec_name\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"channels\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"sample_rate\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"nb_frames\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"bit_rate\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"duration\"),\n",
    "                js.get(\"streams\", [{}, {}])[1].get(\"start_time\")),\n",
    "            )\n",
    "\n",
    "    meta_pd = pd.DataFrame(results, columns=[\"filename\", \"format\", \"video_codec_name\", \"video_height\", \"video_width\",\n",
    "                                            \"video_nb_frames\", \"video_bit_rate\", \"video_duration\", \"video_start_time\",\"video_fps\",\n",
    "                                            \"audio_codec_name\", \"audio_channels\", \"audio_sample_rate\", \"audio_nb_frames\",\n",
    "                                            \"audio_bit_rate\", \"audio_duration\", \"audio_start_time\"])\n",
    "    meta_pd[\"video_fps\"] = meta_pd[\"video_fps\"].apply(lambda x: float(x.split(\"/\")[0])/float(x.split(\"/\")[1]) if len(x.split(\"/\")) == 2 else None)\n",
    "    meta_pd[\"video_duration\"] = meta_pd[\"video_duration\"].astype(np.float32)\n",
    "    meta_pd[\"video_bit_rate\"] = meta_pd[\"video_bit_rate\"].astype(np.float32)\n",
    "    meta_pd[\"video_start_time\"] = meta_pd[\"video_start_time\"].astype(np.float32)\n",
    "    meta_pd[\"video_nb_frames\"] = meta_pd[\"video_nb_frames\"].astype(np.float32)\n",
    "    meta_pd[\"video_bit_rate\"] = meta_pd[\"video_bit_rate\"].astype(np.float32)\n",
    "    meta_pd[\"audio_sample_rate\"] = meta_pd[\"audio_sample_rate\"].astype(np.float32)\n",
    "    meta_pd[\"audio_nb_frames\"] = meta_pd[\"audio_nb_frames\"].astype(np.float32)\n",
    "    meta_pd[\"audio_bit_rate\"] = meta_pd[\"audio_bit_rate\"].astype(np.float32)\n",
    "    meta_pd[\"audio_duration\"] = meta_pd[\"audio_duration\"].astype(np.float32)\n",
    "    meta_pd[\"audio_start_time\"] = meta_pd[\"audio_start_time\"].astype(np.float32)\n",
    "    meta_pd.to_pickle(HOME + \"videos_meta.pkl\")\n",
    "else:\n",
    "    meta_pd = pd.read_pickle(HOME + \"videos_meta.pkl\")\n",
    "meta_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,6, figsize=(22, 3))\n",
    "d = sns.distplot(meta_pd[\"video_fps\"], ax=ax[0])\n",
    "d = sns.distplot(meta_pd[\"video_duration\"], ax=ax[1])\n",
    "d = sns.distplot(meta_pd[\"video_width\"], ax=ax[2])\n",
    "d = sns.distplot(meta_pd[\"video_height\"], ax=ax[3])\n",
    "d = sns.distplot(meta_pd[\"video_nb_frames\"], ax=ax[4])\n",
    "d = sns.distplot(meta_pd[\"video_bit_rate\"], ax=ax[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few info on [bitrate](https://filmora.wondershare.com/video-editing-tips/what-is-video-bitrate.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_json(VIDEOS_FOLDER_TRAIN + \"/metadata.json\").T.reset_index().rename(columns={\"index\": \"filename\"})\n",
    "train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.merge(train_pd, meta_pd[[\"filename\", \"video_height\", \"video_width\", \"video_nb_frames\", \"video_bit_rate\", \"audio_nb_frames\"]], on=\"filename\", how=\"left\")\n",
    "train_pd[\"count\"] = train_pd.groupby([\"original\"])[\"original\"].transform('count')\n",
    "# train_pd.to_pickle(HOME + \"train_meta.pkl\")\n",
    "train_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='7'>Videos Exploration</a>\n",
    "\n",
    "### Frames Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = VIDEOS_FOLDER_TRAIN\n",
    "images_folder_path = IMAGES_FOLDER_TRAIN\n",
    "audios_folder_path = AUDIOS_FOLDER_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# We're only interested in Frames --> uncomment ffextract_audio if you need it\n",
    "AUDIO_FORMAT = \"wav\" # \"aac\"\n",
    "videos_folder = VIDEOS_FOLDER_TRAIN\n",
    "images_folder_path = IMAGES_FOLDER_TRAIN\n",
    "audios_folder_path = AUDIOS_FOLDER_TRAIN\n",
    "if EXTRACT_CONTENT == True:\n",
    "    # 1h20min for chunk#0 (11GB)\n",
    "    # Extract some images + audio track\n",
    "    for idx, row in tqdm(train_pd.iterrows(), total=meta_pd.shape[0]):\n",
    "        try:\n",
    "            video_path = videos_folder + \"/\" + row[\"filename\"]\n",
    "            images_path = images_folder_path + \"/\" + row[\"filename\"][:-4]\n",
    "            audio_path = audios_folder_path + \"/\" + row[\"filename\"][:-4]\n",
    "            # Extract images\n",
    "            if not os.path.exists(images_path): os.makedirs(images_path)\n",
    "            ret = ffextract_frames(video_path, images_path, rate = FRAME_RATE)\n",
    "            # Extract audio\n",
    "            if not os.path.exists(audio_path): os.makedirs(audio_path)\n",
    "            # ret = ffextract_audio(video_path, audio_path + \"/audio.\" + AUDIO_FORMAT)\n",
    "        except:\n",
    "            print(\"Cannot extract frames/audio for:\" + row[\"filename\"])\n",
    "            \n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Fake/Real (this one is obvious)\n",
    "idx = 21 # 27 # 21 # 19 # 12 # 6\n",
    "fake = train_pd[\"filename\"][idx]\n",
    "real = train_pd[\"original\"][idx]\n",
    "vid_width = train_pd[\"video_width\"][idx]\n",
    "vid_real = open(VIDEOS_FOLDER_TRAIN + \"/\" + real, 'rb').read()\n",
    "data_url_real = \"data:video/mp4;base64,\" + b64encode(vid_real).decode()\n",
    "vid_fake = open(VIDEOS_FOLDER_TRAIN + \"/\" + fake, 'rb').read()\n",
    "data_url_fake = \"data:video/mp4;base64,\" + b64encode(vid_fake).decode()\n",
    "HTML(\"\"\"\n",
    "<div style='width: 100%%; display: table;'>\n",
    "    <div style='display: table-row'>\n",
    "        <div style='width: %dpx; display: table-cell;'><b>Real</b>: %s<br/><video width=%d controls><source src=\"%s\" type=\"video/mp4\"></video></div>\n",
    "        <div style='display: table-cell;'><b>Fake</b>: %s<br/><video width=%d controls><source src=\"%s\" type=\"video/mp4\"></video></div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\" % ( int(vid_width/3.2) + 10, \n",
    "       real, int(vid_width/3.2), data_url_real, \n",
    "       fake, int(vid_width/3.2), data_url_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='8'>Face Detection</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_face_cv2(img):\n",
    "    # Move to grayscale\n",
    "    gray_img = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2GRAY)\n",
    "    face_locations = []\n",
    "    face_rects = face_cascade.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)     \n",
    "    for (x,y,w,h) in face_rects: \n",
    "        face_location = (x,y,w,h)\n",
    "        face_locations.append((face_location, 1.0))\n",
    "    return face_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ((x,y,w,h, confidence))\n",
    "def extract_faces(files, source, detector=detect_face_cv2):\n",
    "    results = []\n",
    "    # for idx, file in tqdm(enumerate(files), total=len(files)):\n",
    "    for idx, file in enumerate(files):\n",
    "        try:\n",
    "            img = cv2.cvtColor(cv2.imread(file, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)\n",
    "            face_locations = detector(img)\n",
    "            results.append((source, file[file.find(\"output_\"):], face_locations, len(face_locations)))\n",
    "        except:\n",
    "            print(\"Cannot extract faces for image: %s\" % file)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fake\n",
    "dump_folder = images_folder_path + \"/\" + file[:-4]\n",
    "files = glob.glob(dump_folder + \"/*\")\n",
    "DETECTORS = {\n",
    "    \"cv2\": detect_face_cv2,\n",
    "    #\"mtcnn\": detect_face_mtcnn\n",
    "}\n",
    "faces_pd = None\n",
    "for key, value in DETECTORS.items():\n",
    "    tmp_pd = pd.DataFrame(extract_faces(files, file, detector=value), columns=[\"filename\", \"image\", \"boxes_\" + key , \"faces_\" + key])\n",
    "    if faces_pd is None:\n",
    "        faces_pd = tmp_pd\n",
    "    else:\n",
    "        faces_pd = pd.merge(faces_pd, tmp_pd, on=[\"filename\", \"image\"], how=\"left\")\n",
    "faces_pd.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot faces extracted images\n",
    "def plot_faces_boxes(df, max_cols = 2, max_rows = 6, fsize=(24, 5), max_items=12):    \n",
    "    idx = 0    \n",
    "    for item_idx, item in df.iterrows():\n",
    "        img = cv2.cvtColor(cv2.imread(IMAGES_FOLDER_TRAIN + \"/\" + item[\"filename\"][:-4] +\"/\" + item[\"image\"], cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)    \n",
    "        face_img = img #.copy()\n",
    "        # grid subplots\n",
    "        row = idx // max_cols\n",
    "        col = idx % max_cols\n",
    "        if col == 0: fig = plt.figure(figsize=fsize)\n",
    "        ax = fig.add_subplot(1, max_cols, col + 1)\n",
    "        ax.axis(\"off\")\n",
    "        # display image with boxes\n",
    "        cols = [c for c in df.columns if \"boxes\" in c]\n",
    "        for i, c in enumerate(cols, 0):\n",
    "            face_locations = item[c]\n",
    "            face_confidence = item[c]            \n",
    "            if len(face_locations) > 0:\n",
    "                for face_location in face_locations:        \n",
    "                    ((x,y,w,h), confidence) = face_location\n",
    "                    # face_img = face_img[y:y+h, x:x+w]\n",
    "                    cv2.rectangle(face_img, (x, y), (x+w, y+h), (255,i*255,0), 8)\n",
    "                    cv2.putText(face_img, '%.1f' % (confidence*100.0), (x+w, y+h), cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255,i*255,0), 9, cv2.LINE_AA)\n",
    "                ax.imshow(face_img)\n",
    "            else:\n",
    "                ax.imshow(img)\n",
    "            ax.set_title(\"%s %s / %s - Faces: %d %s %s\" % (item[\"label\"] if \"label\" in df.columns else \"\", \n",
    "                                                           item[\"filename\"], item[\"image\"],\n",
    "                                                           item[\"faces_cv2\"] if \"faces_cv2\" in df.columns else len(face_locations),\n",
    "                                                           item[\"faces_cv2_median\"] if \"faces_cv2_median\" in df.columns else \"\",\n",
    "                                                           item[\"faces\"] if \"faces\" in df.columns else \"\"))\n",
    "        if (col == max_cols -1): plt.show()\n",
    "        idx = idx + 1\n",
    "        if (max_items > 0 and idx >=max_items): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"9\">References</a>\n",
    "\n",
    "* Deepfake,  [Wikipedia](https://en.wikipedia.org/wiki/Deepfake)  \n",
    "* Google DeepFake Database, [Endgadget](https://www.engadget.com/2019/09/25/google-deepfake-database/) \n",
    "* A quick look at the first frame of each video,  [from Kaggle](https://www.kaggle.com/brassmonkey381/a-quick-look-at-the-first-frame-of-each-video)  \n",
    "* Basic EDA Face Detection, split video, ROI, [from Kaggle](https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-roi)  \n",
    "* Face Detection with OpenCV, [from Kaggle](https://www.kaggle.com/serkanpeldek/face-detection-with-opencv)   \n",
    "* Play video and processing, [from Kaggle](https://www.kaggle.com/hamditarek/play-video-and-processing/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
