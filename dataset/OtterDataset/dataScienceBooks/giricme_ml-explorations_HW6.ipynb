{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1ae39ff",
      "metadata": {
        "id": "a1ae39ff"
      },
      "source": [
        "# 16-820: Advanced Computer Vision - HW6 - 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929e5a04",
      "metadata": {
        "id": "929e5a04"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this homework we will work with the state-of-the-art foundation model for segmentation, [SAM](http://segment-anything.com/)  - [Paper](http://ai.meta.com/research/publications/segment-anything/). Foundation models are large deep learning neural networks that are trained on massive datasets. SAM and other segmentation models use input prompts such as a box around an object to generate a mask of just the object. We will learn how to run off-the-shelf deep learning models and use them in your work. We will use 2D segmentation masks, combined with camera geometry and depth, to arrive at dense 3D point clouds from 2D segmentations.  The steps you will implement are:\n",
        "* Run SAM on a single image from the dataset.\n",
        "* Project the mask to 3D points in world coordinates.\n",
        "* For all the unseen views, do:\n",
        "    1. Project all points in world coordinates to the image frame.\n",
        "    2. Automatically generate a new input to SAM and run SAM\n",
        "    3. Project new mask to world coordinates and append to existing coordinates.\n",
        "* Filter the point cloud using an off-the-shelf filtering approach.\n",
        "\n",
        "Homework introduction video [here](https://youtu.be/rUQeJzlkfoU?si=udOHm5W5nRyy0WG8).\n",
        "To submit this homework to gradescope, please submit <strong>your code and the output of your code</strong>. E.g., for Q4 show the function you implemented and the visualizations created in the for loop. For Q1.1, give the K matrix you computed.\n",
        "\n",
        "Instructor: Matthew O'Toole \\\n",
        "OUT: November 21st, 2024 \\\n",
        "DUE: December 6th, 2024 \\\n",
        "TA's: Nikhil Keetha, Ayush Jain, Yuyao Shi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009c75fe",
      "metadata": {
        "id": "009c75fe"
      },
      "source": [
        "### Definitions\n",
        "\n",
        "A couple definitions that will hopefully avoid confusion:\n",
        "\n",
        "These are the existing `frames`:\n",
        "* Camera frame/OpenCV Camera Frame: This is the reference frame for 3D points with respect to the camera. This is the camera frame discussed in class.\n",
        "* Blender Camera Frame: This is the camera frame for 3D points used in Blender, the `y` and `z` axes point in opposite direction w.r.t. the OpenCV camera frame.\n",
        "* World Frame: This is the frame for 3D points with respect to the world origin. This frame differs by a rigid body transformation from any camera frame.\n",
        "* Image Frame: The <strong> 2D points </strong> in the image, e.g., `u`, `v` in range [0,H] and [0,W].\n",
        "\n",
        "If we refer to a `prompt` we mean the box around an object that is to be segmented, which is the input to SAM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0f0de6",
      "metadata": {
        "id": "1b0f0de6"
      },
      "source": [
        "### How to run this homework\n",
        "\n",
        "We will use deep neural networks which require a cuda-enabled graphics card with at least 12GB of VRAM. The easiest way to get access to this is Google Colab, press the button below to open this homework in Google Colab. You'll find a pretty useful tutorial on how to use Google Colab [here](https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj#scrollTo=cLkX9vxT2VBR)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021add91",
      "metadata": {
        "id": "021add91"
      },
      "source": [
        "### How to submit this homework\n",
        "\n",
        "1. You first press run all in the notebook and make sure that all plots come from your code, and are not the plots in the notebook by default.\n",
        "2. Then export as PDF, for the written version of the homework simply submit this and make sure to select all your results and code for the question.\n",
        "3. For the code, submit your iPython notebook file (.ipynb). We can use this to check your homework runs and gives the correct output. If we discover your code cannot reproduce the answer submitted in the written part, you will receive zero points for the question.\n",
        "4. No requirement on filenames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItzRb7u6ZqTq"
      },
      "source": [
        "## FAQs\n",
        "\n",
        "1. Hint: For Q3.1, remember the difference between the image frame (x horizontal), and the coordinates you might get from the mask. E.g., when you retrieve coordinates from the mask, they would not immediate align with the coordinates expected by the intrinsic matrices\n",
        "2. You should not have to modify the viz_pts_3d function.\n",
        "3. You should only call filter_points if thresh is not None, e.g. if thresh is not None: (call filter_points)\n",
        "4. Use another google account, or use Kaggle if you can't connect to Google colab.\n",
        "5. The function mask2cam should contain an if statement to check if thresh is None. If thresh is None, do not run filter_points().\n",
        "6. You should not have to change any of the given plotting functions, if you do there is probably an error in your code.\n",
        "7. In Q4, you should not have to change any code in the main for loop. Only the functions we have separated, i.e., cam2img, keep_dist, filter_for_box, prompt_points_to_box.\n",
        "8. Filter_for_box should not take in K as input, the input of the function is already in world coordinates\n",
        "9. You should not add a thresh argument in mask2cam in the for loop in Q4, this is by design.\n",
        "10. Filtering in filter_for_box should happen in world frame.\n",
        "11. If you are getting unexpected results in Q4, you might be doing the correction for the Blender frame wrong. Remember, we have OpenCV Frame <-> Blender Frame <-> World frame. The 'transforms' given are Blender Frame <-> World frame, make corrections accordingly.\n",
        "12. Another common source of error in Q4 is not getting the correct inverse of the transform. Hint: Google 'inverse of rigid body transforms'.\n",
        "13. To get a pdf for this homework, please follow the following steps:\n",
        "\n",
        "    - Download the python notebook\n",
        "    - Open it in jupyter notebook\n",
        "    - Download as html\n",
        "    - Save as pdf"
      ],
      "id": "ItzRb7u6ZqTq"
    },
    {
      "cell_type": "markdown",
      "id": "78100a0b",
      "metadata": {
        "id": "78100a0b"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/16820AdvancedCV/hw6/blob/main/HW6.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644532a8",
      "metadata": {
        "id": "644532a8"
      },
      "source": [
        "## Environment Set-up with Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fabfee",
      "metadata": {
        "id": "07fabfee"
      },
      "source": [
        "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a3beb144",
      "metadata": {
        "id": "a3beb144"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dd9a89",
      "metadata": {
        "id": "91dd9a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753c0b90-d742-4836-da40-e96cfe8858d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "Torchvision version: 0.20.1+cu121\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\n",
            "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.18.2)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.6)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d) (1.7)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.5)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d) (2.4.0)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.0.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.6)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d) (0.9.9)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (3.0.3)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (3.0.2)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.21.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-_pppu4zh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-_pppu4zh\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "mkdir: cannot create directory ‘ckpts’: File exists\n",
            "--2024-11-27 19:14:53--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.78, 13.226.210.15, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘ckpts/sam_vit_h_4b8939.pth.1’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   183MB/s    in 16s     \n",
            "\n",
            "2024-11-27 19:15:09 (154 MB/s) - ‘ckpts/sam_vit_h_4b8939.pth.1’ saved [2564550879/2564550879]\n",
            "\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K375xNjWAwZ7kmhjTuJccC3y6Ik5tN4q\n",
            "From (redirected): https://drive.google.com/uc?id=1K375xNjWAwZ7kmhjTuJccC3y6Ik5tN4q&confirm=t&uuid=14f1c0d9-6e2a-4367-a549-1e4d2fb390da\n",
            "To: /content/images.zip\n",
            "100% 118M/118M [00:01<00:00, 106MB/s]\n",
            "Archive:  images.zip\n",
            "replace images/data_demo.gif? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "if using_colab:\n",
        "    # install everything\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib os\n",
        "    ! pip install open3d\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir ckpts\n",
        "    !wget -P ckpts https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "    ! pip install gdown\n",
        "    ! gdown 1K375xNjWAwZ7kmhjTuJccC3y6Ik5tN4q #download dataset from gdrive\n",
        "    ! unzip images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDZh0aIDZqTr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "img_size = 400\n",
        "Image(filename=\"images/data_demo.gif\", width=img_size, height=img_size)"
      ],
      "id": "SDZh0aIDZqTr"
    },
    {
      "cell_type": "markdown",
      "id": "022d462d",
      "metadata": {
        "id": "022d462d"
      },
      "source": [
        "## Environment Set-up without Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dfe550a",
      "metadata": {
        "id": "6dfe550a"
      },
      "source": [
        "If you're not running on Google Colab, use the [prep_no_colab.sh](prep_no_colab.sh) script to install the right libraries, pull the model checkpoint and download the data. This script was tested on Ubuntu Linux only. After running the script your folder should look something like this:\n",
        "```\n",
        "├── images\n",
        "│   ├── dataset\n",
        "│   │   ├── train\n",
        "|   |   ├── test\n",
        "|   |   ├── val\n",
        "│   │   ├── transforms_train.json\n",
        "│   │   ├── transforms_test.json\n",
        "│   │   ├── transforms_val.json\n",
        "├── ckpts\n",
        "│   ├── sam_vit_h_4b8939.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-AhAxC9ZqTr"
      },
      "source": [
        "Our recommended method for loading iPython notebooks on your local computer is to use a Visual Studio Code plugin, [here](https://code.visualstudio.com/docs/datascience/jupyter-notebooks) is a short tutorial on how to do that. Are you having issues setting up your system? Problems with Cuda versions? Use Colab instead, or ask a TA if you really want to use your own compute."
      ],
      "id": "a-AhAxC9ZqTr"
    },
    {
      "cell_type": "markdown",
      "id": "0be845da",
      "metadata": {
        "id": "0be845da"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33681dd1",
      "metadata": {
        "id": "33681dd1"
      },
      "source": [
        "Necessary imports and helper functions for displaying points, boxes, and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b28288",
      "metadata": {
        "id": "69b28288"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sys\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29bc90d5",
      "metadata": {
        "id": "29bc90d5"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    # This function is used to visualize the mask on the image in a matplotlib axis.\n",
        "    # bool mask: (H, W). True for each pixel that belongs to the object.\n",
        "    # ax: matplotlib axis\n",
        "    # random_color: if True, use a random color for the mask. Otherwise, use blue.\n",
        "\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    # This function is used to visualize the bounding box on the image in a matplotlib axis.\n",
        "    # box: (4,) array. [x0, y0, x1, y1]\n",
        "    # (x0, y0): top-left corner\n",
        "    # (x1, y1): bottom-right corner\n",
        "    # ax: matplotlib axis\n",
        "\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23842fb2",
      "metadata": {
        "id": "23842fb2"
      },
      "source": [
        "## Q1: Loading and Understanding the Dataset [2 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj1eg7swZqTs"
      },
      "source": [
        "The dataset you'll be working with is a synthetic dataset generated specifically for this homework. We used the free and open-source 3D graphics software tool [Blender](https://www.blender.org/) to render images from 100 different poses. You will have access to the following data:\n",
        "* The intrinsics parameters, constant for all images. The dataset was rendered using a pinhole camera model without distortion. Therefore, the intrinsics can be captured using camera matrix K.\n",
        "* The extrinsics, as a [100 x 4 x 4] array. Each [4 x 4] matrix gives the cam2world transformation.\n",
        "* File path to the 100 images, each of shape [800 x 800 x 3].\n",
        "* 100 depth images, each of shape [800 x 800 x 3]. Each channel has the depth in meters, so 2 channels are redundant.\n",
        "\n",
        "We will now load and visualize the dataset.\n"
      ],
      "id": "pj1eg7swZqTs"
    },
    {
      "cell_type": "markdown",
      "id": "b467b12c",
      "metadata": {
        "id": "b467b12c"
      },
      "source": [
        "#### Q1.1 Compute the camera matrix K [2 pts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c659b851-3165-4f4f-8b0e-7155226b914a",
      "metadata": {
        "id": "c659b851-3165-4f4f-8b0e-7155226b914a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# dataset class provided to load extrinsics, intrinsics and image paths.\n",
        "class Dataset:\n",
        "    def __init__(self, json):\n",
        "        self.json = json # the json file containing the extrinsics, intrinsics and image paths.\n",
        "        self.load_extrinsics()\n",
        "        self.load_intrinsics()\n",
        "        self.compute_intrinsics()\n",
        "\n",
        "    def load_extrinsics(self):\n",
        "        # This function loads the extrinsics parameters from the json file.\n",
        "\n",
        "        with open(self.json) as f:\n",
        "            self.data = json.load(f)\n",
        "        self.frames = self.data['frames'] # 'frames' in the json contains the extrinsics and image path for each image.\n",
        "        self.transforms = np.array([frame['transform_matrix'] for frame in self.frames]) # extrinsic matrix for each image, shape (N, 4, 4)\n",
        "        self.file_paths = np.array([frame['file_path'] for frame in self.frames]) # path to each image, shape (N,)\n",
        "\n",
        "    def load_intrinsics(self):\n",
        "        # This function loads the intrinsics parameters from the json file.\n",
        "        self.f_x = self.data['fl_x'] # focal length in x\n",
        "        self.f_y = self.data['fl_y'] # focal length in y\n",
        "        self.w = self.data['w'] # image width\n",
        "        self.h = self.data['h'] # image height\n",
        "        self.cx = self.data['cx'] # principal point in x\n",
        "        self.cy = self.data['cy'] # principal point in y\n",
        "\n",
        "    def compute_intrinsics(self):\n",
        "        self.K = None # K: the intrinsic matrix, shape (3, 3)\n",
        "        # compute the K matrix form the intrinsic parameters computed in load_intrinsics() : [2 pts]\n",
        "        self.K = np.array([[self.f_x, 0, self.cx],[0, self.f_y, self.cy],[0, 0, 1]])\n",
        "\n",
        "\n",
        "dataset = Dataset('images/dataset/transforms_train.json') # load the dataset\n",
        "np.set_printoptions(precision=3, suppress=True) # do NOT remove this line when you print matrices for grading\n",
        "\n",
        "print('Shape of extrinsic matrices: {}'.format(dataset.transforms.shape)) # all extrinsic matrices, shape (N, 4, 4)\n",
        "#TODO: print the intrinsic matrix and add to your gradescope submission.\n",
        "print('K matrix {}'.format(dataset.K)) # The intrinsic matrix K you computed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77956430",
      "metadata": {
        "id": "77956430"
      },
      "source": [
        "#### Visualize the dataset [0 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35efab91",
      "metadata": {
        "id": "35efab91"
      },
      "source": [
        "Here we show the RGB and depth data that are part of the dataset. Reasoning about algorithm design is often easier when you understand the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c2e4f6b",
      "metadata": {
        "id": "3c2e4f6b"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(os.path.join('images/dataset',dataset.file_paths[0]))\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30125fd",
      "metadata": {
        "id": "e30125fd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('on')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d72f51f-a748-487a-b7eb-d1ace30a4abb",
      "metadata": {
        "id": "8d72f51f-a748-487a-b7eb-d1ace30a4abb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def depthmap_viz(depth,min_d=0.0,max_d=3.5):\n",
        "    # depth: (H,W,3) - depth map, every channel contains the same depth values for that pixel. 2 channels are redundant.\n",
        "\n",
        "    # min_d: minimum depth value to visualize\n",
        "    # max_d: maximum depth value to visualize\n",
        "\n",
        "    depth = np.clip(depth,min_d,max_d)\n",
        "\n",
        "    depth = (depth-min_d)/(max_d - min_d)\n",
        "\n",
        "    image = depth\n",
        "\n",
        "    plt.clf()\n",
        "    plt.imshow(depth,cmap='magma', vmin=min_d,vmax=max_d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6750b66-0e84-442c-83cd-99591983c9a2",
      "metadata": {
        "id": "a6750b66-0e84-442c-83cd-99591983c9a2"
      },
      "outputs": [],
      "source": [
        "depth_location = 'images/dataset/train/depth.npy' # location of ground truth depth maps.\n",
        "depths = np.load(depth_location) # load the depth maps\n",
        "\n",
        "depthmap_viz(depths[0]) # visualize the first depth map\n",
        "plt.show() # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b228b8",
      "metadata": {
        "id": "98b228b8"
      },
      "source": [
        "#### Loading SAM [0 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1927b",
      "metadata": {
        "id": "0bb1927b"
      },
      "source": [
        "The Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.\n",
        "\n",
        "Here we load the SAM model and predictor. Running on CUDA and using the default model are recommended for best results. Do not change any settings, as it will complicate our grading, you may not receive full credit if you alter the SAM settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e28150b",
      "metadata": {
        "id": "7e28150b"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"ckpts/sam_vit_h_4b8939.pth\" # the checkpoint loaded in the setup section.\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\" # loading to GPU.\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723a7d73-0a39-45e2-a671-199ff3b6c6c6",
      "metadata": {
        "id": "723a7d73-0a39-45e2-a671-199ff3b6c6c6"
      },
      "source": [
        "## Q2: Designing our prompt [2 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61ca7ac",
      "metadata": {
        "id": "d61ca7ac"
      },
      "source": [
        "SAM and other segmentation models use input prompts such as a box around an object to generate a mask of the object. From now on if we refer to a `prompt` we mean the box around an object that is to be segmented, which is the input to SAM.\n",
        "\n",
        "In this homework we will start by acquiring a single user-generated `prompt` in one image: a box around the coffee mug. We will then use depth and camera geometry to propogate the mask to other frames. It is therefore important for the one user-specified prompt to be high-quality. Set the `input_box` parameter such that we can get a high-quality segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a47080d0-009e-407c-97b4-d95a4ddfcbbe",
      "metadata": {
        "id": "a47080d0-009e-407c-97b4-d95a4ddfcbbe"
      },
      "source": [
        "Now we load in the first example image into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea92a7b",
      "metadata": {
        "id": "8ea92a7b"
      },
      "outputs": [],
      "source": [
        "#TODO: YOUR CODE HERE\n",
        "# add this plot to gradescope submission.\n",
        "input_box = np.array([365, 275, 425, 335])  # choose correct bounding box [2 pts]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_box(input_box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b35a8814",
      "metadata": {
        "id": "b35a8814"
      },
      "outputs": [],
      "source": [
        "# Here we're running SAM on the image with the bounding box.\n",
        "predictor.set_image(image) # loading the image to the predictor.\n",
        "masks, _, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_box[None, :],\n",
        "    multimask_output=False,\n",
        ")\n",
        "# Calling the predictor with the bounding box.\n",
        "# You will not need to change any of the other arguments in this homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f515d0a6",
      "metadata": {
        "id": "f515d0a6"
      },
      "source": [
        "#### Visualizing the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984b79c1",
      "metadata": {
        "id": "984b79c1"
      },
      "outputs": [],
      "source": [
        "mask = masks[0]\n",
        "h, w = mask.shape[-2:]\n",
        "mask_image = mask.reshape(h, w, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_mask(mask, plt.gca())\n",
        "show_box(input_box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "475ca765-85bb-4622-89ef-329f62be00b2",
      "metadata": {
        "id": "475ca765-85bb-4622-89ef-329f62be00b2"
      },
      "source": [
        "## Q3: Project to 3D  [20 points]\n",
        "\n",
        "As discussed before, the aim of this homework is to use the image mask in one image and propagate it to novel views. In this section we will use the mask generated in the previous question, and project the pixels in the mask to 3D coordinates using depth and camera geometry. Remember we can project points in image coordinates to 3D points by using the P matrix, with P = K[R|t]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa95f1",
      "metadata": {
        "id": "54aa95f1"
      },
      "source": [
        "## Q3.1: Image frame to camera frame [10 pts]\n",
        "\n",
        "In this part of the question you will be asked to project the points from image frame, so pixel coordinates, to a point cloud in the camera frame. For this you will only need the intrinsic matrix K and the depth. You will not need dataset.transforms in Q2.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2c909e-d329-4472-b18e-ee22d9006da5",
      "metadata": {
        "id": "5b2c909e-d329-4472-b18e-ee22d9006da5"
      },
      "outputs": [],
      "source": [
        "def img2cam(points, K,depths=None):\n",
        "    # project the points from image coordinates to camera coordinates [5 pts]\n",
        "    cam_3d = None\n",
        "    # (1) Use the intrinsic matrix K to convert the points from image coordinates to a point cloud in the camera frame.\n",
        "    # (2) Normalize the points to a plane with z=1.\n",
        "    # (3) Use depths to scale the points to be at the correct distance from the camera.\n",
        "    if depths is None:\n",
        "        depths = np.ones(points.shape)\n",
        "    homo_points = np.hstack((points, depths))\n",
        "    cam_3d =(K @ homo_points.T).T\n",
        "    cam_3d = cam_3d / cam_3d[:, 2][:, None]\n",
        "    print(f\"{cam_3d.shape}\")\n",
        "    return cam_3d\n",
        "\n",
        "def filter_points(coords,depths,thresh=2.55):\n",
        "    # filter out points that are too far away in the first mask, this first mask will be very important! [2 pts]\n",
        "    # return filtered coords and depths\n",
        "    # don't make it too complicated, this should be a one-liner.\n",
        "    return np.argwhere(depths > thresh)\n",
        "\n",
        "def mask2cam(mask, K, depths,thresh=None):\n",
        "    # project mask points to camera frame [3 pts]\n",
        "    # steps todo:\n",
        "    # (1) get all coordinates where the mask is True, this should be N x 2\n",
        "    # (2) get the depth values for these coordinates, Nx1\n",
        "    # (3) call filter_points to filter out points that are too far away, with depth above the treshold.\n",
        "    # only call filter_points if thresh is not None\n",
        "    # Here far away means the depth is above a certain threshold.\n",
        "    # (4) call img2cam to convert the points to camera frame using intrinsics and depth.\n",
        "    print(f\"{mask.shape}\")\n",
        "    print(f\"{depths.shape}\")\n",
        "    mask_coords = np.delete(np.argwhere(mask > 0), -1, axis=1)\n",
        "    print(f\"{mask_coords.shape}\")\n",
        "    mask_depths = np.take(depths, mask_coords)\n",
        "    print(f\"{mask_depths.shape}\")\n",
        "    if thresh:\n",
        "        filtered_points = filter_points(mask_coords, mask_depths, thresh)\n",
        "    else:\n",
        "        filtered_points = mask_coords, mask_depths\n",
        "    print(f\"{filtered_points.shape}\")\n",
        "    return img2cam(filtered_points, K, mask_depths[filtered_points[:, 0]][:, 0][:, None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e555af04-62a4-48a0-9d7c-dcae37c1244b",
      "metadata": {
        "id": "e555af04-62a4-48a0-9d7c-dcae37c1244b"
      },
      "outputs": [],
      "source": [
        "cam_pnts_3d = mask2cam(mask_image,dataset.K,depths[0],thresh=2.55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33915abf-f765-40fe-8c43-27a78560dedc",
      "metadata": {
        "id": "33915abf-f765-40fe-8c43-27a78560dedc"
      },
      "outputs": [],
      "source": [
        "def viz_pts_3d(pts,xrange=None,yrange=None,zrange=None,title=None):\n",
        "    # viz the 3D points\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(pts[0,:],pts[1,:],pts[2,:],s=1)\n",
        "    ax.set_xlabel('X [m]')\n",
        "    ax.set_ylabel('Y [m]')\n",
        "    ax.set_zlabel('Z [m]')\n",
        "\n",
        "    if xrange is not None:\n",
        "        ax.set_xlim(xrange)\n",
        "    if yrange is not None:\n",
        "        ax.set_ylim(yrange)\n",
        "    if zrange is not None:\n",
        "        ax.set_zlim(zrange)\n",
        "\n",
        "    if title is not None:\n",
        "        ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "np.save('cam_pnts_3d.npy',cam_pnts_3d)\n",
        "\n",
        "#TODO: add this plot to gradescope submission\n",
        "viz_pts_3d(cam_pnts_3d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38822f88",
      "metadata": {
        "id": "38822f88"
      },
      "source": [
        "## Q3.2: Camera frame to world frame [10 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8442a237",
      "metadata": {
        "id": "8442a237"
      },
      "source": [
        "We now project the points in the camera frame to the world frame, keep in mind that the transforms provided are between the world frame and the Blender camera frame (pictured below). You will need to take this difference into account when using the equations of projective geometry. For example, the intrinsics matrix `K` will expect 3D points in the OpenCV camera frame. Summarizing, these are the existing `frames`:\n",
        "* Camera frame/OpenCV Camera Frame: This is the reference frame for 3D points with respect to the camera, as seen in the class up to now.\n",
        "* Blender Camera Frame: This is the camera frame for 3D points used in Blender, the `y` and `z` axes are pointing in opposite directions w.r.t. OpenCV frame.\n",
        "* World Frame: This is the frame for 3D points with respect to the world origin.\n",
        "* Image Frame: The <strong> 2D points </strong> in the image, e.g., `u`, `v` in range [0,H] and [0,W]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc582fa",
      "metadata": {
        "id": "2fc582fa"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"images/cam_frames.png\", width=img_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f95cb9-6daf-494e-9e98-e922048dc842",
      "metadata": {
        "id": "a6f95cb9-6daf-494e-9e98-e922048dc842"
      },
      "outputs": [],
      "source": [
        "def cam2world(points,transform):\n",
        "    # project camera coordinates to world coordinates [5 pts]\n",
        "    # NOTE: transform is the transformation from the blender camera frame to the world frame.\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "    return None\n",
        "\n",
        "def world2cam(points,transform):\n",
        "    # project world coordinates to camera coordinates [5 pts]\n",
        "    # NOTE: do not use np.linalg.inv to compute the inverse of transform, we will award only partial credit.\n",
        "    # There is an intuitive and elegent way to compute the inverse of transform.\n",
        "    # NOTE: do not forget about blender coordinates!\n",
        "\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd86607e",
      "metadata": {
        "id": "cd86607e"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    # This function is used to visualize the mask on the image in a matplotlib axis.\n",
        "    # bool mask: (H, W). True for each pixel that belongs to the object.\n",
        "    # ax: matplotlib axis\n",
        "    # random_color: if True, use a random color for the mask. Otherwise, use blue.\n",
        "\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    # This function is used to visualize the bounding box on the image in a matplotlib axis.\n",
        "    # box: (4,) array. [x0, y0, x1, y1]\n",
        "    # ax: matplotlib axis\n",
        "\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a4dc72-4fdc-4307-b9ad-2c13c03174f3",
      "metadata": {
        "id": "e5a4dc72-4fdc-4307-b9ad-2c13c03174f3"
      },
      "outputs": [],
      "source": [
        "transform_0 = dataset.transforms[0]\n",
        "\n",
        "world_pts = cam2world(cam_pnts_3d,transform_0)\n",
        "\n",
        "np.save('world_pts.npy',world_pts)\n",
        "#TODO: add this plot to gradescope submission\n",
        "viz_pts_3d(world_pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2600409c",
      "metadata": {
        "id": "2600409c"
      },
      "source": [
        "## Q4: Casting masks to new frames [10 pts]\n",
        "\n",
        "Now we will look at new viewpoints and extract their point clouds. To do this we will loop through new viewpoints one by one, to run SAM on each new image. Each iteration of the loop we add the new 3D points to the previous 3D points, which are then used in the next iteration to create a new mask using projective geometry and depth.\n",
        "\n",
        "A simple approach might be to deploy the projective geometry we have developed, and find the bounding box at each iteration based on the coordinate range of the projected point cloud. That is, we project all 3D points to the novel view, an array of [N,2] with each row an x and y coordinate in the image frame. The bounding box could be simply [min_x,min_y,max_x,max_y]. What would be the problem with an approach like this?\n",
        "\n",
        "Occlussions and noise! Noise can be from some pixels that weren't segmented correctly, occlussions can cause entirely errorneous masks. You will be implementing several functions to improve the results: \\\n",
        "(1) Filter out masks that have a confidence that's too low \\\n",
        "(2) Filter out points that are too far away from anything else we've seen so far. \\\n",
        "(3) Carefully choose the bounding box dimensions to avoid noise to have a big effect on the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67329e8",
      "metadata": {
        "id": "c67329e8"
      },
      "outputs": [],
      "source": [
        "def cam2img(points, K):\n",
        "    # project camera coordinates to image coordinates [3 pts]\n",
        "    # output should be pixel coordinates in the correct range with shape (2, N)\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "212047f1",
      "metadata": {
        "id": "212047f1"
      },
      "outputs": [],
      "source": [
        "# (1) Filter out masks that have a confidence that's too low. [0 pts]\n",
        "\n",
        "score_thresh = 0.85\n",
        "def keep_score(score):\n",
        "    return score > score_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3853414",
      "metadata": {
        "id": "d3853414"
      },
      "outputs": [],
      "source": [
        "# (2) Filter out points that are too far away from anything else we've seen so far.\n",
        "\n",
        "dist_thresh = 0.3 # decide on a good distance threshold [2 pts]\n",
        "\n",
        "def keep_dist(new_pts,existing_pts):\n",
        "    # TODO: YOUR CODE HERE\n",
        "    # compute median of all_world_pts\n",
        "    # compute distance between tmp_world_pts and median of all_world_pts\n",
        "    # reject outliers that are too far away from all other points\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ed70d4",
      "metadata": {
        "id": "b5ed70d4"
      },
      "outputs": [],
      "source": [
        "# (3) Carefully choose the bounding box dimensions to avoid noise to have a big effect on the result.\n",
        "\n",
        "# filter out points n_std away from mean of all points [3 pts]\n",
        "def filter_for_box(world_points,transform, n_std=2):\n",
        "    # TODO: YOUR CODE HERE\n",
        "    # compute mean and std of all points [2 pts]\n",
        "    # filter out points that are n_std away from mean of all points [3 pts]\n",
        "    # transform to cam frame [1 pt]\n",
        "    # you will need intrinscis matrix K here, simply call dataset.K\n",
        "\n",
        "    return None\n",
        "\n",
        "# based on the filtered points, compute the bounding box [2 pts]\n",
        "def prompt_points_to_box(prompt):\n",
        "    # TODO: YOUR CODE HERE\n",
        "    # output: np.array([x0, y0, x1, y1]])\n",
        "    # (x0, y0): top-left corner\n",
        "    # (x1, y1): bottom-right corner\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc68d70",
      "metadata": {
        "id": "6cc68d70"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import copy\n",
        "from sklearn.cluster import KMeans\n",
        "all_world_pts = copy.deepcopy(world_pts)\n",
        "\n",
        "it = 1\n",
        "\n",
        "# show_its = [1,4,10]\n",
        "show_its = [1,4,10,50,75,99]\n",
        "\n",
        "end_idx = -1 # set this to a different number, e.g. 10, for faster debugging\n",
        "\n",
        "#TODO: add all plots generated to gradescope submission\n",
        "for transform, file_path, depth in zip(dataset.transforms[1:end_idx], dataset.file_paths[1:end_idx], depths[1:end_idx]):\n",
        "    # compute 3d points\n",
        "    image = cv2.imread(os.path.join('images/dataset/', file_path))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    prompt_points = filter_for_box(all_world_pts,transform) # (3) Carefully choose the bounding box dimensions to avoid noise to have a big effect on the result.\n",
        "\n",
        "    if it in show_its:\n",
        "        plt.imshow(image)\n",
        "        # scatter plot the img_pts\n",
        "        plt.scatter(prompt_points[0,:],prompt_points[1,:],s=5)\n",
        "        plt.title('It {}, prompt points.'.format(it))\n",
        "        plt.show()\n",
        "\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    prompt_box = prompt_points_to_box(prompt_points) # (3) Choose the bounding box points based on the filtered points.\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        box=prompt_box,\n",
        "        point_labels=[1],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "    if it in show_its:\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.imshow(image)\n",
        "        show_mask(masks, plt.gca())\n",
        "        show_box(prompt_box, plt.gca())\n",
        "        plt.axis('off')\n",
        "        plt.title('It {}, mask.'.format(it))\n",
        "        plt.show()\n",
        "\n",
        "    mask = masks.reshape((h, w, 1))\n",
        "    cam_pnts_3d = mask2cam(mask,dataset.K,depth)\n",
        "    tmp_world_pts = cam2world(cam_pnts_3d,transform)\n",
        "\n",
        "    if keep_score(scores): # (1) Filter out masks that have a score that's too low.\n",
        "        tmp_world_pts = keep_dist(tmp_world_pts,all_world_pts) # (2) Filter out points that are too far away from anything else we've seen so far.\n",
        "        all_world_pts = np.hstack([all_world_pts,tmp_world_pts])\n",
        "\n",
        "    if it in show_its:\n",
        "        viz_pts_3d(all_world_pts,title='It {}, all points found so far.'.format(it),xrange=[-0.1,0.1],yrange=[-0.45,-0.25],zrange=[0.06,0.15])\n",
        "\n",
        "    it+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c160e214",
      "metadata": {
        "id": "c160e214"
      },
      "source": [
        "## Visualize all 3D points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "730ce9ab",
      "metadata": {
        "id": "730ce9ab"
      },
      "outputs": [],
      "source": [
        "#TODO: do NOT need to add to gradescope submission\n",
        "viz_pts_3d(all_world_pts,xrange=[-0.1,0.1],yrange=[-0.45,-0.25],zrange=[0.06,0.15])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a4c771",
      "metadata": {
        "id": "a5a4c771"
      },
      "source": [
        "## Q5: Statistical outlier removal [3 pts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fccfa5",
      "metadata": {
        "id": "c7fccfa5"
      },
      "outputs": [],
      "source": [
        "def filter_points(points,nb_neighbors=20,std_ratio=2.0):\n",
        "    import open3d as o3d\n",
        "    # filter points using open3d statistical outlier removal. [3 pts]\n",
        "    # TODO: YOUR CODE HERE\n",
        "\n",
        "all_world_pts_filtered = filter_points(all_world_pts[:3,:])\n",
        "\n",
        "#TODO: add this plot to gradescope submission\n",
        "viz_pts_3d(all_world_pts_filtered,xrange=[-0.1,0.1],yrange=[-0.45,-0.25],zrange=[0.06,0.15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnq7RgrVZqTy"
      },
      "source": [
        "### Expected Output\n",
        "\n",
        "Below is the output we achieved at the end of the homework, you implementation should be similar to receive full credit."
      ],
      "id": "Pnq7RgrVZqTy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx-2msXHZqTy"
      },
      "outputs": [],
      "source": [
        "Image(filename=\"images/expected_output.png\", width=img_size, height=img_size)"
      ],
      "id": "hx-2msXHZqTy"
    },
    {
      "cell_type": "markdown",
      "id": "a4a24934",
      "metadata": {
        "id": "a4a24934"
      },
      "source": [
        "## Q6 Extra credit: 3D Segmentation without Ground Truth Depth [10 pts Max]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40303ce6",
      "metadata": {
        "id": "40303ce6"
      },
      "source": [
        "So far we have provided you with ground truth depth from the 3D rendering toolbox. For a maximum of 10 extra points, can you achieve similar accuracy without using ground truth depth? You can use any toolbox/repository you like, as long as they infer dense depth maps: depth for every pixel in the image. Here is one approach we believe would be relatively straightforward:\n",
        "* The dataset is formatted for Neural Radiance Fields (NeRFs). You should be able to run NeRF on this dataset with little modifications.\n",
        "* Suggested NeRF pipeline: [Torch-NGP](https://github.com/ashawkey/torch-ngp). It runs fast using a cuda backend, but all the high-level features are implemented in Torch.\n",
        "* Some necessary changes: (1) Modify the code to only use the training data, no testing or validation (not provided in dataset). You could also copy training data to a test and validation folder and create the necessary json files. (2) Modify the code to return all *train* depth maps in a `.npy` array, in units [meters].\n",
        "\n",
        "Any other methods are allowed and encouraged, as long as they infer dense depth maps: depth for every pixel in the image. Keep in mind the difference between z-depth as in Blender (the distance along the cameras principle axis), depth as euclidean distance from the camera center."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}