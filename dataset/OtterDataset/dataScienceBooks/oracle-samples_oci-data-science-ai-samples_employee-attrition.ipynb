{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Oracle Cloud Infrastructure Data Science Demo Notebook\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.<br>\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <font> Predicting Employee Attrition with ADS</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> OCI Data Science PM Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "In this notebook, we will be using an employee attrition dataset. We will start by doing an exploratory data analysis (EDA) to understand the data. Then a model will be trained using `AutoML`. The model will be used to make predictions and evaluate the model to determine how well it generalizes to new data. Then we will do use machine learning explainability (`MLX`) to understand the global and local model behavior. This will all be done using Oracle's Accelerated Data Science, (`ADS`) library.\n",
    "\n",
    "## Other Public Technical Resources \n",
    "\n",
    "- [Deploying a Machine Learning Model with Oracle Functions](https://blogs.oracle.com/datascience/deploying-a-machine-learning-model-with-oracle-functions) End to end demo with instructions on how to build, train, and deploy a machine learning model on OCI using Data Science, Cloudshell and Oracle Functions. \n",
    "- [A simple Guide to Leveraging Parallelization for Machine Learning Tasks](https://blogs.oracle.com/datascience/parallelization-machine-learning) This blog post covers a few options that are available to a data scientist who wants to parallelize a workload done on a data frame. It covers approaches that offer multi-threading and multiprocessing execution. Each method provides benchmarks in terms of speed of execution that you can run in your notebook session.\n",
    "- [Running Python Processes/jobs in the Notebook Session Environment](https://blogs.oracle.com/datascience/execute-a-python-process-in-the-oracle-cloud-infrastructure-data-science-notebook-session-environment)\n",
    "- [Using Resource Principals in the Data Science service](https://blogs.oracle.com/datascience/resource-principals-data-science-service) \n",
    "- [Build and Deploy a Model in 9 Minutes using ONNX on OCI](https://blogs.oracle.com/datascience/deploy-machine-learning-models-with-onnx) \n",
    "\n",
    "\n",
    "## Business Use:\n",
    "\n",
    "Organizations can face significant costs resulting from employee turnover. Some costs are tangible such as training expenses and the time it takes from when an employee starts to when they become a productive team member. Generally, the most important costs are intangible. Consider what is lost when a productive employee quits: corporate knowledge, new product ideas, great project management, and customer relationships. With advances in machine learning and data science, it's possible to not only predict employee attrition but to understand the key variables that influence turnover.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives:\n",
    "By the end of this tutorial, you will know how to:\n",
    " - <a href='#setup'>0. Setup</a> the required packages:\n",
    " - <a href='#data'>1. Open and Visualize Datasets using `ADS`</a>\n",
    "      - <a href='#binaryclassifition'>1.1. Binary Classification</a>\n",
    "      - <a href='#data'>1.2. The Dataset</a>\n",
    "      - <a href='#eda'>1.3. Exploratory Data Analysis</a> \n",
    "      - <a href='#viz'>1.4. Visualize the Dataset Object</a>\n",
    "      - <a href='#sss'>1.5. **(Optional)** Using Oracle Cloud Infrastructure Data Flow (serverless Spark service) to perform data transformations at scale</a>\n",
    "      - <a href='#trans'>1.6. Get and Apply Transformation Recommendations</a> \n",
    " - <a href='#model'>2. Building and Visualizing Models</a>\n",
    "      - <a href='#automl'>2.1. Oracle AutoML</a>\n",
    "      - <a href='#other_sources'>2.2. `ADS` Supports Models from other Sources</a> \n",
    " - <a href='#eval'>3. Evaluate Models using `ADSEvaluator`</a>\n",
    " - <a href='#explain'>4. Explain How the Models Work using `ADSExplainer`</a>\n",
    "      - <a href='#featurepermutation'>4.1. Feature Permutation Importance</a>\n",
    "           - <a href='#fpalgo'>4.1.1. Description of the algorithm</a>\n",
    "           - <a href='#fpinterpret'>4.1.2. Interpreting the output</a>\n",
    "      - <a href='#pdpice'>4.2. Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) Explanations</a>\n",
    "           - <a href='#pdpicealgo'>4.2.1. Description of the algorithm</a>\n",
    "           - <a href='#pdpiceinterpret'>4.2.2. Interpreting the output</a>\n",
    "      - <a href='#localexplanations'>4.3. Local Explanations</a>\n",
    "           - <a href='#localexplanationsalgo'>4.3.1. Description of the algorithm</a>\n",
    "           - <a href='#localexplanationsinterpret'>4.3.2. Interpreting the output</a>\n",
    " - <a href='#save'>5. Saving the model to the model catalog</a>\n",
    " - <a href='#appendix'>**(Optional)** Appendix: Deploy your Model to Oracle Functions</a>\n",
    "      - <a href ='#invoke'>Invoke your Deployed Model</a>\n",
    " - <a href='#conclusion'>6. Conclusion</a>\n",
    " \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do all of the imports necessary to get this notebook working up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "from os import path \n",
    "from os.path import expanduser\n",
    "from os.path import join\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.catalog.project import ProjectCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.data import MLData\n",
    "from ads.common.model import ADSModel\n",
    "from ads.dataflow.dataflow import DataFlow\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from ads.explanations.mlx_whatif_explainer import MLXWhatIfExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "import ads \n",
    "ads.set_auth(auth='resource_principal') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Open and Visualize the Attrition Dataset using `ADS`\n",
    "\n",
    "<a id='binaryclassifition'></a>\n",
    "### Binary Classification\n",
    "\n",
    "Binary classification is a technique of classifying observations into one of two groups. In this notebook, the two groups are those employees that will leave the organization and those that will not. \n",
    "\n",
    "Given the features in the data, the model will determine the optimal criteria for classifying an observation as leaving or not leaving. This optimization is based on the training data. However, we will holdout some of the data to test the model's preformance. Models can over-fit on the training data, that is learn the noise in a dataset and then it will not do a good job at predicting the results on new data (test data). Since we already know the truth for the data in the training dataset, we are really interested in how well it performs on the test data.\n",
    "\n",
    "<a id='data'></a>\n",
    "### The Dataset\n",
    "\n",
    "This is a fictional data set which contains 1,470 rows. There are 36 features. 22 features are ordinal, 11 are categorical, and 3 are constant values. The features include basic demographic information, compensation level, job characteristics, job satisfaction and employee performance metrics. The data is not balanced as fewer employees leave than stay.\n",
    "\n",
    "The first step is to load in the dataset. To do this the `DatasetFactory` singleton object will be used. It is part of the `ADS` library. It is a powerful class to work with datasets from different sources.\n",
    "\n",
    "<font color=gray>Datasets are provided as a convenience.  Datasets are considered Third Party Content and are not considered Materials under Your agreement with Oracle applicable to the Services.  You can access the `orcl_attrition` dataset license [here](oracle_data/UPL.txt). Dataset `orcl_attrition` is distributed under UPL license. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_path = \"https://objectstorage.us-ashburn-1.oraclecloud.com/n/bigdatadatasciencelarge/b/hosted-ds-datasets/o/synthetic%2Forcl_attrition.csv\"\n",
    "\n",
    "employees = DatasetFactory.open(attrition_path,\n",
    "      target=\"Attrition\").set_positive_class('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz'></a>\n",
    "### Visualize the Dataset Object\n",
    "\n",
    "The `show_in_notebook` method can be applied to the dataset itself. When this is done the following is produced:\n",
    "\n",
    "  - Summary, this shows a brief description of the dataset, shape, and a breakdown by feature type\n",
    "  - Feature summary, a visualization created on a dataset sample to give an idea of distribution for each feature.\n",
    "  - Correlations, a map which shows how every feature (numeric and categorical) are correlated\n",
    "  - Data preview, the first five rows of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.show_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.plot(\"MonthlyIncome\", y=\"JobRole\", plot_type='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.get_recommendations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show_in_notebook` method is used in many classes within `ADS`. It makes a best effort to display information that is meaningful to a data scientist about the object. Below, it is applied to the target variable, and it will show the relative frequency of the classes in the data. Since the target is `Attrition` here, False means people who did not leave, and True are people that do.\n",
    "\n",
    "It shows that the data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sss'></a>\n",
    "### (Optional) Using Oracle Cloud Infrastructure Data Flow to perform data transformations at scale\n",
    "\n",
    "You can skip this section and go directly to **Get and Apply Transformation Recommendations**. \n",
    "\n",
    "In this particular example, we have access to additional employee-level datasets that are stored on Oracle Object Storage. These datasets contain timestamp records of every time an employee either entered or exited the office using their keycard. We think that the number of events per day could be predictive of an employee attrition. Since the dataset is quite large, we are going to process this dataset using Data Flow, the serverless spark service on the Oracle Cloud Infrastructure. `ADS` is deeply integrated with Data Flow. Creating Data Flow applications and runs is very easy with `ADS`.\n",
    "\n",
    "Beyond the notebook environment you can use the Data Flow service to perform the data transformation using pyspark. We have a notebook example (dataflow.ipynb) which walks you through the details of creating Data Flow appliations and runs with ADS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just create a simple SparkSQL script to perform the aggregation. All the data files are stored on object storage. **Change the name of the file before running the cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_file_path = path.join(path.expanduser(\"~\"), \"dataflow\", \"employee-keycard-v25.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $pyspark_file_path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # create a spark session\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # load csv file from dataflow public storage\n",
    "    df = spark \\\n",
    "        .read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .load(\"oci://data-science-data-flow-data@bigdatadatasciencelarge/keycard_log.csv\")\n",
    "    \n",
    "    # create a temp view and do some sql operations\n",
    "    df.createOrReplaceTempView(\"keycard\")\n",
    "    query_result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            EmployeeNumber, \n",
    "            COUNT(*)\n",
    "        FROM keycard\n",
    "        GROUP BY EmployeeNumber\n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert the filtered Spark DataFrame into json format\n",
    "    # Note: we are writing to the spark stdout log so that we can retrieve the log later.\n",
    "    print('\\n'.join(query_result_df.toJSON().collect()))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFlow() instance: \n",
    "\n",
    "use_dataflow = False \n",
    "\n",
    "if use_dataflow: \n",
    "\n",
    "    data_flow = DataFlow()\n",
    "\n",
    "    # User would need to update the value assigned to display_name\n",
    "    display_name = \"<insert-your-application-name>\"\n",
    "    # User would need to update the value assigned to script_bucket\n",
    "    script_bucket = \"<insert-the-object-storage-bucket-name-for-your-script>\"\n",
    "    # User would need to update the value assigned to logs_bucket\n",
    "    logs_bucket = \"<insert-the-object-storage-bucket-name-for-the-logs\"  \n",
    "    \n",
    "    # The next step in the process is to prepare and create the application: \n",
    "    \n",
    "    app_config = data_flow.prepare_app(display_name,\n",
    "                                   script_bucket,\n",
    "                                   pyspark_file_path,\n",
    "                                   logs_bucket=logs_bucket,\n",
    "                                   driver_shape='VM.Standard2.4',\n",
    "                                   executor_shape='VM.Standard2.4',\n",
    "                                   num_executors=2)\n",
    "\n",
    "    app = data_flow.create_app(app_config)\n",
    "    \n",
    "    run_display_name = \"keycard_count\"               # User would need to update the value assigned to run_display_name\n",
    "\n",
    "    run_config = app.prepare_run(run_display_name, logs_bucket=logs_bucket)\n",
    "   \n",
    "    # run the application: \n",
    "    \n",
    "    run = app.run(run_config, save_log_to_local=False)\n",
    "    \n",
    "    # Fetch the logs: \n",
    "    \n",
    "    run.fetch_log(\"stdout\").save()\n",
    "    run.fetch_log(\"stderr\").save()\n",
    "    \n",
    "    # Let's now explore the data and merge the output of this sparksql script with the original `ADSDataset`: \n",
    "    \n",
    "    # the PySpark script wrote to the log as jsonL, and we read the log back as `ADS` dataset\n",
    "\n",
    "    keycards = DatasetFactory.open(pd.read_json((str(run.log_stdout)), lines=True))\n",
    "    keycards = keycards.rename(columns={\"count(1)\":\"keycard_counts\"})\n",
    "    keycards.show_in_notebook()\n",
    "    \n",
    "    # Let's merge the keycard data with the employee profile data\n",
    "    \n",
    "    ds = employees.merge(keycards, how='left',on=\"EmployeeNumber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>\n",
    "### Get and Apply Transformation Recommendations\n",
    "\n",
    "`ADS` can help with feature engineering by transforming datasets. For example, it can fix class imbalance by up or downsampling. This is just one example of the many transforms that `ADS` can apply. You can have `ADS` perform an analysis of the data and automatically perform the transformations that it thinks would improve the model. This is done with the `auto_transform()` method. The `suggest_recommendations()` method allows you to explore the suggested transforms using the notebook's UI and select the transformations that you would like it to make.\n",
    "\n",
    "All ADS datasets are immutable; any transforms that are applied result in a new dataset. In this example, the notebook will perform automatic transformations on the data, and it will also fix the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ds = ds.auto_transform(fix_imbalance=False)\n",
    "#ds.visualize_transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use a GUI-based approach with `suggest_recommendations()` as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.suggest_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "## 2. Building and Visualizing Models\n",
    "<a id='automl'></a>\n",
    "### Oracle AutoML\n",
    "\n",
    "The Oracle `AutoML` package automatically tunes a model class to produce the best models. It works with any supervised prediction task (e.g., classification or regression). It supports binary and multi-class classifications as well as regression problems. `AutoML` automates three major stages of the ML pipeline: feature selection, algorithm selection, and hyperparameter tuning. These pieces are combined into a pipeline which automatically optimizes the whole process with minimal user interaction.\n",
    "\n",
    "The Oracle Labs `AutoML` uses the `OracleAutoMLProvider` object to delegates the model training to `AutoML`.\n",
    "\n",
    "`AutoML` has a pipeline-level Python API that quickly jumpstarts the data science process with a quality tuned model. It selects the appropriate features and model class for a given prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the `ADS` `AutoML` driver is used to invoke the Oracle `AutoML` pipeline to tackle the ML problem. It will build and tune models with the `driver.AutoML`. In this example, it will create five models. The four trained models will be Logistic Regression, Light Gradient Boosting Machine, XGBoost, and a Random Forest. In addition, it will create a baseline model for comparison. The baseline model is a naive model that is used to confirm that the trained model learned something meaningful from the data. The baseline, for both classification and regression problems, is called the Zero Rule algorithm, which is also known as ZeroR or the null model. \n",
    "- For a regression modeling problem, the Zero Rule algorithm predicts the mean of the training dataset.\n",
    "- For a classification modeling problem, the Zero Rule algorithm predicts the class with the most observations in the training dataset.\n",
    "\n",
    "A machine learning model must demonstrate that is statistically significantly better at prediction than the baseline model. If not, the model has not learned anything meaningful from the data. The value here delivered by `ADS` is:\n",
    "\n",
    "- A baseline model is always computed with no extra effort by the data scientist.\n",
    "- A baseline model is always available for comparison with the `AutoML` models.\n",
    "- Multiple models can be trained and tuned with limited input from the data scientist. It will select:\n",
    "- ideal feature set\n",
    "- minimal sampling size\n",
    "- best model class to use\n",
    "- the best set of model-specific hyperparameters\n",
    "\n",
    "`ADS` also provides the ability to split a dataset into training and testing datasets using the `train_test_split` method, and `train` will train a set of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = transformed_ds.train_test_split()\n",
    "automl = AutoML(train, provider=OracleAutoMLProvider())\n",
    "model, baseline = automl.train(model_list=[\n",
    "    'LogisticRegression',\n",
    "    'LGBMClassifier',\n",
    "    'XGBClassifier',\n",
    "    'RandomForestClassifier'], min_features=['OverTime', 'JobLevel'], score_metric = \"roc_auc\", time_budget=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our accuracy on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scorer = get_scorer(\"accuracy\") # works with any sklearn scoring function\n",
    "\n",
    "print(\"Oracle AutoML accuracy on test data:\", model.score(test.X, test.y, score_fn = accuracy_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ADSModel` object wraps the model produced by the `AutoML` provider. It will delegate any attributes to the actual model, so we are able to look at any of the following: \n",
    "\n",
    "(✓ indicates it can be visualized)\n",
    "\n",
    "  - ranked_models_\n",
    "  - num_fs_evals_\n",
    "  - selected_features_names_\n",
    "  - selected_model_params_\n",
    "  - tuning_trials_ ✓\n",
    "  - adaptive_sampling_trials_ ✓\n",
    "  - feature_selection_trials_ ✓\n",
    "  - model_selection_trials_ ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.selected_model_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were four learned models (Logistic Regression, Light Gradient Boosting Machine, XGBoost, and a Random Forest). The `ranked_models_` attribute returns the one that performed the best based on the selected model scoring criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.ranked_models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the mean model score for each of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automl.visualize_algorithm_selection_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the relationship between the model's score and the number of features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automl.visualize_feature_selection_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the hyperparameter tuning by plotting the model score for the selected model versus the number of iterations. Generally, the model score will tend to increase as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automl.visualize_tuning_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other_sources'></a>\n",
    "### ADS Supports Models from other Sources\n",
    "\n",
    "Above, `AutoML` built a number of models including a Random Forest model. However, `ADS` is agnostic to the source of the model as it takes advantage of duck typing: something that looks like a model and walks like a model, is a model to ADS. Below, is an example of how to build a `sklearn` model and then use that with `ADS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dataframelabelencoder.py \n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DataFrameLabelEncoder(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.label_encoders = defaultdict(LabelEncoder)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype.name  in [\"object\", \"category\"]:\n",
    "                self.label_encoders[column] = OrdinalEncoder()\n",
    "                self.label_encoders[column].fit(X[column])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        for column, label_encoder in self.label_encoders.items():\n",
    "            X[column] = label_encoder.transform(X[column])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataframelabelencoder import DataFrameLabelEncoder\n",
    "\n",
    "X = train.X.copy()\n",
    "y = train.y.copy()\n",
    "\n",
    "le = DataFrameLabelEncoder()\n",
    "X = le.fit_transform(X)\n",
    "\n",
    "sk_clf = RandomForestClassifier(random_state=42)\n",
    "sk_clf.fit(X, y)\n",
    "\n",
    "sk_model = make_pipeline(le, sk_clf)\n",
    "\n",
    "# Build an ads model from the SVM classifier\n",
    "my_model = ADSModel.from_estimator(sk_model, \n",
    "                                   name=sk_clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sklearn accuracy on test data:\", my_model.score(test.X, test.y, score_fn = accuracy_scorer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "## Evaluate Models using `ADSEvaluator`\n",
    "\n",
    "One of the key advantages of `ADS` is the ability to quickly evaluate any models. ADS supports evaluating:\n",
    "\n",
    "- regression\n",
    "- binary classification\n",
    "- multiclass classification\n",
    "\n",
    "`ADS` supports the ability for you to provide your own evaluation function (given `y_true` and `y_pred` series) for any esoteric calculation that you would like to run.\n",
    "\n",
    "Below, we examine the plots that are commonly used to evaluate model performance. These include the precision-recall, ROC, lift, and gain plots. Each model under study is plotted together, allowing for easy comparison. In addition, the normalized confusion matrices are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ADSEvaluator(test, models=[model, my_model, baseline], \n",
    "                         training_data=train)\n",
    "evaluator.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of common metrics that are used to assess the quality of a model. `ADS` provides a convenient method to compare the models and highlights the model with the highest score in each metric.\n",
    "\n",
    "Note: `AutoML` does its optimization on the validation data, meaning that the `sklearn` `random-forest` model will perform better on the training data. Performance on training data doesn't tell us how the model performs on unseen data. You should look to performance on the `test` dataset to get an idea of which model is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary classification model can have one of four outcomes for each prediction. A true-negative is an outcome where the model correctly predicts the negative case, and a false-negative is an outcome where when the model incorrectly predicts the negative case. A false-positive is when the model incorrectly predicts the positive case, and a true-positive is when the model correctly predicts the positive case. However, not all false-positive and false-negatives have the same importance. For example, a cancer test has a higher cost when it incorrectly says that a patient does not have cancer when they do. The `calculate_cost` method allows the cost to be computed for each model based on the cost of each class of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.calculate_cost(tn_weight=1, fp_weight=3, fn_weight=2, tp_weight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='explanations'></a>\n",
    "# Model Explanations\n",
    "\n",
    "The remaining part of this tutorial demonstrates how we can use the `ADS` explanation module to help better understand the behavior of our trained model. We will first create the required `ADS` explainer objects and then begin generating global and local explanations. \n",
    "\n",
    "Some useful terms for machine learning explainability (MLX):\n",
    "  - **Explainability**: The ability to explain the reasons behind an machine learning model’s prediction.\n",
    "  - **Interpretability**: The level at which a human can understand the explanation.\n",
    "  - **Global Explanations**: Understand the general behavior of a machine learning model as a whole.\n",
    "  - **What-If**: Understand how the change in feature values for either one sample or an entire dataset affects model outcome. \n",
    "  - **Local Explanations**: Understand why the machine learning model made a specific prediction.\n",
    "  - **Model-Agnostic Explanations**: Explanations treat the machine learning model (and feature pre-processing) as a black-box, rather than using properties from the model to guide the explanation.\n",
    "\n",
    "The `ADS` explanation module provides interpretable, model-agnostic, local/global explanations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adsexplainer'></a>\n",
    "## ADSExplainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ADS` provides a general explainer object, `ADSExplainer`, which is used to generate both global and local explanations for machine learning models. `ADSExplainer` takes as input the datasets used to train and evaluate the model (e.g., train and test) and the model itself. Any type of model containing a `predict_proba()` or `predict()` function can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model explainer class\n",
    "explainer = ADSExplainer(test, model)\n",
    "\n",
    "# let's created a global explainer\n",
    "global_explainer = explainer.global_explanation(provider=MLXGlobalExplainer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What-if Scenarios \n",
    "\n",
    "Using the `ADSExplainer` object, we can create a \"WhatIf\" explanation object to generate model explanations. Oracle Labs WhatIf `MLX` is selected as the provider using the `MLXWhatIfExplainer` object. WhatIf explanation supports both explore sample and explore predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whatif_explainer = explainer.whatif_explanation(\n",
    "#                     provider=MLXWhatIfExplainer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Explorer \n",
    "\n",
    "The sample explorer API allows you to explore how a change applied to the feature values of a selected sample impacts the model prediction. You can modify a subset of the available features or all the features. The optional argument `features` lets you specify a list of features to explore while the optional parameter `max_features` lets you select the maximum number of features you want to evaluate. By default, all features are selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whatif_explainer.explore_sample(row_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions Explorer\n",
    "\n",
    "The predictions Explorer tool allows you to explore model predictions across either the marginal distribution (1-feature) or the joint distribution (2-feature) of the features in your train/validation/test dataset. The method `explore_predictions()` has several optional parameters including: \n",
    "\n",
    "* `feature`: the name of the feature to visualize \n",
    "* `label`: either the target label index or name to visualize \n",
    "* `plot_type`: `scatter`, `bar`, `box`.\n",
    "* `discretization`: method to discretize continuous features (Options are no discretization, quartile, decile, or percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whatif_explainer.explore_predictions('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whatif_explainer.explore_predictions('WorkLifeBalance', plot_type='box', discretization='decile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='global'></a>\n",
    "## Global Explanations\n",
    "\n",
    "We will start with generating global explanations for our model. \n",
    "\n",
    "Using the `ADSExplainer` object, we can create a global explanation object to generate global model explanations. Oracle Labs global `MLX` is selected as the provider using the `MLXGlobalExplainer` object. Global explanation supports both feature importance explanations and feature dependence explanations, such as Partial Dependence Plots (PDP) and Individual Conditional Expectations (ICE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the global feature importance explanation and visualize the top 6 features as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = global_explainer.compute_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.show_in_notebook(n_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a detailed scatter plot. This shows the distribution of the importance measure and provides a sense of the variation in the data. Five features will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importances.show_in_notebook(n_features=10, mode='detailed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detailed information used to generate the above plot is available with the `get_global_explanations` method. It returns an array of JSON structures. We will display the results for only one of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importances.get_global_explanation()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show what the model has learned\n",
    "\n",
    "It is great to have an expert with knowledge of what a model should do. However, this is often not available. Plus, models sometimes learn different things than what an expert would speculate should be learned. Generally, models learn important relationships between the data. For many machine learning models, it is difficult or almost impossible to understand what the model has learned. The `ADSExplainer` provides a powerful set of tools that provide the data scientist insight into what a complex model is doing. It does this by building other models and performing simulations on the model's predictions. From this, the `ADSExplainer` learns what has been learned. The `ADSExplainer` provides a powerful set of tools that will provide the data scientist insight into what a complex model is doing. It does this by building other models and performing simulations on the model's predictions. From this, the `ADSExplainer` learns what has been learned.\n",
    "\n",
    "When an explanation does not make sense, it does not mean the explanation is wrong. It is possible that the model has learned new relationships in the data. This allows `MLX` to be used to understand and debug the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Dependence Explanations (PDP & ICE)\n",
    "\n",
    "Next, we will generate global explanations to visualize how different values for the important features interact with the target variable. This is done through Partial Dependence Plots (PDP) and Individual Conditional Expectations (ICE) explanations. \n",
    "\n",
    "The following cell shows how to learn more about the PDP and ICE techniques used in the `MLXGlobalExplainer`. This provides a description of the algorithm and how to interpret the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_explainer.partial_dependence_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the PDP plot for the `OverTime` feature. From this, it can be seen that if an employee does `Overtime`, they are more likely to leave the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanation = explainer.global_explanation().compute_partial_dependence(\"OverTime\")\n",
    "#explanation.show_in_notebook(mode=\"pdp\", labels=[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ICE, the results for each sample can be seen for both the True and False cases (employee leaves or stays). This approach allows the data scientist to see the distribution of importance values. In this example, the output has been centered/pinned on its first prediction. Also, a median line is plotted to show the general trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanation.show_in_notebook(mode=\"ice\", centered=True, \n",
    "#                               show_distribution=True, \n",
    "#                               show_correlation_warning=True, \n",
    "#                               show_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, PDP is able to consider the interaction between multiple variables. In theory, any level of interaction can be used but practically only two-way interaction can be plotted without a significant amount of compute; therefore, it is generally limited to two variables. In this example, the feature importance will be determined between the `OverTime` and `JobLevel` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otjl_explanation = explainer.global_explanation().compute_partial_dependence(\n",
    "#    ['OverTime', 'JobLevel'])\n",
    "\n",
    "\n",
    "#otjl_explanation.show_in_notebook(\n",
    "#    show_distribution=True, show_correlation_warning=False, line_gap=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to the raw data can be obtained by converting it to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otjl_explanation.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='localexplanations'></a>\n",
    "## Local Explanations\n",
    "\n",
    "Global explanations inform the data scientist about the general trends in a model. They do not describe what is happening with a specific prediction. That is the role of local explanations. They are model-agnostic and provide insights into why a model made a specific prediction.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "local_explainer = explainer.local_explanation(provider=MLXLocalExplainer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = 4\n",
    "#(X, y) = test.X.iloc[sample:sample+1], test.y.iloc[sample:sample+1]\n",
    "\n",
    "#local_explainer.explain(X, y).show_in_notebook(labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and display a sample on which to perform a local explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our exploration of `ADS`, `AutoML`, and `MLX` features in the context of a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save'></a>\n",
    "# Saving the model to the model catalog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can save the simple random forest model in the model catalog, using the very flexible `prepare_generic_model()` function to save my model. That function creates an editable template artifact. The function `prepare_generic_model()` can support **any** model and **should always be the preferred way to save models from open source libraries**. \n",
    "\n",
    "`prepare_generic_model()` gives you complete control on the structure of the artifact and the definition fo the functions in `score.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import joblib \n",
    "\n",
    "# Path to artifact directory for my sklearn model: \n",
    "sklearn_path = \"./model-artifact/\"\n",
    "\n",
    "# Creating the artifact template files in the directory: \n",
    "sklearn_artifact = prepare_generic_model(sklearn_path, \n",
    "                                         function_artifacts=False, \n",
    "                                         data_science_env=True,\n",
    "                                         force_overwrite=True)\n",
    "\n",
    "# Creating a joblib pickle object of my random forest model: \n",
    "joblib.dump(sk_model, os.path.join(sklearn_path, \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make some changes to `score.py`, ensuring that `load_model()` reads in the `model.joblib` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting paths for artifact files that need to be modified: \n",
    "\n",
    "encoder_path = os.path.join(sklearn_path, \"dataframelabelencoder.py\")\n",
    "score_path = os.path.join(sklearn_path, \"score.py\")\n",
    "!cp dataframelabelencoder.py {encoder_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {score_path}\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from joblib import load\n",
    "import io \n",
    "import pandas as pd\n",
    "import logging \n",
    "\n",
    "# logging configuration - OPTIONAL \n",
    "logging.basicConfig(format='%(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger_pred = logging.getLogger('model-prediction')\n",
    "logger_pred.setLevel(logging.INFO)\n",
    "logger_feat = logging.getLogger('input-features')\n",
    "logger_feat.setLevel(logging.INFO)\n",
    "\n",
    "from dataframelabelencoder import DataFrameLabelEncoder\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    model_file_name = \"model.joblib\"\n",
    "    # TODO: Load the model from the model_dir using the appropriate loader\n",
    "    # Below is a sample code to load a model file using `cloudpickle` which was serialized using `cloudpickle`\n",
    "    # from cloudpickle import cloudpickle\n",
    "    if model_file_name in contents:\n",
    "        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), model_file_name), \"rb\") as file:\n",
    "            model = load(file) # Use the loader corresponding to your model file.\n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(model_file_name, model_dir))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    assert model is not None, \"Model is not loaded\"\n",
    "    X = pd.read_json(io.StringIO(data)) if isinstance(data, str) else pd.DataFrame.from_dict(data)\n",
    "    preds = model.predict(X).tolist()\n",
    "    logger_pred.info(preds)\n",
    "    logger_feat.info(X)    \n",
    "    return { 'prediction': preds }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the artifact before saving to the catalog\n",
    "\n",
    "It is always a good idea to test your model artifact before saving it to the catalog. Here we load the `score.py` module along with `load_model` and `predict`. We test predict by passing the training dataframe, doing the same for the predict() method of the sklearn model object. Next, we compare the two prediction arrays. These two should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = train.X[:5].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "# add the path of score.py: \n",
    "sys.path.insert(0, sklearn_path)\n",
    "\n",
    "from score import load_model, predict\n",
    "\n",
    "# Load the model to memory \n",
    "_ = load_model()\n",
    "# make predictions on the training dataset: \n",
    "predictions_test = np.asarray(predict(train.X[:5], _)['prediction'])\n",
    "predictions = predict(input_data)\n",
    "\n",
    "\n",
    "# comparing the predictions from the sklearn RandomForest predict() to the predictions \n",
    "# array generated by calling predict() in score.py. Both should arrays should be equal.  \n",
    "#print(\"The two arrays are equal: {}\".format(np.array_equal(predictions_test, predictions['prediction'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = sklearn_artifact.save(project_id=os.environ['PROJECT_OCID'], \n",
    "                               compartment_id=os.environ['NB_SESSION_COMPARTMENT_OCID'], \n",
    "                               display_name=\"sklearn-employee-attrition\",\n",
    "                               description=\"simple sklearn model to predict employee attrition\", \n",
    "                               training_script_path=\"employee-attrition.ipynb\", \n",
    "                               ignore_pending_changes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy your model through Model Deployment \n",
    "\n",
    "Let's do that in the console! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the Model HTTP Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using resource principals. You can alternatively use the config+key flow. \n",
    "using_rps = True\n",
    "# Replace with the uri of your model deployment: \n",
    "uri = ''\n",
    "\n",
    "# payload: \n",
    "input_data = train.X[:5].to_json()\n",
    "body = input_data\n",
    "\n",
    "if using_rps: # using resource principal:     \n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "else: # using config + key: \n",
    "    config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "    auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# submit request to model endpoint: \n",
    "requests.post(uri, json=input_data, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:mlcpuv1]",
   "language": "python",
   "name": "conda-env-mlcpuv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
