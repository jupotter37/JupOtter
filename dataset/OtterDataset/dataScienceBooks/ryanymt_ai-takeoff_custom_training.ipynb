{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bee0db-3820-4c1b-87a0-fd303333db40",
   "metadata": {},
   "source": [
    "## Vertex AI Custom Training Lab\n",
    "In this lab, you learn how to use Vertex AI to train and serve a TensorFlow model using code in a custom container.\n",
    "\n",
    "While you're using TensorFlow for the model code here, you could easily replace it with another framework.\n",
    "\n",
    "### Learning objectives\n",
    "Build and containerize model training code in Vertex Notebooks.\n",
    "Submit a custom model training job to Vertex AI.\n",
    "Deploy your trained model to an endpoint, and use that endpoint to get predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025bdb5-b81c-4a3a-94e1-7b3abd3b3505",
   "metadata": {},
   "source": [
    "### Introduction to Vertex AI\n",
    "This lab uses the newest AI product offering available on Google Cloud. Vertex AI integrates the ML offerings across Google Cloud into a seamless development experience. Previously, models trained with AutoML and custom models were accessible via separate services. The new offering combines both into a single API, along with other new products. You can also migrate existing projects to Vertex AI. If you have any feedback, please see the support page.\n",
    "\n",
    "Vertex AI includes many different products to support end-to-end ML workflows. This lab will focus on the products highlighted below: Training, Prediction, and Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6476462-a404-40b8-9391-067d141772ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Container for training\n",
    "You are going to submit this training job to Vertex by putting your training code in a Docker container and pushing this container to Google Container Registry. Using this approach, you can train a model built with any framework.\n",
    "\n",
    "To start, from the Launcher menu, open a terminal window in your notebook instance.\n",
    "\n",
    "Create a new directory called mpg and cd into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8a046e-425b-43f2-93a5-3077aba8ac25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386389bc-b3da-4692-b4db-1af0aaf71ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘mpg’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf438d5-4d4a-4d15-b2b0-0ea797af65b9",
   "metadata": {},
   "source": [
    "#### Create a Dockerfile\n",
    "Your first step in containerizing your code is to create a Dockerfile. In your Dockerfile you'll include all the commands needed to run your image. It'll install all the libraries you're using and set up the entry point for your training code. From your terminal, create an empty Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d69a5c-35f3-4b0c-9008-1a8bcd85d0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!touch mpg/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7405afff-ee15-4550-8ca1-e7ddc85a133a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpg/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpg/Dockerfile\n",
    "\"\"\"\n",
    "DOCKER FILE\n",
    "\"\"\"\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
    "WORKDIR /root\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9844c-de3f-4bc0-9049-26545520064e",
   "metadata": {},
   "source": [
    "This Dockerfile uses the Deep Learning Container TensorFlow Enterprise 2.3 Docker image. The Deep Learning Containers on Google Cloud come with many common ML and data science frameworks pre-installed. The one you're using includes TF Enterprise 2.3, Pandas, Scikit-learn, and others. After downloading that image, this Dockerfile sets up the entrypoint for your training code. You haven't created these files yet--in the next step, you'll add the code for training and exporting your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bc262-e95c-4047-b81f-af131da06717",
   "metadata": {},
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "In your training job, you'll export your trained TensorFlow model to a Cloud Storage Bucket. Vertex will use this to read your exported model assets and deploy the model. From your terminal, run the following to define an env variable for your project and create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d080d1a-bfcb-41fd-bdfd-13266a4d9407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://fraud123-438914-bucket/...\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "BUCKET_NAME=f\"{PROJECT_ID}-bucket\"\n",
    "!gsutil mb -l \"$REGION\" gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5d1ee-39a3-4ad5-aa96-cc71b2ac43c5",
   "metadata": {},
   "source": [
    "#### Add model training code\n",
    "From your terminal, run the following to create a directory for your training code and a Python file where you'll add the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c78bee53-d159-444e-a047-04cbca8cd77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir trainer\n",
    "!touch trainer/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efb55c-1231-4ad6-8397-505d00922dcb",
   "metadata": {},
   "source": [
    "Next, open the train.py file you just created by navigating to mpg > trainer > train.py and copy the code below (this is adapted from the tutorial in the TensorFlow docs).\n",
    "\n",
    "At the beginning of the file, update the BUCKET variable with the name of the Storage Bucket you created in the previous step:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38481ca9-6ab0-4e20-909d-d1fa8bad8637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "  try:\n",
    "    filename, bucket_name = line.split()\n",
    "  except ValueError:\n",
    "    raise ValueError(\"Invalid arguments. Usage: %%writetemplate <filename> <bucket_name>\")\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    f.write(cell.replace(\"BUCKET_NAME\", BUCKET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a88a84c2-b37a-4590-8464-54773c84312e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid arguments. Usage: %%writetemplate <filename> <bucket_name>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mwritetemplate\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m   filename, bucket_name \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwritetemplate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainer/train.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Train.py file created with proper bucket name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport numpy as np\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport pandas as pd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport pathlib\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport tensorflow as tf\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom tensorflow import keras\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom tensorflow.keras import layers\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(tf.__version__)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m## The Auto MPG dataset\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mThe dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m### Get the data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mFirst download the dataset.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset_path = keras.utils.get_file(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto-mpg.data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset_path\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImport it using pandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcolumn_names = [\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMPG\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mCylinders\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mDisplacement\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mHorsepower\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mWeight\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mAcceleration\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mModel Year\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset = pd.read_csv(dataset_path, names=column_names,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                      na_values = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, comment=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                      sep=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, skipinitialspace=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset.tail()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Bucket name\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mBUCKET = f\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{BUCKET_NAME}\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Clean the data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mThe dataset contains a few unknown values.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset.isna().sum()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTo keep this initial tutorial simple, drop those rows.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset = dataset.dropna()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe `\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m` column is really categorical, not numeric. So convert that to a one-hot:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = dataset[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mOrigin\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].map(\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m1: \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mUSA\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, 2: \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mEurope\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, 3: \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mJapan\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m})\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset = pd.get_dummies(dataset, prefix=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, prefix_sep=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdataset.tail()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Split the data into train and test\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNow split the dataset into a training set and a test set.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou will use the test set in the final evaluation of your model.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_dataset = dataset.sample(frac=0.8,random_state=0)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtest_dataset = dataset.drop(train_dataset.index)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Inspect the data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mHave a quick look at the joint distribution of a few pairs of columns from the training set.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAlso look at the overall statistics:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_stats = train_dataset.describe()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_stats.pop(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMPG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_stats = train_stats.transpose()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_stats\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Split features from labels\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSeparate the target value, or \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, from the features. This label is the value that you will train the model to predict.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_labels = train_dataset.pop(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMPG\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtest_labels = test_dataset.pop(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mMPG\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Normalize the data\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mLook again at the `train_stats` block above and note how different the ranges of each feature are.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mIt is good practice to normalize features that use different scales and ranges. Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNote: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef norm(x):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  return (x - train_stats[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]) / train_stats[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mstd\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnormed_train_data = norm(train_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnormed_test_data = norm(test_dataset)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis normalized data is what we will use to train the model.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mCaution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model, along with the one-hot encoding that we did earlier. That includes the test set as well as live data when the model is used in production.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m## The model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m### Build the model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mLet\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms build our model. Here, we\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mll create a second model later on.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef build_model():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  model = keras.Sequential([\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    layers.Dense(64, activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, input_shape=[len(train_dataset.keys())]),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    layers.Dense(64, activation=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    layers.Dense(1)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  ])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  optimizer = tf.keras.optimizers.RMSprop(0.001)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  model.compile(loss=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                optimizer=optimizer,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                metrics=[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmae\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  return model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel = build_model()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m### Inspect the model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mUse the `.summary` method to print a simple description of the model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.summary()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNow try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mIt seems to be working, and it produces a result of the expected shape and type.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m### Train the model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTrain the model for 1000 epochs, and record the training and validation accuracy in the `history` object.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVisualize the model\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms training progress using the stats stored in the `history` object.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mThis graph shows little improvement, or even degradation, in the validation error after about 100 epochs. Let\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms update the `model.fit` call to automatically stop training when the validation score doesn\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mt improve. We\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mll use an *EarlyStopping callback* that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then it will automatically stop the training.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou can learn more about this callback [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping).\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel = build_model()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEPOCHS = 1000\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# The patience parameter is the amount of epochs to check for improvement\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mearly_stop = keras.callbacks.EarlyStopping(monitor=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m, patience=10)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mearly_history = model.fit(normed_train_data, train_labels,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    epochs=EPOCHS, validation_split = 0.2,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m                    callbacks=[early_stop])\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Export model and save to GCS\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.save(BUCKET + \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m/mpg/model\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mwritetemplate\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m      6\u001b[0m   filename, bucket_name \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid arguments. Usage: \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mwritetemplate <filename> <bucket_name>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m   f\u001b[38;5;241m.\u001b[39mwrite(cell\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBUCKET_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, BUCKET_NAME))\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid arguments. Usage: %%writetemplate <filename> <bucket_name>"
     ]
    }
   ],
   "source": [
    "%%writetemplate trainer/train.py\n",
    "\n",
    "# Train.py file created with proper bucket name\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\"\"\"## The Auto MPG dataset\n",
    "\n",
    "The dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n",
    "\n",
    "### Get the data\n",
    "First download the dataset.\n",
    "\"\"\"\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "dataset_path\n",
    "\n",
    "\"\"\"Import it using pandas\"\"\"\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                      na_values = \"?\", comment='\\t',\n",
    "                      sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset.tail()\n",
    "\n",
    "# Bucket name\n",
    "BUCKET = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "\"\"\"### Clean the data\n",
    "\n",
    "The dataset contains a few unknown values.\n",
    "\"\"\"\n",
    "\n",
    "dataset.isna().sum()\n",
    "\n",
    "\"\"\"To keep this initial tutorial simple, drop those rows.\"\"\"\n",
    "\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "\"\"\"The `\"Origin\"` column is really categorical, not numeric. So convert that to a one-hot:\"\"\"\n",
    "\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "\n",
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()\n",
    "\n",
    "\"\"\"### Split the data into train and test\n",
    "\n",
    "Now split the dataset into a training set and a test set.\n",
    "\n",
    "You will use the test set in the final evaluation of your model.\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\"\"\"### Inspect the data\n",
    "\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the training set.\n",
    "\n",
    "Also look at the overall statistics:\n",
    "\"\"\"\n",
    "\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"MPG\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats\n",
    "\n",
    "\"\"\"### Split features from labels\n",
    "\n",
    "Separate the target value, or \"label\", from the features. This label is the value that you will train the model to predict.\n",
    "\"\"\"\n",
    "\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "\"\"\"### Normalize the data\n",
    "\n",
    "Look again at the `train_stats` block above and note how different the ranges of each feature are.\n",
    "\n",
    "It is good practice to normalize features that use different scales and ranges. Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on.\n",
    "\"\"\"\n",
    "\n",
    "def norm(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)\n",
    "\n",
    "\"\"\"This normalized data is what we will use to train the model.\n",
    "\n",
    "Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model, along with the one-hot encoding that we did earlier. That includes the test set as well as live data when the model is used in production.\n",
    "\n",
    "## The model\n",
    "\n",
    "### Build the model\n",
    "\n",
    "Let's build our model. Here, we'll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model later on.\n",
    "\"\"\"\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "\"\"\"### Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model\n",
    "\"\"\"\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"Now try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it.\n",
    "\n",
    "It seems to be working, and it produces a result of the expected shape and type.\n",
    "\n",
    "### Train the model\n",
    "\n",
    "Train the model for 1000 epochs, and record the training and validation accuracy in the `history` object.\n",
    "\n",
    "Visualize the model's training progress using the stats stored in the `history` object.\n",
    "\n",
    "This graph shows little improvement, or even degradation, in the validation error after about 100 epochs. Let's update the `model.fit` call to automatically stop training when the validation score doesn't improve. We'll use an *EarlyStopping callback* that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then it will automatically stop the training.\n",
    "\n",
    "You can learn more about this callback [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping).\n",
    "\"\"\"\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels,\n",
    "                    epochs=EPOCHS, validation_split = 0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "\n",
    "# Export model and save to GCS\n",
    "model.save(BUCKET + '/mpg/model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0200988-028c-40c8-b5f7-841fbabf214e",
   "metadata": {},
   "source": [
    "#### Build and test the container locally\n",
    "From your terminal, define a variable with the URI of your container image in Google Container Registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7296d7-56d6-48dd-857a-1806f62e3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI=\"gcr.io/$PROJECT_ID/mpg:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad650b5-ea1c-4116-bcc8-4a2bbda6f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build ./ -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e00f4b-566e-4320-b46b-27eae0c49f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42fec5-016d-45ab-8972-3e19c31f3c80",
   "metadata": {},
   "source": [
    "### Run a training job on Vertex AI\n",
    "Vertex AI gives you two options for training models:\n",
    "\n",
    "AutoML: Train high-quality models with minimal effort and ML expertise.\n",
    "\n",
    "\n",
    "Custom training: Run your custom training applications in the cloud using one of Google Cloud's pre-built containers, or use your own.\n",
    "\n",
    "In this lab, you're using custom training via our own custom container on Google Container Registry. To start, navigate to the Model Registry section in the Vertex section of your Cloud console:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d41429-8332-4f62-abbc-7fa70c581649",
   "metadata": {},
   "source": [
    "#### Kick off the training job\n",
    "Click **Create** to enter the parameters for your training job and deployed model:\n",
    "\n",
    "- For **Dataset**, select **No managed dataset**.\n",
    "- Then select **Custom training (advanced)** as your training method and click Continue.\n",
    "- Select **Train new model** and Enter **mpg** (or whatever you'd like to call your model) for Model name.\n",
    "- Click **Continue**.\n",
    "\n",
    "In the **Container settings** step, select **Custom container**:\n",
    "\n",
    "In the first box (**Container image on GCR**), enter the value of your IMAGE_URI variable above. It should be: gcr.io/your-cloud-project/mpg:v1, with your own project name. Leave the rest of the fields blank and click **Continue**.\n",
    "\n",
    "You won't use hyperparameter tuning in this tutorial, so leave the Enable hyperparameter tuning box **unchecked** and click **Continue**.\n",
    "\n",
    "In Compute and pricing, select the Region you have set above and select **Add to existing persistent resource**  and for Persistence Resources choose **ai-takeoff**, choose worker pool and select **1** as Replica Count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521afcb6-1b05-4de1-b4cb-7f42ddc5904a",
   "metadata": {},
   "source": [
    "For the **Prediction container** step, select **Pre-built container** and select **2.11** as the Model framework version.\n",
    "\n",
    "Leave the default settings for the pre-built container as is. For **model directory**, browse your GCS bucket with the mpg subdirectory. This is the path in your model training script where you export your trained model. It should look like gs://Your_project_ID_bucket/mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69f9c3-ad5b-4806-bceb-f677d97af991",
   "metadata": {},
   "source": [
    "![GCS Image](images/img-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0261076-13a5-4c3c-9876-175c5253b407",
   "metadata": {},
   "source": [
    "Vertex will look in this location when it deploys your model. Now you're ready for training! Click **Start training** to kick off the training job. In the **Training** section of your console, select Region and you'll see something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9684cc-dcd1-4ba0-aaec-312ba40a8473",
   "metadata": {},
   "source": [
    "![training](images/img-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1cd18-2878-444b-8bd4-e31ed8c60da3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: The training job will take about 5 minutes to complete.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0caea2-ec33-4f13-bc01-25f6890f9620",
   "metadata": {},
   "source": [
    "### Deploy a model endpoint\n",
    "\n",
    "\n",
    "When you set up your training job, you specified where Vertex AI should look for your exported model assets. As part of our training pipeline, Vertex will create a model resource based on this asset path. The model resource itself isn't a deployed model, but once you have a model you're ready to deploy it to an endpoint. To learn more about models and endpoints in Vertex AI, check out the Get started documentation for Vertex AI.\n",
    "\n",
    "In this step you'll create an endpoint for our trained model. You can use this to get predictions on our model via the Vertex AI API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf73a3-d18d-4c4b-a09c-c632e64d5f04",
   "metadata": {},
   "source": [
    "#### Deploy endpoint\n",
    "\n",
    "When your training job completes, you should see a model named mpg (or whatever you named it) in the Model Registry section of your console:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afd62e-42d6-4032-80c0-0aaf12f37fd5",
   "metadata": {},
   "source": [
    "![registry](images/img-03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a832e-6488-47cc-878a-d8d21858ef91",
   "metadata": {},
   "source": [
    "When your training job ran, Vertex created a model resource for you. In order to use this model, you need to deploy an endpoint. You can have many endpoints per model. Click on the model and select **Deploy & Test** tab and then click **Deploy to endpoint**.\n",
    "\n",
    "Select **Create new endpoint** and give it a name, like v1, and click **Continue**. Leave **Traffic split** at 100 and enter 1 for **Minimum number of compute nodes**. Under **Machine type**, select e2-standard-4. Then click **Done** and then **Deploy**.\n",
    "\n",
    "Deploying the endpoint will take 10-15 minutes. When the endpoint has finished deploying, you'll see the following, which shows one endpoint deployed under your model resource:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0285d-c7de-4987-b8e0-69387313c5a1",
   "metadata": {},
   "source": [
    "![endpoint](images/img-04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28afc01-7234-4143-9053-483a50db7cfe",
   "metadata": {},
   "source": [
    "#### Get predictions on the deployed model\n",
    "You'll get predictions on our trained model from a Python notebook, using the Vertex Python API. Go back to your notebook instance, and create a Python 3 notebook from the Launcher:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a30cb5-6be1-47a3-931f-3b6c45f40e68",
   "metadata": {},
   "source": [
    "![notebook](images/img05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9e21a-206f-4f33-829b-7806c55e641f",
   "metadata": {},
   "source": [
    "In your notebook, run the following in a cell to install the Vertex AI SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a26909-b8e2-4acf-8345-9033904d9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306994c1-8fd4-4cea-98f0-cf0719002028",
   "metadata": {},
   "source": [
    "Then add a cell in your notebook to import the SDK and create a reference to the endpoint you just deployed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ded1c-1ad1-4cd1-b918-5e130f6fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/YOUR-ENDPOINT-ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b1be4-f869-495c-95da-a5165ad55cff",
   "metadata": {},
   "source": [
    "You'll need to replace two values in the endpoint_name string above with your project number and endpoint. You can find your project number by navigating to your project dashboard and getting the Project Number value.\n",
    "\n",
    "You can find your endpoint ID in the endpoints section of the console here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda2120-31e7-4fe2-af88-5a300d4c5c5a",
   "metadata": {},
   "source": [
    "![deployed](images/img06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e51b7-a0af-445b-9916-a758e7b4fd03",
   "metadata": {},
   "source": [
    "Finally, make a prediction to your endpoint by copying and running the code below in a new cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398424e1-1498-43df-ab40-27fbbe805fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mpg = [1.4838871833555929,\n",
    " 1.8659883497083019,\n",
    " 2.234620276849616,\n",
    " 1.0187816540094903,\n",
    " -2.530890710602246,\n",
    " -1.6046416850441676,\n",
    " -0.4651483719733302,\n",
    " -0.4952254087173721,\n",
    " 0.7746763768735953]\n",
    "\n",
    "response = endpoint.predict([test_mpg])\n",
    "\n",
    "print('API response: ', response)\n",
    "\n",
    "print('Predicted MPG: ', response.predictions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221ebbc-e6bd-470c-bd33-e47c14e2e9e8",
   "metadata": {},
   "source": [
    "This example already has normalized values, which is the format our model is expecting.\n",
    "\n",
    "Run this cell, and you should see a prediction output around 16 miles per gallon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1ffd6-13c8-41b5-8342-8212548853ba",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You've learned how to use Vertex AI to:\n",
    "\n",
    "Train a model by providing the training code in a custom container. You used a TensorFlow model in this example, but you can train a model built with any framework using custom containers.\n",
    "Deploy a TensorFlow model using a pre-built container as part of the same workflow you used for training.\n",
    "Create a model endpoint and generate a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4378-0a04-4133-9e49-86387d53ae2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
