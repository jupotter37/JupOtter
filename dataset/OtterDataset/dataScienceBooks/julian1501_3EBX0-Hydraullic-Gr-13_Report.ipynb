{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15057ba9-6d8b-49bd-956b-b4c40621b00d",
   "metadata": {},
   "source": [
    "# Report Final assignment Hydraulic cross sections\n",
    "## 3EBX0 - Machine learning in science\n",
    "## Group 13 (also on kaggle)\n",
    "#### Quincy Salden (1749900), Julian Gootzen (1676512), Sebastiaan Broers (1265091)\n",
    "#### June 23rd 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ef8c2-88fc-4631-a959-78f254708212",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Introduction\n",
    "2. C - Physical dimensions analysis\n",
    "3. D - Symmetry group reflection\n",
    "4. E - Neural network desing and training\n",
    "5. F - Network fitting\n",
    "6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaa010-7887-41b3-a7cb-59a61c7ef85c",
   "metadata": {},
   "source": [
    "### C - Physical dimensions analysis\n",
    "The input parameters in this problem are the areas of each pixel in $[m^2]$ with an additional information component that the pixel can either be turned open (value=1) or closed (value=0). In total these pixels form a 40x40 square 2D grid/matrix. \n",
    "\n",
    "The output is the Hydraulic Throughput $S$ which has unit $[m^4]$ and is given by $S = μLQ/ΔP$\n",
    "where $μ$ is the dynamic viscosity of the fluid in $[Pa * s]$, \n",
    "$L$ the channel length in $[m]$, $ΔP$ the pressure drop with unit $[Pa]$ in the channel and $Q$ the volumetric flow rate in $[m^3/s]$.\n",
    "\n",
    "To rescale input parameters and achieve dimensional homogeneity the pixels can be squared or multiplied with one or multiple neighbouring pixels such that the unit is $m^4$.\n",
    "\n",
    "The value of $S$ can vary multiple orders of magnitude. The limiting cases are when all pixels are open or when all are closed. When all are closed, trivially, the value of $S$ is 0. The upper bound can be found by opening all pixels, yielding a value of order $10^4$ - $10^5$ $m^4$ which leads to a widely taken output data range of  $(0, 10^5)$ $m^4$.\n",
    "\n",
    "In this problem multiple symmetries can be identified. Any rotation of 90 degrees leading to 4 equivalent states will not change the value of $S$. Furthermore, flipping the system along the x or y axis will also not change the outcome. Additionally the fact that $S$ is independent of the fluid properties, channel length and pressure drop means that these parameters can fluctuate freely without altering $S$.\n",
    "\n",
    "Since the input data is given as a 40x40 matrix this will imply that flip operations or rotation operations can be used on any of these matrices as desired. \n",
    "\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305ffdf-7dcd-4a37-918a-f95049acf2f1",
   "metadata": {},
   "source": [
    "### D - Symmetry group reflection\n",
    "\n",
    "The symmetry group in this problem is characterized by $D_4$, yielding the combination of flip and rotation operations in a 2D setting. Because there are 4 rotary orientations and a mirror image for each of them this group yields 8 components.\n",
    "\n",
    "#### Data augmentation \n",
    "For data augmentation the rotational symmetry can be used to create three other unique matrices per image, rotated one, two or three times by 90 degrees. This combined with the flip operation leads to 8 unique matrices, excluding all duplicates.\n",
    "\n",
    "\n",
    "#### Disambiguation\n",
    "To disambiguate the data we will first split up the 40x40 grids in eight octants, where there's someoverlap in the diagonals. Then we'll find the octant where there are the most amount of holes, i.e. where the sum of indexes is maximal. Once we've found the maximal octant, we'll use the symmetries, i.e. rotation and reflection through the y = -x axis, to ensure that the maximal octant is always in the upper left spot. (the bottom of the two in the upper left quadrant) Through this disambiguity the model will be able to better 'understand' the problem at hand.\n",
    "\n",
    "#### Hardwiring \n",
    "Hardwiring the symmetries exploits the property that a CNN is translationally invariant due to convolutional layers with the Conv2D function in combination with max pooling. Parameters like stride and filter size within the conv2D function can be optimized to get maximal balance between running speed and results. Rotation and flip operations can be incorporated in the hardwiring such that all 8 orientations are ran through the network. Dropout operations can be added in order to randomly delete parts of the data to train the CNN better in new situations that it has not explored before. Furthermore, a symmetric net could be applied to all 8 orientations averaged and normalized by taking the sum of all orientations and dividing by 8. This is known as group averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6ce18-1775-4adb-ba8d-aff585982bbe",
   "metadata": {},
   "source": [
    "### E - Neural network design and training\n",
    "First, a naïve neural network was trained which did not lead to optimal results, but gave some good insights. \n",
    "\n",
    "Then, all symmetries were hardwired directly into the network which lead to a RMSPE value of 0.04699, still insufficient. At this point, the CNN consisted of 6 Conv2D (C) layers and 3 max pooling (M) layers alternating in sequence C-C-M-C-C-M-C-C-M. The conv2D layers are using a kernel size of 5 and a stride of 1. The padding is half of the kernel size. Furthermore the 8 orientations are all included in the hardwired network through flip and rotation functions. Lastly, flattening and group averaging is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74946235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplySymmetry(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_lr = np.fliplr(x)\n",
    "        x_ud = np.flipud(x)\n",
    "        x_lr_ud = np.fliplr(np.flipud(x))\n",
    "        x_90 = np.rot90(x,1)\n",
    "        x_180 = np.rot90(x_90,1)\n",
    "        x_270 = np.rot90(x_180,1)\n",
    "        x_diag1 = np.rot90(np.fliplr(x),1)\n",
    "        x_diag2 = np.rot90(np.fliplr(x),2)\n",
    "        \n",
    "        return torch.cat([x, x_lr, x_ud, x_lr_ud, x_diag1, x_diag2], dim=1).view(-1, x.size()[1])\n",
    "\n",
    "\n",
    "class GroupAverage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 2).mean(dim=1).unsqueeze(dim=-1)\n",
    "\n",
    "class SquareRoot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sqrt(x)\n",
    "\n",
    "class HydraullicModel(nn.Module):\n",
    "    \n",
    "    width = 256\n",
    "    \n",
    "    def __init__(self, width=width):\n",
    "        super(HydraullicModel, self).__init__()\n",
    "        ksize = 5\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=ksize, stride=1, padding=ksize//2, bias=False)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.fc1 = nn.Linear(400,1,bias=False)\n",
    "        \n",
    "\n",
    "      \n",
    "    def forward(self, x, width=width):\n",
    "#        ApplySymmetry()\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = F.relu(self.conv2(x)) \n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5(x))   \n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.maxpool(x)\n",
    "       \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "#        GroupAverage()\n",
    "\n",
    "        # SquareRoot()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22178417",
   "metadata": {},
   "source": [
    "It was opted that hardwiring is inefficient and that disambiguity may likely lead to better and faster results. This was then implemented into the code, with the method described in section D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train_data, out_train_data, in_test_data = load_data(\n",
    "    'pub_input.npy', 'pub_out.npy', 'pri_in.npy'\n",
    ")\n",
    "\n",
    "def disambiguate(x):\n",
    "    \"\"\" \n",
    "    rotates / reflects s.t. the lower top left octant has the most 1s.\n",
    "    \"\"\"\n",
    "    sums = []\n",
    "    for i in range(4):\n",
    "\n",
    "        array = np.rot90(x,k = i)\n",
    "\n",
    "        top_left_quadrant = array[:half_size, :half_size]\n",
    "        bot_triangle_sum = sum([top_left_quadrant[i][j] for i in range(20) for j in range(20) if i<=j])\n",
    "        top_triangle_sum = sum([top_left_quadrant[i][j] for i in range(20) for j in range(20) if i>=j])\n",
    "        sums.append(bot_triangle_sum)\n",
    "        sums.append(top_triangle_sum)\n",
    "\n",
    "    max_sums_arg = np.argmax(sums)\n",
    "\n",
    "    if max_sums_arg == 0:\n",
    "        pass \n",
    "    if max_sums_arg == 1:\n",
    "        x = np.transpose(x)\n",
    "    if max_sums_arg == 2:\n",
    "        x = np.rot90(x , 1)\n",
    "    if max_sums_arg == 3:\n",
    "        x = np.rot90(x , 1)\n",
    "        x = np.transpose(x)\n",
    "    if max_sums_arg == 4:\n",
    "        x = np.rot90(x , 2)\n",
    "    if max_sums_arg == 5:\n",
    "        x = np.rot90(x , 2)\n",
    "        x = np.transpose(x)\n",
    "    if max_sums_arg == 6:\n",
    "        x = np.rot90(x , 3)\n",
    "    if max_sums_arg == 7:\n",
    "        x = np.rot90(x , 3)\n",
    "        x = np.transpose(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "for x in in_train_data:\n",
    "    new_x = disambiguate(x)\n",
    "    train_data_list.append(new_x)\n",
    "for x in in_test_data:\n",
    "    new_x = disambiguate(x)\n",
    "    test_data_list.append(new_x)  \n",
    "in_train_data = np.array(train_data_list)\n",
    "in_test_data = np.array(test_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e632c-536b-469f-b216-5ec59aa36eca",
   "metadata": {},
   "source": [
    "### F - Network fitting\n",
    "In order to protect the network from overtraining the latest validation loss average is compared to an earlier validation loss. For exapmle, in the following training run the average of validation losses from the 3 latest epochs is compared to the average validation loss of 17 to 20 epochs ago. Or in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a9404-6e74-4e24-bf48-e71086607daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch > 20 and mean_val_loss > mean(loss_dict['val'][-20:-17]):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9528c68-6ff4-4558-bf3d-f0a9cf5da246",
   "metadata": {},
   "source": [
    "The condition that the number of epochs should be larger then 20 is in place to prevent errors in the second condition. This condition alone still leaves a possibility for overtraining, since no comparison is made between the training and validation loss. Therefore another condition is added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8020a83e-a25a-459e-992f-01c0cdf6a936",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (4048069654.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "if mean_val_loss * 1.05 > mean_train_loss:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277156cf",
   "metadata": {},
   "source": [
    "In order to train our network, without putting too much strain on our university laptops, the training process was adapted to be able to run passively on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40069afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def train_model_repeated(train_loader,\n",
    "                val_loader, \n",
    "                model,\n",
    "                nepochs=1,\n",
    "                lr=1e-4,\n",
    "                loss_fn=rmspe,\n",
    "                batch_size = 32,\n",
    "                plot_result=True):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    duration = 60*60*8\n",
    "    data_dict = {\"nepochs\": [] , \"val_loss\" : [] , \"train_loss\" : [] , \"best_loss\" : []}\n",
    "    results = []\n",
    "    while time.time() - start_time < duration:\n",
    "        \n",
    "        model = HydraullicModel().to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        loss_dict = {\"train\": [], \"val\": []}\n",
    "\n",
    "        # Set up the main tqdm loop for epochs\n",
    "        epochs_tqdm = tqdm(range(nepochs), desc='Training Model')\n",
    "        \n",
    "\n",
    "        for epoch in epochs_tqdm:\n",
    "            model.train()\n",
    "            epoch_loss_sum = 0\n",
    "\n",
    "            # Train loop without tqdm for individual batches\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                y_batch = y_batch.unsqueeze(1)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                epoch_loss_sum += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            avg_train_loss = epoch_loss_sum / len(train_loader)\n",
    "            loss_dict[\"train\"].append(avg_train_loss)\n",
    "\n",
    "            # Validation loop without tqdm for individual batches\n",
    "            # model.eval()\n",
    "            val_loss_sum = 0\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    y_val = y_val.unsqueeze(1)\n",
    "                    y_pred = model(x_val)\n",
    "                    # print(y_val)\n",
    "                    # print(y_pred)\n",
    "                    loss = loss_fn(y_pred, y_val)\n",
    "                    val_loss_sum += loss.item()\n",
    "\n",
    "            avg_val_loss = val_loss_sum / len(val_loader)\n",
    "            loss_dict[\"val\"].append(avg_val_loss)\n",
    "            \n",
    "            # Update tqdm bar only once per epoch with the average losses\n",
    "            epochs_tqdm.set_description(f'Epoch {epoch+1}/{nepochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "            if True:\n",
    "                mean_val_loss = mean(loss_dict['val'][-15:])\n",
    "                mean_train_loss = mean(loss_dict['train'][-15:])\n",
    "            \n",
    "                if epoch > 30 and mean_val_loss > mean_train_loss*1.05:\n",
    "                    data_dict[\"nepochs\"].append(epoch)\n",
    "                    break\n",
    "            if epoch+1 == max_epochs:\n",
    "                data_dict[\"nepochs\"].append(epoch+1)\n",
    "                break\n",
    "        data_dict['val_loss'].append(mean(loss_dict['val'][-5:]))\n",
    "        data_dict['train_loss'].append(mean(loss_dict['train'][-5:]))\n",
    "            \n",
    "        if plot_result:\n",
    "            plot_losses(loss_dict)\n",
    "        \n",
    "        results.append((model, loss_dict))\n",
    "\n",
    "    \n",
    "    return results , data_dict\n",
    "\n",
    "mean_loss_list = []\n",
    "for index in range(len(results)):\n",
    "    loss_dictionary = results[index][1]\n",
    "    mean_loss_list.append(mean([mean(loss_dictionary['train'][-5:]) , mean(loss_dictionary['val'][-5:])]))\n",
    "    best_model = results[np.argmin(mean_loss_list)][0]    \n",
    "data_dict[\"best_loss\"].append(min(mean_loss_list))\n",
    "\n",
    "    \n",
    "current_time = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "torch.save(best_model , f\"model_{current_time}\")\n",
    "df = pd.DataFrame(list(data_dict.items()), columns=['Key', 'Value'])\n",
    "df.to_csv(f'/kaggle/working/data_{current_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302475cf",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After trying different versions of neural networks the final best RMSPE value which was found is 0.03595. Possible future improvements are to make a deeper network or one that makes better or smarter use of the applied symmetries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
