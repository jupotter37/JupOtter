{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody jądrowe (kernel methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy bardzo standardowych narzędzi, tych samych, co na laboratorium 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy scipy pandas matplotlib scikit-learn missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duży zbiór danych do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjemy najpierw bioinformatycznego zbioru danych [Cod-RNA](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#cod-rna). Pochodzi on z artykułu ([link do wersji Open Access](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-173)):\n",
    "\n",
    "> Uzilov, Andrew V., Joshua M. Keegan, and David H. Mathews. *\"Detection of non-coding RNAs on the basis of predicted secondary structure formation free energy change.\"* BMC bioinformatics 7.1 (2006): 1-30. [link](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-173)\n",
    "\n",
    "[Centralny dogmat biologii molekularnej (central dogma of molecular biology)](https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology) mówi, że przepływ informacji genetycznej przebiega z DNA przez RNA do białek. Innymi słowy, DNA zapisuje informację biologiczną, którą potem koduje RNA, z którego są syntezowane białka.\n",
    "\n",
    "W praktyce nie każde DNA i RNA koduje informacje. Kodujące DNA to zaledwie ok. 1-2% ludzkiego genomu, a reszta pełni różne role, głównie regulacyjne. W szczególności [niekodujące RNA (non-coding RNA, ncRNA)](https://en.wikipedia.org/wiki/Non-coding_RNA) pełni wiele różnych zadań, na przykład:\n",
    "- budują rybosomy odpowiadające za syntezę białek ([rRNA](https://en.wikipedia.org/wiki/Ribosomal_RNA))\n",
    "- transportują informację genetyczną ([tRNA](https://en.wikipedia.org/wiki/Transfer_RNA))\n",
    "- regulują ekspresję genów ([lncRNA](https://en.wikipedia.org/wiki/Long_non-coding_RNA))\n",
    "\n",
    "Nie są znane wszystkie funkcje niekodującego RNA, więc jest to aktywne pole badań. Najpierw trzeba jednak sprawdzić, czy dane RNA w ogóle jest kodujące, czy nie. Taka klasyfikacja jest nieoczywista, i też stanowi pole badań w bioinformatyce. Niektóre hipotezy wskazują, że struktura drugorzędowa RNA ma tutaj znacznie.\n",
    "\n",
    "Struktura pierwszorzędowa kwasu nukleinowego (DNA lub RNA) to po prostu liniowa sekwencja nukleotydów, np. ACCUUGCAUC. Struktura drugorzędowa oznacza ułożenie par nukleotydów (np. G-C, A-U) jednego lub dwóch łańcuchów.  Dla DNA to typowo podwójna helisa, natomiast RNA tworzy już bardzo bogate i złożone struktury. Przewidywanie struktury wyższego rzędu to typowe zadanie ML w bioinformatyce, bo sprawdzanie tego eksperymentalnie jest bardzo drogie.\n",
    "\n",
    "![nucleic_acid_structures.png](nucleic_acid_structures.png)\n",
    "\n",
    "Przykładem algorytmu przewidującego strukturę drugorzędową jest [Dynalign](https://www.sciencedirect.com/science/article/abs/pii/S0022283601953513). Opiera się on na obserwacji, że niektóre struktury drugorzędowe RNA są bardziej stabilne od innych. Struktury o niższej energii są, zgodnie z zasadami termodynamiki, wolniejsze, a zatem stabilniejsze i bardziej prawdopodobne. Algorytm ten dodatkowo realizuje **dopasowanie sekwencji (sequence alignment)**, czyli takie \"przyłożenie\" do siebie sekwencji RNA, żeby jak najwięcej par pasowało do siebie. Wejściem do niego są 2 sekwencje RNA, a wyjściem ilość wolnej energii w optymalnej strukturze drugorzędowej RNA - im mniejsza, tym stabilniejsza jest struktura.\n",
    "\n",
    "Artykuł, z którego pochodzi nasz zbiór danych, zaproponował zastosowanie ML do klasyfikacji, czy dwie sekwencje RNA stanowią niekodujące RNA (ncRNA), czy też nie. Zbiór treningowy zbudowano z blisko 60 tysięcy par sekwencji RNA pochodzących z sekwencjonowania genomu bakterii *Escherichia coli* oraz *Salmonella enterica serovar Typhi (Salmonella typhi)*. Klasą pozytywną jest niekodujące RNA (5S rRNA albo tRNA), a negatywną losowo przemieszane nukleotydy z prawdziwej pary (istnieją do tego algorytmy tzw. sequence shuffling). Klasy są w stosunku 1:2, więc mamy klasyfikację lekko niezbalansowaną. Analogicznie stworzono zbiór testowy o wielkości nieco ponad 270 tysięcy par.\n",
    "\n",
    "Jako cech użyto:\n",
    "- wartości z algorytmu Dynalign\n",
    "- długości krótszej sekwencji\n",
    "- częstotliwości zasad azotowych A/U/C w sekwencji 1 (3 cechy)\n",
    "- częstotliwości zasad azotowych A/U/C w sekwencji 2 (3 cechy)\n",
    "\n",
    "Mamy zatem klasyfikację binarną, umiarkowanie niezbalansowaną, i 8 cech numerycznych. Jako metryk autorzy używają precyzji, czułości, krzywych ROC oraz wartości AUROC.\n",
    "\n",
    "My dla uproszczenia użyjemy F1-score, czyli średniej harmonicznej precyzji (precision) i czułości (recall). Obliczanie prawdopodobieństw w SVMach jest kosztowne i daje raczej niskiej jakości predykcje prawdopodobieństw, więc AUROC nie jest dla nich zbyt wygodną metryką."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oryginalny zbiór jest w formacie `svmlight`, z którego korzystają biblioteki LibSVM i Liblinear. Są one standardowymi implementacjami SVMów. Sam ten format danych jest jednak tekstowy i zajmuje bardzo dużo miejsca, dlatego w tym repozytorium jest już przetworzony do formatu Parquet. Wczytamy teraz dane.\n",
    "\n",
    "Później trzeba dokonać normalizacji cech, tak jak zawsze w SVMach. Tak jak autorzy, wykonamy skalowanie min-max do zakresu [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 1 (0.5 punktu)**\n",
    "\n",
    "1. Wczytaj dane treningowe z plików `cod_rna_train.parquet` oraz `cod_rna_test.parquet`.\n",
    "2. Wyodrębnij kolumnę `y` do osobnych zmiennych `y_train` oraz `y_test`.\n",
    "3. Dokonaj skalowania danych do zakresu [-1, 1], tworząc tablice `X_train_scaled` oraz `X_test_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train = pd.read_parquet(\"cod_rna_train.parquet\")\n",
    "X_test = pd.read_parquet(\"cod_rna_test.parquet\")\n",
    "\n",
    "y_train = X_train.pop(\"y\")\n",
    "y_test = X_test.pop(\"y\")\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro dane są gotowe, to wytrenujmy nasz pierwszy klasyfikator - liniowy SVM.\n",
    "\n",
    "W Scikit-learn funkcja kosztu (`loss`) to albo typowy hinge loss, albo squared hinge loss, czyli po prostu podniesiona do kwadratu. Wartość domyślna to `\"squared_hinge\"` - żeby dostać \"typowego\" SVMa, trzeba to zmienić. Jest różniczkowalna i mocniej kara duże błędy, ale jest też bardziej podatna na outliery. Dla zainteresowanych: [przystępny tutorial](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-hinge-squared-hinge-loss-with-keras.md) oraz artykuł naukowy:\n",
    "\n",
    "C.P. Lee, and C.J. Lin. *\"A study on L2-loss (squared hinge-loss) multiclass SVM.\"* Neural computation 25.5 (2013): 1302-1323.\n",
    "\n",
    "Dodatkowo domyślna liczba iteracji solwera (1000) jest często dość niska, szczególnie przy dużych zbiorach. Zwiększymy to od razu, żeby mieć mniej warningów. Powyżej 10 tysięcy iteracji warningami można się już zazwyczaj nie przejmować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 93.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_linear_svc = LinearSVC(loss=\"hinge\", max_iter=10000, random_state=0)\n",
    "clf_linear_svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf_linear_svc.predict(X_test_scaled)\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liniowy SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 2 (0.5 punktu)**\n",
    "\n",
    "Wytrenuj regresję logistyczną jako nasz **model baseline'owy**. Nasze bardziej złożone modele SVM muszą być od niego lepsze, żeby był w ogóle sens je rozważać.\n",
    "\n",
    "Użyj regularyzacji L2. Dokonaj tuningu siły regularyzacji (100 wartości) za pomocą 5-krotnej walidacji skrośnej. Jako metryki do tuningu użyj F1-score.\n",
    "\n",
    "Użyj zbalansowanych wag klas. W razie potrzeby zwiększ maksymalną liczbę iteracji solwera. Może się przydać `n_jobs`. Pamiętaj o ustawieniu `random_state=0`.\n",
    "\n",
    "Sprawdź wyniki algorytmu na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1(y_test, y_pred):\n",
    "    print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 91.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_reg = LogisticRegression(\n",
    "    penalty=\"l2\", random_state=0, max_iter=10000, class_weight=\"balanced\", n_jobs=-1\n",
    ")\n",
    "logistic_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = logistic_reg.predict(X_test_scaled)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 91.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"C\": np.linspace(1e-1, 1e2, 100)}\n",
    "\n",
    "logistic_cv = GridSearchCV(\n",
    "    estimator=logistic_reg, cv=5, param_grid=param_grid, scoring=\"f1\", n_jobs=-1\n",
    ")\n",
    "\n",
    "logistic_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "logistic_reg.set_params(**logistic_cv.best_params_)\n",
    "logistic_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = logistic_reg.predict(X_test_scaled)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nasz SVM bez żadnego tuningu jest zasadniczo tak dobry, jak regresja logistyczna z tuningiem - całkiem nieźle. No ale czas teraz na tuning samego SVMa.\n",
    "\n",
    "Najpierw warto wiedzieć jeszcze parę rzeczy o implementacji użytej w Scikit-learn, korzystającej pod spodem z Liblinear.\n",
    "\n",
    "Argument `dual` wyznacza, czy skorzystać z postaci primal, czy dual. Jeżeli liczba próbek jest znacznie większa od liczby cech, albo ogółem mamy sporo próbek, to lepiej użyć primal `dual=False`. Niestety, wtedy mamy do dyspozycji tylko funkcję kosztu `squared_hinge`.\n",
    "\n",
    "SVM jest stricte jednowątkowy (sekwencyjny), więc równoległość można wykorzystać przy tuningu hiperparametrów. Zdecydowanie warto użyć tu `n_jobs=-1`.\n",
    "\n",
    "**Uwaga:** jako że osobne joby są tworzone jako osobne procesy, to wyłączenie `ConvergenceWarning` jest nietrywialne, a czasem niemożliwe. Sugeruję to po prostu zignorować.\n",
    "\n",
    "**Zadanie 3 (1 punkt)**\n",
    "\n",
    "Dokonaj tuningu hiperparametru `C` liniowego SVM. Użyj 5-krotnej walidacji skrośnej, wybierając model o najwyższym F1-score. Użyj zbalansowanych wag klas i funkcji kosztu hinge loss. Pamiętaj o stałym `random_state`.\n",
    "\n",
    "Zastosuj iteracyjne zagęszczanie siatki. Najpierw sprawdź wartości z zakresu `[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]` i znajdź optymalną wartość `C`. Następnie wedle własnego uznania zagęść siatkę wokół znalezionego optymalnego C. Może się przydać `np.linspace()` albo `list(range())`. Jeżeli w takiej operacji wychodzi optymalne `C` na granicy (najmniejsze albo największe możliwe), to warto sprawdzić większe wartości niż dotychczas rozważane. Powtórz takie działanie 2-3 razy. W praktyce jednak `C` większe niż kilkaset nie ma sensu w kontekście SVMów.\n",
    "\n",
    "Wypisz ostatecznę znalezioną optymalną wartość odwrotności siły optymalizacji `C`. Sprawdź wynik na zbiorze testowym. Czy udało się przebić baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC(\n",
    "    loss=\"hinge\", class_weight=\"balanced\", random_state=0, max_iter=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_gridsearch(estimator, param_grid, X, y):\n",
    "    cv = GridSearchCV(\n",
    "        estimator=estimator, param_grid=param_grid, scoring=\"f1\", n_jobs=-1, cv=5\n",
    "    )\n",
    "    cv.fit(X, y)\n",
    "    C = cv.best_params_[\"C\"]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "first_params = {\"C\": [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]}\n",
    "C1 = iterative_gridsearch(linear_svm, first_params, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "print(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': [10, 50, 90, 130, 170]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_params = {\"C\": list(range(10, 200, 40))}\n",
    "second_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "C2 = iterative_gridsearch(linear_svm, second_params, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "print(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': [150, 170, 190, 210, 230]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_params = {\"C\": list(range(150, 250, 20))}\n",
    "third_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "C3 = iterative_gridsearch(linear_svm, third_params, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "print(C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=230, class_weight=&#x27;balanced&#x27;, loss=&#x27;hinge&#x27;, max_iter=10000,\n",
       "          random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=230, class_weight=&#x27;balanced&#x27;, loss=&#x27;hinge&#x27;, max_iter=10000,\n",
       "          random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=230, class_weight='balanced', loss='hinge', max_iter=10000,\n",
       "          random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svm.set_params(**{\"C\": C3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 91.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linear_svm.fit(X_train_scaled, y_train)\n",
    "y_pred = linear_svm.predict(X_test_scaled)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C ciągle wzrastało, ale powyżej 200 nie było sensu go zwiększać (sugerując się brakiem używania C większego niż kilkaset w praktyce), wynik jest bardzo podobny do baseline'owego ale nie udało się go przebić :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Może się to jeszcze uda poprawić - w końcu w artykule autorzy używali kernel SVM, a nie liniowego!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel SVM w Scikit-learn jest implementowany w klasie `SVC`. Wypróbujmy podstawową wersję, bez tuningu hiperparametrów. Przy okazji wiemy, że złożoność kernel SVM to co najmniej $O(n^2)$ - zobaczmy, ile to zajmuje w praktyce.\n",
    "\n",
    "Warto zawsze ustawić nieco wyższą wartość `cache_size` niż domyślna. Jest to ilość MB na cache'owanie obliczonych wartości kerneli. Warto jednak uważać na zbyt duże wartości:\n",
    "- wartości ponad 2000 mogą [powodować problemy przez buga w LibSVM](https://github.com/scikit-learn/scikit-learn/issues/8012)\n",
    "- sama alokacja dużej ilości pamięci (ciągłej!) może zajmować dużo czasu\n",
    "- nie wszystkie wartości kerneli będą używane często, więc nie wszystkie jest sens cache'ować"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 93.38%\n",
      "Training time: 29.96395516395569 s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf_kernel_svc = SVC(cache_size=512, class_weight=\"balanced\", random_state=0)\n",
    "start_time = time()\n",
    "clf_kernel_svc.fit(X_train_scaled, y_train)\n",
    "end_time = time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "y_pred = clf_kernel_svc.predict(X_test_scaled)\n",
    "\n",
    "print(f\"F1-score: {100 * f1_score(y_test, y_pred):.2f}%\")\n",
    "print(f\"Training time: {training_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik jest ewidentnie lepszy, bez żadnego tuningu! Zajmuje to jednak sporo czasu. Ale nasz zbiór treningowy to ok. 60 tysięcy, a testowy 270 tysięcy - może to predykcja tyle zajmuje, a nie sam trening?\n",
    "\n",
    "**Zadanie 4 (1 punkt)**\n",
    "\n",
    "1. Sprawdź liczbę support vectors dla liniowego i kernel SVM. Sprawdź, jaki procent wszystkich punktów one stanowią. Czy rzadkość (sparsity) została osiągnięta? Może się przydać [ten przykład](https://scikit-learn.org/stable/auto_examples/svm/plot_linearsvc_support_vectors.html), żeby zliczyć support vectors dla liniowego SVM (`X` to u nas `X_train_scaled`).\n",
    "2. Porównaj czas predykcji na zbiorze testowym dla regresji logistycznej, liniowego SVM i kernel SVM. Uwzględniając rozmiar zbioru testowego, czy twoim zdaniem to dużo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support vectors in linear SVM: 14.38%\n",
      "support vectors in kernel SVM: 16.00%\n"
     ]
    }
   ],
   "source": [
    "linear_df = linear_svm.decision_function(X_train_scaled)\n",
    "linear_sv_indices = np.where(np.abs(linear_df) <= 1 + 1e-15)[0]\n",
    "linear_svs = X_train_scaled[linear_sv_indices]\n",
    "linear_sv = len(linear_svs)\n",
    "\n",
    "kernel_svs = clf_kernel_svc.support_vectors_\n",
    "kernel_sv = len(kernel_svs)\n",
    "\n",
    "all_sv = len(X_train_scaled)\n",
    "\n",
    "linear_percent = linear_sv / all_sv * 100\n",
    "kernel_percent = kernel_sv / all_sv * 100\n",
    "print(f\"support vectors in linear SVM: {linear_percent:.2f}%\")\n",
    "print(f\"support vectors in kernel SVM: {kernel_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~15% to już większy wynik więc można powiedzieć że rzadkość nie została osiągnięta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction_time(model, model_name, X):\n",
    "    start_time = time()\n",
    "    model.predict(X)\n",
    "    end_time = time()\n",
    "    print(f\"{model_name}: {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.004256248474121094s\n",
      "Linear SVM: 0.0033180713653564453s\n",
      "Kernel SVM: 104.2533175945282s\n"
     ]
    }
   ],
   "source": [
    "print_prediction_time(logistic_reg, \"Logistic Regression\", X_test_scaled)\n",
    "print_prediction_time(linear_svm, \"Linear SVM\", X_test_scaled)\n",
    "print_prediction_time(clf_kernel_svc, \"Kernel SVM\", X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dla zbioru testowego ok 270 tys. próbek, predykcja z użyciem regresji logistycznej i liniowego SVM była bardzo szybka, jednak dla kernel SVM trwała kilka rzędów dłużej a zbiór nie jest wyjątkowo duży"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taki zbiór jest już dość ekstremalny dla kernel SVM. Tego algorytmu w praktyce nie stosuje się wprost dla zbiorów wielkości dziesiątek tysięcy próbek treningowych, bo byłoby to zbyt wolne. Można tu natomiast zastosować podobną sztuczkę, co z kNN i ANN, czyli aproksymować - w tym wypadku macierz kerneli. Dotyczy tego zadanie dodatkowe.\n",
    "\n",
    "Dodatkowo implementacja ze Scikit-learn potrafi być zaskakująco wolna. W praktyce LibSVM, użyty jako [samodzielna biblioteka](https://pypi.org/project/libsvm-official/), potrafi być o wiele szybszy. Ma za to dużo brzydszy interfejs.\n",
    "\n",
    "W praktyce kernel SVM jest używany najczęściej, kiedy mamy mało danych i trzeba bardzo dokładnego klasyfikatora, a jego duży koszt nie jest tak odczuwalny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mały zbiór danych do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako drugi zbiór wykorzystamy [Iranian Churn Dataset](https://archive.ics.uci.edu/ml/datasets/Iranian+Churn+Dataset) ([Kaggle](https://www.kaggle.com/datasets/royjafari/customer-churn)). Dotyczy on klasyfikacji, czy dany klient telekomunikacyjny zmieni dostawcę, czyli nastąpi tzw. odpływ klienta (*customer churn*). Jest to bardzo ważne i częste zadanie w telekomunikacji i usługach, szczególnie w biznesach, gdzie pozyskanie całkiem nowych klientów jest bardzo trudne albo praktycznie niemożliwe, a trzeba ich w praktyce odebrać konkurencji. To właśnie sytuacja firm telekomunikacyjnych - telefon czy internet ma praktycznie każdy, więc zdobywanie nowych klientów jest drogie. Dużo bardziej opłaca się identyfikować \"ryzykownych\" klientów i ich zachęcić do pozostania, np. promocjami.\n",
    "\n",
    "Dane zostały pozyskane z irańskich firm telekomunikacyjnych (konkretnie z call center obsługującego takie firmy). [Historia telekomunikacji w Iranie](https://en.wikipedia.org/wiki/Communications_in_Iran) jest ciekawa sama w sobie i pokazuje, czemu w tym kraju to było szczególnie ważne. Iran w 2008 roku miał populację ponad 80 milionów ludzi, z czego około 56% poniżej 25 roku życia - idealny i duży rynek dla nowych technologii. Ponadto samo zainteresowanie było ogromne, a tempo rozwoju kraju oraz prywatyzacja państwowych spółek stworzyły niesamowicie konkurencyjny rynek. Churn prediction stało się bardzo ważnym finansowo zadaniem dla wielu spółek.\n",
    "\n",
    "Z perspektywy ML, churn prediction to praktycznie zawsze klasyfikacja binarna i niezbalanansowana - zwykle mało który klient odchodzi.\n",
    "\n",
    "Zbiór ten został zebrany przez autorów na potrzeby artykułu:\n",
    "\n",
    "> Keramati, Abbas, and Seyed MS Ardabili. *\"Churn analysis for an Iranian mobile operator.\"* Telecommunications Policy 35.4 (2011): 344-356.\n",
    "\n",
    "Churn prediction z użyciem tego zbioru zostało przez nich zastosowane w artykułach:\n",
    "\n",
    "> Keramati, Abbas, et al. *\"Improved churn prediction in telecommunication industry using data mining techniques.\"* Applied Soft Computing 24 (2014): 994-1012.\n",
    "\n",
    "> Jafari-Marandi, Ruholla, et al. *\"Optimum profit-driven churn decision making: innovative artificial neural networks in telecom industry.\"* Neural Computing and Applications 32 (2020): 14929-14962.\n",
    "\n",
    "Dane są wysokiej jakości - ludzi monitorowano przez 12 miesięcy, aby uzyskać miarodajną informację w czasie, czy odejdą do innego operatora. Dodatkowo nie mają wartości brakujących i outlierów, a wszystkie zmienne są numeryczne. Zastosowaną w artykułach metryką jest F1-score.\n",
    "\n",
    "Zbiór został wzięty z Kaggle (opublikował go tam Ruholla Jafari-Marandi). W dodatku do zmiennych opisanych na stronie UCI, zawiera on oszacowaną wartość FP i FN dla każdego klienta. Wykorzystamy to później - w końcu utrata dużo płacącego klienta jest bardziej kosztowna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 5 (1 punkt)**\n",
    "\n",
    "1. Wczytaj dane z pliku `customer_churn_data.csv`.\n",
    "2. Wyodrębnij kolumny do zmiennych:\n",
    "   - `Churn` do zmiennej `y`\n",
    "   - `FN` do zmiennej `fn_cost`\n",
    "   - `FP` do zmiennej `fp_cost`\n",
    "3. Narysuj wykres słupkowy (bar plot) częstości klas. Pamiętaj o opisaniu wykresu.\n",
    "4. Podziel zbiór na treningowy i testowy w proporcjach 75%-25% ze stratyfikacją. Pamiętaj o stałym `random_state=0`. Uwaga - podziel też `fn_cost` oraz `fp_cost` na treningowe i testowe.\n",
    "5. Dokonaj standaryzacji zmiennych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'number of occurences')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5x0lEQVR4nO3deViVdf7/8dcRFdHgKLIbIplrIDlqipX7mktmo6YNaplabkNqOtSYS245pc2kWZm5pWnNN50Wc8S1THD7iftShluCuMBBDUHg/v3h5bnmhBrHgAPcz8d1nevy/tyf87nfN3rkdX3uz30fi2EYhgAAAEysjKsLAAAAcDUCEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEYC7Wrx4sSwWi3bv3u3qUvLl8uXLeuaZZ+Tn5yeLxaIePXq4uiQAJUBZVxcAAAXpjTfe0OrVq/Xxxx+rZs2a8vb2dnVJAEoAAhGAYiEjI0MVKlSQxWL5Q+McPHhQNWvW1LPPPnvXfjk5OcrOzpa7u/sfOh6A0oFLZoDJHT16VH379pW/v7/c3d1VvXp19e/fX5mZmQ79rly5opdeekk+Pj6qWrWqevbsqXPnzjn0sVgsmjRpUp5j1KhRQwMHDrRv37oMt379ej3//PPy9fVVxYoVlZmZqVatWiksLEy7du3S448/rooVK+qBBx7QzJkzlZube8fzOHnypCwWizZs2KAjR47IYrHIYrFoy5Yt9n2zZs3S1KlTFRoaKnd3d23evFmStHv3bnXv3l3e3t6qUKGCGjZsqM8++yzPMeLj4/Xoo4+qQoUKCgoKUkxMjBYsWCCLxaKTJ086/XOQpOTkZA0dOlT333+/ypcvr9DQUE2ePFnZ2dl5zu2tt97S7NmzFRoaqvvuu0+RkZGKj4/Pc5wdO3aoW7duqlq1qipUqKCaNWsqOjpakvT999/LYrHo008/zfO+pUuXymKxaNeuXXf8OQOlFTNEgInt27dPjz32mHx8fDRlyhTVqlVLSUlJ+vLLL5WVleUwe/LCCy+oS5cuWrFihc6cOaNXXnlFf/nLX7Rp06Z7Pv7zzz+vLl26aNmyZbp27ZrKlSsn6WZIePbZZzVmzBhNnDhRq1evVkxMjIKCgtS/f//bjhUYGKi4uDgNGzZMNptNy5cvlyTVr19fly9fliT961//Uu3atfXWW2/Jy8tLtWrV0ubNm9WpUyc1bdpU77//vqxWq1auXKk+ffro119/tQeYw4cPq23btqpRo4YWL16sihUr6r333tOKFSvu+fyTk5P1yCOPqEyZMnr99ddVs2ZNxcXFaerUqTp58qQWLVrk0H/evHmqW7eu3nnnHUnShAkT9MQTTygxMVFWq1WS9N///lfdunVTvXr1NHv2bFWvXl0nT57U+vXrJUmPP/64GjZsqHnz5qlv374O48+dO1dNmjRRkyZN7vmcgBLLAGBabdq0MSpXrmykpKTcsc+iRYsMScawYcMc2mfNmmVIMpKSkuxtkoyJEyfmGSMkJMQYMGBAnjH79++fp2/Lli0NScaOHTsc2uvXr2907Njxd8+pZcuWxkMPPeTQlpiYaEgyatasaWRlZTnsq1u3rtGwYUPjxo0bDu1du3Y1AgMDjZycHMMwDKNPnz6Gh4eHkZycbO+TnZ1t1K1b15BkJCYm2tvz+3MYOnSocd999xmnTp1y6PfWW28ZkoxDhw451B8eHm5kZ2fb++3cudOQZHz66af2tpo1axo1a9Y0MjIy7vgzuvXz37t3b56xlixZcsf3AaUZl8wAk/r111+1detW9e7dW76+vr/bv3v37g7bDRo0kCSdOnXqnmt4+umnb9seEBCgRx55JM/x/sixpJvncGsWSpJ++uknHT161L7eKDs72/564oknlJSUpGPHjkmSNm/erLZt28rf39/+fjc3N/Xp0+ee6/n666/VunVrBQUFORy7c+fOkqStW7c69O/SpYvc3Nzs27/9Ozh+/LhOnDihQYMGqUKFCnc8bt++feXn56d58+bZ29599135+vr+ofMBSjICEWBSqampysnJ0f3335+v/lWrVnXYvnU5LSMj455rCAwMzNexbh3vjxzrdsc7f/68JGns2LEqV66cw2vYsGGSpIsXL0qSLl26pICAgDxj3q4tv86fP6+vvvoqz7Efeughh2Pf8nt/BxcuXJCk3/07dXd319ChQ7VixQqlpaXpwoUL+uyzz/TCCy+wyBymxRoiwKS8vb3l5uams2fPFtiY7u7ueRZjSzfDxO380TvKnPXb4/n4+EiSYmJi1LNnz9u+p06dOpJuhpHk5OQ8+2/Xlt+fg4+Pjxo0aKBp06bd9thBQUG3bb+TWzN9+fk7femllzRz5kx9/PHHun79urKzs/Xiiy86dTygNCEQASbl4eGhli1b6vPPP9e0adPs4eCPqFGjhvbv3+/QtmnTJl29evUPj10Y6tSpo1q1amnfvn2aPn36Xfu2bt1aX375pc6fP2+/bJaTk6NVq1bl6Zvfn0PXrl21du1a1axZU1WqVPmDZyPVrl1bNWvW1Mcff6zRo0ffdbYnMDBQvXr10nvvvaesrCx169ZN1atX/8M1ACUVl8wAE5s9e7Zu3Lihpk2basGCBdq8ebNWrlypfv366cqVK06PFxUVpW+//Vavv/66Nm7cqHfffVcvvfSS/Q6o4uiDDz7Qxo0b1bFjR3366af67rvvtGbNGs2YMUO9evWy9/v73/8uSWrTpo1WrVqlr776Sl26dNG1a9fyjJnfn8OUKVNUrlw5NW/eXPPnz9emTZu0du1avffee+rates9zd7NmzdPp06dUrNmzbR06VJt2bJFS5cuve1zmf7617/qxIkTOnPmjEaMGOH0sYDShBkiwMQiIiK0c+dOTZw4UTExMbpy5YoCAgLUpk0blS9f3unxXnnlFaWnp2vx4sV666239Mgjj+izzz7Tk08+WQjVF4zWrVtr586dmjZtmqKjo5WamqqqVauqfv366t27t71fWFiYNmzYoDFjxmjAgAGqUqWKoqKi9PTTT2vIkCEOY+b35xAYGKjdu3frjTfe0D/+8Q+dPXtWnp6eCg0NVadOne5p1qhjx4767rvvNGXKFI0aNUrXr1/X/fffn2dRvCQ98sgjqlGjhjw8PNS2bVunjwWUJhbDMAxXFwEAJdXixYv13HPPKTExUTVq1HB1OU7Zv3+/IiIiNG/ePPsicsCsmCECAJM5ceKETp06pVdffVWBgYF5np4NmBFriADAZN544w21b99eV69e1eeff66KFSu6uiTA5bhkBgAATI8ZIgAAYHoEIgAAYHoEIgAAYHrcZZZPubm5OnfunDw9PYv86wYAAMC9MQxDV65cUVBQkMqUufM8EIEon86dO6fg4GBXlwEAAO7BmTNn7vrFxwSifPL09JR08wfq5eXl4moAAEB+pKenKzg42P57/E4IRPl06zKZl5cXgQgAgBLm95a7sKgaAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYHoEIAACYXllXF4Dir3LlSFeXgCKUlhbn6hIAoMgxQwQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEzPpYFoxowZatKkiTw9PeXn56cePXro2LFjDn0GDhwoi8Xi8GrWrJlDn8zMTI0cOVI+Pj6qVKmSunfvrrNnzzr0SU1NVVRUlKxWq6xWq6KiopSWllbYpwgAAEoAlwairVu3avjw4YqPj1dsbKyys7PVoUMHXbt2zaFfp06dlJSUZH+tXbvWYX90dLRWr16tlStXatu2bbp69aq6du2qnJwce59+/fopISFB69at07p165SQkKCoqKgiOU8AAFC8lXXlwdetW+ewvWjRIvn5+WnPnj1q0aKFvd3d3V0BAQG3HcNms2nhwoVatmyZ2rVrJ0n65JNPFBwcrA0bNqhjx446cuSI1q1bp/j4eDVt2lSStGDBAkVGRurYsWOqU6dOIZ0hAAAoCYrVGiKbzSZJ8vb2dmjfsmWL/Pz8VLt2bQ0ePFgpKSn2fXv27NGNGzfUoUMHe1tQUJDCwsK0fft2SVJcXJysVqs9DElSs2bNZLVa7X1+KzMzU+np6Q4vAABQOhWbQGQYhkaPHq3HHntMYWFh9vbOnTtr+fLl2rRpk95++23t2rVLbdq0UWZmpiQpOTlZ5cuXV5UqVRzG8/f3V3Jysr2Pn59fnmP6+fnZ+/zWjBkz7OuNrFargoODC+pUAQBAMePSS2b/a8SIEdq/f7+2bdvm0N6nTx/7n8PCwtS4cWOFhITom2++Uc+ePe84nmEYslgs9u3//fOd+vyvmJgYjR492r6dnp5OKAIAoJQqFjNEI0eO1JdffqnNmzfr/vvvv2vfwMBAhYSE6Mcff5QkBQQEKCsrS6mpqQ79UlJS5O/vb+9z/vz5PGNduHDB3ue33N3d5eXl5fACAAClk0sDkWEYGjFihL744gtt2rRJoaGhv/ueS5cu6cyZMwoMDJQkNWrUSOXKlVNsbKy9T1JSkg4ePKjmzZtLkiIjI2Wz2bRz5057nx07dshms9n7AAAA83LpJbPhw4drxYoV+s9//iNPT0/7eh6r1SoPDw9dvXpVkyZN0tNPP63AwECdPHlSr776qnx8fPTUU0/Z+w4aNEhjxoxR1apV5e3trbFjxyo8PNx+11m9evXUqVMnDR48WB988IEkaciQIeratSt3mAEAANcGovnz50uSWrVq5dC+aNEiDRw4UG5ubjpw4ICWLl2qtLQ0BQYGqnXr1lq1apU8PT3t/efMmaOyZcuqd+/eysjIUNu2bbV48WK5ubnZ+yxfvlyjRo2y343WvXt3zZ07t/BPEgAAFHsWwzAMVxdREqSnp8tqtcpms5luPVHlypGuLgFFKC0tztUlAECBye/v72KxqBoAAMCVCEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0CEQAAMD0XBqIZsyYoSZNmsjT01N+fn7q0aOHjh075tDHMAxNmjRJQUFB8vDwUKtWrXTo0CGHPpmZmRo5cqR8fHxUqVIlde/eXWfPnnXok5qaqqioKFmtVlmtVkVFRSktLa2wTxEAAJQALg1EW7du1fDhwxUfH6/Y2FhlZ2erQ4cOunbtmr3PrFmzNHv2bM2dO1e7du1SQECA2rdvrytXrtj7REdHa/Xq1Vq5cqW2bdumq1evqmvXrsrJybH36devnxISErRu3TqtW7dOCQkJioqKKtLzBQAAxZPFMAzD1UXccuHCBfn5+Wnr1q1q0aKFDMNQUFCQoqOjNX78eEk3Z4P8/f315ptvaujQobLZbPL19dWyZcvUp08fSdK5c+cUHBystWvXqmPHjjpy5Ijq16+v+Ph4NW3aVJIUHx+vyMhIHT16VHXq1Pnd2tLT02W1WmWz2eTl5VV4P4RiqHLlSFeXgCKUlhbn6hIAoMDk9/d3sVpDZLPZJEne3t6SpMTERCUnJ6tDhw72Pu7u7mrZsqW2b98uSdqzZ49u3Ljh0CcoKEhhYWH2PnFxcbJarfYwJEnNmjWT1Wq19wEAAOZV1tUF3GIYhkaPHq3HHntMYWFhkqTk5GRJkr+/v0Nff39/nTp1yt6nfPnyqlKlSp4+t96fnJwsPz+/PMf08/Oz9/mtzMxMZWZm2rfT09Pv8cwAAEBxV2xmiEaMGKH9+/fr008/zbPPYrE4bBuGkaftt37b53b97zbOjBkz7AuwrVargoOD83MaAACgBCoWgWjkyJH68ssvtXnzZt1///329oCAAEnKM4uTkpJinzUKCAhQVlaWUlNT79rn/PnzeY574cKFPLNPt8TExMhms9lfZ86cufcTBAAAxZpLA5FhGBoxYoS++OILbdq0SaGhoQ77Q0NDFRAQoNjYWHtbVlaWtm7dqubNm0uSGjVqpHLlyjn0SUpK0sGDB+19IiMjZbPZtHPnTnufHTt2yGaz2fv8lru7u7y8vBxeAACgdHLpGqLhw4drxYoV+s9//iNPT0/7TJDVapWHh4csFouio6M1ffp01apVS7Vq1dL06dNVsWJF9evXz9530KBBGjNmjKpWrSpvb2+NHTtW4eHhateunSSpXr166tSpkwYPHqwPPvhAkjRkyBB17do1X3eYAQCA0u0PB6KcnBwdOHBAISEheRY2/5758+dLklq1auXQvmjRIg0cOFCSNG7cOGVkZGjYsGFKTU1V06ZNtX79enl6etr7z5kzR2XLllXv3r2VkZGhtm3bavHixXJzc7P3Wb58uUaNGmW/G6179+6aO3fuPZwxAAAobZx+DlF0dLTCw8M1aNAg5eTk2G+Br1ixor7++us84aa04DlEMAueQwSgNCm05xD9+9//VkREhCTpq6++UmJioo4eParo6Gi99tpr914xAACAizgdiC5evGi/+2vt2rXq1auXateurUGDBunAgQMFXiAAAEBhczoQ+fv76/Dhw8rJydG6devsC5d//fVXhzU7AAAAJYXTi6qfe+459e7dW4GBgbJYLGrfvr2km7ex161bt8ALBAAAKGxOB6JJkyYpLCxMZ86cUa9eveTu7i5JcnNz09/+9rcCLxAAAKCw3dNt93/+858lSdevX7e3DRgwoGAqAgAAKGJOryHKycnRG2+8oWrVqum+++7Tzz//LEmaMGGCFi5cWOAFAgAAFDanA9G0adO0ePFizZo1S+XLl7e3h4eH66OPPirQ4gAAAIqC04Fo6dKl+vDDD/Xss8863FXWoEEDHT16tECLAwAAKApOB6JffvlFDz74YJ723Nxc3bhxo0CKAgAAKEpOB6KHHnpI33//fZ72zz//XA0bNiyQogAAAIqS03eZTZw4UVFRUfrll1+Um5urL774QseOHdPSpUv19ddfF0aNAAAAhcrpGaJu3bpp1apVWrt2rSwWi15//XUdOXJEX331lf0hjQAAACXJPT2HqGPHjurYsWNB1wIAAOASTs8Q7dq1Szt27MjTvmPHDu3evbtAigIAAChKTgei4cOH68yZM3naf/nlFw0fPrxAigIAAChKTgeiw4cP609/+lOe9oYNG+rw4cMFUhQAAEBRcjoQubu76/z583nak5KSVLbsPS1JAgAAcCmnA1H79u0VExMjm81mb0tLS9Orr77KXWYAAKBEcnpK5+2331aLFi0UEhJifxBjQkKC/P39tWzZsgIvEAAAoLA5HYiqVaum/fv3a/ny5dq3b588PDz03HPPqW/fvipXrlxh1AgAAFCo7mnRT6VKlTRkyJCCrgUAAMAl7ikQHT9+XFu2bFFKSopyc3Md9r3++usFUhgAAEBRcToQLViwQC+99JJ8fHwUEBAgi8Vi33frqzwAAABKEqcD0dSpUzVt2jSNHz++MOoBAAAock7fdp+amqpevXoVRi0AAAAu4XQg6tWrl9avX18YtQAAALiE05fMHnzwQU2YMEHx8fEKDw/Pc6v9qFGjCqw4AACAomAxDMNw5g2hoaF3Hsxi0c8///yHiyqO0tPTZbVaZbPZ5OXl5epyilTlypGuLgFFKC0tztUlAECBye/vb6dniBITE/9QYQAAAMWN02uIbsnKytKxY8eUnZ1dkPUAAAAUOacD0a+//qpBgwapYsWKeuihh3T69GlJN9cOzZw5s8ALBAAAKGxOB6KYmBjt27dPW7ZsUYUKFezt7dq106pVqwq0OAAAgKLg9BqiNWvWaNWqVWrWrJnDU6rr16+vEydOFGhxAAAARcHpGaILFy7Iz88vT/u1a9ccAhIAAEBJ4XQgatKkib755hv79q0QtGDBAkVGcns2AAAoeZy+ZDZjxgx16tRJhw8fVnZ2tv75z3/q0KFDiouL09atWwujRgAAgELl9AxR8+bNtX37dv3666+qWbOm1q9fL39/f8XFxalRo0aFUSMAAEChcmqG6MaNGxoyZIgmTJigJUuWFFZNAAAARcqpGaJy5cpp9erVhVULAACASzh9yeypp57SmjVrCqEUAAAA17inb7t/4403tH37djVq1EiVKlVy2M+33QMAgJKGb7vPJ77tHmbBt90DKE34tnsAAIB8uudvuwcAACgtnJ4hev755++6/+OPP77nYgAAAFzB6UCUmprqsH3jxg0dPHhQaWlpatOmTYEVBgAAUFScDkS3ew5Rbm6uhg0bpgceeKBAigIAAChKBbKGqEyZMnr55Zc1Z86cghgOAACgSBXYouoTJ04oOzu7oIYDAAAoMk5fMhs9erTDtmEYSkpK0jfffKMBAwYUWGEAAABFxelAtHfvXoftMmXKyNfXV2+//fbv3oEGAABQHDkdiDZv3lwYdQAAALiM02uIEhMT9eOPP+Zp//HHH3Xy5MmCqAkAAKBIOR2IBg4cqO3bt+dp37FjhwYOHFgQNQEAABQppwPR3r179eijj+Zpb9asmRISEgqiJgAAgCLldCCyWCy6cuVKnnabzaacnBynxvruu+/UrVs3BQUFyWKxaM2aNQ77Bw4cKIvF4vBq1qyZQ5/MzEyNHDlSPj4+qlSpkrp3766zZ8869ElNTVVUVJSsVqusVquioqKUlpbmVK0AAKD0cjoQPf7445oxY4ZD+MnJydGMGTP02GOPOTXWtWvXFBERoblz596xT6dOnZSUlGR/rV271mF/dHS0Vq9erZUrV2rbtm26evWqunbt6lBfv379lJCQoHXr1mndunVKSEhQVFSUU7UCAIDSy+m7zGbNmqUWLVqoTp06evzxxyVJ33//vdLT07Vp0yanxurcubM6d+581z7u7u4KCAi47T6bzaaFCxdq2bJlateunSTpk08+UXBwsDZs2KCOHTvqyJEjWrduneLj49W0aVNJ0oIFCxQZGaljx46pTp06TtUMAABKH6dniOrXr6/9+/erd+/eSklJ0ZUrV9S/f38dPXpUYWFhBV7gli1b5Ofnp9q1a2vw4MFKSUmx79uzZ49u3LihDh062NuCgoIUFhZmX/gdFxcnq9VqD0PSzfVOVqv1tovDb8nMzFR6errDCwAAlE5OzxBJN0PH9OnTC7qWPDp37qxevXopJCREiYmJmjBhgtq0aaM9e/bI3d1dycnJKl++vKpUqeLwPn9/fyUnJ0uSkpOT5efnl2dsPz8/e5/bmTFjhiZPnlywJwQAAIolp2eIFi1apM8//zxP++eff64lS5YUSFG39OnTR126dFFYWJi6deumb7/9VsePH9c333xz1/cZhiGLxWLf/t8/36nPb8XExMhms9lfZ86cufcTAQAAxZrTgWjmzJny8fHJ0+7n51fos0aBgYEKCQmxPxgyICBAWVlZSk1NdeiXkpIif39/e5/z58/nGevChQv2Prfj7u4uLy8vhxcAACidnA5Ep06dUmhoaJ72kJAQnT59ukCKupNLly7pzJkzCgwMlCQ1atRI5cqVU2xsrL1PUlKSDh48qObNm0uSIiMjZbPZtHPnTnufHTt2yGaz2fsAAABzc3oNkZ+fn/bv368aNWo4tO/bt09Vq1Z1aqyrV6/qp59+sm8nJiYqISFB3t7e8vb21qRJk/T0008rMDBQJ0+e1KuvviofHx899dRTkiSr1apBgwZpzJgxqlq1qry9vTV27FiFh4fb7zqrV6+eOnXqpMGDB+uDDz6QJA0ZMkRdu3blDjMAACDpHgLRM888o1GjRsnT01MtWrSQJG3dulV//etf9cwzzzg11u7du9W6dWv79ujRoyVJAwYM0Pz583XgwAEtXbpUaWlpCgwMVOvWrbVq1Sp5enra3zNnzhyVLVtWvXv3VkZGhtq2bavFixfLzc3N3mf58uUaNWqU/W607t273/XZRwAAwFwshmEYzrwhKytLUVFR+vzzz1W27M08lZubq/79++v9999X+fLlC6VQV0tPT5fVapXNZjPdeqLKlSNdXQKKUFpanKtLAIACk9/f304HoluOHz+uffv2ycPDQ+Hh4QoJCbnnYksCAhHMgkAEoDTJ7+/ve3oOkSTVrl1btWrVknT729oBAABKCqfvMpOkpUuXKjw8XB4eHvLw8FCDBg20bNmygq4NAACgSDg9QzR79mxNmDBBI0aM0KOPPirDMPTDDz/oxRdf1MWLF/Xyyy8XRp0AAACFxulA9O6772r+/Pnq37+/ve3JJ5/UQw89pEmTJhGIAABAieP0JbOkpKTbPtCwefPmSkpKKpCiAAAAipLTgejBBx/UZ599lqd91apV9kXWAAAAJYnTl8wmT56sPn366LvvvtOjjz4qi8Wibdu2aePGjbcNSgAAAMWd0zNETz/9tHbs2CEfHx+tWbNGX3zxhXx8fLRz5077V2oAAACUJPf0HKJGjRrpk08+KehaAAAAXOKenkMEAABQmhCIAACA6RGIAACA6eUrEO3fv1+5ubmFXQsAAIBL5CsQNWzYUBcvXpQkPfDAA7p06VKhFgUAAFCU8hWIKleurMTEREnSyZMnmS0CAAClSr5uu3/66afVsmVLBQYGymKxqHHjxnJzc7tt359//rlACwQAAChs+QpEH374oXr27KmffvpJo0aN0uDBg+Xp6VnYtQEAABSJfD+YsVOnTpKkPXv26K9//SuBCAAAlBpOP6l60aJF9j+fPXtWFotF1apVK9CiAAAAipLTzyHKzc3VlClTZLVaFRISourVq6ty5cp64403WGwNAABKJKdniF577TUtXLhQM2fO1KOPPirDMPTDDz9o0qRJun79uqZNm1YYdQIAABQapwPRkiVL9NFHH6l79+72toiICFWrVk3Dhg0jEAEAgBLH6Utmly9fVt26dfO0161bV5cvXy6QogAAAIqS04EoIiJCc+fOzdM+d+5cRUREFEhRAAAARcnpS2azZs1Sly5dtGHDBkVGRspisWj79u06c+aM1q5dWxg1AgAAFCqnZ4hatmyp48eP66mnnlJaWpouX76snj176tixY3r88ccLo0YAAIBC5fQMkSQFBQWxeBoAAJQaTs8QAQAAlDYEIgAAYHoEIgAAYHpOBSLDMHTq1CllZGQUVj0AAABFzulAVKtWLZ09e7aw6gEAAChyTgWiMmXKqFatWrp06VJh1QMAAFDknF5DNGvWLL3yyis6ePBgYdQDAABQ5Jx+DtFf/vIX/frrr4qIiFD58uXl4eHhsJ/vMwMAACWN04HonXfeKYQyAAAAXMfpQDRgwIDCqAMAAMBl7uk5RCdOnNDf//539e3bVykpKZKkdevW6dChQwVaHAAAQFFwOhBt3bpV4eHh2rFjh7744gtdvXpVkrR//35NnDixwAsEAAAobE4Hor/97W+aOnWqYmNjVb58eXt769atFRcXV6DFAQAAFAWnA9GBAwf01FNP5Wn39fXl+UQAAKBEcjoQVa5cWUlJSXna9+7dq2rVqhVIUQAAAEXJ6UDUr18/jR8/XsnJybJYLMrNzdUPP/ygsWPHqn///oVRIwAAQKFyOhBNmzZN1atXV7Vq1XT16lXVr19fLVq0UPPmzfX3v/+9MGoEAAAoVBbDMIx7eeOJEye0d+9e5ebmqmHDhqpVq1ZB11aspKeny2q1ymazycvLy9XlFKnKlSNdXQKKUFoaN0cAKD3y+/vb6Qcz3lKzZk098MADkiSLxXKvwwAAALjcPT2YceHChQoLC1OFChVUoUIFhYWF6aOPPiro2gAAAIqE0zNEEyZM0Jw5czRy5EhFRt68lBIXF6eXX35ZJ0+e1NSpUwu8SAAAgMLk9BoiHx8fvfvuu+rbt69D+6effqqRI0fq4sWLBVpgccEaIpgFa4gAlCb5/f3t9CWznJwcNW7cOE97o0aNlJ2d7exwAAAALud0IPrLX/6i+fPn52n/8MMP9eyzzxZIUQAAAEUpX2uIRo8ebf+zxWLRRx99pPXr16tZs2aSpPj4eJ05c4YHMwIAgBIpX4Fo7969DtuNGjWSdPNZRNLN7zHz9fXVoUOHCrg8AACAwpevQLR58+bCrgMAAMBl7uk5RAXlu+++U7du3RQUFCSLxaI1a9Y47DcMQ5MmTVJQUJA8PDzUqlWrPLNQmZmZGjlypHx8fFSpUiV1795dZ8+edeiTmpqqqKgoWa1WWa1WRUVFKS0trZDPDgAAlBROB6Lr16/rH//4h5544gk1btxYf/rTnxxezrh27ZoiIiI0d+7c2+6fNWuWZs+erblz52rXrl0KCAhQ+/btdeXKFXuf6OhorV69WitXrtS2bdt09epVde3aVTk5OfY+/fr1U0JCgtatW6d169YpISFBUVFRzp46AAAopZx+DlG/fv0UGxurP//5z/L398/ztR0TJ068t0IsFq1evVo9evSQdHN2KCgoSNHR0Ro/frykm7NB/v7+evPNNzV06FDZbDb5+vpq2bJl6tOnjyTp3LlzCg4O1tq1a9WxY0cdOXJE9evXV3x8vJo2bSrp5iLwyMhIHT16VHXq1MlXfTyHCGbBc4gAlCaF9l1m33zzjdauXatHH330DxX4exITE5WcnKwOHTrY29zd3dWyZUtt375dQ4cO1Z49e3Tjxg2HPkFBQQoLC9P27dvVsWNHxcXFyWq12sOQJDVr1kxWq1Xbt2+/YyDKzMxUZmamfTs9Pb0QzhIAABQHTl8yq1atmjw9PQujFgfJycmSJH9/f4d2f39/+77k5GSVL19eVapUuWsfPz+/POP7+fnZ+9zOjBkz7GuOrFargoOD/9D5AACA4svpQPT2229r/PjxOnXqVGHUk8dvL8kZhpGn7bd+2+d2/X9vnJiYGNlsNvvrzJkzTlYOAABKCqcvmTVu3FjXr1/XAw88oIoVK6pcuXIO+y9fvlwghQUEBEi6OcMTGBhob09JSbHPGgUEBCgrK0upqakOs0QpKSlq3ry5vc/58+fzjH/hwoU8s0//y93dXe7u7gVyLgAAoHhzOhD17dtXv/zyi6ZPn37bRdUFJTQ0VAEBAYqNjVXDhg0lSVlZWdq6davefPNNSTcfEFmuXDnFxsaqd+/ekqSkpCQdPHhQs2bNkiRFRkbKZrNp586deuSRRyRJO3bskM1ms4cmAABgbk4Hou3btysuLk4RERF/+OBXr17VTz/9ZN9OTExUQkKCvL29Vb16dUVHR2v69OmqVauWatWqpenTp6tixYrq16+fJMlqtWrQoEEaM2aMqlatKm9vb40dO1bh4eFq166dJKlevXrq1KmTBg8erA8++ECSNGTIEHXt2jXfd5gBAIDSzelAVLduXWVkZBTIwXfv3q3WrVvbt299Z9qAAQO0ePFijRs3ThkZGRo2bJhSU1PVtGlTrV+/3mFR95w5c1S2bFn17t1bGRkZatu2rRYvXiw3Nzd7n+XLl2vUqFH2u9G6d+9+x2cfAQAA83H6OUTr16/X5MmTNW3aNIWHh+dZQ1Ran9HDc4hgFjyHCEBpUmjPIerUqZMkqW3btg7tt+7a+t8nRAMAAJQETgcivugVAACUNk4HopYtWxZGHQAAAC7jdCD67rvv7rq/RYsW91wMAACAKzgdiFq1apWn7X+fRcQaIgAAUNI4/dUdqampDq+UlBStW7dOTZo00fr16wujRgAAgELl9AyR1WrN09a+fXu5u7vr5Zdf1p49ewqkMAAAgKLi9AzRnfj6+urYsWMFNRwAAECRcXqGaP/+/Q7bhmEoKSlJM2fOLJCv8wAAAChqTgeihx9+WBaLRb99wHWzZs308ccfF1hhAAAARcXpQJSYmOiwXaZMGfn6+qpChQoFVhQAAEBRcjoQhYSEFEYdAAAALuN0IJKkjRs3auPGjUpJSVFubq7DPi6bAQCAksbpQDR58mRNmTJFjRs3VmBgoMNDGQEAAEoipwPR+++/r8WLFysqKqow6gEAAChyTj+HKCsrS82bNy+MWgAAAFzC6UD0wgsvaMWKFYVRCwAAgEs4fcns+vXr+vDDD7VhwwY1aNBA5cqVc9g/e/bsAisOAACgKNzTk6offvhhSdLBgwcd9rHAGgAAlEROB6LNmzcXRh0AAAAuU2Bf7goAAFBSEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpEYgAAIDpFetANGnSJFksFodXQECAfb9hGJo0aZKCgoLk4eGhVq1a6dChQw5jZGZmauTIkfLx8VGlSpXUvXt3nT17tqhPBQAAFGPFOhBJ0kMPPaSkpCT768CBA/Z9s2bN0uzZszV37lzt2rVLAQEBat++va5cuWLvEx0drdWrV2vlypXatm2brl69qq5duyonJ8cVpwMAAIqhsq4u4PeULVvWYVboFsMw9M477+i1115Tz549JUlLliyRv7+/VqxYoaFDh8pms2nhwoVatmyZ2rVrJ0n65JNPFBwcrA0bNqhjx45Fei4AAKB4KvYzRD/++KOCgoIUGhqqZ555Rj///LMkKTExUcnJyerQoYO9r7u7u1q2bKnt27dLkvbs2aMbN2449AkKClJYWJi9z51kZmYqPT3d4QUAAEqnYh2ImjZtqqVLl+q///2vFixYoOTkZDVv3lyXLl1ScnKyJMnf39/hPf7+/vZ9ycnJKl++vKpUqXLHPncyY8YMWa1W+ys4OLgAzwwAABQnxfqSWefOne1/Dg8PV2RkpGrWrKklS5aoWbNmkiSLxeLwHsMw8rT9Vn76xMTEaPTo0fbt9PR0QhGAUuenv1ZwdQkoQg/+87qrSyi2ivUM0W9VqlRJ4eHh+vHHH+3rin4705OSkmKfNQoICFBWVpZSU1Pv2OdO3N3d5eXl5fACAAClU4kKRJmZmTpy5IgCAwMVGhqqgIAAxcbG2vdnZWVp69atat68uSSpUaNGKleunEOfpKQkHTx40N4HAACgWF8yGzt2rLp166bq1asrJSVFU6dOVXp6ugYMGCCLxaLo6GhNnz5dtWrVUq1atTR9+nRVrFhR/fr1kyRZrVYNGjRIY8aMUdWqVeXt7a2xY8cqPDzcftcZAABAsQ5EZ8+eVd++fXXx4kX5+vqqWbNmio+PV0hIiCRp3LhxysjI0LBhw5SamqqmTZtq/fr18vT0tI8xZ84clS1bVr1791ZGRobatm2rxYsXy83NzVWnBQAAihmLYRiGq4soCdLT02W1WmWz2Uy3nqhy5UhXl4AilJYW5+oSUIRYVG0uZlxUnd/f3yVqDREAAEBhIBABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTM1Ugeu+99xQaGqoKFSqoUaNG+v77711dEgAAKAZME4hWrVql6Ohovfbaa9q7d68ef/xxde7cWadPn3Z1aQAAwMVME4hmz56tQYMG6YUXXlC9evX0zjvvKDg4WPPnz3d1aQAAwMVMEYiysrK0Z88edejQwaG9Q4cO2r59u4uqAgAAxUVZVxdQFC5evKicnBz5+/s7tPv7+ys5Ofm278nMzFRmZqZ922azSZLS09MLr9BiyjCyXV0CipAZ/42b2ZVMw9UloAiZ8fN965wN4+7/1k0RiG6xWCwO24Zh5Gm7ZcaMGZo8eXKe9uDg4EKpDSgurFarq0sAUFg+MO/n+8qVK3f9/80UgcjHx0dubm55ZoNSUlLyzBrdEhMTo9GjR9u3c3NzdfnyZVWtWvWOIQqlR3p6uoKDg3XmzBl5eXm5uhwABYjPt7kYhqErV64oKCjorv1MEYjKly+vRo0aKTY2Vk899ZS9PTY2Vk8++eRt3+Pu7i53d3eHtsqVKxdmmSiGvLy8+A8TKKX4fJtHfma+TRGIJGn06NGKiopS48aNFRkZqQ8//FCnT5/Wiy++6OrSAACAi5kmEPXp00eXLl3SlClTlJSUpLCwMK1du1YhISGuLg0AALiYaQKRJA0bNkzDhg1zdRkoAdzd3TVx4sQ8l00BlHx8vnE7FuP37kMDAAAo5UzxYEYAAIC7IRABAADTIxABAADTIxABtzFp0iQ9/PDDri4DQD6dPHlSFotFCQkJri4FJRSBCAAAmB6BCChCN27ccHUJAJyQlZXl6hJQRAhEKPZatWqlUaNGady4cfL29lZAQIAmTZrk0Of06dN68skndd9998nLy0u9e/fW+fPn7zru2bNn9cwzz8jb21uVKlVS48aNtWPHDoc+y5YtU40aNWS1WvXMM8/oypUr9n01atTQO++849D/4YcfdqjNYrHo/fff15NPPqlKlSpp6tSp9stxdxsbwO3l5ubqzTff1IMPPih3d3dVr15d06ZNs+//+eef1bp1a1WsWFERERGKi4uz77vdpfB33nlHNWrUsG8PHDhQPXr00IwZMxQUFKTatWvbL8d98cUXdxwbJR+BCCXCkiVLVKlSJe3YsUOzZs3SlClTFBsbK+nmF/f16NFDly9f1tatWxUbG6sTJ06oT58+dxzv6tWratmypc6dO6cvv/xS+/bt07hx45Sbm2vvc+LECa1Zs0Zff/21vv76a23dulUzZ850uvaJEyfqySef1IEDB/T8888X6NiA2cTExOjNN9/UhAkTdPjwYa1YscLhS7pfe+01jR07VgkJCapdu7b69u2r7Oxsp46xceNGHTlyRLGxsfr6668LdGwUYwZQzLVs2dJ47LHHHNqaNGlijB8/3jAMw1i/fr3h5uZmnD592r7/0KFDhiRj586dtx3zgw8+MDw9PY1Lly7ddv/EiRONihUrGunp6fa2V155xWjatKl9OyQkxJgzZ47D+yIiIoyJEyfatyUZ0dHRTo8NIK/09HTD3d3dWLBgQZ59iYmJhiTjo48+srfd+n/gyJEjhmHc/OxFREQ4vG/OnDlGSEiIfXvAgAGGv7+/kZmZ6dTYKPmYIUKJ0KBBA4ftwMBApaSkSJKOHDmi4OBgBQcH2/fXr19flStX1pEjR247XkJCgho2bChvb+87HrNGjRry9PS87TGd0bhx40IbGzCTI0eOKDMzU23btr1jn//9vyIwMFCSnP5shYeHq3z58oUyNoovAhFKhHLlyjlsWywW++UtwzBksVjyvOdO7ZLk4eHxh44pSWXKlJHxm2++ud2i6UqVKjk9NoC8nP3c3vr83/ps/ZHP7O+NjZKPQIQSr379+jp9+rTOnDljbzt8+LBsNpvq1at32/c0aNBACQkJunz58j0f19fXV0lJSfbt9PR0JSYm3vN4AO6uVq1a8vDw0MaNG+/p/b6+vkpOTnYIRTy3CLcQiFDitWvXTg0aNNCzzz6r//f//p927typ/v37q2XLlre9XCVJffv2VUBAgHr06KEffvhBP//8s/7v//7PqbtG2rRpo2XLlun777/XwYMHNWDAALm5uRXUaQH4jQoVKmj8+PEaN26cli5dqhMnTig+Pl4LFy7M1/tbtWqlCxcuaNasWTpx4oTmzZunb7/9tpCrRklBIEKJZ7FYtGbNGlWpUkUtWrRQu3bt9MADD2jVqlV3fE/58uW1fv16+fn56YknnlB4eLhmzpzpVKCJiYlRixYt1LVrVz3xxBPq0aOHatasWRCnBOAOJkyYoDFjxuj1119XvXr11KdPn3yv46lXr57ee+89zZs3TxEREdq5c6fGjh1byBWjpLAYv72gCgAAYDLMEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEo1U6ePCmLxcJXNAC4KwIRAAAwPQIRAAAwPQIRgFIhNzdXb775ph588EG5u7urevXqmjZtWp5+OTk5GjRokEJDQ+Xh4aE6deron//8p0OfLVu26JFHHlGlSpVUuXJlPfroozp16pQkad++fWrdurU8PT3l5eWlRo0aaffu3UVyjgAKT1lXFwAABSEmJkYLFizQnDlz9NhjjykpKUlHjx7N0y83N1f333+/PvvsM/n4+Gj79u0aMmSIAgMD1bt3b2VnZ6tHjx4aPHiwPv30U2VlZWnnzp2yWCySpGeffVYNGzbU/Pnz5ebmpoSEBJUrV66oTxdAAePLXQGUeFeuXJGvr6/mzp2rF154wWHfyZMnFRoaqr179+rhhx++7fuHDx+u8+fP69///rcuX76sqlWrasuWLWrZsmWevl5eXnr33Xc1YMCAwjgVAC7CJTMAJd6RI0eUmZmptm3b5qv/+++/r8aNG8vX11f33XefFixYoNOnT0uSvL29NXDgQHXs2FHdunXTP//5TyUlJdnfO3r0aL3wwgtq166dZs6cqRMnThTKOQEoWgQiACWeh4dHvvt+9tlnevnll/X8889r/fr1SkhI0HPPPaesrCx7n0WLFikuLk7NmzfXqlWrVLt2bcXHx0uSJk2apEOHDqlLly7atGmT6tevr9WrVxf4OQEoWlwyA1DiXb9+Xd7e3vrXv/71u5fMRo4cqcOHD2vjxo32Pu3atdPFixfv+KyiyMhINWnSRP/617/y7Ovbt6+uXbumL7/8skDPCUDRYoYIQIlXoUIFjR8/XuPGjdPSpUt14sQJxcfHa+HChXn6Pvjgg9q9e7f++9//6vjx45owYYJ27dpl35+YmKiYmBjFxcXp1KlTWr9+vY4fP6569eopIyNDI0aM0JYtW3Tq1Cn98MMP2rVrl+rVq1eUpwugEHCXGYBSYcKECSpbtqxef/11nTt3ToGBgXrxxRfz9HvxxReVkJCgPn36yGKxqG/fvho2bJi+/fZbSVLFihV19OhRLVmyRJcuXVJgYKBGjBihoUOHKjs7W5cuXVL//v11/vx5+fj4qGfPnpo8eXJRny6AAsYlMwAAYHpcMgMAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKZHIAIAAKb3/wElcDgxc/WRGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"customer_churn_data.csv\")\n",
    "y = df.pop(\"Churn\")\n",
    "fn_cost = df.pop(\"FN\")\n",
    "fp_cost = df.pop(\"FP\")\n",
    "\n",
    "class_freq = y.value_counts()\n",
    "class_freq.index = [\"no churn\", \"churn\"]\n",
    "\n",
    "ax = class_freq.plot.bar(rot=0, color=[\"#0f0f38\", \"#db6e07\"], title=\"churn frequency\")\n",
    "ax.set_xlabel(\"class\")\n",
    "ax.set_ylabel(\"number of occurences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    fn_cost_train,\n",
    "    fn_cost_test,\n",
    "    fp_cost_train,\n",
    "    fp_cost_test,\n",
    ") = train_test_split(\n",
    "    df, y, fn_cost, fp_cost, test_size=0.25, random_state=0, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 6 (1.5 punktu)**\n",
    "\n",
    "1. Wytrenuj regresję logistyczną jako baseline, dokonując tuningu analogicznie do zadania 2. Pamiętaj o ustawieniu `random_state=0`. Zmierz czas treningu (włącznie z tuningiem). Sprawdź F1-score na zbiorze testowym\n",
    "2. Wytrenuj liniowy SVM:\n",
    "   - ustaw maksymalnie 10000 iteracji i zbalansowane wagi klas\n",
    "   - dokonaj tuningu hiperparametru `C` za pomocą 5-krotnej walidacji skrośnej\n",
    "   - sprawdź 100 wartości z zakresu od $10^{-2}$ do $10^2$\n",
    "   - wypisz znalezioną optymalną wartość `C`\n",
    "   - wybierz model o najwyższym F1-score\n",
    "   - zmierz czas treningu (włącznie z tuningiem)\n",
    "   - sprawdź F1-score na zbiorze testowym\n",
    "   - pamiętaj o ustawieniu `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_time(model, model_name, X, y):\n",
    "    start_time = time()\n",
    "    model.fit(X, y)\n",
    "    end_time = time()\n",
    "    print(f\"{model_name}: {end_time - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 2.8938262462615967s\n",
      "F1-score: 61.05%\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(\n",
    "    penalty=\"l2\", random_state=0, max_iter=10000, class_weight=\"balanced\", n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\"C\": np.linspace(1e-1, 1e2, 100)}\n",
    "\n",
    "logistic_cv = GridSearchCV(\n",
    "    estimator=logistic_reg, cv=5, param_grid=param_grid, scoring=\"f1\", n_jobs=-1\n",
    ")\n",
    "\n",
    "print_training_time(logistic_cv, \"Logistic Regression\", X_train, y_train)\n",
    "logistic_reg.set_params(**logistic_cv.best_params_)\n",
    "\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "y_pred = logistic_reg.predict(X_test)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC(\n",
    "    loss=\"hinge\", class_weight=\"balanced\", random_state=0, max_iter=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/domi/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM: 13.485491752624512s\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": np.linspace(1e-2, 1e2, 100)}\n",
    "\n",
    "linear_cv = GridSearchCV(\n",
    "    estimator=linear_svm, param_grid=param_grid, scoring=\"f1\", n_jobs=-1, cv=5\n",
    ")\n",
    "\n",
    "print_training_time(linear_cv, \"Linear SVM\", X_train, y_train)\n",
    "C = linear_cv.best_params_[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 42.43\n",
      "F1-score: 59.13%\n"
     ]
    }
   ],
   "source": [
    "print(\"C:\", C)\n",
    "linear_svm.set_params(**{\"C\": C})\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 7 (2 punkty)**\n",
    "\n",
    "1. Wytrenuj kernel SVM z domyślnymi hiperparametrami:\n",
    "   - ustaw zbalansowane wagi klas\n",
    "   - zmierz czas treningu (włącznie z tuningiem)\n",
    "   - sprawdź F1-score na zbiorze testowym\n",
    "   - zmierz czas treningu\n",
    "   - pamiętaj o ustawieniu `random_state=0`\n",
    "2. Wytrenuj kernel SVM z tuningiem hiperparametrów:\n",
    "   - sprawdź kilka siatek hiperparametrów (`param_grid` może być listą słowników!)\n",
    "   - dla kerneli RBF sprawdź 10 wartości `C` z zakresu od $10^{-2}$ do $10^2$, a dla `gamma` sprawdź wartość `\"scale\"` oraz 10 wartości z zakresu od $10^{-2}$ do $10^2$\n",
    "   - dla kernela wielomianowego sprawdź 100 wartości `C` z zakresu od $10^{-2}$ do $10^2$ oraz stopnie wielomianu z zakresu [2, 5]\n",
    "   - dla kernela sigmoidalnego sprawdź te same zakresy wartości, co dla RBF\n",
    "3. Dokonaj analizy hiperparametrów po tuningu:\n",
    "   - sprawdź, dla jakich hiperparametrów osiągnięto najwyższy wynik walidacji skrośnej\n",
    "   - sprawdź, jaki był najlepszy wynik osiągnięty dla każdego z trzech kerneli\n",
    "   - wypisz hiperparametry dla każdego z tych przypadków\n",
    "   - który kernel wymagał najmocniejszej regularyzacji i jak sądzisz, czemu ten?\n",
    "   - wytrenuj SVM dla optymalnych hiperparametrów dla każdego z trzech kerneli i sprawdź F1-score na zbiorze testowym\n",
    "   - czy udało się dobrze oszacować wynik za pomocą walidacji skrośnej, tj. czy faktycznie kernel o najwyższym wyniku walidacyjnym miał najwyższy wynik testowy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel SVM: 0.07637786865234375s\n",
      "F1-score: 65.71%\n"
     ]
    }
   ],
   "source": [
    "kernel_svm = SVC(class_weight=\"balanced\", random_state=0)\n",
    "print_training_time(kernel_svm, \"Kernel SVM\", X_train, y_train)\n",
    "y_pred = kernel_svm.predict(X_test)\n",
    "print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"kernel\": [\"rbf\", \"sigmoid\"],\n",
    "        \"C\": np.linspace(1e-2, 1e2, 10),\n",
    "        \"gamma\": [\"scale\"] + list(np.linspace(1e-2, 1e2, 10)),\n",
    "    },\n",
    "    {\"kernel\": [\"poly\"], \"C\": np.linspace(1e-2, 1e2, 100), \"degree\": list(range(2, 6))},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'C': 77.78, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "kernel_svm_cv = GridSearchCV(\n",
    "    estimator=kernel_svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "kernel_svm_cv.fit(X_train, y_train)\n",
    "\n",
    "best_params = kernel_svm_cv.best_params_\n",
    "results = kernel_svm_cv.cv_results_\n",
    "print(\"best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best(best):\n",
    "    score, params = best\n",
    "    kernel = params[\"kernel\"]\n",
    "    print(f\"Kernel: {kernel} score: {100*score:.2f}% params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf score: 83.53% params: {'C': 77.78, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Kernel: poly score: 83.46% params: {'C': 83.84, 'degree': 5, 'kernel': 'poly'}\n",
      "Kernel: sigmoid score: 64.24% params: {'C': 11.12, 'gamma': 0.01, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "mean_scores = list(zip(results[\"mean_test_score\"], results[\"params\"]))\n",
    "\n",
    "best_rbf = list(filter(lambda k: \"rbf\" in k[1][\"kernel\"], mean_scores))\n",
    "best_rbf.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "best_poly = list(filter(lambda k: \"poly\" in k[1][\"kernel\"], mean_scores))\n",
    "best_poly.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "best_sigmoid = list(filter(lambda k: \"sigmoid\" in k[1][\"kernel\"], mean_scores))\n",
    "best_sigmoid.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "print_best(best_rbf[0])\n",
    "print_best(best_poly[0])\n",
    "print_best(best_sigmoid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najwięcej regularyzacji wymagał kernel sigmoidalny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kernel(params, kernel_name, X_train, y_train, y_test):\n",
    "    clf = SVC(class_weight=\"balanced\", random_state=0)\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Kernel: {kernel_name}\", end=\" \")\n",
    "    print_f1(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: rbf F1-score: 84.70%\n",
      "Kernel: poly F1-score: 83.02%\n",
      "Kernel: sigmoid F1-score: 58.38%\n"
     ]
    }
   ],
   "source": [
    "check_kernel(best_rbf[0][1], \"rbf\", X_train, y_train, y_test)\n",
    "check_kernel(best_poly[0][1], \"poly\", X_train, y_train, y_test)\n",
    "check_kernel(best_sigmoid[0][1], \"sigmoid\", X_train, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel o najwyższym wyniku faktycznie miał najwyższy wynik testowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie uwzględniliśmy jednak jeszcze faktu, że nasi klienci mają różną wagę. Według badań koszt zyskania klienta jest około 15 razy wyższy od kosztu utrzymania klienta, który jest zagrożony przejściem dla konkurencji. W związku z tym FP i FN mają różną ważnośc. Dodatkowo sami klienci płacą mniej lub więcej, więc to też trzeba uwzględnić. Na szczęście my mamy już to zrobione, bo mamy macierze `fp_cost` oraz `fn_cost` oznaczające koszt pomylenia się na oba sposoby w przypadku każdego klienta.\n",
    "\n",
    "Powinniśmy zatem używać **ważonej metryki (weighted metric)**, która dla każdego przykładu stosuje odpowiednią wagę, i to w zależności od tego, w jaki sposób się pomyliliśmy. Tego typu metryki typowo działają jak funkcje kosztu - im mniejsza wartość, tym lepiej, bo często dosłownie reprezentują koszt, jaki firma musi ponieść przy stosowaniu danego algorytmu. Wykorzystanie takiej metryki to **cost-sensitive classification**.\n",
    "\n",
    "**Zadanie 7 (0.5 punktu)**\n",
    "\n",
    "1. Uzupełnij kod funkcji `churn_cost()`, która oblicza koszt dokonania danej klasyfikacji.\n",
    "2. Stwórz funkcję w postaci akceptowanej przez `GridSearchCV` za pomocą `make_scorer`. Zwróć uwagę, żeby podać do niej prawidłowe argumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def churn_cost(\n",
    "    y_true: Union[np.ndarray, pd.Series],\n",
    "    y_pred: Union[np.ndarray, pd.Series],\n",
    "    fp_cost: Union[np.ndarray, pd.Series],\n",
    "    fn_cost: Union[np.ndarray, pd.Series],\n",
    ") -> float:\n",
    "    # make sure all data is Numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    fp_cost = np.array(fp_cost)\n",
    "    fn_cost = np.array(fn_cost)\n",
    "\n",
    "    # check which rows are for FP and FN\n",
    "\n",
    "    # get costs for FP and FN\n",
    "    fp = fn = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 0:\n",
    "            fp += fp_cost[i]\n",
    "\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            fn += fn_cost[i]\n",
    "\n",
    "    # return sum of costs\n",
    "    return fp + fn\n",
    "\n",
    "\n",
    "churn_cost_score = make_scorer(\n",
    "    churn_cost,\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 8 (1 punkt)**\n",
    "\n",
    "1. Wytrenuj klasyfikatory stały `DummyClassifier`:\n",
    "   - taki, który zawsze przewiduje klasę negatywną (brak zmiany)\n",
    "   - taki, który zawsze przewiduje klasę pozytywną (churn)\n",
    "2. Sprawdź metrykę F1-score oraz nasz nowo zdefiniowany churn cost dla takich rozwiązań na zbiorze testowym. Wykorzystaj `fp_cost_test` oraz `fn_cost_test`.\n",
    "3. Lepiej jest stosować reklamy dla wszystkich, czy po prostu zgodzić się na churn i nic z tym nie robić? Uwzględniając liczbę próbek w zbiorze testowym, czy twoim zdaniem wysoki koszt? Załóż, że walutą są dolary - irański rial zawsze był tak słabą walutą, że i tak używano walut zagranicznych, co widać też w tym zbiorze. Wartość każdego klienta jest typowo wysoka, bo to obliczona tzw. lifetime value - przewidywana potencjalna wartość klienta w przyszłości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative F1-score: 0.00%\n",
      "positive F1-score: 27.19%\n",
      "negative cost: 15871.64\n",
      "positive cost: 69593.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "negative_dummy = DummyClassifier(strategy=\"constant\", random_state=0, constant=0)\n",
    "positive_dummy = DummyClassifier(strategy=\"constant\", random_state=0, constant=1)\n",
    "\n",
    "negative_dummy.fit(X_train, y_train)\n",
    "positive_dummy.fit(X_train, y_train)\n",
    "\n",
    "negative_pred = negative_dummy.predict(X_test)\n",
    "positive_pred = positive_dummy.predict(X_test)\n",
    "\n",
    "negative_cost = churn_cost(y_test, negative_pred, fp_cost_test, fn_cost_test)\n",
    "positive_cost = churn_cost(y_test, positive_pred, fp_cost_test, fn_cost_test)\n",
    "\n",
    "print(\"negative \", end=\"\")\n",
    "print_f1(y_test, negative_pred)\n",
    "\n",
    "print(\"positive \", end=\"\")\n",
    "print_f1(y_test, positive_pred)\n",
    "\n",
    "print(f\"negative cost: {negative_cost:.2f}\")\n",
    "print(f\"positive cost: {positive_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.141680203045684\n",
      "88.31624302030455\n"
     ]
    }
   ],
   "source": [
    "negative_ratio = negative_cost / len(y_test)\n",
    "positive_ratio = positive_cost / len(y_test)\n",
    "print(negative_ratio)\n",
    "print(positive_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jeśli będziemy przewidywać że nie nastąpi odpływ to koszt \"pomyłek\" będzie znacznie mniejszy niż jeśli będziemy przewidywać że odpływ nastąpi, dlatego lepiej zgodzić się na churn i nic z tym nie robić niż jak będziemy stosować reklamy dla wszystkich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 9 (1 punkt)**\n",
    "\n",
    "1. Wytrenuj liniowy SVM, wykorzystaj znalezioną wcześniej optymalną wartość `C`. Innych parametrów użyj tak, jak wcześniej. Jaki jest koszt przy wdrożeniu takiego rozwiązania? Ile zyskujemy względem najlepszego rozwiązania z poprzedniego zadania?\n",
    "2. Wytrenuj kernel SVM, wykorzystaj znalezione wcześniej optymalne wartości hiperparametrów. Innych parametrów użyj tak, jak wcześniej. O ile lepszy jest koszt? O ile lepszy jest od modelu liniowego, jakim jest liniowy SVM?\n",
    "3. Czy twoim zdaniem warto wdrażać rozwiązania oparte o ML do takich zadań?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear svm churn cost: 11629.98\n",
      "linear cost profit from negative cost: 4241.67\n"
     ]
    }
   ],
   "source": [
    "linear_svm = LinearSVC(\n",
    "    loss=\"hinge\", class_weight=\"balanced\", random_state=0, max_iter=10000, C=42.43\n",
    ")\n",
    "\n",
    "linear_svm.fit(X_train, y_train)\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "linear_cost = churn_cost(y_test, y_pred, fp_cost_test, fn_cost_test)\n",
    "\n",
    "print(f\"linear svm churn cost: {linear_cost:.2f}\")\n",
    "print(f\"linear cost profit from negative cost: {(negative_cost - linear_cost):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel svm churn cost: 3547.27\n",
      "kernel cost profit from linear svm cost: 8082.70\n"
     ]
    }
   ],
   "source": [
    "kernel_svm = SVC(class_weight=\"balanced\", random_state=0)\n",
    "kernel_svm.set_params(**kernel_svm_cv.best_params_)\n",
    "\n",
    "kernel_svm.fit(X_train, y_train)\n",
    "y_pred = kernel_svm.predict(X_test)\n",
    "kernel_cost = churn_cost(y_test, y_pred, fp_cost_test, fn_cost_test)\n",
    "print(f\"kernel svm churn cost: {kernel_cost:.2f}\")\n",
    "print(f\"kernel cost profit from linear svm cost: {(linear_cost - kernel_cost):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage profit from the best dummy classifier: 77.6502516059458%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"percentage profit from the best dummy classifier: {(1 - (kernel_cost / negative_cost))*100}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdecydowanie warto wdrażać rozwiązania oparte o ML do takich zadań. W tym przypadku udało nam się dzięki kernel svm obniżyć najniższy koszt pomyłki przy stałym przewidywaniu odpływu o 77%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)\n",
    "\n",
    "W przypadku, kiedy zastosowanie kernel SVM wprost byłoby zbyt kosztowne, można zamiast tego przybliżyć cechy kernelowe. Realizują to algorytmy grupy kernel approximation.\n",
    "\n",
    "Metoda Nyströma to algorytm uniwersalny, który potrafi przybliżyć dowolny kernel. Wykorzystuje do tego truncated SVD, czyli przybliża wprost kernel matrix za pomocą macierzy niższej rangi. Ceną tej prostoty i uniwersalności jest nie najlepsza prędkość działania w porównaniu do bardziej specjalistycznych algorytmów. Proste wyprowadzenie tej metody można znaleźć [tutaj](https://stats.stackexchange.com/questions/261149/nystroem-method-for-kernel-approximation).\n",
    "\n",
    "`RBFSampler` w Scikit-learn implementuje metodę Random Kitchen Sinks. Ten algorytm za pomocą próbkowania Monte Carlo przybliża coraz lepiej macierz kernela RBF. W bibliotece Scikit-learn-extra zaimplementowane jest rozwinięcie tego algorytmu o nazwie [Fastfood](https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.kernel_approximation.Fastfood.html), który stosuje nieco bardziej zaawansowany algorytm przybliżający, który powinien być jeszcze szybszy i oszczędniejszy pamięciowo.\n",
    "\n",
    "Hiperparametry tych metod są podobne do zwykłego kernel SVM, ale dodatkowo mamy wymiarowość naszego przybliżenia, czyli `n_components`. Im większe, tym dokładniej przybliżamy, ale też tym więcej mamy cech. Wejściem do metody Nyströma jest macierz cech X, a wyjściem macierz cech kernelowych X mająca rozmiar `n_samples * n_components`. Na takich cechach trenuje się już zwykły liniowy SVM.\n",
    "\n",
    "1. Zaimplementuj `Pipeline` składający się z przybliżenia kernela oraz liniowego SVM: dla metody Nyströma, oraz dla obu algorytmów dla kernela RBF.\n",
    "2. Wytrenuj algorytmy z domyślnymi hiperparametrami. Porównaj jakość predykcji oraz szybkość z liniowym SVM i z kernel SVM. Czy warto dokonać takiej aproksymacji?\n",
    "3. Dokonaj optymalizacji hiperparametrów `C`, `gamma` oraz `n_components`. Czy udało się poprawić wynik względem zwykłego kernel SVM w rozsądnym czasie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
