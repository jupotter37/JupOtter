{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "Shakespeare example"
      ],
      "metadata": {
        "id": "iVZDLeXrwuMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "21DioIIwrGsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi7GT1jFpmmh",
        "outputId": "2921d633-0360-45f5-b574-c185c857deaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 106 (delta 36), reused 91 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (106/106), 3.15 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "/content/nanoGPT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Clonamos el repositorio de nanoGPT y las librerías necesarias\n",
        "%cd /content/\n",
        "!git clone https://github.com/jordiluque/nanoGPT\n",
        "\n",
        "%cd nanoGPT\n",
        "\n",
        "!pip3 install -r requirements.txt --quiet\n",
        "!pip3 install transformers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curado de datos y preparación de tokens\n",
        "\n",
        "Este script de python se descarga un ejemplo de un libro de Shakespeare"
      ],
      "metadata": {
        "id": "SQCZTBT-2jV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/nanoGPT/data/shakespeare/prepare.py"
      ],
      "metadata": {
        "id": "f9tlGSy6NPG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descargamos y curamos los datos"
      ],
      "metadata": {
        "id": "KtwKHPfgBOVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nanoGPT/\n",
        "#!pip install codecarbon --quiet\n",
        "#!codecarbon init\n",
        "#!echo \"log_level = CRITICAL\" >> .codecarbon.config\n",
        "#!echo \"save_to_api = True\" >> .codecarbon.config\n",
        "from data.shakespeare.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.fetch()\n",
        "ds.save('input.txt')\n",
        "ds.load('input.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/shakespeare/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G8LKje9rRb0",
        "outputId": "3c49d4d2-6fd9-46d0-9743-cc25bbcd666f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n",
            "train has 301966 tokens\n",
            "val has 36059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver un pequeño ejemplo de\n",
        "cómo son los datos de entrenamiento"
      ],
      "metadata": {
        "id": "tigDPKFMA87g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat input.txt | head -100"
      ],
      "metadata": {
        "id": "JChEzslSsGsF",
        "outputId": "7cff2e3a-c223-4e26-a80f-6f850a7b5e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall know we\n",
            "have strong arms too.\n",
            "\n",
            "MENENIUS:\n",
            "Why, masters, my good friends, mine honest neighbours,\n",
            "Will you undo yourselves?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Dividimos el texto en tokens"
      ],
      "metadata": {
        "id": "Ak3bxB6a7iI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Un diccionario formado por tokens, en este caso,\n",
        "#@markdown ## de carácteres en lugar de palabras\n",
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSF4nLX9gmi3",
        "outputId": "4212f745-61cc-4ffb-b634-e7faac889fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "jx3KC0gG3mnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando como corpus de entrenamiento una novela de Shakespeare y un diccionario vamos a entrenar un modelo de nano-GPT desde cero.\n",
        "\n",
        "El diccionario lo hemos obtenido en el paso anterior  de carácteres que hemos obtenido mirando las diferentes ocurrencias en el corpus de entrenamiento: **!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz**\n",
        "Estos son nuestros tokens de entrenamiento. Tokenizar consiste en traducir un texto utilizando en, este caso, estos tokens."
      ],
      "metadata": {
        "id": "WojNo_WH5jix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento con carácteres\n",
        "\n",
        "Entrenamos un mini-GPT desde cero con 100 iteraciones"
      ],
      "metadata": {
        "id": "d5rS89Mo5dgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python train.py config/train_shakespeare_char.py --max_iters=100 --eval_iters=20 --eval_interval=50 --out_dir='out-shakespeare-char-100' --batch_size=32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNGNhLWgHqe",
        "outputId": "44f1a799-dec0-471a-e0ca-b36c54a3ec13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 100\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 50\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: batch_size = 32\n",
            "tokens per iteration will be: 8,192\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2806\n",
            "[2023-06-07 13:05:45,869] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:46,574] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:48,592] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:49,112] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:49,780] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:50,269] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:50,959] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:51,291] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:51,698] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:52,000] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:52,562] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:05:52,864] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2673, time 29529.48ms, mfu -100.00%\n",
            "iter 10: loss 3.2243, time 59.56ms, mfu 3.13%\n",
            "iter 20: loss 2.8209, time 59.87ms, mfu 3.13%\n",
            "iter 30: loss 2.6313, time 60.54ms, mfu 3.12%\n",
            "iter 40: loss 2.5636, time 60.87ms, mfu 3.12%\n",
            "step 50: train loss 2.5241, val loss 2.5330\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 50: loss 2.5404, time 4474.10ms, mfu 2.81%\n",
            "iter 60: loss 2.5175, time 60.41ms, mfu 2.84%\n",
            "iter 70: loss 2.5111, time 60.15ms, mfu 2.86%\n",
            "iter 80: loss 2.5290, time 60.79ms, mfu 2.88%\n",
            "iter 90: loss 2.5057, time 59.38ms, mfu 2.91%\n",
            "step 100: train loss 2.4717, val loss 2.5011\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 100: loss 2.4899, time 818.25ms, mfu 2.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintetizando una respuesta (no prompting)\n",
        "\n",
        "El resultado es realmente pobre. El modelo intenta generar tokens (los del diccionario anterior) pero con tan pocas iteraciones de entrenamiento, sólo 100, genera texto **sin sentido**."
      ],
      "metadata": {
        "id": "vwoBTtv3-sQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir='out-shakespeare-char-100' --device=cuda --num_samples=1 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUvTvubzhIXZ",
        "outputId": "5b17b202-43c8-4cb8-ad26-ec748da4ae26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: device = cuda\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "And the RO:\n",
            "Thowilen is s, be mat sen bobe t eranthath my calatanss ar hapat us ce he.\n",
            "War d las aten wice my.\n",
            "\n",
            "Thedatom oroupenowhavetof wath bl t st nd ll, bes iree sengcin lat Her d ov ts, and t l nghire transesel lind pe l.\n",
            "Hashe ce wiry pthar aissplle y hellinde n Badod sigod whomery wod methake o Windo when piiche the m fouris hane hiend t so mower;\n",
            "Wh\n",
            "\n",
            "An ad nthrurt f s t; wor t mef t te maleronthefaf Pre?\n",
            "\n",
            "Wheam.\n",
            "Whe--sean stied isar adsal the ERDLAnd fin cour ay aney IOUS:\n",
            "\n",
            "IOfo t ce wi\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento con 200 iteraciones\n",
        "Mas iteraciones de entrenamiento (200 iteraciones)\n",
        "Continuamos entrenando el nano-GPT, partiendo del anterior, hasta las 200 iteraciones.\n"
      ],
      "metadata": {
        "id": "MsAHe33w_Vky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --init_from=\"resume\" --max_iters=200 \\\n",
        "  --eval_iters=20 --eval_interval=100 --out_dir='out-shakespeare-char-100' --batch_size=32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYirrvs6oEYh",
        "outputId": "f8ef0190-3368-4ade-dd85-a2175f686c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: init_from = resume\n",
            "Overriding: max_iters = 200\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: batch_size = 32\n",
            "tokens per iteration will be: 8,192\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Resuming training from out-shakespeare-char-100\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 100: train loss 2.4789, val loss 2.4984\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "[2023-06-07 13:15:25,412] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:25,728] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:26,191] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:26,504] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:26,927] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:27,257] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:27,767] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:28,248] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:28,878] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:29,350] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:30,199] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:15:30,695] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 100: loss 2.4647, time 14188.33ms, mfu -100.00%\n",
            "iter 110: loss 2.4645, time 59.38ms, mfu 3.14%\n",
            "iter 120: loss 2.4968, time 59.58ms, mfu 3.14%\n",
            "iter 130: loss 2.4548, time 60.91ms, mfu 3.13%\n",
            "iter 140: loss 2.4194, time 60.63ms, mfu 3.12%\n",
            "iter 150: loss 2.4249, time 61.46ms, mfu 3.11%\n",
            "iter 160: loss 2.4780, time 60.29ms, mfu 3.11%\n",
            "iter 170: loss 2.4102, time 61.05ms, mfu 3.11%\n",
            "iter 180: loss 2.4092, time 60.74ms, mfu 3.10%\n",
            "iter 190: loss 2.4017, time 60.55ms, mfu 3.10%\n",
            "step 200: train loss 2.3446, val loss 2.3830\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 200: loss 2.3528, time 3321.98ms, mfu 2.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sintetizando una respuesta (no prompting)\n",
        "\n",
        "El resultado es un poco mejor. El modelo genera tokens (los del diccionario de carácteres) Sin embargo, con sólo 200 iteraciones de entrenamiento, el texto sigue sin tener sentido, pero empezamos a ver un cierto patrón, el **diálogo**.\n",
        "\n"
      ],
      "metadata": {
        "id": "imXFp7MaAtYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --device=\"cuda\" --out_dir='out-shakespeare-char-100' --num_samples=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWuao7NoeqX",
        "outputId": "ea5d0e31-ad88-4444-bd1f-0f984f45aec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "CANUDIIILA:\n",
            "My will th la, ber\n",
            "\n",
            "Hiser bobe ding Shir--\n",
            "SABRD:\n",
            "Tand bar hipar ut he hent?\n",
            "F dilasoate ar ce my.\n",
            "\n",
            "\n",
            "DUMatom INour\n",
            "Y:\n",
            "GALAUCf is he me mil ndilll, es iree sengein lat Herid ov the andove pomanonderansesel lind te l.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHES:\n",
            "Any:\n",
            "Shar aise hewhy.\n",
            "Herinde n BESpetelaves homy y wod methake o Windo wher pis al the m dourise we shire s poousower;\n",
            "We\n",
            "\n",
            "ANROMIONCHET:\n",
            "A sor; irist me me te maleronthe ar Prerd my o myourdoure!\n",
            "\n",
            "\n",
            "\n",
            "Sou sar adat Wise ERD sthinin cour ay aney Ire thechan t ce hy\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamos un nano-GPT con 1000 iteraciones\n",
        "![An image](https://github.com/karpathy/nanoGPT/blob/master/assets/gpt2_124M_loss.png?raw=true)\n",
        "\n",
        "Normalmente, cuando se entrena un modelo de aprendizaje automático se validan los resultados del entrenamiento a cada paso (step) o iteración contra una loss (función de pérdidas) que se intenta mejorar. La siguiente gráfica muestra un comportamiento clásico durante el entrenamiento, en dónde, se intenta minimizar esa loss en el conjunto de validación.\n"
      ],
      "metadata": {
        "id": "P2eO5UZzAdVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --max_iters=1000 --init_from=\"resume\" --log_interval=100 --eval_iters=20 --eval_interval=500 --out_dir='out-shakespeare-char-100' --batch_size=32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGk2OOIJo3kv",
        "outputId": "18dc55f5-95be-431c-eaee-47ff2f810b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 1000\n",
            "Overriding: init_from = resume\n",
            "Overriding: log_interval = 100\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 500\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: batch_size = 32\n",
            "tokens per iteration will be: 8,192\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Resuming training from out-shakespeare-char-100\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "[2023-06-07 13:18:53,553] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:53,891] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:54,385] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:54,692] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:55,108] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:55,418] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:55,982] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:56,328] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:56,756] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:57,078] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:57,510] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-06-07 13:18:57,820] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 200: loss 2.3383, time 10357.77ms, mfu -100.00%\n",
            "iter 300: loss 2.1670, time 60.75ms, mfu 3.07%\n",
            "iter 400: loss 1.9219, time 60.81ms, mfu 3.07%\n",
            "step 500: train loss 1.7063, val loss 1.8870\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 500: loss 1.8002, time 4657.67ms, mfu 2.76%\n",
            "iter 600: loss 1.7309, time 60.95ms, mfu 2.79%\n",
            "iter 700: loss 1.6052, time 60.31ms, mfu 2.82%\n",
            "iter 800: loss 1.5659, time 60.86ms, mfu 2.85%\n",
            "iter 900: loss 1.5276, time 59.99ms, mfu 2.87%\n",
            "step 1000: train loss 1.4191, val loss 1.6275\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 1000: loss 1.4835, time 843.95ms, mfu 2.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sintetizando una respuesta (no prompting)\n",
        "\n",
        "El resultado es un mucho mejor. El modelo intenta generar tokens (los del diccionario de carácteres) con 1000 iteraciones de entrenamiento. El texto ya tiene sentido y los carácteres (tokens) que nuestro modelo genera empiezan a describir las regularidades del lenguaje trnasformándose en palabras inglesas."
      ],
      "metadata": {
        "id": "AsRd8775_97F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --device=\"cuda\" --out_dir='out-shakespeare-char-100' --num_samples=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERJZxxYBp1HH",
        "outputId": "8b92fcfb-4668-4167-f5d9-359caefb83b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "And they brince.\n",
            "\n",
            "SOMERSET:\n",
            "Were is thy be does angry.\n",
            "\n",
            "MENENIUS:\n",
            "I am it is us come.\n",
            "\n",
            "SICINIUS:\n",
            "I endreat my father, an I must ones,\n",
            "to that I command did that misery bothing at Here;\n",
            "White and him possion to be pelplance, and thus wond my soul\n",
            "That should fan'd and nor ope to the hopes of demother\n",
            "To Warwick thy city, the more rive and\n",
            "To curs of the house\n",
            "To kind thruft for treason! my lord, I look the paint of my off,\n",
            "Her son! whom didst advant, and he mothing straight no vex'd\n",
            "Whose the wou\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning (adaptación) con palabras\n",
        "\n",
        "Un modelo pre-entrenado como GTP-2, entrenado con un corpus inmenso, se adapta o finetunea a la novela de Shakespeare. Ahora en lugar de carácteres, nuestro diccionario de **tokens** son palabras. El entrenamiento, en este caso, consiste no parte de cero, si no que aprovecha un modelo ya entrenado como GPT-2. Con palabras, el modelo sintetiza más rápidamente el lenguaje y encuentra correlaciones de manera mas sencilla (no ha de aprender a formar palabras). La síntesis genera directamente las palabras **tokens** del diccionario, en lugar de carácteres."
      ],
      "metadata": {
        "id": "fSjGpKSnB2pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Un diccionario formado por tokens, en este caso,\n",
        "#@markdown ## de palabras en lugar de carácteres.\n",
        "#@markdown ## Fine-tuneamos un GPT-2 de tamaño medio con 40 iteraciones\n",
        "%cd /content/nanoGPT/\n",
        "from data.shakespeare.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.fetch()\n",
        "ds.save('input.txt')\n",
        "ds.load('input.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/shakespeare/')\n",
        "!python train.py config/finetune_shakespeare.py --max_iters=40"
      ],
      "metadata": {
        "id": "jUWhzAOhrta8",
        "outputId": "33b1ab02-313d-4c7d-84de-47d4813ec082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n",
            "train has 301966 tokens\n",
            "val has 36059 tokens\n",
            "Overriding config with config/finetune_shakespeare.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-shakespeare'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: max_iters = 40\n",
            "tokens per iteration will be: 32,768\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 4.14MB/s]\n",
            "Downloading pytorch_model.bin: 100% 548M/548M [00:07<00:00, 76.4MB/s]\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 613kB/s]\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.1872, val loss 4.0326\n",
            "iter 0: loss 4.6577, time 43885.20ms, mfu -100.00%\n",
            "iter 1: loss 3.6388, time 2738.12ms, mfu -100.00%\n",
            "iter 2: loss 3.7512, time 2739.26ms, mfu -100.00%\n",
            "iter 3: loss 3.8444, time 2744.39ms, mfu -100.00%\n",
            "iter 4: loss 3.7710, time 2752.75ms, mfu -100.00%\n",
            "step 5: train loss 3.6270, val loss 3.4013\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 5: loss 4.0607, time 10108.26ms, mfu 0.89%\n",
            "iter 6: loss 3.8367, time 2768.23ms, mfu 1.12%\n",
            "iter 7: loss 3.8423, time 2774.22ms, mfu 1.34%\n",
            "iter 8: loss 4.2731, time 2785.08ms, mfu 1.52%\n",
            "iter 9: loss 3.6963, time 2796.30ms, mfu 1.69%\n",
            "step 10: train loss 3.6094, val loss 3.3710\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 10: loss 3.3592, time 10522.89ms, mfu 1.61%\n",
            "iter 11: loss 3.8318, time 2802.89ms, mfu 1.77%\n",
            "iter 12: loss 3.4319, time 2805.94ms, mfu 1.91%\n",
            "iter 13: loss 3.5096, time 2805.40ms, mfu 2.04%\n",
            "iter 14: loss 3.4060, time 2815.12ms, mfu 2.16%\n",
            "step 15: train loss 3.4975, val loss 3.3996\n",
            "iter 15: loss 3.1890, time 4559.70ms, mfu 2.14%\n",
            "iter 16: loss 2.6352, time 2824.96ms, mfu 2.24%\n",
            "iter 17: loss 3.8001, time 2835.03ms, mfu 2.33%\n",
            "iter 18: loss 3.4129, time 2844.42ms, mfu 2.42%\n",
            "iter 19: loss 3.2702, time 2858.52ms, mfu 2.49%\n",
            "step 20: train loss 3.4277, val loss 3.4040\n",
            "iter 20: loss 3.4978, time 4639.99ms, mfu 2.43%\n",
            "iter 21: loss 3.4422, time 2889.29ms, mfu 2.50%\n",
            "iter 22: loss 3.9077, time 2899.18ms, mfu 2.56%\n",
            "iter 23: loss 3.5717, time 2908.73ms, mfu 2.61%\n",
            "iter 24: loss 4.0967, time 2919.05ms, mfu 2.66%\n",
            "step 25: train loss 3.4130, val loss 3.2819\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 25: loss 3.2786, time 13760.50ms, mfu 2.46%\n",
            "iter 26: loss 3.7292, time 2949.34ms, mfu 2.52%\n",
            "iter 27: loss 3.7978, time 2970.63ms, mfu 2.57%\n",
            "iter 28: loss 3.1839, time 2981.11ms, mfu 2.61%\n",
            "iter 29: loss 3.4593, time 2996.58ms, mfu 2.65%\n",
            "step 30: train loss 3.3484, val loss 3.3523\n",
            "iter 30: loss 3.0856, time 4889.71ms, mfu 2.57%\n",
            "iter 31: loss 3.2273, time 3034.17ms, mfu 2.61%\n",
            "iter 32: loss 3.4405, time 3037.91ms, mfu 2.64%\n",
            "iter 33: loss 3.1201, time 3048.35ms, mfu 2.67%\n",
            "iter 34: loss 3.2771, time 3055.42ms, mfu 2.70%\n",
            "step 35: train loss 3.3409, val loss 3.2985\n",
            "iter 35: loss 3.7279, time 4958.43ms, mfu 2.61%\n",
            "iter 36: loss 3.4453, time 3026.33ms, mfu 2.65%\n",
            "iter 37: loss 3.3499, time 3015.00ms, mfu 2.68%\n",
            "iter 38: loss 3.6824, time 2992.54ms, mfu 2.71%\n",
            "iter 39: loss 3.4561, time 2982.29ms, mfu 2.74%\n",
            "step 40: train loss 3.3791, val loss 3.2990\n",
            "iter 40: loss 3.2666, time 4829.71ms, mfu 2.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sintetizando una respuesta con palabras (no prompting)\n",
        "\n",
        "Resultado con 40 iteraciones y adaptando un GPT2 pre-entrenado\n",
        "El resultado con palabras y el fine-tuning con sólo 40 iteraciones ya es muy bueno. Mucho mejor que con 40 iteraciones entrenando desde cero y/o empleando un diccionario de carácteres."
      ],
      "metadata": {
        "id": "SCzysQT8Ev3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --device=\"cuda\" --out_dir=out-shakespeare --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGJU3yA5gFb3",
        "outputId": "87d9ddee-cdeb-4514-d48d-537a64b618e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-shakespeare\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "\n",
            "- * * All creatures that are created in the image of God are created in the image of man.\n",
            "\n",
            "NASB.: And now again the LORD spoke to the sons of men; but he spoke not.\n",
            "\n",
            "LORD:\n",
            "That is he did command you,\n",
            "As God himself commanded you,\n",
            "And ordained you man,\n",
            "So that you may know how I am made.\n",
            "\n",
            "NASB.:\n",
            "I heard you, therefore, call me Lord\n",
            "And order you to\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sintetizando una respuesta con palabras (prompting)\n",
        "\n",
        "Resultado con 40 iteraciones y adaptando un GPT2 pre-entrenado\n",
        "Ahora probamos a sesgar la respuesta de la síntesis preguntando por algo. \n",
        "Utilizando la técnica de \"Prompting\" preguntamos al modelo como si fueramos el rey Richard, \"KING RICHARD II: \n",
        "        What is the answer to life, the universe, and everything?\"\n",
        "Pedimos por dos ejemplos (samples) **--num_samples=2** que el modelo nos devuelve separados por una línea de guiones."
      ],
      "metadata": {
        "id": "9mNkpbfcFgPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --device=\"cuda\" \\\n",
        "    --out_dir=out-shakespeare \\\n",
        "    --start=\"KING RICHARD II: \\\n",
        "        What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=2 --max_new_tokens=100 --device=cuda"
      ],
      "metadata": {
        "id": "D6q5QE5FygnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239d86af-13c3-471e-cffd-06bb0ca7d442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-shakespeare\n",
            "Overriding: start = KING RICHARD II:          What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 2\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: device = cuda\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "KING RICHARD II:          What is the answer to life, the universe, and everything?\n",
            "\n",
            "MARY LUCIFERUS:    \n",
            "Sir, if you will, you will meet me in the presence of the dead,\n",
            "by the good and the unjust, and by the wise and by the blind.\n",
            "\n",
            "CUT TO:\n",
            "\n",
            "JOHN EDWARD IV:    \n",
            "I am dead; it is my nature to die.\n",
            "\n",
            "JOHN EDWARD IV:       \n",
            "Why, if I am dead,\n",
            "---------------\n",
            "KING RICHARD II:          What is the answer to life, the universe, and everything? What is the answer to our souls, angels, and all that is in heaven? What is the answer to our bodies, and what is the answer to earth? What is the answer to all things, and what is the answer to all things? What is the answer to all things? What is the answer to all things? What is the answer to hell? What is the answer to heaven? What is the answer to all the world? What is the answer to all the sea? What is\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento de una novela en castellano\n",
        "\n",
        "En este caso vamos a realizar el mismo ejercico con una novela de Calderón de la Barca. ¿¿Con castellano?? Sí a pesar de ser entrenado con un corpus mayoritariamente en inglés, GPT2 es capaz de aprender las regularidades de otras lenguas. Los sueños, sueños son... "
      ],
      "metadata": {
        "id": "eZOPU47NGTZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curado de datos y preparación de tokens\n",
        "\n",
        "Este script de python se descarga un ejemplo de un libro de Calderón de la Barca, \"La vida es sueño\""
      ],
      "metadata": {
        "id": "8O-h_1LtG4rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.calderon.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.load('./data/calderon/cap1-4.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/calderon/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN5O3AUx0bhR",
        "outputId": "013c9407-d706-485e-dde8-2041349f2707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 799499 tokens\n",
            "val has 86982 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## ¿Cómo son los datos?\n",
        "!cat ./data/calderon/cap1-4.txt | head -100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LCbjOf0gp1",
        "outputId": "b72be4eb-0acf-437f-9bd6-11cf44efaecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿ESCENA PRIMERA.\r\n",
            "\r\n",
            "ROSAURA, CLARIN.\r\n",
            "\r\n",
            "_(Rosaura vestida de hombre aparece en lo alto de las peñas, y baja á\r\n",
            "lo llano; tras ella viene Clarin.)_\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  Hipogrifo violento\r\n",
            "  Que corriste parejas con el viento,\r\n",
            "  ¿Dónde rayo sin llama,\r\n",
            "  Pájaro sin matiz, pez sin escama,\r\n",
            "  Y bruto sin instinto\r\n",
            "  Natural, al confuso laberinto\r\n",
            "  Destas desnudas peñas\r\n",
            "  Te desbocas, arrastras y despeñas?\r\n",
            "  Quédate en este monte,\r\n",
            "  Donde tengan los brutos su Faetonte;\r\n",
            "  Que yo, sin más camino\r\n",
            "  Que el que me dan las leyes del destino.\r\n",
            "  Ciega y desesperada\r\n",
            "  Bajaré la aspereza enmarañada\r\n",
            "  Deste monte eminente,\r\n",
            "  Que arruga al sol el ceño de su frente.\r\n",
            "  Mal, Polonia, recibes\r\n",
            "  A un extranjero, pues con sangre escribes\r\n",
            "  Su entrada en tus arenas,\r\n",
            "  Y apénas llega, cuando llega á penas.\r\n",
            "  Bien mi suerte lo dice;\r\n",
            "  ¿Mas dónde halló piedad un infelice?\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  Dí dos, y no me dejes\r\n",
            "  En la posada á mí cuando te quejes;\r\n",
            "  Que si dos hemos sido\r\n",
            "  Los que de nuestra patria hemos salido\r\n",
            "  A probar aventuras,\r\n",
            "  Dos los que entre desdichas y locuras\r\n",
            "  Aquí habemos llegado,\r\n",
            "  Y dos los que del monte hemos rodado,\r\n",
            "  ¿No es razon que yo sienta\r\n",
            "  Meterme en el pesar, y no en la cuenta?\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  No te quiero dar parte\r\n",
            "  En mis quejas, Clarin, por no quitarte,\r\n",
            "  Llorando tu desvelo,\r\n",
            "  El derecho que tienes tú al consuelo.\r\n",
            "  Que tanto gusto habia\r\n",
            "  En quejarse, un filósofo decia,\r\n",
            "  Que, á trueco de quejarse,\r\n",
            "  Habian las desdichas de buscarse.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  El filósofo era\r\n",
            "  Un borracho barbon: ¡oh! ¡quién le diera\r\n",
            "  Más de mil bofetadas!\r\n",
            "  Quejárase despues de muy bien dadas.\r\n",
            "  ¿Mas qué haremos, señora,\r\n",
            "  A pié, solos, perdidos y á esta hora\r\n",
            "  En un desierto monte,\r\n",
            "  Cuando se parte el sol á otro horizonte?\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  ¡Quién ha visto sucesos tan extraños!\r\n",
            "  Mas si la vista no padece engaños\r\n",
            "  Que hace la fantasía,\r\n",
            "  A la medrosa luz que áun tiene el dia,\r\n",
            "  Me parece que veo\r\n",
            "  Un edificio.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "               Ó miente mi deseo,\r\n",
            "  Ó termino las señas.\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  Rústico nace entre desnudas peñas\r\n",
            "  Un palacio tan breve,\r\n",
            "  Que al sol apénas á mirar se atreve:\r\n",
            "  Con tan rudo artificio\r\n",
            "  La arquitectura está de su edificio,\r\n",
            "  Que parece, á las plantas\r\n",
            "  De tantas rocas y de peñas tantas\r\n",
            "  Que al sol tocan la lumbre,\r\n",
            "  Peñasco que ha rodado de la cumbre.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  Vámonos acercando;\r\n",
            "  Que este es mucho mirar, señora, cuando\r\n",
            "  Es mejor que la gente\r\n",
            "  Que habita en ella, generosamente\r\n",
            "  Nos admita.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning (adaptación) con palabras\n",
        "\n",
        "Un modelo pre-entrenado como GTP-2, entrenado con un corpus inmenso, se adapta o fine-tunea a la novela La vida es sueño. Nuestro diccionario de **tokens** son palabras. El entrenamiento, en este caso, no parte de cero, si no que aprovecha un modelo ya entrenado como GPT-2. Ahora el diccionario de palabras se extrae del propio entrenamiento y son palabras en castellano. "
      ],
      "metadata": {
        "id": "FvxjjIj7HjGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptando a castellano con 40 iteraciones un GPT2 pre-entrenado"
      ],
      "metadata": {
        "id": "oOCwRVg9H5wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/finetune_calderon.py --max_iters=40 --eval_interval=5 --eval_iters=40 --device='cuda'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoUdHnXb14W8",
        "outputId": "0528a773-b727-45ee-ef9e-5620ebea2c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_calderon.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-calderon'\n",
            "eval_interval = 100\n",
            "eval_iters = 500\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'calderon'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'calderon'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 2\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 200\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: max_iters = 40\n",
            "Overriding: eval_interval = 5\n",
            "Overriding: eval_iters = 40\n",
            "Overriding: device = cuda\n",
            "tokens per iteration will be: 65,536\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.0732, val loss 4.0328\n",
            "iter 0: loss 3.7879, time 51809.06ms, mfu -100.00%\n",
            "iter 1: loss 3.4072, time 4465.43ms, mfu -100.00%\n",
            "iter 2: loss 3.5977, time 4483.10ms, mfu -100.00%\n",
            "iter 3: loss 3.2755, time 4504.13ms, mfu -100.00%\n",
            "iter 4: loss 3.8978, time 4524.38ms, mfu -100.00%\n",
            "step 5: train loss 3.4948, val loss 3.5399\n",
            "saving checkpoint to out-calderon\n",
            "iter 5: loss 3.8288, time 18686.01ms, mfu 0.96%\n",
            "iter 6: loss 3.6120, time 4515.86ms, mfu 1.26%\n",
            "iter 7: loss 3.2543, time 4525.46ms, mfu 1.53%\n",
            "iter 8: loss 3.4168, time 4543.78ms, mfu 1.78%\n",
            "iter 9: loss 3.3638, time 4559.29ms, mfu 1.99%\n",
            "step 10: train loss 3.2810, val loss 3.3568\n",
            "saving checkpoint to out-calderon\n",
            "iter 10: loss 3.5060, time 13468.82ms, mfu 1.93%\n",
            "iter 11: loss 3.4733, time 4594.68ms, mfu 2.12%\n",
            "iter 12: loss 3.5303, time 4621.39ms, mfu 2.30%\n",
            "iter 13: loss 3.0911, time 4634.94ms, mfu 2.46%\n",
            "iter 14: loss 2.8414, time 4651.75ms, mfu 2.60%\n",
            "step 15: train loss 3.1942, val loss 3.2757\n",
            "saving checkpoint to out-calderon\n",
            "iter 15: loss 3.0273, time 14185.00ms, mfu 2.47%\n",
            "iter 16: loss 2.8982, time 4729.78ms, mfu 2.60%\n",
            "iter 17: loss 3.3894, time 4739.77ms, mfu 2.72%\n",
            "iter 18: loss 3.1251, time 4784.31ms, mfu 2.82%\n",
            "iter 19: loss 2.9602, time 4804.24ms, mfu 2.91%\n",
            "step 20: train loss 3.1625, val loss 3.2446\n",
            "saving checkpoint to out-calderon\n",
            "iter 20: loss 2.9843, time 13959.09ms, mfu 2.75%\n",
            "iter 21: loss 2.9816, time 4735.46ms, mfu 2.85%\n",
            "iter 22: loss 3.1543, time 4751.34ms, mfu 2.95%\n",
            "iter 23: loss 2.8126, time 4789.11ms, mfu 3.03%\n",
            "iter 24: loss 2.9090, time 4822.94ms, mfu 3.10%\n",
            "step 25: train loss 3.0169, val loss 3.2543\n",
            "iter 25: loss 3.2439, time 8264.36ms, mfu 3.00%\n",
            "iter 26: loss 3.5462, time 4918.80ms, mfu 3.07%\n",
            "iter 27: loss 2.7196, time 4924.68ms, mfu 3.13%\n",
            "iter 28: loss 3.3117, time 4981.43ms, mfu 3.18%\n",
            "iter 29: loss 3.0944, time 5024.49ms, mfu 3.22%\n",
            "step 30: train loss 2.9999, val loss 3.2181\n",
            "saving checkpoint to out-calderon\n",
            "iter 30: loss 3.1312, time 14420.01ms, mfu 3.02%\n",
            "iter 31: loss 2.7882, time 5068.88ms, mfu 3.07%\n",
            "iter 32: loss 2.6448, time 5088.16ms, mfu 3.12%\n",
            "iter 33: loss 2.8886, time 5071.54ms, mfu 3.16%\n",
            "iter 34: loss 2.8052, time 5060.72ms, mfu 3.20%\n",
            "step 35: train loss 2.9019, val loss 3.0494\n",
            "saving checkpoint to out-calderon\n",
            "iter 35: loss 3.1049, time 14383.85ms, mfu 3.00%\n",
            "iter 36: loss 3.1253, time 4943.08ms, mfu 3.07%\n",
            "iter 37: loss 2.9952, time 4926.01ms, mfu 3.12%\n",
            "iter 38: loss 2.5632, time 4958.39ms, mfu 3.17%\n",
            "iter 39: loss 3.0403, time 4960.96ms, mfu 3.22%\n",
            "step 40: train loss 2.8692, val loss 2.9882\n",
            "saving checkpoint to out-calderon\n",
            "iter 40: loss 3.0059, time 14395.51ms, mfu 3.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintetizando una respuesta en castellano con palabras (prompting)\n",
        "\n",
        "Resultado con 40 iteraciones y adaptando a castellano un GPT2 pre-entrenado. Sesgamos la respuesta de la síntesis preguntando por algo, utilizando la técnica de \"Prompting\" preguntamos al modelo como si fueramos el Demonio, \n",
        "\n",
        "DEMONIO.\n",
        "        ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
        "\n",
        "Pedimos por dos ejemplos (samples) que el modelo nos devuelve separados por una línea de guiones."
      ],
      "metadata": {
        "id": "hv0RZvdXJNjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --device=cuda \\\n",
        "    --out_dir=out-calderon \\\n",
        "    --start=\"DEMONIO. \\\n",
        "        ¿Cuál es la respuesta a la vida, el universo y a todo?\" \\\n",
        "    --num_samples=2 --max_new_tokens=100 "
      ],
      "metadata": {
        "id": "HHzsc0Ix2M2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507bc35b-ca1c-408c-de01-67cac05433df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-calderon\n",
            "Overriding: start = DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "Overriding: num_samples = 2\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: device = cuda\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "\n",
            "MARCOS.\n",
            "                    Este y el dolor, espero y pues, años con el mis cuatro y el muyo.\n",
            "\n",
            "ESCENA XII.\n",
            "\n",
            "HONOR, CALVIN.\n",
            "\n",
            "                          Y á\n",
            "---------------\n",
            "DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "\n",
            "YES. \n",
            "\n",
            "D. O.M., cárin.                    Que uní, la muerte, para te gusta, cuanto,\n",
            "  Pues en deja, el mano ha de un verdad\n",
            "  ¿Por tener, cuando?\n",
            "\n",
            "D. \n",
            "\n",
            "D. O.M., cárin. \n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/finetune_calderon.py --init_from='resume' --device='cuda' \\\n",
        "  --max_iters=400 --eval_interval=20 --eval_iters=40 "
      ],
      "metadata": {
        "id": "VSppj6Pi2O4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4549f96c-cd0c-4b32-a23a-5d5a6a63d0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_calderon.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-calderon'\n",
            "eval_interval = 100\n",
            "eval_iters = 500\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'calderon'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'calderon'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 2\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 200\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: init_from = resume\n",
            "Overriding: device = cuda\n",
            "Overriding: max_iters = 400\n",
            "Overriding: eval_interval = 20\n",
            "Overriding: eval_iters = 40\n",
            "tokens per iteration will be: 65,536\n",
            "Resuming training from out-calderon\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 80: train loss 2.5336, val loss 2.9498\n",
            "iter 80: loss 2.2786, time 36367.05ms, mfu -100.00%\n",
            "iter 81: loss 2.4721, time 4644.19ms, mfu -100.00%\n",
            "iter 82: loss 2.3533, time 4676.38ms, mfu -100.00%\n",
            "iter 83: loss 2.4455, time 4726.82ms, mfu -100.00%\n",
            "iter 84: loss 2.5435, time 4756.69ms, mfu -100.00%\n",
            "iter 85: loss 2.4990, time 4787.79ms, mfu 3.75%\n",
            "iter 86: loss 2.5215, time 4815.73ms, mfu 3.75%\n",
            "iter 87: loss 2.6673, time 4845.80ms, mfu 3.75%\n",
            "iter 88: loss 2.2200, time 4897.49ms, mfu 3.74%\n",
            "iter 89: loss 2.6830, time 4930.71ms, mfu 3.73%\n",
            "iter 90: loss 2.4307, time 4986.15ms, mfu 3.72%\n",
            "iter 91: loss 2.4388, time 5014.41ms, mfu 3.70%\n",
            "iter 92: loss 2.5144, time 5065.34ms, mfu 3.69%\n",
            "iter 93: loss 2.1921, time 5097.45ms, mfu 3.67%\n",
            "iter 94: loss 2.3128, time 5117.88ms, mfu 3.65%\n",
            "iter 95: loss 2.0910, time 5084.70ms, mfu 3.64%\n",
            "iter 96: loss 2.9159, time 5047.68ms, mfu 3.63%\n",
            "iter 97: loss 2.8193, time 4993.54ms, mfu 3.63%\n",
            "iter 98: loss 2.5414, time 4958.35ms, mfu 3.63%\n",
            "iter 99: loss 2.0445, time 4937.11ms, mfu 3.63%\n",
            "step 100: train loss 2.4735, val loss 2.8252\n",
            "iter 100: loss 2.6465, time 8358.70ms, mfu 3.48%\n",
            "iter 101: loss 2.8394, time 4899.25ms, mfu 3.50%\n",
            "iter 102: loss 2.0483, time 4909.25ms, mfu 3.52%\n",
            "iter 103: loss 2.5386, time 4904.86ms, mfu 3.53%\n",
            "iter 104: loss 2.7240, time 4916.71ms, mfu 3.54%\n",
            "iter 105: loss 2.3759, time 4946.59ms, mfu 3.55%\n",
            "iter 106: loss 2.5029, time 4972.66ms, mfu 3.56%\n",
            "iter 107: loss 2.2903, time 4974.74ms, mfu 3.56%\n",
            "iter 108: loss 2.4599, time 5002.38ms, mfu 3.57%\n",
            "iter 109: loss 2.3581, time 5013.62ms, mfu 3.57%\n",
            "iter 110: loss 2.6162, time 5021.43ms, mfu 3.57%\n",
            "iter 111: loss 2.6161, time 5012.60ms, mfu 3.57%\n",
            "iter 112: loss 2.1902, time 4990.97ms, mfu 3.57%\n",
            "iter 113: loss 2.7029, time 4992.84ms, mfu 3.58%\n",
            "iter 114: loss 2.4298, time 4994.77ms, mfu 3.58%\n",
            "iter 115: loss 3.0784, time 4976.40ms, mfu 3.58%\n",
            "iter 116: loss 2.1178, time 4957.99ms, mfu 3.59%\n",
            "iter 117: loss 2.2303, time 4941.54ms, mfu 3.59%\n",
            "iter 118: loss 2.7448, time 4943.43ms, mfu 3.59%\n",
            "iter 119: loss 2.3261, time 4930.51ms, mfu 3.60%\n",
            "step 120: train loss 2.3706, val loss 2.7359\n",
            "saving checkpoint to out-calderon\n",
            "iter 120: loss 2.4742, time 14053.36ms, mfu 3.37%\n",
            "iter 121: loss 2.3269, time 4927.47ms, mfu 3.40%\n",
            "iter 122: loss 2.8410, time 4967.70ms, mfu 3.42%\n",
            "iter 123: loss 2.3761, time 4997.25ms, mfu 3.43%\n",
            "iter 124: loss 2.3516, time 5047.29ms, mfu 3.45%\n",
            "iter 125: loss 2.4232, time 5066.48ms, mfu 3.46%\n",
            "iter 126: loss 2.1363, time 5064.98ms, mfu 3.47%\n",
            "iter 127: loss 2.3876, time 5001.42ms, mfu 3.48%\n",
            "iter 128: loss 2.1398, time 5004.39ms, mfu 3.49%\n",
            "iter 129: loss 2.4682, time 4981.64ms, mfu 3.50%\n",
            "iter 130: loss 2.6218, time 4970.90ms, mfu 3.51%\n",
            "iter 131: loss 2.4500, time 4946.50ms, mfu 3.52%\n",
            "iter 132: loss 2.2905, time 4936.24ms, mfu 3.54%\n",
            "iter 133: loss 2.1338, time 4932.53ms, mfu 3.55%\n",
            "iter 134: loss 2.3809, time 4919.46ms, mfu 3.56%\n",
            "iter 135: loss 2.1723, time 4936.95ms, mfu 3.57%\n",
            "iter 136: loss 2.7414, time 4937.51ms, mfu 3.57%\n",
            "iter 137: loss 2.3010, time 4943.82ms, mfu 3.58%\n",
            "iter 138: loss 2.7040, time 4957.87ms, mfu 3.58%\n",
            "iter 139: loss 2.2169, time 4965.43ms, mfu 3.59%\n",
            "step 140: train loss 2.3841, val loss 2.7936\n",
            "iter 140: loss 2.0639, time 8486.26ms, mfu 3.44%\n",
            "iter 141: loss 2.1276, time 4987.91ms, mfu 3.46%\n",
            "iter 142: loss 2.1881, time 5006.73ms, mfu 3.47%\n",
            "iter 143: loss 2.2634, time 4981.60ms, mfu 3.48%\n",
            "iter 144: loss 2.1021, time 4972.00ms, mfu 3.50%\n",
            "iter 145: loss 2.0659, time 4976.05ms, mfu 3.51%\n",
            "iter 146: loss 2.4309, time 4970.12ms, mfu 3.52%\n",
            "iter 147: loss 1.9917, time 4962.04ms, mfu 3.53%\n",
            "iter 148: loss 2.0597, time 4948.45ms, mfu 3.54%\n",
            "iter 149: loss 2.3468, time 4966.83ms, mfu 3.55%\n",
            "iter 150: loss 2.3795, time 4950.04ms, mfu 3.55%\n",
            "iter 151: loss 2.3595, time 4954.91ms, mfu 3.56%\n",
            "iter 152: loss 2.1915, time 4945.17ms, mfu 3.57%\n",
            "iter 153: loss 2.4298, time 4963.31ms, mfu 3.57%\n",
            "iter 154: loss 2.3534, time 4951.36ms, mfu 3.58%\n",
            "iter 155: loss 2.3182, time 4953.78ms, mfu 3.58%\n",
            "iter 156: loss 1.9982, time 4963.61ms, mfu 3.59%\n",
            "iter 157: loss 2.0983, time 4963.90ms, mfu 3.59%\n",
            "iter 158: loss 1.9873, time 4962.73ms, mfu 3.59%\n",
            "iter 159: loss 2.1392, time 4940.85ms, mfu 3.60%\n",
            "step 160: train loss 2.3090, val loss 2.7803\n",
            "iter 160: loss 2.1758, time 8424.13ms, mfu 3.45%\n",
            "iter 161: loss 2.3396, time 4944.93ms, mfu 3.47%\n",
            "iter 162: loss 2.4673, time 4950.29ms, mfu 3.49%\n",
            "iter 163: loss 2.0348, time 4957.75ms, mfu 3.50%\n",
            "iter 164: loss 1.7917, time 4969.75ms, mfu 3.51%\n",
            "iter 165: loss 2.6511, time 4978.59ms, mfu 3.52%\n",
            "iter 166: loss 2.3688, time 4992.06ms, mfu 3.53%\n",
            "iter 167: loss 2.4366, time 4981.89ms, mfu 3.54%\n",
            "iter 168: loss 1.9478, time 4980.57ms, mfu 3.54%\n",
            "iter 169: loss 2.5683, time 4979.78ms, mfu 3.55%\n",
            "iter 170: loss 2.0263, time 4987.59ms, mfu 3.55%\n",
            "iter 171: loss 2.5084, time 4968.24ms, mfu 3.56%\n",
            "iter 172: loss 2.4466, time 4972.60ms, mfu 3.57%\n",
            "iter 173: loss 1.7926, time 4971.31ms, mfu 3.57%\n",
            "iter 174: loss 2.2325, time 4973.87ms, mfu 3.57%\n",
            "iter 175: loss 2.2989, time 4971.56ms, mfu 3.58%\n",
            "iter 176: loss 1.7710, time 4953.86ms, mfu 3.58%\n",
            "iter 177: loss 2.0410, time 4951.18ms, mfu 3.59%\n",
            "iter 178: loss 2.1493, time 4965.26ms, mfu 3.59%\n",
            "iter 179: loss 2.4828, time 4951.36ms, mfu 3.59%\n",
            "step 180: train loss 2.2664, val loss 2.6986\n",
            "saving checkpoint to out-calderon\n",
            "iter 180: loss 2.0523, time 14391.67ms, mfu 3.36%\n",
            "iter 181: loss 2.1019, time 4927.19ms, mfu 3.39%\n",
            "iter 182: loss 2.2377, time 4949.50ms, mfu 3.41%\n",
            "iter 183: loss 1.9467, time 5000.93ms, mfu 3.43%\n",
            "iter 184: loss 2.3299, time 5043.46ms, mfu 3.44%\n",
            "iter 185: loss 2.0653, time 5044.34ms, mfu 3.46%\n",
            "iter 186: loss 1.9370, time 5024.39ms, mfu 3.47%\n",
            "iter 187: loss 1.9025, time 5003.51ms, mfu 3.48%\n",
            "iter 188: loss 2.0315, time 4996.39ms, mfu 3.49%\n",
            "iter 189: loss 2.0123, time 4988.17ms, mfu 3.50%\n",
            "iter 190: loss 2.3332, time 4965.51ms, mfu 3.51%\n",
            "iter 191: loss 2.4796, time 4960.23ms, mfu 3.52%\n",
            "iter 192: loss 2.0529, time 4941.69ms, mfu 3.54%\n",
            "iter 193: loss 2.3148, time 4939.71ms, mfu 3.55%\n",
            "iter 194: loss 2.4256, time 4938.33ms, mfu 3.55%\n",
            "iter 195: loss 1.9215, time 4936.31ms, mfu 3.56%\n",
            "iter 196: loss 2.3264, time 4939.98ms, mfu 3.57%\n",
            "iter 197: loss 2.6814, time 4953.24ms, mfu 3.58%\n",
            "iter 198: loss 2.0787, time 4961.38ms, mfu 3.58%\n",
            "iter 199: loss 2.1594, time 4952.07ms, mfu 3.59%\n",
            "step 200: train loss 2.2860, val loss 2.6802\n",
            "saving checkpoint to out-calderon\n",
            "iter 200: loss 2.2127, time 14282.75ms, mfu 3.35%\n",
            "iter 201: loss 2.7341, time 4926.69ms, mfu 3.38%\n",
            "iter 202: loss 2.0957, time 4937.14ms, mfu 3.41%\n",
            "iter 203: loss 2.4679, time 4942.23ms, mfu 3.43%\n",
            "iter 204: loss 2.3601, time 4926.94ms, mfu 3.45%\n",
            "iter 205: loss 2.1013, time 4911.82ms, mfu 3.47%\n",
            "iter 206: loss 2.6511, time 4917.05ms, mfu 3.49%\n",
            "iter 207: loss 2.3658, time 4892.97ms, mfu 3.51%\n",
            "iter 208: loss 2.2065, time 4907.59ms, mfu 3.52%\n",
            "iter 209: loss 2.1461, time 4899.39ms, mfu 3.54%\n",
            "iter 210: loss 2.2580, time 4893.41ms, mfu 3.55%\n",
            "iter 211: loss 2.2663, time 4886.57ms, mfu 3.56%\n",
            "iter 212: loss 2.4040, time 4896.02ms, mfu 3.57%\n",
            "iter 213: loss 2.1630, time 4872.37ms, mfu 3.59%\n",
            "iter 214: loss 1.9938, time 4892.09ms, mfu 3.59%\n",
            "iter 215: loss 1.9629, time 4881.49ms, mfu 3.60%\n",
            "iter 216: loss 2.1948, time 4889.31ms, mfu 3.61%\n",
            "iter 217: loss 2.2509, time 4880.76ms, mfu 3.62%\n",
            "iter 218: loss 2.3754, time 4892.88ms, mfu 3.62%\n",
            "iter 219: loss 1.9321, time 4887.25ms, mfu 3.63%\n",
            "step 220: train loss 2.2380, val loss 2.7270\n",
            "iter 220: loss 2.0628, time 8314.14ms, mfu 3.48%\n",
            "iter 221: loss 2.0898, time 4899.94ms, mfu 3.50%\n",
            "iter 222: loss 2.1589, time 4892.76ms, mfu 3.52%\n",
            "iter 223: loss 1.9605, time 4898.69ms, mfu 3.53%\n",
            "iter 224: loss 2.2242, time 4890.13ms, mfu 3.55%\n",
            "iter 225: loss 1.7417, time 4900.36ms, mfu 3.56%\n",
            "iter 226: loss 2.2959, time 4905.00ms, mfu 3.57%\n",
            "iter 227: loss 2.1457, time 4896.70ms, mfu 3.58%\n",
            "iter 228: loss 1.9767, time 4938.56ms, mfu 3.58%\n",
            "iter 229: loss 2.1867, time 4920.08ms, mfu 3.59%\n",
            "iter 230: loss 2.3549, time 4925.97ms, mfu 3.60%\n",
            "iter 231: loss 1.9947, time 4940.97ms, mfu 3.60%\n",
            "iter 232: loss 1.9987, time 4915.97ms, mfu 3.61%\n",
            "iter 233: loss 1.7422, time 4923.76ms, mfu 3.61%\n",
            "iter 234: loss 2.0587, time 4913.46ms, mfu 3.61%\n",
            "iter 235: loss 2.2989, time 4904.81ms, mfu 3.62%\n",
            "iter 236: loss 2.0381, time 4904.57ms, mfu 3.62%\n",
            "iter 237: loss 2.5089, time 4903.72ms, mfu 3.63%\n",
            "iter 238: loss 2.2619, time 4907.78ms, mfu 3.63%\n",
            "iter 239: loss 2.5679, time 4894.21ms, mfu 3.63%\n",
            "step 240: train loss 2.2067, val loss 2.6877\n",
            "iter 240: loss 2.2022, time 8291.33ms, mfu 3.49%\n",
            "iter 241: loss 1.9742, time 4876.27ms, mfu 3.51%\n",
            "iter 242: loss 1.8822, time 4863.11ms, mfu 3.53%\n",
            "iter 243: loss 2.0303, time 4868.46ms, mfu 3.54%\n",
            "iter 244: loss 2.3777, time 4880.96ms, mfu 3.56%\n",
            "iter 245: loss 2.3769, time 4889.34ms, mfu 3.57%\n",
            "iter 246: loss 1.8807, time 4889.60ms, mfu 3.58%\n",
            "iter 247: loss 2.2033, time 4897.85ms, mfu 3.59%\n",
            "iter 248: loss 2.4411, time 4895.58ms, mfu 3.60%\n",
            "iter 249: loss 2.0798, time 4918.70ms, mfu 3.60%\n",
            "iter 250: loss 2.1504, time 4931.05ms, mfu 3.61%\n",
            "iter 251: loss 2.1325, time 4925.02ms, mfu 3.61%\n",
            "iter 252: loss 1.7800, time 4930.67ms, mfu 3.61%\n",
            "iter 253: loss 1.6346, time 4923.95ms, mfu 3.62%\n",
            "iter 254: loss 1.8800, time 4910.17ms, mfu 3.62%\n",
            "iter 255: loss 2.5425, time 4920.75ms, mfu 3.62%\n",
            "iter 256: loss 2.0153, time 4906.97ms, mfu 3.63%\n",
            "iter 257: loss 2.3814, time 4902.84ms, mfu 3.63%\n",
            "iter 258: loss 2.1060, time 4882.11ms, mfu 3.64%\n",
            "iter 259: loss 2.0003, time 4883.80ms, mfu 3.64%\n",
            "step 260: train loss 2.1157, val loss 2.7221\n",
            "iter 260: loss 2.4954, time 8326.47ms, mfu 3.49%\n",
            "iter 261: loss 2.1918, time 4874.74ms, mfu 3.51%\n",
            "iter 262: loss 2.0801, time 4880.62ms, mfu 3.53%\n",
            "iter 263: loss 2.6819, time 4881.58ms, mfu 3.54%\n",
            "iter 264: loss 2.1731, time 4880.00ms, mfu 3.56%\n",
            "iter 265: loss 2.2871, time 4888.82ms, mfu 3.57%\n",
            "iter 266: loss 2.6274, time 4877.59ms, mfu 3.58%\n",
            "iter 267: loss 1.7026, time 4889.48ms, mfu 3.59%\n",
            "iter 268: loss 2.2426, time 4883.05ms, mfu 3.60%\n",
            "iter 269: loss 2.3477, time 4893.93ms, mfu 3.61%\n",
            "iter 270: loss 2.3455, time 4885.98ms, mfu 3.61%\n",
            "iter 271: loss 2.0297, time 4889.42ms, mfu 3.62%\n",
            "iter 272: loss 2.0388, time 4893.46ms, mfu 3.62%\n",
            "iter 273: loss 2.2567, time 4900.50ms, mfu 3.63%\n",
            "iter 274: loss 2.0821, time 4877.54ms, mfu 3.63%\n",
            "iter 275: loss 1.9450, time 4903.62ms, mfu 3.64%\n",
            "iter 276: loss 2.0895, time 4910.15ms, mfu 3.64%\n",
            "iter 277: loss 2.2463, time 4911.24ms, mfu 3.64%\n",
            "iter 278: loss 1.8001, time 4903.19ms, mfu 3.64%\n",
            "iter 279: loss 2.1208, time 4905.87ms, mfu 3.64%\n",
            "step 280: train loss 2.1527, val loss 2.7172\n",
            "iter 280: loss 2.4002, time 8344.45ms, mfu 3.50%\n",
            "iter 281: loss 2.6296, time 4897.49ms, mfu 3.51%\n",
            "iter 282: loss 1.9240, time 4909.62ms, mfu 3.53%\n",
            "iter 283: loss 1.8597, time 4886.28ms, mfu 3.54%\n",
            "iter 284: loss 2.0568, time 4897.44ms, mfu 3.55%\n",
            "iter 285: loss 2.3606, time 4890.63ms, mfu 3.57%\n",
            "iter 286: loss 2.4757, time 4898.71ms, mfu 3.58%\n",
            "iter 287: loss 2.1945, time 4904.61ms, mfu 3.59%\n",
            "iter 288: loss 2.2199, time 4887.59ms, mfu 3.59%\n",
            "iter 289: loss 1.8269, time 4883.24ms, mfu 3.60%\n",
            "iter 290: loss 2.0975, time 4885.67ms, mfu 3.61%\n",
            "iter 291: loss 2.1547, time 4878.51ms, mfu 3.62%\n",
            "iter 292: loss 1.9074, time 4873.55ms, mfu 3.62%\n",
            "iter 293: loss 2.0404, time 4892.13ms, mfu 3.63%\n",
            "iter 294: loss 1.8841, time 4880.89ms, mfu 3.63%\n",
            "iter 295: loss 1.7482, time 4884.52ms, mfu 3.64%\n",
            "iter 296: loss 2.2234, time 4901.38ms, mfu 3.64%\n",
            "iter 297: loss 2.4322, time 4893.02ms, mfu 3.64%\n",
            "iter 298: loss 2.5125, time 4882.85ms, mfu 3.65%\n",
            "iter 299: loss 2.0620, time 4881.10ms, mfu 3.65%\n",
            "step 300: train loss 2.1231, val loss 2.7160\n",
            "iter 300: loss 2.2938, time 8309.63ms, mfu 3.50%\n",
            "iter 301: loss 1.8884, time 4896.80ms, mfu 3.52%\n",
            "iter 302: loss 1.8423, time 4892.95ms, mfu 3.53%\n",
            "iter 303: loss 2.1371, time 4891.10ms, mfu 3.55%\n",
            "iter 304: loss 2.3952, time 4874.78ms, mfu 3.56%\n",
            "iter 305: loss 2.3326, time 4893.95ms, mfu 3.57%\n",
            "iter 306: loss 2.1477, time 4898.52ms, mfu 3.58%\n",
            "iter 307: loss 1.9987, time 4890.84ms, mfu 3.59%\n",
            "iter 308: loss 1.9287, time 4898.30ms, mfu 3.60%\n",
            "iter 309: loss 2.3433, time 4891.12ms, mfu 3.61%\n",
            "iter 310: loss 2.0917, time 4889.73ms, mfu 3.61%\n",
            "iter 311: loss 1.9289, time 4887.86ms, mfu 3.62%\n",
            "iter 312: loss 1.7034, time 4892.37ms, mfu 3.62%\n",
            "iter 313: loss 2.0039, time 4888.89ms, mfu 3.63%\n",
            "iter 314: loss 1.9943, time 4884.86ms, mfu 3.63%\n",
            "iter 315: loss 2.3635, time 4881.71ms, mfu 3.64%\n",
            "iter 316: loss 1.7740, time 4897.17ms, mfu 3.64%\n",
            "iter 317: loss 2.4061, time 4879.54ms, mfu 3.65%\n",
            "iter 318: loss 1.8931, time 4882.26ms, mfu 3.65%\n",
            "iter 319: loss 2.1254, time 4868.27ms, mfu 3.65%\n",
            "step 320: train loss 2.0928, val loss 2.7163\n",
            "iter 320: loss 1.9379, time 8290.30ms, mfu 3.50%\n",
            "iter 321: loss 2.1163, time 4872.13ms, mfu 3.52%\n",
            "iter 322: loss 2.0795, time 4871.44ms, mfu 3.54%\n",
            "iter 323: loss 2.2363, time 4878.81ms, mfu 3.55%\n",
            "iter 324: loss 1.9421, time 4880.41ms, mfu 3.57%\n",
            "iter 325: loss 2.0418, time 4888.86ms, mfu 3.58%\n",
            "iter 326: loss 2.4163, time 4891.44ms, mfu 3.59%\n",
            "iter 327: loss 2.4898, time 4887.75ms, mfu 3.60%\n",
            "iter 328: loss 2.2141, time 4887.66ms, mfu 3.60%\n",
            "iter 329: loss 2.1981, time 4896.87ms, mfu 3.61%\n",
            "iter 330: loss 2.4444, time 4882.81ms, mfu 3.62%\n",
            "iter 331: loss 2.0667, time 4892.55ms, mfu 3.62%\n",
            "iter 332: loss 2.1142, time 4890.48ms, mfu 3.63%\n",
            "iter 333: loss 2.2881, time 4875.84ms, mfu 3.63%\n",
            "iter 334: loss 2.0155, time 4887.02ms, mfu 3.64%\n",
            "iter 335: loss 1.8378, time 4887.70ms, mfu 3.64%\n",
            "iter 336: loss 2.1034, time 4895.34ms, mfu 3.64%\n",
            "iter 337: loss 1.6294, time 4882.50ms, mfu 3.65%\n",
            "iter 338: loss 1.8377, time 4873.90ms, mfu 3.65%\n",
            "iter 339: loss 1.9574, time 4881.23ms, mfu 3.65%\n",
            "step 340: train loss 2.0910, val loss 2.7594\n",
            "iter 340: loss 2.1611, time 8302.75ms, mfu 3.50%\n",
            "iter 341: loss 1.7589, time 4872.46ms, mfu 3.52%\n",
            "iter 342: loss 2.1285, time 4885.17ms, mfu 3.54%\n",
            "iter 343: loss 1.7602, time 4884.51ms, mfu 3.55%\n",
            "iter 344: loss 1.6506, time 4874.07ms, mfu 3.57%\n",
            "iter 345: loss 2.2057, time 4886.72ms, mfu 3.58%\n",
            "iter 346: loss 1.8688, time 4887.76ms, mfu 3.59%\n",
            "iter 347: loss 1.9152, time 4880.98ms, mfu 3.60%\n",
            "iter 348: loss 2.0575, time 4869.23ms, mfu 3.61%\n",
            "iter 349: loss 2.1644, time 4873.75ms, mfu 3.61%\n",
            "iter 350: loss 1.8482, time 4860.73ms, mfu 3.62%\n",
            "iter 351: loss 1.8161, time 4878.81ms, mfu 3.63%\n",
            "iter 352: loss 1.7961, time 4870.35ms, mfu 3.63%\n",
            "iter 353: loss 1.9919, time 4877.13ms, mfu 3.64%\n",
            "iter 354: loss 2.1323, time 4870.72ms, mfu 3.64%\n",
            "iter 355: loss 1.7500, time 4896.59ms, mfu 3.65%\n",
            "iter 356: loss 2.0270, time 4883.68ms, mfu 3.65%\n",
            "iter 357: loss 1.8633, time 4885.42ms, mfu 3.65%\n",
            "iter 358: loss 1.5407, time 4891.93ms, mfu 3.65%\n",
            "iter 359: loss 1.9658, time 4878.42ms, mfu 3.66%\n",
            "step 360: train loss 2.0611, val loss 2.6985\n",
            "iter 360: loss 1.8266, time 8331.51ms, mfu 3.51%\n",
            "iter 361: loss 2.1639, time 4888.78ms, mfu 3.52%\n",
            "iter 362: loss 2.2560, time 4898.25ms, mfu 3.54%\n",
            "iter 363: loss 1.8341, time 4906.38ms, mfu 3.55%\n",
            "iter 364: loss 1.8917, time 4892.79ms, mfu 3.56%\n",
            "iter 365: loss 1.7274, time 4899.48ms, mfu 3.57%\n",
            "iter 366: loss 2.0547, time 4878.33ms, mfu 3.58%\n",
            "iter 367: loss 2.3013, time 4894.73ms, mfu 3.59%\n",
            "iter 368: loss 1.7607, time 4898.10ms, mfu 3.60%\n",
            "iter 369: loss 2.0550, time 4892.18ms, mfu 3.61%\n",
            "iter 370: loss 2.0435, time 4897.13ms, mfu 3.61%\n",
            "iter 371: loss 1.8430, time 4903.04ms, mfu 3.62%\n",
            "iter 372: loss 2.0842, time 4891.26ms, mfu 3.62%\n",
            "iter 373: loss 1.6474, time 4898.63ms, mfu 3.63%\n",
            "iter 374: loss 2.0527, time 4893.12ms, mfu 3.63%\n",
            "iter 375: loss 1.8565, time 4879.29ms, mfu 3.64%\n",
            "iter 376: loss 2.3376, time 4898.38ms, mfu 3.64%\n",
            "iter 377: loss 1.8690, time 4898.15ms, mfu 3.64%\n",
            "iter 378: loss 2.0183, time 4895.25ms, mfu 3.65%\n",
            "iter 379: loss 1.9098, time 4888.22ms, mfu 3.65%\n",
            "step 380: train loss 2.0112, val loss 2.7142\n",
            "iter 380: loss 1.9188, time 8349.51ms, mfu 3.50%\n",
            "iter 381: loss 1.9612, time 4902.27ms, mfu 3.52%\n",
            "iter 382: loss 1.8410, time 4900.49ms, mfu 3.53%\n",
            "iter 383: loss 2.0546, time 4889.70ms, mfu 3.54%\n",
            "iter 384: loss 2.2226, time 4878.80ms, mfu 3.56%\n",
            "iter 385: loss 2.2235, time 4881.62ms, mfu 3.57%\n",
            "iter 386: loss 1.8626, time 4882.62ms, mfu 3.58%\n",
            "iter 387: loss 1.8337, time 4888.32ms, mfu 3.59%\n",
            "iter 388: loss 1.7692, time 4893.16ms, mfu 3.60%\n",
            "iter 389: loss 1.9343, time 4925.79ms, mfu 3.60%\n",
            "iter 390: loss 1.8687, time 4946.22ms, mfu 3.61%\n",
            "iter 391: loss 2.2799, time 4965.49ms, mfu 3.61%\n",
            "iter 392: loss 2.0583, time 4984.14ms, mfu 3.61%\n",
            "iter 393: loss 1.8475, time 4987.96ms, mfu 3.61%\n",
            "iter 394: loss 2.0163, time 5001.90ms, mfu 3.60%\n",
            "iter 395: loss 1.4346, time 5021.54ms, mfu 3.60%\n",
            "iter 396: loss 2.2044, time 4998.85ms, mfu 3.60%\n",
            "iter 397: loss 1.8932, time 4997.76ms, mfu 3.60%\n",
            "iter 398: loss 2.0999, time 4983.95ms, mfu 3.60%\n",
            "iter 399: loss 2.1264, time 4962.91ms, mfu 3.60%\n",
            "step 400: train loss 2.0338, val loss 2.7980\n",
            "iter 400: loss 1.9976, time 8456.94ms, mfu 3.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sintetizando una respuesta en castellano con palabras con 400 iteraciones (prompting)\n",
        "\n",
        "Resultado con 400 iteraciones y adaptando a castellano un GPT2 pre-entrenado. Sesgamos la respuesta de la síntesis preguntando por algo, utilizando la técnica de \"Prompting\" preguntamos al modelo como si fueramos el Demonio, \n",
        "\n",
        "DEMONIO.\n",
        "        ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
        "\n",
        "Pedimos por dos ejemplos (samples) que el modelo nos devuelve separados por una línea de guiones. Con el argumento **--max_new_tokens=100** podemos controlar el tamaño, en carácteres, de la salida sintetizada."
      ],
      "metadata": {
        "id": "DfooKgzpKHFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --device=cuda \\\n",
        "    --out_dir=out-calderon \\\n",
        "    --start=\"DEMONIO. \\\n",
        "        ¿Cuál es la respuesta a la vida, el universo y a todo?\" \\\n",
        "    --num_samples=2 --max_new_tokens=100"
      ],
      "metadata": {
        "id": "7QLYgvXnATFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b1a198-a92b-4c5f-853e-7a1783f9b313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-calderon\n",
            "Overriding: start = DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "Overriding: num_samples = 2\n",
            "Overriding: max_new_tokens = 100\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "\n",
            "MARCELA.\n",
            "\n",
            "                   Eso no me desmaya.\n",
            "\n",
            "D. PED.\n",
            "\n",
            "  ¿Qué es la voz, esta parte, es la parte?\n",
            "\n",
            "MARCELA.\n",
            "\n",
            "  Es desmayóme; pero ¿quién es ese?\n",
            "\n",
            "D. PED.\n",
            "\n",
            "  Y á\n",
            "---------------\n",
            "DEMONIO.          ¿Cuál es la respuesta a la vida, el universo y a todo?\n",
            "\n",
            "SOLDADO.\n",
            "\n",
            "  ¡Ay de mí!\n",
            "\n",
            "ALCUZC.\n",
            "\n",
            "              Que aí, sé una dama te ofendo, cuanto,\n",
            "  Mi luz de mi sirena de mi irá verdades.\n",
            "\n",
            "SOLDADO.\n",
            "\n",
            "  Dí, ¿qué traidor?\n",
            "\n",
            "ALCUZC.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}