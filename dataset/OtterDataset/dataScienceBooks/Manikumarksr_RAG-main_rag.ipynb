{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pypdf import PdfReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "The Yeast Connectome:\n",
      "Constructing a Connectome Database\n",
      "from Scientific Literature using GPT\n",
      "Models\n",
      "A Thesis submitted for the completion of\n",
      "requirements for the degree of\n",
      "Bachelor of Science\n",
      "(Research)\n",
      "by\n",
      "Mani Kumar R\n",
      "Undergraduate Programme\n",
      "Indian Institute of Science\n",
      "Under the supervision of\n",
      "Prof. Marek Mutwil\n",
      "School of Biological Sciences, Nanyang Technological University,\n",
      "Singapore\n",
      "Prof. Mohit Kumar Jolly\n",
      "Department of Bioengineering, Indian Institute of Science\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader('Mani_kumar_Bachelor_Thesis.pdf') \n",
    "print(len(reader.pages)) \n",
    "page = reader.pages[0] \n",
    "print(page.extract_text()) #Print the first page of the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\n",
    "for i in range(len(reader.pages)): \n",
    "    page = reader.pages[i]\n",
    "    text+=page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48737\n",
      "1310\n",
      " INTRODUCTION 3\n",
      "Large Language Models have demonstrated superior capabilities in natural language\n",
      "processing tasks and beyond\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "sentences = text.split(\".\")\n",
    "print(len(sentences))\n",
    "print(sentences[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksrma\\anaconda3\\envs\\ml\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ksrma\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ksrma\\anaconda3\\envs\\ml\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings/vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksrma\\AppData\\Local\\Temp\\ipykernel_5312\\1529181893.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the vector database client\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=\"Bachelor_Thesis\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize!\n",
    "qdrant.upload_points(\n",
    "    collection_name=\"Bachelor_Thesis\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(sentence),  # Encode sentence to vector\n",
    "            payload={\"content\":sentence},\n",
    "        ) for idx, sentence in enumerate(sentences) \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': ' we processed\\nmore than 84,427 research abstracts and full text articles from leading journals(Figure\\n3'}, {'content': ' Then, We evaluated the fine-tuned model based on itâ€™s performance on\\n20 random articles'}, {'content': ' We processed 84,427 publications (Figure 3'}]\n"
     ]
    }
   ],
   "source": [
    "#function to Search in Pdf \n",
    "def search_in_pdf(query:str):\n",
    "    hits = qdrant.search(\n",
    "        collection_name=\"Bachelor_Thesis\",\n",
    "        query_vector=encoder.encode(query).tolist(),\n",
    "        limit=3\n",
    "    )\n",
    "    return [hit.payload for hit in hits]\n",
    "\n",
    "\n",
    "query=\"Number of researh articles processed?\"\n",
    "print(search_in_pdf(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Yes, the number of research articles processed by the research team is more than 84,427.</s>', refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Now time to connect to the local large language model\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\", # \"http://<Your api-server IP>:port\"    Your llamaserver url\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "\n",
    "user_prompt=\"In the Bachelor Thesis, what is the number of research articles processed?\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are chatbot, Your top priority is to help guide users with their requests.\"},\n",
    "        {\"role\": \"user\", \"content\":user_prompt },\n",
    "        {\"role\": \"assistant\", \"content\": str(search_in_pdf(user_prompt))}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
