{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space and Time Complexities\n",
    "\n",
    "If we want to compare different data structures and algorithms to solve our problems, we need a way to measure their performance. We achieve this by considering space and time complexity. \n",
    "- With space complexity, we're estimating the memory taken in respect to the size of the input data to solve the problem.\n",
    "- With time complexity, we're estimating the number of operations executed in respect to the size of the input data to solve the problem.\n",
    "\n",
    "When calculating the space and time complexity for an input size, we have a range of values to consider depending on various factors in the problem. For our purposes, we want to estimate the worst possible outcome (upper range) produced from an input size. We want to ask ourselves, \"How does the number of operations executed or memory taken *grow* with respect to our input size?\" This asymptotic analysis is described through Big O Notation. We model a function O(n) that expresses the number of operations executed or memory taken as the input size, n, approaches infinity. In the following graph, you can see some common shapes of our complexities growing in relation to our input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./assets/1_img0.png\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Complexities\n",
    "\n",
    "**Big O Notation ordered from most to least performant**\n",
    "- Constant: O(c)\n",
    "- Logarithmic: O(log(n))\n",
    "- Linear: O(n)\n",
    "- Linearithmic: O(nlog(n))\n",
    "- Quadratic: O(n^2)\n",
    "- Exponential: O(2^n)\n",
    "- Factorial: O(n!)\n",
    "\n",
    "**Understanding constant and linear functions**\n",
    "- **Constant**: The input size has no impact on the complexity.\n",
    "- **Linear**: The input size is directly proportional to the complexity.\n",
    "\n",
    "**Understanding polynomials**\n",
    "- **Quadratic**: n^2 = n * n. The difference between n and n^2 is the difference between traversing a line of blocks vs a square of blocks. Similarly, the difference between n and n^3 is the difference between traversing a line of blocks vs a cube of blocks. To expand this analogy beyond 3 dimensions, you can picture n^4 as traversing n cubes, and n^5 as traversing n * n * cubes. Hopefully, it gives you a sense for how much work is created by multiplying a value by itself.\n",
    "\n",
    "Another way to think about them is through trees. This is done in the next part.\n",
    "\n",
    "**Understanding exponentials and factorials**\n",
    "\n",
    "Visualizing equations with powers as trees (refer to the figure below) will help us better understand how they grow and later recognize them in algorithms. Given an exponential function *b^l*, the base *b* indicates the number of branches in a tree, while the exponent *l* indicates the levels in the tree. This can be seen in the image below. \n",
    "\n",
    "- **Exponential**: The outputs of these functions build upon itself. The outputs for the functions n^2 and 2^n differ wildly as n increases. Calculate it for yourself to see the difference.\n",
    "- **Factorials**: We can see factorials grow both a trees' branches and levels with respect to n unlike polynomial or exponential functions. Following this pattern, what could be even more expensive? Try n^n. Unlike factorials, it doesn't gradually increase in branches. It starts of with all the branches. \n",
    "\n",
    "**Understanding Logarithms**\n",
    "- **Logarithms**: They are just inversions of exponential functions. An exponential function 2^n doubles at each level. A logirthm shrinks by half at each level. In other words, logarithms shrink as quickly as exponentials grow, making them as cheap as exponentials are expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./assets/1_img1.png\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Performance\n",
    "There's a lot that comes together in understanding the performance of our code. Depending on our use case, we may need to consider some of the other factors:\n",
    "- **Average Case Analysis**: Sometimes the worst or best case complexities can be rare occurances. We get a more realistic picture by studying an algorithm's complexity with respect to the average input size. \n",
    "- **Amortized Analysis**: Asymtotic analysis can be too pessimistic. By averaging the worst complexities in a sequence over that sequence, we may have a fairer look at the algorithms performance.\n",
    "- **Execution Time**: Algorithms with the same time complexity, may take different times due to having different constant factors. Morever, asymtotic analysis ignores the lower order terms that may still have an impact.\n",
    "- **Hardware and System Dependencies**: The machine and system can impact an algorithm's complexity.\n",
    "- **Parallism and Concurrency**: Complexity analysis doesn't take the benefits and challenges associated parallism and concurrency.\n",
    "\n",
    "How do we do each of this? You can run emprical tests and statistically analyze, theoretically prove, profile and benchmark the algorithm's code, profile and benchmark the environments, and finally, profile and benchmark the parallel optimizations respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Data_structure\n",
    "- https://en.wikipedia.org/wiki/Algorithm\n",
    "- https://en.wikipedia.org/wiki/Space_complexity\n",
    "- https://en.wikipedia.org/wiki/Time_complexity\n",
    "- https://en.wikipedia.org/wiki/Big_O_notation\n",
    "- https://en.wikipedia.org/wiki/Average-case_complexity\n",
    "- https://en.wikipedia.org/wiki/Amortized_analysis\n",
    "- https://en.wikipedia.org/wiki/Worst-case_execution_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
