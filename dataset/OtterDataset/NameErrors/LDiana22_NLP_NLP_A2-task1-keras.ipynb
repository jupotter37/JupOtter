{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SICK Training data](http://www.site.uottawa.ca/~diana/csi5386/A2_2019/SICK_train.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_hyp 28\n",
      "Max_evi 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "file_name=\"data/training.txt\"\n",
    "with open(file_name,\"r\") as data:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    max_evi, max_hyp = 0, 0 \n",
    "    count = 1\n",
    "    for row in train:\n",
    "        hyp = len(row[\"sentence_A\"].split())\n",
    "        if hyp > max_hyp:\n",
    "            max_hyp = hyp\n",
    "        evi = len(row[\"sentence_B\"].split())\n",
    "        if evi > max_evi:\n",
    "            max_evi = evi\n",
    "    print(\"Max_hyp %s\" % str(max_hyp))        \n",
    "    print(\"Max_evi %s\" % str(max_evi))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_zip_file = \"data/glove.6B.zip\"\n",
    "glove_vectors_file = \"data/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import zipfile, urllib.request, shutil, os\n",
    "    \n",
    "#large file - 862 MB\n",
    "if (not os.path.isfile(glove_zip_file) and\n",
    "    not os.path.isfile(glove_vectors_file)):\n",
    "    with urllib.request.urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\") as response, open(glove_zip_file, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unzip_single_file(zip_file_name, output_file_name):\n",
    "    \"\"\"\n",
    "        If the outFile is already created, don't recreate\n",
    "        If the outFile does not exist, create it from the zipFile\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(output_file_name):\n",
    "        with open(output_file_name, 'wb') as out_file:\n",
    "            with zipfile.ZipFile(zip_file_name) as zipped:\n",
    "                for info in zipped.infolist():\n",
    "                    if output_file_name in info.filename:\n",
    "                        with zipped.open(info) as requested_file:\n",
    "                            out_file.write(requested_file.read())\n",
    "                            return\n",
    "\n",
    "unzip_single_file(glove_zip_file, glove_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "glove_wordmap = {}\n",
    "with open(glove_vectors_file, \"r\", encoding=\"utf8\") as glove:\n",
    "    for line in glove:\n",
    "        name, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[name] = np.fromstring(vector, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5d4125641ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"entailment_judgment\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msplit_data_into_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m720\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2793\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1414\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m720\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5d4125641ea3>\u001b[0m in \u001b[0;36msplit_data_into_scores\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "def split_data_into_scores(file_name=\"data/training.txt\"):\n",
    "\n",
    "    with open(file_name,\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        labels = []\n",
    "        for row in train:\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "    print(Counter(labels).most_common())\n",
    "split_data_into_scores()\n",
    "print(1414/(2793+1414+720))\n",
    "print(2793/(2793+1414+720))\n",
    "print(720/(2793+1414+720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import os\n",
    "from keras.regularizers import l2, l1_l2\n",
    "from keras.callbacks import *\n",
    "# from visualizer import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras import utils\n",
    "from keras.utils.np_utils import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils.visualize_util import plot  # THIS IS BAD\n",
    "# from data_reader import *\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccCallBack(Callback):\n",
    "    def __init__(self, xtrain, ytrain, xdev, ydev, xtest, ytest, vocab, opts):\n",
    "        self.xtrain = xtrain\n",
    "        self.ytrain = ytrain\n",
    "        self.xdev = xdev\n",
    "        self.ydev = ydev\n",
    "        self.xtest = xtest\n",
    "        self.ytest = ytest\n",
    "        self.vocab = vocab\n",
    "        self.opts = opts\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_acc = compute_acc(self.xtrain, self.ytrain, self.vocab, self.model, self.opts)\n",
    "        dev_acc = compute_acc(self.xdev, self.ydev, self.vocab, self.model, self.opts)\n",
    "        test_acc = compute_acc(self.xtest, self.ytest, self.vocab, self.model, self.opts)\n",
    "        logging.info('----------------------------------')\n",
    "        logging.info('Epoch ' + str(epoch) + ' train loss:' + str(logs.get('loss')) + ' - Validation loss: ' + str(\n",
    "            logs.get('val_loss')) + ' train acc: ' + str(train_acc[0]) + '/' + str(train_acc[1]) + ' dev acc: ' + str(\n",
    "            dev_acc[0]) + '/' + str(dev_acc[1]) + ' test acc: ' + str(test_acc[0]) + '/' + str(test_acc[1]))\n",
    "        logging.info('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(X, Y, vocab, model, opts):\n",
    "    scores = model.predict(X, batch_size=opts['batch_size'])\n",
    "    print(\"***\")\n",
    "    col_sum = np.sum(np.array(scores), axis=0)\n",
    "    print(col_sum)\n",
    "    prediction = np.zeros(scores.shape)\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = np.argmax(scores[i])\n",
    "        prediction[i][l] = 1.0\n",
    "    assert np.array_equal(np.ones(prediction.shape[0]), np.sum(prediction, axis=1))\n",
    "    plabels = np.argmax(prediction, axis=1)\n",
    "    \n",
    "    tlabels = np.argmax(Y, axis=1)\n",
    "    acc = sum([1 if x==y else 0 for x,y in list(zip(tlabels, plabels))])/len(tlabels)\n",
    "    return acc, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_H_n(X):\n",
    "    ans = X[:, -1, :]  # get last element from time dim\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_Y(X, xmaxlen):\n",
    "    return X[:, :xmaxlen, :]  # get first xmaxlen elem from time dim\n",
    "\n",
    "\n",
    "def get_R(X):\n",
    "    Y, alpha = X[0], X[1]\n",
    "    ans = K.T.batched_dot(Y, alpha)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(opts):\n",
    "    k = 2 * opts['lstm_units']  # 200\n",
    "    L = opts['xmaxlen']  # 35\n",
    "    N = opts['xmaxlen'] + opts['ymaxlen']\n",
    "    \n",
    "    main_input = Input(shape=(N,), dtype='int32', name='main_input') #(N,70)\n",
    "    x = Embedding(output_dim=opts['emb'], input_dim=len(VOCABULARY.keys())+1, input_length=N, name='x')(main_input)\n",
    "    drop_out = Dropout(0.6, name='dropout')(x) # 70,50\n",
    "    lstm_fwd = LSTM(opts['lstm_units'], return_sequences=True, name='lstm_fwd')(drop_out)\n",
    "    lstm_bwd = LSTM(opts['lstm_units'], return_sequences=True, go_backwards=True, name='lstm_bwd')(drop_out)\n",
    "    #70,100\n",
    "    bilstm = merge([lstm_fwd, lstm_bwd], name='bilstm', mode='concat')\n",
    "    #70,200\n",
    "    drop_out = Dropout(0.3, name=\"d_bilstm\")(bilstm)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    h_star = Activation('tanh')(drop_out)\n",
    "#     Y = Lambda(get_Y, arguments={\"xmaxlen\": L}, name=\"Y\", output_shape=(L, k))(h_star)\n",
    "#     WY = TimeDistributed(Dense(k, W_regularizer=l2(0.05)), name=\"WY\")(Y)#35,200\n",
    "\n",
    "#     M = Activation('tanh', name=\"M\")(WY)\n",
    "\n",
    "#     alpha_ = TimeDistributed(Dense(k, activation='linear'), name=\"alpha_\")(WY)\n",
    "\n",
    "#     alpha = Dense(k, activation='softmax', name=\"alpha\")(M) #35,200 Dense_33\n",
    "\n",
    "#     Wr = Dense(k, W_regularizer=l2(0.05), name=\"Dense_Wr\")(WY) #200,35\n",
    "#     #   \n",
    "#     h_star = Activation('tanh')(Wr)\n",
    "\n",
    "    \n",
    "    \n",
    "    flat_h_star = Flatten(name=\"flat_h_star\")(h_star)\n",
    "    out = Dense(3, W_regularizer=l2(0.03), activation='softmax')(flat_h_star)\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(opts['lr']))\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    h_n = Lambda(get_H_n, output_shape=(k,), name=\"h_n\")(drop_out)\n",
    "    #200\n",
    "    \n",
    "    \n",
    "    Y = Lambda(get_Y, arguments={\"xmaxlen\": L}, name=\"Y\", output_shape=(L, k))(drop_out)\n",
    "    #35,200\n",
    "    Whn = Dense(k, W_regularizer=l2(0.05), name=\"Wh_n\")(h_n) #200\n",
    "    Whn_x_e = RepeatVector(L, name=\"Wh_n_x_e\")(Whn)#35,200\n",
    "    \n",
    "\n",
    "    \n",
    "    WY = TimeDistributed(Dense(k, W_regularizer=l2(0.01)), name=\"WY\")(Y)#35,200\n",
    "    merged = merge([Whn_x_e, WY], name=\"merged\", mode='sum')\n",
    "    M = Activation('tanh', name=\"M\")(merged)\n",
    "    #35,200\n",
    "\n",
    "    alpha_ = TimeDistributed(Dense(k, activation='linear'), name=\"alpha_\")(M)\n",
    "\n",
    "#     flat_alpha = Flatten(name=\"flat_alpha\")(alpha_)\n",
    "#     alpha = Dense(L, activation='softmax', name=\"alpha\")(flat_alpha) #35,200 Dense_33\n",
    "    \n",
    "    \n",
    "    alpha = Dense(k, activation='softmax', name=\"alpha\")(M) #35,200 Dense_33\n",
    "\n",
    "    \n",
    "#     alpha = RepeatVector(k, name=\"alpha_rep\")(alpha)\n",
    "    \n",
    "    \n",
    "    Y_trans = Permute((2, 1), name=\"y_trans\")(Y)  # of shape (None,200,35)\n",
    "    \n",
    "    r = merge([Y, alpha], output_shape=(k, 1), name=\"r_\")#, mode=get_R)\n",
    "\n",
    "    \n",
    "#     r = merge([Y_trans, alpha], output_shape=(k, 1), name=\"r_\")#, mode=get_R)\n",
    "    #200,35\n",
    "\n",
    "#     r = Reshape((k,), name=\"r\")(r_)\n",
    "    Wr = Dense(k, W_regularizer=l2(0.01), name=\"Dense_Wr\")(r) #200,35\n",
    "#     Wh = Dense(k, W_regularizer=l2(0.01), name=\"Dense_Wh\")(Whn_x_e)\n",
    "#     Wh = Permute((2, 1), name=\"Wh_trans\")(Wh)\n",
    "    \n",
    "    \n",
    "    Wh = Dense(k, W_regularizer=l2(0.01))(Whn_x_e)\n",
    "    \n",
    "    \n",
    "    merged = merge([Wr, Wh], mode='sum')\n",
    "    h_star = Activation('tanh')(merged)\n",
    "    \n",
    "    flat_h_star = Flatten(name=\"flat_h_star\")(h_star)\n",
    "    out = Dense(1, activation='softmax')(flat_h_star)\n",
    "    #1\n",
    "    #relatedness score\n",
    "    \n",
    "    output = out\n",
    "    model = Model(input=[main_input], output=output)\n",
    "    model.summary()\n",
    "    # plot(model, 'model.png')\n",
    "    # # model.compile(loss={'output':'binary_crossentropy'}, optimizer=Adam())\n",
    "    # model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adam(options.lr))\n",
    "    model.compile(loss='MSE',optimizer=Adam(opts['lr']))\n",
    "    #MSE\n",
    "    return model\n",
    "\n",
    "# build_model(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Load & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        yaml_string = model.to_yaml()\n",
    "        open(archpath, 'w').write(yaml_string)\n",
    "    else:\n",
    "        with open(archpath, 'w') as f:\n",
    "            f.write(model.to_json())\n",
    "    model.save_weights(wtpath)\n",
    "\n",
    "\n",
    "def load_model(wtpath, archpath, mode='yaml'):\n",
    "    if mode == 'yaml':\n",
    "        model = model_from_yaml(open(archpath).read())  # ,custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    else:\n",
    "        with open(archpath) as f:\n",
    "            model = model_from_json(f.read())  # , custom_objects={\"MyEmbedding\": MyEmbedding})\n",
    "    model.load_weights(wtpath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def score_setup(row):\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }\n",
    "    score = np.zeros((3,))\n",
    "    tag = row[\"entailment_judgment\"]\n",
    "    score[convert_dict[tag]] += 1\n",
    "    return score\n",
    "#     return convert_dict[row[\"entailment_judgment\"]]\n",
    "VOCABULARY  = {}\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def split_data_into_scores(max_hyp, max_evi, file_name=\"data/training.txt\"):\n",
    "\n",
    "    global VOCABULARY, tokenizer\n",
    "    import csv\n",
    "    with open(file_name,\"r\") as data:\n",
    "        train = csv.DictReader(data , delimiter='\\t')\n",
    "        evi_sentences = np.empty((0,max_evi))\n",
    "        hyp_sentences = np.empty((0,max_hyp))\n",
    "        labels = []\n",
    "        scores = []\n",
    "        count = 1\n",
    "        for row in train:\n",
    "            hyp = row[\"sentence_A\"].lower()\n",
    "            evi = row[\"sentence_B\"].lower()\n",
    "            tokenizer.fit_on_texts([hyp])\n",
    "            tokenizer.fit_on_texts([evi])\n",
    "            hyp_seq = np.array(tokenizer.texts_to_sequences([hyp])[0])\n",
    "            \n",
    "            padded_hyp = np.pad(hyp_seq,\n",
    "                                (max_hyp-np.shape(hyp_seq)[0],0),\n",
    "                                       'constant',\n",
    "                                       constant_values=(0,))\n",
    "            hyp_sentences = np.append(hyp_sentences, [padded_hyp], axis=0)\n",
    "            count += 1\n",
    "            \n",
    "            evi_seq = np.array(tokenizer.texts_to_sequences([evi])[0])\n",
    "            padded_evi = np.pad(evi_seq,\n",
    "                               (max_evi-np.shape(evi_seq)[0],0),\n",
    "                                'constant',\n",
    "                                constant_values=(0,))\n",
    "            evi_sentences = np.append(evi_sentences, [padded_evi], axis=0)\n",
    "            labels.append(row[\"entailment_judgment\"])\n",
    "            scores.append(score_setup(row))\n",
    "        print(\"Vocabulary size: %s\" % str(len(tokenizer.word_counts.keys())+1))\n",
    "        VOCABULARY = tokenizer.word_index\n",
    "        VOCABULARY['unk'] = 0\n",
    "        print(np.shape(hyp_sentences))\n",
    "        print(np.shape(evi_sentences))\n",
    "        return hyp_sentences, evi_sentences, np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### tokenizer scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# t = Tokenizer()\n",
    "# fit_text = [\"The earth is an awesome place live\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.word_index)\n",
    "# fit_text = [\"Ana has apples\"]\n",
    "# t.fit_on_texts(fit_text)\n",
    "# print(t.texts_to_sequences([\"Ana is an awesome apple\"]))\n",
    "# # \n",
    "# print(len(t.word_index.keys()))\n",
    "# # from keras.utils.np_utils import to_categorical\n",
    "# # print(to_categorical([1], num_classes=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "# import numpy as np\n",
    "# a=np.empty((0,))\n",
    "# for i in range(0,10):\n",
    "#     a = np.append(a, [1,2,3], axis=0)\n",
    "# print(a)\n",
    "\n",
    "# print(np.pad(a, (0,0), 'constant', constant_values=(0,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model + options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'lstm_units': 50,\n",
    "    'xmaxlen': 32,\n",
    "    'ymaxlen': 32,\n",
    "    'emb': 75, #dimension of the embedding\n",
    "    'max_features': len(VOCABULARY.keys())+1, #vocabulary dim+1\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.005,\n",
    "    'epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "x (Embedding)                    (None, 64, 75)        174525      main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 64, 75)        0           x[0][0]                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_fwd (LSTM)                  (None, 64, 50)        25200       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_bwd (LSTM)                  (None, 64, 50)        25200       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bilstm (Merge)                   (None, 64, 100)       0           lstm_fwd[0][0]                   \n",
      "                                                                   lstm_bwd[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "d_bilstm (Dropout)               (None, 64, 100)       0           bilstm[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 64, 100)       0           d_bilstm[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flat_h_star (Flatten)            (None, 6400)          0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             19203       flat_h_star[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 244,128\n",
      "Trainable params: 244,128\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python 36 64\\lib\\site-packages\\ipykernel_launcher.py:44: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "model = build_model(opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2185\n",
      "(4500, 32)\n",
      "(4500, 32)\n",
      "(4500, 32)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  4.  5. 13.\n",
      "   2.  6.  3.  1.  7.  8. 14. 15.  9.  2. 10.  3. 11. 12.]]\n",
      "out\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  4.  5. 13.\n",
      "  2.  6.  3.  1.  7.  8. 14. 15.  9.  2. 10.  3. 11. 12.]\n",
      "***\n",
      "(4500, 64)\n",
      "***\n",
      "Vocabulary size: 2217\n",
      "(500, 32)\n",
      "(500, 32)\n",
      "***\n",
      "(500, 64)\n",
      "***\n",
      "Vocabulary size: 2326\n",
      "(4927, 32)\n",
      "(4927, 32)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,Z_train=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/training.txt\")\n",
    "print(np.shape(X_train))\n",
    "print(X_train[:1])\n",
    "\n",
    "print(\"out\")\n",
    "print(X_train[0])\n",
    "xy_train = np.concatenate((X_train, Y_train), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_train))\n",
    "print(\"***\")\n",
    "\n",
    "X_dev,Y_dev,Z_dev=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/dev.txt\")\n",
    "xy_dev = np.concatenate((X_dev, Y_dev), axis=1)\n",
    "print(\"***\")\n",
    "print(np.shape(xy_dev))\n",
    "print(\"***\")\n",
    "\n",
    "X_test,Y_test,Z_test=split_data_into_scores(opts['xmaxlen'], opts['ymaxlen'], \"data/test_labeled.txt\")\n",
    "xy_test = np.concatenate((X_test, Y_test), axis=1)\n",
    "\n",
    "train_dict = {'input': xy_train, 'output': Z_train}\n",
    "dev_dict = {'input': xy_dev, 'output': Z_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_units': 50, 'xmaxlen': 32, 'ymaxlen': 32, 'emb': 75, 'max_features': 1, 'batch_size': 32, 'lr': 0.005, 'epochs': 10}\n",
      "2326\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(xy_train[:2],Z_train[:2])#,\n",
    "#                             batch_size=opts['batch_size'])\n",
    "#                             epochs=opts['epochs'])\n",
    "print(opts)\n",
    "print(len(VOCABULARY.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 3)\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.5570 - val_loss: 1.7799\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 13s - loss: 0.5305 - val_loss: 1.8249\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.5073 - val_loss: 2.0619\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.4833 - val_loss: 2.2322s: 0.481\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.4676 - val_loss: 2.0012\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.4784 - val_loss: 2.1286\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 12s - loss: 0.4418 - val_loss: 2.3408\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 11s - loss: 0.4511 - val_loss: 1.9187\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 11s - loss: 0.4494 - val_loss: 2.1323\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 11s - loss: 0.4090 - val_loss: 2.6018\n",
      "***\n",
      "[1234.3131  2604.8425   660.84265]\n",
      "***\n",
      "[114.71675  336.78394   48.499393]\n",
      "***\n",
      "[1121.7454 3164.21    641.0432]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Z_train))\n",
    "print(Z_train[:3])\n",
    "\n",
    "history = model.fit(xy_train,Z_train,\n",
    "                            batch_size=opts['batch_size'],\n",
    "                            epochs=opts['epochs'],\n",
    "                            validation_data=(xy_dev, Z_dev)\n",
    "                            )\n",
    "scores = model.predict(xy_test, batch_size=opts['batch_size'])\n",
    "# scores = np.reshape(scores, (1,np.shape(scores)[0]))[0]\n",
    "\n",
    "# print(scores[:3])\n",
    "# print(np.shape(scores))\n",
    "# print(Z_test[:3])\n",
    "# test_acc = stats.pearsonr(scores, Z_test)\n",
    "# print(test_acc)\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "\n",
    "# if test_acc[0] > 0.59:\n",
    "opts_name = \"task1-%s\" % str(test_acc[0])\n",
    "save_model(model, 't1-model_weights-%s-%s.weights' % (str(opts_name), str(test_acc[0])),\n",
    "           't1-model_arch_att-%s-%s.yaml' % (str(opts_name), str(test_acc[0])))\n",
    "with open(opts_name, \"w\") as f:\n",
    "    f.write(str(opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xy_train,Z_train,\n",
    "                            batch_size=opts['batch_size'],\n",
    "                            epochs=opts['epochs'],\n",
    "                            validation_data=(xy_dev, Z_dev),\n",
    "                            callbacks=[\n",
    "                                AccCallBack(xy_train, Z_train, xy_dev, Z_dev, xy_test, Z_test, VOCABULARY, opts),\n",
    "                                EarlyStopping()\n",
    "                            ]\n",
    "                            )\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model, opts)\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "\n",
    "# if test_acc[0] > 0.58:\n",
    "opts_name = \"optsss-%s\" % str(test_acc[0])\n",
    "save_model(model, 'model_weights-%s-%s.weights' % (str(opts_name), str(test_acc[0])),\n",
    "           'model_arch_att-%s-%s.yaml' % (str(opts_name), str(test_acc[0])))\n",
    "with open(opts_name, \"w\") as f:\n",
    "    f.write(str(opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = load_model('model_weights-opts-6-0.5902171706921048-0.5902171706921048.weights', 'model_arch_att-optsss-0.5254718895879845-0.5254718895879845.yaml')\n",
    "# model2 = load_model('model_weights-opts-4-0.5810838238278871-0.5810838238278871.weights', 'model_arch_att-opts-4-0.5810838238278871-0.5810838238278871.yaml')\n",
    "model2 = load_model('model_weights-opts-3-0.5786482646640958.weights', 'model_arch_att-opts-3-0.5786482646640958.yaml')\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model2, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model2, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model2, opts)\n",
    "\n",
    "\n",
    "convert_dict = {\n",
    "      0:'ENTAILMENT',\n",
    "      1:'NEUTRAL',\n",
    "      2:'CONTRADICTION'\n",
    "    }\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "scores = model2.predict(xy_test, batch_size=opts['batch_size'])\n",
    "print(\"***\")\n",
    "col_sum = np.sum(np.array(scores), axis=0)\n",
    "print(col_sum)\n",
    "prediction = []\n",
    "with open(\"Results.txt\", \"r\") as f:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    count = 1\n",
    "    for row in train[:3]:\n",
    "        print(row)\n",
    "        \n",
    "#     for i in range(scores.shape[0]):\n",
    "#         l = np.argmax(scores[i])\n",
    "#         prediction.append(convert_dict[l])\n",
    "#         f.write(\"%s\\t%s\\n\" % (str(IDS[i]), prediction[i]))\n",
    "\n",
    "\n",
    "\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(Z_train, axis=0))\n",
    "print(np.sum(Z_dev, axis=0))\n",
    "print(np.sum(Z_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = load_model('model_weights-opts-6-0.5902171706921048-0.5902171706921048.weights', 'model_arch_att-optsss-0.5254718895879845-0.5254718895879845.yaml')\n",
    "# model2 = load_model('model_weights-opts-4-0.5810838238278871-0.5810838238278871.weights', 'model_arch_att-opts-4-0.5810838238278871-0.5810838238278871.yaml')\n",
    "model2 = load_model('model_weights-opts-3-0.5786482646640958.weights', 'model_arch_att-opts-3-0.5786482646640958.yaml')\n",
    "\n",
    "train_acc = compute_acc(xy_train, Z_train, VOCABULARY, model2, opts)\n",
    "dev_acc = compute_acc(xy_dev, Z_dev, VOCABULARY, model2, opts)\n",
    "test_acc = compute_acc(xy_test, Z_test, VOCABULARY, model2, opts)\n",
    "\n",
    "\n",
    "convert_dict = {\n",
    "      0:'ENTAILMENT',\n",
    "      1:'NEUTRAL',\n",
    "      2:'CONTRADICTION'\n",
    "    }\n",
    "\n",
    "print(train_acc)\n",
    "print(dev_acc)\n",
    "print(test_acc)\n",
    "scores = model2.predict(xy_test, batch_size=opts['batch_size'])\n",
    "print(\"***\")\n",
    "col_sum = np.sum(np.array(scores), axis=0)\n",
    "print(col_sum)\n",
    "prediction = []\n",
    "with open(\"Results.txt\", \"w\") as f:\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = np.argmax(scores[i])\n",
    "        prediction.append(convert_dict[l])\n",
    "        f.write(\"%s\\t%s\\n\" % (str(IDS[i]), prediction[i]))\n",
    "\n",
    "\n",
    "\n",
    "    convert_dict = {\n",
    "      'ENTAILMENT': 0,\n",
    "      'NEUTRAL': 1,\n",
    "      'CONTRADICTION': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_labeled.txt\",\"r\") as data:\n",
    "    train = csv.DictReader(data , delimiter='\\t')\n",
    "    IDS = []\n",
    "    for row in train:\n",
    "        IDS.append(row[\"pair_ID\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
