{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cdbe62",
   "metadata": {},
   "source": [
    "\n",
    "# Compute Average of Numbers in an RDD using PySpark with flatMap\n",
    "\n",
    "This notebook demonstrates how to calculate the average of a list of numbers using PySpark with the `flatMap` transformation. \n",
    "The `compute_average_flatmap` function transforms each RDD element into a sequence of `(sum, count)` pairs.\n",
    "\n",
    "## Steps Involved\n",
    "1. Parallelize the list into an RDD.\n",
    "2. Use `flatMap` to convert each number into a `(1, (number, 1))` pair for aggregation.\n",
    "3. Use `reduceByKey` to sum up the values and counts.\n",
    "4. Calculate the average by dividing the total sum by the count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634305fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     18\u001b[0m names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJane Smith\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice Johnson\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m split_names(sc, \u001b[43mfull_names\u001b[49m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_names' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "def split_names(sc, full_names):\n",
    "    # Parallelize the list into an RDD\n",
    "    rdd = sc.parallelize(full_names)\n",
    "    \n",
    "    # Use flatMap to split each full name into first and last name\n",
    "    # For each full name, flatMap will return a list of individual names\n",
    "    split_rdd = rdd.flatMap(lambda name: name.split(\" \"))\n",
    "    \n",
    "    # Collect the results\n",
    "    \n",
    "    result = split_rdd.collect()\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\"]\n",
    "    result = split_names(sc, full_names)\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
