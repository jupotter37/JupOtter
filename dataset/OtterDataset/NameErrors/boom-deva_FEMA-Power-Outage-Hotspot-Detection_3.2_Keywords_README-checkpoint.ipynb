{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed method for expanding power outage keywords:\n",
    "\n",
    "## Forward:\n",
    "As you are by now aware, our twitter scraping function was designed to seek out keywords which we though were relevant to power outages.  This prefiltering feature was very helpful for cutting through the overwhelming volume of tweets that are posted in the United States each day, and we believed it would increse the proportion of tweets from blackout zones in our data.  \n",
    "\n",
    "In the current version of our scraping function, data is drawn from twitter on a very limited number of keywords.  Ideally, our keyword vocabulary would be much more expansive, and it would be even better if those keywords were chosen from a corpus of confirmed tweets written from actual blackout events.  \n",
    "\n",
    "Unfortunately, isolating blackouts based on Twitter data proved very challenging.  Without indepenantly confirmed locations for our tweets, we have not been able to implement the improved keyword selection that we origionally envisioned.  Nevertheless, we believe that improved keywords are imperitive to any future rollout of this project.  In this notebook, we will lay out the theory and practice that we think best for expanding and enriching the keyword vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step in upgrading our keywords is to explore a body of words which we can prove are related to power outages.  \n",
    "\n",
    "To that end: we will use Gensim and a corpus of Wikipedia articles that we are repurposing here from some earlier classwork.  Our goal is to expand the keywords by disambiguating from words we have chosen that relate to blackouts; this should return a body of words that we may not have even considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Wiki text-data: takes ~ 3 minutes to load:\n",
    "\n",
    "# t0 = time.time()\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./lexvec.enwiki+newscrawl.300d.W.pos.vectors')\n",
    "# print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My identified terms:\n",
    "\n",
    "words = ['blackout','power','outage','electric','electricity','electrical','transformer','watt','wattage','arc',\n",
    "         'circuit','breaker','cable','fault','conductor','fuse','riser','insulator','meter',\n",
    "         'interruption','maintenance','relay','grid','severe','weather','storm','substation','surge','switch',\n",
    "         'switchyard','station','transmission','system','lines','line','frequency','voltage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of new terms, organized by parent term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-df467e97635c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbanker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m.39\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbanker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "banker = {}\n",
    "for word in words:\n",
    "    arr = pd.DataFrame(model.most_similar(word,topn=20),columns=['word','val'])\n",
    "    arr = arr[arr['val']>.39]\n",
    "    banker[word] = list(arr.word)\n",
    "    \n",
    "banker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2: List Layout:\n",
    "bank = []\n",
    "for word in words:\n",
    "    arr = pd.DataFrame(model.most_similar(word,topn=20),columns=['word','val'])\n",
    "    arr = arr[arr['val']>.39]\n",
    "    for i in arr.word:\n",
    "        bank.append(i)\n",
    "\n",
    "# Add origional terms to list\n",
    "for word in words:\n",
    "    bank.append(word)\n",
    "\n",
    "#print(bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "As you can see, we now have a working dictionary of far, far more potential keywords.  This is great outcome for a number of reasons.  The first is that we've achieved the desired outcome; tweets could just as easily be scraped from this larger body of keywords as they were from the smaller group we used to generate this new list.  \n",
    "\n",
    "One unexpected but welcome outcome of generating these new keywords is that some of these keywords are clearly \"off theme\".  While \"electrical\" - a term we disambiguated from above - is concievably desirable as a keyword, the terms we generated from it are clearly not.  These terms are all very technical, and are unlikely to appear in a tweet pertaining to a blackout.\n",
    "\n",
    "The great thing about this is that we could now begin to refine our keywords based on what does or does not appear to work. \n",
    "\n",
    "Whatever keywords that we choose to keep at this stage, the next and final stage in our keyword refinement would depend on having a body of confirmed tweets from actual blackouts.  If we had that data, we would now proceed to count vectorize these tweets, and perform a term frequency analysis on that corpus that would help us winnow the useful keywords we gathered from less useful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
