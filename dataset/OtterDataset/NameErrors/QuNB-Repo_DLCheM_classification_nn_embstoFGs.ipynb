{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network takes in labelled embeddings and learns relationship between embedding and FG label classification. Saliency maps are used to study which parts of the embedding contributing to which FG label and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the labelled embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#define label path embedding\n",
    "embslabeled_datafilepath = '../data/embs/model1-10000/layer6/Oembs/embstransform.csv'\n",
    "\n",
    "#use pandas to load the embedding data as a dataframe\n",
    "embslabeled_data = pd.read_csv(embslabeled_datafilepath,delimiter=',')\n",
    "\n",
    "#testing fractions...  \n",
    "traintest_fraction = 0.5  #should be large enough to have >1 members in each class\n",
    "testval_fraction = 0.2\n",
    "\n",
    "n_features = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_677/3461214628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ldalabel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membslabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='ldalabel', data=embslabeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove classes that have one member only (i.e water, methanol, formaldehyde... in certain classificaiton systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([6, 14, 0], dtype='int64')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amerelsamman/anaconda3/envs/dlchem/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_677/1589517314.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#print(embslabeled_data_proc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Remove rows with missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ldalabel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membslabeled_data_proc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#class counts is a pandas df, with index, that is the counts of each label value in ldalabel column\n",
    "class_counts = embslabeled_data['ldalabel'].value_counts()\n",
    "\n",
    "#which classes are less than two? this is the \n",
    "less_than_2 = class_counts[class_counts < 3].index\n",
    "print(less_than_2)\n",
    "\n",
    "#remove rows with classes that have less than 2 members\n",
    "#use boolean indexing with isin function, ~ means those NOT satisfying condition\n",
    "embslabeled_data_proc = embslabeled_data[~embslabeled_data['ldalabel'].isin(less_than_2)]\n",
    "\n",
    "#Find the now unique labels, and sort them\n",
    "unique_labels = np.sort(embslabeled_data_proc['ldalabel'].unique())\n",
    "\n",
    "#Create a label map, such that for each label is an ordered index labeling system\n",
    "#No such thing as missing label, 0-N, because we deleted things we have to make a dictionary of the transformation\n",
    "#For each label there is index i \n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "#Use the label map on the ldalabel column, and the map function for a pandas column df\n",
    "embslabeled_data_proc['ldalabel'] = embslabeled_data_proc['ldalabel'].map(label_map)\n",
    "\n",
    "#print(embslabeled_data_proc)\n",
    "# Remove rows with missing values\n",
    "sns.countplot(x='ldalabel', data=embslabeled_data_proc)\n",
    "\n",
    "\n",
    "print('count_label_21', embslabeled_data_proc['ldalabel'].value_counts()[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#The embedding data is the first 128 columns\n",
    "X = embslabeled_data_proc.iloc[:, 0:n_features]\n",
    "#the ldalabel is in the 133th column\n",
    "y = embslabeled_data_proc.iloc[:, 133]\n",
    "\n",
    "#split into train, test,\n",
    "#then split into val\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=traintest_fraction,stratify=y,random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test,y_test,test_size=testval_fraction,stratify=y_test,random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = n_features\n",
    "NUM_CLASSES = len(unique_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the multiclass classification neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, num_class) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Dataset class which will be used conjointly with DataLoader to efficiently load batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "#Define the dataset class that will initialize the X,y dataset \n",
    "#As either train, val, test_dataset\n",
    "#This class is cool because it can be used conjugtly with DataLoader \n",
    "#To efficiently load batches\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "dtype = torch.float32\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train.values).type(dtype), torch.from_numpy(y_train.values).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val.values).type(dtype), torch.from_numpy(y_val.values).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test.values).type(dtype), torch.from_numpy(y_test.values).long())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Tangent: we need to define a weight sampler, that will put weights on smaller classes so that they are represented more in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recount class counts  after embslabeled data has been processed as above\n",
    "class_counts = embslabeled_data_proc['ldalabel'].value_counts()\n",
    "\n",
    "#get the target labels as a list\n",
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "#tensor the list\n",
    "target_list = torch.tensor(target_list)\n",
    "\n",
    "#Determine class weight for each class using the 1/class counts\n",
    "class_weights = 1./torch.tensor(class_counts, dtype=torch.float) \n",
    "\n",
    "#weigh each of the targets using the class weights\n",
    "#in other words apply the weights on the target list\n",
    "class_weights_all = class_weights[target_list]\n",
    "\n",
    "\n",
    "#Define the weighted random sampler\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train_loader, val_loader and test_loader which will load the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          sampler=weighted_sampler\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define multi_class accuracy function, takes in the y_pred and y_test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "\n",
    "    #This transfroms the NUM_CLASS dimensional output \n",
    "    #To a probability scores for each class\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "\n",
    "    \n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "\n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aae8ee8808a42bfbf9b38ee38d33602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.07663 | Val Loss: 1.21638 | Train Acc: 59.879| Val Acc: 27.857\n",
      "Epoch 002: | Train Loss: 0.46322 | Val Loss: 0.77634 | Train Acc: 75.636| Val Acc: 30.500\n",
      "Epoch 003: | Train Loss: 0.34105 | Val Loss: 0.67038 | Train Acc: 78.591| Val Acc: 32.643\n",
      "Epoch 004: | Train Loss: 0.27112 | Val Loss: 0.65039 | Train Acc: 80.848| Val Acc: 32.357\n",
      "Epoch 005: | Train Loss: 0.23500 | Val Loss: 0.59991 | Train Acc: 81.591| Val Acc: 33.071\n",
      "Epoch 006: | Train Loss: 0.23294 | Val Loss: 0.60432 | Train Acc: 80.818| Val Acc: 34.857\n",
      "Epoch 007: | Train Loss: 0.22059 | Val Loss: 0.65446 | Train Acc: 82.273| Val Acc: 33.714\n",
      "Epoch 008: | Train Loss: 0.21346 | Val Loss: 0.55643 | Train Acc: 82.303| Val Acc: 35.286\n",
      "Epoch 009: | Train Loss: 0.22095 | Val Loss: 0.50667 | Train Acc: 83.500| Val Acc: 38.500\n",
      "Epoch 010: | Train Loss: 0.20592 | Val Loss: 0.51306 | Train Acc: 83.000| Val Acc: 45.000\n",
      "Epoch 011: | Train Loss: 0.20583 | Val Loss: 0.50548 | Train Acc: 84.424| Val Acc: 45.357\n",
      "Epoch 012: | Train Loss: 0.18015 | Val Loss: 0.44780 | Train Acc: 86.015| Val Acc: 47.857\n",
      "Epoch 013: | Train Loss: 0.16796 | Val Loss: 0.44077 | Train Acc: 86.652| Val Acc: 46.714\n",
      "Epoch 014: | Train Loss: 0.20366 | Val Loss: 0.43949 | Train Acc: 85.091| Val Acc: 45.929\n",
      "Epoch 015: | Train Loss: 0.17920 | Val Loss: 0.43000 | Train Acc: 86.470| Val Acc: 47.071\n",
      "Epoch 016: | Train Loss: 0.15455 | Val Loss: 0.40312 | Train Acc: 87.500| Val Acc: 46.857\n",
      "Epoch 017: | Train Loss: 0.17517 | Val Loss: 0.42256 | Train Acc: 87.894| Val Acc: 54.143\n",
      "Epoch 018: | Train Loss: 0.17790 | Val Loss: 0.38005 | Train Acc: 87.652| Val Acc: 49.929\n",
      "Epoch 019: | Train Loss: 0.15850 | Val Loss: 0.36163 | Train Acc: 88.424| Val Acc: 55.857\n",
      "Epoch 020: | Train Loss: 0.16310 | Val Loss: 0.37634 | Train Acc: 88.667| Val Acc: 53.286\n",
      "Epoch 021: | Train Loss: 0.15998 | Val Loss: 0.39119 | Train Acc: 88.788| Val Acc: 52.071\n",
      "Epoch 022: | Train Loss: 0.15960 | Val Loss: 0.36798 | Train Acc: 88.076| Val Acc: 61.786\n",
      "Epoch 023: | Train Loss: 0.15727 | Val Loss: 0.36793 | Train Acc: 89.136| Val Acc: 59.143\n",
      "Epoch 024: | Train Loss: 0.16008 | Val Loss: 0.33951 | Train Acc: 89.621| Val Acc: 61.000\n",
      "Epoch 025: | Train Loss: 0.14845 | Val Loss: 0.37735 | Train Acc: 89.061| Val Acc: 59.357\n",
      "Epoch 026: | Train Loss: 0.15591 | Val Loss: 0.32356 | Train Acc: 89.424| Val Acc: 61.571\n",
      "Epoch 027: | Train Loss: 0.14462 | Val Loss: 0.34180 | Train Acc: 89.879| Val Acc: 60.643\n",
      "Epoch 028: | Train Loss: 0.14520 | Val Loss: 0.30763 | Train Acc: 90.061| Val Acc: 65.929\n",
      "Epoch 029: | Train Loss: 0.15010 | Val Loss: 0.33068 | Train Acc: 90.939| Val Acc: 66.643\n",
      "Epoch 030: | Train Loss: 0.15496 | Val Loss: 0.34468 | Train Acc: 90.379| Val Acc: 72.143\n",
      "Epoch 031: | Train Loss: 0.15395 | Val Loss: 0.31855 | Train Acc: 90.045| Val Acc: 77.143\n",
      "Epoch 032: | Train Loss: 0.15542 | Val Loss: 0.31737 | Train Acc: 89.727| Val Acc: 73.714\n",
      "Epoch 033: | Train Loss: 0.13567 | Val Loss: 0.28657 | Train Acc: 91.303| Val Acc: 82.571\n",
      "Epoch 034: | Train Loss: 0.12805 | Val Loss: 0.33360 | Train Acc: 91.530| Val Acc: 81.857\n",
      "Epoch 035: | Train Loss: 0.13307 | Val Loss: 0.30765 | Train Acc: 91.500| Val Acc: 88.071\n",
      "Epoch 036: | Train Loss: 0.14218 | Val Loss: 0.29213 | Train Acc: 90.985| Val Acc: 87.929\n",
      "Epoch 037: | Train Loss: 0.12942 | Val Loss: 0.28493 | Train Acc: 91.348| Val Acc: 87.429\n",
      "Epoch 038: | Train Loss: 0.12948 | Val Loss: 0.27252 | Train Acc: 91.636| Val Acc: 90.143\n",
      "Epoch 039: | Train Loss: 0.12640 | Val Loss: 0.27540 | Train Acc: 92.348| Val Acc: 90.286\n",
      "Epoch 040: | Train Loss: 0.12780 | Val Loss: 0.29572 | Train Acc: 92.106| Val Acc: 90.929\n",
      "Epoch 041: | Train Loss: 0.12713 | Val Loss: 0.28850 | Train Acc: 92.424| Val Acc: 88.214\n",
      "Epoch 042: | Train Loss: 0.11702 | Val Loss: 0.25338 | Train Acc: 92.621| Val Acc: 90.929\n",
      "Epoch 043: | Train Loss: 0.14006 | Val Loss: 0.27948 | Train Acc: 91.712| Val Acc: 89.500\n",
      "Epoch 044: | Train Loss: 0.12501 | Val Loss: 0.26696 | Train Acc: 91.985| Val Acc: 90.214\n",
      "Epoch 045: | Train Loss: 0.13694 | Val Loss: 0.26456 | Train Acc: 91.833| Val Acc: 89.143\n",
      "Epoch 046: | Train Loss: 0.12685 | Val Loss: 0.29833 | Train Acc: 92.758| Val Acc: 91.643\n",
      "Epoch 047: | Train Loss: 0.12578 | Val Loss: 0.26635 | Train Acc: 92.970| Val Acc: 90.214\n",
      "Epoch 048: | Train Loss: 0.12264 | Val Loss: 0.28650 | Train Acc: 92.394| Val Acc: 88.143\n",
      "Epoch 049: | Train Loss: 0.13242 | Val Loss: 0.25429 | Train Acc: 92.288| Val Acc: 90.929\n",
      "Epoch 050: | Train Loss: 0.13307 | Val Loss: 0.26562 | Train Acc: 92.394| Val Acc: 91.214\n",
      "Epoch 051: | Train Loss: 0.12413 | Val Loss: 0.26713 | Train Acc: 92.455| Val Acc: 90.286\n",
      "Epoch 052: | Train Loss: 0.12556 | Val Loss: 0.26384 | Train Acc: 92.530| Val Acc: 92.643\n",
      "Epoch 053: | Train Loss: 0.11993 | Val Loss: 0.25125 | Train Acc: 93.288| Val Acc: 93.929\n",
      "Epoch 054: | Train Loss: 0.11780 | Val Loss: 0.26632 | Train Acc: 93.182| Val Acc: 93.571\n",
      "Epoch 055: | Train Loss: 0.10920 | Val Loss: 0.23807 | Train Acc: 93.652| Val Acc: 91.929\n",
      "Epoch 056: | Train Loss: 0.10894 | Val Loss: 0.26507 | Train Acc: 93.106| Val Acc: 92.714\n",
      "Epoch 057: | Train Loss: 0.10509 | Val Loss: 0.27021 | Train Acc: 93.682| Val Acc: 91.929\n",
      "Epoch 058: | Train Loss: 0.11743 | Val Loss: 0.23994 | Train Acc: 93.212| Val Acc: 92.571\n",
      "Epoch 059: | Train Loss: 0.12774 | Val Loss: 0.25627 | Train Acc: 92.970| Val Acc: 92.357\n",
      "Epoch 060: | Train Loss: 0.09877 | Val Loss: 0.29798 | Train Acc: 94.045| Val Acc: 92.714\n",
      "Epoch 061: | Train Loss: 0.13020 | Val Loss: 0.24344 | Train Acc: 92.773| Val Acc: 93.500\n",
      "Epoch 062: | Train Loss: 0.10642 | Val Loss: 0.25368 | Train Acc: 93.561| Val Acc: 93.357\n",
      "Epoch 063: | Train Loss: 0.11205 | Val Loss: 0.27302 | Train Acc: 93.606| Val Acc: 92.786\n",
      "Epoch 064: | Train Loss: 0.11709 | Val Loss: 0.24118 | Train Acc: 93.712| Val Acc: 93.429\n",
      "Epoch 065: | Train Loss: 0.11058 | Val Loss: 0.31474 | Train Acc: 93.636| Val Acc: 92.714\n",
      "Epoch 066: | Train Loss: 0.11940 | Val Loss: 0.26378 | Train Acc: 93.076| Val Acc: 91.571\n",
      "Epoch 067: | Train Loss: 0.11352 | Val Loss: 0.27283 | Train Acc: 92.955| Val Acc: 91.571\n",
      "Epoch 068: | Train Loss: 0.10733 | Val Loss: 0.27830 | Train Acc: 93.561| Val Acc: 93.929\n",
      "Epoch 069: | Train Loss: 0.11875 | Val Loss: 0.23324 | Train Acc: 93.106| Val Acc: 94.357\n",
      "Epoch 070: | Train Loss: 0.10575 | Val Loss: 0.26524 | Train Acc: 93.667| Val Acc: 94.429\n",
      "Epoch 071: | Train Loss: 0.11812 | Val Loss: 0.25289 | Train Acc: 93.000| Val Acc: 94.214\n",
      "Epoch 072: | Train Loss: 0.09622 | Val Loss: 0.23984 | Train Acc: 94.318| Val Acc: 94.143\n",
      "Epoch 073: | Train Loss: 0.10860 | Val Loss: 0.25225 | Train Acc: 93.985| Val Acc: 95.071\n",
      "Epoch 074: | Train Loss: 0.10347 | Val Loss: 0.23755 | Train Acc: 93.333| Val Acc: 92.929\n",
      "Epoch 075: | Train Loss: 0.10033 | Val Loss: 0.26682 | Train Acc: 94.121| Val Acc: 93.643\n",
      "Epoch 076: | Train Loss: 0.10619 | Val Loss: 0.28903 | Train Acc: 93.985| Val Acc: 92.500\n",
      "Epoch 077: | Train Loss: 0.09421 | Val Loss: 0.22556 | Train Acc: 93.515| Val Acc: 93.643\n",
      "Epoch 078: | Train Loss: 0.10320 | Val Loss: 0.23481 | Train Acc: 94.273| Val Acc: 93.786\n",
      "Epoch 079: | Train Loss: 0.10712 | Val Loss: 0.26091 | Train Acc: 93.621| Val Acc: 94.000\n",
      "Epoch 080: | Train Loss: 0.09566 | Val Loss: 0.21182 | Train Acc: 93.576| Val Acc: 94.000\n",
      "Epoch 081: | Train Loss: 0.10441 | Val Loss: 0.23277 | Train Acc: 93.803| Val Acc: 94.357\n",
      "Epoch 082: | Train Loss: 0.09988 | Val Loss: 0.23025 | Train Acc: 93.803| Val Acc: 94.571\n",
      "Epoch 083: | Train Loss: 0.08730 | Val Loss: 0.22692 | Train Acc: 94.409| Val Acc: 94.429\n",
      "Epoch 084: | Train Loss: 0.09233 | Val Loss: 0.26377 | Train Acc: 94.333| Val Acc: 94.571\n",
      "Epoch 085: | Train Loss: 0.10305 | Val Loss: 0.25298 | Train Acc: 94.136| Val Acc: 93.714\n",
      "Epoch 086: | Train Loss: 0.09456 | Val Loss: 0.22681 | Train Acc: 94.864| Val Acc: 94.714\n",
      "Epoch 087: | Train Loss: 0.09796 | Val Loss: 0.26273 | Train Acc: 94.515| Val Acc: 94.214\n",
      "Epoch 088: | Train Loss: 0.09716 | Val Loss: 0.23549 | Train Acc: 94.152| Val Acc: 95.286\n",
      "Epoch 089: | Train Loss: 0.08689 | Val Loss: 0.24050 | Train Acc: 94.667| Val Acc: 95.214\n",
      "Epoch 090: | Train Loss: 0.11343 | Val Loss: 0.29325 | Train Acc: 93.773| Val Acc: 93.571\n",
      "Epoch 091: | Train Loss: 0.09639 | Val Loss: 0.22769 | Train Acc: 94.545| Val Acc: 95.143\n",
      "Epoch 092: | Train Loss: 0.10120 | Val Loss: 0.23448 | Train Acc: 93.758| Val Acc: 93.429\n",
      "Epoch 093: | Train Loss: 0.08834 | Val Loss: 0.25665 | Train Acc: 94.727| Val Acc: 94.929\n",
      "Epoch 094: | Train Loss: 0.08943 | Val Loss: 0.23338 | Train Acc: 94.576| Val Acc: 94.786\n",
      "Epoch 095: | Train Loss: 0.09566 | Val Loss: 0.23681 | Train Acc: 94.758| Val Acc: 95.643\n",
      "Epoch 096: | Train Loss: 0.07967 | Val Loss: 0.24226 | Train Acc: 95.318| Val Acc: 94.571\n",
      "Epoch 097: | Train Loss: 0.09200 | Val Loss: 0.22923 | Train Acc: 94.364| Val Acc: 95.143\n",
      "Epoch 098: | Train Loss: 0.08860 | Val Loss: 0.23071 | Train Acc: 94.682| Val Acc: 95.143\n",
      "Epoch 099: | Train Loss: 0.09639 | Val Loss: 0.22375 | Train Acc: 94.273| Val Acc: 95.143\n",
      "Epoch 100: | Train Loss: 0.09324 | Val Loss: 0.26539 | Train Acc: 94.758| Val Acc: 95.071\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#Define device that will do the training/testing\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#Initialize model\n",
    "model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "#Define loss criterion and adam optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#Initialize accuracy/loss stats\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "#Begin training\n",
    "print(\"Begin training.\")\n",
    "for epoch in tqdm(range(1,EPOCHS+1)):\n",
    "\n",
    "    #initialize the epoch loss/acc\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "\n",
    "    #initialize model training\n",
    "    model.train()\n",
    "    #for each batch in train loader, perform a training loop\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        \n",
    "        #Load batches onto device\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        #zero the gradient of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Run model on X_batch to get y_pred\n",
    "        y_train_pred = model(X_train_batch)\n",
    "\n",
    "        #find the loss using CrossEntropyLoss\n",
    "        train_loss = criterion(y_train_pred,y_train_batch)\n",
    "        #compute accuracy\n",
    "        train_acc = multi_acc(y_train_pred,y_train_batch)\n",
    "\n",
    "        #run the backpropagater tuner on batch\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #add train_loss and train_acc across all batches\n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "\n",
    "      \n",
    "    # VALIDATION , do not use gradient optimization  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "\n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saliency Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a training data of a certain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 0.08330302  1.23542741 -1.47281274  0.78042333 -0.22600995]\n",
      "aft [ 8.33030236e-02  1.23542741e+00 -1.47281274e+00  7.80423329e-01\n",
      " -2.26009949e-01  0.00000000e+00  7.00000000e+00  1.67475200e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.18955577  0.57104083 -0.05912879  0.32964634  0.21362102]\n",
      "aft [-1.89555774e-01  5.71040826e-01 -5.91287895e-02  3.29646336e-01\n",
      "  2.13621018e-01  1.00000000e+00  1.10000000e+01  1.67769600e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.32842306 -0.03065422 -0.80085608 -1.39765748 -0.44264568]\n",
      "aft [-3.28423058e-01 -3.06542243e-02 -8.00856075e-01 -1.39765748e+00\n",
      " -4.42645675e-01  2.00000000e+00  1.00000000e+00  6.73732200e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.59899834 -0.22641683 -0.8796634  -0.23121758 -0.91690324]\n",
      "aft [-5.98998340e-01 -2.26416832e-01 -8.79663397e-01 -2.31217581e-01\n",
      " -9.16903236e-01  3.00000000e+00  1.50000000e+01  3.97809700e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.8116795  -0.79953939 -0.7884131   1.42098654 -1.12047175]\n",
      "aft [-8.11679501e-01 -7.99539391e-01 -7.88413103e-01  1.42098654e+00\n",
      " -1.12047175e+00  4.00000000e+00  9.00000000e+00  1.00258800e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.03812138  0.10823903  0.09791816 -1.13576734 -1.08831313]\n",
      "aft [-3.81213753e-02  1.08239030e-01  9.79181632e-02 -1.13576734e+00\n",
      " -1.08831313e+00  5.00000000e+00  2.00000000e+00  1.13932540e+07]\n",
      "bef [-0.12185964  0.08073783  0.04723216 -0.07309917 -0.87333118]\n",
      "aft [-1.21859635e-01  8.07378315e-02  4.72321616e-02 -7.30991728e-02\n",
      " -8.73331183e-01  6.00000000e+00  5.00000000e+00  6.26652800e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.12534745  0.07725732  0.14164998 -0.19544933 -1.14312897]\n",
      "aft [-1.25347452e-01  7.72573151e-02  1.41649984e-01 -1.95449331e-01\n",
      " -1.14312897e+00  7.00000000e+00  1.30000000e+01  8.90034600e+06]\n",
      "bef [ 0.79765427 -0.46972378  0.29647861  1.60795973 -1.47328702]\n",
      "aft [ 7.97654271e-01 -4.69723775e-01  2.96478610e-01  1.60795973e+00\n",
      " -1.47328702e+00  8.00000000e+00  1.30000000e+01  4.28694500e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 0.2935596  -0.76812825  0.19897345  2.51037422 -1.86712784]\n",
      "aft [ 2.93559602e-01 -7.68128250e-01  1.98973453e-01  2.51037422e+00\n",
      " -1.86712784e+00  9.00000000e+00  3.00000000e+00  6.59198100e+06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-0.49645403 -0.96702043  0.51487412 -1.05049753  0.09753011]\n",
      "aft [-4.96454031e-01 -9.67020428e-01  5.14874116e-01 -1.05049753e+00\n",
      "  9.75301149e-02  1.00000000e+01  7.00000000e+00  1.57876600e+07]\n",
      "bef [ 0.34824706 -0.25229758  0.77133481  0.26016119  1.86415666]\n",
      "aft [ 3.48247061e-01 -2.52297583e-01  7.71334806e-01  2.60161188e-01\n",
      "  1.86415666e+00  1.10000000e+01  5.00000000e+00  1.67667200e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 0.08347426  2.25184652 -2.41087377 -2.0402628  -1.13587044]\n",
      "aft [ 8.34742567e-02  2.25184652e+00 -2.41087377e+00 -2.04026280e+00\n",
      " -1.13587044e+00  1.20000000e+01  1.10000000e+01  1.67584650e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-1.05193841 -0.74484513 -0.2047871   0.06988304 -1.26961428]\n",
      "aft [-1.05193841e+00 -7.44845132e-01 -2.04787099e-01  6.98830393e-02\n",
      " -1.26961428e+00  1.30000000e+01  1.00000000e+00  1.67702730e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 0.4366788   1.16590284 -0.94284366  0.61007726 -0.61481461]\n",
      "aft [ 4.36678803e-01  1.16590284e+00 -9.42843656e-01  6.10077260e-01\n",
      " -6.14814614e-01  1.40000000e+01  3.00000000e+00  1.67169470e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 1.045968   -0.33225498  0.14204668 -2.05279416  0.52437029]\n",
      "aft [ 1.04596800e+00 -3.32254978e-01  1.42046681e-01 -2.05279416e+00\n",
      "  5.24370289e-01  1.50000000e+01  1.50000000e+01  1.67387400e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-1.33190516 -3.34033616 -1.14005482  2.92309501  0.56534286]\n",
      "aft [-1.33190516e+00 -3.34033616e+00 -1.14005482e+00  2.92309501e+00\n",
      "  5.65342860e-01  1.60000000e+01  3.00000000e+00  1.30471730e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 1.44113251  0.07267799 -0.85621606  0.31304795 -0.69622087]\n",
      "aft [ 1.44113251e+00  7.26779930e-02 -8.56216061e-01  3.13047953e-01\n",
      " -6.96220875e-01  1.70000000e+01  5.00000000e+00  1.45246370e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [ 1.14233987 -0.43838522 -1.35263806  4.76788886 -0.66860901]\n",
      "aft [ 1.14233987e+00 -4.38385219e-01 -1.35263806e+00  4.76788886e+00\n",
      " -6.68609007e-01  1.80000000e+01  5.00000000e+00  8.38873600e+06]\n",
      "bef [ 1.36522333 -0.17242349  1.78657395 -4.23765283  1.83251629]\n",
      "aft [ 1.36522333e+00 -1.72423489e-01  1.78657395e+00 -4.23765283e+00\n",
      "  1.83251629e+00  1.90000000e+01  9.00000000e+00  1.43812030e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef [-2.00403701  0.72483358  2.90160164  1.17436398 -2.70977255]\n",
      "aft [-2.00403701e+00  7.24833581e-01  2.90160164e+00  1.17436398e+00\n",
      " -2.70977255e+00  2.00000000e+01  7.00000000e+00  1.22116670e+07]\n",
      "bef [-0.32583115 -1.59119911 -1.03237908  0.59798536  2.23468998]\n",
      "aft [-3.25831149e-01 -1.59119911e+00 -1.03237908e+00  5.97985361e-01\n",
      "  2.23468998e+00  2.10000000e+01  7.00000000e+00  1.08242340e+07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aelsamma\\.conda\\envs\\dlchem2\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from captum.attr import Saliency\n",
    "\n",
    "\n",
    "means_saliency = np.zeros((1,n_features+3))\n",
    "for _class in range(NUM_CLASSES):\n",
    "\n",
    "    Xy_eval = embslabeled_data_proc[embslabeled_data_proc.iloc[:,133] == _class] \n",
    "    X_eval = Xy_eval.iloc[:,0:n_features]\n",
    "    y_eval = Xy_eval.iloc[:,133]\n",
    "\n",
    "    color_y = Xy_eval.iloc[0,134]\n",
    "    marker_y = Xy_eval.iloc[0,135]\n",
    "    ldalabel_y = Xy_eval.iloc[0,133]\n",
    "\n",
    "    eval_dataset = ClassifierDataset(torch.from_numpy(X_eval.values).type(dtype), torch.from_numpy(y_eval.values).long())\n",
    "    eval_loader = DataLoader(dataset=eval_dataset, batch_size=1)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    saliency_maps = np.zeros((1,n_features))\n",
    "    for X_eval_batch, y_eval_batch in eval_loader:\n",
    "\n",
    "        # Choose a data point from your dataset\n",
    "        input_data = torch.tensor(X_eval_batch, requires_grad=True)\n",
    "\n",
    "        #output on the whole batch\n",
    "        output_data = model(input_data)\n",
    "\n",
    "        # Calculate the loss with respect to class 8\n",
    "        loss = criterion(output_data,y_eval_batch)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute the saliency map\n",
    "        saliency = Saliency(model)\n",
    "        saliency_map = saliency.attribute(input_data,target=_class,abs=False)\n",
    "\n",
    "\n",
    "        saliency_maps = np.vstack((saliency_maps,saliency_map))\n",
    "\n",
    "    saliency_maps = np.delete(saliency_maps,0,0)\n",
    "\n",
    "    mean_saliency_forthis_class = np.mean(saliency_maps,axis=0)\n",
    "    mean_saliency_forthis_class = np.hstack((mean_saliency_forthis_class,ldalabel_y,color_y,marker_y))\n",
    "    means_saliency = np.vstack((means_saliency,mean_saliency_forthis_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/saliency/model1qm9-0-10000Oembs/means_saliency_per_class.csv',means_saliency,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_saliency = np.delete(means_saliency,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -3.77282575e-01 -2.26902674e-01 -1.64474916e-02\n",
      "  -4.77162838e-01 -2.24862826e-01 -2.93427389e-01 -1.28744838e-01\n",
      "  -3.31091049e-02 -7.05672913e-01  1.52296594e-01  1.70658570e-01\n",
      "  -3.51679702e-01  5.50290325e-01 -5.57675293e-01 -1.45237195e-01\n",
      "  -1.70483873e-01  2.80714111e-01 -3.96362000e-01  1.28223818e-01\n",
      "  -5.13152361e-02]\n",
      " [-3.77282575e-01  1.00000000e+00  6.66963807e-01 -1.46253106e-01\n",
      "   7.43237981e-01  3.22404140e-01  3.51353402e-01 -5.11653340e-01\n",
      "  -5.28086280e-01  4.42458262e-01 -5.40389519e-01  7.53749826e-01\n",
      "   3.21213943e-01 -1.51506486e-03  5.38028871e-01 -3.52287558e-01\n",
      "   2.57486721e-02 -6.44062642e-01  3.70055392e-01 -2.68593330e-01\n",
      "  -1.64395828e-01]\n",
      " [-2.26902674e-01  6.66963807e-01  1.00000000e+00  6.26878963e-01\n",
      "   5.14789514e-01  6.47763559e-01  6.04594432e-01  1.00767232e-01\n",
      "   2.04318803e-01  9.40344539e-02 -8.67300867e-01  5.27508113e-01\n",
      "   8.09718307e-01  2.82208360e-01 -1.98046386e-01  1.96708244e-01\n",
      "   1.65385403e-01  1.55616856e-02 -4.11874044e-01  1.03825892e-01\n",
      "  -1.66824314e-01]\n",
      " [-1.64474916e-02 -1.46253106e-01  6.26878963e-01  1.00000000e+00\n",
      "  -1.46405651e-01  4.29912129e-01  3.47043611e-01  6.30321744e-01\n",
      "   7.95808127e-01 -2.28122124e-01 -5.08204246e-01 -1.72994283e-01\n",
      "   7.54356767e-01  2.48770096e-01 -7.70934128e-01  7.45998898e-01\n",
      "   1.59986467e-01  6.82978582e-01 -8.92678161e-01  3.26442984e-01\n",
      "   1.01549853e-01]\n",
      " [-4.77162838e-01  7.43237981e-01  5.14789514e-01 -1.46405651e-01\n",
      "   1.00000000e+00  7.47853508e-01  8.02362384e-01 -7.29089393e-02\n",
      "  -1.74563753e-01  4.15958503e-01 -6.96803107e-01  5.56942176e-01\n",
      "   4.31271960e-01 -2.32710050e-03  4.49769463e-01 -5.73804468e-01\n",
      "   9.31175198e-02 -6.03630537e-01  3.59976950e-01  2.79172449e-01\n",
      "  -7.09133446e-01]\n",
      " [-2.24862826e-01  3.22404140e-01  6.47763559e-01  4.29912129e-01\n",
      "   7.47853508e-01  1.00000000e+00  9.93389189e-01  4.93315228e-01\n",
      "   4.71181429e-01 -1.15515889e-03 -9.04613342e-01  3.33311631e-01\n",
      "   7.16782376e-01  3.07582311e-01 -2.14160456e-01 -2.03146117e-01\n",
      "   2.28616221e-01 -6.76526651e-04 -3.03284238e-01  6.56626671e-01\n",
      "  -7.96607681e-01]\n",
      " [-2.93427389e-01  3.51353402e-01  6.04594432e-01  3.47043611e-01\n",
      "   8.02362384e-01  9.93389189e-01  1.00000000e+00  4.53685429e-01\n",
      "   4.12880849e-01  8.18889044e-02 -8.72320960e-01  3.19129014e-01\n",
      "   6.91144988e-01  2.30798832e-01 -1.18966568e-01 -2.59786080e-01\n",
      "   2.01951002e-01 -8.84479085e-02 -1.97984846e-01  6.54032289e-01\n",
      "  -8.18309914e-01]\n",
      " [-1.28744838e-01 -5.11653340e-01  1.00767232e-01  6.30321744e-01\n",
      "  -7.29089393e-02  4.93315228e-01  4.53685429e-01  1.00000000e+00\n",
      "   9.61621654e-01 -4.24591241e-01 -3.43085148e-01 -3.43242868e-01\n",
      "   3.29368903e-01  3.30190990e-01 -5.32808442e-01  3.51728843e-01\n",
      "   5.43165546e-01  7.62052773e-01 -6.36939015e-01  4.31896952e-01\n",
      "  -2.99147449e-01]\n",
      " [-3.31091049e-02 -5.28086280e-01  2.04318803e-01  7.95808127e-01\n",
      "  -1.74563753e-01  4.71181429e-01  4.12880849e-01  9.61621654e-01\n",
      "   1.00000000e+00 -4.08797306e-01 -3.52909403e-01 -3.95807056e-01\n",
      "   4.68813085e-01  2.97951526e-01 -7.15357581e-01  5.28425196e-01\n",
      "   3.82082318e-01  8.16282729e-01 -7.90912886e-01  4.92981407e-01\n",
      "  -1.79447255e-01]\n",
      " [-7.05672913e-01  4.42458262e-01  9.40344539e-02 -2.28122124e-01\n",
      "   4.15958503e-01 -1.15515889e-03  8.18889044e-02 -4.24591241e-01\n",
      "  -4.08797306e-01  1.00000000e+00  1.13258177e-01 -2.20752850e-01\n",
      "   3.22388965e-01 -8.85677483e-01  5.44611488e-01  3.75674749e-02\n",
      "  -5.50428252e-01 -7.17908390e-01  6.12986126e-01  3.89250762e-02\n",
      "   1.57587691e-01]\n",
      " [ 1.52296594e-01 -5.40389519e-01 -8.67300867e-01 -5.08204246e-01\n",
      "  -6.96803107e-01 -9.04613342e-01 -8.72320960e-01 -3.43085148e-01\n",
      "  -3.52909403e-01  1.13258177e-01  1.00000000e+00 -6.00459401e-01\n",
      "  -7.08736630e-01 -5.00073218e-01  2.01441513e-01  1.31501556e-01\n",
      "  -3.63550688e-01 -5.05308357e-02  3.94146501e-01 -3.50013738e-01\n",
      "   6.09462371e-01]\n",
      " [ 1.70658570e-01  7.53749826e-01  5.27508113e-01 -1.72994283e-01\n",
      "   5.56942176e-01  3.33311631e-01  3.19129014e-01 -3.43242868e-01\n",
      "  -3.95807056e-01 -2.20752850e-01 -6.00459401e-01  1.00000000e+00\n",
      "   3.82379392e-03  6.07885287e-01  2.67955318e-01 -5.91442867e-01\n",
      "   3.35519009e-01 -3.17059588e-01  9.54608829e-02 -2.57564699e-01\n",
      "  -4.01126812e-01]\n",
      " [-3.51679702e-01  3.21213943e-01  8.09718307e-01  7.54356767e-01\n",
      "   4.31271960e-01  7.16782376e-01  6.91144988e-01  3.29368903e-01\n",
      "   4.68813085e-01  3.22388965e-01 -7.08736630e-01  3.82379392e-03\n",
      "   1.00000000e+00 -9.57236184e-02 -3.88277483e-01  4.12151229e-01\n",
      "  -1.45576812e-01  6.22455789e-02 -4.58887351e-01  5.29951625e-01\n",
      "  -1.92597562e-01]\n",
      " [ 5.50290325e-01 -1.51506486e-03  2.82208360e-01  2.48770096e-01\n",
      "  -2.32710050e-03  3.07582311e-01  2.30798832e-01  3.30190990e-01\n",
      "   2.97951526e-01 -8.85677483e-01 -5.00073218e-01  6.07885287e-01\n",
      "  -9.57236184e-02  1.00000000e+00 -3.84033236e-01 -2.29017563e-01\n",
      "   6.50185925e-01  4.98454156e-01 -5.45042763e-01 -4.60975296e-02\n",
      "  -3.78346337e-01]\n",
      " [-5.57675293e-01  5.38028871e-01 -1.98046386e-01 -7.70934128e-01\n",
      "   4.49769463e-01 -2.14160456e-01 -1.18966568e-01 -5.32808442e-01\n",
      "  -7.15357581e-01  5.44611488e-01  2.01441513e-01  2.67955318e-01\n",
      "  -3.88277483e-01 -3.84033236e-01  1.00000000e+00 -5.35312199e-01\n",
      "   8.03206184e-02 -7.30494255e-01  9.28114225e-01 -5.17184831e-01\n",
      "  -2.01684362e-03]\n",
      " [-1.45237195e-01 -3.52287558e-01  1.96708244e-01  7.45998898e-01\n",
      "  -5.73804468e-01 -2.03146117e-01 -2.59786080e-01  3.51728843e-01\n",
      "   5.28425196e-01  3.75674749e-02  1.31501556e-01 -5.91442867e-01\n",
      "   4.12151229e-01 -2.29017563e-01 -5.35312199e-01  1.00000000e+00\n",
      "  -7.50239276e-02  6.08023185e-01 -5.92481342e-01 -5.17988120e-02\n",
      "   6.87158425e-01]\n",
      " [-1.70483873e-01  2.57486721e-02  1.65385403e-01  1.59986467e-01\n",
      "   9.31175198e-02  2.28616221e-01  2.01951002e-01  5.43165546e-01\n",
      "   3.82082318e-01 -5.50428252e-01 -3.63550688e-01  3.35519009e-01\n",
      "  -1.45576812e-01  6.50185925e-01  8.03206184e-02 -7.50239276e-02\n",
      "   1.00000000e+00  4.98462343e-01 -2.29864868e-01 -3.57143728e-01\n",
      "  -1.92879471e-01]\n",
      " [ 2.80714111e-01 -6.44062642e-01  1.55616856e-02  6.82978582e-01\n",
      "  -6.03630537e-01 -6.76526651e-04 -8.84479085e-02  7.62052773e-01\n",
      "   8.16282729e-01 -7.17908390e-01 -5.05308357e-02 -3.17059588e-01\n",
      "   6.22455789e-02  4.98454156e-01 -7.30494255e-01  6.08023185e-01\n",
      "   4.98462343e-01  1.00000000e+00 -8.39626558e-01  3.66714974e-02\n",
      "   1.98723818e-01]\n",
      " [-3.96362000e-01  3.70055392e-01 -4.11874044e-01 -8.92678161e-01\n",
      "   3.59976950e-01 -3.03284238e-01 -1.97984846e-01 -6.36939015e-01\n",
      "  -7.90912886e-01  6.12986126e-01  3.94146501e-01  9.54608829e-02\n",
      "  -4.58887351e-01 -5.45042763e-01  9.28114225e-01 -5.92481342e-01\n",
      "  -2.29864868e-01 -8.39626558e-01  1.00000000e+00 -3.18451020e-01\n",
      "  -2.93522353e-02]\n",
      " [ 1.28223818e-01 -2.68593330e-01  1.03825892e-01  3.26442984e-01\n",
      "   2.79172449e-01  6.56626671e-01  6.54032289e-01  4.31896952e-01\n",
      "   4.92981407e-01  3.89250762e-02 -3.50013738e-01 -2.57564699e-01\n",
      "   5.29951625e-01 -4.60975296e-02 -5.17184831e-01 -5.17988120e-02\n",
      "  -3.57143728e-01  3.66714974e-02 -3.18451020e-01  1.00000000e+00\n",
      "  -6.31528820e-01]\n",
      " [-5.13152361e-02 -1.64395828e-01 -1.66824314e-01  1.01549853e-01\n",
      "  -7.09133446e-01 -7.96607681e-01 -8.18309914e-01 -2.99147449e-01\n",
      "  -1.79447255e-01  1.57587691e-01  6.09462371e-01 -4.01126812e-01\n",
      "  -1.92597562e-01 -3.78346337e-01 -2.01684362e-03  6.87158425e-01\n",
      "  -1.92879471e-01  1.98723818e-01 -2.93522353e-02 -6.31528820e-01\n",
      "   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the inner product matrix\n",
    "inner_product_matrix = np.dot(means_saliency[:,0:5], means_saliency[:,0:5].T)\n",
    "\n",
    "# Calculate the magnitudes of the rows in the original matrix\n",
    "row_magnitudes = np.linalg.norm(means_saliency[:,0:5], axis=1)\n",
    "\n",
    "\n",
    "# Normalize the inner product matrix\n",
    "normalized_matrix = inner_product_matrix / (row_magnitudes[:, np.newaxis] * row_magnitudes)\n",
    "\n",
    "print(normalized_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_matrix = np.column_stack((normalized_matrix,means_saliency[:,-3:]))\n",
    "\n",
    "np.savetxt('../saliency_innerproduct.csv',labeled_matrix,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlchem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
