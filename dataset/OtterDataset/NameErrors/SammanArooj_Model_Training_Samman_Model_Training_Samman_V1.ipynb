{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SammanArooj/Model_Training_Samman/blob/main/Model_Training_Samman_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**Table of Content**</p>"
      ],
      "metadata": {
        "id": "AfpLfvDY9h56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Import Library\n",
        "2.   Import Dataset\n",
        "3.   Data Preprocessing\n",
        "4.   Model Training\n",
        "\n"
      ],
      "metadata": {
        "id": "iXHrYhxO9reC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HLNoZfkBUGc"
      },
      "source": [
        "\n",
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**1-Import library**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkuJaCW9bT6u"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbN0QbsScujH",
        "outputId": "2150c56e-4798-4259-ae47-4babc4d68575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "slixlMlwdJji",
        "outputId": "e3397ad6-f990-4be9-ec06-9588b3125e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpC21eawvwSY"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**2-Import Dataset**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R60NmwbGDNp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5abf1d4-c188-403d-bfd4-029de0599648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfFYn2f_b5u9",
        "outputId": "f93457d6-3eca-4e32-ee59-3fb4eb63dd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing the intents..........\n"
          ]
        }
      ],
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!',\"'\",]\n",
        "print(\"Processing the intents..........\")\n",
        "path_to_save_model =\"/content/gdrive/MyDrive/TrainedModel/\"\n",
        "with open('/content/gdrive/MyDrive/Dataset/intentsNew.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "45pcboNfFTot"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yl5GZnxZnI3"
      },
      "source": [
        "\n",
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**3-Data Preprocessing**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICA7KJTRD768",
        "outputId": "586598c6-5beb-4ea9-cdd3-102b77cee9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looping through intents to convert them to words, classes, documents and ignore_words.........\n"
          ]
        }
      ],
      "source": [
        "print(\"Looping through intents to convert them to words, classes, documents and ignore_words.........\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        #add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-azsX0L8c7hX",
        "outputId": "c6e7a2e4-d6cd-4474-dbb8-f8db4038b799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "296 documents\n",
            "115 classes [' ADMISSION OF FOREIGN/DUAL NATIONAL', ' Additional Specialization Certificate', ' Admission Status', ' Alumni Association ', ' Citizen complaint portal ', ' Club and Societies ', ' Courses Catalogue', ' Degree/Transcript Verification ', ' Digiskills ', ' Disabled Student Fee structure(Local)', ' Disabled Student Fee structure(Overseas)', ' Exam Superintendent Registration ', ' FACULTY OF SCIENCE & TECHNOLOGY Programs', ' Faculty of Arts Programs', ' Faculty of Education Programs', ' Faculty of Management Programs', ' Grading Scheme', ' How to Apply Online', ' JavaScript and CSS', ' Life at VU', ' MOU and Collaboration', ' New Registration', ' ORIC ', ' PROFESSIONAL COURSES DEVELOPMENT', ' PROTECTION AGAINST HARASSMENT  ', ' Polymorphism', ' Prospectus', ' RIGHT OF ACCESS TO INFORMATION ', ' Recent Advertisements of VU', ' Schedule of other Charges', ' Short Certificate Course', ' Student Startup', ' Study Scheme', ' Suitability of  VU', ' Tender', ' Vendor Registration ', ' XML and JSON', ' Zero Semester', 'Academic Calendar', 'Admission Schedule', 'Admission through Course Exemption/Transfer of Credit Hour(S)', 'Admission through Entry Option', 'Board of Governors', 'Bootstrap', 'Campus changed', 'Chancellor', \"Chancellor's Message\", \"Chancellor's Mission \", \"Chancellor's Vision \", 'Cisco C++ course', 'Cisco academy Password', 'Credit', 'Data types ', 'Dev C++', 'Educator Scholarships', 'Faculty Members', 'Fee Structure', 'Fee Structure(Local)', 'Fee Structure(Overseas)', 'File handling in C/C++ ', 'IF-ELSE statement ', 'Introduction of VU ', 'Introduction to  C/ C+ Programming', 'Lecture Schedule', 'Local and Global variables', 'Loops ', 'NBP Student Loan Scheme', 'OOP', 'Organizational Structure', 'PAKISTAN BAIT-UL-MAL STIPENDS', 'PUNJAB EDUCATIONAL ENDOWMENT FUND (PEEF) ', 'PUNJAB WORKERS WELFARE BOARD', 'Paper Re-checking', 'QEC ', 'Result', 'Signed and unsigned data types ', 'Student Handbook', 'Syeda Mubarik Begum Scholarships', 'Usefull Links ', 'VSpace ', 'VU Bookshop', 'VU Scholarships', 'about', 'add or drop courses', 'admission', 'appear for exam', 'campus change', 'comparable courses', 'complaint', 'concern certificate', 'contact admission department', 'continue education afterc ompleting degree', 'course selection', 'course syllabus', 'create account', 'duplicateid', 'expensive education', 'freeze account', 'functions', 'functions type', 'functon decleartion', 'greetings', 'help', 'how to study', 'institution', 'lecture handouts and dvd', 'migration certificate', 'operate computer', 'recognized degree', 'standards of education', 'student status (Overseas to Pakistan and Vice Versa)', 'student status updation', 'thanks', 'uniform education', 'where to study']\n",
            "353 unique lemmatized words ['&', \"''\", \"'s\", '(', ')', '10', '16', '``', 'a', 'able', 'about', 'academic', 'academy', 'account', 'acdmey', 'act', 'add', 'additional', 'admission', 'admitted', 'advertisement', 'after', 'against', 'all', 'alumnus', 'an', 'and', 'any', 'appear', 'appending', 'apply', 'are', 'art', 'association', 'at', 'b', 'bait', 'bait-ul-mal', 'be', 'begum', 'between', 'board', 'bookshop', 'bootstrap', 'building', 'but', 'by', 'c', 'c++', 'c/c++', 'calendar', 'calender', 'campus', 'can', 'candidate', 'card', 'catalogue', 'cell', 'certificate', 'certificate/no', 'certifucate', 'chancellor', 'change', 'changed', 'charge', 'check', 'choose', 'cisco', 'club', 'coding', 'collaboration', 'comparable', 'compared', 'complaint', 'completing', 'computer', 'concept', 'concern', 'contact', 'content', 'contents/syllabus', 'continue', 'could', 'country', 'course', 'create', 'credit', 'cs', 'data', 'declare', 'decleration', 'degree', 'department', 'depth', 'dev', 'develop', 'development', 'difference', 'different', 'digiskills', 'diifference', 'disabled', 'discipline', 'do', 'do-while', 'doe', 'download', 'drop', 'duplicate', 'dvd', 'dy', 'education', 'educator', 'elearning', 'endowment', 'enhancement', 'entry', 'evening', 'exam', 'exemption', 'existed', 'expensive', 'explain', 'faculty', 'fee', 'file', 'for', 'foreign/dual', 'form', 'freeze', 'freeze/unfreeze', 'from', 'function', 'fund', 'general', 'get', 'give', 'global', 'good', 'government', 'governor', 'grading', 'guidlines', 'hand', 'handbook', 'handling', 'handout', 'harassment', 'have', 'hello', 'help', 'helpful', 'hey', 'hi', 'holding', 'hour', 'how', 'i', 'id', 'if', 'important', 'in', 'institution', 'international', 'introduction', 'is', 'it', 'javascript', 'json', 'k', 'karien', 'keise', 'ki', 'know', 'language', 'larger', 'latest', 'learn', 'lectuer', 'lecture', 'life', 'link', 'list', 'liye', 'loan', 'local', 'login', 'loop', 'mal', 'management', 'may', 'me', 'mean', 'member', 'message', 'migration', 'mission', 'mode', 'morning', 'mou', 'mubarik', 'must', 'my', \"n't\", 'national', 'nbp', 'need', 'new', 'noc', 'number', 'objection', 'obtain', 'obtaining', 'of', 'offered', 'one', 'online', 'oop', 'open', 'opening', 'operate', 'option', 'or', 'organizational', 'oric', 'other', 'over', 'overseas', 'pakistan', 'pakistani', 'paper', 'password', 'peef', 'please', 'policy', 'polymorphism', 'possible', 'private', 'problem', 'procedure', 'professional', 'program', 'programming', 'prospectus', 'protection', 'province', 'punjab', 'qec', 'quality', 'raise', 're-checking', 'really', 'recent', 'recheching', 'recognized', 'register', 'registered', 'registration', 'reset', 'residing', 'review', 'rti', 's', 'save', 'schedule', 'scheme', 'scholarship', 'science', 'see', 'select', 'selection', 'selection/to', 'semester', 'service', 'short', 'should', 'signed', 'sir', 'society', 'specialization', 'stand', 'standard', 'startup', 'statement', 'status', 'stipend', 'structure', 'student', 'study', 'study/degree', 'subject', 'suitability', 'suitable', 'superintendent', 'support', 'syeda', 'syllabus', 'system', 'technology', 'tender', 'thank', 'thanks', 'that', 'the', 'there', 'this', 'through', 'to', 'transcript', 'transfer', 'type', 'ul', 'unfreeze', 'uniform', 'university', 'unsigned', 'use', 'variable', 'various', 'vendor', 'verification', 'verify', 'version', 'very', 'view', 'virtual', 'vision', 'vspace', 'vu', 'vup', 'want', 'we', 'website', 'welfare', 'what', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'window', 'with', 'within', 'woman', 'work', 'worker', 'workplace', 'xml', 'year', 'you', 'zero']\n"
          ]
        }
      ],
      "source": [
        "# lemmaztize and lower each word and remove duplicates\n",
        "nltk.download('wordnet')\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('texts.pkl','wb'))\n",
        "pickle.dump(classes,open('labels.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZDzp9HIMs30",
        "outputId": "c1ca8e22-7a3f-40be-eec1-207ead63741f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the Data for Our Model......\n",
            "Creating an List (Empty) for Output.....\n",
            "Creating Training Set, Bag of Words for our Model.....\n",
            "Training data created\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating the Data for Our Model......\")\n",
        "training = []\n",
        "output = []\n",
        "print(\"Creating an List (Empty) for Output.....\")\n",
        "output_empty = [0] * len(classes)\n",
        "print(\"Creating Training Set, Bag of Words for our Model.....\")\n",
        "\n",
        "for document in documents:\n",
        "  #initialize our bag of words\n",
        "  bag = []\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = document[0]\n",
        "  #lemmatize each word - create base word, in attempt to represent related words\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  #create our bag of words array with 1, if word match found in current pattern\n",
        "  for word in words:\n",
        "    bag.append(1) if word in pattern_words else bag.append(0)\n",
        "\n",
        "    #output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "#shuffle our features into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "#create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiA5mfO_bf5G"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4-Model Training**</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-gDV4j8l0L0"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.1-Neural Netowrk**</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em4aCw6ImU3R"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.1.1 Model Saving**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwFda9VsmgQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "d86fd6e5-60c9-4c3b-f2e4-2a806c2f5e8f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4591df58651b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chat_model_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.save('chat_model_1', hist)\n",
        "print(\"model created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvFngP6nzWA"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2- LSTM**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nVA9smHqt4v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Flatten,Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLxnK1_RLO4a"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.1 - Model Architecture 1**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcFSuMKmq2C_",
        "outputId": "104597a5-b89a-4ddf-ce05-e163451ee196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 353, 64)           16896     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 353, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 353, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 353, 32)           12416     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 353, 32)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 353, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16)                64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 115)               1955      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34851 (136.14 KB)\n",
            "Trainable params: 34627 (135.26 KB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "817/817 [==============================] - 40s 39ms/step - loss: 4.1991 - accuracy: 0.0807\n",
            "Epoch 2/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 2.9511 - accuracy: 0.2264\n",
            "Epoch 3/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 2.3028 - accuracy: 0.3340\n",
            "Epoch 4/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 1.7836 - accuracy: 0.4390\n",
            "Epoch 5/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 1.4148 - accuracy: 0.5299\n",
            "Epoch 6/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 1.1908 - accuracy: 0.5943\n",
            "Epoch 7/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 1.0093 - accuracy: 0.6548\n",
            "Epoch 8/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.8404 - accuracy: 0.7064\n",
            "Epoch 9/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.9880 - accuracy: 0.7006\n",
            "Epoch 10/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.9475 - accuracy: 0.6677\n",
            "Epoch 11/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.6111 - accuracy: 0.7819\n",
            "Epoch 12/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.4756 - accuracy: 0.8275\n",
            "Epoch 13/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.4387 - accuracy: 0.8466\n",
            "Epoch 14/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 1.1574 - accuracy: 0.6814\n",
            "Epoch 15/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.8564 - accuracy: 0.6905\n",
            "Epoch 16/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.5308 - accuracy: 0.8101\n",
            "Epoch 17/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.5228 - accuracy: 0.8250\n",
            "Epoch 18/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.3640 - accuracy: 0.8727\n",
            "Epoch 19/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2913 - accuracy: 0.8931\n",
            "Epoch 20/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.2854 - accuracy: 0.8984\n",
            "Epoch 21/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2770 - accuracy: 0.9043\n",
            "Epoch 22/200\n",
            "817/817 [==============================] - 33s 41ms/step - loss: 0.3531 - accuracy: 0.8916\n",
            "Epoch 23/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2047 - accuracy: 0.9273\n",
            "Epoch 24/200\n",
            "817/817 [==============================] - 38s 47ms/step - loss: 0.2066 - accuracy: 0.9286\n",
            "Epoch 25/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2422 - accuracy: 0.9217\n",
            "Epoch 26/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2038 - accuracy: 0.9323\n",
            "Epoch 27/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1569 - accuracy: 0.9439\n",
            "Epoch 28/200\n",
            "817/817 [==============================] - 31s 39ms/step - loss: 0.4280 - accuracy: 0.8917\n",
            "Epoch 29/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.4157 - accuracy: 0.8729\n",
            "Epoch 30/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1939 - accuracy: 0.9347\n",
            "Epoch 31/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.1527 - accuracy: 0.9472\n",
            "Epoch 32/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1415 - accuracy: 0.9512\n",
            "Epoch 33/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1617 - accuracy: 0.9461\n",
            "Epoch 34/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1221 - accuracy: 0.9574\n",
            "Epoch 35/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.2390 - accuracy: 0.9309\n",
            "Epoch 36/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1138 - accuracy: 0.9605\n",
            "Epoch 37/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 1.5711 - accuracy: 0.6653\n",
            "Epoch 38/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 1.5597 - accuracy: 0.5132\n",
            "Epoch 39/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.9365 - accuracy: 0.6705\n",
            "Epoch 40/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.6519 - accuracy: 0.7669\n",
            "Epoch 41/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.5312 - accuracy: 0.8139\n",
            "Epoch 42/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.4138 - accuracy: 0.8573\n",
            "Epoch 43/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.4153 - accuracy: 0.8622\n",
            "Epoch 44/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 4.2248 - accuracy: 0.1661\n",
            "Epoch 45/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 3.8225 - accuracy: 0.1133\n",
            "Epoch 46/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 3.2074 - accuracy: 0.1776\n",
            "Epoch 47/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 2.8042 - accuracy: 0.2364\n",
            "Epoch 48/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 2.5139 - accuracy: 0.2864\n",
            "Epoch 49/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 2.2956 - accuracy: 0.3318\n",
            "Epoch 50/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 2.0950 - accuracy: 0.3761\n",
            "Epoch 51/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 1.8350 - accuracy: 0.4396\n",
            "Epoch 52/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 1.6487 - accuracy: 0.4861\n",
            "Epoch 53/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 1.4273 - accuracy: 0.5430\n",
            "Epoch 54/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 1.2250 - accuracy: 0.5974\n",
            "Epoch 55/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 1.0695 - accuracy: 0.6421\n",
            "Epoch 56/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.9267 - accuracy: 0.6886\n",
            "Epoch 57/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.8589 - accuracy: 0.7153\n",
            "Epoch 58/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.7340 - accuracy: 0.7539\n",
            "Epoch 59/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.6496 - accuracy: 0.7847\n",
            "Epoch 60/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.6206 - accuracy: 0.7970\n",
            "Epoch 61/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.5788 - accuracy: 0.8120\n",
            "Epoch 62/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.5315 - accuracy: 0.8316\n",
            "Epoch 63/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.4469 - accuracy: 0.8513\n",
            "Epoch 64/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.4233 - accuracy: 0.8603\n",
            "Epoch 65/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.4103 - accuracy: 0.8639\n",
            "Epoch 66/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.3961 - accuracy: 0.8718\n",
            "Epoch 67/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.3259 - accuracy: 0.8889\n",
            "Epoch 68/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.3770 - accuracy: 0.8827\n",
            "Epoch 69/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.2939 - accuracy: 0.9002\n",
            "Epoch 70/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.3054 - accuracy: 0.8997\n",
            "Epoch 71/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2893 - accuracy: 0.9038\n",
            "Epoch 72/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2578 - accuracy: 0.9124\n",
            "Epoch 73/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.2490 - accuracy: 0.9147\n",
            "Epoch 74/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2903 - accuracy: 0.9062\n",
            "Epoch 75/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.3480 - accuracy: 0.8977\n",
            "Epoch 76/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2561 - accuracy: 0.9137\n",
            "Epoch 77/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2073 - accuracy: 0.9285\n",
            "Epoch 78/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.2927 - accuracy: 0.9040\n",
            "Epoch 79/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2110 - accuracy: 0.9267\n",
            "Epoch 80/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2013 - accuracy: 0.9302\n",
            "Epoch 81/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2145 - accuracy: 0.9263\n",
            "Epoch 82/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1869 - accuracy: 0.9341\n",
            "Epoch 83/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1889 - accuracy: 0.9340\n",
            "Epoch 84/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1943 - accuracy: 0.9323\n",
            "Epoch 85/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1823 - accuracy: 0.9371\n",
            "Epoch 86/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1716 - accuracy: 0.9389\n",
            "Epoch 87/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1788 - accuracy: 0.9374\n",
            "Epoch 88/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1862 - accuracy: 0.9359\n",
            "Epoch 89/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1673 - accuracy: 0.9406\n",
            "Epoch 90/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2214 - accuracy: 0.9302\n",
            "Epoch 91/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.1750 - accuracy: 0.9395\n",
            "Epoch 92/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1734 - accuracy: 0.9395\n",
            "Epoch 93/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.3201 - accuracy: 0.9125\n",
            "Epoch 94/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1985 - accuracy: 0.9306\n",
            "Epoch 95/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1571 - accuracy: 0.9441\n",
            "Epoch 96/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1454 - accuracy: 0.9490\n",
            "Epoch 97/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1455 - accuracy: 0.9481\n",
            "Epoch 98/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1722 - accuracy: 0.9406\n",
            "Epoch 99/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.1636 - accuracy: 0.9430\n",
            "Epoch 100/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.1508 - accuracy: 0.9466\n",
            "Epoch 101/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.1421 - accuracy: 0.9498\n",
            "Epoch 102/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.2069 - accuracy: 0.9362\n",
            "Epoch 103/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1503 - accuracy: 0.9468\n",
            "Epoch 104/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1471 - accuracy: 0.9472\n",
            "Epoch 105/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.5078 - accuracy: 0.8825\n",
            "Epoch 106/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.1906 - accuracy: 0.9336\n",
            "Epoch 107/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1520 - accuracy: 0.9463\n",
            "Epoch 108/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1586 - accuracy: 0.9446\n",
            "Epoch 109/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1446 - accuracy: 0.9488\n",
            "Epoch 110/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1391 - accuracy: 0.9501\n",
            "Epoch 111/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1400 - accuracy: 0.9499\n",
            "Epoch 112/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1307 - accuracy: 0.9537\n",
            "Epoch 113/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1384 - accuracy: 0.9503\n",
            "Epoch 114/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1387 - accuracy: 0.9505\n",
            "Epoch 115/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1250 - accuracy: 0.9554\n",
            "Epoch 116/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1272 - accuracy: 0.9546\n",
            "Epoch 117/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.2487 - accuracy: 0.9307\n",
            "Epoch 118/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1378 - accuracy: 0.9508\n",
            "Epoch 119/200\n",
            "817/817 [==============================] - 31s 39ms/step - loss: 0.1319 - accuracy: 0.9525\n",
            "Epoch 120/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1221 - accuracy: 0.9556\n",
            "Epoch 121/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1226 - accuracy: 0.9554\n",
            "Epoch 122/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.1225 - accuracy: 0.9554\n",
            "Epoch 123/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1294 - accuracy: 0.9539\n",
            "Epoch 124/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1218 - accuracy: 0.9560\n",
            "Epoch 125/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.2437 - accuracy: 0.9280\n",
            "Epoch 126/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.2039 - accuracy: 0.9404\n",
            "Epoch 127/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1179 - accuracy: 0.9578\n",
            "Epoch 128/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1163 - accuracy: 0.9588\n",
            "Epoch 129/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1598 - accuracy: 0.9463\n",
            "Epoch 130/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1507 - accuracy: 0.9494\n",
            "Epoch 131/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1169 - accuracy: 0.9575\n",
            "Epoch 132/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1111 - accuracy: 0.9600\n",
            "Epoch 133/200\n",
            "817/817 [==============================] - 31s 37ms/step - loss: 0.1067 - accuracy: 0.9615\n",
            "Epoch 134/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1052 - accuracy: 0.9611\n",
            "Epoch 135/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1286 - accuracy: 0.9561\n",
            "Epoch 136/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1399 - accuracy: 0.9534\n",
            "Epoch 137/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1085 - accuracy: 0.9609\n",
            "Epoch 138/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.1047 - accuracy: 0.9623\n",
            "Epoch 139/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1469 - accuracy: 0.9512\n",
            "Epoch 140/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.1059 - accuracy: 0.9615\n",
            "Epoch 141/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1071 - accuracy: 0.9618\n",
            "Epoch 142/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1070 - accuracy: 0.9610\n",
            "Epoch 143/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1018 - accuracy: 0.9630\n",
            "Epoch 144/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1017 - accuracy: 0.9626\n",
            "Epoch 145/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1038 - accuracy: 0.9621\n",
            "Epoch 146/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1062 - accuracy: 0.9621\n",
            "Epoch 147/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1322 - accuracy: 0.9551\n",
            "Epoch 148/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1020 - accuracy: 0.9626\n",
            "Epoch 149/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1044 - accuracy: 0.9630\n",
            "Epoch 150/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1018 - accuracy: 0.9629\n",
            "Epoch 151/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1014 - accuracy: 0.9631\n",
            "Epoch 152/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1007 - accuracy: 0.9637\n",
            "Epoch 153/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0997 - accuracy: 0.9639\n",
            "Epoch 154/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0887 - accuracy: 0.9678\n",
            "Epoch 155/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1113 - accuracy: 0.9604\n",
            "Epoch 156/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.2027 - accuracy: 0.9401\n",
            "Epoch 157/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1024 - accuracy: 0.9629\n",
            "Epoch 158/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0983 - accuracy: 0.9638\n",
            "Epoch 159/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.1013 - accuracy: 0.9640\n",
            "Epoch 160/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0874 - accuracy: 0.9676\n",
            "Epoch 161/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0904 - accuracy: 0.9665\n",
            "Epoch 162/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0909 - accuracy: 0.9668\n",
            "Epoch 163/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1134 - accuracy: 0.9604\n",
            "Epoch 164/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0894 - accuracy: 0.9664\n",
            "Epoch 165/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.6506 - accuracy: 0.8543\n",
            "Epoch 166/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1584 - accuracy: 0.9439\n",
            "Epoch 167/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1364 - accuracy: 0.9523\n",
            "Epoch 168/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1090 - accuracy: 0.9610\n",
            "Epoch 169/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1114 - accuracy: 0.9600\n",
            "Epoch 170/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.1074 - accuracy: 0.9618\n",
            "Epoch 171/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.1537 - accuracy: 0.9521\n",
            "Epoch 172/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.0976 - accuracy: 0.9645\n",
            "Epoch 173/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0935 - accuracy: 0.9664\n",
            "Epoch 174/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0930 - accuracy: 0.9661\n",
            "Epoch 175/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0907 - accuracy: 0.9672\n",
            "Epoch 176/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0934 - accuracy: 0.9665\n",
            "Epoch 177/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0947 - accuracy: 0.9659\n",
            "Epoch 178/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0880 - accuracy: 0.9676\n",
            "Epoch 179/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0943 - accuracy: 0.9664\n",
            "Epoch 180/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0837 - accuracy: 0.9693\n",
            "Epoch 181/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0835 - accuracy: 0.9696\n",
            "Epoch 182/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0930 - accuracy: 0.9665\n",
            "Epoch 183/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.0827 - accuracy: 0.9694\n",
            "Epoch 184/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0854 - accuracy: 0.9696\n",
            "Epoch 185/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0904 - accuracy: 0.9675\n",
            "Epoch 186/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0808 - accuracy: 0.9701\n",
            "Epoch 187/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0787 - accuracy: 0.9712\n",
            "Epoch 188/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0791 - accuracy: 0.9704\n",
            "Epoch 189/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0820 - accuracy: 0.9701\n",
            "Epoch 190/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0822 - accuracy: 0.9699\n",
            "Epoch 191/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0927 - accuracy: 0.9669\n",
            "Epoch 192/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0800 - accuracy: 0.9704\n",
            "Epoch 193/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0931 - accuracy: 0.9667\n",
            "Epoch 194/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0785 - accuracy: 0.9709\n",
            "Epoch 195/200\n",
            "817/817 [==============================] - 30s 36ms/step - loss: 0.0766 - accuracy: 0.9721\n",
            "Epoch 196/200\n",
            "817/817 [==============================] - 30s 37ms/step - loss: 0.0796 - accuracy: 0.9702\n",
            "Epoch 197/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0714 - accuracy: 0.9736\n",
            "Epoch 198/200\n",
            "817/817 [==============================] - 32s 39ms/step - loss: 0.0772 - accuracy: 0.9720\n",
            "Epoch 199/200\n",
            "817/817 [==============================] - 32s 40ms/step - loss: 0.0818 - accuracy: 0.9704\n",
            "Epoch 200/200\n",
            "817/817 [==============================] - 31s 38ms/step - loss: 0.0731 - accuracy: 0.9724\n"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.01 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQz12ssLy1C"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.2 - Model Architecture 2**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkU6STQLw-S",
        "outputId": "784af326-42e3-48f5-fe08-64e505a446d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_7 (LSTM)               (None, 353, 128)          66560     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 353, 128)          0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 353, 64)           49408     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 353, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 353, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 353, 32)           12416     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 353, 32)           0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 353, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 16)                64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 115)               1955      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133923 (523.14 KB)\n",
            "Trainable params: 133699 (522.26 KB)\n",
            "Non-trainable params: 224 (896.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "817/817 [==============================] - 56s 60ms/step - loss: 4.6774 - accuracy: 0.0317\n",
            "Epoch 2/100\n",
            "817/817 [==============================] - 49s 60ms/step - loss: 4.3611 - accuracy: 0.0743\n",
            "Epoch 3/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 4.1001 - accuracy: 0.0929\n",
            "Epoch 4/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.8784 - accuracy: 0.1086\n",
            "Epoch 5/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.6722 - accuracy: 0.1272\n",
            "Epoch 6/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 3.4553 - accuracy: 0.1532\n",
            "Epoch 7/100\n",
            "817/817 [==============================] - 47s 58ms/step - loss: 3.2476 - accuracy: 0.1838\n",
            "Epoch 8/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 3.0509 - accuracy: 0.2141\n",
            "Epoch 9/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.8726 - accuracy: 0.2390\n",
            "Epoch 10/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.7202 - accuracy: 0.2650\n",
            "Epoch 11/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.5775 - accuracy: 0.2919\n",
            "Epoch 12/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.4175 - accuracy: 0.3243\n",
            "Epoch 13/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 2.2523 - accuracy: 0.3631\n",
            "Epoch 14/100\n",
            "817/817 [==============================] - 48s 58ms/step - loss: 2.0835 - accuracy: 0.3977\n",
            "Epoch 15/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.9249 - accuracy: 0.4319\n",
            "Epoch 16/100\n",
            "817/817 [==============================] - 49s 59ms/step - loss: 1.7853 - accuracy: 0.4622\n",
            "Epoch 17/100\n",
            "817/817 [==============================] - 49s 60ms/step - loss: 1.6644 - accuracy: 0.4909\n",
            "Epoch 18/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.5532 - accuracy: 0.5150\n",
            "Epoch 19/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.4555 - accuracy: 0.5411\n",
            "Epoch 20/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.3741 - accuracy: 0.5610\n",
            "Epoch 21/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.3118 - accuracy: 0.5780\n",
            "Epoch 22/100\n",
            "817/817 [==============================] - 49s 59ms/step - loss: 1.2368 - accuracy: 0.5959\n",
            "Epoch 23/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.1680 - accuracy: 0.6165\n",
            "Epoch 24/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.1144 - accuracy: 0.6316\n",
            "Epoch 25/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.0594 - accuracy: 0.6488\n",
            "Epoch 26/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 1.0047 - accuracy: 0.6664\n",
            "Epoch 27/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 0.9628 - accuracy: 0.6805\n",
            "Epoch 28/100\n",
            "817/817 [==============================] - 48s 59ms/step - loss: 0.9011 - accuracy: 0.6978\n",
            "Epoch 29/100\n",
            " 35/817 [>.............................] - ETA: 45s - loss: 0.9423 - accuracy: 0.6922"
          ]
        }
      ],
      "source": [
        "#Create the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(len(train_x[0]),1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(16,return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "model.summary()\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "#adam = Adam(learning_rate=lr_schedule)\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGcC29CeWi4y"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.2.1 - Model Saving**</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueVFk8ZWYS4s",
        "outputId": "1be94ae0-6820-4b9f-bcd0-a103aad8f244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_2', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezj7AyRaQ1Kl"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.3 - Model Architecture 3**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJqEH_qWb-8",
        "outputId": "21242e2e-0468-4289-b301-1f70a88fe9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 128)          33792     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 64)           256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89489 (349.57 KB)\n",
            "Trainable params: 89297 (348.82 KB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=64,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ZFEBvntTGt",
        "outputId": "f96deecc-a620-4aa9-f7a7-a27bada9adf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "932/932 [==============================] - 89s 80ms/step - loss: 4.5237 - accuracy: 0.0520\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 3.7492 - accuracy: 0.1474\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 3.2352 - accuracy: 0.2316\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.7851 - accuracy: 0.3036\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 2.4090 - accuracy: 0.3725\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 2.0772 - accuracy: 0.4478\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.7939 - accuracy: 0.5148\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.5713 - accuracy: 0.5678\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.3666 - accuracy: 0.6160\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 1.2013 - accuracy: 0.6577\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.1264 - accuracy: 0.6824\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 1.2004 - accuracy: 0.6516\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.8443 - accuracy: 0.7513\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.7252 - accuracy: 0.7837\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.8774 - accuracy: 0.7444\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.6226 - accuracy: 0.8124\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.5666 - accuracy: 0.8262\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.5038 - accuracy: 0.8444\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4577 - accuracy: 0.8591\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.4877 - accuracy: 0.8506\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3760 - accuracy: 0.8852\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 73s 79ms/step - loss: 0.7034 - accuracy: 0.8027\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 73s 78ms/step - loss: 0.4057 - accuracy: 0.8716\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.3222 - accuracy: 0.9016\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.2788 - accuracy: 0.9162\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.2577 - accuracy: 0.9236\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.2141 - accuracy: 0.9397\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1942 - accuracy: 0.9459\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1748 - accuracy: 0.9508\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1582 - accuracy: 0.9571\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1455 - accuracy: 0.9603\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.1420 - accuracy: 0.9614\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1262 - accuracy: 0.9656\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1160 - accuracy: 0.9687\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1170 - accuracy: 0.9677\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1075 - accuracy: 0.9699\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0997 - accuracy: 0.9726\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0975 - accuracy: 0.9725\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0927 - accuracy: 0.9743\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0884 - accuracy: 0.9760\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1597 - accuracy: 0.9528\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0862 - accuracy: 0.9760\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0803 - accuracy: 0.9778\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0789 - accuracy: 0.9787\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0696 - accuracy: 0.9814\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.1011 - accuracy: 0.9714\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0888 - accuracy: 0.9745\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 74s 79ms/step - loss: 0.0687 - accuracy: 0.9809\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0600 - accuracy: 0.9840\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0578 - accuracy: 0.9844\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 74s 80ms/step - loss: 0.0562 - accuracy: 0.9844\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0529 - accuracy: 0.9862\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0539 - accuracy: 0.9853\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0554 - accuracy: 0.9852\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0486 - accuracy: 0.9873\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0555 - accuracy: 0.9848\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0472 - accuracy: 0.9875\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0434 - accuracy: 0.9885\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0437 - accuracy: 0.9883\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 77s 83ms/step - loss: 0.0482 - accuracy: 0.9866\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 1.8266 - accuracy: 0.6585\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.6956 - accuracy: 0.7751\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.3384 - accuracy: 0.8836\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2430 - accuracy: 0.9169\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1965 - accuracy: 0.9355\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1629 - accuracy: 0.9475\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1416 - accuracy: 0.9549\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1350 - accuracy: 0.9579\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1189 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.1042 - accuracy: 0.9680\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.9430 - accuracy: 0.7589\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.2478 - accuracy: 0.9144\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1939 - accuracy: 0.9343\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.1517 - accuracy: 0.9503\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1292 - accuracy: 0.9583\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1177 - accuracy: 0.9625\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.1046 - accuracy: 0.9674\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.1538 - accuracy: 0.9545\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0958 - accuracy: 0.9706\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0812 - accuracy: 0.9749\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0730 - accuracy: 0.9781\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0717 - accuracy: 0.9784\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0665 - accuracy: 0.9798\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0638 - accuracy: 0.9807\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0615 - accuracy: 0.9810\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0605 - accuracy: 0.9812\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0541 - accuracy: 0.9832\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0606 - accuracy: 0.9812\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0531 - accuracy: 0.9838\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0498 - accuracy: 0.9844\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0480 - accuracy: 0.9844\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0460 - accuracy: 0.9856\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 75s 81ms/step - loss: 0.0445 - accuracy: 0.9858\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0455 - accuracy: 0.9854\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 75s 80ms/step - loss: 0.0434 - accuracy: 0.9862\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0416 - accuracy: 0.9865\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0432 - accuracy: 0.9860\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 76s 82ms/step - loss: 0.0406 - accuracy: 0.9869\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0404 - accuracy: 0.9867\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 76s 81ms/step - loss: 0.0397 - accuracy: 0.9872\n"
          ]
        }
      ],
      "source": [
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiXTxc8XNsYE",
        "outputId": "6974d4b2-d88e-41c2-d2d1-d55be208fc22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_3', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQWlRcZXEmuq"
      },
      "source": [
        "#<p style=\"font-family:JetBrains Mono; font-weight:normal; letter-spacing: 1px; color:#207d06; font-size:100%; text-align:left;padding: 0px; border-bottom: 3px solid #207d06;\">**4.2.4 - Model Architecture 4**</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIS8x11cEzeT",
        "outputId": "d088e33b-d89c-4935-95a9-c3f42273b025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 128)          0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 128)          512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 353681 (1.35 MB)\n",
            "Trainable params: 353233 (1.35 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 141s 132ms/step - loss: 4.6924 - accuracy: 0.0300\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 125s 134ms/step - loss: 4.0253 - accuracy: 0.1153\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 3.4509 - accuracy: 0.2022\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.9579 - accuracy: 0.2890\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.5412 - accuracy: 0.3532\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 2.1979 - accuracy: 0.4147\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 1.8938 - accuracy: 0.4812\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.6091 - accuracy: 0.5525\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 1.3446 - accuracy: 0.6254\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 1.1106 - accuracy: 0.6866\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.9190 - accuracy: 0.7362\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.7668 - accuracy: 0.7779\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.6491 - accuracy: 0.8119\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.5574 - accuracy: 0.8390\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.4864 - accuracy: 0.8608\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.4312 - accuracy: 0.8749\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3855 - accuracy: 0.8877\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3482 - accuracy: 0.8984\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.3189 - accuracy: 0.9063\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2918 - accuracy: 0.9139\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2675 - accuracy: 0.9213\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2470 - accuracy: 0.9265\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2308 - accuracy: 0.9308\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2167 - accuracy: 0.9358\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.2019 - accuracy: 0.9393\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1881 - accuracy: 0.9448\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1775 - accuracy: 0.9467\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1664 - accuracy: 0.9496\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1594 - accuracy: 0.9519\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1498 - accuracy: 0.9537\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1445 - accuracy: 0.9548\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1361 - accuracy: 0.9577\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1311 - accuracy: 0.9586\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1246 - accuracy: 0.9605\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1200 - accuracy: 0.9617\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1172 - accuracy: 0.9624\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1096 - accuracy: 0.9653\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1062 - accuracy: 0.9660\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.1007 - accuracy: 0.9686\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0979 - accuracy: 0.9699\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0938 - accuracy: 0.9707\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0871 - accuracy: 0.9733\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0822 - accuracy: 0.9747\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0799 - accuracy: 0.9753\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0785 - accuracy: 0.9761\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0783 - accuracy: 0.9757\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0706 - accuracy: 0.9781\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0688 - accuracy: 0.9789\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0691 - accuracy: 0.9782\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0676 - accuracy: 0.9787\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0601 - accuracy: 0.9814\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0681 - accuracy: 0.9784\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0582 - accuracy: 0.9815\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0592 - accuracy: 0.9810\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0552 - accuracy: 0.9819\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0534 - accuracy: 0.9820\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0519 - accuracy: 0.9827\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0506 - accuracy: 0.9833\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 126s 136ms/step - loss: 0.0542 - accuracy: 0.9821\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0476 - accuracy: 0.9842\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0461 - accuracy: 0.9852\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0472 - accuracy: 0.9846\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 126s 135ms/step - loss: 0.0525 - accuracy: 0.9833\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0510 - accuracy: 0.9833\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0464 - accuracy: 0.9848\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0439 - accuracy: 0.9861\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0438 - accuracy: 0.9864\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0454 - accuracy: 0.9846\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0428 - accuracy: 0.9854\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0460 - accuracy: 0.9835\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0442 - accuracy: 0.9853\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0410 - accuracy: 0.9851\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0407 - accuracy: 0.9858\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0404 - accuracy: 0.9863\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0368 - accuracy: 0.9878\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0367 - accuracy: 0.9879\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0348 - accuracy: 0.9885\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0339 - accuracy: 0.9886\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0333 - accuracy: 0.9891\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0364 - accuracy: 0.9874\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0327 - accuracy: 0.9890\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0325 - accuracy: 0.9893\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0318 - accuracy: 0.9897\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9903\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0294 - accuracy: 0.9904\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0298 - accuracy: 0.9904\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0314 - accuracy: 0.9900\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 127s 137ms/step - loss: 0.0291 - accuracy: 0.9906\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0418 - accuracy: 0.9866\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0301 - accuracy: 0.9901\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0266 - accuracy: 0.9914\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0265 - accuracy: 0.9918\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9920\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0262 - accuracy: 0.9917\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0258 - accuracy: 0.9917\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9916\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0292 - accuracy: 0.9907\n",
            "Epoch 98/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0275 - accuracy: 0.9914\n",
            "Epoch 99/100\n",
            "932/932 [==============================] - 127s 136ms/step - loss: 0.0256 - accuracy: 0.9918\n",
            "Epoch 100/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.0231 - accuracy: 0.9927\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pn5pr4HjEmJ",
        "outputId": "38ff6c94-d6ef-4a1a-a890-3130ff589e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model created and saved\n"
          ]
        }
      ],
      "source": [
        "model.save( path_to_save_model+'chat_model_4', hist)\n",
        "print(\"model created and saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X71mU4ZljETL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeHAlQgIjDpx",
        "outputId": "de9b52ff-3d23-4d8d-d62c-1e55b5094734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 391, 256)          133120    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 391, 256)          1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 391, 128)          164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 391, 128)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 391, 64)           41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 391, 64)           0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 391, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 32)                10368     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 32)                128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 113)               3729      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 354193 (1.35 MB)\n",
            "Trainable params: 353489 (1.35 MB)\n",
            "Non-trainable params: 704 (2.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "932/932 [==============================] - 145s 137ms/step - loss: 4.6558 - accuracy: 0.0336\n",
            "Epoch 2/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 4.1301 - accuracy: 0.1049\n",
            "Epoch 3/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.7473 - accuracy: 0.1478\n",
            "Epoch 4/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 3.4378 - accuracy: 0.1868\n",
            "Epoch 5/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 3.1494 - accuracy: 0.2339\n",
            "Epoch 6/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.8716 - accuracy: 0.2799\n",
            "Epoch 7/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.6211 - accuracy: 0.3216\n",
            "Epoch 8/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 2.4007 - accuracy: 0.3596\n",
            "Epoch 9/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.2027 - accuracy: 0.3998\n",
            "Epoch 10/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 2.0192 - accuracy: 0.4404\n",
            "Epoch 11/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.8442 - accuracy: 0.4803\n",
            "Epoch 12/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.6993 - accuracy: 0.5156\n",
            "Epoch 13/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.5580 - accuracy: 0.5511\n",
            "Epoch 14/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.4138 - accuracy: 0.5903\n",
            "Epoch 15/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 1.2874 - accuracy: 0.6236\n",
            "Epoch 16/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1761 - accuracy: 0.6543\n",
            "Epoch 17/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.0703 - accuracy: 0.6818\n",
            "Epoch 18/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.9511 - accuracy: 0.7191\n",
            "Epoch 19/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.9754 - accuracy: 0.7076\n",
            "Epoch 20/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.8029 - accuracy: 0.7618\n",
            "Epoch 21/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.7363 - accuracy: 0.7807\n",
            "Epoch 22/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6862 - accuracy: 0.7938\n",
            "Epoch 23/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.6902 - accuracy: 0.7925\n",
            "Epoch 24/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5948 - accuracy: 0.8207\n",
            "Epoch 25/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5565 - accuracy: 0.8309\n",
            "Epoch 26/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5487 - accuracy: 0.8341\n",
            "Epoch 27/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5008 - accuracy: 0.8481\n",
            "Epoch 28/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4716 - accuracy: 0.8583\n",
            "Epoch 29/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4422 - accuracy: 0.8665\n",
            "Epoch 30/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4298 - accuracy: 0.8706\n",
            "Epoch 31/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.6385 - accuracy: 0.8112\n",
            "Epoch 32/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.5284 - accuracy: 0.8354\n",
            "Epoch 33/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3811 - accuracy: 0.8849\n",
            "Epoch 34/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3357 - accuracy: 0.8987\n",
            "Epoch 35/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.3031 - accuracy: 0.9090\n",
            "Epoch 36/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.4038 - accuracy: 0.8822\n",
            "Epoch 37/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2943 - accuracy: 0.9132\n",
            "Epoch 38/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2569 - accuracy: 0.9228\n",
            "Epoch 39/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5749 - accuracy: 0.8398\n",
            "Epoch 40/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2453 - accuracy: 0.9276\n",
            "Epoch 41/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2448 - accuracy: 0.9261\n",
            "Epoch 42/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2230 - accuracy: 0.9331\n",
            "Epoch 43/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2210 - accuracy: 0.9344\n",
            "Epoch 44/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2366 - accuracy: 0.9296\n",
            "Epoch 45/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2673 - accuracy: 0.9200\n",
            "Epoch 46/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1893 - accuracy: 0.9443\n",
            "Epoch 47/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2203 - accuracy: 0.9347\n",
            "Epoch 48/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1895 - accuracy: 0.9435\n",
            "Epoch 49/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1884 - accuracy: 0.9439\n",
            "Epoch 50/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1903 - accuracy: 0.9426\n",
            "Epoch 51/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1809 - accuracy: 0.9463\n",
            "Epoch 52/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1735 - accuracy: 0.9479\n",
            "Epoch 53/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1785 - accuracy: 0.9456\n",
            "Epoch 54/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.1548 - accuracy: 0.7643\n",
            "Epoch 55/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3068 - accuracy: 0.6001\n",
            "Epoch 56/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 1.3468 - accuracy: 0.6587\n",
            "Epoch 57/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 2.1607 - accuracy: 0.4120\n",
            "Epoch 58/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 1.1217 - accuracy: 0.6412\n",
            "Epoch 59/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.7630 - accuracy: 0.7456\n",
            "Epoch 60/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5916 - accuracy: 0.8025\n",
            "Epoch 61/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.5010 - accuracy: 0.8335\n",
            "Epoch 62/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.4271 - accuracy: 0.8602\n",
            "Epoch 63/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.3834 - accuracy: 0.8765\n",
            "Epoch 64/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3504 - accuracy: 0.8858\n",
            "Epoch 65/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.3145 - accuracy: 0.8993\n",
            "Epoch 66/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.2871 - accuracy: 0.9081\n",
            "Epoch 67/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2809 - accuracy: 0.9097\n",
            "Epoch 68/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2499 - accuracy: 0.9205\n",
            "Epoch 69/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2296 - accuracy: 0.9271\n",
            "Epoch 70/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2267 - accuracy: 0.9280\n",
            "Epoch 71/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2176 - accuracy: 0.9305\n",
            "Epoch 72/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.2333 - accuracy: 0.9261\n",
            "Epoch 73/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2141 - accuracy: 0.9325\n",
            "Epoch 74/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2009 - accuracy: 0.9362\n",
            "Epoch 75/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1968 - accuracy: 0.9375\n",
            "Epoch 76/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.2325 - accuracy: 0.9285\n",
            "Epoch 77/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1756 - accuracy: 0.9442\n",
            "Epoch 78/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1708 - accuracy: 0.9459\n",
            "Epoch 79/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1660 - accuracy: 0.9473\n",
            "Epoch 80/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1923 - accuracy: 0.9391\n",
            "Epoch 81/100\n",
            "932/932 [==============================] - 129s 139ms/step - loss: 0.1514 - accuracy: 0.9520\n",
            "Epoch 82/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1617 - accuracy: 0.9489\n",
            "Epoch 83/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1406 - accuracy: 0.9558\n",
            "Epoch 84/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1413 - accuracy: 0.9550\n",
            "Epoch 85/100\n",
            "932/932 [==============================] - 130s 139ms/step - loss: 0.1374 - accuracy: 0.9566\n",
            "Epoch 86/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1682 - accuracy: 0.9468\n",
            "Epoch 87/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1444 - accuracy: 0.9540\n",
            "Epoch 88/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1271 - accuracy: 0.9593\n",
            "Epoch 89/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1262 - accuracy: 0.9602\n",
            "Epoch 90/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1205 - accuracy: 0.9622\n",
            "Epoch 91/100\n",
            "932/932 [==============================] - 128s 137ms/step - loss: 0.1179 - accuracy: 0.9625\n",
            "Epoch 92/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1276 - accuracy: 0.9594\n",
            "Epoch 93/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1176 - accuracy: 0.9630\n",
            "Epoch 94/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1193 - accuracy: 0.9627\n",
            "Epoch 95/100\n",
            "932/932 [==============================] - 128s 138ms/step - loss: 0.1077 - accuracy: 0.9659\n",
            "Epoch 96/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1090 - accuracy: 0.9656\n",
            "Epoch 97/100\n",
            "932/932 [==============================] - 129s 138ms/step - loss: 0.1111 - accuracy: 0.9654\n",
            "Epoch 98/100\n",
            "409/932 [============>.................] - ETA: 1:11 - loss: 0.1219 - accuracy: 0.9610"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=128,input_shape=(len(train_x[0]),1),return_sequences=True)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=64,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(units=32,return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(units=16,return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(len(train_y[0]),activation=\"softmax\"))\n",
        "\n",
        "# Build the model\n",
        "model.build(input_shape=(None, len(train_x[0]), 1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(learning_rate=0.001 ,momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=128, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVKnyKo1Ryg3"
      },
      "outputs": [],
      "source": [
        "model.save( path_to_save_model+'chat_model_5', hist)\n",
        "print(\"model created and saved\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}