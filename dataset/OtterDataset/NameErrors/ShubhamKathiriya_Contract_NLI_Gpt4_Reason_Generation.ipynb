{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39428a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#import torch \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#import torch.nn.functional as F\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#from transformers import AutoModelForCausalLM,  StoppingCriteria, StoppingCriteriaList, AutoTokenizer,BitsAndBytesConfig, pipeline, AutoConfig , TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#import trl\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chat_template\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:38\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     cached_file,\n\u001b[1;32m     31\u001b[0m     extract_commit_hash,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     logging,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoder_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     CONFIG_MAPPING_NAMES,\n\u001b[1;32m     41\u001b[0m     AutoConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     replace_list_option_in_docstrings,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[1;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     46\u001b[0m CLASS_DOCSTRING \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m    This is a generic model class that will be instantiated as one of the model classes of the library when created\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m    with the [`~BaseAutoModelClass.from_pretrained`] class method or the [`~BaseAutoModelClass.from_config`] class\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m    This class cannot be instantiated directly using `__init__()` (throws an error).\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:115\u001b[0m\n\u001b[1;32m    112\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlignDevicesHook, add_hook_to_module\n\u001b[1;32m    118\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenerateDecoderOnlyOutput\u001b[39;00m(ModelOutput):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Outputs of decoder-only generation models, when using non-beam methods.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     cpu_offload,\n\u001b[1;32m     19\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m skip_first_batches\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_torch_state_dict_into_shards\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SAFE_MODEL_NAME,\n\u001b[1;32m     29\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[1;32m     30\u001b[0m     SAMPLER_NAME,\n\u001b[1;32m     31\u001b[0m     SCALER_NAME,\n\u001b[1;32m     32\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     33\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     34\u001b[0m     get_pretty_name,\n\u001b[1;32m     35\u001b[0m     is_mlu_available,\n\u001b[1;32m     36\u001b[0m     is_torch_xla_available,\n\u001b[1;32m     37\u001b[0m     is_xpu_available,\n\u001b[1;32m     38\u001b[0m     load,\n\u001b[1;32m     39\u001b[0m     save,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_xla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxm\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/utils/__init__.py:204\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    194\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[1;32m    195\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m         get_active_deepspeed_plugin,\n\u001b[1;32m    201\u001b[0m     )\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    205\u001b[0m     disable_fsdp_ram_efficient_loading,\n\u001b[1;32m    206\u001b[0m     enable_fsdp_ram_efficient_loading,\n\u001b[1;32m    207\u001b[0m     ensure_weights_retied,\n\u001b[1;32m    208\u001b[0m     load_fsdp_model,\n\u001b[1;32m    209\u001b[0m     load_fsdp_optimizer,\n\u001b[1;32m    210\u001b[0m     merge_fsdp_weights,\n\u001b[1;32m    211\u001b[0m     save_fsdp_model,\n\u001b[1;32m    212\u001b[0m     save_fsdp_optimizer,\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    215\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    216\u001b[0m     _filter_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     prepare_tpu,\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# For docs\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "\n",
    "#import torch \n",
    "#import torch.nn.functional as F\n",
    "#from transformers import AutoModelForCausalLM,  StoppingCriteria, StoppingCriteriaList, AutoTokenizer,BitsAndBytesConfig, pipeline, AutoConfig , TrainingArguments, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer\n",
    "#import trl\n",
    "from trl import apply_chat_template\n",
    "#from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "#from peft import LoraConfig, PeftConfig\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from datasets import Dataset\n",
    "\n",
    "#import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41372cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = 'datasets/train.json'\n",
    "TEST_DATASET_PATH = 'datasets/test.json'\n",
    "DEV_DATASET_PATH = 'datasets/dev.json'\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\", \"\")  \n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")  \n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")  \n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "oai_client = AzureOpenAI(  \n",
    "    azure_endpoint=endpoint,  \n",
    "    api_key=subscription_key,  \n",
    "    api_version=\"2024-05-01-preview\",  \n",
    ") \n",
    "\n",
    "\n",
    "reason_system_prompt = '''\n",
    "You are a legal agent. You are provided with statements from a contract and a hypothesis and an annotation json.\n",
    "Definition of Input is as follows:\n",
    "1. Contract: A legal NDA provided by the user in line by line format as described below\n",
    "2. Hypothesis: A statement that user asks corresponding to the contract\n",
    "3. Annotation: A json object containing two keys, choice and spans. \n",
    "\n",
    "Choice contains following values:\n",
    "\n",
    "3.1. Entailment: The hypothesis is directly supported by one or more of the statements from the contract, meaning if the statement is true, then the hypothesis is true. The implication should be unambiguous and without assumptions.\n",
    "3.2. Contradiction: The hypothesis contradicts one or more of the statements from the contract, meaning if the statement is true, then the hypothesis is false. This implication should be unambiguous and without assumptions.\n",
    "3.3. Not Mentioned: In cases where none of the above conclusions can be drawn without assumptions.\n",
    "\n",
    "Spans: Statement ids which supports the choice\n",
    "\n",
    "Your task is to evalaute and tell your thoughts for the choice given in the annotation and why the given statement ids are selected. \n",
    "Dont repeat statement from contract, instead refer to them by id value e.g \"statement_id_1 mentions this\". \n",
    "Give your thought step wise:\n",
    "1. First refer to each span mentioned and tell why do you think its choosen.\n",
    "2. Then give your opinion on how the statement supports the choice.\n",
    "3. If you dont agree with the choice mentioned, return does_not_agree yes else no.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Input Format:\n",
    "Contract:\n",
    "[[statement_id_1]]: <1st statement>\n",
    "[[statement_id_2]]: <2nd statement>\n",
    "..\n",
    "\n",
    "Hypothesis:\n",
    "<hypothesis_statement>\n",
    "\n",
    "Annotation:\n",
    "```json\n",
    "{\n",
    "    \"choice\": \"<one of Entailment, Contradiction, NotMentioned>\",\n",
    "    \"spans\": [\"<statement ids that supports the above choice>\"],\n",
    "}\n",
    "```\n",
    "\n",
    "Output Format:\n",
    "```json\n",
    "{\n",
    "    \"thoughts\": \"<thoughts for annotation in less than 150 words>\",\n",
    "    \"does_not_agree\": \"<yes/no>\"\n",
    "}\n",
    "```\n",
    "'''\n",
    "\n",
    "reason_user_prompt = '''\n",
    "Contract: \n",
    "{0}\n",
    "\n",
    "Hypothesis:\n",
    "{1}\n",
    "\n",
    "Annotation:\n",
    "{2}\n",
    "'''\n",
    "\n",
    "ft_system_prompt = '''\n",
    "You are a legal agent. You are provided with statements from a contract and a hypothesis. Your task is to evaluate whether the hypothesis statement is:\n",
    "1. Entailment: The hypothesis is directly supported by one or more of the statements from the contract, meaning if the statement is true, then the hypothesis is true. The implication should be unambiguous and without assumptions.\n",
    "2. Contradiction: The hypothesis contradicts one or more of the statements from the contract, meaning if the statement is true, then the hypothesis is false. This implication should be unambiguous and without assumptions.\n",
    "3. Not Mentioned: In cases where none of the above conclusions can be drawn without assumptions.\n",
    "\n",
    "\n",
    "Input Format:\n",
    "Contract:\n",
    "[[statement_id_1]]: <1st statement>\n",
    "[[statement_id_2]]: <2nd statement>\n",
    "..\n",
    "\n",
    "Hypothesis:\n",
    "<hypothesis_statement>\n",
    "\n",
    "Output Format:\n",
    "```json\n",
    "{\n",
    "    \"thoughts\": \"<explain the thought behind selection>\"\n",
    "    \"choice\": \"<one of Entailment, Contradiction, NotMentioned>\",\n",
    "    \"spans\": [\"<mention most relevant statement ids that directly support the choice in case of Entailement and Contradictory, otherwise keep empty>\"],\n",
    "}\n",
    "```\n",
    "'''\n",
    "\n",
    "ft_user_prompt = '''\n",
    "Contract:\n",
    "{0}\n",
    "Hypothesis:\n",
    "{1}\n",
    "'''\n",
    "\n",
    "\n",
    "chat_template = '''\n",
    "<|system|>\n",
    "{0}\n",
    "<|user|>\n",
    "{1}<|end|>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "response_template = '''<|assistant|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445694be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path,'r') as f:\n",
    "        dataset =  json.load(f)\n",
    "    return dataset\n",
    "\n",
    "def get_embedding(model, tokenizer, text: str, aggregation_method: str = None):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "    if aggregation_method == 'mean':\n",
    "        # Mean pool the token embeddings to get a sentence embedding\n",
    "        embedding = last_hidden_state.mean(dim=1)\n",
    "    elif aggregation_method == 'last_token':\n",
    "        # Get the hidden state of the last token\n",
    "        embedding = last_hidden_state[:, -1, :]  # Shape: [batch_size, hidden_size]\n",
    "    else:\n",
    "        # If no valid aggregation method is specified, return the last hidden state\n",
    "        embedding = last_hidden_state\n",
    "\n",
    "    # Normalize the embedding (if applicable)\n",
    "    if aggregation_method in ['mean', 'last_token']:\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def construct_statement_annotation_pair(dataset_input):\n",
    "    statement_annotation_pair = {}\n",
    "    for i,document in enumerate(dataset_input):\n",
    "        #print(document['id'])\n",
    "        statement_annotation_pair[i] = {}\n",
    "        statement_annotation_pair[i]['statement'] = ''\n",
    "        for idx,span in enumerate(document['spans']):\n",
    "            #print(span)\n",
    "            statement_annotation_pair[i]['statement'] += f'''[[statement_id_{idx}]]: {document['text'][span[0]:span[1]]}\\n'''\n",
    "            statement_annotation_pair[i]['target'] =  document['annotation_sets'][0]\n",
    "            statement_annotation_pair[i]['doc_id'] = document['id']\n",
    "    return statement_annotation_pair\n",
    "\n",
    "def get_reason_from_gpt4(contract,hypothesis,annotation):  \n",
    "    # Prepare the chat prompt  \n",
    "    chat_prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": reason_system_prompt\n",
    "    },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": reason_user_prompt.format(contract,hypothesis,annotation)\n",
    "        }\n",
    "    ]   \n",
    "\n",
    "    # Generate the completion  \n",
    "    completion = oai_client.chat.completions.create(  \n",
    "        model=deployment,  \n",
    "        messages=chat_prompt,  \n",
    "        max_tokens=200,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.95,  \n",
    "        frequency_penalty=0,  \n",
    "        presence_penalty=0,  \n",
    "        stop=None,  \n",
    "        stream=False,\n",
    "        response_format = { \"type\": \"json_object\" }\n",
    "    )  \n",
    "    response = json.loads(json.loads(completion.to_json())[\"choices\"][0]['message']['content'])\n",
    "    return response['thoughts'],response['does_not_agree']\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset_for_finetuning_method1(train_contract_statements_and_annotations,hypothesis_set,tokeniser,max_tokens=8192,augment_reason=True,test_data=False):\n",
    "    '''\n",
    "    {\n",
    "        \"prompt\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"}],\n",
    "        \"completion\": [{\"role\": \"assistant\", \"content\": \"It is blue.\"}]\n",
    "    }\n",
    "    '''\n",
    "    dataset = []\n",
    "    total_points = 0\n",
    "    skipped_points = 0\n",
    "    for idx,contract in tqdm_notebook(train_contract_statements_and_annotations.items()):\n",
    "        #current_system_prompt = ft_system_prompt.format(contract['statement'])\n",
    "        for key,value in tqdm_notebook(hypothesis_set.items()):\n",
    "            #current_user_prompt = f\"Evaluate following hypothesis: \\n{value['hypothesis']}\"\n",
    "            response = \"\",\"no\"\n",
    "            if augment_reason:\n",
    "                try:\n",
    "                    response=get_reason_from_gpt4(contract['statement'],value['hypothesis'],contract['target']['annotations'][key])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                #print(response[0],response[1])\n",
    "                contract['target']['annotations'][key] = { **{\"thoughts\":response[0]}, **contract['target']['annotations'][key]}\n",
    "                #print(contract['target']['annotations'][key])                                 \n",
    "            #print(contract['target']['annotations'][key])\n",
    "            data_point = {\n",
    "                \"doc_id\": idx,\n",
    "                \"prompt\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": ft_system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": ft_user_prompt.format(contract['statement'],value['hypothesis'])\n",
    "                    }\n",
    "                ],\n",
    "                \"completion\": [\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": f\"```json\\n{json.dumps(contract['target']['annotations'][key])}\\n```\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            chatified_text = apply_chat_template(data_point,tokeniser)\n",
    "            chatified_text['is_contradicted_by_gpt4'] = response[1]\n",
    "            chatified_text['doc_id'] = contract['doc_id']\n",
    "            prompt_tokens = len(tokeniser(chatified_text['prompt'],truncation=False)['input_ids'])\n",
    "            completion_tokens = len(tokeniser(chatified_text['completion'],truncation=False)['input_ids'])\n",
    "            total_points += 1\n",
    "            if(prompt_tokens  + completion_tokens <= max_tokens):\n",
    "#                 print(chatified_text)\n",
    "                dataset.append(chatified_text)\n",
    "            else:\n",
    "                skipped_points += 1\n",
    "\n",
    "    return Dataset.from_list(dataset),total_points,skipped_points\n",
    "    \n",
    "    \n",
    "\n",
    "'''\n",
    "With All Statements\n",
    "'''\n",
    "def generate_message_target_pair(idx,train_contract_statements_and_annotations,nda,model=None,tokeniser=None):\n",
    "    sim_scores = {}\n",
    "    example_data = train_contract_statements_and_annotations[idx]\n",
    "    current_system_prompt = system_prompt.format(example_data['statement'])\n",
    "    hypothesis_embed = get_embedding(model,tokeniser,hypothesis_set[nda]['hypothesis'],'mean')\n",
    "    if model is not None and tokeniser is not None:\n",
    "        for idx,statement in enumerate(example_data['statement'].split('\\n')):\n",
    "            if len(statement) > 0:\n",
    "                statement_embed = get_embedding(model,tokeniser,statement,'mean')\n",
    "                sim_scores[statement.split(']]')[0].split('[[')[-1]] = ((statement_embed@hypothesis_embed.T).item())\n",
    "\n",
    "    current_user_prompt = hypothesis_set[nda]['hypothesis']\n",
    "    chat_template_current = chat_template.format(current_system_prompt,current_user_prompt)\n",
    "    messages = [ \n",
    "        {\"role\": \"system\", \"content\": current_system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": current_user_prompt},\n",
    "    ]\n",
    "    target = example_data['target']['annotations'][nda]\n",
    "\n",
    "    # Get the top 5 items sorted by value\n",
    "    top_5_statements = sorted(sim_scores.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "\n",
    "    return messages,target,sim_scores,top_5_statements\n",
    "\n",
    "def generate_message_target_pair_with_top_5_staements(idx, train_contract_statements_and_annotations, nda, model=None, tokeniser=None):\n",
    "    sim_scores = {}\n",
    "    example_data = train_contract_statements_and_annotations[idx]\n",
    "    hypothesis_embed = get_embedding(model, tokeniser, hypothesis_set[nda]['hypothesis'], 'mean')\n",
    "\n",
    "    if model is not None and tokeniser is not None:\n",
    "        for idx, statement in enumerate(example_data['statement'].split('\\n')):\n",
    "            if len(statement) > 0:\n",
    "                statement_embed = get_embedding(model, tokeniser, statement, 'mean')\n",
    "                # Store similarity scores\n",
    "                statement_id = statement.split(']]')[0].split('[[')[-1]\n",
    "                sim_scores[int(statement_id.split('statement_id_')[-1])] = (statement_embed @ hypothesis_embed.T).item()\n",
    "\n",
    "    current_user_prompt = hypothesis_set[nda]['hypothesis']\n",
    "    target = example_data['target']['annotations'][nda]\n",
    "\n",
    "    # Get the top 5 items sorted by value\n",
    "    top_5_statements = sorted(sim_scores.items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    # Initialize system prompt components\n",
    "    formatted_statements = []\n",
    "    ids_to_format = set()  # To store unique IDs for formatting\n",
    "\n",
    "    for statement_id, _ in top_5_statements:\n",
    "        # Assume that statement_id is an integer or can be converted to an integer\n",
    "        statement_id = int(statement_id)\n",
    "        ids_to_format.update({statement_id-1, statement_id, statement_id + 1})\n",
    "    ids_to_format = list(map(lambda x: f'statement_id_{x}', ids_to_format))\n",
    "    print(ids_to_format)\n",
    "\n",
    "    # Format statements only for the collected IDs\n",
    "    for statement in example_data['statement'].split('\\n'):\n",
    "        # Assuming statement has a format where the ID is part of it\n",
    "        statement_id =  statement.split(']]')[0].split('[[')[-1]  # Extract the ID\n",
    "        if statement_id in ids_to_format:\n",
    "            formatted_statements.append(statement)\n",
    "\n",
    "    # Construct the system prompt using the filtered statements\n",
    "    current_system_prompt = system_prompt.format('\\n'.join(formatted_statements))\n",
    "\n",
    "    # Construct the chat messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": current_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": current_user_prompt},\n",
    "    ]\n",
    "\n",
    "    return messages, target, sim_scores, top_5_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(TRAIN_DATASET_PATH)\n",
    "hypothesis_set = train_dataset['labels']\n",
    "dev_dataset = load_dataset(DEV_DATASET_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True) \n",
    "test_dataset = load_dataset(TEST_DATASET_PATH)\n",
    "\n",
    "train_contract_statements_and_annotations = construct_statement_annotation_pair(train_dataset['documents'])\n",
    "dev_contract_statements_and_annotations = construct_statement_annotation_pair(dev_dataset['documents'])\n",
    "test_contract_statements_and_annotations = construct_statement_annotation_pair(test_dataset['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1751d1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_contract_statements_and_annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m max_tok_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(max_tok_string)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train_dataset_processed, total_points, skipped_points = prepare_dataset_for_finetuning_method1(train_contract_statements_and_annotations,hypothesis_set,tokenizer,max_tokens=7168)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train_dataset_processed = train_dataset_processed.map(lambda x: {'text': x['prompt'] + \" \" + x['completion']})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# dev_dataset_processed.save_to_disk(f'{DEV_DATASET_PATH}_{max_tok_string}.hf_with_thoughts_and_opinion')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(total_points, skipped_points)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m test_dataset_processed, total_points, skipped_points \u001b[38;5;241m=\u001b[39m prepare_dataset_for_finetuning_method1(test_contract_statements_and_annotations,hypothesis_set,tokenizer,max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7168\u001b[39m,augment_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m test_dataset_processed \u001b[38;5;241m=\u001b[39m test_dataset_processed\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     14\u001b[0m test_dataset_processed\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_DATASET_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_tok_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hf_with_thoughts_and_opinion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_contract_statements_and_annotations' is not defined"
     ]
    }
   ],
   "source": [
    "max_seq = 7168\n",
    "max_tok_string = f'{max_seq/1000:.0f}k'\n",
    "# print(max_tok_string)\n",
    "# train_dataset_processed, total_points, skipped_points = prepare_dataset_for_finetuning_method1(train_contract_statements_and_annotations,hypothesis_set,tokenizer,max_tokens=7168)\n",
    "# train_dataset_processed = train_dataset_processed.map(lambda x: {'text': x['prompt'] + \" \" + x['completion']})\n",
    "# train_dataset_processed.save_to_disk(f'{TRAIN_DATASET_PATH}_{max_tok_string}.hf_with_thoughts_and_opinion')\n",
    "# print(total_points, skipped_points)\n",
    "# dev_dataset_processed, total_points, skipped_points = prepare_dataset_for_finetuning_method1(dev_contract_statements_and_annotations,hypothesis_set,tokenizer,max_tokens=7168)\n",
    "# dev_dataset_processed = dev_dataset_processed.map(lambda x: {'text': x['prompt'] + \" \" + x['completion']})\n",
    "# dev_dataset_processed.save_to_disk(f'{DEV_DATASET_PATH}_{max_tok_string}.hf_with_thoughts_and_opinion')\n",
    "# print(total_points, skipped_points)\n",
    "test_dataset_processed, total_points, skipped_points = prepare_dataset_for_finetuning_method1(test_contract_statements_and_annotations,hypothesis_set,tokenizer,max_tokens=7168,augment_reason=False)\n",
    "test_dataset_processed = test_dataset_processed.map(lambda x: {'text': x['prompt'] + \" \" + x['completion']})\n",
    "test_dataset_processed.save_to_disk(f'{TEST_DATASET_PATH}_{max_tok_string}.hf_with_thoughts_and_opinion')\n",
    "print(total_points, skipped_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4dd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
