{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:04.078784Z",
     "iopub.status.busy": "2024-12-14T04:44:04.078311Z",
     "iopub.status.idle": "2024-12-14T04:44:04.106459Z",
     "shell.execute_reply": "2024-12-14T04:44:04.105662Z",
     "shell.execute_reply.started": "2024-12-14T04:44:04.078731Z"
    },
    "id": "gU9ZJU7v-QB-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kaggle=0\n",
    "collab=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:04.107964Z",
     "iopub.status.busy": "2024-12-14T04:44:04.107713Z",
     "iopub.status.idle": "2024-12-14T04:44:04.120873Z",
     "shell.execute_reply": "2024-12-14T04:44:04.119829Z",
     "shell.execute_reply.started": "2024-12-14T04:44:04.107940Z"
    },
    "id": "PkSNB_ee-Q-A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if kaggle:\n",
    "    import os\n",
    "    os.listdir(\"/kaggle/input/go-go-nihongo\")\n",
    "    # !cd /kaggle/input/go-go-nihongo\n",
    "    # print(os.getcwd())\n",
    "    %cd /kaggle/input/go-go-nihongo\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:04.122240Z",
     "iopub.status.busy": "2024-12-14T04:44:04.121899Z",
     "iopub.status.idle": "2024-12-14T04:44:04.127536Z",
     "shell.execute_reply": "2024-12-14T04:44:04.126586Z",
     "shell.execute_reply.started": "2024-12-14T04:44:04.122202Z"
    },
    "id": "fln_7HqX8Bi4",
    "outputId": "a153ed15-d233-4506-d79d-9d0714b345ad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if collab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  %cd /content/drive/MyDrive/Projects/GO_GO_Nihongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:04.129544Z",
     "iopub.status.busy": "2024-12-14T04:44:04.129272Z",
     "iopub.status.idle": "2024-12-14T04:44:46.470375Z",
     "shell.execute_reply": "2024-12-14T04:44:46.469334Z",
     "shell.execute_reply.started": "2024-12-14T04:44:04.129505Z"
    },
    "id": "jqyjrsVR4PX-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "%pip install janome mojimoji\n",
    "!pip install sentencepiece\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:46.471827Z",
     "iopub.status.busy": "2024-12-14T04:44:46.471549Z",
     "iopub.status.idle": "2024-12-14T04:44:59.623148Z",
     "shell.execute_reply": "2024-12-14T04:44:59.622239Z",
     "shell.execute_reply.started": "2024-12-14T04:44:46.471802Z"
    },
    "id": "jK88JoQ79a_G",
    "outputId": "9ac4ba27-ff62-4173-ecb6-286d46695619",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "WARNING:tensorflow:From C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_23652\\416642369.py:30: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import mojimoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "import unicodedata\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ignore warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(tf.__version__)\n",
    "# gpu\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_L_ygdR9a_Q"
   },
   "source": [
    "# load text file\n",
    "\n",
    "this dataset is aleady implemented a SentenceSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:59.625452Z",
     "iopub.status.busy": "2024-12-14T04:44:59.624442Z",
     "iopub.status.idle": "2024-12-14T04:44:59.955301Z",
     "shell.execute_reply": "2024-12-14T04:44:59.954454Z",
     "shell.execute_reply.started": "2024-12-14T04:44:59.625407Z"
    },
    "id": "jYlzMp1U9a_T",
    "outputId": "d2d9ca51-43fa-4257-cad1-247c6a51ca1f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 は 今日 は とても 良い 日 です ｡\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "import mojimoji\n",
    "\n",
    "# Initialize Japanese tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def preprocess_japanese_text(text):\n",
    "    # Convert full-width characters to half-width\n",
    "    text = mojimoji.zen_to_han(text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text, wakati=True)\n",
    "\n",
    "    # Join tokens with a space to preserve word boundaries for translation models\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    # Remove any unnecessary whitespace\n",
    "    preprocessed_text = preprocessed_text.strip()\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"今日は今日はとても良い日です。\"\n",
    "processed_text = preprocess_japanese_text(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:44:59.957893Z",
     "iopub.status.busy": "2024-12-14T04:44:59.957495Z",
     "iopub.status.idle": "2024-12-14T04:45:00.407540Z",
     "shell.execute_reply": "2024-12-14T04:45:00.406570Z",
     "shell.execute_reply.started": "2024-12-14T04:44:59.957853Z"
    },
    "id": "OXTHbwXE-dg8",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>english</th>\n",
       "      <th>japanese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>While in England I often consulted the guidebook.</td>\n",
       "      <td>イギリスにいる間、私はよくそのガイドブックを参考にした。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Look at the sports car over there.</td>\n",
       "      <td>あそこのスポーツカーを見なさい。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Never did I expect that he would fail the exam...</td>\n",
       "      <td>彼が試験に失敗するなんて私は予想もしなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>He knows no foreign language except English.</td>\n",
       "      <td>彼は英語以外の外国語は全く知らない。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>All their secrets have been revealed.</td>\n",
       "      <td>彼らの秘密が全部暴かれた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>1996</td>\n",
       "      <td>John had a bad cold last week.</td>\n",
       "      <td>ジョンは先週ひどい風邪をひいた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>1997</td>\n",
       "      <td>She couldn't utter a word.</td>\n",
       "      <td>彼女は一言も発せられなかった。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>Hanako has forgotten her umbrella again.</td>\n",
       "      <td>花子はまた傘を忘れてきた。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>1999</td>\n",
       "      <td>Tom is going to do something about it.</td>\n",
       "      <td>トムは事態をなんとかしようとします。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>The leader dismissed the demonstrators in the ...</td>\n",
       "      <td>指揮者はその公園でデモ隊を解散させた。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    id                                            english  \\\n",
       "0              0     1  While in England I often consulted the guidebook.   \n",
       "1              1     2                 Look at the sports car over there.   \n",
       "2              2     3  Never did I expect that he would fail the exam...   \n",
       "3              3     4       He knows no foreign language except English.   \n",
       "4              4     5              All their secrets have been revealed.   \n",
       "...          ...   ...                                                ...   \n",
       "1995        1995  1996                     John had a bad cold last week.   \n",
       "1996        1996  1997                         She couldn't utter a word.   \n",
       "1997        1997  1998           Hanako has forgotten her umbrella again.   \n",
       "1998        1998  1999             Tom is going to do something about it.   \n",
       "1999        1999  2000  The leader dismissed the demonstrators in the ...   \n",
       "\n",
       "                          japanese  \n",
       "0     イギリスにいる間、私はよくそのガイドブックを参考にした。  \n",
       "1                 あそこのスポーツカーを見なさい。  \n",
       "2          彼が試験に失敗するなんて私は予想もしなかった。  \n",
       "3               彼は英語以外の外国語は全く知らない。  \n",
       "4                    彼らの秘密が全部暴かれた。  \n",
       "...                            ...  \n",
       "1995              ジョンは先週ひどい風邪をひいた。  \n",
       "1996               彼女は一言も発せられなかった。  \n",
       "1997                 花子はまた傘を忘れてきた。  \n",
       "1998            トムは事態をなんとかしようとします。  \n",
       "1999           指揮者はその公園でデモ隊を解散させた。  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"./datas/Aniki.csv\",index_col=False)\n",
    "df = pd.read_csv(\"./datas/eng_jap_dataset.csv\",index_col=False)\n",
    "# first_run Uncomment the below line for the 1st run\n",
    "df=df[:2000] \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:13.716346Z",
     "iopub.status.busy": "2024-12-14T04:45:13.715979Z",
     "iopub.status.idle": "2024-12-14T04:45:13.746563Z",
     "shell.execute_reply": "2024-12-14T04:45:13.745925Z",
     "shell.execute_reply.started": "2024-12-14T04:45:13.716314Z"
    },
    "id": "lGSCDfAq-eT0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "\n",
    "emoji_list_datas_path = \"./datas//Emoji Sheets - Emoji Only.csv\"\n",
    "emoji_df = pd.read_csv(emoji_list_datas_path)\n",
    "\n",
    "# Extract the emojis into a list\n",
    "emoji_list = emoji_df[\"Emoji_List\"].tolist()\n",
    "\n",
    "# Start the pattern string\n",
    "pattern = \"[\"\n",
    "\n",
    "# Append each code point to the pattern string, ensuring each one is 8 digits\n",
    "for cp in emoji_list:\n",
    "    pattern += f\"\\\\U{cp[1:]:0>8}\"\n",
    "\n",
    "# Close the pattern string\n",
    "pattern += \"]\"\n",
    "\n",
    "# Compile the regular expression\n",
    "emoji_pattern = re.compile(pattern, re.UNICODE)\n",
    "\n",
    "\n",
    "# function to completely remove the emojis from the comments using re\n",
    "def remove_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # or return an empty string: return ''\n",
    "\n",
    "    # Compile the regular expression\n",
    "    emoji_pattern = re.compile(pattern, re.UNICODE)\n",
    "\n",
    "    # Use the sub method to remove emojis\n",
    "    text_no_emojis = emoji_pattern.sub(r\"\", text)\n",
    "    return text_no_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:14.048769Z",
     "iopub.status.busy": "2024-12-14T04:45:14.048415Z",
     "iopub.status.idle": "2024-12-14T04:45:14.105952Z",
     "shell.execute_reply": "2024-12-14T04:45:14.105101Z",
     "shell.execute_reply.started": "2024-12-14T04:45:14.048740Z"
    },
    "id": "EsjBj17t-eRn",
    "outputId": "3476144c-0579-4fb7-f3cb-eb338f16ebae",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing emojis from English text: 100%|██████████| 2000/2000 [00:00<00:00, 15690.55it/s]\n",
      "Removing emojis from Japanese text: 100%|██████████| 2000/2000 [00:00<00:00, 40496.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Removing emojis from English text\")\n",
    "df[\"english\"] = df[\"english\"].progress_apply(remove_emojis)\n",
    "\n",
    "tqdm.pandas(desc=\"Removing emojis from Japanese text\")\n",
    "df[\"japanese\"] = df[\"japanese\"].progress_apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:14.538244Z",
     "iopub.status.busy": "2024-12-14T04:45:14.537522Z",
     "iopub.status.idle": "2024-12-14T04:45:14.542150Z",
     "shell.execute_reply": "2024-12-14T04:45:14.541274Z",
     "shell.execute_reply.started": "2024-12-14T04:45:14.538212Z"
    },
    "id": "7H6LU3sE-eO0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import mojimoji\n",
    "\n",
    "\n",
    "def zen_to_han(text):\n",
    "    return mojimoji.zen_to_han(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:14.973048Z",
     "iopub.status.busy": "2024-12-14T04:45:14.972706Z",
     "iopub.status.idle": "2024-12-14T04:45:14.980084Z",
     "shell.execute_reply": "2024-12-14T04:45:14.979266Z",
     "shell.execute_reply.started": "2024-12-14T04:45:14.973018Z"
    },
    "id": "eyups2aI-eKM",
    "outputId": "bf6991a9-0766-4425-e4e7-421c92c26542",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('こ ん に ち は 。   今 日 は', 'Hello world e ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "# English might have accent like é but Japanese doesn't have any accent I just create different function to ascii for Japanese and English\n",
    "# Removing accented characters\n",
    "def english_unicode_to_ascii(text):\n",
    "    return \"\".join(\n",
    "        ascii_text\n",
    "        for ascii_text in unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"utf-8\", \"ignore\")\n",
    "    )\n",
    "\n",
    "\n",
    "def japanese_unicode_to_ascii(text):\n",
    "    return \" \".join(ascii_text for ascii_text in unicodedata.normalize(\"NFKD\", text))\n",
    "\n",
    "\n",
    "japanese_unicode_to_ascii(\"こんにちは。 今日は\"), english_unicode_to_ascii(\n",
    "    \"Hello world é \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:15.309419Z",
     "iopub.status.busy": "2024-12-14T04:45:15.308941Z",
     "iopub.status.idle": "2024-12-14T04:45:15.317596Z",
     "shell.execute_reply": "2024-12-14T04:45:15.316664Z",
     "shell.execute_reply.started": "2024-12-14T04:45:15.309371Z"
    },
    "id": "CEuhx5KH-eHo",
    "outputId": "34a9134f-da53-4eaf-e83d-11de67563adb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは 世界 this is an example sentence\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Define regex pattern for allowed characters (Latin letters and Japanese characters)\n",
    "    allowed_pattern = r\"[^a-zA-Z\\u4E00-\\u9FFF\\u3040-\\u30FF\\s]\"\n",
    "    # Replace everything not in allowed pattern with a space\n",
    "    cleaned_text = re.sub(allowed_pattern, \" \", text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"こんにちは、世界! This is an example sentence.\"\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:15.659048Z",
     "iopub.status.busy": "2024-12-14T04:45:15.658714Z",
     "iopub.status.idle": "2024-12-14T04:45:23.866257Z",
     "shell.execute_reply": "2024-12-14T04:45:23.865220Z",
     "shell.execute_reply.started": "2024-12-14T04:45:15.659020Z"
    },
    "id": "V9yWKMCA-kk1",
    "outputId": "7086a9f7-3cf4-4cf8-a611-61e158bd29f2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: janome in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: mojimoji in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.0.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "start_ 猫 と 犬 が けんか し て いる _end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install janome mojimoji\n",
    "from janome.tokenizer import Tokenizer\n",
    "import mojimoji\n",
    "\n",
    "# Initialize Japanese tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def preprocess_japanese_text(text):\n",
    "\n",
    "    # TODO: Uncomment the line below to convert Japanese text to ASCII characters PROBLEM: cause the problem in tokenization\n",
    "    # text=japanese_unicode_to_ascii(text)\n",
    "\n",
    "    # Convert full-width characters to half-width\n",
    "    text = mojimoji.zen_to_han(text)\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text, wakati=True)\n",
    "\n",
    "    # Join tokens with a space to preserve word boundaries for translation models\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "    # Remove any unnecessary whitespace\n",
    "    preprocessed_text = preprocessed_text.strip()\n",
    "\n",
    "    return \"start_ \" + preprocessed_text + \" _end\"\n",
    "\n",
    "\n",
    "text = \"猫と犬がけんかしている。\"\n",
    "processed_text = preprocess_japanese_text(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:23.868983Z",
     "iopub.status.busy": "2024-12-14T04:45:23.868561Z",
     "iopub.status.idle": "2024-12-14T04:45:23.877064Z",
     "shell.execute_reply": "2024-12-14T04:45:23.876045Z",
     "shell.execute_reply.started": "2024-12-14T04:45:23.868939Z"
    },
    "id": "g3n8MNtg-kez",
    "outputId": "55611e9f-9c2e-4052-c096-3136860b8525",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_ hello how are you _end'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# def preprocess_english_text(text):\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#     # Normalize text\n",
    "#     text = text.lower()\n",
    "\n",
    "#     text=english_unicode_to_ascii(text)\n",
    "\n",
    "#     text=clean_text(text)\n",
    "\n",
    "#     # Tokenize text\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "\n",
    "#     # Join tokens with a space to preserve word boundaries for translation models\n",
    "#     preprocessed_text = \" \".join(tokens)\n",
    "\n",
    "#     # Remove any unnecessary whitespace\n",
    "#     preprocessed_text = preprocessed_text.strip()\n",
    "\n",
    "#     return \"start_ \" + preprocessed_text + \" _end\"\n",
    "\n",
    "#     # # Convert tokens to token IDs\n",
    "#     # token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "#     # # Add special tokens\n",
    "#     # token_ids = tokenizer.build_inputs_with_special_tokens(token_ids)\n",
    "\n",
    "#     # # Create attention mask\n",
    "#     # attention_mask = [1] * len(token_ids)\n",
    "\n",
    "#     # return token_ids, attention_mask\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def preprocess_english_text(text):\n",
    "\n",
    "    # Normalize text (you need to define these functions)\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = english_unicode_to_ascii(text)  # Placeholder for your normalization\n",
    "    text = clean_text(text)  # Placeholder for your cleaning function\n",
    "\n",
    "    # Add special tokens\n",
    "    preprocessed_text = f\"start_ {text} _end\"\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "token = preprocess_english_text(text)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:23.878784Z",
     "iopub.status.busy": "2024-12-14T04:45:23.878416Z",
     "iopub.status.idle": "2024-12-14T04:45:24.341576Z",
     "shell.execute_reply": "2024-12-14T04:45:24.340554Z",
     "shell.execute_reply.started": "2024-12-14T04:45:23.878746Z"
    },
    "id": "zNsTpC2B-kcP",
    "outputId": "0954db3a-7bd3-4d84-ef6d-24cf2efd5556",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing English text: 100%|██████████| 2000/2000 [00:00<00:00, 138932.54it/s]\n",
      "Preprocessing Japanese text: 100%|██████████| 2000/2000 [00:01<00:00, 1302.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm  import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing English text\")\n",
    "df[\"english_preprocessed\"] = df[\"english\"].progress_apply(preprocess_english_text)\n",
    "\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing Japanese text\")\n",
    "df[\"japanese_preprocessed\"] = df[\"japanese\"].progress_apply(preprocess_japanese_text)\n",
    "\n",
    "# df.to_csv(\"./datas/eng_jap_dataset_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMlaIsar9bAf"
   },
   "source": [
    "# tokenize\n",
    "tokenize each language word based on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.343795Z",
     "iopub.status.busy": "2024-12-14T04:45:24.343500Z",
     "iopub.status.idle": "2024-12-14T04:45:24.348796Z",
     "shell.execute_reply": "2024-12-14T04:45:24.347894Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.343767Z"
    },
    "id": "ACN_sW7u9bAh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    # transform a list of num sample into a 2D Numpy array of shape\n",
    "    # Fixed length because length of sequence of integers are different\n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.350247Z",
     "iopub.status.busy": "2024-12-14T04:45:24.349920Z",
     "iopub.status.idle": "2024-12-14T04:45:24.368445Z",
     "shell.execute_reply": "2024-12-14T04:45:24.367527Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.350221Z"
    },
    "id": "JCZ7huXZ9bAs",
    "outputId": "2f2d5e2d-6b48-44ac-b3c4-bd226552dcd0",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtokenize\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis place is good\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mこんにちは 今日は いい天気 。\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoday is so cold start_\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# e.g.\n",
    "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold start_\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDfQw_5t9bA5"
   },
   "source": [
    "# create clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.369941Z",
     "iopub.status.busy": "2024-12-14T04:45:24.369599Z",
     "iopub.status.idle": "2024-12-14T04:45:24.379375Z",
     "shell.execute_reply": "2024-12-14T04:45:24.378532Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.369906Z"
    },
    "id": "sIM8-10h9bA6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cleate a clean dataset\n",
    "def create_dataset( ja,en):\n",
    "\n",
    "    # input_tensor, target_tensor: 2d numpy array\n",
    "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
    "    input_tensor, input_lang_tokenize = tokenize(ja)\n",
    "    target_tensor, target_lang_tokenize = tokenize(en)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.380527Z",
     "iopub.status.busy": "2024-12-14T04:45:24.380267Z",
     "iopub.status.idle": "2024-12-14T04:45:24.419482Z",
     "shell.execute_reply": "2024-12-14T04:45:24.418680Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.380502Z"
    },
    "id": "jEJJ9BMgATdD",
    "outputId": "a9339169-184b-48de-fe54-0f2bb2d2b9fc",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(df[\"japanese_preprocessed\"],df[\"english_preprocessed\"])\n",
    "(len(input_tensor), len(target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.420929Z",
     "iopub.status.busy": "2024-12-14T04:45:24.420601Z",
     "iopub.status.idle": "2024-12-14T04:45:24.428343Z",
     "shell.execute_reply": "2024-12-14T04:45:24.427547Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.420902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved as tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define paths\n",
    "kaggle = True  # Set to True if running on Kaggle\n",
    "input_tokenizer_saving_path = \"/kaggle/working/output/input_tokenizer.pkl\" if kaggle else \"./output/input_tokenizer.pkl\"\n",
    "target_tokenizer_saving_path = \"/kaggle/working/output/target_tokenizer.pkl\" if kaggle else \"./output/target_tokenizer.pkl\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(input_tokenizer_saving_path), exist_ok=True)\n",
    "\n",
    "# Save the tokenizer to a .pkl file\n",
    "with open(input_tokenizer_saving_path, 'wb') as file:\n",
    "    pickle.dump(input_lang_tokenize, file)\n",
    "\n",
    "with open(target_tokenizer_saving_path, 'wb') as file:\n",
    "    pickle.dump(target_lang_tokenize, file)\n",
    "    \n",
    "print(\"Tokenizer saved as tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.429750Z",
     "iopub.status.busy": "2024-12-14T04:45:24.429434Z",
     "iopub.status.idle": "2024-12-14T04:45:24.439788Z",
     "shell.execute_reply": "2024-12-14T04:45:24.438911Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.429722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Later, load the tokenizer back\n",
    "with open(input_tokenizer_saving_path, 'rb') as file:\n",
    "    input_lang_tokenize_test = pickle.load(file)\n",
    "\n",
    "with open(target_tokenizer_saving_path, 'rb') as file:\n",
    "    target_lang_tokenize_test = pickle.load(file)\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "# print(input_lang_tokenize_test.word_index) \n",
    "# print(input_lang_tokenize_test.word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.442909Z",
     "iopub.status.busy": "2024-12-14T04:45:24.442535Z",
     "iopub.status.idle": "2024-12-14T04:45:24.451840Z",
     "shell.execute_reply": "2024-12-14T04:45:24.450929Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.442881Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 87, 3, 46, 384, 2]], [[1, 115, 8, 72, 297, 2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input_jap = preprocess_japanese_text(\"こんにちは 今日は いい天気 。\")  # Adjust for your input language\n",
    "preprocessed_input_eng = preprocess_english_text(\"today is so cold\")  \n",
    "input_sequence_jap = input_lang_tokenize.texts_to_sequences([preprocessed_input_jap])\n",
    "input_sequence_eng = target_lang_tokenize.texts_to_sequences([preprocessed_input_eng])\n",
    "(input_sequence_jap, input_sequence_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.452974Z",
     "iopub.status.busy": "2024-12-14T04:45:24.452716Z",
     "iopub.status.idle": "2024-12-14T04:45:24.463703Z",
     "shell.execute_reply": "2024-12-14T04:45:24.463032Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.452937Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 87, 3, 46, 384, 2]], [[1, 115, 8, 72, 297, 2]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_input_jap = preprocess_japanese_text(\"こんにちは 今日は いい天気 。\")  # Adjust for your input language\n",
    "preprocessed_input_eng = preprocess_english_text(\"today is so cold\")  \n",
    "input_sequence_jap = input_lang_tokenize_test.texts_to_sequences([preprocessed_input_jap])\n",
    "input_sequence_eng = target_lang_tokenize_test.texts_to_sequences([preprocessed_input_eng])\n",
    "(input_sequence_jap, input_sequence_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.464930Z",
     "iopub.status.busy": "2024-12-14T04:45:24.464637Z",
     "iopub.status.idle": "2024-12-14T04:45:24.473041Z",
     "shell.execute_reply": "2024-12-14T04:45:24.472246Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.464891Z"
    },
    "id": "lzlBqTlO9bA-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def max_length(input_tensor, target_tensor):\n",
    "\n",
    "    # max length of input sentense and target sentense\n",
    "    english_len = [len(i) for i in target_tensor ]\n",
    "\n",
    "    japanese_len = [len(i) for i in input_tensor]\n",
    "\n",
    "     # print max length\n",
    "    print(\"english length:\", max(english_len))\n",
    "    print(\"japanese length:\", max(japanese_len))\n",
    "\n",
    "    max_len_input=  max(japanese_len)\n",
    "    max_len_target =  max(english_len)\n",
    "\n",
    "    return max_len_input, max_len_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.474293Z",
     "iopub.status.busy": "2024-12-14T04:45:24.473969Z",
     "iopub.status.idle": "2024-12-14T04:45:24.487693Z",
     "shell.execute_reply": "2024-12-14T04:45:24.486822Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.474253Z"
    },
    "id": "Ew-q24gJ9bBB",
    "outputId": "59eee1bb-8b58-48e8-daab-76b0d1026ab0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52]\n",
      "english length: 52\n",
      "japanese length: 82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82, 52)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_input, max_length_target = max_length(input_tensor, target_tensor)\n",
    "(max_length_input, max_length_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.488930Z",
     "iopub.status.busy": "2024-12-14T04:45:24.488664Z",
     "iopub.status.idle": "2024-12-14T04:45:24.498782Z",
     "shell.execute_reply": "2024-12-14T04:45:24.497928Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.488907Z"
    },
    "id": "Y_HvHj059bBG",
    "outputId": "e4d54bc7-664d-499f-ca3e-7d82264430c7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 1600 200 200 200 200\n",
      "(1600, 82) (1600, 52) (200, 82) (200, 52) (200, 82) (200, 52)\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "X_train, X_test, \\\n",
    "    Y_train, Y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_test, X_val, \\\n",
    "    Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, shuffle=True)\n",
    "\n",
    "\n",
    "# show length\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test), len(X_val), len(Y_val))\n",
    "print(X_train.shape, Y_train.shape, X_test.shape,Y_test.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.499866Z",
     "iopub.status.busy": "2024-12-14T04:45:24.499587Z",
     "iopub.status.idle": "2024-12-14T04:45:24.507100Z",
     "shell.execute_reply": "2024-12-14T04:45:24.506194Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.499841Z"
    },
    "id": "w12Za_JK9bBJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.508257Z",
     "iopub.status.busy": "2024-12-14T04:45:24.508021Z",
     "iopub.status.idle": "2024-12-14T04:45:24.517507Z",
     "shell.execute_reply": "2024-12-14T04:45:24.516674Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.508233Z"
    },
    "id": "v6yT2odT9bBP",
    "outputId": "09a7fc25-c9d0-4358-f751-bcc6c97e03a8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "1----->start_\n",
      "12----->私\n",
      "9----->が\n",
      "532----->正しい\n",
      "15----->と\n",
      "1275----->わかる\n",
      "66----->でしょ\n",
      "29----->う\n",
      "2----->_end\n",
      "output lang: index to word mapping\n",
      "1----->start_\n",
      "7----->you\n",
      "38----->will\n",
      "128----->see\n",
      "15----->that\n",
      "4----->i\n",
      "90----->am\n",
      "192----->right\n",
      "2----->_end\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(input_lang_tokenize, X_train[10])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(target_lang_tokenize, Y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSHdoFwQ9bBT"
   },
   "source": [
    "# define parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.518995Z",
     "iopub.status.busy": "2024-12-14T04:45:24.518509Z",
     "iopub.status.idle": "2024-12-14T04:45:24.937304Z",
     "shell.execute_reply": "2024-12-14T04:45:24.936428Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.518970Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train steps per epoch: 25\n",
      "Validation steps per epoch: 3\n",
      "Total unique words in the input: 3324\n",
      "Total unique words in the target: 2962\n",
      "Vocabulary input size: 3325\n",
      "Vocabulary target size: 2963\n",
      "File_name: config_B64_D0.3_E300_U512\n"
     ]
    }
   ],
   "source": [
    "# config_B64_D0.3_E300_U512\n",
    "# first_run ensure that the value printed below is never 0\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration details\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "dropout_rate = 0.3\n",
    "embedding_dim = 300\n",
    "units = 512\n",
    "\n",
    "# Calculating steps per epoch\n",
    "train_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "val_steps_per_epoch = len(X_val) // BATCH_SIZE\n",
    "\n",
    "print(f\"Train steps per epoch: {train_steps_per_epoch}\")\n",
    "print(f\"Validation steps per epoch: {val_steps_per_epoch}\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
    "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
    "\n",
    "print(f\"Total unique words in the input: {len(input_lang_tokenize.word_index)}\")\n",
    "print(f\"Total unique words in the target: {len(target_lang_tokenize.word_index)}\")\n",
    "print(f\"Vocabulary input size: {vocab_inp_size}\")\n",
    "print(f\"Vocabulary target size: {vocab_tar_size}\")\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f\"File_name: config_B{BATCH_SIZE}_D{dropout_rate}_E{embedding_dim}_U{units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 82), dtype=int32, numpy=\n",
       " array([[   1,   12,    3, ...,    0,    0,    0],\n",
       "        [   1,   12,    3, ...,    0,    0,    0],\n",
       "        [   1,   13,    3, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1, 1119,    8, ...,    0,    0,    0],\n",
       "        [   1,    3,   46, ...,    0,    0,    0],\n",
       "        [   1, 1402,  209, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 52), dtype=int32, numpy=\n",
       " array([[  1,   4,  39, ...,   0,   0,   0],\n",
       "        [  1,   4,  43, ...,   0,   0,   0],\n",
       "        [  1,  16,  11, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [  1,  84,  19, ...,   0,   0,   0],\n",
       "        [  1,  22, 175, ...,   0,   0,   0],\n",
       "        [  1,   4,  39, ...,   0,   0,   0]])>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1600, 82)\n",
      "Y_train shape: (1600, 52)\n",
      "X_val shape: (200, 82)\n",
      "Y_val shape: (200, 52)\n",
      "X_test shape: (200, 82)\n",
      "Y_test shape: (200, 52)\n"
     ]
    }
   ],
   "source": [
    "assert X_train is not None, \"X_train is None\"\n",
    "assert Y_train is not None, \"Y_train is None\"\n",
    "assert X_val is not None, \"X_val is None\"\n",
    "assert Y_val is not None, \"Y_val is None\"\n",
    "\n",
    "# Optionally, you can also check the shapes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Y_val shape: {Y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.any(np.isnan(X_train)))  # Check if any NaN values in X_train\n",
    "print(np.any(np.isnan(Y_train)))  # Check if any NaN values in Y_train\n",
    "print(np.any(np.isnan(X_val)))    # Check if any NaN values in X_val\n",
    "print(np.any(np.isnan(Y_val)))    # Check if any NaN values in Y_val\n",
    "\n",
    "print(np.any(X_train == None))    # Check if there are any None values\n",
    "print(np.any(Y_train == None))    # Check if there are any None values\n",
    "print(np.any(X_val == None))      # Check if there are any None values\n",
    "print(np.any(Y_val == None))      # Check if there are any None values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_seq           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_seq          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">997,500</span> │ input_seq[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_13        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">888,900</span> │ target_seq[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,665,024</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,665,024</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │ lstm_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │ lstm_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_6   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)    │                   │            │ lstm_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2963</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,520,019</span> │ attention_layer_… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_seq           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ target_seq          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │    \u001b[38;5;34m997,500\u001b[0m │ input_seq[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_13        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │    \u001b[38;5;34m888,900\u001b[0m │ target_seq[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m512\u001b[0m), │  \u001b[38;5;34m1,665,024\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m512\u001b[0m), │  \u001b[38;5;34m1,665,024\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │ lstm_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │ lstm_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer_6   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ lstm_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)    │                   │            │ lstm_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2963\u001b[0m)      │  \u001b[38;5;34m1,520,019\u001b[0m │ attention_layer_… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,736,467</span> (25.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,736,467\u001b[0m (25.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,736,467</span> (25.70 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,736,467\u001b[0m (25.70 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 1 of layer \"functional_6\" is incompatible with the layer: expected shape=(None, 52), found shape=(64, 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m seq2seq_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mseq2seq_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m     \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# Adjust these variables with your data\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     58\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 1 of layer \"functional_6\" is incompatible with the layer: expected shape=(None, 52), found shape=(64, 82)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention, Layer, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
    "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
    "embedding_dim = 300\n",
    "units = 512\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Define input sequences\n",
    "input_seq = Input(shape=(max_length_input,), name='input_seq')\n",
    "target_seq = Input(shape=(max_length_target,), name='target_seq')\n",
    "\n",
    "# Encoder\n",
    "encoder_embedding = Embedding(vocab_inp_size, embedding_dim)(input_seq)\n",
    "encoder_dropout = Dropout(dropout_rate)(encoder_embedding)\n",
    "encoder_lstm, state_h, state_c = LSTM(units, return_state=True, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Decoder\n",
    "decoder_embedding = Embedding(vocab_tar_size, embedding_dim)(target_seq)\n",
    "decoder_dropout = Dropout(dropout_rate)(decoder_embedding)\n",
    "decoder_lstm, _, _ = LSTM(units, return_state=True, return_sequences=True)(decoder_dropout, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def call(self, inputs):\n",
    "        decoder_lstm, encoder_lstm = inputs\n",
    "        attention_output = Attention()([decoder_lstm, encoder_lstm])\n",
    "        context_vector = tf.reduce_sum(attention_output, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# Apply Attention Layer\n",
    "context_vector = AttentionLayer()([decoder_lstm, encoder_lstm])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(vocab_tar_size, activation='softmax')(context_vector)\n",
    "\n",
    "# Define model\n",
    "seq2seq_model = Model(inputs=[input_seq, target_seq], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "seq2seq_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "seq2seq_model.summary()\n",
    "\n",
    "# Training the model\n",
    "history = seq2seq_model.fit(\n",
    "    [X_train, X_train],\n",
    "     Y_train,# Adjust these variables with your data\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=[X_val, Y_val],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_translation_model(japanese_vocab_size, english_vocab_size, embedding_dim=256, units=512):\n",
    "    # Encoder (Japanese -> hidden states)\n",
    "    encoder_input = layers.Input(shape=(max_length_input,), name='encoder_input')\n",
    "    encoder_embedding = layers.Embedding(input_dim=japanese_vocab_size, output_dim=embedding_dim)(encoder_input)\n",
    "\n",
    "    # First LSTM layer\n",
    "    encoder_lstm1 = layers.LSTM(units, return_sequences=True, return_state=True, name='encoder_lstm1')\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_embedding)\n",
    "\n",
    "    # Second LSTM layer (takes output from the first layer)\n",
    "    encoder_lstm2 = layers.LSTM(units, return_state=True, name='encoder_lstm2')\n",
    "    encoder_output, state_h, state_c = encoder_lstm2(encoder_output1)\n",
    "\n",
    "    encoder_states = [state_h, state_c] # States from the second LSTM layer\n",
    "\n",
    "    # Decoder (English -> output sequence)\n",
    "    decoder_input = layers.Input(shape=(max_length_target,), name='decoder_input')\n",
    "    decoder_embedding = layers.Embedding(input_dim=english_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
    "    decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    # Reshape encoder output to match the attention mechanism's expected shape\n",
    "    encoder_output_expanded = layers.Reshape(target_shape=(-1, units))(encoder_output)\n",
    "\n",
    "    # Attention layer that combines the encoder and decoder outputs\n",
    "    attention_layer = layers.Attention(name='attention_layer')\n",
    "    attention_output = attention_layer([decoder_output, encoder_output_expanded])\n",
    "\n",
    "    # Concatenate attention output with decoder LSTM output\n",
    "    concat_layer = layers.Concatenate(axis=-1, name='concat_layer')([decoder_output, attention_output])\n",
    "\n",
    "    # Output layer\n",
    "    decoder_dense = layers.Dense(english_vocab_size, activation='softmax', name='decoder_output')\n",
    "    output = decoder_dense(concat_layer)\n",
    "\n",
    "    # Build the model\n",
    "    model = tf.keras.Model([encoder_input, decoder_input], output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_translation_model(vocab_inp_size, vocab_tar_size, embedding_dim, units)\n",
    "\n",
    "# Model summary to inspect architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Update model training with callbacks\n",
    "history = model.fit(\n",
    "    [X_train, X_train],\n",
    "    Y_train,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_test, X_test], Y_test),\n",
    "    callbacks=[early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjAVocfI9bCI"
   },
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.938779Z",
     "iopub.status.busy": "2024-12-14T04:45:24.938447Z",
     "iopub.status.idle": "2024-12-14T04:45:24.943268Z",
     "shell.execute_reply": "2024-12-14T04:45:24.942424Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.938745Z"
    },
    "id": "CJWSAHhl9bCJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# @register_keras_serializable()\n",
    "# class Encoder(tf.keras.Model):\n",
    "#     def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.enc_units = enc_units\n",
    "#         self.dropout = Dropout(dropout_rate)\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "#                                                             return_sequences=True,\n",
    "#                                                             recurrent_initializer='glorot_uniform')\n",
    "\n",
    "#         self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "#                                                     return_sequences=True,\n",
    "#                                                     return_state=True,\n",
    "#                                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "#     def call(self, x, hidden):\n",
    "#         x = self.embedding(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.first_lstm(x, initial_state =hidden)\n",
    "#         output, state_h, state_c = self.final_lstm(x)\n",
    "#         state = [state_h, state_c ]\n",
    "\n",
    "#         return output, state\n",
    "\n",
    "#     def initialize_hidden_state(self):\n",
    "#             return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.945742Z",
     "iopub.status.busy": "2024-12-14T04:45:24.945431Z",
     "iopub.status.idle": "2024-12-14T04:45:24.956393Z",
     "shell.execute_reply": "2024-12-14T04:45:24.955664Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.945716Z"
    },
    "id": "V53dyDZO_F35",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:24.957562Z",
     "iopub.status.busy": "2024-12-14T04:45:24.957306Z",
     "iopub.status.idle": "2024-12-14T04:45:24.990281Z",
     "shell.execute_reply": "2024-12-14T04:45:24.989651Z",
     "shell.execute_reply.started": "2024-12-14T04:45:24.957532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.first_lstm = LSTM(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.final_lstm = LSTM(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.first_lstm(x, initial_state=hidden)\n",
    "        output, state_h, state_c = self.final_lstm(x)\n",
    "        state = [state_h, state_c]\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return (\n",
    "            tf.zeros((self.batch_size, self.enc_units)),\n",
    "            tf.zeros((self.batch_size, self.enc_units)),\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Encoder, self).get_config()\n",
    "        config.update({\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"enc_units\": self.enc_units,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "    @classmethod\n",
    "    def build_from_config(cls, config):\n",
    "        # Simply reuse from_config to build from a configuration\n",
    "        return cls.from_config(config)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSn1VowJ9bCS"
   },
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:25.392941Z",
     "iopub.status.busy": "2024-12-14T04:45:25.392192Z",
     "iopub.status.idle": "2024-12-14T04:45:25.397417Z",
     "shell.execute_reply": "2024-12-14T04:45:25.396328Z",
     "shell.execute_reply.started": "2024-12-14T04:45:25.392906Z"
    },
    "id": "cctmI1mA9bCS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# @register_keras_serializable()\n",
    "# class Attention(tf.keras.models.Model):\n",
    "\n",
    "#     def __init__(self, units: int, *args, **kwargs):\n",
    "\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.units = units\n",
    "\n",
    "#         self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
    "#         self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
    "#         self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
    "#         self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
    "\n",
    "#     def call(self, input, memory):\n",
    "\n",
    "#         q = self.q_dense_layer(input)\n",
    "#         k = self.k_dense_layer(memory)\n",
    "#         v = self.v_dense_layer(memory)\n",
    "\n",
    "#         depth = self.units // 2\n",
    "#         q *= depth ** -0.5  # for scaled dot production\n",
    "\n",
    "#         # caluclate relation between query and key\n",
    "#         logit = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "#         attention_weight = tf.nn.softmax(logit)\n",
    "\n",
    "#         attention_output = tf.matmul(attention_weight, v)\n",
    "#         return self.output_dense_layer(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBBPzDxK9bCg"
   },
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.033951Z",
     "iopub.status.busy": "2024-12-14T04:45:26.033552Z",
     "iopub.status.idle": "2024-12-14T04:45:26.038717Z",
     "shell.execute_reply": "2024-12-14T04:45:26.037786Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.033921Z"
    },
    "id": "1nED5gCZ9bCh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# @register_keras_serializable()\n",
    "# class Decoder(tf.keras.Model):\n",
    "#     def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.dec_units = dec_units\n",
    "#         self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.dropout = Dropout(dropout_rate)\n",
    "#         self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "#                                                             return_sequences=True)\n",
    "#         self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "#                                                             return_sequences=True,\n",
    "#                                                             return_state=True)\n",
    "\n",
    "#         self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "#         self.attention = Attention(self.dec_units)\n",
    "\n",
    "#     def call(self, x, hidden, enc_output):\n",
    "#         x = self.embedding(x)\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         x =  self.first_lstm(x)\n",
    "#         output, state_h, state_c = self.final_lstm(x)\n",
    "#         state = [state_h, state_c]\n",
    "#         attention_weights = self.attention(output, enc_output)\n",
    "#         output = tf.concat([output, attention_weights], axis=-1)\n",
    "\n",
    "\n",
    "#         output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "#         output = self.fc(output)\n",
    "\n",
    "#         return  output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.535990Z",
     "iopub.status.busy": "2024-12-14T04:45:26.535230Z",
     "iopub.status.idle": "2024-12-14T04:45:26.539623Z",
     "shell.execute_reply": "2024-12-14T04:45:26.538584Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.535954Z"
    },
    "id": "37-lrVTN_F4X",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.572759Z",
     "iopub.status.busy": "2024-12-14T04:45:26.572402Z",
     "iopub.status.idle": "2024-12-14T04:45:26.601211Z",
     "shell.execute_reply": "2024-12-14T04:45:26.600518Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.572730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units: int, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "        # Dense layers for Q, K, and V\n",
    "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
    "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
    "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
    "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
    "\n",
    "    def __call__(self, query, memory, mask=None):\n",
    "        # Apply dense layers for Q, K, V\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "\n",
    "        # Scale the query vector for stable dot-product\n",
    "        depth = self.units ** 0.5\n",
    "        q *= depth ** -0.5  # scaling for stability\n",
    "\n",
    "        # Compute the attention logits (query-key interaction)\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        # Apply mask (optional) for padding or other unwanted areas\n",
    "        if mask is not None:\n",
    "            logits += (mask * -1e9)\n",
    "\n",
    "        # Normalize with softmax to get attention weights\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "        # Compute the output by weighting the values\n",
    "        attention_output = tf.matmul(attention_weights, v)\n",
    "        return self.output_dense_layer(attention_output)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.dropout_rate=dropout_rate\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True)\n",
    "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return the arguments that were passed to __init__\n",
    "        config = super(Decoder, self).get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'dec_units': self.dec_units,\n",
    "            'batch_size': self.batch_size,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Create the instance from config\n",
    "        return cls(**config)\n",
    "\n",
    "    def __call__(self, x, hidden, enc_output, mask=None):  # Ensure mask is a keyword argument\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # First LSTM layer\n",
    "        x = self.first_lstm(x)\n",
    "\n",
    "        # Final LSTM layer\n",
    "        output, state_h, state_c = self.final_lstm(x)\n",
    "        state = [state_h, state_c]\n",
    "\n",
    "        # Apply attention mechanism, passing mask as a keyword argument\n",
    "        attention_weights = self.attention(output, enc_output, mask=mask)\n",
    "\n",
    "        # Concatenate attention output with LSTM output\n",
    "        output = tf.concat([output, attention_weights], axis=-1)\n",
    "\n",
    "        # Reshape to match the dimensions for the final dense layer\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # Final dense layer to predict vocabulary\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "# Instantiate the decoder\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyXf1bn9bC1"
   },
   "source": [
    "# optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.651722Z",
     "iopub.status.busy": "2024-12-14T04:45:26.651379Z",
     "iopub.status.idle": "2024-12-14T04:45:26.666087Z",
     "shell.execute_reply": "2024-12-14T04:45:26.665289Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.651691Z"
    },
    "id": "wROZueWU9bC3",
    "outputId": "a4428ca8-c26b-46b6-e0e4-f73aae812581",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "def calculate_accuracy(real, pred):\n",
    "    # Ensure both tensors have the same dtype\n",
    "    real = tf.cast(real, dtype=tf.int32)\n",
    "    pred_tokens = tf.argmax(pred, axis=-1, output_type=tf.int32)  # Ensure int32 output\n",
    "\n",
    "    correct = tf.equal(real, pred_tokens)  # Compare predicted and actual tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # Mask padding tokens\n",
    "    correct = tf.math.logical_and(correct, mask)  # Only consider non-padding tokens\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))  # Average accuracy\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtDLuqB49bC7"
   },
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.705756Z",
     "iopub.status.busy": "2024-12-14T04:45:26.705384Z",
     "iopub.status.idle": "2024-12-14T04:45:26.711421Z",
     "shell.execute_reply": "2024-12-14T04:45:26.710499Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.705724Z"
    },
    "id": "0CWEvq3W9bC8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './train_checkpoints_aniki_data---tryyyy' if not kaggle else '/kaggle/working//train_checkpoints_aniki_data---tryyyy'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "checkpoint_second = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"model_checkpoint.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja0mhQ4j9bDB"
   },
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.769331Z",
     "iopub.status.busy": "2024-12-14T04:45:26.769020Z",
     "iopub.status.idle": "2024-12-14T04:45:26.776840Z",
     "shell.execute_reply": "2024-12-14T04:45:26.775806Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.769293Z"
    },
    "id": "YQCFFTZOSJoG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(EPOCHS, train_loss_history, val_loss_history, train_accuracy_history, val_accuracy_history):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss and accuracy per epoch.\n",
    "\n",
    "    Args:\n",
    "        EPOCHS (int): Number of training epochs.\n",
    "        train_loss_history (list): Training loss history.\n",
    "        val_loss_history (list): Validation loss history.\n",
    "        train_accuracy_history (list): Training accuracy history.\n",
    "        val_accuracy_history (list): Validation accuracy history.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, EPOCHS + 1), train_loss_history, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_loss_history, label='Validation Loss', marker='o')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, EPOCHS + 1), train_accuracy_history, label='Train Accuracy', marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_accuracy_history, label='Validation Accuracy', marker='o')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.802407Z",
     "iopub.status.busy": "2024-12-14T04:45:26.802113Z",
     "iopub.status.idle": "2024-12-14T04:45:26.809419Z",
     "shell.execute_reply": "2024-12-14T04:45:26.808513Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.802381Z"
    },
    "id": "TQSV9kkd9bDB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add accuracy calculation to the training step\n",
    "@tf.function\n",
    "def train_step_with_accuracy(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Calculate accuracy for this step\n",
    "            batch_accuracy = calculate_accuracy(targ[:, t], predictions)\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "            # Use teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    batch_accuracy = total_accuracy / int(targ.shape[1])\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.831145Z",
     "iopub.status.busy": "2024-12-14T04:45:26.830592Z",
     "iopub.status.idle": "2024-12-14T04:45:26.834667Z",
     "shell.execute_reply": "2024-12-14T04:45:26.833811Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.831115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first_run change this to epoch 2 for the first run\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:26.866131Z",
     "iopub.status.busy": "2024-12-14T04:45:26.865581Z",
     "iopub.status.idle": "2024-12-14T04:45:50.593576Z",
     "shell.execute_reply": "2024-12-14T04:45:50.592675Z",
     "shell.execute_reply.started": "2024-12-14T04:45:26.866101Z"
    },
    "id": "rCI9q4-C9bDI",
    "outputId": "ad4fc8b4-5b0d-4638-fc6b-e2f153919d6c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Restore the latest checkpoint if it exists\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "    print(f\"Restored from {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch.\")\n",
    "\n",
    "\n",
    "# Lists to store metrics\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "val_loss_history = []\n",
    "val_accuracy_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Training loop\n",
    "    for (batch, (inp, targ)) in enumerate(tqdm(train_dataset.take(train_steps_per_epoch), desc=\"Training\", leave=False)):\n",
    "        train_batch_loss, train_batch_accuracy = train_step_with_accuracy(inp, targ, enc_hidden)\n",
    "        train_loss += train_batch_loss\n",
    "        train_accuracy += train_batch_accuracy\n",
    "\n",
    "    # Validation loop\n",
    "    for (batch, (val_inp, val_tar)) in enumerate(tqdm(val_dataset.take(val_steps_per_epoch), desc=\"Validation\", leave=False)):\n",
    "        val_batch_loss, val_batch_accuracy = train_step_with_accuracy(val_inp, val_tar, enc_hidden)\n",
    "        val_loss += val_batch_loss\n",
    "        val_accuracy += val_batch_accuracy\n",
    "\n",
    "    # Save metrics for this epoch\n",
    "    train_loss /= train_steps_per_epoch\n",
    "    train_accuracy /= train_steps_per_epoch\n",
    "    val_loss /= val_steps_per_epoch\n",
    "    val_accuracy /= val_steps_per_epoch\n",
    "\n",
    "    train_loss_history.append(train_loss.numpy())\n",
    "    train_accuracy_history.append(train_accuracy.numpy())\n",
    "    val_loss_history.append(val_loss.numpy())\n",
    "    val_accuracy_history.append(val_accuracy.numpy())\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "plot_training_metrics(\n",
    "    EPOCHS=EPOCHS,\n",
    "    train_loss_history=train_loss_history,\n",
    "    val_loss_history=val_loss_history,\n",
    "    train_accuracy_history=train_accuracy_history,\n",
    "    val_accuracy_history=val_accuracy_history\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:50.595354Z",
     "iopub.status.busy": "2024-12-14T04:45:50.595066Z",
     "iopub.status.idle": "2024-12-14T04:45:51.172975Z",
     "shell.execute_reply": "2024-12-14T04:45:51.172115Z",
     "shell.execute_reply.started": "2024-12-14T04:45:50.595326Z"
    },
    "id": "fheWYJEnSVVn",
    "outputId": "5faba64f-a818-4901-ddcf-8a285555c447",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_training_metrics(\n",
    "    EPOCHS=EPOCHS,\n",
    "    train_loss_history=train_loss_history,\n",
    "    val_loss_history=val_loss_history,\n",
    "    train_accuracy_history=train_accuracy_history,\n",
    "    val_accuracy_history=val_accuracy_history\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOhXwpZlDwJp"
   },
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.174467Z",
     "iopub.status.busy": "2024-12-14T04:45:51.174115Z",
     "iopub.status.idle": "2024-12-14T04:45:51.329408Z",
     "shell.execute_reply": "2024-12-14T04:45:51.328584Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.174428Z"
    },
    "id": "RVDzzIKJBsBU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(f\"{checkpoint_dir}/ckpt-{EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.332401Z",
     "iopub.status.busy": "2024-12-14T04:45:51.331710Z",
     "iopub.status.idle": "2024-12-14T04:45:51.336219Z",
     "shell.execute_reply": "2024-12-14T04:45:51.335264Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.332370Z"
    },
    "id": "nQnK48R0BuZT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# into base model\n",
    "encoder = checkpoint.encoder\n",
    "decoder = checkpoint.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gu5bPpdEHVz"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Check bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.337405Z",
     "iopub.status.busy": "2024-12-14T04:45:51.337184Z",
     "iopub.status.idle": "2024-12-14T04:45:51.347964Z",
     "shell.execute_reply": "2024-12-14T04:45:51.347097Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.337383Z"
    },
    "id": "wJ8r9JNREs01",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(sentence,encoder,decoder,target_lang_tokenize):\n",
    "    inputs = tf.convert_to_tensor(sentence)\n",
    "    result = ''\n",
    "    inputs = tf.expand_dims(inputs, axis=0)\n",
    "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state = decoder(dec_input,\n",
    "                                                             hidden_state,\n",
    "                                                             enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.349133Z",
     "iopub.status.busy": "2024-12-14T04:45:51.348894Z",
     "iopub.status.idle": "2024-12-14T04:45:51.360343Z",
     "shell.execute_reply": "2024-12-14T04:45:51.359644Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.349109Z"
    },
    "id": "87pBzvXjRkJL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_reference(lang, tensor):\n",
    "    all_sentence_list = []\n",
    "\n",
    "    for word_list in tensor:\n",
    "      sentence_list = []\n",
    "\n",
    "      for t in word_list:\n",
    "          if not t == 0:\n",
    "              # Index number assigned to each word\n",
    "              sentence_list.append(lang.index_word[t])\n",
    "      all_sentence_list.append(sentence_list)\n",
    "    return all_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.361424Z",
     "iopub.status.busy": "2024-12-14T04:45:51.361156Z",
     "iopub.status.idle": "2024-12-14T04:45:51.369471Z",
     "shell.execute_reply": "2024-12-14T04:45:51.368626Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.361399Z"
    },
    "id": "JGwwlmHtRrfb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "limit=1000\n",
    "# create reference\n",
    "reference = create_reference(target_lang_tokenize, Y_test.tolist()[:limit])\n",
    "jap_reference = create_reference(input_lang_tokenize, X_test.tolist()[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:51.370719Z",
     "iopub.status.busy": "2024-12-14T04:45:51.370354Z",
     "iopub.status.idle": "2024-12-14T04:45:54.946522Z",
     "shell.execute_reply": "2024-12-14T04:45:54.945587Z",
     "shell.execute_reply.started": "2024-12-14T04:45:51.370682Z"
    },
    "id": "UAAMX-vQEKB_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# create hypothesis\n",
    "hypothesis = []\n",
    "for i in tqdm(X_test[:limit]):\n",
    "  hypothesis.append(predict(i,encoder,decoder,target_lang_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:54.948528Z",
     "iopub.status.busy": "2024-12-14T04:45:54.947853Z",
     "iopub.status.idle": "2024-12-14T04:45:54.957157Z",
     "shell.execute_reply": "2024-12-14T04:45:54.956133Z",
     "shell.execute_reply.started": "2024-12-14T04:45:54.948487Z"
    },
    "id": "-i0Jl_21Ee7k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "score = 0\n",
    "smoothie = SmoothingFunction().method2\n",
    "for i in range(len(reference)):\n",
    "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
    "\n",
    "score /= len(reference)\n",
    "print(\"The bleu score is: \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:45:54.960282Z",
     "iopub.status.busy": "2024-12-14T04:45:54.959929Z",
     "iopub.status.idle": "2024-12-14T04:45:54.980542Z",
     "shell.execute_reply": "2024-12-14T04:45:54.979726Z",
     "shell.execute_reply.started": "2024-12-14T04:45:54.960245Z"
    },
    "id": "C9Q9_v3uxblE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_references = [\" \".join(ref[1:-1]) for ref in reference]\n",
    "processed_jap_references = [\" \".join(ref[1:-1]) for ref in jap_reference]\n",
    "processed_hypothesis = [re.sub(r\"_end\", \"\", hyp).strip() for hyp in hypothesis]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Jap_reference\": processed_jap_references,\n",
    "    \"Hypothesis\": processed_hypothesis,\n",
    "    \"Reference\": processed_references\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T13:11:35.619362Z",
     "iopub.status.busy": "2024-12-13T13:11:35.619010Z",
     "iopub.status.idle": "2024-12-13T13:11:35.632206Z",
     "shell.execute_reply": "2024-12-13T13:11:35.631275Z",
     "shell.execute_reply.started": "2024-12-13T13:11:35.619334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "history_df = pd.DataFrame({\n",
    "    \"Train Loss\": train_loss_history,\n",
    "    \"Train Accuracy\": train_accuracy_history,\n",
    "    \"Validation Loss\": val_loss_history,\n",
    "    \"Validation Accuracy\": val_accuracy_history\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "history_df.to_csv(f\"{checkpoint_dir}/training_history.csv\", index=False)\n",
    "\n",
    "print(\"Training history saved to 'training_history.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:46:28.822818Z",
     "iopub.status.busy": "2024-12-14T04:46:28.822110Z",
     "iopub.status.idle": "2024-12-14T04:46:29.159809Z",
     "shell.execute_reply": "2024-12-14T04:46:29.159063Z",
     "shell.execute_reply.started": "2024-12-14T04:46:28.822782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "encoder_saving_path=\"/kaggle/working/output/encoder_model.keras\" if kaggle else \"./output/encoder_model.keras\"\n",
    "decoder_saving_path=\"/kaggle/working/output/decoder_model.keras\" if kaggle else \"./output/decoder_model.keras\"\n",
    "\n",
    "os.makedirs(os.path.dirname(encoder_saving_path), exist_ok=True)\n",
    "print(encoder.built)  # Returns True if the model is built\n",
    "\n",
    "# Save encoder and decoder in Keras format\n",
    "encoder.save(encoder_saving_path)\n",
    "decoder.save(decoder_saving_path)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the encoder model\n",
    "loaded_encoder = tf.keras.models.load_model(encoder_saving_path, custom_objects={\"Encoder\": Encoder})\n",
    "\n",
    "# Load the decoder model\n",
    "loaded_decoder = tf.keras.models.load_model(decoder_saving_path, custom_objects={'Decoder': Decoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T04:46:36.116463Z",
     "iopub.status.busy": "2024-12-14T04:46:36.115835Z",
     "iopub.status.idle": "2024-12-14T04:46:39.244812Z",
     "shell.execute_reply": "2024-12-14T04:46:39.243873Z",
     "shell.execute_reply.started": "2024-12-14T04:46:36.116429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# create hypothesis\n",
    "hypothesis = []\n",
    "for i in tqdm(X_test[:limit]):\n",
    "  hypothesis.append(predict(i,loaded_encoder,loaded_decoder,target_lang_tokenize))\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "score = 0\n",
    "smoothie = SmoothingFunction().method2\n",
    "for i in range(len(reference)):\n",
    "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
    "\n",
    "score /= len(reference)\n",
    "print(\"The bleu score is: \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "input_tokenizer_saving_path=\"/kaggle/working/output/input_tokenizer.pkl\" if kaggle else \"./output/input_tokenizer.pkl\"\n",
    "target_tokenizer_saving_path=\"/kaggle/working/output/target_tokenizer.pkl\" if kaggle else \"./output/target_tokenizer.pkl\"\n",
    "\n",
    "# Save the tokenizer to a .pkl file\n",
    "with open(input_tokenizer_saving_path, 'wb') as file:\n",
    "    pickle.dump(input_lang_tokenize, file)\n",
    "\n",
    "with open(target_tokenizer_saving_path, 'wb') as file:\n",
    "    pickle.dump(target_lang_tokenize, file)\n",
    "    \n",
    "print(\"Tokenizer saved as tokenizer.pkl\")\n",
    "\n",
    "# Later, load the tokenizer back\n",
    "with open(input_tokenizer_saving_path, 'rb') as file:\n",
    "    input_lang_tokenize_test = pickle.load(file)\n",
    "\n",
    "with open(target_tokenizer_saving_path, 'rb') as file:\n",
    "    target_lang_tokenize_test = pickle.load(file)\n",
    "print(\"Tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_sequence(input_text, encoder, decoder, input_tokenizer, target_tokenizer, max_target_length):\n",
    "    # Preprocess the input\n",
    "    preprocessed_input = preprocess_japanese_text(input_text)  # Adjust for your input language\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([preprocessed_input])\n",
    "    input_sequence = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=max_length_input, padding='post')\n",
    "\n",
    "    # Encode the input\n",
    "    # Change here to set batch size to 1\n",
    "    initial_hidden = encoder.initialize_hidden_state()\n",
    "    initial_hidden = [tf.zeros((1, encoder.enc_units)), tf.zeros((1, encoder.enc_units))]  # Adjust shape to (1, units)\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(input_sequence, initial_hidden)\n",
    "\n",
    "    # Prepare the decoder's initial input and states\n",
    "    decoder_input = tf.expand_dims([target_tokenizer.word_index['start_']], 0)  # Start token\n",
    "\n",
    "    # Correct here to use the state returned by the encoder with proper batch dimension\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        # Predict the next word\n",
    "        predictions, dec_hidden = decoder(decoder_input, dec_hidden, enc_output)\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        predicted_token = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(predicted_token)\n",
    "\n",
    "        # Stop if end token is predicted\n",
    "        if target_tokenizer.index_word[predicted_token] == '_end':\n",
    "            break\n",
    "\n",
    "        # Use the predicted token as the next input\n",
    "        decoder_input = tf.expand_dims([predicted_token], 0)\n",
    "\n",
    "    # Convert the tokens back to text\n",
    "    result_text = ' '.join([target_tokenizer.index_word[token] for token in result if token in target_tokenizer.index_word])\n",
    "    return result_text\n",
    "\n",
    "\n",
    "custom_input = \"彼は犬を飼っています\"\n",
    "predicted_output = predict_sequence(\n",
    "    custom_input,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    input_lang_tokenize,\n",
    "    target_lang_tokenize,\n",
    "    max_length_target\n",
    ")\n",
    "print(f\"Input: {custom_input}\")\n",
    "print(f\"Translation: {predicted_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6TI6KPBz9rH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def predict(input_text, encoder, decoder, input_lang_tokenizer, target_lang_tokenizer, max_length_input, max_length_target):\n",
    "#     input_seq = input_lang_tokenizer.texts_to_sequences([input_text])\n",
    "#     input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_length_input, padding='post')\n",
    "\n",
    "#     # Ensure the input sequence has batch size 1\n",
    "#     input_seq = tf.expand_dims(input_seq[0], axis=0)  # Shape becomes (1, max_length_input)\n",
    "\n",
    "#     # Create initial hidden state (replace with appropriate dimensions and type)\n",
    "#     # Assuming your encoder uses LSTM, you might need to create two hidden states (h and c)\n",
    "#     hidden_state = tf.zeros((1, encoder.units))  # Replace encoder.units with the actual number of units in your encoder\n",
    "#     cell_state = tf.zeros((1, encoder.units))  # Replace encoder.units with the actual number of units in your encoder\n",
    "\n",
    "#     # Pass through the encoder, providing the hidden state\n",
    "#     enc_output, enc_hidden = encoder(input_seq, hidden_state=hidden_state, cell_state=cell_state)\n",
    "\n",
    "#     # Set up decoder input and initial hidden state\n",
    "#     dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)  # Shape: (1, 1)\n",
    "#     dec_hidden = enc_hidden\n",
    "\n",
    "#     translation = []\n",
    "#     for t in range(max_length_target):\n",
    "#         # Pass through the decoder\n",
    "#         dec_output, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "#         # Get the token with the highest probability\n",
    "#         predicted_id = tf.argmax(dec_output[0]).numpy()\n",
    "\n",
    "#         # Stop if the end token is predicted\n",
    "#         if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "#             break\n",
    "\n",
    "#         # Append the predicted word to the translation\n",
    "#         translation.append(target_lang_tokenizer.index_word[predicted_id])\n",
    "\n",
    "#         # Update the decoder input to the predicted token\n",
    "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "#     return ' '.join(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKZ9TUP22Jq1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Example input text\n",
    "# user_input = \"こんにちは\"\n",
    "\n",
    "# # Call the predict function\n",
    "# translation = predict(\n",
    "#     input_text=user_input,\n",
    "#     encoder=encoder,\n",
    "#     decoder=decoder,\n",
    "#     input_lang_tokenizer=input_lang_tokenize,\n",
    "#     target_lang_tokenizer=target_lang_tokenize,\n",
    "#     max_length_input=max_length_input,\n",
    "#     max_length_target=max_length_target\n",
    "# )\n",
    "\n",
    "# print(f\"Input: {user_input}\")\n",
    "# print(f\"Translation: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PSzrPUXwX81",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# text=\"私は犬を飼っています\"\n",
    "# text=remove_emojis(text)\n",
    "# text=zen_to_han(text)\n",
    "# text=preprocess_japanese_text(text)\n",
    "# input_tensor_try, input_lang_tokenize_try = tokenize(text)\n",
    "# input_tensor_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyUzOgwF8kw9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# predict(input_tensor_try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNkXiemq9bDO"
   },
   "source": [
    "# Translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-DcisWA9bDP",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def preprocess_sentence(en_text):\n",
    "#     # normalize Japanese\n",
    "#     ja_text = japanese_unicode_to_ascii(en_text)\n",
    "#     ja_text = preprocess_japanese_text(ja_text)\n",
    "#     ja_text = zen_to_han(ja_text)\n",
    "\n",
    "#     # add StTART and END sentence\n",
    "#     # ja_text = \"start_ \" + ja_text + \" _end\"\n",
    "#     return ja_text\n",
    "\n",
    "# def evaluate(sentence):\n",
    "\n",
    "#     attention_plot = np.zeros((max_length_target, max_length_input))\n",
    "#     sentence = preprocess_sentence(sentence).strip()  # Remove extra spaces at start/end\n",
    "#     inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ') if i in input_lang_tokenize.word_index]\n",
    "\n",
    "#     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "#                                                            maxlen=max_length_input,\n",
    "#                                                            padding='post')\n",
    "\n",
    "#     inputs = tf.convert_to_tensor(inputs)\n",
    "#     result = ''\n",
    "#     hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "#     enc_out, state = encoder(inputs, hidden)\n",
    "#     hidden_state = state\n",
    "#     dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
    "#     for t in range(max_length_target):\n",
    "#         predictions, hidden_state = decoder(dec_input, hidden_state, enc_out)\n",
    "#         predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "#         result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
    "#         if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
    "#             return result, sentence\n",
    "\n",
    "#         # the predicted ID is fed back into the model\n",
    "#         dec_input = tf.expand_dims([predicted_id], 0)\n",
    "#     return result, sentence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui6bFxRt9bDd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# def result(sentence):\n",
    "#     result, sentence = evaluate(sentence)\n",
    "\n",
    "#     return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2duMXKX9bDh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # result, sentence = result(\"he has a dog\")\n",
    "# result, sentence = result(\"彼は犬を飼っています\")\n",
    "# print('Input: %s' % (sentence))\n",
    "# print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlCu-uvtwOft",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # result, sentence = result(\"What is this!?\")\n",
    "# result, sentence = result(\"これは何ですか!?\")\n",
    "# print('Input: %s' % (sentence))\n",
    "# print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2FZwaiky-yj",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # result, sentence = result(\"Actually I did it\")\n",
    "# result, sentence = result(\"実は、私がやりました。\")\n",
    "# print('Input: %s' % (sentence))\n",
    "# print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67d09FoPz2Al",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# result, sentence = result(\"I love her\")\n",
    "# print('Input: %s' % (sentence))\n",
    "# print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2saSpKK0Cwl",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6032580,
     "sourceId": 10098172,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
