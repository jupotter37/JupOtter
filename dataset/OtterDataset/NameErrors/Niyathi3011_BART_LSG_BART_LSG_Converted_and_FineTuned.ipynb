{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOT3WhYJHh4K"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "References: \n",
        "* https://github.com/dmmiller612/bert-extractive-summarizer\n",
        "* https://github.com/ccdv-ai/convert_checkpoint_to_lsg\n",
        "* https://medium.com/@ferlatti.aldo/fine-tuning-a-chat-summarizer-c18625bc817d\n",
        "* https://github.com/AldoF95/bart-chat-summarizer-finetuning\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlQzFBTFy9X4"
      },
      "source": [
        "# Initializing a BART facebook/bart-large style configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fRTz_qPLkL1",
        "outputId": "bc9dae7b-8a44-4ab3-ee1a-61d23837f1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eUDsB1mwx2RV"
      },
      "outputs": [],
      "source": [
        "from logging import warn\n",
        "import torch\n",
        "import transformers\n",
        "from transformers.models.bart.modeling_bart import *\n",
        "from transformers.models.bart.modeling_bart import _expand_mask\n",
        "import torch.nn as nn\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ1hnbVfLY7A",
        "outputId": "72fd9a87-54d3-4ca4-c18e-6d3fcb23fb9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.27.3\n"
          ]
        }
      ],
      "source": [
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X_-2n9k4xvon"
      },
      "outputs": [],
      "source": [
        "from logging import warn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jOaPX5Max8YO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OfRYSkE-x9WM"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp48PL1ix-7w",
        "outputId": "85301e3e-f138-414e-da8e-62bfd6c3e955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1cGd_w9EyWfV"
      },
      "outputs": [],
      "source": [
        "from transformers import BartConfig, BartModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0xJvkePOyoJi"
      },
      "outputs": [],
      "source": [
        "configuration = BartConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y75qZyM0y6yN"
      },
      "source": [
        "# Initializing a model (with random weights) from the facebook/bart-large style configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6a2YRq4kzRRD"
      },
      "outputs": [],
      "source": [
        "class LSGBartConfig(BartConfig):\n",
        "    \"\"\"\n",
        "    This class overrides :class:`~transformers.BartConfig`. Please check the superclass for the appropriate\n",
        "    documentation alongside usage examples.\n",
        "    \"\"\"\n",
        "\n",
        "    base_model_prefix = \"lsg\"\n",
        "    model_type = \"bart\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        adaptive=True,\n",
        "        base_model_prefix=\"lsg\",\n",
        "        block_size=128,\n",
        "        lsh_num_pre_rounds=1,\n",
        "        mask_first_token=False,\n",
        "        num_global_tokens=1,\n",
        "        pass_global_tokens_to_decoder=True,\n",
        "        pool_with_global=True,\n",
        "        sparse_block_size=128,\n",
        "        sparsity_factor=2,\n",
        "        sparsity_type=\"norm\",\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"Constructs LSGConfig.\"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.adaptive = adaptive\n",
        "        self.auto_map = AUTO_MAP\n",
        "        self.base_model_prefix = base_model_prefix\n",
        "        self.block_size = block_size\n",
        "        self.lsh_num_pre_rounds = lsh_num_pre_rounds\n",
        "        self.mask_first_token = mask_first_token\n",
        "        self.num_global_tokens = num_global_tokens\n",
        "        self.pass_global_tokens_to_decoder = pass_global_tokens_to_decoder\n",
        "        self.pool_with_global = pool_with_global\n",
        "        self.sparse_block_size = sparse_block_size\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "        self.sparsity_type = sparsity_type\n",
        "\n",
        "        if sparsity_type not in [None, \"none\", \"norm\", \"lsh\", \"pooling\", \"stride\", \"block_stride\"]:\n",
        "            logger.warning(\n",
        "                \"[WARNING CONFIG]: sparsity_mode not in [None, 'none', 'norm', 'lsh', 'pooling', 'stride', 'block_stride'], \\\n",
        "                    setting sparsity_type=None, computation will skip sparse attention\")\n",
        "            self.sparsity_type = None\n",
        "\n",
        "        if self.sparsity_type in [\"stride\", \"block_stride\"]:\n",
        "            if self.sparsity_factor > self.encoder_attention_heads:\n",
        "                logger.warning(\n",
        "                \"[WARNING CONFIG]: sparsity_factor > encoder_attention_heads is not recommended for stride/block_stride sparsity\"\n",
        "            )\n",
        "        \n",
        "        if self.num_global_tokens < 1:\n",
        "            logger.warning(\n",
        "                \"[WARNING CONFIG]: num_global_tokens < 1 is not compatible, setting num_global_tokens=1\"\n",
        "            )\n",
        "            self.num_global_tokens = 1\n",
        "        elif self.num_global_tokens > 512:\n",
        "            logger.warning(\n",
        "                \"[WARNING CONFIG]: num_global_tokens > 512 is not allowed, setting num_global_tokens=512\"\n",
        "            )\n",
        "            self.num_global_tokens = 512\n",
        "        \n",
        "        if self.sparsity_factor > 0:\n",
        "            assert self.block_size % self.sparsity_factor == 0, \"[ERROR CONFIG]: block_size must be divisible by sparsity_factor\"\n",
        "            assert self.block_size//self.sparsity_factor >= 1, \"[ERROR CONFIG]: make sure block_size >= sparsity_factor\"\n",
        "            \n",
        "        if self.mask_first_token and not pool_with_global:\n",
        "            logger.warning(\n",
        "                \"[WARNING CONFIG]: pool_with_global==False is not compatible with mask_first_token==True. Setting pool_with_global to True.\")\n",
        "            self.pool_with_global = True\n",
        "        \n",
        "        if hasattr(self, \"position_embedding_type\"):\n",
        "            if self.position_embedding_type != \"absolute\":\n",
        "                logger.warning(\n",
        "                \"[WARNING CONFIG]: LSG Attention is not compatible with relative positional embedding and will skip its computation. Set position_embedding_type='absolute' to remove this warning.\")\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QG74fKLGzhZ3"
      },
      "outputs": [],
      "source": [
        "class BaseSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        is_decoder=False,\n",
        "        bias=True,\n",
        "        ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if (self.head_dim * num_heads) != self.embed_dim:\n",
        "            raise ValueError(\n",
        "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n",
        "                f\" and `num_heads`: {num_heads}).\"\n",
        "            )\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (\n",
        "            self.num_heads,\n",
        "            self.head_dim,\n",
        "        )\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def reshape_output(self, context_layer):\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.embed_dim,)\n",
        "        return context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    def project_QKV(self, hidden_states):\n",
        "\n",
        "        query_layer = self.transpose_for_scores(self.q_proj(hidden_states))\n",
        "        key_layer = self.transpose_for_scores(self.k_proj(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.v_proj(hidden_states))\n",
        "        return query_layer, key_layer, value_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "L7WG035j2MsN"
      },
      "outputs": [],
      "source": [
        "    \n",
        "class BaseAttentionProduct(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Compute attention: softmax(Q @ K.T) @ V\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(config.attention_dropout)\n",
        "\n",
        "    def forward(self, query_layer, key_layer, value_layer, attention_mask=None):\n",
        "        \n",
        "        d = query_layer.shape[-1]\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = query_layer @ key_layer.transpose(-1, -2) / math.sqrt(d)\n",
        "\n",
        "        del query_layer\n",
        "        del key_layer\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "            del attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        context_layer = self.dropout(attention_probs) @ value_layer\n",
        "\n",
        "        return context_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PYEPP0592SGh"
      },
      "outputs": [],
      "source": [
        "class LSGAttentionProduct(nn.Module):\n",
        "\n",
        "    def __init__(self, config, block_size=None, sparse_block_size=None, sparsity_factor=4):\n",
        "        \"\"\"\n",
        "        Compute block or overlapping blocks attention products\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.sparse_block_size = sparse_block_size\n",
        "        self.sparsity_factor = sparsity_factor\n",
        "\n",
        "        if self.block_size is None:\n",
        "            self.block_size = config.block_size\n",
        "\n",
        "        if self.sparse_block_size is None:\n",
        "            self.sparse_block_size = config.sparse_block_size\n",
        "\n",
        "        # Shape of blocks\n",
        "        self.local_shapes = (self.block_size*3, self.block_size)\n",
        "        if self.sparse_block_size and self.sparsity_factor > 0:\n",
        "            self.sparse_shapes = (self.sparse_block_size*3, self.block_size//self.sparsity_factor)\n",
        "\n",
        "        self.attention = BaseAttentionProduct(config)\n",
        "\n",
        "    def build_lsg_inputs(self, hidden_states, sparse_hidden_states, global_hidden_states, is_attn_mask=False):\n",
        "\n",
        "        # Build local tokens\n",
        "        local_hidden_states = self.reshape_to_local_block(hidden_states, is_attn_mask)\n",
        "        del hidden_states\n",
        "\n",
        "        # Build sparse tokens\n",
        "        if sparse_hidden_states is not None:\n",
        "            sparse_hidden_states = self.reshape_to_sparse_block(sparse_hidden_states, is_attn_mask)\n",
        "\n",
        "        return self.cat_global_sparse_local_tokens(global_hidden_states, sparse_hidden_states, local_hidden_states)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        query_layer, \n",
        "        key_layer, \n",
        "        value_layer, \n",
        "        attention_mask=None, \n",
        "        sparse_key=None,\n",
        "        sparse_value=None, \n",
        "        sparse_mask=None, \n",
        "        global_key=None, \n",
        "        global_value=None, \n",
        "        global_mask=None\n",
        "        ):\n",
        "\n",
        "        # Input batch, heads, length, hidden_size\n",
        "        n, h, t, d = query_layer.size()\n",
        "        n_blocks = t // self.block_size\n",
        "        assert t % self.block_size == 0\n",
        "\n",
        "        key_layer = self.build_lsg_inputs(\n",
        "            key_layer, \n",
        "            sparse_key, \n",
        "            global_key\n",
        "            )\n",
        "        del sparse_key\n",
        "        del global_key\n",
        "\n",
        "        value_layer = self.build_lsg_inputs(\n",
        "            value_layer, \n",
        "            sparse_value, \n",
        "            global_value\n",
        "            )\n",
        "        del sparse_value\n",
        "        del global_value\n",
        "\n",
        "        attention_mask = self.build_lsg_inputs(\n",
        "            attention_mask, \n",
        "            sparse_mask, \n",
        "            global_mask.transpose(-1, -2), \n",
        "            is_attn_mask=True\n",
        "            ).transpose(-1, -2)\n",
        "        del sparse_mask\n",
        "        del global_mask\n",
        "\n",
        "        # expect (..., t, d) shape\n",
        "        # Compute attention\n",
        "        context_layer = self.attention(\n",
        "                query_layer=self.chunk(query_layer, n_blocks), \n",
        "                key_layer=key_layer,\n",
        "                value_layer=value_layer,\n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "        return context_layer.reshape(n, h, -1, d)\n",
        "\n",
        "    def reshape_to_local_block(self, hidden_states, is_attn_mask=False):\n",
        "\n",
        "        size, step = self.local_shapes\n",
        "        s = (size - step) // 2\n",
        "\n",
        "        # Pad before block reshaping\n",
        "        if is_attn_mask: \n",
        "            pad_value = torch.finfo(hidden_states.dtype).min  \n",
        "            hidden_states = hidden_states.transpose(-1, -2)\n",
        "        else: \n",
        "            pad_value = 0\n",
        "\n",
        "        hidden_states = torch.nn.functional.pad(\n",
        "            hidden_states.transpose(-1, -2), \n",
        "            pad=(s, s),\n",
        "            value=pad_value\n",
        "            ).transpose(-1, -2)\n",
        "\n",
        "        # Make blocks\n",
        "        hidden_states = hidden_states.unfold(-2, size=size, step=step).transpose(-1, -2)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "    def reshape_to_sparse_block(self, hidden_states, is_attn_mask=False):\n",
        "\n",
        "        size, step = self.sparse_shapes\n",
        "\n",
        "        # In case of odd case\n",
        "        odd_offset = (step % 2)\n",
        "\n",
        "        # n, h, t, d*2 + 1\n",
        "        size = size*2 \n",
        "        s = (size - step) // 2 + odd_offset\n",
        "\n",
        "        # Pad before block reshaping\n",
        "        if is_attn_mask:\n",
        "            pad_value = torch.finfo(hidden_states.dtype).min \n",
        "            hidden_states = hidden_states.transpose(-1, -2)\n",
        "        else: \n",
        "            pad_value = 0\n",
        "\n",
        "        hidden_states = torch.nn.functional.pad(\n",
        "            hidden_states.transpose(-1, -2), \n",
        "            pad=(s, s),\n",
        "            value=pad_value\n",
        "            ).transpose(-1, -2)\n",
        "\n",
        "        # Make blocks\n",
        "        hidden_states = hidden_states.unfold(-2, size=size, step=step).transpose(-1, -2)\n",
        "\n",
        "        # Fix case where block_size == sparsify_factor\n",
        "        if odd_offset: \n",
        "            hidden_states = hidden_states[..., :-1, :, :]\n",
        "\n",
        "        # Indexes for selection\n",
        "        u = (size - self.block_size * 3 // self.sparsity_factor) // 2 + odd_offset\n",
        "        s = self.sparse_block_size\n",
        "\n",
        "        u_ = u + odd_offset\n",
        "        return torch.cat([hidden_states[..., u-s:u, :], hidden_states[..., -u_:-u_+s, :]], dim=-2)\n",
        "\n",
        "    def cat_global_sparse_local_tokens(self, x_global, x_sparse=None, x_local=None, dim=-2):\n",
        "\n",
        "        n, h, b, t, d = x_local.size()\n",
        "        x_global = x_global.unsqueeze(-3).expand(-1, -1, b, -1, -1)\n",
        "        if x_sparse is not None:\n",
        "            return torch.cat([x_global, x_sparse, x_local], dim=dim)\n",
        "        return torch.cat([x_global, x_local], dim=dim)\n",
        "\n",
        "    def chunk(self, x, n_blocks):\n",
        "\n",
        "        t, d = x.size()[-2:]\n",
        "        return x.reshape(*x.size()[:-2], n_blocks, -1, d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rxIJYudc2c2z"
      },
      "outputs": [],
      "source": [
        "class LSGBartEncoderAttention(BaseSelfAttention):\n",
        "    '''\n",
        "    Compute local attention with overlapping blocs\n",
        "    Use global attention for tokens with highest norm\n",
        "    '''\n",
        "    def __init__(\n",
        "        self, \n",
        "        config, \n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout\n",
        "        ):\n",
        "\n",
        "        super().__init__(embed_dim, num_heads, dropout)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.sparse_block_size = config.sparse_block_size\n",
        "        self.num_global_tokens = config.num_global_tokens\n",
        "        self.sparsity_factor = config.sparsity_factor\n",
        "\n",
        "        self.attention = LSGAttentionProduct(\n",
        "            config, \n",
        "            block_size=config.block_size, \n",
        "            sparse_block_size=config.sparse_block_size, \n",
        "            sparsity_factor=self.sparsity_factor, \n",
        "            )\n",
        "\n",
        "        self.full_attention = BaseAttentionProduct(config)\n",
        "\n",
        "        sparse_functions = {\n",
        "            \"norm\": self.get_sparse_tokens_with_norm, \n",
        "            \"pooling\": self.get_sparse_tokens_with_pooling,\n",
        "            \"lsh\": self.get_sparse_tokens_with_lsh,\n",
        "            \"stride\": self.get_sparse_tokens_with_stride,\n",
        "            \"block_stride\": self.get_sparse_tokens_with_block_stride,\n",
        "            }\n",
        "        \n",
        "        self.sparsity_type = config.sparsity_type\n",
        "        self.get_sparse_elements = sparse_functions.get(self.sparsity_type, lambda x, y, z: (None, None, None))\n",
        "            \n",
        "        if config.sparsity_type == \"lsh\":\n",
        "            self.lsh_num_pre_rounds = config.lsh_num_pre_rounds\n",
        "        \n",
        "    def get_sparse_tokens_with_norm(self, keys, values, mask):\n",
        "        \n",
        "        if self.sparsity_factor == 1:\n",
        "            return keys, values, mask.expand(-1, keys.size()[1], -1, -1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            block_size = min(self.block_size, self.sparse_block_size)\n",
        "            key_norm = keys.detach().norm(dim=-1, keepdim=True)\n",
        "            key_norm = key_norm * ~mask.transpose(-1, -2).bool()\n",
        "            key_norm = self.chunk(key_norm, block_size)\n",
        "\n",
        "            n, h, b, t, d = key_norm.size()\n",
        "            \n",
        "            idx = key_norm.argsort(dim=-2) \n",
        "            del key_norm\n",
        "            idx += (torch.arange(b, device=keys.device)*t).reshape(1, 1, b, 1, 1)\n",
        "\n",
        "            split = (t - block_size // self.sparsity_factor, block_size // self.sparsity_factor)\n",
        "            sparse_idx = idx.split(split, -2)[-1].reshape(n, h, -1, 1)\n",
        "        \n",
        "        d = keys.size()[-1]\n",
        "        keys = keys.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        values = values.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        mask = mask.expand(-1, h, -1, -1).transpose(-1, -2).gather(dim=-2, index=sparse_idx).transpose(-1, -2)\n",
        "\n",
        "        return keys, values, mask\n",
        "\n",
        "    def get_sparse_tokens_with_pooling(self, keys, values, mask):\n",
        "        \n",
        "        if self.sparsity_factor == 1:\n",
        "            return keys, values, mask.expand(-1, keys.size()[1], -1, -1)\n",
        "\n",
        "        keys = self.chunk(keys, self.sparsity_factor)\n",
        "        values = self.chunk(values, self.sparsity_factor)\n",
        "\n",
        "        n, h, b, t, d = keys.size()\n",
        "        mask = mask.reshape(n, 1, b, 1, t)\n",
        "        mask = ~mask.transpose(-1, -2).bool()\n",
        "\n",
        "        keys = keys * mask\n",
        "        values = values * mask\n",
        "\n",
        "        mask = mask.sum(dim=-2)\n",
        "        keys = keys.sum(dim=-2) / (mask + 1e-6)\n",
        "        values = values.sum(dim=-2) / (mask + 1e-6)\n",
        "\n",
        "        mask = (1. - mask.clamp(0, 1)) \n",
        "        mask *= torch.finfo(mask.dtype).min\n",
        "        return keys.reshape(n, h, -1, d), values.reshape(n, h, -1, d), mask.expand(-1, h, -1, -1).transpose(-1, -2)\n",
        "\n",
        "    def get_sparse_tokens_with_stride(self, keys, values, mask):\n",
        "\n",
        "        if self.sparsity_factor == 1:\n",
        "            return keys, values, mask.expand(-1, keys.size()[1], -1, -1)\n",
        "\n",
        "        n, h, t, d = keys.size()\n",
        "        sparse_idx = torch.arange(t // self.sparsity_factor, device=keys.device) * self.sparsity_factor\n",
        "        sparse_idx = sparse_idx.reshape(1, 1, -1, 1) + (torch.arange(h, device=keys.device) % self.sparsity_factor).reshape(1, h, 1, 1)\n",
        "        sparse_idx = sparse_idx.expand(n, h, -1, 1)\n",
        "\n",
        "        keys = keys.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        values = values.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        mask = mask.expand(-1, h, -1, -1).transpose(-1, -2).gather(dim=-2, index=sparse_idx).transpose(-1, -2)\n",
        "\n",
        "        return keys, values, mask\n",
        "\n",
        "    def get_sparse_tokens_with_block_stride(self, keys, values, mask):\n",
        "\n",
        "        if self.sparsity_factor == 1:\n",
        "            return keys, values, mask.expand(-1, keys.size()[1], -1, -1)\n",
        "\n",
        "        n, h, t, d = keys.size()\n",
        "\n",
        "        t, b = self.block_size, t // self.block_size\n",
        "        sparse_idx = torch.arange(t // self.sparsity_factor, device=keys.device)\n",
        "        sparse_idx = sparse_idx.reshape(1, 1, 1, -1, 1) + torch.arange(h, device=keys.device).reshape(1, h, 1, 1, 1) * (t // self.sparsity_factor)\n",
        "        sparse_idx = (sparse_idx % t) \n",
        "        sparse_idx = sparse_idx + torch.arange(b, device=keys.device).reshape(1, 1, -1, 1, 1) * t\n",
        "        sparse_idx = sparse_idx.reshape(1, h, -1, 1).expand(n, h, -1, 1)\n",
        "\n",
        "        keys = keys.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        values = values.gather(dim=-2, index=sparse_idx.expand(-1, -1, -1, d))\n",
        "        mask = mask.expand(-1, h, -1, -1).transpose(-1, -2).gather(dim=-2, index=sparse_idx).transpose(-1, -2)\n",
        "\n",
        "        return keys, values, mask\n",
        "        \n",
        "    def get_sparse_tokens_with_lsh(self, keys, values, mask):\n",
        "        \n",
        "        if self.sparsity_factor == 1:\n",
        "            return keys, values, mask.expand(-1, keys.size()[1], -1, -1)\n",
        "\n",
        "        block_size = min(self.block_size, self.sparse_block_size)\n",
        "        keys = self.chunk(keys, block_size)\n",
        "        values = self.chunk(values, block_size)\n",
        "\n",
        "        n, h, b, t, d = keys.size()\n",
        "        mask = mask.reshape(n, 1, b, 1, t)\n",
        "        mask = ~mask.transpose(-1, -2).bool()\n",
        "\n",
        "        keys = keys * mask\n",
        "        values = values * mask\n",
        "        mask = mask.expand(-1, h, -1, -1, -1).float()\n",
        "\n",
        "        extra_factor = 1\n",
        "        \n",
        "        for _ in range(self.lsh_num_pre_rounds):\n",
        "            keys, values, mask = self.lsh_round(keys, values, mask, t*extra_factor)\n",
        "\n",
        "        keys, values, mask = self.lsh_round(keys, values, mask, t//self.sparsity_factor)\n",
        "        keys /= mask + 1e-8\n",
        "        values /= mask + 1e-8\n",
        "\n",
        "        mask = (1. - mask.clamp(0, 1)) \n",
        "        mask *= torch.finfo(mask.dtype).min\n",
        "        return keys.reshape(n, h, -1, d), values.reshape(n, h, -1, d), mask.transpose(-1, -2).reshape(n, h, 1, -1)\n",
        "\n",
        "    def lsh_round(self, keys, values, mask, output_size):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            n_hashes = output_size // 2\n",
        "            n, h, b, t, d = keys.size()\n",
        "            binary_mask = mask.clamp(0, 1)\n",
        "\n",
        "            indexes = (torch.nn.functional.normalize(keys, dim=-1) * binary_mask) @ torch.randn(1, h, 1, d, n_hashes, device=keys.device)\n",
        "            indexes = torch.cat([indexes, -indexes], dim=-1).argmax(dim=-1, keepdim=True)\n",
        "\n",
        "        n, h, b, t, d = keys.size()\n",
        "        \n",
        "        x_ = torch.zeros(n, h, b, output_size, d, device=keys.device)\n",
        "        mask_ = torch.zeros(n, h, b, output_size, 1, device=keys.device)\n",
        "        keys = torch.scatter_add(x_, dim=-2, index=indexes.expand(-1, -1, -1, -1, d), src=keys)\n",
        "        values = torch.scatter_add(x_, dim=-2, index=indexes.expand(-1, -1, -1, -1, d), src=values)\n",
        "        mask = torch.scatter_add(mask_, dim=-2, index=indexes, src=mask)\n",
        "\n",
        "        return keys[..., :output_size, :], values[..., :output_size, :], mask[..., :output_size, :]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        layer_head_mask=None,\n",
        "        output_attentions=False\n",
        "        ):\n",
        "\n",
        "        query_layer, key_layer, value_layer = self.project_QKV(hidden_states)\n",
        "        outputs = self.not_causal_forward(\n",
        "            query_layer,\n",
        "            key_layer,\n",
        "            value_layer, \n",
        "            attention_mask=attention_mask[:, :, :1, :], \n",
        "            head_mask=layer_head_mask, \n",
        "            output_attentions=output_attentions\n",
        "            )\n",
        "        \n",
        "        return self.out_proj(outputs), None, None\n",
        "\n",
        "    def not_causal_forward(\n",
        "        self,\n",
        "        query_layer,\n",
        "        key_layer,\n",
        "        value_layer,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        ):\n",
        "\n",
        "        n, h, t, d = query_layer.size()\n",
        "\n",
        "        # Cat global mask\n",
        "        attention_mask = torch.nn.functional.pad(attention_mask, (self.num_global_tokens, 0), value=0)\n",
        "        \n",
        "        # Use normal attention if local attention covers every tokens\n",
        "        if t <= 2 * self.block_size + self.num_global_tokens:\n",
        "            context_layer = self.full_attention(\n",
        "                query_layer=query_layer, \n",
        "                key_layer=key_layer, \n",
        "                value_layer=value_layer, \n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "            return self.reshape_output(context_layer)\n",
        "\n",
        "        # Split input into global tokens and other tokens\n",
        "        split = (self.num_global_tokens, t - self.num_global_tokens)\n",
        "        global_query, query_layer = query_layer.split(split, dim=-2)\n",
        "        \n",
        "        # Get global_attention\n",
        "        bos = self.full_attention(\n",
        "            query_layer=global_query, \n",
        "            key_layer=key_layer, \n",
        "            value_layer=value_layer, \n",
        "            attention_mask=attention_mask\n",
        "            )\n",
        "        \n",
        "        # Split K Q M on global and non global\n",
        "        global_key, key_layer = key_layer.split(split, dim=-2)\n",
        "        global_value, value_layer = value_layer.split(split, dim=-2)\n",
        "        global_mask, attention_mask = attention_mask.split(split, dim=-1)\n",
        "        \n",
        "        n, h, t, d = key_layer.size()\n",
        "\n",
        "        # Get sparse idx\n",
        "        sparse_key, sparse_value, sparse_mask = (None, None, None)\n",
        "\n",
        "        if self.sparse_block_size and self.sparsity_factor > 0:\n",
        "            sparse_key, sparse_value, sparse_mask = self.get_sparse_elements(key_layer, value_layer, attention_mask)\n",
        "        \n",
        "        # Expand masks on heads\n",
        "        attention_mask = attention_mask.expand(-1, h, -1, -1)\n",
        "        global_mask = global_mask.expand(-1, h, -1, -1)\n",
        "\n",
        "        # Compute dot product attention\n",
        "        context_layer = self.attention(\n",
        "            query_layer, \n",
        "            key_layer, \n",
        "            value_layer, \n",
        "            attention_mask,\n",
        "            sparse_key=sparse_key, \n",
        "            sparse_value=sparse_value, \n",
        "            sparse_mask=sparse_mask,\n",
        "            global_key=global_key,\n",
        "            global_value=global_value,\n",
        "            global_mask=global_mask\n",
        "            )\n",
        "\n",
        "        # Merge global and local-sparse tokens\n",
        "        context_layer = torch.cat([bos, context_layer], dim=-2)\n",
        "        context_layer = self.reshape_output(context_layer)\n",
        "        \n",
        "        return context_layer\n",
        "\n",
        "    def chunk(self, x, chunk_size):\n",
        "\n",
        "        n, h, t, d = x.size()\n",
        "        return x.reshape(n, h, -1, chunk_size, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "feZ6jU7OaS2O"
      },
      "outputs": [],
      "source": [
        "class LSGBartModel(LSGBartPretrainedModel, BartModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        LSGBartPretrainedModel.__init__(self, config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        self.pass_global_tokens_to_decoder = config.pass_global_tokens_to_decoder\n",
        "        self.num_global_tokens = config.num_global_tokens\n",
        "\n",
        "        self.encoder = LSGBartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        ):\n",
        "\n",
        "        # different to other models, Bart automatically creates decoder_input_ids from\n",
        "        # input_ids if no decoder_input_ids are provided\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # Pad mask for global tokens\n",
        "        if self.pass_global_tokens_to_decoder and attention_mask is not None:\n",
        "            attention_mask = torch.nn.functional.pad(attention_mask, pad=(self.num_global_tokens, 0), value=1)\n",
        "            \n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pGbHlmYLaf7u"
      },
      "outputs": [],
      "source": [
        "class LSGBartForConditionalGeneration(LSGBartPretrainedModel, BartForConditionalGeneration):\n",
        "    \n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        LSGBartPretrainedModel.__init__(self, config)\n",
        "        self.model = LSGBartModel(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "    \n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d4Uc5VZnYZHY"
      },
      "outputs": [],
      "source": [
        "class LSGBartEncoderLayer(BartEncoderLayer):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__(config)\n",
        "        self.self_attn = LSGBartEncoderAttention(\n",
        "            config=config,\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "J3c2Jo3ZYccZ"
      },
      "outputs": [],
      "source": [
        "class LSGBartPretrainedModel(BartPretrainedModel):\n",
        "\n",
        "    config_class = LSGBartConfig\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "\n",
        "        if isinstance(module, (BartDecoder, BartEncoder, LSGBartEncoder)):\n",
        "            module.gradient_checkpointing = value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kikXZHA1Yf38"
      },
      "outputs": [],
      "source": [
        "class PretrainedLSGBartModel(LSGBartPretrainedModel):\n",
        "\n",
        "    def __init_subclass__(self):\n",
        "        warnings.warn(\n",
        "            \"The class `PretrainedBartModel` has been depreciated, please use `LSGBartPretrainedModel` instead.\",\n",
        "            FutureWarning,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SV2x_p_iLwLe"
      },
      "outputs": [],
      "source": [
        "model = BartModel(configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Zn3i2DL2qpH"
      },
      "outputs": [],
      "source": [
        "class LSGBartEncoderLayer(BartEncoderLayer):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__(config)\n",
        "        self.self_attn = LSGBartEncoderAttention(\n",
        "            config=config,\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "cdf81b8e07c04d6298572665c9ecd797",
            "f3b96716f33d4c12b60a7e7c881f7b49",
            "e5ce2309797f405c99db6f9d50adae85",
            "1eb15e040f7048119d6ce1e047536571",
            "96090ae37c4b42ca9d191feb6189a171",
            "bb0e089851f34b468c9a4ca1535eab66",
            "ef6d3fd10836403689efcf3bff9e5e34",
            "ffadc80c4b8c453db25f289a251da07f",
            "981e13580da84a79b87b52edc6dc1902",
            "f7ff5dbb4b5c4e5787b0225aa9aa6995",
            "1c6360f3153c4a24a254942c7b78f6a0",
            "8a56d7122e974bb2b1bbf6b505217078",
            "b98fb35807544e4fba47a6c51565bfec",
            "8fa9ceeb1d554ef6bf04755f09f922fb",
            "a6d29e4635284c26a9ad755429fd33c8",
            "3e9c348fe5fe4bd7bca352cba0ced7e2",
            "81dc1c33d6a54a608c76bb7d4b0dd304",
            "17a969daf49041cfb7ea16342efcab78",
            "2c435b2e93a8423caeec1be41cd15ff5",
            "aad33dcb8df24b86b31b59522f150a6d",
            "dc9aaab69e71477ab6054570e524f136",
            "165fabcb18134fee8846a0bdcb845dd3",
            "3c1fdb0d09ef4e7fafb99ba2537cfbb0",
            "2a999ecf046343c7bd5f11bd59b02389",
            "a2e8d99b608249dc83325defc199edbb",
            "03e66732222d4ec78972292fbe5aa684",
            "0a4ef61d15844e3aa76fb35c38407945",
            "096a02f4765c402a866dddbb978980d3",
            "8184a9a3df3a4c8e967ebfc12eba3f7a",
            "34a4452bf97f4f06aa472e7fe58b64b0",
            "c2d7d18497bf47e48213b036d040d1f4",
            "850d4b5a59734fa4862ef26c8c8839ce",
            "ec222d9d914b4102ab9cfafa23aa3586",
            "c762fe02b295418fada6833c35cd1528",
            "0929f3d9895b4e6394701c8753ee9ed4",
            "11f56497be194c18af6af207339882d0",
            "f0a97fc57f9c4734bdb2981d8448c5f9",
            "457a21fbcf5f46debf43ed1953f03b3a",
            "0b2c8bf3af7e45d5a432a96be04c175e",
            "41d64bad4f1a46ab94f829b474edda76",
            "d2f23bf2c3204bcd9f08590d610b93c7",
            "49a9a6fa9b534611ab954c03d60de3ec",
            "a3647034dd4d48ce9a8a6d4cc7f06865",
            "e0e314a4c1aa46a996bfde249e6d451f",
            "1056ad2f318445b3ada9ef306626aea8",
            "7704d52ef81749188242093e16d9079e",
            "f59205ed75f24837a0444095d8a58555",
            "633f086dc7ca4bed8a56f0d6e91ae08b",
            "c5d7928c9f214558a3f293e4e1248daf",
            "fe4ba6ecd3ea45ab9fc6440bae450fc2",
            "0e5558da06274b96b02ea90ddce78379",
            "9dd188fed0f34e73a8f6b2517dd54726",
            "9d32d932a50b4db3a16a063c3f550787",
            "9e9340426833457b855ef32d2c63bad4",
            "d5b3990ee083405f90957c7b52a7ff7e"
          ]
        },
        "id": "GZ1UMv4HNiEw",
        "outputId": "5371e01b-51f7-4e05-ba60-6d87a647ea5e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdf81b8e07c04d6298572665c9ecd797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a56d7122e974bb2b1bbf6b505217078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c1fdb0d09ef4e7fafb99ba2537cfbb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c762fe02b295418fada6833c35cd1528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()lve/main/config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1056ad2f318445b3ada9ef306626aea8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loading Model and tokenizer\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large', add_prefix_space=True)\n",
        "\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipHNcd5U2-7A"
      },
      "outputs": [],
      "source": [
        "class LSGBartPretrainedModel(BartPretrainedModel):\n",
        "\n",
        "    config_class = LSGBartConfig\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "\n",
        "        if isinstance(module, (BartDecoder, BartEncoder, LSGBartEncoder)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TeHUWq93Ag3"
      },
      "outputs": [],
      "source": [
        "class PretrainedLSGBartModel(LSGBartPretrainedModel):\n",
        "\n",
        "    def __init_subclass__(self):\n",
        "        warnings.warn(\n",
        "            \"The class `PretrainedBartModel` has been depreciated, please use `LSGBartPretrainedModel` instead.\",\n",
        "            FutureWarning,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vsmMv_K3EHD"
      },
      "outputs": [],
      "source": [
        "class LSGBartEncoder(LSGBartPretrainedModel, BartEncoder):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    :class:`BartEncoderLayer`.\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, embed_tokens=None):\n",
        "\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([LSGBartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # \n",
        "        # assert hasattr(config, \"num_global_tokens\")\n",
        "        # self.num_global_tokens = config.num_global_tokens\n",
        "        self.pad_idx = config.pad_token_id\n",
        "\n",
        "        assert hasattr(config, \"block_size\") and hasattr(config, \"adaptive\")\n",
        "        self.block_size = config.block_size\n",
        "        self.adaptive = config.adaptive\n",
        "        self.mask_first_token = config.mask_first_token\n",
        "        self.pool_with_global = config.pool_with_global\n",
        "        # self.pass_global_tokens_to_decoder = config.pass_global_tokens_to_decoder\n",
        "\n",
        "        self.global_embeddings = nn.Embedding(512, embedding_dim=config.d_model)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None\n",
        "        ):\n",
        "\n",
        "        \n",
        "        inputs_ = input_ids if input_ids is not None else inputs_embeds\n",
        "        n, t = inputs_.size()[:2]\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(n, t, device=inputs_.device, dtype=inputs_.dtype)\n",
        "        if self.mask_first_token:\n",
        "            attention_mask[:, 0] = 0\n",
        "\n",
        "        b = self.block_size * 2\n",
        "        pad = t % self.block_size\n",
        "        \n",
        "        # Check if t is multiple of block_size and pad\n",
        "        if self.adaptive and t > b and pad > 0:\n",
        "            pad_length = self.block_size - pad\n",
        "            if input_ids is not None:\n",
        "                input_ids = torch.nn.functional.pad(input_ids, (0, pad_length), value=self.pad_idx)\n",
        "            else:\n",
        "                inputs_embeds = torch.nn.functional.pad(inputs_embeds.transpose(-1, -2), (0, pad_length), value=0.).transpose(-1, -2)\n",
        "            attention_mask = torch.nn.functional.pad(attention_mask, (0, pad_length), value=0)\n",
        "        \n",
        "        n, t_ = attention_mask.size()\n",
        "        \n",
        "        encoder_outputs = self.forward_with_adaptive(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            )\n",
        "        \n",
        "        context = encoder_outputs[0]\n",
        "        diff = t - t_\n",
        "\n",
        "        # if self.pass_global_tokens_to_decoder:\n",
        "        #     offset = self.num_global_tokens\n",
        "        # else:\n",
        "        #     if self.pool_with_global:\n",
        "        #         context[:, self.num_global_tokens] = context[:, 0]\n",
        "        #     context = context[..., self.num_global_tokens:, :]\n",
        "        #     offset = 0\n",
        "\n",
        "        # Adapt sequence to initial shape\n",
        "        if diff < 0:\n",
        "            context = context[:, :t + offset]\n",
        "        \n",
        "        if return_dict:\n",
        "            encoder_outputs.last_hidden_state = context\n",
        "        else:\n",
        "            encoder_outputs = (context, ) + encoder_outputs[1:]\n",
        "        \n",
        "        return encoder_outputs\n",
        "\n",
        "    def forward_with_adaptive(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        ):\n",
        "        \n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(inputs_embeds)\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "\n",
        "        # Add global tokens\n",
        "        n, t, d = hidden_states.size()\n",
        "        global_idx = torch.arange(device=hidden_states.device).reshape(1, -1)\n",
        "        hidden_states = torch.cat([self.global_embeddings(global_idx).expand(n, -1, -1), hidden_states], dim=-2)\n",
        "\n",
        "        hidden_states = self.layernorm_embedding(hidden_states)\n",
        "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            if head_mask.size()[0] != (len(self.layers)):\n",
        "                raise ValueError(\n",
        "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "                )\n",
        "\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rBOVPQGO3MsR"
      },
      "outputs": [],
      "source": [
        "class LSGBartModel(LSGBartPretrainedModel, BartModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        LSGBartPretrainedModel.__init__(self, config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "        # self.pass_global_tokens_to_decoder = config.pass_global_tokens_to_decoder\n",
        "        # self.num_global_tokens = config.num_global_tokens\n",
        "\n",
        "        self.encoder = LSGBartEncoder(config, self.shared)\n",
        "        self.decoder = BartDecoder(config, self.shared)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        cross_attn_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        ):\n",
        "\n",
        "        # different to other models, Bart automatically creates decoder_input_ids from\n",
        "        # input_ids if no decoder_input_ids are provided\n",
        "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "            decoder_input_ids = shift_tokens_right(\n",
        "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "            )\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        # Pad mask for global tokens\n",
        "        # if self.pass_global_tokens_to_decoder and attention_mask is not None:\n",
        "        #     attention_mask = torch.nn.functional.pad(attention_mask, pad=(self.num_global_tokens, 0), value=1)\n",
        "            \n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            cross_attn_head_mask=cross_attn_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6JMBPxj73aYU"
      },
      "outputs": [],
      "source": [
        "class LSGBartForConditionalGeneration(LSGBartPretrainedModel, BartForConditionalGeneration):\n",
        "    \n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head\\.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        LSGBartPretrainedModel.__init__(self, config)\n",
        "        self.model = LSGBartModel(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "    \n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2yAWN4syVe8w"
      },
      "outputs": [],
      "source": [
        "AUTO_MAP = {\n",
        "        \"AutoModel\": \"modeling_lsg_bart.LSGBartModel\",\n",
        "        \"AutoModelForSeq2SeqLM\": \"modeling_lsg_bart.LSGBartForConditionalGeneration\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF0OGKJ2VaXP",
        "outputId": "c0c2e410-1532-44e5-8022-722a9d620512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-919a567cf55f>:6: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
            "  warn(\"AutoRegister isn't available, you'll have to manually copy modeling.py after .save_pretrained(...).\")\n",
            "WARNING:root:AutoRegister isn't available, you'll have to manually copy modeling.py after .save_pretrained(...).\n",
            "<ipython-input-30-919a567cf55f>:7: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
            "  warn(\"Update to transformers >= 4.23.1 to fix.\")\n",
            "WARNING:root:Update to transformers >= 4.23.1 to fix.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    LSGBartConfig.register_for_auto_class()\n",
        "    for key, value in AUTO_MAP.items():\n",
        "        str_to_class(value.split(\".\")[-1]).register_for_auto_class(key)\n",
        "except:\n",
        "    warn(\"AutoRegister isn't available, you'll have to manually copy modeling.py after .save_pretrained(...).\")\n",
        "    warn(\"Update to transformers >= 4.23.1 to fix.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nwDGH3heWG-l"
      },
      "outputs": [],
      "source": [
        "class ConversionScript():\n",
        "\n",
        "    _ARCHITECTURE_TYPE_DICT = {}\n",
        "    _ARCHITECTURE_TYPE_DICT = {**{\"LSG\" + k: v for k, v in _ARCHITECTURE_TYPE_DICT.items()}, **_ARCHITECTURE_TYPE_DICT}\n",
        "    _BASE_ARCHITECTURE_TYPE = None\n",
        "    _DEFAULT_ARCHITECTURE_TYPE = None\n",
        "    _CONFIG_MODULE = None\n",
        "\n",
        "    _DEFAULT_CONFIG_POSITIONAL_OFFSET = 0\n",
        "    _DEFAULT_POSITIONAL_OFFSET = 0\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        initial_model, \n",
        "        model_name, \n",
        "        max_sequence_length, \n",
        "        architecture, \n",
        "        random_global_init, \n",
        "        global_positional_stride, \n",
        "        keep_first_global_token, \n",
        "        resize_lsg, \n",
        "        model_kwargs, \n",
        "        use_token_ids,\n",
        "        use_auth_token,\n",
        "        config,\n",
        "        save_model,\n",
        "        seed\n",
        "        ):\n",
        "        \n",
        "        self.initial_model = initial_model\n",
        "        self.model_name = model_name\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.architecture = architecture\n",
        "        self.random_global_init = random_global_init\n",
        "        self.global_positional_stride = global_positional_stride\n",
        "        self.keep_first_global_token = keep_first_global_token\n",
        "        self.resize_lsg = resize_lsg\n",
        "        self.model_kwargs = model_kwargs\n",
        "        self.use_token_ids = use_token_ids\n",
        "        self.use_auth_token = use_auth_token\n",
        "        self.config = config\n",
        "        self.save_model = save_model\n",
        "\n",
        "    def save(self, model, tokenizer):\n",
        "\n",
        "        model.save_pretrained(self.model_name)\n",
        "        tokenizer.save_pretrained(self.model_name)\n",
        "\n",
        "    def process(self):\n",
        "        \n",
        "        (lsg_architecture, lsg_model), initial_architecture = self.get_architecture()\n",
        "        is_base_architecture, is_lsg, keep_first_global = self.get_additional_params(lsg_architecture, initial_architecture)\n",
        "        model, tokenizer = self.get_model(lsg_architecture, lsg_model)\n",
        "        model, tokenizer = self.update_config(model, tokenizer)\n",
        "\n",
        "        # Get the module prefix to update\n",
        "        module_prefix = self.get_module(model, is_base_architecture)\n",
        "\n",
        "        # Update global embedding\n",
        "        if not (is_lsg and self.resize_lsg):\n",
        "            bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id\n",
        "            bos_id = bos_id if bos_id is not None else model.config.bos_token_id\n",
        "            mask_id = tokenizer.mask_token_id\n",
        "            if self.random_global_init:\n",
        "                self.update_global_randomly(module_prefix, bos_id, self.global_positional_stride, keep_first_global)\n",
        "            else:\n",
        "                self.update_global(module_prefix, bos_id, mask_id, self.global_positional_stride, keep_first_global)\n",
        "\n",
        "        # Update positional\n",
        "        self.update_positions(module_prefix, self.max_sequence_length)\n",
        "\n",
        "        # For Pegasus\n",
        "        self.update_positions_with_model(model, self.max_sequence_length)\n",
        "\n",
        "        if self.save_model:\n",
        "            self.save(model, tokenizer)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def get_architecture(self):\n",
        "        if self.architecture is not None:\n",
        "            return self.validate_architecture(self.architecture)\n",
        "\n",
        "        architectures = self.config.architectures\n",
        "        if architectures is not None:\n",
        "            architecture = architectures if isinstance(architectures, str) else architectures[0]\n",
        "            return self.validate_architecture(architecture)\n",
        "\n",
        "        return self.validate_architecture(self._DEFAULT_ARCHITECTURE_TYPE)\n",
        "\n",
        "    def validate_architecture(self, architecture):\n",
        "        _architecture = self._ARCHITECTURE_TYPE_DICT.get(architecture, None)\n",
        "\n",
        "        s = \"\\n * \" + \"\\n * \".join([k for k in self._ARCHITECTURE_TYPE_DICT.keys()])\n",
        "        assert _architecture is not None, f\"Provided/config architecture is wrong, make sure it is in: {s}\"\n",
        "        return _architecture, architecture\n",
        "\n",
        "    def get_model(self, lsg_architecture, lsg_model):\n",
        "        config = self._CONFIG_MODULE.from_pretrained(\n",
        "            self.initial_model, \n",
        "            architectures=lsg_architecture, \n",
        "            trust_remote_code=True, \n",
        "            use_auth_token=self.use_auth_token,\n",
        "            **json.loads(self.model_kwargs.replace(\"'\", \"\\\"\"))\n",
        "            )\n",
        "        model = lsg_model.from_pretrained(self.initial_model, use_auth_token=self.use_auth_token, config=config, trust_remote_code=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.initial_model, use_auth_token=self.use_auth_token, trust_remote_code=True)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def update_config(self, model, tokenizer):\n",
        "\n",
        "        # Update tokenizer and config\n",
        "        tokenizer.model_max_length = self.max_sequence_length\n",
        "        tokenizer.init_kwargs['model_max_length'] = self.max_sequence_length\n",
        "\n",
        "        max_pos = self.max_sequence_length\n",
        "        model.config.max_position_embeddings = max_pos + self._DEFAULT_CONFIG_POSITIONAL_OFFSET\n",
        "        model.config._name_or_path = self.model_name\n",
        "        return model, tokenizer\n",
        "\n",
        "    def get_additional_params(self, _architecture, initial_architecture):\n",
        "\n",
        "        # Hack because of architecture\n",
        "        is_base_architecture = True if _architecture in [self._BASE_ARCHITECTURE_TYPE, \"LSG\" + self._BASE_ARCHITECTURE_TYPE] else False\n",
        "\n",
        "        # Check if it is LSG architecture\n",
        "        if vars(self.config).get(\"base_model_prefix\", None) == \"lsg\" or \"LSG\" in initial_architecture:\n",
        "            is_lsg_architecture = True\n",
        "        else: \n",
        "            is_lsg_architecture = False\n",
        "\n",
        "        if is_lsg_architecture and not self.resize_lsg:\n",
        "            warnings.warn(\"LSG architecture detected, to resize positional embedding only, add --resize_lsg (won't affect global embedding)\")\n",
        "        if is_lsg_architecture and not self.keep_first_global_token and not self.resize_lsg:\n",
        "            warnings.warn(\"LSG architecture detected, to keep the same first global token, add --keep_first_global_token\")\n",
        "\n",
        "        keep_first = False\n",
        "        if self.keep_first_global_token:\n",
        "            if is_lsg_architecture:\n",
        "                keep_first = True\n",
        "            else:\n",
        "                warnings.warn(\"--keep_first_global_token won't be used if the initial model isn't a LSG model\")\n",
        "        return is_base_architecture, is_lsg_architecture, keep_first\n",
        "\n",
        "    def get_module(self, model, is_base_architecture):\n",
        "        if is_base_architecture:\n",
        "            return\n",
        "        return\n",
        "\n",
        "    def update_global_randomly(self, module_prefix, bos_id, stride, keep_first_global):\n",
        "        pass\n",
        "\n",
        "    def update_global(self, module_prefix, bos_id, mask_id, stride, keep_first_global):\n",
        "        pass\n",
        "\n",
        "    def update_positions(self, module_prefix, max_pos):\n",
        "        pass\n",
        "    \n",
        "    def update_positions_with_model(self, model, max_pos):\n",
        "        pass\n",
        "\n",
        "    def order_positions(self, positions, stride):\n",
        "        n, d = positions.size()\n",
        "        if n % 512 != 0:\n",
        "            if n > 512:\n",
        "                positions = positions[:512*(n//512)]\n",
        "            else:\n",
        "                mean = positions.mean(dim=0, keepdim=True).expand(512 - n, -1)\n",
        "                std = positions.std(dim=0, keepdim=True).expand(512 - n, -1)\n",
        "                positions = torch.cat([positions, torch.normal(mean, std)], dim=0)\n",
        "            n, d = positions.size()\n",
        "\n",
        "        factor = n // 512\n",
        "        positions = positions.reshape(-1, factor, d)[:, 0]\n",
        "        positions = positions.reshape(-1, stride//factor, d).transpose(0, 1).reshape(-1, d)\n",
        "        return positions\n",
        "\n",
        "    def run_test(self):\n",
        "        pass\n",
        "    \n",
        "    def run_models(self, lsg_path, max_length, hidden_size, text, auto_map, gradient_checkpointing=True, is_encoder_decoder=False):\n",
        "\n",
        "        from transformers import AutoTokenizer, AutoConfig, AutoModel, pipeline\n",
        "        from transformers import AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModelForQuestionAnswering\n",
        "        from transformers import AutoModelForMaskedLM, AutoModelForCausalLM\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(lsg_path)\n",
        "        \n",
        "        long_text = text * 200\n",
        "\n",
        "        for name in auto_map.keys():\n",
        "\n",
        "            if name == \"AutoConfig\":\n",
        "                continue\n",
        "\n",
        "            model = getattr(sys.modules[\"transformers\"], name)\n",
        "            print(\"\\n\\n\" + \"=\"*5 + \" \" + name + \" \" + \"=\"*5 + \"\\n\")\n",
        "            model = model.from_pretrained(lsg_path, trust_remote_code=True, is_decoder=\"Causal\" in name)\n",
        "            \n",
        "            if gradient_checkpointing:\n",
        "                model.gradient_checkpointing_enable()\n",
        "\n",
        "            if \"QuestionAnswering\" in name:\n",
        "                tokens = tokenizer(\"context\", long_text, return_tensors=\"pt\", truncation=True)\n",
        "                inputs_embeds = torch.randn(1, max_length, hidden_size)\n",
        "            elif \"MultipleChoice\" in name:\n",
        "                num_choices = 4\n",
        "                tokens = tokenizer([long_text]*num_choices, return_tensors=\"pt\", truncation=True)\n",
        "                tokens = {k: v.reshape(1, num_choices, -1) for k, v in tokens.items()}\n",
        "                inputs_embeds = torch.randn(1, num_choices, max_length//4, hidden_size)\n",
        "            else:\n",
        "                tokens = tokenizer(long_text, return_tensors=\"pt\", truncation=True)\n",
        "                inputs_embeds = torch.randn(1, max_length, hidden_size)\n",
        "\n",
        "            if model.config.model_type != \"pegasus\":\n",
        "                model(**tokens)\n",
        "                \n",
        "            if not is_encoder_decoder:\n",
        "                model(inputs_embeds=inputs_embeds)\n",
        "            elif \"decoder_input_ids\" in model.forward.__code__.co_varnames:\n",
        "                decoder_input_ids = tokens.input_ids[:, :256]\n",
        "                if \"SequenceClassification\" not in name:\n",
        "                    model(**tokens, decoder_input_ids=decoder_input_ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IE_kPx3dWKPe"
      },
      "outputs": [],
      "source": [
        "class BartConversionScript(ConversionScript):\n",
        "\n",
        "    _ARCHITECTURE_TYPE_DICT = {\n",
        "        \"BartModel\": (\"LSGBartModel\", LSGBartModel),\n",
        "        \"BartForConditionalGeneration\": (\"LSGBartForConditionalGeneration\", LSGBartForConditionalGeneration),\n",
        "        # \"BartForCausalLM\": (\"LSGBartForCausalLM\", LSGBartForCausalLM),\n",
        "        # \"BartForQuestionAnswering\": (\"LSGBartForQuestionAnswering\", LSGBartForQuestionAnswering),\n",
        "        # \"BartForSequenceClassification\": (\"LSGBartForSequenceClassification\", LSGBartForSequenceClassification),\n",
        "        }\n",
        "    _ARCHITECTURE_TYPE_DICT = {**{\"LSG\" + k: v for k, v in _ARCHITECTURE_TYPE_DICT.items()}, **_ARCHITECTURE_TYPE_DICT}\n",
        "\n",
        "    _BASE_ARCHITECTURE_TYPE = \"BartModel\"\n",
        "    _DEFAULT_ARCHITECTURE_TYPE = \"BartForConditionalGeneration\"\n",
        "    _CONFIG_MODULE = LSGBartConfig\n",
        "\n",
        "    _DEFAULT_CONFIG_POSITIONAL_OFFSET = 0\n",
        "    _DEFAULT_POSITIONAL_OFFSET = 2\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_module(self, model, is_base_architecture):\n",
        "        if is_base_architecture:\n",
        "            return model\n",
        "        return model.model\n",
        "\n",
        "    def update_global_randomly(self, module_prefix, bos_id, stride, keep_first_global):\n",
        "\n",
        "        import torch\n",
        "        from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "        u = module_prefix.shared.weight.clone()\n",
        "        cov = torch.cov(u.T)\n",
        "        m = MultivariateNormal(u.mean(dim=0), cov)\n",
        "        w = m.sample((512,))\n",
        "        w[0] = u[bos_id]\n",
        "\n",
        "        positions = module_prefix.encoder.embed_positions.weight.clone()[self._DEFAULT_POSITIONAL_OFFSET:] \n",
        "        positions = self.order_positions(positions, stride)\n",
        "        \n",
        "        if keep_first_global:\n",
        "            module_prefix.encoder.global_embeddings.weight.data[1:] = (w + positions)[1:]\n",
        "        else:\n",
        "            module_prefix.encoder.global_embeddings.weight.data = w + positions\n",
        "\n",
        "    def update_global(self, module_prefix, bos_id, mask_id, stride, keep_first_global):\n",
        "\n",
        "        u = module_prefix.shared.weight.clone()\n",
        "        positions = module_prefix.encoder.embed_positions.weight.clone()[self._DEFAULT_POSITIONAL_OFFSET:]\n",
        "        positions = self.order_positions(positions, stride)\n",
        "\n",
        "        positions[0] += u[bos_id]\n",
        "        positions[1:] += u[mask_id].unsqueeze(0)\n",
        "\n",
        "        if keep_first_global:\n",
        "            module_prefix.encoder.global_embeddings.weight.data[1:] = positions[1:]\n",
        "        else:\n",
        "            module_prefix.encoder.global_embeddings.weight.data = positions\n",
        "\n",
        "    def update_positions(self, module_prefix, max_pos):\n",
        "\n",
        "        # Encoder\n",
        "        position_embeddings_weights = module_prefix.encoder.embed_positions.weight.clone()\n",
        "        current_max_position = position_embeddings_weights.size()[0]\n",
        "\n",
        "        new_position_embeddings_weights = torch.cat([\n",
        "            position_embeddings_weights[:self._DEFAULT_POSITIONAL_OFFSET]] + \n",
        "            [position_embeddings_weights[self._DEFAULT_POSITIONAL_OFFSET:] for _ in range(max_pos//current_max_position + 1)], \n",
        "            dim=0)[:max_pos + self._DEFAULT_POSITIONAL_OFFSET]\n",
        "\n",
        "        #processed_module.encoder.embed_positions.position_ids = torch.arange(max_pos + 2, device=processed_module.encoder.embed_positions.position_ids.device).unsqueeze(0)\n",
        "        module_prefix.encoder.embed_positions.weight.data = new_position_embeddings_weights\n",
        "\n",
        "        # Decoder\n",
        "        position_embeddings_weights = module_prefix.decoder.embed_positions.weight.clone()\n",
        "        current_max_position = position_embeddings_weights.size()[0]\n",
        "\n",
        "        new_position_embeddings_weights = torch.cat([\n",
        "            position_embeddings_weights[:self._DEFAULT_POSITIONAL_OFFSET]] + \n",
        "            [position_embeddings_weights[self._DEFAULT_POSITIONAL_OFFSET:] for _ in range(max_pos//current_max_position + 1)], \n",
        "            dim=0)[:max_pos + self._DEFAULT_POSITIONAL_OFFSET]\n",
        "\n",
        "        module_prefix.decoder.embed_positions.weight.data = new_position_embeddings_weights\n",
        "\n",
        "    def run_test(self):\n",
        "        \n",
        "        from transformers import AutoConfig, AutoTokenizer\n",
        "\n",
        "        initial_path = self.initial_model\n",
        "        lsg_path = self.model_name\n",
        "\n",
        "        config = AutoConfig.from_pretrained(lsg_path, trust_remote_code=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(lsg_path)\n",
        "        text = f\"Paris is the {tokenizer.mask_token} of France.\"\n",
        "\n",
        "        max_length = config.max_position_embeddings - 20\n",
        "        hidden_size = config.hidden_size\n",
        "\n",
        "        self.run_models(lsg_path, max_length, hidden_size, text, AUTO_MAP, is_encoder_decoder=True)\n",
        "        self.run_pipeline(lsg_path, initial_path, tokenizer, text)\n",
        "\n",
        "    def run_pipeline(self, lsg_path, initial_path, tokenizer, text):\n",
        "\n",
        "        from transformers import AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(lsg_path, trust_remote_code=True)\n",
        "        pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "        pipe_lsg = pipe(text)\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(initial_path, trust_remote_code=True)\n",
        "        pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "        pipe_initial = pipe(text)\n",
        "  \n",
        "        print(\"\\n\\n\" + \"=\"*5 + \" LSG PIPELINE \" + \"=\"*5 + \"\\n\")\n",
        "        print(text)\n",
        "        print(pipe_lsg[0])\n",
        "        print(\"\\n\\n\" + \"=\"*5 + \" INITIAL PIPELINE \" + \"=\"*5 + \"\\n\")\n",
        "        print(text)\n",
        "        print(pipe_initial[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "n9M1IW3ZDJgM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "from transformers.models.auto.modeling_auto import *\n",
        "import json\n",
        "\n",
        "# from .albert.convert_albert_checkpoint import *\n",
        "# from .bart.convert_bart_checkpoint import *\n",
        "# from .barthez.convert_barthez_checkpoint import *\n",
        "# from .bert.convert_bert_checkpoint import *\n",
        "# from .camembert.convert_camembert_checkpoint import *\n",
        "# from .distilbert.convert_distilbert_checkpoint import *\n",
        "# from .electra.convert_electra_checkpoint import *\n",
        "# from .mbart.convert_mbart_checkpoint import *\n",
        "# from .pegasus.convert_pegasus_checkpoint import *\n",
        "# from .roberta.convert_roberta_checkpoint import *\n",
        "# from .xlm_roberta.convert_xlm_roberta_checkpoint import *\n",
        "\n",
        "_AUTH_MODELS = {\n",
        "    # \"albert\": AlbertConversionScript,\n",
        "    \"bart\": BartConversionScript,\n",
        "    # \"barthez\": BarthezConversionScript,\n",
        "    # \"bert\": BertConversionScript,\n",
        "    # \"camembert\": CamembertConversionScript,\n",
        "    # \"distilbert\": DistilBertConversionScript,\n",
        "    # \"electra\": ElectraConversionScript,\n",
        "    # \"mbart\": MBartConversionScript,\n",
        "    # \"pegasus\": PegasusConversionScript,\n",
        "    # \"roberta\": RobertaConversionScript,\n",
        "    # \"xlm-roberta\": XLMRobertaConversionScript,\n",
        "}\n",
        "\n",
        "class LSGConverter():\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        max_sequence_length=4096, \n",
        "        random_global_init=False, \n",
        "        global_positional_stride=64, \n",
        "        keep_first_global_token=False, \n",
        "        resize_lsg=False, \n",
        "        use_token_ids=True, \n",
        "        use_auth_token=True,\n",
        "        hub_token = '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',\n",
        "        seed=123, \n",
        "        ):\n",
        "        \"\"\"\n",
        "        max_sequence_length (int): new max sequence length\n",
        "        random_global_init (bool): randomly initialize global tokens\n",
        "        global_positional_stride (int): position stride between global tokens\n",
        "        keep_first_global_token (bool): keep or replace the first global token (<s> + pos 0)\n",
        "        resize_lsg (bool): only resize an existing LSG model\n",
        "        use_token_ids (bool): use token_type_ids to build global tokens\n",
        "        use_auth_token (bool): use HF auth token or not\n",
        "        seed (int): seed\n",
        "        \"\"\"\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.random_global_init = random_global_init\n",
        "        self.global_positional_stride = global_positional_stride\n",
        "        self.keep_first_global_token = keep_first_global_token\n",
        "        self.resize_lsg = resize_lsg\n",
        "        self.use_token_ids = use_token_ids\n",
        "        self.seed = seed\n",
        "\n",
        "    def convert_from_pretrained(\n",
        "        self, \n",
        "        model_name_or_path, \n",
        "        architecture=None, \n",
        "        use_auth_token=False,\n",
        "        **model_kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        mode_name_or_path (str): path to the model to convert\n",
        "        architecture (str): specific architecture (optional)\n",
        "        model_kwargs: additional model args\n",
        "        \"\"\"\n",
        "\n",
        "        config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True, use_auth_token=use_auth_token)\n",
        "\n",
        "        model_type = config.model_type\n",
        "        model_kwargs = json.dumps(model_kwargs, indent=4)\n",
        "\n",
        "        if model_type in _AUTH_MODELS.keys():\n",
        "            converter = _AUTH_MODELS[model_type](\n",
        "                initial_model=model_name_or_path, \n",
        "                model_name=model_name_or_path, \n",
        "                max_sequence_length=self.max_sequence_length, \n",
        "                architecture=architecture, \n",
        "                random_global_init=self.random_global_init, \n",
        "                global_positional_stride=self.global_positional_stride, \n",
        "                keep_first_global_token=self.keep_first_global_token, \n",
        "                resize_lsg=self.resize_lsg, \n",
        "                model_kwargs=model_kwargs, \n",
        "                use_token_ids=self.use_token_ids,\n",
        "                use_auth_token=use_auth_token,\n",
        "                config=config,\n",
        "                save_model=False,\n",
        "                seed=self.seed\n",
        "                )\n",
        "            return converter.process()\n",
        "\n",
        "# \"\"\"\n",
        "# model, tokenizer = LSGConverter().convert_from_pretrained(\"bert-base-uncased\", num_global_tokens=15)\n",
        "# print(model)\n",
        "# print(model.config)\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbgPTjYmHwNn",
        "outputId": "adcd79ed-48db-4616-f449-d628780f8a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.9/dist-packages (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (3.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BwR8U2IMDUPe"
      },
      "outputs": [],
      "source": [
        "converter = LSGConverter(max_sequence_length=4096)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "qhA6vvWZERCU",
        "outputId": "093f0d25-fb42-427d-9ab7-7b165f053f24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d51489c7edb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/bart-large\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BartModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'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'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-7dd1229a263c>\u001b[0m in \u001b[0;36mconvert_from_pretrained\u001b[0;34m(self, model_name_or_path, architecture, use_auth_token, **model_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 )\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-36bf434438b5>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mlsg_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsg_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_architecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mis_base_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_lsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_first_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_additional_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsg_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_architecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsg_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsg_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-36bf434438b5>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, lsg_architecture, lsg_model)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             )\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2498\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0;31m# Check first if we are `from_pt`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-00db3ae035c8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# self.num_global_tokens = config.num_global_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSGBartEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LSGBartEncoder' is not defined"
          ]
        }
      ],
      "source": [
        "model, tokenizer = converter.convert_from_pretrained(\"facebook/bart-large\", architecture=\"BartModel\", use_auth_token='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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_qEeOA2a5TZ"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw7LoJSuG_Pg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGqOWr0EG_hM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwC-ddwW5w19"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBVZuroi62NQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model = AutoModel.from_pretrained(\"ccdv/lsg-bart-large-4096\", trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ccdv/lsg-bart-large-4096\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il8yhy0i67dt"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "# from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TLXUmxS_nG7"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"ccdv/lsg-bart-base-4096\", \n",
        "    trust_remote_code=True, \n",
        "    num_global_tokens=64,\n",
        "    block_size=64,\n",
        "    sparse_block_size=64,\n",
        "    sparsity_factor=2,\n",
        "    sparsity_type=\"norm\",\n",
        "    mask_first_token=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABxA6EdC7wwn"
      },
      "outputs": [],
      "source": [
        "max_input = 2048\n",
        "max_target = 512\n",
        "batch_size = 3\n",
        "model_checkpoints = \"ccdv/lsg-bart-large-4096\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCcY_44b8iOn"
      },
      "outputs": [],
      "source": [
        "pip install glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc38Te398pVh"
      },
      "outputs": [],
      "source": [
        " import glob2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikMzixQX-I-X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7APaYbC8QrK"
      },
      "outputs": [],
      "source": [
        "# dataset_test_path = '/content/sample_data/train-dataset'\n",
        "# dataset_test_all_files = glob2.glob(dataset_test_path+ \"/*.txt\")\n",
        "# len(dataset_test_all_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV5bK6IS8seU"
      },
      "outputs": [],
      "source": [
        "# dataset_summary_path = '/content/sample_data/train-summary'\n",
        "# dataset_summary_path_all_files = glob2.glob(dataset_summary_path + \"/*.txt\")\n",
        "# len(dataset_summary_path_all_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziE-CSdnQMYZ"
      },
      "outputs": [],
      "source": [
        "# validate_dataset_test_path = '/content/sample_data/validate-dataset'\n",
        "# validate_dataset_test_all_files = glob2.glob(validate_dataset_test_path+ \"/*.txt\")\n",
        "# len(validate_dataset_test_all_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAY2nSJSQou3"
      },
      "outputs": [],
      "source": [
        "# validate_dataset_summary_path = '/content/sample_data/validate-summary'\n",
        "# validate_dataset_summary_path_all_files = glob2.glob(validate_dataset_summary_path + \"/*.txt\")\n",
        "# len(validate_dataset_summary_path_all_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMh-pHKp89M3"
      },
      "outputs": [],
      "source": [
        "# data_source = []\n",
        "# names = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCYU2Q5h83Pn"
      },
      "outputs": [],
      "source": [
        "# for filename in dataset_test_all_files:\n",
        "#         with open(filename, 'r') as f:\n",
        "#             p = filename.rfind(\"/\")\n",
        "#             names.append(filename[p+1:])\n",
        "#             a = f.read()\n",
        "#             data_source.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6NHzLVIQv-O"
      },
      "outputs": [],
      "source": [
        "# validate_data_source = []\n",
        "# validate_names = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZbvLBqGQ0W7"
      },
      "outputs": [],
      "source": [
        "# for filename in validate_dataset_test_all_files:\n",
        "#         with open(filename, 'r') as f:\n",
        "#             p = filename.rfind(\"/\")\n",
        "#             validate_names.append(filename[p+1:])\n",
        "#             a = f.read()\n",
        "#             validate_data_source.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92FEyv4u8-gM"
      },
      "outputs": [],
      "source": [
        "# data_summary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYMxYGdj9AqP"
      },
      "outputs": [],
      "source": [
        "# for filename in dataset_summary_path_all_files:\n",
        "#         with open(filename, 'r') as f: \n",
        "#             a = f.read()\n",
        "#             l = len(a)\n",
        "#             data_summary.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tJu31CKQ-eF"
      },
      "outputs": [],
      "source": [
        "# validate_data_summary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBAwsCZnRBMD"
      },
      "outputs": [],
      "source": [
        "# for filename in validate_dataset_summary_path_all_files:\n",
        "#         with open(filename, 'r') as f: \n",
        "#             a = f.read()\n",
        "#             l = len(a)\n",
        "#             validate_data_summary.append(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taDIyNXT8Ilv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "# import textrank # or any other summarization library you prefer\n",
        "# for i in range (1,5):\n",
        "# data = []\n",
        "\n",
        "# for i in range(1,40):\n",
        "#   filename = str(i)+'.txt'\n",
        "#   datasetfilename = '/content/sample_data/data-train/'+str(i)+ '.txt' \n",
        "#   summaryfilename = '/content/sample_data/data-summary-train/'+str(i)+ '.txt' \n",
        "#   with open(datasetfilename, \"r\", encoding = \"utf-8\") as f:\n",
        "#     text = f.read()\n",
        "#   with open(summaryfilename, \"r\", encoding = \"utf-8\") as f:\n",
        "#     summary = f.read()\n",
        "  \n",
        "#   data.append({\"id\": filename,\"dialogue\": text, \"summary\": summary}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t92NPYbLLAmo"
      },
      "outputs": [],
      "source": [
        "# with open('/content/data.csv', \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "#     writer = csv.DictWriter(f, fieldnames=[\"id\", \"dialogue\", \"summary\"])\n",
        "#     writer.writeheader()\n",
        "#     writer.writerows(data) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz-IlBUcLNiO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD7c6qJvLPxt"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/sample_data/bert_passed_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3xtDzYUDMRf"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI-83UO01dYn"
      },
      "outputs": [],
      "source": [
        "validation_df = pd.read_csv('/content/sample_data/bert_passed_validation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6otwhON1j5w"
      },
      "outputs": [],
      "source": [
        "validation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOKguTmTDJ3F"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric, load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgzCBDke-QOL"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LE4wSWrDmUQ"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6rqk2sI1tFE"
      },
      "outputs": [],
      "source": [
        "validation_dataset = datasets.Dataset.from_pandas(validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTbAzXzX1w_Q"
      },
      "outputs": [],
      "source": [
        "validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjhlWqWCCNw0"
      },
      "outputs": [],
      "source": [
        "padding = \"max_length\"\n",
        "ignore_pad_token_for_loss = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJIoBVTlLes6"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_to_process):\n",
        "  #get the dialogue text\n",
        "  inputs = [dialogue for dialogue in data_to_process['judgement']]\n",
        "  #tokenize text\n",
        "  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n",
        "\n",
        "  #tokenize labels\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    targets = tokenizer(data_to_process['summary'], max_length=max_target, padding='max_length', truncation=True)\n",
        "    \n",
        "  model_inputs['labels'] = targets['input_ids']\n",
        "  #reuturns input_ids, attention_masks, labels\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymqdvVUgM8Zi"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset, load_metric, load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7VmrsL0DnOz"
      },
      "outputs": [],
      "source": [
        "train_tokenize_data = train_dataset.map(preprocess_data, batched = True, remove_columns=['Filename', 'judgement', 'summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um0EmYHT2DTI"
      },
      "outputs": [],
      "source": [
        "validation_tokenize_data = validation_dataset.map(preprocess_data, batched = True, remove_columns=['Filename', 'judgement', 'summary'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk6o7p9t-B8E"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset, load_metric, load_from_disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qybABebU-TZw"
      },
      "outputs": [],
      "source": [
        "'/content/sample_data/dataset-training'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZHEfEnO-xLv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "havzqtDR8QV0"
      },
      "outputs": [],
      "source": [
        "# tokenize_data = model_inputs\n",
        "# samples = [tokenize_data[i] for i in range(8)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGSX7sVe9_L4"
      },
      "outputs": [],
      "source": [
        "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBHQK0kvBWRf"
      },
      "outputs": [],
      "source": [
        "# train_sample = tokenize_data.shuffle(seed=123).select(range(1000))\n",
        "# validation_sample = tokenize_data.shuffle(seed=123).select(range(500))\n",
        "# test_sample = tokenize_data.shuffle(seed=123).select(range(200))\n",
        "\n",
        "# train_sample = tokenize_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zrn3PhAP_jNQ"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints, trust_remote_code = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxlsfCcYCd6-"
      },
      "outputs": [],
      "source": [
        "# model.trust_remote_code=True, \n",
        "model.num_global_tokens=256,\n",
        "model.block_size=256,\n",
        "model.sparse_block_size=256,\n",
        "model.sparsity_factor=2,\n",
        "model.sparsity_type=\"norm\",\n",
        "model.mask_first_token=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnfp8t2HCA9T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KhbtIxI_3Rn"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W73jrQPVChXI"
      },
      "outputs": [],
      "source": [
        "label_pad_token_id = -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgLoDfNV_4La"
      },
      "outputs": [],
      "source": [
        "collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=label_pad_token_id,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gH_Coca_7zt"
      },
      "outputs": [],
      "source": [
        "def compute_rouge(pred):\n",
        "  predictions, labels = pred\n",
        "  #decode the predictions\n",
        "  decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  #decode labels\n",
        "  decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  #compute results\n",
        "  res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n",
        "  #get %\n",
        "  res = {key: value.mid.fmeasure * 100 for key, value in res.items()}\n",
        "\n",
        "  pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "  res['gen_len'] = np.mean(pred_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k, v in res.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO-KZAswAAGs"
      },
      "outputs": [],
      "source": [
        "args = transformers.Seq2SeqTrainingArguments(\n",
        "    'conversation-summ',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size= 1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    fp16=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjGbVReHSnj6"
      },
      "outputs": [],
      "source": [
        "labels = [1, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMqDEu26RLlp"
      },
      "outputs": [],
      "source": [
        "pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SGFmoZJRIVm"
      },
      "outputs": [],
      "source": [
        "metric = load_metric('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il_0TopSSzXL"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision \n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF0ZpwI0AK6o"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model, \n",
        "    args,\n",
        "    train_dataset=train_tokenize_data,\n",
        "    eval_dataset=validation_tokenize_data,\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_rouge\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTnUjqXXAPyo"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCf_2p20Pv21"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsefFPLHAT2O"
      },
      "outputs": [],
      "source": [
        " trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## REFERENCES\n",
        "\n",
        "Inspired from: \n",
        "\n",
        "https://github.com/AldoF95/bart-chat-summarizer-finetuning/blob/main/Bart_large_xsum_fine_tuned_samsum.ipynb\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/bart\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/bert"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03e66732222d4ec78972292fbe5aa684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_850d4b5a59734fa4862ef26c8c8839ce",
            "placeholder": "",
            "style": "IPY_MODEL_ec222d9d914b4102ab9cfafa23aa3586",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.72kB/s]"
          }
        },
        "0929f3d9895b4e6394701c8753ee9ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b2c8bf3af7e45d5a432a96be04c175e",
            "placeholder": "",
            "style": "IPY_MODEL_41d64bad4f1a46ab94f829b474edda76",
            "value": "Downloading ()lve/main/config.json: 100%"
          }
        },
        "096a02f4765c402a866dddbb978980d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4ef61d15844e3aa76fb35c38407945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b2c8bf3af7e45d5a432a96be04c175e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e5558da06274b96b02ea90ddce78379": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1056ad2f318445b3ada9ef306626aea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7704d52ef81749188242093e16d9079e",
              "IPY_MODEL_f59205ed75f24837a0444095d8a58555",
              "IPY_MODEL_633f086dc7ca4bed8a56f0d6e91ae08b"
            ],
            "layout": "IPY_MODEL_c5d7928c9f214558a3f293e4e1248daf"
          }
        },
        "11f56497be194c18af6af207339882d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f23bf2c3204bcd9f08590d610b93c7",
            "max": 1628,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49a9a6fa9b534611ab954c03d60de3ec",
            "value": 1628
          }
        },
        "165fabcb18134fee8846a0bdcb845dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17a969daf49041cfb7ea16342efcab78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c6360f3153c4a24a254942c7b78f6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb15e040f7048119d6ce1e047536571": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7ff5dbb4b5c4e5787b0225aa9aa6995",
            "placeholder": "",
            "style": "IPY_MODEL_1c6360f3153c4a24a254942c7b78f6a0",
            "value": " 899k/899k [00:01&lt;00:00, 813kB/s]"
          }
        },
        "2a999ecf046343c7bd5f11bd59b02389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_096a02f4765c402a866dddbb978980d3",
            "placeholder": "",
            "style": "IPY_MODEL_8184a9a3df3a4c8e967ebfc12eba3f7a",
            "value": "Downloading ()okenizer_config.json: 100%"
          }
        },
        "2c435b2e93a8423caeec1be41cd15ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a4452bf97f4f06aa472e7fe58b64b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1fdb0d09ef4e7fafb99ba2537cfbb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a999ecf046343c7bd5f11bd59b02389",
              "IPY_MODEL_a2e8d99b608249dc83325defc199edbb",
              "IPY_MODEL_03e66732222d4ec78972292fbe5aa684"
            ],
            "layout": "IPY_MODEL_0a4ef61d15844e3aa76fb35c38407945"
          }
        },
        "3e9c348fe5fe4bd7bca352cba0ced7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d64bad4f1a46ab94f829b474edda76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "457a21fbcf5f46debf43ed1953f03b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a9a6fa9b534611ab954c03d60de3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "633f086dc7ca4bed8a56f0d6e91ae08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9340426833457b855ef32d2c63bad4",
            "placeholder": "",
            "style": "IPY_MODEL_d5b3990ee083405f90957c7b52a7ff7e",
            "value": " 1.02G/1.02G [00:08&lt;00:00, 124MB/s]"
          }
        },
        "7704d52ef81749188242093e16d9079e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe4ba6ecd3ea45ab9fc6440bae450fc2",
            "placeholder": "",
            "style": "IPY_MODEL_0e5558da06274b96b02ea90ddce78379",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8184a9a3df3a4c8e967ebfc12eba3f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81dc1c33d6a54a608c76bb7d4b0dd304": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "850d4b5a59734fa4862ef26c8c8839ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a56d7122e974bb2b1bbf6b505217078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b98fb35807544e4fba47a6c51565bfec",
              "IPY_MODEL_8fa9ceeb1d554ef6bf04755f09f922fb",
              "IPY_MODEL_a6d29e4635284c26a9ad755429fd33c8"
            ],
            "layout": "IPY_MODEL_3e9c348fe5fe4bd7bca352cba0ced7e2"
          }
        },
        "8fa9ceeb1d554ef6bf04755f09f922fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c435b2e93a8423caeec1be41cd15ff5",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aad33dcb8df24b86b31b59522f150a6d",
            "value": 456318
          }
        },
        "96090ae37c4b42ca9d191feb6189a171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981e13580da84a79b87b52edc6dc1902": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d32d932a50b4db3a16a063c3f550787": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dd188fed0f34e73a8f6b2517dd54726": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e9340426833457b855ef32d2c63bad4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2e8d99b608249dc83325defc199edbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34a4452bf97f4f06aa472e7fe58b64b0",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d7d18497bf47e48213b036d040d1f4",
            "value": 26
          }
        },
        "a3647034dd4d48ce9a8a6d4cc7f06865": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6d29e4635284c26a9ad755429fd33c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc9aaab69e71477ab6054570e524f136",
            "placeholder": "",
            "style": "IPY_MODEL_165fabcb18134fee8846a0bdcb845dd3",
            "value": " 456k/456k [00:00&lt;00:00, 510kB/s]"
          }
        },
        "aad33dcb8df24b86b31b59522f150a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b98fb35807544e4fba47a6c51565bfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81dc1c33d6a54a608c76bb7d4b0dd304",
            "placeholder": "",
            "style": "IPY_MODEL_17a969daf49041cfb7ea16342efcab78",
            "value": "Downloading ()olve/main/merges.txt: 100%"
          }
        },
        "bb0e089851f34b468c9a4ca1535eab66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d7d18497bf47e48213b036d040d1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5d7928c9f214558a3f293e4e1248daf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c762fe02b295418fada6833c35cd1528": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0929f3d9895b4e6394701c8753ee9ed4",
              "IPY_MODEL_11f56497be194c18af6af207339882d0",
              "IPY_MODEL_f0a97fc57f9c4734bdb2981d8448c5f9"
            ],
            "layout": "IPY_MODEL_457a21fbcf5f46debf43ed1953f03b3a"
          }
        },
        "cdf81b8e07c04d6298572665c9ecd797": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3b96716f33d4c12b60a7e7c881f7b49",
              "IPY_MODEL_e5ce2309797f405c99db6f9d50adae85",
              "IPY_MODEL_1eb15e040f7048119d6ce1e047536571"
            ],
            "layout": "IPY_MODEL_96090ae37c4b42ca9d191feb6189a171"
          }
        },
        "d2f23bf2c3204bcd9f08590d610b93c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b3990ee083405f90957c7b52a7ff7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc9aaab69e71477ab6054570e524f136": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e314a4c1aa46a996bfde249e6d451f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5ce2309797f405c99db6f9d50adae85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffadc80c4b8c453db25f289a251da07f",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_981e13580da84a79b87b52edc6dc1902",
            "value": 898822
          }
        },
        "ec222d9d914b4102ab9cfafa23aa3586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6d3fd10836403689efcf3bff9e5e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0a97fc57f9c4734bdb2981d8448c5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3647034dd4d48ce9a8a6d4cc7f06865",
            "placeholder": "",
            "style": "IPY_MODEL_e0e314a4c1aa46a996bfde249e6d451f",
            "value": " 1.63k/1.63k [00:00&lt;00:00, 124kB/s]"
          }
        },
        "f3b96716f33d4c12b60a7e7c881f7b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb0e089851f34b468c9a4ca1535eab66",
            "placeholder": "",
            "style": "IPY_MODEL_ef6d3fd10836403689efcf3bff9e5e34",
            "value": "Downloading ()olve/main/vocab.json: 100%"
          }
        },
        "f59205ed75f24837a0444095d8a58555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd188fed0f34e73a8f6b2517dd54726",
            "max": 1018571383,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d32d932a50b4db3a16a063c3f550787",
            "value": 1018571383
          }
        },
        "f7ff5dbb4b5c4e5787b0225aa9aa6995": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4ba6ecd3ea45ab9fc6440bae450fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffadc80c4b8c453db25f289a251da07f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
