{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBOOST with Doc2Vec word embedding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Contents of this file</h3>\n",
    "\n",
    "1. XGBoost basic model\n",
    "\n",
    "2. Hyper-parameter tuning\n",
    "\n",
    "3. Cross validation of final model\n",
    "\n",
    "4. Micro-scopic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = pd.read_pickle(\"norm_train_data.uu\")\n",
    "norm_test_data = pd.read_pickle(\"norm_test_data.uu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = norm_train_data['label'].astype(int)\n",
    "test_labels = norm_test_data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of normalized train data: (807647, 36)\n",
      "current columns:\n",
      "Index(['index', 'label', 'comment', 'author', 'subreddit', 'score', 'ups',\n",
      "       'downs', 'date', 'created_utc', 'parent_comment', 'word_count',\n",
      "       'punctuation_count', 'has_repeated', 'exclaim_count', 'qns_mark_count',\n",
      "       'ellipses_mark_count', 'interjection_count', 'laughter_words_count',\n",
      "       'capitalized_word_count', 'partial_capital_word_count',\n",
      "       'emoticon_count', 'clean_comment', 'lemmatized_comment',\n",
      "       'lemmatized_parent_comment', 'clean_parent_comment',\n",
      "       'lemmatized_clean_comment', 'clean_lemmatized_parent_comment',\n",
      "       'vectorized_comment', 'vectorized_clean_comment', 'vectorized_parent',\n",
      "       'vectorized_clean_parent', 'cosine_similarity_dirty_comments',\n",
      "       'cosine_similarity_clean_comments', 'comment_sentiment',\n",
      "       'parent_comment_sentiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('shape of normalized train data: {}'.format(norm_train_data.shape))\n",
    "print('current columns:\\n{}'.format(norm_train_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>cosine_similarity_clean_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.076470e+05</td>\n",
       "      <td>807647.00000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.00000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "      <td>807647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.053429e+05</td>\n",
       "      <td>0.50035</td>\n",
       "      <td>6.912703</td>\n",
       "      <td>5.529600</td>\n",
       "      <td>-0.146174</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.00039</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.518440</td>\n",
       "      <td>0.510574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.918408e+05</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>47.521331</td>\n",
       "      <td>41.687109</td>\n",
       "      <td>0.353281</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.111349</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.035844</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.038180</td>\n",
       "      <td>0.00287</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.107341</td>\n",
       "      <td>0.108868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-507.000000</td>\n",
       "      <td>-507.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.525755e+05</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446967</td>\n",
       "      <td>0.437965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.053250e+05</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.508140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.580795e+05</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590040</td>\n",
       "      <td>0.582497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.010824e+06</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>5818.000000</td>\n",
       "      <td>5163.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index         label          score            ups  \\\n",
       "count  8.076470e+05  807647.00000  807647.000000  807647.000000   \n",
       "mean   5.053429e+05       0.50035       6.912703       5.529600   \n",
       "std    2.918408e+05       0.50000      47.521331      41.687109   \n",
       "min    0.000000e+00       0.00000    -507.000000    -507.000000   \n",
       "25%    2.525755e+05       0.00000       1.000000       0.000000   \n",
       "50%    5.053250e+05       1.00000       2.000000       1.000000   \n",
       "75%    7.580795e+05       1.00000       4.000000       3.000000   \n",
       "max    1.010824e+06       1.00000    5818.000000    5163.000000   \n",
       "\n",
       "               downs     word_count  punctuation_count   has_repeated  \\\n",
       "count  807647.000000  807647.000000      807647.000000  807647.000000   \n",
       "mean       -0.146174       0.004264           0.000212       0.012556   \n",
       "std         0.353281       0.004743           0.001167       0.111349   \n",
       "min        -1.000000       0.000000           0.000000       0.000000   \n",
       "25%         0.000000       0.001801           0.000102       0.000000   \n",
       "50%         0.000000       0.003602           0.000204       0.000000   \n",
       "75%         0.000000       0.005853           0.000306       0.000000   \n",
       "max         0.000000       1.000000           1.000000       1.000000   \n",
       "\n",
       "       exclaim_count  qns_mark_count  ellipses_mark_count  interjection_count  \\\n",
       "count  807647.000000   807647.000000        807647.000000       807647.000000   \n",
       "mean        0.002202        0.013006             0.004861            0.008823   \n",
       "std         0.006939        0.035844             0.019006            0.038180   \n",
       "min         0.000000        0.000000             0.000000            0.000000   \n",
       "25%         0.000000        0.000000             0.000000            0.000000   \n",
       "50%         0.000000        0.000000             0.000000            0.000000   \n",
       "75%         0.000000        0.000000             0.000000            0.000000   \n",
       "max         1.000000        1.000000             1.000000            1.000000   \n",
       "\n",
       "       laughter_words_count  capitalized_word_count  \\\n",
       "count          807647.00000           807647.000000   \n",
       "mean                0.00039                0.000141   \n",
       "std                 0.00287                0.002421   \n",
       "min                 0.00000                0.000000   \n",
       "25%                 0.00000                0.000000   \n",
       "50%                 0.00000                0.000000   \n",
       "75%                 0.00000                0.000000   \n",
       "max                 1.00000                1.000000   \n",
       "\n",
       "       partial_capital_word_count  emoticon_count  \\\n",
       "count               807647.000000   807647.000000   \n",
       "mean                     0.000083        0.001157   \n",
       "std                      0.001425        0.010118   \n",
       "min                      0.000000        0.000000   \n",
       "25%                      0.000000        0.000000   \n",
       "50%                      0.000000        0.000000   \n",
       "75%                      0.000000        0.000000   \n",
       "max                      1.000000        1.000000   \n",
       "\n",
       "       cosine_similarity_dirty_comments  cosine_similarity_clean_comments  \n",
       "count                     807647.000000                     807647.000000  \n",
       "mean                           0.518440                          0.510574  \n",
       "std                            0.107341                          0.108868  \n",
       "min                            0.000000                          0.000000  \n",
       "25%                            0.446967                          0.437965  \n",
       "50%                            0.516835                          0.508140  \n",
       "75%                            0.590040                          0.582497  \n",
       "max                            1.000000                          1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(test_data, prediction):\n",
    "    cm = pd.DataFrame(confusion_matrix(test_data, prediction))\n",
    "    cm.columns = ['Predicted Y=0','Predicted Y=1']\n",
    "    cm.index = ['True Y=0','True Y=1']\n",
    "    display(cm)\n",
    "\n",
    "    accuracy = (cm.iloc[0,0]+cm.iloc[1,1])/(cm.iloc[0,0]+cm.iloc[1,1]+cm.iloc[0,1]+cm.iloc[1,0])\n",
    "    print('Accuracy: '+str(accuracy))\n",
    "\n",
    "    # Possible ways to further improve accuracy is to try adjusting the level of significance.\n",
    "\n",
    "    # Calculate Sensitivity (true positive rate)\n",
    "    sensitivity = cm.iloc[1, 1]/(cm.iloc[1, 1] + cm.iloc[1, 0])\n",
    "    print('Sensitivity: '+ str(sensitivity))\n",
    "\n",
    "    # Calculate Specificity (true negative rate)\n",
    "    specificity = cm.iloc[0, 0]/(cm.iloc[0, 0] + cm.iloc[0, 1])\n",
    "    print('Specificity: '+str(specificity))\n",
    "\n",
    "    #Precision\n",
    "    precision = cm.iloc[1,1]/(cm.iloc[1,1]+cm.iloc[0,1])\n",
    "    print('Precision: '+str(precision))\n",
    "\n",
    "    # AUC\n",
    "    print('ROC-AUC:',roc_auc_score(test_data, prediction))\n",
    "\n",
    "    #F1 Score\n",
    "    print(\"F1 score:\", round(f1_score(test_data, prediction), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>XGBoost with only the dirty vectorized comments</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirty_comments = norm_train_data.loc[:, ['label', 'vectorized_comment',]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>vectorized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.23815727, 0.0650364, 0.274105, 0.47422692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.06878692, 0.64647627, -0.0010952452, 0.650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.10693383, 0.044041224, 0.08560472, 0.07766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.24483381, -0.35850817, 0.5864438, -0.03253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.16459303, 0.44265932, 0.76053095, 0.579429...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                 vectorized_comment\n",
       "0      0  [-0.23815727, 0.0650364, 0.274105, 0.47422692,...\n",
       "1      0  [-0.06878692, 0.64647627, -0.0010952452, 0.650...\n",
       "2      1  [-0.10693383, 0.044041224, 0.08560472, 0.07766...\n",
       "3      1  [-0.24483381, -0.35850817, 0.5864438, -0.03253...\n",
       "4      1  [-0.16459303, 0.44265932, 0.76053095, 0.579429..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dirty_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_dirty_comments.iloc[0:10, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.23815727"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset['vectorized_comment'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vec_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f84aedfab5a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vec_dim{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectorized_comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconvert_array_to_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vec_dim' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_array_to_cols(df, vec_dim):\n",
    "    for row_num in df['vectorized_comment'].index:\n",
    "        for i in range(vec_dim):\n",
    "            df.loc[row_num, 'vec_dim{}'.format(i)] = df.loc[row_num, 'vectorized_comment'][i]\n",
    "        \n",
    "convert_array_to_cols(train_subset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>vectorized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.23815727, 0.0650364, 0.274105, 0.47422692,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.06878692, 0.64647627, -0.0010952452, 0.650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.10693383, 0.044041224, 0.08560472, 0.07766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.24483381, -0.35850817, 0.5864438, -0.03253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.16459303, 0.44265932, 0.76053095, 0.579429...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                 vectorized_comment\n",
       "0      0  [-0.23815727, 0.0650364, 0.274105, 0.47422692,...\n",
       "1      0  [-0.06878692, 0.64647627, -0.0010952452, 0.650...\n",
       "2      1  [-0.10693383, 0.044041224, 0.08560472, 0.07766...\n",
       "3      1  [-0.24483381, -0.35850817, 0.5864438, -0.03253...\n",
       "4      1  [-0.16459303, 0.44265932, 0.76053095, 0.579429..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirty_comments = train_dirty_comments.loc[:, ['label', 'vectorized_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umer/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:37] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 1h 3min 24s, sys: 17.5 s, total: 1h 3min 42s\n",
      "Wall time: 8min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(list(train_dirty_comments.loc[:, 'vectorized_comment']), train_dirty_comments['label'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>70150</td>\n",
       "      <td>38562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>30805</td>\n",
       "      <td>62649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0          70150          38562\n",
       "True Y=1          30805          62649"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6568809789974576\n",
      "Sensitivity: 0.6703725897232863\n",
      "Specificity: 0.6452829494444036\n",
      "Precision: 0.6189939828674749\n",
      "ROC-AUC: 0.6578277695838448\n",
      "F1 score: 0.6437\n"
     ]
    }
   ],
   "source": [
    "y_pred_1 = xgb_model.predict(list(norm_test_data['vectorized_comment']))\n",
    "\n",
    "metrics(y_pred_1, norm_test_data['label'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xgb_baseline_dirty_comments', y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>XGBoost with only clean comments (To compare against the uncleaned comments)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_comments = norm_train_data.loc[:, ['label', 'vectorized_clean_comment']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umer/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:48:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_baseline_model_clean = xgb.XGBClassifier()\n",
    "xgb_model.fit(list(train_clean_comments.loc[:, 'vectorized_clean_comment']), train_clean_comments['label'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clean_comments = xgb_model.predict(list(norm_test_data['vectorized_clean_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>69644</td>\n",
       "      <td>38205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>31311</td>\n",
       "      <td>63006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0          69644          38205\n",
       "True Y=1          31311          63006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.656143960903416\n",
      "Sensitivity: 0.6680237921053469\n",
      "Specificity: 0.6457547126074419\n",
      "Precision: 0.6225212674511664\n",
      "ROC-AUC: 0.6568892523563944\n",
      "F1 score: 0.6445\n"
     ]
    }
   ],
   "source": [
    "metrics(y_pred_clean_comments, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> XG boost with all features with vectorized comments </h3>\n",
    "Note: basic XGboost without any feature selection and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'label', 'comment', 'author', 'subreddit', 'score', 'ups',\n",
       "       'downs', 'date', 'created_utc', 'parent_comment', 'word_count',\n",
       "       'punctuation_count', 'has_repeated', 'exclaim_count', 'qns_mark_count',\n",
       "       'ellipses_mark_count', 'interjection_count', 'laughter_words_count',\n",
       "       'capitalized_word_count', 'partial_capital_word_count',\n",
       "       'emoticon_count', 'clean_comment', 'lemmatized_comment',\n",
       "       'lemmatized_parent_comment', 'clean_parent_comment',\n",
       "       'lemmatized_clean_comment', 'clean_lemmatized_parent_comment',\n",
       "       'vectorized_comment', 'vectorized_clean_comment', 'vectorized_parent',\n",
       "       'vectorized_clean_parent', 'cosine_similarity_dirty_comments',\n",
       "       'cosine_similarity_clean_comments', 'comment_sentiment',\n",
       "       'parent_comment_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_engineered_features = norm_train_data.copy().drop(['vectorized_clean_comment', 'vectorized_parent', 'vectorized_clean_parent',\n",
    "                                                            'comment', 'parent_comment', 'clean_comment', 'clean_parent_comment',\n",
    "                                                             'lemmatized_comment', 'lemmatized_clean_comment', 'lemmatized_parent_comment', 'clean_lemmatized_parent_comment', \n",
    "                                                             'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'cosine_similarity_clean_comments',\n",
    "                                                             'parent_comment_sentiment'], axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_engineered_features = train_with_engineered_features.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'word_count', 'punctuation_count', 'has_repeated',\n",
       "       'exclaim_count', 'qns_mark_count', 'ellipses_mark_count',\n",
       "       'interjection_count', 'laughter_words_count', 'capitalized_word_count',\n",
       "       'partial_capital_word_count', 'emoticon_count', 'vectorized_comment',\n",
       "       'cosine_similarity_dirty_comments', 'comment_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_engineered_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>cosine_similarity_clean_comments</th>\n",
       "      <th>comment_sentiment</th>\n",
       "      <th>parent_comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552133</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579743</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510553</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423121</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471443</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  word_count  punctuation_count  has_repeated  exclaim_count  \\\n",
       "0      0    0.002251           0.000000           0.0            0.0   \n",
       "1      0    0.004052           0.000102           0.0            0.0   \n",
       "2      1    0.000450           0.000000           0.0            0.0   \n",
       "3      1    0.004052           0.000306           0.0            0.0   \n",
       "4      1    0.011256           0.000510           0.0            0.0   \n",
       "\n",
       "   qns_mark_count  ellipses_mark_count  interjection_count  \\\n",
       "0        0.000000                  0.0                 0.0   \n",
       "1        0.000000                  0.0                 0.0   \n",
       "2        0.000000                  0.0                 0.0   \n",
       "3        0.111111                  0.0                 0.0   \n",
       "4        0.000000                  0.0                 0.0   \n",
       "\n",
       "   laughter_words_count  capitalized_word_count  partial_capital_word_count  \\\n",
       "0                   0.0                0.000000                         0.0   \n",
       "1                   0.0                0.001203                         0.0   \n",
       "2                   0.0                0.000000                         0.0   \n",
       "3                   0.0                0.000000                         0.0   \n",
       "4                   0.0                0.000000                         0.0   \n",
       "\n",
       "   emoticon_count  cosine_similarity_clean_comments comment_sentiment  \\\n",
       "0             0.0                          0.552133              None   \n",
       "1             0.0                          0.579743              None   \n",
       "2             0.0                          0.510553              None   \n",
       "3             0.0                          0.423121              None   \n",
       "4             0.0                          0.471443              None   \n",
       "\n",
       "  parent_comment_sentiment  \n",
       "0                     None  \n",
       "1                     None  \n",
       "2                     None  \n",
       "3                     None  \n",
       "4                     None  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_all_engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_numeric(x):\n",
    "    if x == 'neu':\n",
    "        return 0\n",
    "    elif x == 'pos':\n",
    "        return 1\n",
    "    elif x == 'neg': \n",
    "        return -1\n",
    "\n",
    "train_with_engineered_features['comment_sentiment'] = train_with_engineered_features['comment_sentiment'].apply(sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>vectorized_comment</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.23815727, 0.0650364, 0.274105, 0.47422692,...</td>\n",
       "      <td>0.538655</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.06878692, 0.64647627, -0.0010952452, 0.650...</td>\n",
       "      <td>0.582335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.10693383, 0.044041224, 0.08560472, 0.07766...</td>\n",
       "      <td>0.515965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.24483381, -0.35850817, 0.5864438, -0.03253...</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.16459303, 0.44265932, 0.76053095, 0.579429...</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  word_count  punctuation_count  has_repeated  exclaim_count  \\\n",
       "0      0    0.002251           0.000000           0.0            0.0   \n",
       "1      0    0.004052           0.000102           0.0            0.0   \n",
       "2      1    0.000450           0.000000           0.0            0.0   \n",
       "3      1    0.004052           0.000306           0.0            0.0   \n",
       "4      1    0.011256           0.000510           0.0            0.0   \n",
       "\n",
       "   qns_mark_count  ellipses_mark_count  interjection_count  \\\n",
       "0        0.000000                  0.0                 0.0   \n",
       "1        0.000000                  0.0                 0.0   \n",
       "2        0.000000                  0.0                 0.0   \n",
       "3        0.111111                  0.0                 0.0   \n",
       "4        0.000000                  0.0                 0.0   \n",
       "\n",
       "   laughter_words_count  capitalized_word_count  partial_capital_word_count  \\\n",
       "0                   0.0                0.000000                         0.0   \n",
       "1                   0.0                0.001203                         0.0   \n",
       "2                   0.0                0.000000                         0.0   \n",
       "3                   0.0                0.000000                         0.0   \n",
       "4                   0.0                0.000000                         0.0   \n",
       "\n",
       "   emoticon_count                                 vectorized_comment  \\\n",
       "0             0.0  [-0.23815727, 0.0650364, 0.274105, 0.47422692,...   \n",
       "1             0.0  [-0.06878692, 0.64647627, -0.0010952452, 0.650...   \n",
       "2             0.0  [-0.10693383, 0.044041224, 0.08560472, 0.07766...   \n",
       "3             0.0  [-0.24483381, -0.35850817, 0.5864438, -0.03253...   \n",
       "4             0.0  [-0.16459303, 0.44265932, 0.76053095, 0.579429...   \n",
       "\n",
       "   cosine_similarity_dirty_comments  comment_sentiment  \n",
       "0                          0.538655                 -1  \n",
       "1                          0.582335                  1  \n",
       "2                          0.515965                  0  \n",
       "3                          0.533188                  0  \n",
       "4                          0.496283                 -1  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 µs, sys: 525 µs, total: 575 µs\n",
      "Wall time: 1.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cols_to_include = ['cosine_similarity_dirty_comments', 'word_count', 'punctuation_count', 'has_repeated', \n",
    "                  'exclaim_count', 'qns_mark_count', 'ellipses_mark_count', 'interjection_count', 'laughter_words_count', \n",
    "                   'capitalized_word_count', 'partial_capital_word_count', 'emoticon_count','comment_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 s, sys: 33.2 s, total: 1min 12s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_arr = pd.DataFrame(train_with_engineered_features['vectorized_comment'].tolist()).join(train_with_engineered_features[cols_to_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.238157</td>\n",
       "      <td>0.065036</td>\n",
       "      <td>0.274105</td>\n",
       "      <td>0.474227</td>\n",
       "      <td>-0.621571</td>\n",
       "      <td>-0.069404</td>\n",
       "      <td>0.174043</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>-0.124769</td>\n",
       "      <td>0.213284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.068787</td>\n",
       "      <td>0.646476</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>0.650450</td>\n",
       "      <td>-0.583917</td>\n",
       "      <td>-0.192484</td>\n",
       "      <td>0.059875</td>\n",
       "      <td>0.275911</td>\n",
       "      <td>0.214799</td>\n",
       "      <td>0.530081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.106934</td>\n",
       "      <td>0.044041</td>\n",
       "      <td>0.085605</td>\n",
       "      <td>0.077661</td>\n",
       "      <td>-0.173773</td>\n",
       "      <td>0.229178</td>\n",
       "      <td>-0.039625</td>\n",
       "      <td>0.034472</td>\n",
       "      <td>0.239187</td>\n",
       "      <td>0.244954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.244834</td>\n",
       "      <td>-0.358508</td>\n",
       "      <td>0.586444</td>\n",
       "      <td>-0.032538</td>\n",
       "      <td>-0.859644</td>\n",
       "      <td>0.177933</td>\n",
       "      <td>-0.296081</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.194230</td>\n",
       "      <td>0.360871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.164593</td>\n",
       "      <td>0.442659</td>\n",
       "      <td>0.760531</td>\n",
       "      <td>0.579430</td>\n",
       "      <td>-1.001434</td>\n",
       "      <td>-0.723228</td>\n",
       "      <td>0.187690</td>\n",
       "      <td>-1.085732</td>\n",
       "      <td>0.177928</td>\n",
       "      <td>-0.013857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.238157  0.065036  0.274105  0.474227 -0.621571 -0.069404  0.174043   \n",
       "1 -0.068787  0.646476 -0.001095  0.650450 -0.583917 -0.192484  0.059875   \n",
       "2 -0.106934  0.044041  0.085605  0.077661 -0.173773  0.229178 -0.039625   \n",
       "3 -0.244834 -0.358508  0.586444 -0.032538 -0.859644  0.177933 -0.296081   \n",
       "4 -0.164593  0.442659  0.760531  0.579430 -1.001434 -0.723228  0.187690   \n",
       "\n",
       "          7         8         9  ...  has_repeated  exclaim_count  \\\n",
       "0  0.008883 -0.124769  0.213284  ...           0.0            0.0   \n",
       "1  0.275911  0.214799  0.530081  ...           0.0            0.0   \n",
       "2  0.034472  0.239187  0.244954  ...           0.0            0.0   \n",
       "3  0.004410  0.194230  0.360871  ...           0.0            0.0   \n",
       "4 -1.085732  0.177928 -0.013857  ...           0.0            0.0   \n",
       "\n",
       "   qns_mark_count  ellipses_mark_count  interjection_count  \\\n",
       "0        0.000000                  0.0                 0.0   \n",
       "1        0.000000                  0.0                 0.0   \n",
       "2        0.000000                  0.0                 0.0   \n",
       "3        0.111111                  0.0                 0.0   \n",
       "4        0.000000                  0.0                 0.0   \n",
       "\n",
       "   laughter_words_count  capitalized_word_count  partial_capital_word_count  \\\n",
       "0                   0.0                0.000000                         0.0   \n",
       "1                   0.0                0.001203                         0.0   \n",
       "2                   0.0                0.000000                         0.0   \n",
       "3                   0.0                0.000000                         0.0   \n",
       "4                   0.0                0.000000                         0.0   \n",
       "\n",
       "   emoticon_count  comment_sentiment  \n",
       "0             0.0                 -1  \n",
       "1             0.0                  1  \n",
       "2             0.0                  0  \n",
       "3             0.0                  0  \n",
       "4             0.0                 -1  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umer/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:39:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 1h 27min 59s, sys: 1min 6s, total: 1h 29min 5s\n",
      "Wall time: 13min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_model_with_engineered_features.fit((X_train_arr),\n",
    "                                       train_with_engineered_features['label'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_engineered_features = norm_test_data.copy().drop(['vectorized_clean_comment', 'vectorized_parent_comment',\n",
    "                                                            'vectorized_clean_parent_comment',\n",
    "                                                            'comment', 'parent_comment', 'clean_comment', 'clean_parent_comment',\n",
    "                                                             'lemmatized_comment', 'lemmatized_clean_comment', 'lemmatized_parent_comment',\n",
    "                                                            'lemmatized_clean_parent_comment', 'author', 'subreddit', 'score',\n",
    "                                                            'ups', 'downs', 'date', 'created_utc', 'cosine_similarity_clean_comments',\n",
    "                                                             'index'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_with_engineered_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_with_engineered_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_engineered_features['comment_sentiment'] = test_with_engineered_features['comment_sentiment'].apply(sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(test_with_engineered_features['vectorized_comment'].tolist()).join(test_with_engineered_features[cols_to_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2260461 , -0.5411476 , -0.5956079 ,  1.6732603 , -0.35423955,\n",
       "       -0.28117514,  1.0684463 ,  0.13423854, -0.8507427 ,  1.8262612 ,\n",
       "       -0.7148748 ,  0.4975234 , -0.86549145, -0.6982111 ,  0.2134533 ,\n",
       "        0.18298747,  0.5885876 , -0.76619554,  1.0453223 , -0.90714866,\n",
       "       -0.69089735,  1.1539632 ,  0.81549114,  0.21821809,  0.13845769,\n",
       "        0.7591418 ,  0.02087549, -0.16588932, -0.28434   ,  0.12744519,\n",
       "        0.12692545,  0.5278704 ,  0.24552296,  0.34511566, -0.4861376 ,\n",
       "       -0.6935075 , -1.005111  ,  0.3457867 ,  1.1153344 ,  0.74597263,\n",
       "       -0.12379747,  0.04674913,  1.4676919 ,  0.14065263, -1.2901154 ,\n",
       "        0.08728639,  0.04770548,  1.5965544 , -0.2411312 , -0.31156746,\n",
       "       -0.02273787, -1.2519696 ,  0.13721712, -0.20597284,  1.1222237 ,\n",
       "       -0.28557092, -0.3173024 , -0.8671589 , -1.1810979 , -1.302789  ,\n",
       "       -1.290435  ,  0.01784645, -0.6765577 , -0.46088463,  0.18265541,\n",
       "       -1.1688226 ,  0.5169917 , -0.3895155 ,  0.7157061 ,  0.18098469,\n",
       "        0.09345379,  0.0877275 , -0.67896163,  0.19621952, -0.21623124,\n",
       "        0.8160912 , -0.36400715, -1.2302519 ,  0.6501827 ,  0.17373958,\n",
       "        2.102477  ,  0.39456564, -0.61934924, -0.9636704 , -0.22877662,\n",
       "        0.5925699 , -1.0949658 ,  0.72359484, -0.793238  ,  0.27991998,\n",
       "        0.00229598,  0.35484284, -1.8297951 ,  0.10980434, -0.02731025,\n",
       "       -1.3052177 , -0.8800249 ,  0.08947629, -0.92595494,  1.2309114 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.iloc[0, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = X_test_df.drop(columns='vectorized_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202166, 113)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dirty_comments_w_engineered_features = xgb_model_with_engineered_features.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>76839</td>\n",
       "      <td>42214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>24116</td>\n",
       "      <td>58997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0          76839          42214\n",
       "True Y=1          24116          58997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6719032873974853\n",
      "Sensitivity: 0.7098408191257686\n",
      "Specificity: 0.6454184270870956\n",
      "Precision: 0.582910948414698\n",
      "ROC-AUC: 0.6776296231064323\n",
      "F1 score: 0.6401\n"
     ]
    }
   ],
   "source": [
    "metrics(y_pred_dirty_comments_w_engineered_features, test_with_engineered_features['label'].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>XGboost model with only features and no vectors</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only_engineered_features = norm_train_data.copy().drop(['vectorized_clean_comment', 'vectorized_parent', 'vectorized_clean_parent',\n",
    "                                                            'comment', 'parent_comment', 'clean_comment', 'clean_parent_comment',\n",
    "                                                             'lemmatized_comment', 'lemmatized_clean_comment', 'lemmatized_parent_comment', 'clean_lemmatized_parent_comment', \n",
    "                                                             'author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'cosine_similarity_clean_comments',\n",
    "                                                             'parent_comment_sentiment', 'vectorized_comment'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(807647, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_only_engineered_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417033</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538655</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582335</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5664</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515965</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366838</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>907940</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  label  word_count  punctuation_count  has_repeated  exclaim_count  \\\n",
       "0  417033      0    0.002251           0.000000           0.0            0.0   \n",
       "1   59081      0    0.004052           0.000102           0.0            0.0   \n",
       "2    5664      1    0.000450           0.000000           0.0            0.0   \n",
       "3  366838      1    0.004052           0.000306           0.0            0.0   \n",
       "4  907940      1    0.011256           0.000510           0.0            0.0   \n",
       "\n",
       "   qns_mark_count  ellipses_mark_count  interjection_count  \\\n",
       "0        0.000000                  0.0                 0.0   \n",
       "1        0.000000                  0.0                 0.0   \n",
       "2        0.000000                  0.0                 0.0   \n",
       "3        0.111111                  0.0                 0.0   \n",
       "4        0.000000                  0.0                 0.0   \n",
       "\n",
       "   laughter_words_count  capitalized_word_count  partial_capital_word_count  \\\n",
       "0                   0.0                0.000000                         0.0   \n",
       "1                   0.0                0.001203                         0.0   \n",
       "2                   0.0                0.000000                         0.0   \n",
       "3                   0.0                0.000000                         0.0   \n",
       "4                   0.0                0.000000                         0.0   \n",
       "\n",
       "   emoticon_count  cosine_similarity_dirty_comments comment_sentiment  \n",
       "0             0.0                          0.538655               neg  \n",
       "1             0.0                          0.582335               pos  \n",
       "2             0.0                          0.515965               neu  \n",
       "3             0.0                          0.533188               neu  \n",
       "4             0.0                          0.496283               neg  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_only_engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only_engineered_features = train_only_engineered_features.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only_engineered_features['comment_sentiment'] = train_only_engineered_features['comment_sentiment'].apply(sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:56:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_2 = xgb.XGBClassifier()\n",
    "xgb_model_2.fit(train_only_engineered_features.iloc[:, 1:], train_only_engineered_features['label'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'label', 'comment', 'author', 'subreddit', 'score', 'ups',\n",
       "       'downs', 'date', 'created_utc', 'parent_comment', 'lemmatized_comment',\n",
       "       'lemmatized_parent_comment', 'clean_comment', 'clean_parent_comment',\n",
       "       'lemmatized_clean_comment', 'lemmatized_clean_parent_comment',\n",
       "       'vectorized_comment', 'vectorized_clean_comment',\n",
       "       'vectorized_parent_comment', 'vectorized_clean_parent_comment',\n",
       "       'cosine_similarity_dirty_comments', 'cosine_similarity_clean_comments',\n",
       "       'word_count', 'punctuation_count', 'has_repeated', 'exclaim_count',\n",
       "       'qns_mark_count', 'ellipses_mark_count', 'interjection_count',\n",
       "       'laughter_words_count', 'capitalized_word_count',\n",
       "       'partial_capital_word_count', 'emoticon_count', 'comment_sentiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['vectorized_parent' 'vectorized_clean_parent'\\n 'clean_lemmatized_parent_comment' 'parent_comment_sentiment']\n",
    "test_only_engineered_features = norm_test_data.copy().drop(['vectorized_clean_comment', 'vectorized_parent_comment',\n",
    "                                                            'vectorized_clean_parent_comment',\n",
    "                                                            'comment', 'parent_comment', 'clean_comment', 'clean_parent_comment',\n",
    "                                                             'lemmatized_comment', 'lemmatized_clean_comment', 'lemmatized_parent_comment',\n",
    "                                                            'lemmatized_clean_parent_comment', 'author', 'subreddit', 'score',\n",
    "                                                            'ups', 'downs', 'date', 'created_utc', 'cosine_similarity_clean_comments',\n",
    "                                                             'vectorized_comment', 'index'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only_engineered_features['comment_sentiment'] = test_only_engineered_features['comment_sentiment'].apply(sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cosine_similarity_dirty_comments</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>has_repeated</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>qns_mark_count</th>\n",
       "      <th>ellipses_mark_count</th>\n",
       "      <th>interjection_count</th>\n",
       "      <th>laughter_words_count</th>\n",
       "      <th>capitalized_word_count</th>\n",
       "      <th>partial_capital_word_count</th>\n",
       "      <th>emoticon_count</th>\n",
       "      <th>comment_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452225</td>\n",
       "      <td>0.013007</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.451581</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.431966</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.410118</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>0.042056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  cosine_similarity_dirty_comments  word_count  punctuation_count  \\\n",
       "0     1                          0.452225    0.013007           0.009346   \n",
       "1     0                          0.451581    0.008504           0.014019   \n",
       "2     1                          0.825300    0.001001           0.014019   \n",
       "3     1                          0.431966    0.008504           0.014019   \n",
       "4     0                          0.410118    0.010005           0.042056   \n",
       "\n",
       "   has_repeated  exclaim_count  qns_mark_count  ellipses_mark_count  \\\n",
       "0           0.0            0.0             0.0               0.0000   \n",
       "1           0.0            0.0             0.0               0.0000   \n",
       "2           0.0            0.0             0.0               0.0625   \n",
       "3           0.0            0.0             0.0               0.0000   \n",
       "4           0.0            0.0             0.0               0.0625   \n",
       "\n",
       "   interjection_count  laughter_words_count  capitalized_word_count  \\\n",
       "0                 0.0                   0.0                     0.0   \n",
       "1                 0.0                   0.0                     0.0   \n",
       "2                 0.0                   0.0                     0.0   \n",
       "3                 0.0                   0.0                     0.0   \n",
       "4                 0.0                   0.0                     0.0   \n",
       "\n",
       "   partial_capital_word_count  emoticon_count  comment_sentiment  \n",
       "0                    0.000000             0.0                 -1  \n",
       "1                    0.000000             0.0                  1  \n",
       "2                    0.000000             0.0                  0  \n",
       "3                    0.000000             0.0                 -1  \n",
       "4                    0.142857             0.0                  0  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_only_engineered_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_only_engineered_features = xgb_model_2.predict(test_only_engineered_features.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Y=0</th>\n",
       "      <th>Predicted Y=1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Y=0</th>\n",
       "      <td>100813</td>\n",
       "      <td>100433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Y=1</th>\n",
       "      <td>142</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted Y=0  Predicted Y=1\n",
       "True Y=0         100813         100433\n",
       "True Y=1            142            778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5025127865219671\n",
      "Sensitivity: 0.8456521739130435\n",
      "Specificity: 0.5009441181439631\n",
      "Precision: 0.007686911501714241\n",
      "ROC-AUC: 0.6732981460285032\n",
      "F1 score: 0.0152\n"
     ]
    }
   ],
   "source": [
    "metrics(y_pred_only_engineered_features, test_only_engineered_features['label'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_engineered_w_orig_features = norm_train_data.copy().drop(['vectorized_clean_comment', 'vectorized_parent', 'vectorized_clean_parent',\n",
    "                                                            'comment', 'parent_comment', 'clean_comment', 'clean_parent_comment',\n",
    "                                                             'lemmatized_comment', 'lemmatized_clean_comment', 'lemmatized_parent_comment', 'clean_lemmatized_parent_comment', \n",
    "                                                            'date', 'created_utc', 'cosine_similarity_clean_comments',\n",
    "                                                             'parent_comment_sentiment', 'vectorized_comment', 'index'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>HyperParameter Tuning for baseline model with dirty comments</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umer/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:29:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_model_with_engineered_features.fit((X_train_arr),\n",
    "                                       train_with_engineered_features['label'], verbose=True)\n",
    "param_test1 = {\n",
    "    'max_depth':range(6, 10, 3),\n",
    "    'min_child_weight':range(6,10, 3),\n",
    "#     'learning_rate':[i/100.0 for i in range(20,51, 10)],\n",
    "    'n_estimators': range(100,301,100),\n",
    "#     \"early_stopping_rounds\" : 10,\n",
    "#     \"eval_metric\" = 'accuracy',\n",
    "#     \"eval_set\" : [[X_test_df, test_with_engineered_features['label']]]\n",
    "#      'reg_lambda':[i/10.0 for i in range(1,10)],\n",
    "#      'reg_alpha':[i/10.0 for i in range(1,10)]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(objective= 'binary:logistic', label_encoder=False, nthread=4), \n",
    " param_grid = param_test1, scoring='accuracy',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train_arr, train_with_engineered_features['label'].astype(int), verbose=True)\n",
    "gsearch.cv_results_, gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_all_engineered_features = norm_test_data.copy().drop(['index','author','subreddit','score','ups','downs','parent_comment',\n",
    "                      'clean_comment','lemmatized_comment','lemmatized_clean_comment','lemmatized_parent_comment','clean_parent_comment',\n",
    "                      'clean_lemmatized_parent_comment','vectorized_comment',\n",
    "                      'vectorized_clean_comment',\t'vectorized_parent',\t'vectorized_clean_parent','date',\n",
    "                      'created_utc','comment','cosine_similarity_dirty_comments'], axis = 1)\n",
    "\n",
    "test_data_all_engineered_features['comment_sentiment'] = test_data_all_engineered_features['comment_sentiment'].apply(sentiment_to_numeric)\n",
    "test_data_all_engineered_features['parent_comment_sentiment'] = test_data_all_engineered_features['parent_comment_sentiment'].apply(sentiment_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all_engineered_features = xgb_model.predict(test_data_all_engineered_features.iloc[: , 1: ])\n",
    "metrics(y_pred_all_engineered_features, test_data_all_engineered_features['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
