{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8059e1-f900-4bf6-827a-3fad491ce8c9",
   "metadata": {},
   "source": [
    "## PyTorch Work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb41085-ee9e-4e4a-8519-58fb316d1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d14a25-1bbe-48d9-a075-7bb1afde3212",
   "metadata": {},
   "source": [
    "### Directory Structure Based Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76bba84-e3c6-4fd8-8303-9c9ed3d9fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        #transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "mitosis_dataset = datasets.ImageFolder(root='../model_dev/Data_CMC_COADEL_224_1/train',#'MITOS_Datasets/Data_CMC_COADEL_224_1/train',\n",
    "                                           transform=data_transform)\n",
    "dataset_loader = torch.utils.data.DataLoader(mitosis_dataset,\n",
    "                                             batch_size=4, shuffle=True,\n",
    "                                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f7bd8f-5883-4b55-a80f-d8aee99404a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/jmwolf/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fda37cd18644bf878ad19cf6f0e935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/233M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to /home/jmwolf/.cache/torch/hub/checkpoints/squeezenet1_0-b66bff10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3297e540d9ca45448cee248a6ab8bfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/4.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/jmwolf/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7c479b3e5b49e6807c4ecfafd9c447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "#densenet = models.densenet161(pretrained=True)\n",
    "#inception = models.inception_v3(pretrained=True)\n",
    "#googlenet = models.googlenet(pretrained=True)\n",
    "#shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "#mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "#resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
    "#wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n",
    "#mnasnet = models.mnasnet1_0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba7c1817-d1b1-40d0-a995-b6b2ac8c9d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'ConvNeXt',\n",
       " 'DenseNet',\n",
       " 'EfficientNet',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'MNASNet',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'RegNet',\n",
       " 'ResNet',\n",
       " 'ShuffleNetV2',\n",
       " 'SqueezeNet',\n",
       " 'VGG',\n",
       " 'VisionTransformer',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'feature_extraction',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be67e078-1e8c-4139-b747-fd6ae77a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a83f09-7f0b-45ba-aeb8-e08a03dfa0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "798012ae-cf72-4510-a524-20c2b3df17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = 'MITOS_Datasets/Data_CMC_COADEL_224_1/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 50\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a50f27-713b-4cfe-b16b-7129e53d4570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3080'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e1219-52e8-4015-ad6e-95030f994cbe",
   "metadata": {},
   "source": [
    "### Define Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be49f874-24e4-4229-951b-fdd57adb1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5d3cd96-257f-484b-8d5f-095030789cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a24f32-ad1f-47c4-b580-6805b24be0d6",
   "metadata": {},
   "source": [
    "### Intialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec3f392d-7448-4cab-abf3-58e7cd37817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 128#224\n",
    "        \n",
    "    elif model_name == \"resnet152\":\n",
    "        \"\"\" Resnet152\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 128#224    \n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 128#224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 128#224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 128#224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5865afc7-9552-42f6-9a46-c1500305a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "# inception\n",
    "# densenet\n",
    "# squeezenet\n",
    "# vgg\n",
    "# alexnet\n",
    "# resnet\n",
    "model_name = \"vgg\"\n",
    "model_ft, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16a72ee8-640b-4c48-a50c-aaad60ec113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.fc = nn.Linear(512, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f603de9-6efd-423c-bdfe-a550fdd8439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e72c02e7-0c81-490c-8b2a-2f4802c53d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'Mitosis']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(mitosis_dataset, batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88b89e94-0077-49f5-9c99-a67690331be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ba6f036-0bea-44a8-8311-14cfa39b3874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "363d1c00-3b58-40bd-aa60-a9bd4be6f1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.6713 Acc: 0.6795\n",
      "val Loss: 0.5940 Acc: 0.6817\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.6935 Acc: 0.6748\n",
      "val Loss: 0.5880 Acc: 0.6931\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.7013 Acc: 0.6711\n",
      "val Loss: 0.5853 Acc: 0.7216\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.6956 Acc: 0.6754\n",
      "val Loss: 0.6418 Acc: 0.7212\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.6943 Acc: 0.6745\n",
      "val Loss: 0.6600 Acc: 0.7229\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.6960 Acc: 0.6752\n",
      "val Loss: 0.5751 Acc: 0.7013\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.6935 Acc: 0.6719\n",
      "val Loss: 0.5649 Acc: 0.7110\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.6998 Acc: 0.6717\n",
      "val Loss: 0.6546 Acc: 0.7208\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.6988 Acc: 0.6739\n",
      "val Loss: 0.5756 Acc: 0.7031\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.6934 Acc: 0.6766\n",
      "val Loss: 0.5613 Acc: 0.7227\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.6944 Acc: 0.6755\n",
      "val Loss: 0.5666 Acc: 0.7084\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.7040 Acc: 0.6721\n",
      "val Loss: 0.5806 Acc: 0.6886\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.7003 Acc: 0.6722\n",
      "val Loss: 0.5872 Acc: 0.7157\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6992 Acc: 0.6736\n",
      "val Loss: 0.5679 Acc: 0.7169\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.6962 Acc: 0.6736\n",
      "val Loss: 0.7105 Acc: 0.7203\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.6989 Acc: 0.6711\n",
      "val Loss: 0.5728 Acc: 0.7102\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.6969 Acc: 0.6744\n",
      "val Loss: 0.5750 Acc: 0.7216\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.6957 Acc: 0.6736\n",
      "val Loss: 0.5869 Acc: 0.6901\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.6998 Acc: 0.6730\n",
      "val Loss: 0.5702 Acc: 0.7203\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.7049 Acc: 0.6751\n",
      "val Loss: 0.5978 Acc: 0.7231\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.7016 Acc: 0.6724\n",
      "val Loss: 0.6485 Acc: 0.6540\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.6913 Acc: 0.6745\n",
      "val Loss: 0.5697 Acc: 0.7225\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.6988 Acc: 0.6738\n",
      "val Loss: 0.5886 Acc: 0.7258\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.7009 Acc: 0.6743\n",
      "val Loss: 0.6050 Acc: 0.7217\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.6994 Acc: 0.6714\n",
      "val Loss: 0.5627 Acc: 0.7148\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.6995 Acc: 0.6733\n",
      "val Loss: 0.5797 Acc: 0.7086\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.7071 Acc: 0.6693\n",
      "val Loss: 0.6491 Acc: 0.6483\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.6963 Acc: 0.6739\n",
      "val Loss: 0.6609 Acc: 0.6284\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.6976 Acc: 0.6713\n",
      "val Loss: 0.5948 Acc: 0.6790\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.6978 Acc: 0.6736\n",
      "val Loss: 0.5589 Acc: 0.7177\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.6919 Acc: 0.6740\n",
      "val Loss: 0.6291 Acc: 0.7225\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.6924 Acc: 0.6749\n",
      "val Loss: 0.6967 Acc: 0.6114\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.7019 Acc: 0.6696\n",
      "val Loss: 0.5663 Acc: 0.7066\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.7033 Acc: 0.6738\n",
      "val Loss: 0.6012 Acc: 0.6803\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.7009 Acc: 0.6702\n",
      "val Loss: 0.5795 Acc: 0.7013\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.7018 Acc: 0.6730\n",
      "val Loss: 0.5888 Acc: 0.6864\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.6981 Acc: 0.6776\n",
      "val Loss: 0.6456 Acc: 0.6322\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.6991 Acc: 0.6738\n",
      "val Loss: 0.7040 Acc: 0.5876\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.6947 Acc: 0.6707\n",
      "val Loss: 0.6583 Acc: 0.7234\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.6982 Acc: 0.6734\n",
      "val Loss: 0.6590 Acc: 0.7243\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.7013 Acc: 0.6740\n",
      "val Loss: 0.5903 Acc: 0.6875\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.6931 Acc: 0.6778\n",
      "val Loss: 0.6946 Acc: 0.5848\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.7031 Acc: 0.6704\n",
      "val Loss: 0.6264 Acc: 0.7225\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.7026 Acc: 0.6713\n",
      "val Loss: 0.5965 Acc: 0.6788\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.6975 Acc: 0.6778\n",
      "val Loss: 0.5739 Acc: 0.7219\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.7012 Acc: 0.6734\n",
      "val Loss: 0.5607 Acc: 0.7154\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.7022 Acc: 0.6706\n",
      "val Loss: 0.5688 Acc: 0.7143\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.7079 Acc: 0.6680\n",
      "val Loss: 0.5848 Acc: 0.7257\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.6965 Acc: 0.6742\n",
      "val Loss: 0.5679 Acc: 0.7202\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.7040 Acc: 0.6707\n",
      "val Loss: 0.5917 Acc: 0.7247\n",
      "\n",
      "Training complete in 109m 9s\n",
      "Best val Acc: 0.725823\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30b5c8fc-6a03-431a-83b2-e1630ced9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'vgg_224_mitosis.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29e5306-2430-4a5b-837c-a81f55b09586",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the model for this run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# inception\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# densenet\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# alexnet\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# resnet\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvgg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model_ft, input_size \u001b[38;5;241m=\u001b[39m initialize_model(model_name, \u001b[38;5;241m2\u001b[39m, \u001b[43mfeature_extract\u001b[49m, use_pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m model_ft\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extract' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the model for this run\n",
    "# inception\n",
    "# densenet\n",
    "# squeezenet\n",
    "# vgg\n",
    "# alexnet\n",
    "# resnet\n",
    "model_name = \"vgg\"\n",
    "model_ft, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)\n",
    "# Print the model we just instantiated\n",
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e88be28-e408-4cd1-9c69-4d7dc3b31944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.6530 Acc: 0.6357\n",
      "val Loss: 0.6441 Acc: 0.6710\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.6551 Acc: 0.6320\n",
      "val Loss: 0.6403 Acc: 0.6799\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.6526 Acc: 0.6320\n",
      "val Loss: 0.6457 Acc: 0.6676\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.6333\n",
      "val Loss: 0.6436 Acc: 0.6724\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.6529 Acc: 0.6338\n",
      "val Loss: 0.6407 Acc: 0.6767\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.6527 Acc: 0.6333\n",
      "val Loss: 0.6430 Acc: 0.6748\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.6528 Acc: 0.6336\n",
      "val Loss: 0.6409 Acc: 0.6791\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.6527 Acc: 0.6319\n",
      "val Loss: 0.6444 Acc: 0.6698\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.6535 Acc: 0.6319\n",
      "val Loss: 0.6436 Acc: 0.6704\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.6523 Acc: 0.6342\n",
      "val Loss: 0.6426 Acc: 0.6740\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.6545 Acc: 0.6313\n",
      "val Loss: 0.6452 Acc: 0.6683\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.6351\n",
      "val Loss: 0.6456 Acc: 0.6666\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.6527 Acc: 0.6345\n",
      "val Loss: 0.6421 Acc: 0.6769\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.6522 Acc: 0.6373\n",
      "val Loss: 0.6443 Acc: 0.6703\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.6524 Acc: 0.6341\n",
      "val Loss: 0.6431 Acc: 0.6723\n",
      "\n",
      "Training complete in 13m 48s\n",
      "Best val Acc: 0.679868\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06af81e5-dd78-4665-b7ba-ea61e755bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'vgg_mitosis.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55027e96-8129-49f3-ad1b-bf8a36c3b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "model_ft, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4fe361b-e36d-4d29-b23f-947e0e3ff742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.7059 Acc: 0.5229\n",
      "val Loss: 0.7040 Acc: 0.5021\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.7045 Acc: 0.5260\n",
      "val Loss: 0.7040 Acc: 0.5010\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n",
      "train Loss: 0.7070 Acc: 0.5236\n",
      "val Loss: 0.7041 Acc: 0.5025\n",
      "\n",
      "Epoch 3/14\n",
      "----------\n",
      "train Loss: 0.7033 Acc: 0.5267\n",
      "val Loss: 0.7039 Acc: 0.5045\n",
      "\n",
      "Epoch 4/14\n",
      "----------\n",
      "train Loss: 0.7048 Acc: 0.5244\n",
      "val Loss: 0.7044 Acc: 0.5025\n",
      "\n",
      "Epoch 5/14\n",
      "----------\n",
      "train Loss: 0.7043 Acc: 0.5271\n",
      "val Loss: 0.7040 Acc: 0.5033\n",
      "\n",
      "Epoch 6/14\n",
      "----------\n",
      "train Loss: 0.7042 Acc: 0.5265\n",
      "val Loss: 0.7038 Acc: 0.5043\n",
      "\n",
      "Epoch 7/14\n",
      "----------\n",
      "train Loss: 0.7025 Acc: 0.5283\n",
      "val Loss: 0.7038 Acc: 0.5047\n",
      "\n",
      "Epoch 8/14\n",
      "----------\n",
      "train Loss: 0.7053 Acc: 0.5254\n",
      "val Loss: 0.7039 Acc: 0.5032\n",
      "\n",
      "Epoch 9/14\n",
      "----------\n",
      "train Loss: 0.7056 Acc: 0.5253\n",
      "val Loss: 0.7043 Acc: 0.5038\n",
      "\n",
      "Epoch 10/14\n",
      "----------\n",
      "train Loss: 0.7058 Acc: 0.5239\n",
      "val Loss: 0.7038 Acc: 0.5038\n",
      "\n",
      "Epoch 11/14\n",
      "----------\n",
      "train Loss: 0.7028 Acc: 0.5284\n",
      "val Loss: 0.7032 Acc: 0.5028\n",
      "\n",
      "Epoch 12/14\n",
      "----------\n",
      "train Loss: 0.7038 Acc: 0.5280\n",
      "val Loss: 0.7041 Acc: 0.5036\n",
      "\n",
      "Epoch 13/14\n",
      "----------\n",
      "train Loss: 0.7020 Acc: 0.5295\n",
      "val Loss: 0.7044 Acc: 0.5029\n",
      "\n",
      "Epoch 14/14\n",
      "----------\n",
      "train Loss: 0.7048 Acc: 0.5239\n",
      "val Loss: 0.7039 Acc: 0.5032\n",
      "\n",
      "Training complete in 8m 43s\n",
      "Best val Acc: 0.504743\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8844821-574f-42d8-b8ed-0bae9bf00fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'alexnet_mitosis.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aae81b82-e2e1-4028-9b5d-28150063a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet152\"\n",
    "model_ft, input_size = initialize_model(model_name, 2, feature_extract, use_pretrained=True)\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87285d78-c4ec-4d9c-a294-a8e12c457877",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'resnet152_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc05f087-ec0d-4c92-ba05-b34cfff77e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e282673-8ff0-4672-9ae3-a427ffcadf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
