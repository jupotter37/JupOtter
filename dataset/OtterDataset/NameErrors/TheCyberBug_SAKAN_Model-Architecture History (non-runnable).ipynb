{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaeca72-1fc9-45f3-8281-b99b84ab2781",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The model had no improv with CNN + BiLSTM + KAN ever after tweaking the learning rate + decay, batch size, dropout or even the concat which was cat(c3/5/7) + cat(final_layer)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de9d38-3e30-4ad7-a67c-6aa6a8e19fee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Given the augmented dataset N<sub>1</sub> = 400/10K and N<sub>orig</sub> = 40/10K Train/Test Examples, the performance dropped for BEST_SMALL from 89.38% to <85% (84.47%@Val/EP6). \n",
    "Model clearly overfits and has no learning capabilities!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698c670-9648-4c7a-b5ab-1d28adb7ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Test with BIG + BiLSTM\n",
    "# ACC - 89.05%\n",
    "# Not faring well with aug data - tops @ 88.5%\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        self.conv_pre = nn.Conv2d(1, 128, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn_pre = nn.BatchNorm2d(128)\n",
    "        self.pool_pre = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pre = nn.Dropout(0.1)\n",
    "        \n",
    "        self.flatten_pre = nn.Flatten()\n",
    "        \n",
    "        # Conv layers with different kernel sizes for 1D input\n",
    "        self.conv1 = nn.Conv1d(128, 256, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(128, 256, 5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, 7, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 256, 1024, (3, 1), padding=(1, 0))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(1024)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # # Post PCAT Conv 2\n",
    "        # self.conv_pcat2 = nn.Conv2d(16384, 32768, (3, 3), padding=(1, 1))\n",
    "\n",
    "        # self.bn_pcat2 = nn.BatchNorm2d(32768)\n",
    "        # self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        # self.dropout_pcat2 = nn.Dropout(0.15)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(1024, 2048, (3, 1), padding=(1, 0))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(2048)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 4\n",
    "        self.conv_pcat4 = nn.Conv2d(2048, 4096, (3, 1), padding=(1, 0))\n",
    "        self.bn_pcat4 = nn.BatchNorm2d(4096)\n",
    "        self.pool_pcat4 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat4 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 5\n",
    "        self.conv_pcat5 = nn.Conv2d(4096, 8192, (3, 1), padding=(1, 0))\n",
    "\n",
    "        self.bn_pcat5 = nn.BatchNorm2d(8192)\n",
    "        self.pool_pcat5 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat5 = nn.Dropout(0.6)\n",
    "\n",
    "        self.flatten_post = nn.Flatten()\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(input_size=24576, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([256*2, 256, 64, 2], num_grids=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        x = self.conv_pre(x)  # shape: (batch_size, 1024, seq_len, 1)\n",
    "        x = self.bn_pre(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool_pre(x)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x = self.dropout_pre(x)\n",
    "        x = x.squeeze(3)  # shape: (batch_size, 1024, seq_len)\n",
    "        \n",
    "        x_flat_pre = self.flatten_pre(x)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 768, seq_len // 2)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 768, seq_len // 2, 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 1024, seq_len // 2, 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 1024, seq_len // 4, 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # # PCAT2 Conv2D layer\n",
    "        # x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 4, 1)\n",
    "        # x_out = self.bn_pcat2(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 8, 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "\n",
    "        # PCAT4 Conv2D layer\n",
    "        x_out = self.conv_pcat4(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat4(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat4(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        x_out = self.dropout_pcat4(x_out)\n",
    "\n",
    "        # PCAT5 Conv2D layer\n",
    "        x_out = self.conv_pcat5(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat5(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat5(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        x_out = self.dropout_pcat5(x_out)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        # x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 4096 * (seq_len // 32))\n",
    "        x_flat_post = self.flatten_post(x_out)\n",
    "\n",
    "        x_conv_final = torch.cat((x_flat_pre, x_flat_post), dim=1)\n",
    "\n",
    "        # Pass through BiLSTM\n",
    "        bilstm_out, _ = self.bilstm(x_conv_final)  # bilstm_out shape: (batch_size, seq_len, hidden_size*2)\n",
    "\n",
    "        # Use the last hidden state from both directions\n",
    "        bilstm_out = bilstm_out.squeeze(1)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(bilstm_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c86635-fc7d-4bb3-8bcf-c27188ae3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ACC 88.8%\n",
    "# For the next model I found I don't need the conv2d, since after the merge I deal with (N x 1) so the whole kernel = (3, 1) was pointless\n",
    "# NOTE for myself: Don't work for too long... you make v v stupid mistakes\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        self.conv_pre = nn.Conv2d(1, 128, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn_pre = nn.BatchNorm2d(128)\n",
    "        self.pool_pre = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pre = nn.Dropout(0.1)\n",
    "        \n",
    "        # Conv layers with different kernel sizes for 1D input\n",
    "        self.conv1 = nn.Conv1d(128, 256, 1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(128, 256, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, 5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 256, 1536, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(1536)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # # Post PCAT Conv 2\n",
    "        # self.conv_pcat2 = nn.Conv2d(16384, 32768, (3, 3), padding=(1, 1))\n",
    "\n",
    "        # self.bn_pcat2 = nn.BatchNorm2d(32768)\n",
    "        # self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        # self.dropout_pcat2 = nn.Dropout(0.15)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(1536, 3072, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(3072)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 4\n",
    "        self.conv_pcat4 = nn.Conv2d(3072, 6144, (3, 3), padding=(1, 1))\n",
    "        self.bn_pcat4 = nn.BatchNorm2d(6144)\n",
    "        self.pool_pcat4 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat4 = nn.Dropout(0.6)\n",
    "\n",
    "        # Post PCAT Conv 5\n",
    "        # self.conv_pcat5 = nn.Conv2d(6144, 12288, (3, 3), padding=(1, 1))\n",
    "\n",
    "        # self.bn_pcat5 = nn.BatchNorm2d(12288)\n",
    "        # self.pool_pcat5 = nn.MaxPool2d((2, 1))\n",
    "        # self.dropout_pcat5 = nn.Dropout(0.6)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([49152, 16, 4, 2], num_grids=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        x = self.conv_pre(x)  # shape: (batch_size, 1024, seq_len, 1)\n",
    "        x = self.bn_pre(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool_pre(x)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x = self.dropout_pre(x)\n",
    "        x = x.squeeze(3)  # shape: (batch_size, 1024, seq_len)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 768, seq_len // 2)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 768, seq_len // 2, 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 1024, seq_len // 2, 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 1024, seq_len // 4, 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # # PCAT2 Conv2D layer\n",
    "        # x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 4, 1)\n",
    "        # x_out = self.bn_pcat2(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 8, 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "\n",
    "        # PCAT4 Conv2D layer\n",
    "        x_out = self.conv_pcat4(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat4(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat4(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        x_out = self.dropout_pcat4(x_out)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 4096 * (seq_len // 32))\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da666ef-e860-4d43-8559-df49f23297ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGES - Batch size 32 -> 64\n",
    "# RESULT, almost nothing 0.8876 ACC\n",
    "\n",
    "# Batch the data\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Epoch 20, Loss: 0.0011186991391703485, Accuracy: 0.981075\n",
    "# Val_Loss: 0.005742809381335974, Val_Accuracy: 0.881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ce741-6db7-4c79-8b62-6885b9af9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Increases Conv Dims\n",
    "# Decreases KANs\n",
    "# Overall performance comparable to previous model\n",
    "# ACC 88.81%\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        self.conv_pre = nn.Conv2d(1, 256, (1, embed_dim), padding=(0, 0))\n",
    "        \n",
    "        # Conv layers with different kernel sizes for 1D input\n",
    "        self.conv1 = nn.Conv1d(256, 512, 1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(256, 512, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(256, 512, 5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 512, 4096, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(4096)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # # Post PCAT Conv 2\n",
    "        # self.conv_pcat2 = nn.Conv2d(16384, 32768, (3, 3), padding=(1, 1))\n",
    "\n",
    "        # self.bn_pcat2 = nn.BatchNorm2d(32768)\n",
    "        # self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        # self.dropout_pcat2 = nn.Dropout(0.15)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(4096, 8192, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(8192)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([262144, 50, 6, 2], num_grids=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        x = self.conv_pre(x)  # shape: (batch_size, 1024, seq_len, 1)\n",
    "        x = x.squeeze(3)  # shape: (batch_size, 1024, seq_len)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 768, seq_len // 2)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 768, seq_len // 2, 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 1024, seq_len // 2, 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 1024, seq_len // 4, 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # # PCAT2 Conv2D layer\n",
    "        # x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 4, 1)\n",
    "        # x_out = self.bn_pcat2(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 8, 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 4096 * (seq_len // 32))\n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145040a3-388e-422d-b576-07f509fd5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model tested for pre-conv over 1 word then passed to Conv 1-3-5\n",
    "# Model perf is stable (not overfitting) but would be desirable to capture more\n",
    "# ACC - 88.92% @ Epoch 6\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        self.conv_pre = nn.Conv2d(1, 256, (1, embed_dim), padding=(0, 0))\n",
    "        \n",
    "        # Conv layers with different kernel sizes for 1D input\n",
    "        self.conv1 = nn.Conv1d(256, 512, 1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(256, 512, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(256, 512, 5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 512, 4096, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(4096)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.15)\n",
    "\n",
    "        # # Post PCAT Conv 2\n",
    "        # self.conv_pcat2 = nn.Conv2d(16384, 32768, (3, 3), padding=(1, 1))\n",
    "\n",
    "        # self.bn_pcat2 = nn.BatchNorm2d(32768)\n",
    "        # self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        # self.dropout_pcat2 = nn.Dropout(0.15)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(4096, 8192, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(8192)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.15)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([131072, 64, 8, 2], num_grids=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Pre-convolution layer\n",
    "        x = self.conv_pre(x)  # shape: (batch_size, 1024, seq_len, 1)\n",
    "        x = x.squeeze(3)  # shape: (batch_size, 1024, seq_len)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 256, seq_len)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 256, seq_len // 2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 768, seq_len // 2)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 768, seq_len // 2, 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 1024, seq_len // 2, 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 1024, seq_len // 4, 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # # PCAT2 Conv2D layer\n",
    "        # x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 4, 1)\n",
    "        # x_out = self.bn_pcat2(x_out)\n",
    "        # x_out = F.relu(x_out)\n",
    "        # x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 2048, seq_len // 8, 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 8, 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 4096, seq_len // 32, 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 4096 * (seq_len // 32))\n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 32  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dece705-388e-43c7-ae32-639839113d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x_out\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Hyperparams\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m)  \u001b[38;5;66;03m# Vocabulary size of GPT-2 tokenizer\u001b[39;00m\n\u001b[0;32m    122\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m  \u001b[38;5;66;03m# Embedding dimension\u001b[39;00m\n\u001b[0;32m    123\u001b[0m num_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Number of classes (negative, positive)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# BEST__SMALL\n",
    "# Model 89.38% ACC\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers with different kernel sizes\n",
    "        self.conv1 = nn.Conv2d(1, 256, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 256, (3, embed_dim), padding=(1, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.25) \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 256, (5, embed_dim), padding=(2, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.dropout3 = nn.Dropout(0.25) \n",
    "        \n",
    "        # Post PCAT Conv 1\n",
    "        self.final_conv = nn.Conv2d(3 * 256, 1024, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        self.bn_final = nn.BatchNorm2d(1024)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 2\n",
    "        self.conv_pcat2 = nn.Conv2d(1024, 2048, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat2 = nn.BatchNorm2d(2048)\n",
    "        self.pool_pcat2 = nn.MaxPool2d((2, 1))\n",
    "        self.dropout_pcat2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Post PCAT Conv 3\n",
    "        self.conv_pcat3 = nn.Conv2d(2048, 4096, (3, 3), padding=(1, 1))\n",
    "\n",
    "        self.bn_pcat3 = nn.BatchNorm2d(4096)\n",
    "        self.pool_pcat3 = nn.MaxPool2d((4, 1))\n",
    "        self.dropout_pcat3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([32768, 256, 64, 8, 2], num_grids=8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 100, seq_len-0, 1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.squeeze(3)  # shape: (batch_size, 100, seq_len-0)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 100, (seq_len-0)//2)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 100, seq_len-2, 1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = x2.squeeze(3)  # shape: (batch_size, 100, seq_len-2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 100, (seq_len-2)//2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 100, seq_len-4, 1)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = x3.squeeze(3)  # shape: (batch_size, 100, seq_len-4)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 100, (seq_len-4)//2)\n",
    "        x3 = self.dropout3(x3) \n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 300, ...)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 300, ..., 1)\n",
    "        \n",
    "        # PCAT1 Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        x_out = self.dropout_pcat1(x_out)\n",
    "\n",
    "        # PCAT2 Conv2D layer\n",
    "        x_out = self.conv_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_pcat2(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat2(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "\n",
    "        # PCAT3 Conv2D layer\n",
    "        x_out = self.conv_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        x_out = self.bn_pcat3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_pcat3(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 100)\n",
    "        x_out = self.dropout_pcat3(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 32  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a45c2-7f5f-4a4b-8a45-5a395a34393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.0% Val\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv layers with different kernel sizes\n",
    "        self.conv1 = nn.Conv2d(1, 100, (1, embed_dim), padding=(0, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 100, (3, embed_dim), padding=(1, 0))\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim), padding=(2, 0))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Final Conv2D layer after concatenation\n",
    "        self.final_conv = nn.Conv2d(3 * 100, 500, (3, 3), padding=(1, 1))\n",
    "        \n",
    "        # Additional layers\n",
    "        self.bn_final = nn.BatchNorm2d(100)\n",
    "        self.pool_final = nn.MaxPool2d((2, 1))\n",
    "        self.dropout = nn.Dropout(0.65)\n",
    "        \n",
    "        # Dense layer\n",
    "        #self.fc = nn.Linear(100, num_classes)\n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([32000, 512, 128, 2], num_grids=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # shape: (batch_size, 1, seq_len, embed_dim)\n",
    "        \n",
    "        # Conv layer 1\n",
    "        x1 = self.conv1(x)  # shape: (batch_size, 100, seq_len-0, 1)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = x1.squeeze(3)  # shape: (batch_size, 100, seq_len-0)\n",
    "        x1 = self.pool1(x1)  # shape: (batch_size, 100, (seq_len-0)//2)\n",
    "        \n",
    "        # Conv layer 2\n",
    "        x2 = self.conv2(x)  # shape: (batch_size, 100, seq_len-2, 1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = x2.squeeze(3)  # shape: (batch_size, 100, seq_len-2)\n",
    "        x2 = self.pool2(x2)  # shape: (batch_size, 100, (seq_len-2)//2)\n",
    "        \n",
    "        # Conv layer 3\n",
    "        x3 = self.conv3(x)  # shape: (batch_size, 100, seq_len-4, 1)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = x3.squeeze(3)  # shape: (batch_size, 100, seq_len-4)\n",
    "        x3 = self.pool3(x3)  # shape: (batch_size, 100, (seq_len-4)//2)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)  # shape: (batch_size, 300, ...)\n",
    "        x_cat = x_cat.unsqueeze(3)  # shape: (batch_size, 300, ..., 1)\n",
    "        \n",
    "        # Final Conv2D layer\n",
    "        x_out = self.final_conv(x_cat)  # shape: (batch_size, 100, ..., 1)\n",
    "        #x_out = self.bn_final(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        x_out = self.pool_final(x_out)  # shape: (batch_size, 100, ..., 1)\n",
    "        \n",
    "        # Flatten the tensor for the dense layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)  # shape: (batch_size, 100)\n",
    "        x_out = self.dropout(x_out)\n",
    "        \n",
    "        # Dense layer\n",
    "        x_out = self.kan(x_out)  # shape: (batch_size, num_classes)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = CustomCNN(vocab_size, embed_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b89d2-19dc-4a9e-bb42-e8f14c777518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "class SentimentCNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(SentimentCNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # convolutional layers with batch Normalization\n",
    "        self.conv1 = nn.Conv2d(1, 100, (1, embed_dim), padding=(0, 0))  # 1-gram\n",
    "        self.bn1 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1, 100, (3, embed_dim), padding=(1, 0))  # 3-gram\n",
    "        self.bn2 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim), padding=(2, 0))  # 5-gram\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.conv_pcat1 = nn.Conv2d(1, 200, (3, 3), padding=(0,0))\n",
    "        self.bn_pcat1 = nn.BatchNorm2d(200)\n",
    "        \n",
    "        \n",
    "        # Replace dense layers with FastKAN\n",
    "        self.kan = KAN([37500, 512, 128, num_class], num_grids=4)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        conv1_out = F.relu(self.bn1(self.conv1(embedded))).squeeze(3)\n",
    "        conv2_out = F.relu(self.bn2(self.conv2(embedded))).squeeze(3)\n",
    "        conv3_out = F.relu(self.bn3(self.conv3(embedded))).squeeze(3)\n",
    "        \n",
    "        pooled1 = F.max_pool1d(conv1_out, 2).squeeze(2)\n",
    "        pooled2 = F.max_pool1d(conv2_out, 2).squeeze(2)\n",
    "        pooled3 = F.max_pool1d(conv3_out, 2).squeeze(2)\n",
    "\n",
    "        cat = torch.cat((pooled1, pooled2, pooled3), 0)\n",
    "\n",
    "        conv_pcat1_out = F.relu(self.bn_pcat1(self.conv_pcat1(cat))).squeeze(3)\n",
    "        pooled_pcat1 = F.max_pool2d(conv1_out, 2).squeeze(2)\n",
    "        \n",
    "        #cat = cat.view(cat.size(0), -1)\n",
    "        dropout = self.dropout(pooled_pcat1)\n",
    "        \n",
    "        # Use kan for final classification\n",
    "        out = self.kan(dropout)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "vocab_size = len(tokenizer)  # Vocabulary size of GPT-2 tokenizer\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_class = 2  # Number of classes (negative, positive)\n",
    "\n",
    "model = SentimentCNNModel(vocab_size, embed_dim, num_class)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
