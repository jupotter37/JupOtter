{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from opyenxes.model.XLog import XLog\n",
    "from opyenxes.data_in.XUniversalParser import XUniversalParser\n",
    "from opyenxes.classification.XEventAttributeClassifier import XEventAttributeClassifier\n",
    "\n",
    "from prefixspan import PrefixSpan\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### configuration\n",
    "data_path = \"../logs/bpic2015_1.xes\"\n",
    "traces_picklepath  = data_path.replace(\".xes\", \"_raw_traces.pickled\")\n",
    "traces_dictionarypath = data_path.replace(\".xes\", \"_dictionaries.pickled\")\n",
    "target_column = \"concept:name\"\n",
    "categorical_feature_names = [target_column, 'monitoringResource', 'question', 'org:resource']\n",
    "date_feature_names = [\"time:timestamp\", \"dateFinished\", \"planned\"]\n",
    "eosmarker = \"<EOS>\"\n",
    "### configuration end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown extension: http://www.xes-standard.org/meta_org.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_time.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_life.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_concept.xesext\n",
      "Unknown extension: http://www.xes-standard.org/meta_general.xesext\n"
     ]
    }
   ],
   "source": [
    "with open(data_path) as bpic_file:\n",
    "    eventlog = XUniversalParser().parse(bpic_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eventlog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6563285e91f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mncores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mntraces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meventlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eventlog' is not defined"
     ]
    }
   ],
   "source": [
    "ncores = multiprocessing.cpu_count()\n",
    "ntraces = len(eventlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data trace-wise from XES format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079de21dda2d4da097ae25b70c87e509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1199), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateStop not in DataFrame!\n",
      "dateStop not in DataFrame!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all attributes\n",
    "column_names = []\n",
    "\n",
    "for event in eventlog[0]:\n",
    "    for attribute in event.get_attributes():\n",
    "        column_names.append(attribute)\n",
    "        \n",
    "column_names = set(column_names) # remove duplicates\n",
    "column_names = list(column_names)\n",
    "\n",
    "def create_dataframe_from_trace(t):\n",
    "    df = pd.DataFrame(columns=column_names, index=range(0,len(t)))\n",
    "    for event_idx, event in enumerate(t):\n",
    "        event_attributes = event.get_attributes()\n",
    "        df.iloc[event_idx][\"__case_id\"] = 0\n",
    "        \n",
    "        for attribute in event_attributes:\n",
    "            if attribute not in column_names:\n",
    "                print(attribute, \"not in DataFrame!\")\n",
    "            else:\n",
    "                df[attribute].values[event_idx] = event_attributes[attribute].get_value()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "traces = []\n",
    "\n",
    "for _ in tqdm_notebook(ppool.imap(create_dataframe_from_trace, eventlog),\n",
    "                       total=len(eventlog),\n",
    "                       unit=\"traces\"):\n",
    "        traces.append(_)\n",
    "\n",
    "ppool.close()\n",
    "del eventlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [t[(t[\"lifecycle:transition\"] == \"complete\")].reset_index(drop=True) for t in traces]\n",
    "traces = [t for t in traces if not t.empty]\n",
    "\n",
    "pickle.dump(traces, open(traces_picklepath, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = pickle.load(open(traces_picklepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create complete feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column because it contains only a single value: lifecycle:transition\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "def avg_time(datetimes):\n",
    "    n = max(1, len(datetimes))\n",
    "    total = sum(dt.hour * 3600 + dt.minute * 60 + dt.second for dt in datetimes)\n",
    "    avg = total / n\n",
    "    minutes, seconds = divmod(int(avg), 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return datetime.combine(date(1900, 1, 1), time(hours, minutes, seconds))\n",
    "\n",
    "# activityNameNL and activityNameEN are the same and correlate strongly with concept:name\n",
    "# action_code also correlates strongly with concept:name\n",
    "dropcols = [\"activityNameNL\", \"activityNameEN\", \"action_code\", \"dueDate\"]\n",
    "for t in traces:\n",
    "    t.drop(columns=dropcols, inplace=True, errors='ignore')\n",
    "\n",
    "# fill any NA values\n",
    "for i in range(0,len(traces)):\n",
    "    for c in traces[0].columns:\n",
    "        if c in categorical_feature_names:\n",
    "            traces[i][c].fillna(\"NULL\", inplace=True)\n",
    "        elif c in date_feature_names:\n",
    "            traces[i][c] = pd.to_datetime(traces[i][c], utc=True)\n",
    "            r = avg_time(traces[i][traces[i][c].notnull()][c])\n",
    "            traces[i][c].fillna(r, inplace=True)\n",
    "        else:\n",
    "            traces[i][c].fillna(0, inplace=True)\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)\n",
    "\n",
    "for col in eventlog_df.columns:\n",
    "    if eventlog_df[col].nunique() == 1:\n",
    "        for t in traces:\n",
    "            t.drop(columns=[col], inplace=True)\n",
    "        eventlog_df.drop(columns=[col], inplace=True)\n",
    "        print(\"Dropped column because it contains only a single value:\", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate correlated or unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix.wolff2/anaconda3/envs/thesis/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/felix.wolff2/anaconda3/envs/thesis/lib/python3.6/site-packages/ipykernel/__main__.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/felix.wolff2/anaconda3/envs/thesis/lib/python3.6/site-packages/ipykernel/__main__.py:13: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "        https://stackoverflow.com/questions/46498455/categorical-features-correlation\"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "eventlog_df = pd.concat(traces, ignore_index=True)\n",
    "cramers_v_df = pd.DataFrame(index=eventlog_df.columns,\n",
    "                            columns=eventlog_df.columns,\n",
    "                            dtype=np.float)\n",
    "\n",
    "for col_a,col_b in itertools.product(eventlog_df.columns, repeat=2):   \n",
    "    if col_a == col_b:\n",
    "        cramers_v_df[col_a][col_b] = 1\n",
    "    else:\n",
    "        try:\n",
    "            candidate = pd.crosstab(eventlog_df[col_a], eventlog_df[col_b]).as_matrix()\n",
    "            cramers_v_df[col_a][col_b] = cramers_v(candidate)\n",
    "        except:\n",
    "            cramers_v_df[col_a][col_b] = 0\n",
    "        \n",
    "pickle.dump(cramers_v_df, open('../../docker_share/bpic2015_1/correlation_crosstab.pickled', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert timestamps to relative scale in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to total running time in hours\n",
    "bos_idx = 0\n",
    "for i in range(0, len(traces)):\n",
    "    tlen = len(traces[i])-1\n",
    "    \n",
    "    for c in date_feature_names:\n",
    "        traces[i][c] = pd.to_datetime(traces[i][c], utc=True)\n",
    "        dfs = traces[i][c] - traces[i][c][bos_idx]\n",
    "        traces[i][c] = dfs.map(lambda d: int(d.total_seconds()/(60*60))).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for cf in categorical_feature_names:\n",
    "    cf_dict = { 'to_int': {}, 'to_cat': {} }\n",
    "    events = eventlog_df[cf].unique().tolist()\n",
    "    if cf == target_column: events.append(eosmarker)\n",
    "    cf_dict['to_int'] = dict((c, i) for i, c in enumerate(events))\n",
    "    cf_dict['to_cat'] = dict((i, c) for i, c in enumerate(events))\n",
    "    feature_dict[cf] = cf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SP2 feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every trace and encode the presence of an activity\n",
    "sp2_prefix = \"SP2_\"\n",
    "activity_labels = [ \"{0}{1}\".format(sp2_prefix,a) for a in eventlog_df[\"concept:name\"].unique() ]\n",
    "\n",
    "def enrich_trace_with_sp2(t):\n",
    "    sp2_df = pd.DataFrame(columns=activity_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    for col in sp2_df.columns: sp2_df[col].values[:] = 0\n",
    "    sp2_df[\"{0}{1}\".format(sp2_prefix, t[\"concept:name\"][0])].values[0]  = 1\n",
    "    \n",
    "    for i in range(1,len(t)):\n",
    "        first_activity_name = t[\"concept:name\"].iloc[i]\n",
    "        col = \"{0}{1}\".format(sp2_prefix,first_activity_name)\n",
    "        \n",
    "        sp2_df.values[i] = sp2_df.values[i-1]\n",
    "        sp2_df[col].values[i] = 1\n",
    "        \n",
    "    return sp2_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "sp2_traces = []\n",
    "\n",
    "for _ in tqdm_notebook(ppool.imap(enrich_trace_with_sp2, traces),\n",
    "                       total=len(traces),\n",
    "                       unit=\"traces\"):\n",
    "        sp2_traces.append(_)\n",
    "        \n",
    "ppool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PrefixSpan feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefixspan requires an array of arrays with one subarray for every trace\n",
    "encoded_traces = [ t[target_column].map(feature_dict[target_column]['to_int']).tolist() for t in traces ]\n",
    "prefixspan_traces = PrefixSpan(encoded_traces)\n",
    "closed_sequences = prefixspan_traces.topk(25, closed=True) # support is how often the subsequence appears in total\n",
    "# http://sequenceanalysis.github.io/slides/analyzing_sequential_user_behavior_part2.pdf, slide 5\n",
    "\n",
    "# only take subsequence which are at a certain level of support? like if ss[0]/len(traces) < .90\n",
    "#ps_topkc = list(filter(lambda x: x[0]/len(traces) > .90, ps_topkc))\n",
    "closed_sequences = [ p[1] for p in closed_sequences ]\n",
    "pftrace_args = [ (t, closed_sequences[:], feature_dict[target_column]['to_int']) for t in traces ] # enrich traces with copy of mined subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped__enrich_trace_with_subseq(args):\n",
    "    return enrich_trace_with_subseq(*args)\n",
    "\n",
    "def enrich_trace_with_subseq(t, ps, event_to_int):\n",
    "    col_prefix = \"PFS_\"\n",
    "    subseq_labels = [ \"{0}{1}\".format(col_prefix,ss_idx) for ss_idx, ss in enumerate(ps) ]\n",
    "    subseq_df = pd.DataFrame(columns=subseq_labels, index=range(0,len(t)), dtype=np.bool)\n",
    "    \n",
    "    subseq_df[:].values[:] = False\n",
    "    activity_codes = t[\"concept:name\"].map(event_to_int)\n",
    "    tlen = len(t)\n",
    "    \n",
    "    for i in range(0, tlen):\n",
    "        # loop through all subsequences\n",
    "        for subseq_idx, subseq in enumerate(ps):\n",
    "            if tlen <= i+len(subseq):\n",
    "                continue\n",
    "                \n",
    "            # check if the subsequence takes place in the following fields\n",
    "            subsequence_found = True\n",
    "            j = 0\n",
    "            while subsequence_found and j < len(subseq):\n",
    "                if subseq[j] != activity_codes[j+i]:\n",
    "                    subsequence_found = False\n",
    "                j += 1\n",
    "                    \n",
    "            if subsequence_found:\n",
    "                subseq_df.values[j:,subseq_idx] = True\n",
    "        \n",
    "    return subseq_df\n",
    "\n",
    "ppool = multiprocessing.Pool(ncores)\n",
    "pf_traces = []\n",
    "        \n",
    "for _ in tqdm_notebook(ppool.imap(wrapped__enrich_trace_with_subseq, pftrace_args),\n",
    "                       total=len(pftrace_args),\n",
    "                       unit=\"traces\"):\n",
    "        pf_traces.append(_)\n",
    "        \n",
    "ppool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and normalize ordinal and categorical feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_feature_names = traces[0].columns.difference(categorical_feature_names)\n",
    "n_target_classes = max(feature_dict[target_column]['to_int'].values()) + 1\n",
    "final_traces = copy.deepcopy(traces)\n",
    "\n",
    "ordinal_traces = [None] * len(traces)\n",
    "categorical_traces = [None] * len(traces)\n",
    "target_traces = [None] * len(traces)\n",
    "\n",
    "# Concatenate all features into one feature dataframe per trace\n",
    "for i in range(0, len(traces)):\n",
    "    \n",
    "    # Create TARGET feature column by shifting target column\n",
    "    targets = final_traces[i][target_column].shift(-1).map(feature_dict[target_column]['to_int']).to_frame(\"TARGET\")\n",
    "    targets.values[len(targets)-1] = feature_dict[target_column]['to_int'][eosmarker]\n",
    "    target_traces[i] = pd.DataFrame(np_utils.to_categorical(targets, num_classes=n_target_classes, dtype='bool')).add_prefix(\"TARGET_\")\n",
    "    \n",
    "    # Create separate dfs for ordinal and categorical traces\n",
    "    ordinal_traces[i] = final_traces[i][ordinal_feature_names].astype(np.float32)\n",
    "    categorical_traces[i] = final_traces[i][categorical_feature_names].astype(np.str)\n",
    "    \n",
    "    # min-max-normalization of ordinal features PER TRACE\n",
    "    assert len(ordinal_traces[i]) == len(traces[i]), i\n",
    "    x = ordinal_traces[i]\n",
    "    denominator = x.max(axis=0) - x.min(axis=0)\n",
    "    \n",
    "    for j in range(0, len(denominator)):\n",
    "        if(denominator[j] == 0):\n",
    "            denominator[j] += 1\n",
    "            \n",
    "    ordinal_traces[i] = (x-x.min(axis=0)) / denominator\n",
    "    assert len(ordinal_traces[i]) == len(traces[i]), i\n",
    "    \n",
    "del final_traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sava data sets per variable type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices for stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = list(range(len(ordinal_traces)))\n",
    "y = [len(t) for t in ordinal_traces]\n",
    "train_indices, test_indices, _, _ = train_test_split(X,y, test_size=0.25, random_state=42)\n",
    "\n",
    "save_path = \"/home/felix.wolff2/master-thesis-code/logs/bpic2015_1/\"\n",
    "def save_trace_dataset(dataset, settype, purpose):\n",
    "    suffix = \"{0}_{1}.pickled\".format(settype, purpose)\n",
    "    p = save_path + suffix\n",
    "    pickle.dump(dataset, open(p, \"wb\"), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def filter_by_indices(a, a_idx):\n",
    "    return [ a[i] for i in range(len(a)) if i in a_idx ]\n",
    "\n",
    "save_trace_dataset(feature_dict, 'mapping', 'dict')\n",
    "save_trace_dataset(filter_by_indices(ordinal_traces, train_indices), 'ordinal', 'train')\n",
    "save_trace_dataset(filter_by_indices(categorical_traces, train_indices), 'categorical', 'train')\n",
    "save_trace_dataset(filter_by_indices(sp2_traces, train_indices), 'sp2', 'train')\n",
    "save_trace_dataset(filter_by_indices(pf_traces, train_indices), 'pfs', 'train')\n",
    "save_trace_dataset(filter_by_indices(target_traces, train_indices),'target', 'train')\n",
    "save_trace_dataset(filter_by_indices(ordinal_traces, test_indices), 'ordinal', 'test')\n",
    "save_trace_dataset(filter_by_indices(categorical_traces, test_indices), 'categorical', 'test')\n",
    "save_trace_dataset(filter_by_indices(sp2_traces, test_indices), 'sp2', 'test')\n",
    "save_trace_dataset(filter_by_indices(pf_traces, test_indices), 'pfs', 'test')\n",
    "save_trace_dataset(filter_by_indices(target_traces, test_indices),'target', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min trace length 2\n",
      "Max trace length 101\n",
      "Avg trace length 43.55045871559633\n",
      "# traces 1199\n",
      "Number of events 52217\n",
      "Number of activities 398\n"
     ]
    }
   ],
   "source": [
    "lens = [len(t) for t in traces]\n",
    "print(\"Min trace length\", min(lens))\n",
    "print(\"Max trace length\", max(lens))\n",
    "print(\"Avg trace length\", np.mean(lens))\n",
    "print(\"# traces\", len(traces))\n",
    "print(\"Number of events\", sum(lens))\n",
    "print(\"Number of activities\", len(eventlog_df[\"concept:name\"].unique().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:thesis]",
   "language": "python",
   "name": "conda-env-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
