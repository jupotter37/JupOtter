{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6oOqWYZRaS4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning Homework 5 Template\n",
    "\n",
    "Import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "IPnPKPjeRaS5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "          # not necessary, we will just use some toy dataset in the package\n",
    "import numpy as np               # numpy\n",
    "import matplotlib.pyplot as plt  # matplotlib\n",
    "import pandas as pd\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ2j6972RaS6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1: Introduction to neural networks\n",
    "\n",
    "In this part, you are not allowed to use machine learning packages like pytorch or sklearn.\n",
    "You can still use numpy (and it is recommended to use numpy for better performance).\n",
    "\n",
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3BtO-GWfRaS7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ps5_data_x = pd.read_csv('data/ps5_data.csv', delimiter=',')\n",
    "ps5_data_x = ps5_data_x.fillna(0).to_numpy()\n",
    "ps5_data_y = np.loadtxt('data/ps5_data-labels.csv', delimiter=',', dtype=np.int) - 1\n",
    "ps5_theta1 = np.loadtxt('data/ps5_theta1.csv', delimiter=',')\n",
    "ps5_theta2 = np.loadtxt('data/ps5_theta2.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: (1912, 400) (5000,)\n",
      "Weight shapes: (25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "print('Data shapes:', ps5_data_x.shape, ps5_data_y.shape)\n",
    "print('Weight shapes:', ps5_theta1.shape, ps5_theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps5_data_x[0].shape\n",
    "x = ps5_data_x[0]\n",
    "np.append(x, [1])\n",
    "ps5_data_x[0].shape\n",
    "labels = ps5_data_y[:1912].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0v8qh0LRaS8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 1\n",
    "Implement a neuron unit: Write a function ``forward_linear_layer`` that takes as input the activations\n",
    "from the previous layer and the input weights for that that layer, and returns the\n",
    "activation value for that neuron.\n",
    "\n",
    "Again, you should not use pytorch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "hiQxom1KRaS8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_linear_layer(input, weights):\n",
    "    return sig(np.dot(weights, input))\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.insert(x, [0], [1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (401,) and (400,) not aligned: 401 (dim 0) != 400 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108/787347536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforward_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps5_data_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mps5_theta1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_108/3239683223.py\u001b[0m in \u001b[0;36mforward_linear_layer\u001b[0;34m(input, weights)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madd_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (401,) and (400,) not aligned: 401 (dim 0) != 400 (dim 0)"
     ]
    }
   ],
   "source": [
    "forward_linear_layer(ps5_data_x[0],ps5_theta1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25A_4SCWRaS9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 2\n",
    "Using the function in (1), write a function that takes input $x^{(i)}$ of dimension 400\n",
    "and the 10,025 weights and outputs the 10 output values of the neural network\n",
    "as a list/numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "kU5pjtOLRaS9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _forward(x, weight1, weight2):\n",
    "    forwarded = np.array([])\n",
    "    for weight in weight1:\n",
    "        forwarded = np.append(forwarded, forward_linear_layer(x,weight))\n",
    "    second_forwarded = np.array([])\n",
    "    for weight in weight2:\n",
    "        second_forwarded = np.append(second_forwarded, forward_linear_layer(forwarded,weight))\n",
    "    return second_forwarded\n",
    "\n",
    "def layers(x, weight1, weight2):\n",
    "    h = [x]\n",
    "    for w in weight1, weight2:\n",
    "        h.append(forward_linear_layer(add_bias(h[-1]), w))\n",
    "    return h\n",
    "           \n",
    "def forward(x, weight1, weight2):\n",
    "    return layers(x, weight1, weight2)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(ps5_data_x[0], ps5_theta1,ps5_theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r8TQ5b3RaS9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 3\n",
    "Using the function in (2), write a function that classifies an image as a number\n",
    "between 0 and 9. Thus the output of the function is an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dPenIm1vRaS-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classify(x, weight1, weight2):\n",
    "    return np.argmax(forward(x, weight1, weight2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nEknk2_RaS_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 4\n",
    "Use the function in (3) to classify all 5000 digits in the data set. What is the\n",
    "error rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DB3Uhx-LRaS_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is 0.024582\n"
     ]
    }
   ],
   "source": [
    "def classification_error(x, y, weight1, weight2):\n",
    "    return len(np.nonzero(y - classify(x.T, weight1, weight2))[0]) / len(y)\n",
    "\n",
    "print('The error rate is %.6f' % classification_error(ps5_data_x, ps5_data_y[0:1912], ps5_theta1, ps5_theta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108/2669333034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_108/2669333034.py\u001b[0m in \u001b[0;36mget_one_hot\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "def get_one_hot(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b\n",
    "get_one_hot(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYnRE_rYRaTA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Write down the cross-entropy loss function for this neural network. Write a program to\n",
    "evaluate the cost with the given weights and with the 5000 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "mV0LdsNrRaTA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 1.547686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def loss_function(x, y, weight1, weight2):\n",
    "    results = forward(x.T, weight1, weight2)\n",
    "    return np.sum(\n",
    "        - np.sum(get_one_hot(y) * results.T, axis=1)\n",
    "        + logsumexp(results, axis=0)\n",
    "    ) / len(y)\n",
    "\n",
    "\n",
    "def get_softmax(pred):\n",
    "    return np.exp(pred)/np.sum(np.exp(pred),axis=0)\n",
    "\n",
    "print('The loss is %.6f' % loss_function(ps5_data_x, ps5_data_y[:1912], ps5_theta1, ps5_theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY3hiimSRaTB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 6\n",
    "In order to find the optimal weights, we need to take the partial derivatives of\n",
    "the cost function. For this we use back propagation. Provide pseudo-code\n",
    "showing how we calculate the partial derivatives using back propagation. (No\n",
    "actual coding is needed here.) Be fully detailed with the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RY9ELiXRaTB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "func error_deriv(output_neuron) = $$\\frac{\\partial MLE}{\\partial z_j}= - \\frac{\\partial}{\\partial z_j}\\sum_{{i=1}}^{num classes} (label_i \\cdot\\log{s_i})$$\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C_0}{\\partial w^L} = \\frac{\\partial z^L}{\\partial w^L}\\frac{\\partial a^L}{\\partial z^L}\\frac{\\partial C_0}{\\partial a^L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_0 = (a^L = y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^L = w^L a^{L-1} + b^L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: gradient of the neural network's function wrt its parameters, evaluated at x: $$\\nabla_\\theta L(y,h_\\theta (x))$$\n",
    "Initialize $$g = \\nabla_\\theta L(y,h_\\theta (x)) \\in \\mathbb{R}^{d_L}$$\n",
    "for l=L,L-1,...,1 do:\n",
    "$$g \\leftarrow \\nabla_{z^{(l)}} L(y,h_\\theta (x)) = g\\bigodot\\sigma^{(l)'}(z^{(l)})\\in \\mathbb{R}^{d_l}\\\\\n",
    "\\nabla_{b^{(l)}} L(y,h_\\theta (x)) \\leftarrow g \\mathbb{R}^{d_l} \\\\\n",
    "\\nabla_{w^{(l)}} L(y,h_\\theta (x)) \\leftarrow g \\times a^{(l-1)^{transpose}} \\in\\mathbb{R}^{d_l\\times d_{l-1}}\\\\\n",
    "g \\leftarrow \\nabla_{a^{(l-1)}} L(y,h_\\theta (x)) = w^{(l-1)^{transpose}}g\\in\\mathbb{R}^{d_{l-1}}\n",
    "$$\n",
    "end for,\n",
    "return gradients calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^L = \\sigma(z^L) \\\\ \n",
    "\\frac{\\partial C_0}{\\partial a^L} = 2(a^L-y)\\\\\n",
    "\\frac{\\partial a^L}{\\partial z^L} = \\sigma'(z^L)\\\\\n",
    "\\frac{\\partial z^L}{\\partial w^L} = a^{L-1}\\\\\n",
    "\\frac{\\partial C}{\\partial w^L} = \\frac{1}{n}\\sum_{k=0}^{n-1}\\frac{\\partial C_k}{\\partial w^L}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C_0}{\\partial w_jk^L} = \\frac{\\partial z_j^L}{\\partial w_jk^L}\\frac{\\partial a_j^L}{\\partial z_j^L}\\frac{\\partial C_0}{\\partial a_j^L}\\\\\n",
    "\\frac{\\partial C}{\\partial w_jk^l} = a_k^{l-1}\\sigma'(z_j^l)2(a_j^L - y_j)$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwOO6Z76RaTB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Question 7\n",
    "\n",
    "Write code to calculate the 10,025 partial derivatives. Calculate the mean,\n",
    "median, and standard deviation of the absolute value of the partial derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\\frac{\\partial C_0}{\\partial w^L} = \\frac{\\partial z^L}{\\partial w^L}\\frac{\\partial a^L}{\\partial z^L}\\frac{\\partial C_0}{\\partial a^L}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "0Ae-adbGRaTC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ds(x):\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "\n",
    "def gradient_calculation(x, y, weight1, weight2):\n",
    "    h1, h2, h3 = layers(x.T, weight1, weight2)\n",
    "    g1, g2 = add_bias(h1), add_bias(h2)\n",
    "    exp_h3 = np.exp(h3) \n",
    "    dl_dhD = - get_one_hot(y).T + (exp_h3 / np.sum(exp_h3, axis=0))\n",
    "    d_sigma_2 = ds(np.dot(weight2,g2))\n",
    "    d_weight2 = np.dot(d_sigma_2 * dl_dhD, g2.T)\n",
    "    s_2 = h2.shape[0]\n",
    "    G_2 = np.hstack((\n",
    "        np.identity(s_2),\n",
    "        np.zeros((s_2,1)),))\n",
    "    v = np.dot(\n",
    "        G_2,\n",
    "        np.dot(\n",
    "            weight2.T,\n",
    "            d_sigma_2 * dl_dhD,\n",
    "        ),\n",
    "    )\n",
    "    d_weight1 = np.dot(ds(np.dot(weight1, g1)) * v, g1.T)\n",
    "    return d_weight1, d_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "CDS2pok6RaTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of weight1: mean 7.655e-01, median 1.031e-01, std 1.405e+00\n",
      "Gradient of weight2: mean 4.661e+00, median 1.361e+00, std 6.167e+00\n"
     ]
    }
   ],
   "source": [
    "d_weight1, d_weight2 = gradient_calculation(ps5_data_x, ps5_data_y[:1912], ps5_theta1, ps5_theta2)\n",
    "for i, d_weight in enumerate([d_weight1, d_weight2]):\n",
    "    print('Gradient of weight%d: mean %.3e, median %.3e, std %.3e' % (i + 1,\n",
    "                                                                      np.mean(np.abs(d_weight)),\n",
    "                                                                      np.median(np.abs(d_weight)),\n",
    "                                                                      np.std(np.abs(d_weight))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo1okorDRaTC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Implement the model training function using numpy. Implement stochastic gradient descent to optimize the model.\n",
    "Plot your training progress (training, validation loss vs. epochs) in the function.\n",
    "\n",
    "To avoid overfitting, the function returns the optimal model parameters in the epoch that gives you the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "RiWvglEWRaTD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_train(train_x, train_y, val_x, val_y, n_epochs, learning_rate):\n",
    "    n_weights = 2\n",
    "    weights_df = pd.DataFrame(columns=['validation_loss',\n",
    "            'training_loss',\n",
    "            'validation_error',\n",
    "            'training_error','weights'])\n",
    "    wh = [[np.random.rand(*ps5_theta1.shape) - 0.5, np.random.rand(*ps5_theta2.shape) - 0.5]]\n",
    "    for epoch in range(n_epochs):\n",
    "        curr_w = wh[-1]\n",
    "        validation_loss = loss_function(val_x, val_y, curr_w[0], curr_w[1])\n",
    "        training_loss = loss_function(train_x, train_y, curr_w[0], curr_w[1])\n",
    "        val_err = classification_error(val_x, val_y, curr_w[0], curr_w[1] )\n",
    "        train_err = classification_error(train_x, train_y, curr_w[0], curr_w[1])\n",
    "        weights_df = weights_df.append({'validation_loss': validation_loss,\n",
    "            'training_loss': training_loss,\n",
    "            'validation_error': val_err,\n",
    "            'training_error': train_err,'weights':curr_w},ignore_index=True)\n",
    "\n",
    "        gradients = gradient_calculation(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            curr_w[0],\n",
    "            curr_w[1]\n",
    "        )\n",
    "        \n",
    "        wh.append([\n",
    "            curr_w[i] - learning_rate * gradients[i]\n",
    "            for i in range(n_weights)\n",
    "        ])\n",
    "    weights_df['training_loss'].plot()\n",
    "    weights_df['validation_loss'].plot()\n",
    "    idx = weights_df['validation_error'].argmin()\n",
    "    best_weights = list(weights_df['weights'].loc[[idx]])[0]\n",
    "    return best_weights[0], best_weights[1], weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFRHXgVARaTD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 9\n",
    "\n",
    "Use 60% data for training, 20% data for validation and 20% for testing using the  ``split_data`` function.\n",
    "Train the model with learning rate 0.01.\n",
    "\n",
    "Train your model for at least 100 epochs. If your code runs very slow, check whether your\n",
    "loss function and gradient calculation is optimized using numpy operations (e.g., ``np.matmul``, ``np.dot``) instead of\n",
    "nested python ``for`` loops.\n",
    "\n",
    "Report the epoch that gives you the lowest validation loss. Also report the final accuracy on all 3 splits. Comment on the result you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "4OiWi7stRaTD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch with lowest val loss: 97\n",
      "Final training error 0.5204882301656495\n",
      "Final validation error 0.5445026178010471\n",
      "Final test error 0.5535248041775457\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def split_data(data_x, data_y, train_ratio=0.6, val_ratio=0.2):\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    n_train_samples = int(data_x.shape[0] * train_ratio)\n",
    "    n_val_samples = int(data_x.shape[0] * val_ratio)\n",
    "    train_indices = indices[:n_train_samples]\n",
    "    val_indices = indices[n_train_samples:n_train_samples + n_val_samples]\n",
    "    test_indices = indices[n_train_samples + n_val_samples:]\n",
    "    return data_x[train_indices], data_y[train_indices], data_x[val_indices], data_y[val_indices], data_x[test_indices], data_y[test_indices]\n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = split_data(ps5_data_x, ps5_data_y)\n",
    "weight1, weight2, df = model_train(train_x, train_y, val_x, val_y, 100, 1e-2)\n",
    "print(\"epoch with lowest val loss:\", df['validation_error'].argmin())\n",
    "print('Final training error', classification_error(train_x, train_y, weight1, weight2))\n",
    "print('Final validation error', classification_error(val_x, val_y, weight1, weight2))\n",
    "print('Final test error', classification_error(test_x, test_y, weight1, weight2))\n",
    "\n",
    "# add code / comment below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KM9MqZCRcUW"
   },
   "source": [
    "### Question 10\n",
    "\n",
    "For each weight $w$, obtain a random number $r$ in the range $[0.9w, 1.1w]$ and replace $w$ with $w+r$. (You will need to make $10,025$ replacements with $10,025$ random numbers.) Using these new test examples, repeat Questions~6 and 8. Comment on how your results changed.  The idea behind this question is to see how ``robust'' the neural network output is when we perturb a bit the neural network's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "VFb84JHdRc3I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch with lowest val loss: 97\n",
      "Final training error 0.5204882301656495\n",
      "Final validation error 0.5445026178010471\n",
      "Final test error 0.5535248041775457\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "for weights in (ps5_theta1,ps5_theta2):\n",
    "    for i, w in enumerate(weights):\n",
    "        weights[i] = random.uniform(.9,1.1)*w + w\n",
    "        \n",
    "train_x, train_y, val_x, val_y, test_x, test_y = split_data(ps5_data_x, ps5_data_y)\n",
    "weight1, weight2, df = model_train(train_x, train_y, val_x, val_y, 100, 1e-2)\n",
    "print(\"epoch with lowest val loss:\", df['validation_error'].argmin())\n",
    "print('Final training error', classification_error(train_x, train_y, weight1, weight2))\n",
    "print('Final validation error', classification_error(val_x, val_y, weight1, weight2))\n",
    "print('Final test error', classification_error(test_x, test_y, weight1, weight2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXgvIXa4RaTE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Part 2: Convolutional Neural Network\n",
    "\n",
    "**Starting from this part, you are allowed (and expected to) use PyTorch for model training and evaluation.**\n",
    "\n",
    "In Q11, you are required to use PyTorch to perform image classification on CIFAR 10. You can acquire CIFAR 10 by running the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "h-6h-UdSRaTE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaba61e8db645d289ec04d60d62e0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to .\n",
      "Files already downloaded and verified\n",
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "cifar10_train = torchvision.datasets.CIFAR10('.', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10('.', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "print(len(cifar10_train), len(cifar10_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dE2k-NDRaTE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will not directly use the full set. We will split ``cifar10_train_val`` into two datasets, a training and a validation.\n",
    "The training set should contain the first 1000 images, and the validation set should contain the last 100 images. Also, for the test set,\n",
    "we only keep the first 100 images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Un2cs4xJRaTF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 100 100\n"
     ]
    }
   ],
   "source": [
    "train_set = torch.utils.data.Subset(cifar10_train, np.arange(1000))\n",
    "val_set = torch.utils.data.Subset(cifar10_train, np.arange(len(cifar10_train) - 100, len(cifar10_train)))\n",
    "test_set = torch.utils.data.Subset(cifar10_test, np.arange(100))\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tfds-nightly\n",
      "  Downloading tfds_nightly-4.5.2.dev202204300044-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (3.17.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (1.6.0)\n",
      "Collecting etils[epath-no-tf]\n",
      "  Downloading etils-0.5.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (0.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (2.27.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (2.3)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (5.4.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (0.3.4)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (4.62.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (1.19.4)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tfds-nightly) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tfds-nightly) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tfds-nightly) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tfds-nightly) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tfds-nightly) (1.26.8)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[epath-no-tf]->tfds-nightly) (3.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tfds-nightly) (1.54.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[epath-no-tf]->tfds-nightly) (3.7.4.3)\n",
      "Installing collected packages: etils, tfds-nightly\n",
      "Successfully installed etils-0.5.0 tfds-nightly-4.5.2.dev202204300044\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 8s 0us/step\n",
      "170508288/170498071 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c40dH1A_RaTF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 11\n",
    "\n",
    "Implement a convolutional neural network. Your network should have at least 2 convolutional layers, 1 max pooling\n",
    "layer and 1 fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D10FXI9oRaTG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    pass  # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vYuXv3mRaTG"
   },
   "source": [
    "### Question 12\n",
    "\n",
    "Implement the training function. Plot training/validation loss vs. number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "mlo3U0OeRaTG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the first basic model I tried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 07:35:02.903229: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2022-04-30 07:35:03.670594: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 07:35:07.147320: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 13s 4ms/step - loss: 1.5030 - accuracy: 0.4541 - val_loss: 1.2612 - val_accuracy: 0.5546\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1437 - accuracy: 0.5951 - val_loss: 1.1027 - val_accuracy: 0.6105\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9824 - accuracy: 0.6544 - val_loss: 0.9870 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8814 - accuracy: 0.6920 - val_loss: 0.9362 - val_accuracy: 0.6734\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8090 - accuracy: 0.7176 - val_loss: 0.8863 - val_accuracy: 0.6883\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7512 - accuracy: 0.7384 - val_loss: 0.8823 - val_accuracy: 0.7006\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6944 - accuracy: 0.7566 - val_loss: 0.8856 - val_accuracy: 0.7002\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6523 - accuracy: 0.7715 - val_loss: 0.8493 - val_accuracy: 0.7083\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6060 - accuracy: 0.7876 - val_loss: 0.8992 - val_accuracy: 0.7022\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5671 - accuracy: 0.8008 - val_loss: 0.9552 - val_accuracy: 0.6897\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5332 - accuracy: 0.8116 - val_loss: 0.9066 - val_accuracy: 0.7075\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4971 - accuracy: 0.8246 - val_loss: 0.9149 - val_accuracy: 0.7172\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4692 - accuracy: 0.8342 - val_loss: 0.9381 - val_accuracy: 0.7055\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4369 - accuracy: 0.8454 - val_loss: 0.9885 - val_accuracy: 0.7010\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4116 - accuracy: 0.8529 - val_loss: 1.0520 - val_accuracy: 0.7046\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3790 - accuracy: 0.8655 - val_loss: 1.1387 - val_accuracy: 0.6962\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3573 - accuracy: 0.8723 - val_loss: 1.1256 - val_accuracy: 0.6929\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3338 - accuracy: 0.8814 - val_loss: 1.1588 - val_accuracy: 0.6991\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3168 - accuracy: 0.8870 - val_loss: 1.1932 - val_accuracy: 0.6958\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2902 - accuracy: 0.8968 - val_loss: 1.2740 - val_accuracy: 0.6930\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2728 - accuracy: 0.9024 - val_loss: 1.3907 - val_accuracy: 0.7025\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2618 - accuracy: 0.9059 - val_loss: 1.5235 - val_accuracy: 0.6851\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2448 - accuracy: 0.9128 - val_loss: 1.4921 - val_accuracy: 0.6974\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2297 - accuracy: 0.9177 - val_loss: 1.5314 - val_accuracy: 0.6940\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2192 - accuracy: 0.9214 - val_loss: 1.5965 - val_accuracy: 0.6883\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2109 - accuracy: 0.9228 - val_loss: 1.6665 - val_accuracy: 0.6875\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2043 - accuracy: 0.9273 - val_loss: 1.6457 - val_accuracy: 0.6793\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1819 - accuracy: 0.9347 - val_loss: 1.7299 - val_accuracy: 0.6939\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1911 - accuracy: 0.9333 - val_loss: 1.7966 - val_accuracy: 0.6814\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1764 - accuracy: 0.9368 - val_loss: 1.8458 - val_accuracy: 0.6834\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1663 - accuracy: 0.9400 - val_loss: 1.9021 - val_accuracy: 0.6886\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1672 - accuracy: 0.9406 - val_loss: 1.9933 - val_accuracy: 0.6838\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1543 - accuracy: 0.9450 - val_loss: 2.1542 - val_accuracy: 0.6743\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1572 - accuracy: 0.9444 - val_loss: 2.0683 - val_accuracy: 0.6852\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1511 - accuracy: 0.9460 - val_loss: 2.1768 - val_accuracy: 0.6843\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1440 - accuracy: 0.9498 - val_loss: 2.1603 - val_accuracy: 0.6863\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1464 - accuracy: 0.9493 - val_loss: 2.2726 - val_accuracy: 0.6648\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1431 - accuracy: 0.9496 - val_loss: 2.2838 - val_accuracy: 0.6817\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1378 - accuracy: 0.9517 - val_loss: 2.2751 - val_accuracy: 0.6795\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1365 - accuracy: 0.9515 - val_loss: 2.4353 - val_accuracy: 0.6745\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1239 - accuracy: 0.9574 - val_loss: 2.4244 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1406 - accuracy: 0.9510 - val_loss: 2.4568 - val_accuracy: 0.6758\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1198 - accuracy: 0.9583 - val_loss: 2.5135 - val_accuracy: 0.6790\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1305 - accuracy: 0.9550 - val_loss: 2.5874 - val_accuracy: 0.6697\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1241 - accuracy: 0.9559 - val_loss: 2.5824 - val_accuracy: 0.6784\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1165 - accuracy: 0.9592 - val_loss: 2.5703 - val_accuracy: 0.6776\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1135 - accuracy: 0.9620 - val_loss: 2.7940 - val_accuracy: 0.6739\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1223 - accuracy: 0.9586 - val_loss: 2.6774 - val_accuracy: 0.6708\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1123 - accuracy: 0.9610 - val_loss: 2.7394 - val_accuracy: 0.6730\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1125 - accuracy: 0.9620 - val_loss: 2.8096 - val_accuracy: 0.6716\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1202 - accuracy: 0.9586 - val_loss: 2.7077 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1135 - accuracy: 0.9624 - val_loss: 2.8427 - val_accuracy: 0.6671\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1044 - accuracy: 0.9642 - val_loss: 2.7716 - val_accuracy: 0.6740\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1110 - accuracy: 0.9629 - val_loss: 2.8490 - val_accuracy: 0.6826\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1054 - accuracy: 0.9646 - val_loss: 2.9005 - val_accuracy: 0.6876\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1078 - accuracy: 0.9639 - val_loss: 2.8382 - val_accuracy: 0.6842\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1016 - accuracy: 0.9655 - val_loss: 2.9722 - val_accuracy: 0.6748\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1031 - accuracy: 0.9661 - val_loss: 3.0432 - val_accuracy: 0.6795\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0951 - accuracy: 0.9679 - val_loss: 3.0737 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1102 - accuracy: 0.9637 - val_loss: 3.0438 - val_accuracy: 0.6732\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1004 - accuracy: 0.9673 - val_loss: 3.1412 - val_accuracy: 0.6790\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0978 - accuracy: 0.9677 - val_loss: 3.1183 - val_accuracy: 0.6695\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1036 - accuracy: 0.9653 - val_loss: 3.1381 - val_accuracy: 0.6799\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0898 - accuracy: 0.9695 - val_loss: 3.0846 - val_accuracy: 0.6694\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0968 - accuracy: 0.9681 - val_loss: 3.3853 - val_accuracy: 0.6684\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1048 - accuracy: 0.9654 - val_loss: 3.1889 - val_accuracy: 0.6764\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0947 - accuracy: 0.9704 - val_loss: 3.3396 - val_accuracy: 0.6654\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0943 - accuracy: 0.9694 - val_loss: 3.2030 - val_accuracy: 0.6751\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0966 - accuracy: 0.9686 - val_loss: 3.2699 - val_accuracy: 0.6622\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0914 - accuracy: 0.9690 - val_loss: 3.4388 - val_accuracy: 0.6636\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0908 - accuracy: 0.9713 - val_loss: 3.3291 - val_accuracy: 0.6683\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0939 - accuracy: 0.9694 - val_loss: 3.2332 - val_accuracy: 0.6693\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0923 - accuracy: 0.9705 - val_loss: 3.3896 - val_accuracy: 0.6732\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0919 - accuracy: 0.9694 - val_loss: 3.4643 - val_accuracy: 0.6717\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0946 - accuracy: 0.9704 - val_loss: 3.4473 - val_accuracy: 0.6625\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0894 - accuracy: 0.9711 - val_loss: 3.4502 - val_accuracy: 0.6632\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0878 - accuracy: 0.9718 - val_loss: 3.5027 - val_accuracy: 0.6789\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0796 - accuracy: 0.9725 - val_loss: 3.4909 - val_accuracy: 0.6789\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0916 - accuracy: 0.9710 - val_loss: 3.4113 - val_accuracy: 0.6768\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0864 - accuracy: 0.9723 - val_loss: 3.5506 - val_accuracy: 0.6727\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0858 - accuracy: 0.9729 - val_loss: 3.5720 - val_accuracy: 0.6740\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0866 - accuracy: 0.9730 - val_loss: 3.6749 - val_accuracy: 0.6702\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0862 - accuracy: 0.9736 - val_loss: 3.6117 - val_accuracy: 0.6705\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0836 - accuracy: 0.9737 - val_loss: 3.5190 - val_accuracy: 0.6739\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0821 - accuracy: 0.9729 - val_loss: 3.7240 - val_accuracy: 0.6666\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0961 - accuracy: 0.9704 - val_loss: 3.5515 - val_accuracy: 0.6782\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0742 - accuracy: 0.9762 - val_loss: 3.6135 - val_accuracy: 0.6748\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0924 - accuracy: 0.9712 - val_loss: 3.6557 - val_accuracy: 0.6766\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0751 - accuracy: 0.9766 - val_loss: 3.6962 - val_accuracy: 0.6749\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0761 - accuracy: 0.9758 - val_loss: 3.6402 - val_accuracy: 0.6814\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0888 - accuracy: 0.9722 - val_loss: 3.7296 - val_accuracy: 0.6738\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0826 - accuracy: 0.9752 - val_loss: 3.7892 - val_accuracy: 0.6709\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0784 - accuracy: 0.9750 - val_loss: 3.7633 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0752 - accuracy: 0.9762 - val_loss: 3.8118 - val_accuracy: 0.6801\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0772 - accuracy: 0.9763 - val_loss: 4.0039 - val_accuracy: 0.6733\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0906 - accuracy: 0.9725 - val_loss: 3.9945 - val_accuracy: 0.6735\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0752 - accuracy: 0.9762 - val_loss: 3.8579 - val_accuracy: 0.6763\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0854 - accuracy: 0.9744 - val_loss: 3.9178 - val_accuracy: 0.6730\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0776 - accuracy: 0.9758 - val_loss: 3.9294 - val_accuracy: 0.6733\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0827 - accuracy: 0.9754 - val_loss: 3.9262 - val_accuracy: 0.6682\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0816 - accuracy: 0.9752 - val_loss: 3.8964 - val_accuracy: 0.6689\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0818 - accuracy: 0.9755 - val_loss: 3.9458 - val_accuracy: 0.6722\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0784 - accuracy: 0.9757 - val_loss: 4.0057 - val_accuracy: 0.6715\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0742 - accuracy: 0.9768 - val_loss: 4.1095 - val_accuracy: 0.6653\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0774 - accuracy: 0.9752 - val_loss: 4.1448 - val_accuracy: 0.6703\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0807 - accuracy: 0.9753 - val_loss: 4.1281 - val_accuracy: 0.6770\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0755 - accuracy: 0.9776 - val_loss: 4.1948 - val_accuracy: 0.6717\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0799 - accuracy: 0.9759 - val_loss: 4.1183 - val_accuracy: 0.6697\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0793 - accuracy: 0.9756 - val_loss: 3.9510 - val_accuracy: 0.6773\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0830 - accuracy: 0.9751 - val_loss: 4.1975 - val_accuracy: 0.6728\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0742 - accuracy: 0.9784 - val_loss: 4.0632 - val_accuracy: 0.6761\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0812 - accuracy: 0.9757 - val_loss: 4.2113 - val_accuracy: 0.6681\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0728 - accuracy: 0.9776 - val_loss: 4.0961 - val_accuracy: 0.6687\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0738 - accuracy: 0.9776 - val_loss: 4.2340 - val_accuracy: 0.6666\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0841 - accuracy: 0.9754 - val_loss: 4.1889 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0738 - accuracy: 0.9787 - val_loss: 4.2404 - val_accuracy: 0.6641\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0821 - accuracy: 0.9753 - val_loss: 4.3334 - val_accuracy: 0.6641\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0705 - accuracy: 0.9783 - val_loss: 4.3107 - val_accuracy: 0.6686\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0796 - accuracy: 0.9765 - val_loss: 4.4104 - val_accuracy: 0.6754\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0736 - accuracy: 0.9784 - val_loss: 4.3037 - val_accuracy: 0.6660\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0692 - accuracy: 0.9789 - val_loss: 4.4658 - val_accuracy: 0.6684\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0914 - accuracy: 0.9745 - val_loss: 4.2825 - val_accuracy: 0.6702\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 4.3988 - val_accuracy: 0.6636\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0707 - accuracy: 0.9786 - val_loss: 4.4880 - val_accuracy: 0.6724\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0732 - accuracy: 0.9789 - val_loss: 4.3737 - val_accuracy: 0.6689\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0736 - accuracy: 0.9787 - val_loss: 4.5302 - val_accuracy: 0.6708\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0855 - accuracy: 0.9756 - val_loss: 4.4884 - val_accuracy: 0.6728\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0683 - accuracy: 0.9791 - val_loss: 4.5424 - val_accuracy: 0.6766\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0715 - accuracy: 0.9792 - val_loss: 4.3287 - val_accuracy: 0.6695\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0899 - accuracy: 0.9739 - val_loss: 4.5727 - val_accuracy: 0.6720\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0705 - accuracy: 0.9799 - val_loss: 4.5644 - val_accuracy: 0.6640\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0831 - accuracy: 0.9770 - val_loss: 4.4254 - val_accuracy: 0.6722\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0656 - accuracy: 0.9806 - val_loss: 4.7404 - val_accuracy: 0.6716\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0702 - accuracy: 0.9787 - val_loss: 4.6414 - val_accuracy: 0.6644\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0718 - accuracy: 0.9793 - val_loss: 4.7540 - val_accuracy: 0.6728\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0825 - accuracy: 0.9770 - val_loss: 4.7078 - val_accuracy: 0.6753\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0715 - accuracy: 0.9799 - val_loss: 4.6749 - val_accuracy: 0.6679\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0688 - accuracy: 0.9808 - val_loss: 4.9096 - val_accuracy: 0.6660\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0690 - accuracy: 0.9800 - val_loss: 4.9202 - val_accuracy: 0.6729\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0802 - accuracy: 0.9770 - val_loss: 4.8518 - val_accuracy: 0.6714\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0770 - accuracy: 0.9782 - val_loss: 4.8613 - val_accuracy: 0.6671\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0730 - accuracy: 0.9787 - val_loss: 4.8160 - val_accuracy: 0.6719\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0770 - accuracy: 0.9787 - val_loss: 5.1935 - val_accuracy: 0.6637\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0739 - accuracy: 0.9793 - val_loss: 4.8773 - val_accuracy: 0.6721\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0823 - accuracy: 0.9776 - val_loss: 4.8081 - val_accuracy: 0.6634\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0694 - accuracy: 0.9795 - val_loss: 4.8767 - val_accuracy: 0.6734\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0687 - accuracy: 0.9810 - val_loss: 5.0094 - val_accuracy: 0.6758\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0783 - accuracy: 0.9782 - val_loss: 4.8453 - val_accuracy: 0.6664\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0709 - accuracy: 0.9802 - val_loss: 5.0041 - val_accuracy: 0.6674\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0715 - accuracy: 0.9795 - val_loss: 5.0495 - val_accuracy: 0.6721\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0735 - accuracy: 0.9790 - val_loss: 5.1658 - val_accuracy: 0.6717\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0735 - accuracy: 0.9811 - val_loss: 5.1802 - val_accuracy: 0.6718\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0747 - accuracy: 0.9793 - val_loss: 5.2683 - val_accuracy: 0.6674\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0827 - accuracy: 0.9779 - val_loss: 5.1773 - val_accuracy: 0.6661\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0657 - accuracy: 0.9817 - val_loss: 5.1590 - val_accuracy: 0.6682\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0803 - accuracy: 0.9786 - val_loss: 5.1862 - val_accuracy: 0.6746\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0713 - accuracy: 0.9812 - val_loss: 5.2667 - val_accuracy: 0.6664\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0708 - accuracy: 0.9805 - val_loss: 5.3756 - val_accuracy: 0.6750\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0854 - accuracy: 0.9782 - val_loss: 5.2595 - val_accuracy: 0.6680\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0654 - accuracy: 0.9816 - val_loss: 5.3480 - val_accuracy: 0.6708\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0755 - accuracy: 0.9799 - val_loss: 5.3809 - val_accuracy: 0.6706\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0717 - accuracy: 0.9814 - val_loss: 5.4155 - val_accuracy: 0.6684\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0823 - accuracy: 0.9786 - val_loss: 5.4761 - val_accuracy: 0.6718\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0773 - accuracy: 0.9796 - val_loss: 5.4973 - val_accuracy: 0.6660\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0705 - accuracy: 0.9814 - val_loss: 5.4650 - val_accuracy: 0.6709\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0825 - accuracy: 0.9784 - val_loss: 5.3571 - val_accuracy: 0.6690\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0685 - accuracy: 0.9816 - val_loss: 6.0512 - val_accuracy: 0.6640\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0814 - accuracy: 0.9788 - val_loss: 5.5784 - val_accuracy: 0.6686\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0662 - accuracy: 0.9824 - val_loss: 5.4679 - val_accuracy: 0.6739\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0726 - accuracy: 0.9808 - val_loss: 5.6031 - val_accuracy: 0.6712\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0835 - accuracy: 0.9798 - val_loss: 5.3715 - val_accuracy: 0.6718\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 5.5498 - val_accuracy: 0.6704\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0769 - accuracy: 0.9793 - val_loss: 5.6814 - val_accuracy: 0.6748\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0808 - accuracy: 0.9799 - val_loss: 5.6163 - val_accuracy: 0.6699\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0751 - accuracy: 0.9810 - val_loss: 5.9735 - val_accuracy: 0.6593\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0776 - accuracy: 0.9803 - val_loss: 5.6602 - val_accuracy: 0.6765\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0723 - accuracy: 0.9810 - val_loss: 5.8788 - val_accuracy: 0.6725\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0822 - accuracy: 0.9808 - val_loss: 5.9389 - val_accuracy: 0.6737\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 6.2662 - val_accuracy: 0.6668\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0794 - accuracy: 0.9805 - val_loss: 5.8107 - val_accuracy: 0.6719\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0788 - accuracy: 0.9798 - val_loss: 6.0695 - val_accuracy: 0.6753\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0778 - accuracy: 0.9811 - val_loss: 5.8919 - val_accuracy: 0.6613\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0749 - accuracy: 0.9819 - val_loss: 5.7312 - val_accuracy: 0.6677\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0697 - accuracy: 0.9818 - val_loss: 5.9721 - val_accuracy: 0.6724\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0840 - accuracy: 0.9791 - val_loss: 6.1094 - val_accuracy: 0.6726\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0761 - accuracy: 0.9816 - val_loss: 6.0379 - val_accuracy: 0.6695\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0829 - accuracy: 0.9798 - val_loss: 5.7785 - val_accuracy: 0.6689\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0735 - accuracy: 0.9818 - val_loss: 5.8753 - val_accuracy: 0.6764\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0783 - accuracy: 0.9806 - val_loss: 6.0034 - val_accuracy: 0.6695\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0667 - accuracy: 0.9827 - val_loss: 6.0288 - val_accuracy: 0.6738\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0841 - accuracy: 0.9801 - val_loss: 6.1140 - val_accuracy: 0.6707\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0808 - accuracy: 0.9804 - val_loss: 6.3167 - val_accuracy: 0.6621\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0783 - accuracy: 0.9814 - val_loss: 6.2155 - val_accuracy: 0.6638\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0801 - accuracy: 0.9803 - val_loss: 6.1731 - val_accuracy: 0.6670\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0697 - accuracy: 0.9828 - val_loss: 6.0958 - val_accuracy: 0.6632\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0818 - accuracy: 0.9804 - val_loss: 6.0950 - val_accuracy: 0.6581\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0750 - accuracy: 0.9817 - val_loss: 6.1148 - val_accuracy: 0.6663\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0750 - accuracy: 0.9820 - val_loss: 6.0798 - val_accuracy: 0.6624\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0805 - accuracy: 0.9801 - val_loss: 6.2672 - val_accuracy: 0.6704\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 6.3706 - val_accuracy: 0.6691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0810172a60>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=200,\n",
    "    validation_data=(test_images, test_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlACNxVjRaTG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 13\n",
    "\n",
    "Bonus: include more training data for your model and/or adjusting the model architecture to build a stronger model.\n",
    "Report the hyper-parameter settings you tried and the corresponding validation accuracy.\n",
    "\n",
    "After you acquire the best model on the validation set, report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "loQQNre5RaTG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d_58. Consider increasing the input size. Received input shape [None, 1, 1, 64] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_164/201840714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    303\u001b[0m           \u001b[0;34mf'One of the dimensions in the output is <= 0 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m           \u001b[0;34mf'due to downsampling in {self.name}. Consider '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_58. Consider increasing the input size. Received input shape [None, 1, 1, 64] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(GaussianNoise(0.1))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(GaussianNoise(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNoise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108/3087897798.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GaussianNoise' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(GaussianNoise(0.1))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(.2, input_shape=(2,)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(.2, input_shape=(2,)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(GaussianNoise(0.1))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(.2, input_shape=(2,)))\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dense(10))\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9329 - accuracy: 0.2537 - val_loss: 1.6550 - val_accuracy: 0.3823\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5717 - accuracy: 0.4222 - val_loss: 1.4378 - val_accuracy: 0.4756\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4251 - accuracy: 0.4859 - val_loss: 1.3332 - val_accuracy: 0.5160\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3349 - accuracy: 0.5243 - val_loss: 1.2646 - val_accuracy: 0.5484\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2636 - accuracy: 0.5529 - val_loss: 1.3219 - val_accuracy: 0.5468\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2092 - accuracy: 0.5767 - val_loss: 1.1427 - val_accuracy: 0.5887\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1650 - accuracy: 0.5941 - val_loss: 1.1057 - val_accuracy: 0.6214\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1211 - accuracy: 0.6089 - val_loss: 1.0793 - val_accuracy: 0.6255\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0820 - accuracy: 0.6227 - val_loss: 1.0686 - val_accuracy: 0.6292\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0588 - accuracy: 0.6307 - val_loss: 1.0436 - val_accuracy: 0.6365\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0285 - accuracy: 0.6443 - val_loss: 0.9824 - val_accuracy: 0.6578\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0118 - accuracy: 0.6492 - val_loss: 0.9743 - val_accuracy: 0.6638\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9845 - accuracy: 0.6594 - val_loss: 0.9697 - val_accuracy: 0.6669\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9630 - accuracy: 0.6661 - val_loss: 0.9594 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9519 - accuracy: 0.6713 - val_loss: 0.9410 - val_accuracy: 0.6764\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9297 - accuracy: 0.6797 - val_loss: 0.9331 - val_accuracy: 0.6832\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.9115 - accuracy: 0.6845 - val_loss: 0.9177 - val_accuracy: 0.6901\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8997 - accuracy: 0.6908 - val_loss: 0.8829 - val_accuracy: 0.6995\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8891 - accuracy: 0.6944 - val_loss: 0.8934 - val_accuracy: 0.7024\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8808 - accuracy: 0.6991 - val_loss: 0.9317 - val_accuracy: 0.6852\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8648 - accuracy: 0.7009 - val_loss: 0.8656 - val_accuracy: 0.7061\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8464 - accuracy: 0.7075 - val_loss: 0.9025 - val_accuracy: 0.6943\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8367 - accuracy: 0.7126 - val_loss: 0.8869 - val_accuracy: 0.7001\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8256 - accuracy: 0.7168 - val_loss: 0.8716 - val_accuracy: 0.6996\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8204 - accuracy: 0.7161 - val_loss: 0.8669 - val_accuracy: 0.7090\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8087 - accuracy: 0.7227 - val_loss: 0.8674 - val_accuracy: 0.7084\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8022 - accuracy: 0.7257 - val_loss: 0.8677 - val_accuracy: 0.7008\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7968 - accuracy: 0.7270 - val_loss: 0.8799 - val_accuracy: 0.7063\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7854 - accuracy: 0.7306 - val_loss: 0.8597 - val_accuracy: 0.7123\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7778 - accuracy: 0.7346 - val_loss: 0.8603 - val_accuracy: 0.7217\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7704 - accuracy: 0.7371 - val_loss: 0.8388 - val_accuracy: 0.7234\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7627 - accuracy: 0.7398 - val_loss: 0.8314 - val_accuracy: 0.7250\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7631 - accuracy: 0.7408 - val_loss: 0.8244 - val_accuracy: 0.7266\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7460 - accuracy: 0.7465 - val_loss: 0.8411 - val_accuracy: 0.7232\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7435 - accuracy: 0.7486 - val_loss: 0.8159 - val_accuracy: 0.7286\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7301 - accuracy: 0.7519 - val_loss: 0.8274 - val_accuracy: 0.7271\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7301 - accuracy: 0.7515 - val_loss: 0.8204 - val_accuracy: 0.7272\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7292 - accuracy: 0.7520 - val_loss: 0.8069 - val_accuracy: 0.7351\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7194 - accuracy: 0.7544 - val_loss: 0.8190 - val_accuracy: 0.7261\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7120 - accuracy: 0.7581 - val_loss: 0.8122 - val_accuracy: 0.7330\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7077 - accuracy: 0.7610 - val_loss: 0.8129 - val_accuracy: 0.7333\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7058 - accuracy: 0.7592 - val_loss: 0.8351 - val_accuracy: 0.7221\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7017 - accuracy: 0.7609 - val_loss: 0.8338 - val_accuracy: 0.7243\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6975 - accuracy: 0.7660 - val_loss: 0.8205 - val_accuracy: 0.7297\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6896 - accuracy: 0.7666 - val_loss: 0.8158 - val_accuracy: 0.7299\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6808 - accuracy: 0.7700 - val_loss: 0.8078 - val_accuracy: 0.7293\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6844 - accuracy: 0.7694 - val_loss: 0.8213 - val_accuracy: 0.7304\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6781 - accuracy: 0.7694 - val_loss: 0.8025 - val_accuracy: 0.7338\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6669 - accuracy: 0.7741 - val_loss: 0.8631 - val_accuracy: 0.7263\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6659 - accuracy: 0.7751 - val_loss: 0.8090 - val_accuracy: 0.7379\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6629 - accuracy: 0.7764 - val_loss: 0.7961 - val_accuracy: 0.7420\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6587 - accuracy: 0.7774 - val_loss: 0.7949 - val_accuracy: 0.7417\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6594 - accuracy: 0.7787 - val_loss: 0.8270 - val_accuracy: 0.7299\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6539 - accuracy: 0.7789 - val_loss: 0.8220 - val_accuracy: 0.7302\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6439 - accuracy: 0.7827 - val_loss: 0.8049 - val_accuracy: 0.7374\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6397 - accuracy: 0.7853 - val_loss: 0.8100 - val_accuracy: 0.7381\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6394 - accuracy: 0.7820 - val_loss: 0.8282 - val_accuracy: 0.7342\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6277 - accuracy: 0.7884 - val_loss: 0.8768 - val_accuracy: 0.7120\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6325 - accuracy: 0.7890 - val_loss: 0.7854 - val_accuracy: 0.7474\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6301 - accuracy: 0.7883 - val_loss: 0.7960 - val_accuracy: 0.7405\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6266 - accuracy: 0.7895 - val_loss: 0.8236 - val_accuracy: 0.7303\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6238 - accuracy: 0.7898 - val_loss: 0.8222 - val_accuracy: 0.7417\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6200 - accuracy: 0.7897 - val_loss: 0.8169 - val_accuracy: 0.7418\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6142 - accuracy: 0.7932 - val_loss: 0.7935 - val_accuracy: 0.7427\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6139 - accuracy: 0.7930 - val_loss: 0.8079 - val_accuracy: 0.7387\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6110 - accuracy: 0.7943 - val_loss: 0.7866 - val_accuracy: 0.7441\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6099 - accuracy: 0.7942 - val_loss: 0.8269 - val_accuracy: 0.7329\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6095 - accuracy: 0.7942 - val_loss: 0.8046 - val_accuracy: 0.7429\n",
      "Epoch 69/200\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7972Restoring model weights from the end of the best epoch: 59.\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6003 - accuracy: 0.7972 - val_loss: 0.8083 - val_accuracy: 0.7444\n",
      "Epoch 00069: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f03764c9550>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=200,\n",
    "    validation_data=(test_images, test_labels),callbacks = [callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108/3079180271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_ndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model below has the best performance. The architecture is displayed in the model summary, printed below. The loss and accuracy respectively after 125 epochs of training: [0.4184347093105316, 0.8980000019073486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 23s 0us/step\n",
      "170508288/170498071 [==============================] - 23s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 23:42:09.486964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:09.608330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:09.608695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:09.610681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:09.610980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:09.611202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:12.004642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:12.004957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:12.005206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1052] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-11 23:42:12.005412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14803 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n",
      "/tmp/ipykernel_108/4150165142.py:61: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 23:42:15.778942: I tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 25s 27ms/step - loss: 1.8845 - accuracy: 0.4288 - val_loss: 1.3569 - val_accuracy: 0.5843 - lr: 0.0010\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 1.2503 - accuracy: 0.5943 - val_loss: 1.1477 - val_accuracy: 0.6525 - lr: 0.0010\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 1.0745 - accuracy: 0.6534 - val_loss: 1.0836 - val_accuracy: 0.6858 - lr: 0.0010\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.9692 - accuracy: 0.6926 - val_loss: 0.9269 - val_accuracy: 0.7177 - lr: 0.0010\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.9017 - accuracy: 0.7184 - val_loss: 0.8568 - val_accuracy: 0.7516 - lr: 0.0010\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.8562 - accuracy: 0.7369 - val_loss: 0.7697 - val_accuracy: 0.7786 - lr: 0.0010\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.8225 - accuracy: 0.7503 - val_loss: 0.8681 - val_accuracy: 0.7557 - lr: 0.0010\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7920 - accuracy: 0.7631 - val_loss: 0.7064 - val_accuracy: 0.7949 - lr: 0.0010\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7758 - accuracy: 0.7719 - val_loss: 0.7718 - val_accuracy: 0.7787 - lr: 0.0010\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7591 - accuracy: 0.7776 - val_loss: 0.7457 - val_accuracy: 0.7916 - lr: 0.0010\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.7394 - accuracy: 0.7872 - val_loss: 0.7390 - val_accuracy: 0.7933 - lr: 0.0010\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7295 - accuracy: 0.7888 - val_loss: 0.7521 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.7173 - accuracy: 0.7964 - val_loss: 0.7539 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7103 - accuracy: 0.7994 - val_loss: 0.6988 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.7048 - accuracy: 0.8002 - val_loss: 0.6832 - val_accuracy: 0.8141 - lr: 0.0010\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6886 - accuracy: 0.8071 - val_loss: 0.9116 - val_accuracy: 0.7612 - lr: 0.0010\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6842 - accuracy: 0.8090 - val_loss: 0.6955 - val_accuracy: 0.8111 - lr: 0.0010\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6818 - accuracy: 0.8101 - val_loss: 0.6186 - val_accuracy: 0.8382 - lr: 0.0010\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6736 - accuracy: 0.8151 - val_loss: 0.6276 - val_accuracy: 0.8325 - lr: 0.0010\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6459 - accuracy: 0.8247 - val_loss: 0.6993 - val_accuracy: 0.8172 - lr: 0.0010\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6424 - accuracy: 0.8283 - val_loss: 0.6290 - val_accuracy: 0.8387 - lr: 0.0010\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6432 - accuracy: 0.8275 - val_loss: 0.6658 - val_accuracy: 0.8251 - lr: 0.0010\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6410 - accuracy: 0.8299 - val_loss: 0.6572 - val_accuracy: 0.8350 - lr: 0.0010\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6374 - accuracy: 0.8309 - val_loss: 0.6270 - val_accuracy: 0.8364 - lr: 0.0010\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.6334 - accuracy: 0.8323 - val_loss: 0.5997 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6291 - accuracy: 0.8324 - val_loss: 0.6267 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6294 - accuracy: 0.8340 - val_loss: 0.6044 - val_accuracy: 0.8518 - lr: 0.0010\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6242 - accuracy: 0.8377 - val_loss: 0.6405 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6245 - accuracy: 0.8333 - val_loss: 0.7453 - val_accuracy: 0.8103 - lr: 0.0010\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.6232 - accuracy: 0.8370 - val_loss: 0.6027 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Epoch 35/125\n",
      " 19/781 [..............................] - ETA: 19s - loss: 0.6088 - accuracy: 0.8470"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5970 - accuracy: 0.8469 - val_loss: 0.6410 - val_accuracy: 0.8396 - lr: 0.0010\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5963 - accuracy: 0.8477 - val_loss: 0.5766 - val_accuracy: 0.8603 - lr: 0.0010\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5999 - accuracy: 0.8478 - val_loss: 0.6174 - val_accuracy: 0.8562 - lr: 0.0010\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5968 - accuracy: 0.8465 - val_loss: 0.5802 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5928 - accuracy: 0.8493 - val_loss: 0.6162 - val_accuracy: 0.8499 - lr: 0.0010\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5925 - accuracy: 0.8487 - val_loss: 0.6452 - val_accuracy: 0.8453 - lr: 0.0010\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5908 - accuracy: 0.8520 - val_loss: 0.6019 - val_accuracy: 0.8507 - lr: 0.0010\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5881 - accuracy: 0.8490 - val_loss: 0.5774 - val_accuracy: 0.8603 - lr: 0.0010\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5909 - accuracy: 0.8474 - val_loss: 0.6207 - val_accuracy: 0.8514 - lr: 0.0010\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5864 - accuracy: 0.8506 - val_loss: 0.5702 - val_accuracy: 0.8638 - lr: 0.0010\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5861 - accuracy: 0.8514 - val_loss: 0.6484 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5830 - accuracy: 0.8528 - val_loss: 0.6398 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5872 - accuracy: 0.8529 - val_loss: 0.5842 - val_accuracy: 0.8593 - lr: 0.0010\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5850 - accuracy: 0.8520 - val_loss: 0.5519 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.5805 - accuracy: 0.8537 - val_loss: 0.6049 - val_accuracy: 0.8493 - lr: 0.0010\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5837 - accuracy: 0.8523 - val_loss: 0.6213 - val_accuracy: 0.8509 - lr: 0.0010\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5796 - accuracy: 0.8519 - val_loss: 0.6185 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5784 - accuracy: 0.8539 - val_loss: 0.5488 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5819 - accuracy: 0.8528 - val_loss: 0.5424 - val_accuracy: 0.8717 - lr: 0.0010\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5745 - accuracy: 0.8570 - val_loss: 0.5818 - val_accuracy: 0.8613 - lr: 0.0010\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5768 - accuracy: 0.8551 - val_loss: 0.5513 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5748 - accuracy: 0.8570 - val_loss: 0.6325 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5792 - accuracy: 0.8543 - val_loss: 0.5888 - val_accuracy: 0.8572 - lr: 0.0010\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5749 - accuracy: 0.8550 - val_loss: 0.5849 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5773 - accuracy: 0.8562 - val_loss: 0.5975 - val_accuracy: 0.8535 - lr: 0.0010\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5810 - accuracy: 0.8544 - val_loss: 0.7130 - val_accuracy: 0.8272 - lr: 0.0010\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5337 - accuracy: 0.8684 - val_loss: 0.5687 - val_accuracy: 0.8676 - lr: 5.0000e-04\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5161 - accuracy: 0.8728 - val_loss: 0.5610 - val_accuracy: 0.8654 - lr: 5.0000e-04\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.5045 - accuracy: 0.8743 - val_loss: 0.5061 - val_accuracy: 0.8828 - lr: 5.0000e-04\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.5007 - accuracy: 0.8752 - val_loss: 0.5024 - val_accuracy: 0.8820 - lr: 5.0000e-04\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4936 - accuracy: 0.8778 - val_loss: 0.5467 - val_accuracy: 0.8685 - lr: 5.0000e-04\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4909 - accuracy: 0.8790 - val_loss: 0.5160 - val_accuracy: 0.8774 - lr: 5.0000e-04\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4875 - accuracy: 0.8792 - val_loss: 0.5424 - val_accuracy: 0.8699 - lr: 5.0000e-04\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4830 - accuracy: 0.8800 - val_loss: 0.4950 - val_accuracy: 0.8828 - lr: 5.0000e-04\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4806 - accuracy: 0.8792 - val_loss: 0.4955 - val_accuracy: 0.8826 - lr: 5.0000e-04\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4790 - accuracy: 0.8800 - val_loss: 0.5253 - val_accuracy: 0.8727 - lr: 5.0000e-04\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4756 - accuracy: 0.8809 - val_loss: 0.4846 - val_accuracy: 0.8808 - lr: 5.0000e-04\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4731 - accuracy: 0.8803 - val_loss: 0.4542 - val_accuracy: 0.8913 - lr: 5.0000e-04\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4717 - accuracy: 0.8817 - val_loss: 0.5027 - val_accuracy: 0.8766 - lr: 5.0000e-04\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4758 - accuracy: 0.8790 - val_loss: 0.4779 - val_accuracy: 0.8832 - lr: 5.0000e-04\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4700 - accuracy: 0.8812 - val_loss: 0.5041 - val_accuracy: 0.8766 - lr: 5.0000e-04\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4674 - accuracy: 0.8821 - val_loss: 0.4994 - val_accuracy: 0.8755 - lr: 5.0000e-04\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4655 - accuracy: 0.8814 - val_loss: 0.5134 - val_accuracy: 0.8719 - lr: 5.0000e-04\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4613 - accuracy: 0.8830 - val_loss: 0.5302 - val_accuracy: 0.8704 - lr: 5.0000e-04\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4618 - accuracy: 0.8811 - val_loss: 0.5030 - val_accuracy: 0.8751 - lr: 5.0000e-04\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4596 - accuracy: 0.8833 - val_loss: 0.5254 - val_accuracy: 0.8708 - lr: 5.0000e-04\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4582 - accuracy: 0.8828 - val_loss: 0.5725 - val_accuracy: 0.8612 - lr: 5.0000e-04\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4607 - accuracy: 0.8822 - val_loss: 0.4820 - val_accuracy: 0.8803 - lr: 5.0000e-04\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4524 - accuracy: 0.8849 - val_loss: 0.4967 - val_accuracy: 0.8780 - lr: 5.0000e-04\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4550 - accuracy: 0.8832 - val_loss: 0.5037 - val_accuracy: 0.8775 - lr: 5.0000e-04\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4575 - accuracy: 0.8817 - val_loss: 0.4910 - val_accuracy: 0.8795 - lr: 5.0000e-04\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4361 - accuracy: 0.8885 - val_loss: 0.5263 - val_accuracy: 0.8713 - lr: 3.0000e-04\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4258 - accuracy: 0.8932 - val_loss: 0.4992 - val_accuracy: 0.8812 - lr: 3.0000e-04\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4210 - accuracy: 0.8945 - val_loss: 0.4608 - val_accuracy: 0.8871 - lr: 3.0000e-04\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4224 - accuracy: 0.8920 - val_loss: 0.4471 - val_accuracy: 0.8901 - lr: 3.0000e-04\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4176 - accuracy: 0.8947 - val_loss: 0.4792 - val_accuracy: 0.8829 - lr: 3.0000e-04\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4139 - accuracy: 0.8945 - val_loss: 0.4958 - val_accuracy: 0.8789 - lr: 3.0000e-04\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4120 - accuracy: 0.8943 - val_loss: 0.4556 - val_accuracy: 0.8918 - lr: 3.0000e-04\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4113 - accuracy: 0.8959 - val_loss: 0.4802 - val_accuracy: 0.8836 - lr: 3.0000e-04\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4113 - accuracy: 0.8944 - val_loss: 0.4641 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4099 - accuracy: 0.8950 - val_loss: 0.4767 - val_accuracy: 0.8791 - lr: 3.0000e-04\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4046 - accuracy: 0.8985 - val_loss: 0.4434 - val_accuracy: 0.8889 - lr: 3.0000e-04\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4051 - accuracy: 0.8957 - val_loss: 0.4863 - val_accuracy: 0.8781 - lr: 3.0000e-04\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.4054 - accuracy: 0.8958 - val_loss: 0.4478 - val_accuracy: 0.8887 - lr: 3.0000e-04\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4074 - accuracy: 0.8957 - val_loss: 0.4477 - val_accuracy: 0.8883 - lr: 3.0000e-04\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.4063 - accuracy: 0.8945 - val_loss: 0.4435 - val_accuracy: 0.8893 - lr: 3.0000e-04\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3974 - accuracy: 0.8969 - val_loss: 0.4820 - val_accuracy: 0.8799 - lr: 3.0000e-04\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3987 - accuracy: 0.8973 - val_loss: 0.4695 - val_accuracy: 0.8811 - lr: 3.0000e-04\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.4009 - accuracy: 0.8962 - val_loss: 0.4432 - val_accuracy: 0.8915 - lr: 3.0000e-04\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 19s 25ms/step - loss: 0.3977 - accuracy: 0.8969 - val_loss: 0.4642 - val_accuracy: 0.8844 - lr: 3.0000e-04\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3989 - accuracy: 0.8959 - val_loss: 0.4813 - val_accuracy: 0.8803 - lr: 3.0000e-04\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3964 - accuracy: 0.8972 - val_loss: 0.4596 - val_accuracy: 0.8849 - lr: 3.0000e-04\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 20s 25ms/step - loss: 0.3934 - accuracy: 0.8970 - val_loss: 0.4649 - val_accuracy: 0.8859 - lr: 3.0000e-04\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.3947 - accuracy: 0.8957 - val_loss: 0.5025 - val_accuracy: 0.8752 - lr: 3.0000e-04\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 20s 26ms/step - loss: 0.3926 - accuracy: 0.8979 - val_loss: 0.4184 - val_accuracy: 0.8980 - lr: 3.0000e-04\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4184 - accuracy: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4184347093105316, 0.8980000019073486]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    " \n",
    "\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = tf.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f8121feff40>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MachineLearning_Homework5_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
