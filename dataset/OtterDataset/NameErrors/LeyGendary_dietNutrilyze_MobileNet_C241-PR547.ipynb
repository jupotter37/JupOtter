{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11021 images belonging to 42 classes.\n",
      "Found 3147 images belonging to 42 classes.\n",
      "Epoch 1/20\n",
      "345/345 - 743s - loss: 0.5187 - accuracy: 0.8589 - val_loss: 0.2294 - val_accuracy: 0.9330 - 743s/epoch - 2s/step\n",
      "Epoch 2/20\n",
      "345/345 - 742s - loss: 0.2333 - accuracy: 0.9270 - val_loss: 0.2203 - val_accuracy: 0.9326 - 742s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "345/345 - 721s - loss: 0.1795 - accuracy: 0.9438 - val_loss: 0.2153 - val_accuracy: 0.9415 - 721s/epoch - 2s/step\n",
      "Epoch 4/20\n",
      "345/345 - 869s - loss: 0.1478 - accuracy: 0.9512 - val_loss: 0.2177 - val_accuracy: 0.9412 - 869s/epoch - 3s/step\n",
      "Epoch 5/20\n",
      "345/345 - 755s - loss: 0.1159 - accuracy: 0.9607 - val_loss: 0.1730 - val_accuracy: 0.9568 - 755s/epoch - 2s/step\n",
      "Epoch 6/20\n",
      "345/345 - 813s - loss: 0.1154 - accuracy: 0.9616 - val_loss: 0.1833 - val_accuracy: 0.9520 - 813s/epoch - 2s/step\n",
      "Epoch 7/20\n",
      "345/345 - 820s - loss: 0.1000 - accuracy: 0.9682 - val_loss: 0.1925 - val_accuracy: 0.9501 - 820s/epoch - 2s/step\n",
      "Epoch 8/20\n",
      "345/345 - 889s - loss: 0.0984 - accuracy: 0.9680 - val_loss: 0.2002 - val_accuracy: 0.9501 - 889s/epoch - 3s/step\n",
      "Epoch 9/20\n",
      "345/345 - 766s - loss: 0.0948 - accuracy: 0.9695 - val_loss: 0.2158 - val_accuracy: 0.9511 - 766s/epoch - 2s/step\n",
      "Epoch 10/20\n",
      "345/345 - 633s - loss: 0.0826 - accuracy: 0.9731 - val_loss: 0.1845 - val_accuracy: 0.9558 - 633s/epoch - 2s/step\n",
      "Epoch 11/20\n",
      "345/345 - 634s - loss: 0.0762 - accuracy: 0.9759 - val_loss: 0.1809 - val_accuracy: 0.9596 - 634s/epoch - 2s/step\n",
      "Epoch 12/20\n",
      "345/345 - 645s - loss: 0.0876 - accuracy: 0.9719 - val_loss: 0.1933 - val_accuracy: 0.9527 - 645s/epoch - 2s/step\n",
      "Epoch 13/20\n",
      "345/345 - 644s - loss: 0.0785 - accuracy: 0.9750 - val_loss: 0.1769 - val_accuracy: 0.9590 - 644s/epoch - 2s/step\n",
      "Epoch 14/20\n",
      "345/345 - 632s - loss: 0.0589 - accuracy: 0.9809 - val_loss: 0.1827 - val_accuracy: 0.9593 - 632s/epoch - 2s/step\n",
      "Epoch 15/20\n",
      "345/345 - 619s - loss: 0.0654 - accuracy: 0.9789 - val_loss: 0.2457 - val_accuracy: 0.9498 - 619s/epoch - 2s/step\n",
      "Epoch 16/20\n",
      "345/345 - 631s - loss: 0.0720 - accuracy: 0.9789 - val_loss: 0.1861 - val_accuracy: 0.9631 - 631s/epoch - 2s/step\n",
      "Epoch 17/20\n",
      "345/345 - 637s - loss: 0.0661 - accuracy: 0.9793 - val_loss: 0.2191 - val_accuracy: 0.9561 - 637s/epoch - 2s/step\n",
      "Epoch 18/20\n",
      "345/345 - 660s - loss: 0.0500 - accuracy: 0.9833 - val_loss: 0.2238 - val_accuracy: 0.9555 - 660s/epoch - 2s/step\n",
      "Epoch 19/20\n",
      "345/345 - 654s - loss: 0.0715 - accuracy: 0.9785 - val_loss: 0.2413 - val_accuracy: 0.9593 - 654s/epoch - 2s/step\n",
      "Epoch 20/20\n",
      "345/345 - 610s - loss: 0.0559 - accuracy: 0.9832 - val_loss: 0.2091 - val_accuracy: 0.9603 - 610s/epoch - 2s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Fine-tuning the entire model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-5\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     66\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     67\u001b[0m     train_generator,\n\u001b[0;32m     68\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,  \u001b[38;5;66;03m# Continue training for remaining epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     73\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Food Nutrilyze/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'Food Nutrilyze/val',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load Pretrained Model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Adding custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(42, activation='softmax')(x)  # Updated number of classes to 42\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tuning only top layers\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # Fewer epochs because transfer learning converges quickly\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fine-tuning the entire model\n",
    "base_model.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=80,  # Continue training for remaining epochs\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    verbose=2\n",
    ")\n",
    "                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11021 images belonging to 42 classes.\n",
      "Found 3147 images belonging to 42 classes.\n",
      "Epoch 1/20\n",
      "345/345 - 898s - loss: 0.5240 - accuracy: 0.8557 - val_loss: 0.2609 - val_accuracy: 0.9282 - 898s/epoch - 3s/step\n",
      "Epoch 2/20\n",
      "345/345 - 537s - loss: 0.2262 - accuracy: 0.9302 - val_loss: 0.2275 - val_accuracy: 0.9323 - 537s/epoch - 2s/step\n",
      "Epoch 3/20\n",
      "345/345 - 479s - loss: 0.1678 - accuracy: 0.9456 - val_loss: 0.1753 - val_accuracy: 0.9511 - 479s/epoch - 1s/step\n",
      "Epoch 4/20\n",
      "345/345 - 454s - loss: 0.1630 - accuracy: 0.9499 - val_loss: 0.1829 - val_accuracy: 0.9473 - 454s/epoch - 1s/step\n",
      "Epoch 5/20\n",
      "345/345 - 452s - loss: 0.1231 - accuracy: 0.9629 - val_loss: 0.1774 - val_accuracy: 0.9523 - 452s/epoch - 1s/step\n",
      "Epoch 6/20\n",
      "345/345 - 484s - loss: 0.1114 - accuracy: 0.9667 - val_loss: 0.1969 - val_accuracy: 0.9479 - 484s/epoch - 1s/step\n",
      "Epoch 7/20\n",
      "345/345 - 555s - loss: 0.0969 - accuracy: 0.9689 - val_loss: 0.1787 - val_accuracy: 0.9533 - 555s/epoch - 2s/step\n",
      "Epoch 8/20\n",
      "345/345 - 531s - loss: 0.0970 - accuracy: 0.9680 - val_loss: 0.1670 - val_accuracy: 0.9555 - 531s/epoch - 2s/step\n",
      "Epoch 9/20\n",
      "345/345 - 493s - loss: 0.0850 - accuracy: 0.9731 - val_loss: 0.1934 - val_accuracy: 0.9558 - 493s/epoch - 1s/step\n",
      "Epoch 10/20\n",
      "345/345 - 539s - loss: 0.0869 - accuracy: 0.9715 - val_loss: 0.1997 - val_accuracy: 0.9530 - 539s/epoch - 2s/step\n",
      "Epoch 11/20\n",
      "345/345 - 532s - loss: 0.0781 - accuracy: 0.9732 - val_loss: 0.2150 - val_accuracy: 0.9495 - 532s/epoch - 2s/step\n",
      "Epoch 12/20\n",
      "345/345 - 508s - loss: 0.0767 - accuracy: 0.9738 - val_loss: 0.2135 - val_accuracy: 0.9527 - 508s/epoch - 1s/step\n",
      "Epoch 13/20\n",
      "345/345 - 481s - loss: 0.0926 - accuracy: 0.9705 - val_loss: 0.2098 - val_accuracy: 0.9504 - 481s/epoch - 1s/step\n",
      "Epoch 14/20\n",
      "345/345 - 506s - loss: 0.0679 - accuracy: 0.9785 - val_loss: 0.1944 - val_accuracy: 0.9546 - 506s/epoch - 1s/step\n",
      "Epoch 15/20\n",
      "345/345 - 458s - loss: 0.0682 - accuracy: 0.9782 - val_loss: 0.2012 - val_accuracy: 0.9574 - 458s/epoch - 1s/step\n",
      "Epoch 16/20\n",
      "345/345 - 471s - loss: 0.0754 - accuracy: 0.9765 - val_loss: 0.2460 - val_accuracy: 0.9501 - 471s/epoch - 1s/step\n",
      "Epoch 17/20\n",
      "345/345 - 534s - loss: 0.0644 - accuracy: 0.9793 - val_loss: 0.2472 - val_accuracy: 0.9527 - 534s/epoch - 2s/step\n",
      "Epoch 18/20\n",
      "345/345 - 528s - loss: 0.0697 - accuracy: 0.9780 - val_loss: 0.2199 - val_accuracy: 0.9492 - 528s/epoch - 2s/step\n",
      "Epoch 19/20\n",
      "345/345 - 454s - loss: 0.0544 - accuracy: 0.9834 - val_loss: 0.1972 - val_accuracy: 0.9600 - 454s/epoch - 1s/step\n",
      "Epoch 20/20\n",
      "345/345 - 407s - loss: 0.0570 - accuracy: 0.9821 - val_loss: 0.2274 - val_accuracy: 0.9568 - 407s/epoch - 1s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Fine-tuning the entire model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-5\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     66\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     67\u001b[0m     train_generator,\n\u001b[0;32m     68\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,  \u001b[38;5;66;03m# Continue training for remaining epochs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     73\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Food Nutrilyze/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'Food Nutrilyze/val',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load Pretrained Model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Adding custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(42, activation='softmax')(x)  # Updated number of classes to 42\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tuning only top layers\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # Fewer epochs because transfer learning converges quickly\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fine-tuning the entire model\n",
    "base_model.trainable = True\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=80,  # Continue training for remaining epochs\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    verbose=2\n",
    ")\n",
    "                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model as an H5 file\n",
    "model.save('food_nutrilyze_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to tf js\n",
    "!pip install --upgrade tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Path to the model and output directory\n",
    "model_path = r'/content/fruit_classifier_model.h5'\n",
    "output_path = r'/content/output'\n",
    "\n",
    "# Construct the command\n",
    "command = f'tensorflowjs_converter --input_format keras {model_path} {output_path}'\n",
    "\n",
    "# Execute the command\n",
    "try:\n",
    "    subprocess.run(command, shell=True, check=True)\n",
    "    print(\"Konversi model TensorFlow ke TensorFlow.js berhasil.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Konversi model TensorFlow ke TensorFlow.js gagal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the folder you want to zip\n",
    "folder_to_zip = '/content/output'\n",
    "\n",
    "# Specify the output file path and name\n",
    "output_filename = '/content/capstone-C241-PR547'\n",
    "\n",
    "# Create the zip file\n",
    "shutil.make_archive(output_filename, 'zip', folder_to_zip)\n",
    "\n",
    "print(f\"Folder '{folder_to_zip}' has been zipped as '{output_filename}.zip'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arni",
   "language": "python",
   "name": "arni"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
