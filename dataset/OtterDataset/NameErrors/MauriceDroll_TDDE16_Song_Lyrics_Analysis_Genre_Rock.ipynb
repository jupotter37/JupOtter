{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c664878-4aea-49c8-bdcd-0c6a10e83c42",
   "metadata": {},
   "source": [
    "<h1> GENRE: POP </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58b5aa-e4aa-47ea-8de6-39736199bc11",
   "metadata": {},
   "source": [
    "<h3>(1) Access the data from the dataset </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36f509d-632c-432c-853f-e7dfee496973",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sebastiandizon/genius-song-lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19708af4",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/Subdatasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m subdataset_20 \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(decade_toplists[\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Save to hard drive\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43msubdataset_70\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Subdatasets/subdataset_70_rock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m subdataset_80\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Subdatasets/subdataset_80_rock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m subdataset_90\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Subdatasets/subdataset_90_rock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:1500\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[0;34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Path(dataset_path)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve() \u001b[38;5;129;01min\u001b[39;00m parent_cache_files_paths:\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\n\u001b[1;32m   1497\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to overwrite \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(dataset_path)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but a dataset can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt overwrite itself.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1498\u001b[0m         )\n\u001b[0;32m-> 1500\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Get json serializable state\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1504\u001b[0m     key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[key]\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1511\u001b[0m     ]\n\u001b[1;32m   1512\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/implementations/local.py:52\u001b[0m, in \u001b[0;36mLocalFileSystem.makedirs\u001b[0;34m(self, path, exist_ok)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakedirs\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Subdatasets'"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from datasets import Dataset\n",
    "type_of_music= 'rock'\n",
    "TOP_K = 1000\n",
    "decade_heaps = {70: [], 80: [], 90: [], 00: [], 10:[], 20:[]}  # z.B. 70=70er, 0=00er etc.\n",
    "\n",
    "def get_decade(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return 70\n",
    "    elif 1980 <= year <= 1989:\n",
    "        return 80\n",
    "    elif 1990 <= year <= 1999:\n",
    "        return 90\n",
    "    elif 2000 <= year <= 2009:\n",
    "        return 00\n",
    "    elif 2010 <= year <= 2019:\n",
    "        return 10\n",
    "    elif 2020 <= year <= 2029:\n",
    "        return 20\n",
    "    return None\n",
    "\n",
    "i = 0  # Zähler für tie-break\n",
    "for song in ds[\"train\"]:\n",
    "    if song[\"tag\"] != type_of_music:\n",
    "        continue\n",
    "    elif song[\"language\"] != \"en\":\n",
    "        continue\n",
    "    dec = get_decade(song[\"year\"])\n",
    "    if dec is None:\n",
    "        continue\n",
    "\n",
    "    views = song[\"views\"]\n",
    "    heap = decade_heaps[dec]\n",
    "\n",
    "    # Das Tupel enthält jetzt (views, i, song).\n",
    "    # Der zweite Wert i ist rein für Tie-Break (und garantiert sortierbar).\n",
    "    if len(heap) < TOP_K:\n",
    "        heapq.heappush(heap, (views, i, song))\n",
    "    else:\n",
    "        # Vergleiche nur, ob aktuelle views (oder Tie-Break i) größer sind \n",
    "        # als das Minimum im Heap (heap[0]).\n",
    "        if (views, i) > (heap[0][0], heap[0][1]):\n",
    "            heapq.heapreplace(heap, (views, i, song))\n",
    "    i += 1\n",
    "\n",
    "# Am Ende hat jede Dekade max. 1000 Elemente.\n",
    "decade_toplists = {}\n",
    "for dec, heap in decade_heaps.items():\n",
    "    # Sortieren nach (views, i) absteigend.\n",
    "    # Falls dir der Index egal ist, kannst du einfach nur nach views sortieren.\n",
    "    sorted_heap = sorted(heap, key=lambda x: (x[0], x[1]), reverse=True)\n",
    "    # x[0] = views, x[1] = tie-break index, x[2] = song\n",
    "    songs_only = [item[2] for item in sorted_heap]\n",
    "    decade_toplists[dec] = songs_only\n",
    "\n",
    "# Optional: zu HF Dataset konvertieren\n",
    "subdataset_70 = Dataset.from_list(decade_toplists[70])\n",
    "subdataset_80 = Dataset.from_list(decade_toplists[80])\n",
    "subdataset_90 = Dataset.from_list(decade_toplists[90])\n",
    "subdataset_00 = Dataset.from_list(decade_toplists[00])\n",
    "subdataset_10 = Dataset.from_list(decade_toplists[10])\n",
    "subdataset_20 = Dataset.from_list(decade_toplists[20])\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "# Save to hard drive\n",
    "subdataset_70.save_to_disk(path + \"/Subdatasets/subdataset_70_rock\")\n",
    "subdataset_80.save_to_disk(path + \"/Subdatasets/subdataset_80_rock\")\n",
    "subdataset_90.save_to_disk(path + \"/Subdatasets/subdataset_90_rock\")\n",
    "subdataset_00.save_to_disk(path + \"/Subdatasets/subdataset_00_rock\")\n",
    "subdataset_10.save_to_disk(path + \"/Subdatasets/subdataset_10_rock\")\n",
    "subdataset_20.save_to_disk(path + \"/Subdatasets/subdataset_20_rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b8c7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c471fac1f7654989b76427ea58f1fab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46ff212a75945eeb5f48277f93ea7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ae4a53947d441fa5fc74adde7e83c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0f2db77a104b1b9692bbc99f57a6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192d2b2c0aca4998b8db5348eadb89e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88284ff1e9f54b6984dd59cd0830bda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "# Save to hard drive\n",
    "subdataset_70.save_to_disk(path + \"/Subdatasets/rock/subdataset_70_rock\")\n",
    "subdataset_80.save_to_disk(path + \"/Subdatasets/rock/subdataset_80_rock\")\n",
    "subdataset_90.save_to_disk(path + \"/Subdatasets/rock/subdataset_90_rock\")\n",
    "subdataset_00.save_to_disk(path + \"/Subdatasets/rock/subdataset_00_rock\")\n",
    "subdataset_10.save_to_disk(path + \"/Subdatasets/rock/subdataset_10_rock\")\n",
    "subdataset_20.save_to_disk(path + \"/Subdatasets/rock/subdataset_20_rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca33b8-ff6c-4ab3-aae9-8de32db0f00e",
   "metadata": {},
   "source": [
    "<h2> LDA </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c1930-379d-4c13-8378-0c7e31382765",
   "metadata": {},
   "source": [
    "<h2> Remove the bad words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0155b7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subdataset_00' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubdataset_00\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subdataset_00' is not defined"
     ]
    }
   ],
   "source": [
    "subdataset_00[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc19c9ee-4499-49cb-ba77-32ff6fa3fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "#file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "with open(swear_words_file, 'r') as f:\n",
    "    swear_words = set(f.read().splitlines())\n",
    "\n",
    "print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "#print(swear_words)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecd6a07-2735-4e22-8936-6201e2f4454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {\"weren't\", 'both', 'bridge', 'don', 'won', 've', \"you'd\", 'he', 'him', 'been', \"hadn't\", 'weren', 'once', 'doesn', 'other', \"you'll\", 'wasn', 't', 'have', 'until', 'when', 'will', 'through', \"don't\", 'o', 'didn', 'wouldn', 'chorus', 'aren', 'below', 'nor', 'about', 'needn', 'not', \"isn't\", \"mightn't\", 'an', 'get', 'because', 'shouldn', 'being', 'like', 'down', 'they', 'than', 'myself', 'own', 'yeah', 'haven', 'ours', 'shan', 'under', 'oh', 'all', 'can', 'y', \"that'll\", \"aren't\", 'na', 'verse', 'its', 'where', 'at', 'my', 'more', 'same', 'very', \"needn't\", 'are', 'for', 'intro', 'between', 'most', 'm', \"shouldn't\", 'couldn', \"couldn't\", 'why', 'themselves', 'from', \"shan't\", \"wouldn't\", 'but', 'yourselves', 'before', 'did', 'a', 'his', \"hasn't\", 'got', \"you've\", 'how', 'some', 'yo', 'la', 'outro', 'who', 'such', 'to', 'those', 'does', \"you're\", 'during', 'ain', 'you', \"mustn't\", \"haven't\", 'whom', 'having', \"wasn't\", 'were', 'only', 'be', 'then', \"didn't\", 'am', 'after', 'd', 'off', 'out', 'too', 'by', 'there', 'into', 'll', 'yours', 'itself', 'them', 'these', 'if', 'theirs', 'was', 'just', 'mightn', 'the', 'hadn', \"should've\", 'isn', 'which', 'this', 'hasn', 'i', 'our', 'she', 'me', 'so', 'ma', 'himself', 'yourself', 'herself', 'it', 'any', 'or', 'against', 'no', 'had', 'we', 'should', 'mustn', 'with', 'each', 'and', 'has', 'your', 'as', 'further', \"it's\", \"doesn't\", 'here', 'do', 'is', 'of', 'above', 's', 'again', 'few', \"she's\", \"won't\", 'hook', 'hers', 'in', 'that', 'their', 'over', 'on', 'now', 'up', 'while', 'ourselves', 're', 'her', 'what', 'doing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import gensim\n",
    "# import re\n",
    "# from gensim.corpora.dictionary import Dictionary\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# # Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Beispiel-Liste von Strings\n",
    "\n",
    "\n",
    "# # Stopwords definieren\n",
    "# stop_words = set(stopwords.words('english')+[\"like\", \"oh\", \"na\", \"la\", \"yo\", \"get\", \"yeah\", \"got\",\"verse\", \"chorus\", \"hook\", \"bridge\", \"outro\", \"intro\"])\n",
    "# print(\"Stopwords:\", stop_words)\n",
    "# # Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "# def preprocess(doc):\n",
    "#     doc = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", doc) # delete unncessary things\n",
    "#     #doc = remove_stopwords(doc.lower())\n",
    "#     doc = remove_repeated_lines(doc)\n",
    "    \n",
    "#     tokens = word_tokenize(doc.lower())  # Kleinbuchstaben und Tokenisierung\n",
    "    \n",
    "#     tokens = [word for word in tokens if word not in stop_words]  # Stopwords entfernen\n",
    "    \n",
    "#     return tokens\n",
    "\n",
    "# def remove_repeated_lines(doc):\n",
    "#     lines = doc.split(\"\\n\")\n",
    "#     return \"\\n\".join(sorted(set(lines), key=lines.index))\n",
    "\n",
    "# def sort_out_bad_words(doc,word_list, replacement=\" \"):\n",
    "#     \"\"\"\n",
    "#     Ersetzt Wörter in einem Text, wenn sie in einer Liste vorkommen.\n",
    "    \n",
    "#     :param text: Der Eingabetext (String).\n",
    "#     :param word_list: Liste der Wörter, die ersetzt werden sollen.\n",
    "#     :param replacement: Der Text, durch den die Wörter ersetzt werden sollen.\n",
    "#     :return: Der bearbeitete Text.\n",
    "#     \"\"\"\n",
    "#     # Erstelle ein Set für einen schnelleren Lookup\n",
    "#     word_set = set(word_list)\n",
    "    \n",
    "#     # Splitte den Text in Wörter\n",
    "#     words = doc.split()\n",
    "    \n",
    "#     # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "#     replaced_words = [replacement if word.lower() in word_set else word for word in words]\n",
    "    \n",
    "#     # Füge den Text wieder zusammen\n",
    "#     return \" \".join(replaced_words)\n",
    " \n",
    "\n",
    "# def process_subdatasets(subdataset):\n",
    "#     # Get rid of the bad words and replace it with \"swear word\"\n",
    "#     without_swear = [sort_out_bad_words(doc, swear_words ) for doc in subdataset]\n",
    "\n",
    "#     # Preprocessing für alle Dokumente anwenden\n",
    "#     processed_docs = [preprocess(doc) for doc in without_swear]\n",
    "\n",
    "#     if stop_words in processed_docs:\n",
    "#         print(\"FEHLER: Stopwords nicht entfernt!\")\n",
    "        \n",
    "#     # Erstellen eines Wörterbuchs\n",
    "#     dictionary = Dictionary(processed_docs)\n",
    "\n",
    "#     # Erstellen eines Bag-of-Words-Korpus\n",
    "#     corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "#     # LDA-Modell trainieren\n",
    "#     lda_model = gensim.models.LdaModel(\n",
    "#         corpus=corpus,\n",
    "#         id2word=dictionary,\n",
    "#         num_topics=5,  # Anzahl der Themen\n",
    "#         random_state=42,\n",
    "#         passes=40,  # Anzahl der Durchgänge\n",
    "#         alpha='symmetric',\n",
    "#         eta='auto'\n",
    "#         #alpha='auto'  # Dirichlet-Hyperparameter Entscheidung zwischen Auto und symetric\n",
    "#     )\n",
    "    \n",
    "#     id2word = dictionary\n",
    "#     dict_topic = {}\n",
    "#     # Themen anzeigen\n",
    "#     #print(\"Themen und ihre Schlüsselwörter:\")\n",
    "#     for idx, topic in lda_model.print_topics(num_words=5):\n",
    "#         #print(f\"Topic {idx}: {topic}\")\n",
    "#         dict_topic[idx] = topic \n",
    "\n",
    "#     return dict_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd87712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {'bridge', 'so', \"needn't\", 'but', 'his', 'few', 'get', 'won', \"hasn't\", \"it's\", 'against', 'now', \"you'd\", 'yo', 'couldn', 'didn', 'they', 'mightn', 'the', 'or', 'aren', 'because', \"won't\", 've', 'mustn', 'under', 'in', 'having', 'why', 'were', 'not', 'and', 're', 'themselves', 'there', 'when', 't', 'which', 'its', 'before', 'are', 'here', 'just', 'ourselves', 'of', \"didn't\", 'we', 'once', 'did', 'being', 'she', 'up', 'is', 'very', 'wouldn', 'too', 'hook', 'again', 'during', 'her', \"shan't\", 'don', \"wouldn't\", 'your', 'any', 'hadn', 'be', 'intro', 's', 'isn', 'can', 'other', 'all', \"hadn't\", 'like', 'nor', \"you've\", 'above', 'yours', 'd', 'off', \"isn't\", 'with', \"haven't\", 'than', 'hasn', 'from', 'chorus', 'each', 'on', 'wasn', 'na', 'those', 'needn', 'to', 'am', \"weren't\", 'down', 'how', 'himself', 'll', 'haven', \"wasn't\", 'yourself', 'herself', 'own', \"mightn't\", 'ours', 'myself', 'at', 'most', 'o', 'outro', 'our', 'him', 'no', 'hers', 'over', 'ain', 'shan', 'has', 'further', 'such', 'ma', \"aren't\", 'below', \"you're\", 'he', 'these', \"she's\", 'through', 'been', 'what', 'their', 'it', 'whom', 'y', 'should', 'that', 'after', 'm', 'yeah', 'for', \"mustn't\", 'weren', 'some', 'them', 'was', 'a', 'do', 'as', 'both', 'doesn', \"shouldn't\", 'got', 'have', \"that'll\", 'out', \"doesn't\", 'i', 'while', 'until', 'by', 'my', 'theirs', 'will', 'shouldn', 'verse', 'itself', 'oh', 'then', 'into', 'same', 'if', \"couldn't\", 'only', \"should've\", 'who', 'me', 'more', 'yourselves', 'about', 'does', \"don't\", 'where', 'la', 'this', \"you'll\", 'you', 'an', 'between', 'had', 'doing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "used_swear_words = []\n",
    "# clean the text\n",
    "import re\n",
    "\n",
    "\n",
    "# Stopwords definieren\n",
    "stop_words = set(stopwords.words('english')+[\"like\", \"oh\", \"na\", \"la\", \"yo\", \"get\", \"yeah\", \"got\",\"verse\", \"chorus\", \"hook\", \"bridge\", \"outro\", \"intro\"])\n",
    "print(\"Stopwords:\", stop_words)\n",
    "\n",
    "def clean_text(document_list):\n",
    "    \n",
    "    cleaned_document_list = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        # Delete HTML-Tags\n",
    "        text = re.sub(r\"<.*?>\", \"\", single_doc)\n",
    "        # Delete Sonderzeichen und Zahlen\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        # Delete uncessary whitespaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # lower case the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        cleaned_document_list.append(text)\n",
    "    \n",
    "    return cleaned_document_list\n",
    "\n",
    "\n",
    "def lemmatize_docs(subdataset):\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    lemmatized_document_list = []\n",
    "    lemmatized = []\n",
    "    \n",
    "    for single_doc in subdataset:\n",
    "        nlp_doc = nlp(single_doc)\n",
    "\n",
    "        for token in nlp_doc:\n",
    "        # Nur Tokens behalten, die:\n",
    "        # - kein Stopwort sind\n",
    "        # - kein Satzzeichen sind\n",
    "        # - eine gewisse Länge haben (z. B. > 1 Zeichen)\n",
    "            if not token.is_stop and not token.is_punct and len(token.text.strip()) > 1:\n",
    "                lemmatized.append(token.lemma_.lower())\n",
    "        \n",
    "        \n",
    "        lemmatized_document_list.append(lemmatized)\n",
    "    \n",
    "    return lemmatized_document_list\n",
    "    \n",
    "\n",
    "def get_swear_words():\n",
    "\n",
    "    path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "    path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "    #file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "    with open(swear_words_file, 'r') as f:\n",
    "        swear_words = set(f.read().splitlines())\n",
    "\n",
    "    print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "    return swear_words\n",
    "\n",
    "def get_manual_stop_words():\n",
    "    \n",
    "    path = '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Manually_Banned_Words.txt'\n",
    "    path_manually_stop_words = '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/stopwords_en.txt'\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text_banned = f.read()\n",
    "    \n",
    "    with open(path_manually_stop_words, 'r', encoding='utf-8') as f:\n",
    "        text_stopwords = f.read()\n",
    "        \n",
    "    banned_words = set(text_banned.split())\n",
    "    stop_words = set(text_stopwords.split())\n",
    "    \n",
    "    return banned_words.union(stop_words)\n",
    "\n",
    "# Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "def preprocess_stop_words(document_list):\n",
    "    # Stopwords definieren\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))  # Standard-Stopwords\n",
    "    #additional_words = {\"like\", \"oh\", \"na\", \"la\", \"yo\", \"you\", \"get\", \"yeah\" \n",
    "    #                       \"the\", \"and\", \"of\", \"on\", \"[Intro]\", \"[Storyteller]\"}\n",
    "    \n",
    "    # import the manual files of stop words\n",
    "    additional_words = get_manual_stop_words()\n",
    "    swear_words = get_swear_words()\n",
    "      \n",
    "    word_set = stop_words.union(additional_words)\n",
    "    word_set = word_set.union(swear_words)\n",
    "        \n",
    "    replacement = \"\"\n",
    "    list_of_tokens = []\n",
    "    for single_doc in document_list:\n",
    "\n",
    "        words = single_doc.split()\n",
    "        \n",
    "        # replace word if not in the list\n",
    "        replaced_words = []\n",
    "        for word in words:\n",
    "            if word.lower() in word_set:\n",
    "                replaced_words.append(replacement)  \n",
    "                \n",
    "                if word.lower() in swear_words:\n",
    "                    used_swear_words.append(word) # save word to the used swear words list\n",
    "            else:\n",
    "                replaced_words.append(word)  \n",
    "                \n",
    "        list_of_tokens.append(str(\" \".join(replaced_words)))\n",
    "              \n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "\n",
    "    unique_lines = []\n",
    "    seen = set()\n",
    "    for line in doc:\n",
    "        if line not in seen:\n",
    "            unique_lines.append(line)\n",
    "            seen.add(line)\n",
    "    # Am Ende kannst du wieder eine Liste zurückgeben:\n",
    "    return unique_lines\n",
    "\n",
    "def tokenize_all_docs(document_list):\n",
    "    list_of_tokens = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        \n",
    "        if type(single_doc) == str:\n",
    "            tokens = word_tokenize(single_doc)\n",
    "            list_of_tokens.append(tokens)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def get_used_swear_words():\n",
    "    return used_swear_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", doc) # delete unncessary things\n",
    "    #doc = remove_stopwords(doc.lower())\n",
    "    doc = remove_repeated_lines(doc)\n",
    "    \n",
    "    tokens = word_tokenize(doc.lower())  # Kleinbuchstaben und Tokenisierung\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stopwords entfernen\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "    lines = doc.split(\"\\n\")\n",
    "    return \"\\n\".join(sorted(set(lines), key=lines.index))\n",
    "\n",
    "def sort_out_bad_words(doc,word_list, replacement=\" \"):\n",
    "\n",
    "    # Erstelle ein Set für einen schnelleren Lookup\n",
    "    word_set = set(word_list)\n",
    "    \n",
    "    # Splitte den Text in Wörter\n",
    "    words = doc.split()\n",
    "    \n",
    "    # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "    replaced_words = [replacement if word.lower() in word_set else word for word in words]\n",
    "    \n",
    "    # Füge den Text wieder zusammen\n",
    "    return \" \".join(replaced_words)\n",
    " \n",
    "\n",
    "def process_subdatasets(subdataset):\n",
    "    # Get rid of the bad words and replace it with \"swear word\"\n",
    "    #without_swear = [sort_out_bad_words(doc, swear_words ) for doc in subdataset]\n",
    "\n",
    "    # Preprocessing für alle Dokumente anwenden\n",
    "    #processed_docs = [preprocess(doc) for doc in without_swear]\n",
    "\n",
    "    cleaned_dataset_no_strange_characters = clean_text(subdataset)\n",
    "\n",
    "    cleaned_dataset_no_swears = preprocess_stop_words(cleaned_dataset_no_strange_characters)\n",
    "\n",
    "    lemmatized_dataset = lemmatize_docs(cleaned_dataset_no_swears)\n",
    "    \n",
    "    #tokenized_dataset = tokenize_all_docs(lemmatized_dataset)\n",
    "\n",
    "\n",
    "    dictionary = Dictionary(lemmatized_dataset)\n",
    "    \n",
    "    # Erstellen eines Bag-of-Words-Korpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in lemmatized_dataset]\n",
    "\n",
    "    # LDA-Modell trainieren\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=5,  # Anzahl der Themen\n",
    "        random_state=42,\n",
    "        passes=40,  # Anzahl der Durchgänge\n",
    "        alpha='symmetric',\n",
    "        eta='auto'\n",
    "        #alpha='auto'  # Dirichlet-Hyperparameter Entscheidung zwischen Auto und symetric\n",
    "    )\n",
    "    \n",
    "    id2word = dictionary\n",
    "    dict_topic = {}\n",
    "    # Themen anzeigen\n",
    "    #print(\"Themen und ihre Schlüsselwörter:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5):\n",
    "\n",
    "        dict_topic[idx] = topic \n",
    "\n",
    "    return dict_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750121e6-67ce-4d46-bfb2-08910881b4c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rock/Analysis_Folder/LDA/topics_rock_LDA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Datei schreiben\u001b[39;00m\n\u001b[1;32m     40\u001b[0m output_file \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis_Folder/LDA/topics_rock_LDA.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     42\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecade\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     43\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()  \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rock/Analysis_Folder/LDA/topics_rock_LDA.csv'"
     ]
    }
   ],
   "source": [
    "amount = 1000\n",
    "from datasets import load_from_disk\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rock/\"\n",
    "\n",
    "\n",
    "rock_70_lyrics = process_subdatasets(load_from_disk(path + 'subdataset_70_rock')['lyrics'][:amount])\n",
    "rock_80_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_80_rock\")['lyrics'][:amount])\n",
    "rock_90_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_90_rock\")['lyrics'][:amount])\n",
    "rock_00_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_00_rock\")['lyrics'][:amount])\n",
    "rock_10_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_10_rock\")['lyrics'][:amount])\n",
    "rock_20_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_20_rock\")['lyrics'][:amount])\n",
    "\n",
    "list_of_lyrics = {\n",
    "    \"70s\": rock_70_lyrics,\n",
    "    \"80s\": rock_80_lyrics,\n",
    "    \"90s\": rock_90_lyrics,\n",
    "    \"00s\": rock_00_lyrics,\n",
    "    \"10s\": rock_10_lyrics,\n",
    "    \"20s\": rock_20_lyrics\n",
    "}\n",
    "\n",
    "import csv\n",
    "\n",
    "data_for_csv = []\n",
    "\n",
    "for decade_name, decade in list_of_lyrics.items():\n",
    "    i = 0\n",
    "    for topics in decade:\n",
    "        i = i + 1\n",
    "        data_for_csv.append({\n",
    "            \"Method\": \"LDA\",\n",
    "            \"Genre\": \"Rock\",\n",
    "            \"Decade\": decade_name,\n",
    "            \"Topic\": str(decade[topics])\n",
    "        })\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "# Datei schreiben\n",
    "output_file = path + \"Analysis_Folder/LDA/topics_rock_LDA.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  \n",
    "    writer.writerows(data_for_csv) \n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d17fedf-8bf3-4661-b436-4ee4b3361548",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "output_file = path + \"Analysis_Folder/topics_rock_LDA.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  \n",
    "    writer.writerows(data_for_csv) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837921f6",
   "metadata": {},
   "source": [
    "<h3>BiGrams </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fff58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Token\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "def calculate_with_bigrams(subdataset):\n",
    "    # Phrases braucht eine Liste von tokenisierten Sätzen\n",
    "    # mindestens in 2 Dokuemnten min_count=2\n",
    "    # threshold= strikt, wie oft ein bigram vorkommen muss (höher)\n",
    "    \n",
    "    cleaned_dataset_no_strange_characters = clean_text(subdataset)\n",
    "\n",
    "    cleaned_dataset_no_swears = preprocess_stop_words(cleaned_dataset_no_strange_characters)\n",
    "\n",
    "    lemmatized_dataset = lemmatize_docs(cleaned_dataset_no_swears)\n",
    "    \n",
    "    #tokenized_dataset = tokenize_all_docs(lemmatized_dataset)\n",
    "    tokenized_dataset=lemmatized_dataset\n",
    "    \n",
    "    bigram = Phrases(tokenized_dataset, min_count=1, threshold=1)\n",
    "\n",
    "\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "\n",
    "    tokenized_corpus_bigrams = [bigram_phraser[sent] for sent in tokenized_dataset]\n",
    "    #for sent in tokenized_corpus_bigrams[:5]:\n",
    "    #    print(\"Nach Bigram-Phraser:\", sent)\n",
    "    #    print(\"-----\")\n",
    "\n",
    "\n",
    "    # 1) Dictionary\n",
    "    dictionary = Dictionary(tokenized_corpus_bigrams)\n",
    "\n",
    "    # 2) Bag-of-Words\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in tokenized_corpus_bigrams]\n",
    "\n",
    "    # 3) LDA trainieren\n",
    "    lda_model = LdaModel(corpus=corpus_bow,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=2,  # Beispiel, wähle was passt\n",
    "                        passes=10,     # Mehrfache Durchläufe\n",
    "                        random_state=42)\n",
    "\n",
    "    # Ergebnis inspizieren\n",
    "    #for idx, topic in lda_model.show_topics(formatted=False, num_words=5):\n",
    "    #    print(f\"Topic {idx}: {[word for word, _ in topic]}\")\n",
    "        \n",
    "    dict_topic = {}\n",
    "    for idx, topic in lda_model.print_topics(num_words=10):\n",
    "\n",
    "        dict_topic[idx] = topic \n",
    "\n",
    "    return dict_topic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62db3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Completly Done\n",
      "CSV-Datei '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/LDA_BiGrams/topics_rock_LDA_1000_11_1.csv' wurde erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "amount = 1000\n",
    "from datasets import load_from_disk\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rock/\"\n",
    "\n",
    "\n",
    "rap_70_lyrics = calculate_with_bigrams(load_from_disk(path + 'subdataset_70_rock')['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_80_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_80_rock\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_90_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_90_rock\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_00_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_00_rock\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_10_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_10_rock\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_20_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_20_rock\")['lyrics'][:amount])\n",
    "print(\"Completly Done\")\n",
    "\n",
    "list_of_lyrics = {\n",
    "    \"70s\": rap_70_lyrics,\n",
    "    \"80s\": rap_80_lyrics,\n",
    "    \"90s\": rap_90_lyrics,\n",
    "    \"00s\": rap_00_lyrics,\n",
    "    \"10s\": rap_10_lyrics,\n",
    "    \"20s\": rap_20_lyrics\n",
    "}\n",
    "\n",
    "import csv\n",
    "\n",
    "data_for_csv = []\n",
    "\n",
    "for decade_name, decade in list_of_lyrics.items():\n",
    "    i = 0\n",
    "    for topics in decade:\n",
    "        i = i + 1\n",
    "        data_for_csv.append({\n",
    "            \"Method\": \"BiGrams\",\n",
    "            \"Genre\": \"Rock\",\n",
    "            \"Decade\": decade_name,\n",
    "            \"Topic\": str(decade[topics])\n",
    "        })\n",
    "        \n",
    "\n",
    "\n",
    "# Datei schreiben\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/LDA_BiGrams/\"\n",
    "\n",
    "output_file = path + \"topics_rock_LDA_1000_11_1.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  \n",
    "    writer.writerows(data_for_csv) \n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599b719-6c7e-4a39-a10d-9afa5bf73fc6",
   "metadata": {},
   "source": [
    "# tipps von ChaGPT\n",
    "https://chatgpt.com/c/676f1ffc-b134-8012-9f03-6bd466bda148\n",
    "\n",
    "- Herausfinden, wieviele Topics am geeingesten sind\n",
    "- warum sind weiterhin so viele Stop words enthalten\n",
    "- wieviele Lieder nehm ich überhaupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2b480-22d5-4cf4-80e5-a80de53db53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subdataset_80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c6e5b-0527-44eb-9dc8-8f963f035875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(subdataset_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41e834-cb3b-42fc-8321-42e4b9050621",
   "metadata": {},
   "source": [
    "<h1> BERT TOPIC MODELLING </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94fb4e4f-0e6c-4320-8c6a-6ed5dba60686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# clean the text\n",
    "import re\n",
    "\n",
    "def clean_text(document_list):\n",
    "    \n",
    "    cleaned_document_list = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        # Delete HTML-Tags\n",
    "        text = re.sub(r\"<.*?>\", \"\", single_doc)\n",
    "        # Delete Sonderzeichen und Zahlen\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        # Delete uncessary whitespaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # lower case the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        cleaned_document_list.append(text)\n",
    "    \n",
    "    return cleaned_document_list\n",
    "\n",
    "def get_swear_words():\n",
    "\n",
    "    path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "    path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "    #file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "    with open(swear_words_file, 'r') as f:\n",
    "        swear_words = set(f.read().splitlines())\n",
    "\n",
    "    print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "    return swear_words\n",
    "\n",
    "# Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "def preprocess_stop_words(document_list):\n",
    "    # Stopwords definieren\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))  # Standard-Stopwords\n",
    "    additional_words = {\"like\", \"oh\", \"na\", \"la\", \"yo\", \"you\", \"get\", \n",
    "                            \"the\", \"and\", \"of\", \"on\", \"[Intro]\", \"[Storyteller]\"}\n",
    "    \n",
    "    swear_words = get_swear_words()\n",
    "      \n",
    "    word_set = stop_words.union(additional_words)\n",
    "    word_set = word_set.union(swear_words)\n",
    "        \n",
    "    replacement = \"\"\n",
    "    list_of_tokens = []\n",
    "    for single_doc in document_list:\n",
    "        # Kombiniere Standard-Stopwords mit zusätzlichen Wörtern\n",
    "\n",
    "        # Splitte den Text in Wörter\n",
    "        words = single_doc.split()\n",
    "        \n",
    "        # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "        replaced_words = []\n",
    "        for word in words:\n",
    "            if word.lower() in word_set:\n",
    "                replaced_words.append(replacement)  # Ersetze das Wort\n",
    "            else:\n",
    "                replaced_words.append(word)  # Behalte das Wort\n",
    "        \n",
    "        # Füge den Text wieder zusammen\n",
    "        list_of_tokens.append(str(\" \".join(replaced_words)))\n",
    "              \n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "    # doc ist eine Liste von Strings, z.B. [\"Zeile1\", \"Zeile2\", ...]\n",
    "    # Wiederholte Zeilen entfernen und Reihenfolge beibehalten\n",
    "    unique_lines = []\n",
    "    seen = set()\n",
    "    for line in doc:\n",
    "        if line not in seen:\n",
    "            unique_lines.append(line)\n",
    "            seen.add(line)\n",
    "    # Am Ende kannst du wieder eine Liste zurückgeben:\n",
    "    return unique_lines\n",
    "\n",
    "def tokenize_all_docs(document_list):\n",
    "    list_of_tokens = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        \n",
    "        if type(single_doc) == str:\n",
    "            tokens = word_tokenize(single_doc)\n",
    "            list_of_tokens.append(tokens)\n",
    "        else:\n",
    "            print(\"Type von Document\", type(single_doc))\n",
    "            continue\n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6970a026-c9cf-4da3-bc7d-8368dfec0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "\n",
    "\n",
    "def processBertTopics(dataset,sentenceTransformer):\n",
    "    i = 0\n",
    "    # Initialize your topic model\n",
    "    #embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    embedding_model = SentenceTransformer(sentenceTransformer)\n",
    "    topic_model = BERTopic(embedding_model=embedding_model)\n",
    "    # Fit and transform the text data\n",
    "    #without_repatition = remove_repeated_lines(dataset)\n",
    "    \n",
    "    # clean dataset from HTML-Tags, special characters and numbers\n",
    "    cleaned_song_lyrics = clean_text(dataset)\n",
    "    \n",
    "    # Get rid of the bad words and replace it with NOTHING\n",
    "    swear_words = set(stopwords.words('english'))\n",
    "    #without_swear = [sort_out_bad_words(doc) for doc in cleaned_song_lyrics]\n",
    "    \n",
    "    #Deleting the stopwords\n",
    "    without_stop_words = preprocess_stop_words(cleaned_song_lyrics)\n",
    "    \n",
    "    tokenized_docs = tokenize_all_docs(without_stop_words)\n",
    "    \n",
    "    # Convert 2 strings\n",
    "    tokenized_docs = [\" \".join(tokens) for tokens in tokenized_docs]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokenized_docs = [doc for doc in tokenized_docs if doc]\n",
    "    \n",
    "    # Remove documents with less than 3 words    \n",
    "    #tokenized_docs = [doc for doc in tokenized_docs if len(doc.split()) > 3]\n",
    "    #print(\"tokenized_docs\", tokenized_docs)\n",
    "\n",
    "    if tokenized_docs == [] or len(tokenized_docs) == 0:\n",
    "        i = i+1\n",
    "    else: \n",
    "        topics, probs = topic_model.fit_transform(tokenized_docs)\n",
    "\n",
    "    # Ergebnisse\n",
    "    #print(\"Themenzuweisung:\", topics)\n",
    "    #print(\"Wahrscheinlichkeiten:\", probs)\n",
    "\n",
    "    str_topic_with_probs = []\n",
    "    #for t, p in zip(topics, probs):\n",
    "    #    str_topic_with_probs.append(str(t) + \": \" + str(p))\n",
    "\n",
    "    print(i, \"documents were empty!\")   \n",
    "    return topic_model.get_topics()\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6ae7c-546b-4eb3-8f54-d43536abfbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subdataset_90' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Beispiel: Anzahl der Texte, die verarbeitet werden sollen\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# INPUT TWEET IST BEREITS NORMALISIERT!\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenisierung und Padding/Truncation\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m lyrics \u001b[38;5;241m=\u001b[39m \u001b[43msubdataset_90\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m][:amount]\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(lyrics, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Verarbeitung der Eingabe durch das Modell\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subdataset_90' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# # Modell und Tokenizer laden\n",
    "# bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "# # Anzahl der Songtexte, die verarbeitet werden sollen\n",
    "# amount = 10  # Beispiel: Anzahl der Texte, die verarbeitet werden sollen\n",
    "\n",
    "# # INPUT TWEET IST BEREITS NORMALISIERT!\n",
    "# # Tokenisierung und Padding/Truncation\n",
    "# lyrics = subdataset_90['lyrics'][:amount]\n",
    "# inputs = tokenizer(lyrics, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# # Verarbeitung der Eingabe durch das Modell\n",
    "# with torch.no_grad():\n",
    "#     features = bertweet(**inputs)  # Models outputs sind jetzt Tupel\n",
    "\n",
    "# # Ausgabe der Features (optional: erster Layer oder hidden states)\n",
    "# print(features.last_hidden_state)  # Beispielausgabe\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad529d32-bc48-4ec2-b26f-7cb3e1c0ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "def run_analytics_bert_different_Sentence_Transformers(amount_per_iteration,name_of_run, sentence_transformer):\n",
    "\n",
    "    start_time = time.time()\n",
    "    amount = amount_per_iteration\n",
    "\n",
    "    path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rock/\"\n",
    "\n",
    "    \n",
    "    rap_70_lyrics_bert = processBertTopics(load_from_disk(path + 'subdataset_70_rock')['lyrics'][:amount],sentence_transformer)\n",
    "    rap_80_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_80_rock\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_90_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_90_rock\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_00_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_00_rock\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_10_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_10_rock\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_20_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_20_rock\")['lyrics'][:amount],sentence_transformer)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    list_of_lyrics_bert = {\n",
    "        \"70s\": rap_70_lyrics_bert,\n",
    "        \"80s\": rap_80_lyrics_bert,\n",
    "        \"90s\": rap_90_lyrics_bert,\n",
    "        \"00s\": rap_00_lyrics_bert,\n",
    "        \"10s\": rap_10_lyrics_bert,\n",
    "        \"20s\": rap_20_lyrics_bert,\n",
    "    }\n",
    "    end_time = time.time()\n",
    "\n",
    "    amount_of_time = end_time - start_time\n",
    "    \n",
    "    data_for_csv_bert = []\n",
    "    \n",
    "    for decade_name, decade in list_of_lyrics_bert.items():\n",
    "        i = 0\n",
    "        \n",
    "        for topics in decade:\n",
    "            i = i + 1\n",
    "            data_for_csv_bert.append({\n",
    "                \"Method\": \"BERT\",\n",
    "                \"Genre\": \"Rock\",\n",
    "                \"Decade\": decade_name,\n",
    "                \"Topic\": str(decade[topics])\n",
    "            })\n",
    "    data_for_csv_bert.append({\n",
    "                \"Method\": \"TIME\",\n",
    "                \"Genre\": \"TIME\",\n",
    "                \"Decade\": \"TIME\",\n",
    "                \"Topic\": str(amount_of_time)\n",
    "            })\n",
    "    \n",
    "    # Write to csv\n",
    "    now = datetime.datetime.now()\n",
    "    path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/BERT/Rock/\"\n",
    "    \n",
    "    # Datei schreiben\n",
    "    name_of_file = \"topics_rock_BERT_\" + \"6\"+\":\"+ \"21\"+ \":50\" + \"_MiniLM.csv\"\n",
    "    output_file = os.path.join(path, name_of_file)\n",
    "    \n",
    "\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "        writer.writeheader()  \n",
    "        writer.writerows(data_for_csv_bert) \n",
    "   \n",
    "    print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70ccbf7-d026-4549-a865-36a22b423889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "CSV-Datei '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/BERT/Rock/topics_rock_BERT_6:21:50_MiniLM.csv' wurde erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "run_analytics_bert_different_Sentence_Transformers(1000,\"BERT_mit_stop_words\",\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c2ee952-5a1e-48d5-bc2d-fedd50cd225d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241m.\u001b[39mget_topics(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "topic_model.get_topics(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608b690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
