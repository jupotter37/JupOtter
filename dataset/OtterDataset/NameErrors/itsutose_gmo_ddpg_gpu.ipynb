{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:246: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n",
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  np.bool8: (False, True),\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "# pip install gym=0.25.2 ※0.26.2だと正しく学習できない（原因は未調査）\n",
    "\n",
    "\n",
    "class ActorModel(keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        self.noise_stdev = 0.2  # ノイズ用の標準偏差\n",
    "        self.action_space = action_space # 連続値の値域が与えられる. Box(-2.0, 2.0, (1,), float32)]\n",
    "        # Boxについて\n",
    "        # from gym.space import Box でimportする\n",
    "        # -2.0 ~ 2.0 の値域で (1,) 1次元 という意味\n",
    "\n",
    "        # Envアクション用\n",
    "        self.action_centor = (action_space.high + action_space.low)/2  # 中心値は0.0\n",
    "        self.action_scale = action_space.high - self.action_centor     # スケールは2.0\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense3 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.actions = keras.layers.Dense(action_space.shape[0], activation=\"tanh\")\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = Adam(lr=0.003)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, inputs, training=False): # __call__メソッドであり，self()の形で実行可能\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        actions = self.actions(x)\n",
    "        return actions\n",
    "\n",
    "    # 状態を元にactionを算出\n",
    "    def sample_action(self, state, training=False):\n",
    "        actions = self(state.reshape((1,-1))) # def call()を呼び出している\n",
    "        action = actions[0].numpy()\n",
    "\n",
    "        if training:\n",
    "            # 学習用\n",
    "            # ノイズを混ぜる\n",
    "            noise = np.random.normal(0, self.noise_stdev, size=self.action_space.shape)\n",
    "            action = np.clip(action + noise, -1, 1) # action は -1 ~ 1の範囲で出力 ∵actionsの出力がtanh\n",
    "\n",
    "            # 環境用のアクションと学習用のアクションを返す\n",
    "            return (action * self.action_scale + self.action_centor), action\n",
    "        else:\n",
    "            # テスト用、環境に渡すアクションのみを返す\n",
    "            return action * self.action_scale + self.action_centor\n",
    "\n",
    "\n",
    "class CriticModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 各レイヤーを定義\n",
    "        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense2 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense3 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.value1 = keras.layers.Dense(1, activation=\"linear\")\n",
    "        self.dense4 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense5 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.dense6 = keras.layers.Dense(32, activation=\"relu\")\n",
    "        self.value2 = keras.layers.Dense(1, activation=\"linear\")\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Adam(lr=0.003)\n",
    "\n",
    "    # Forward pass\n",
    "    def call(self, states, actions, training=False):\n",
    "        x = tf.concat([states, actions], axis=1)\n",
    "        x1 = self.dense1(x)\n",
    "        x1 = self.dense2(x1)\n",
    "        x1 = self.dense3(x1)\n",
    "        q1 = self.value1(x1)\n",
    "        x2 = self.dense4(x)\n",
    "        x2 = self.dense5(x2)\n",
    "        x2 = self.dense6(x2)\n",
    "        q2 = self.value2(x2)\n",
    "        return q1, q2\n",
    "\n",
    "        # CriticModelのインスタンスでそのまま呼び出し可能 ∵__call__メソッド\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47131583  0.8819645  -5.401876  ]]\n",
      "[[ 0.47131583  0.8819645  -5.401876  ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray([0.47131583, 0.8819645, -5.401876])\n",
    "print(a.reshape(1,-1))\n",
    "print(a.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1,), 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym.spaces import Box\n",
    "\n",
    "# 1次元の行動空間を定義（範囲: -2.0 ~ 2.0）\n",
    "action_space = Box(low=-2.0, high=2.0, shape=(1,), dtype=np.float32)\n",
    "action_space.shape, action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(\n",
    "        actor_model, \n",
    "        target_actor_model, \n",
    "        critic_model, \n",
    "        target_critic_model, \n",
    "        experiences, \n",
    "        batch_size, \n",
    "        gamma,\n",
    "        all_train_count,\n",
    "        actor_update_interval,\n",
    "        target_policy_noise_stddev,\n",
    "        target_policy_clip_range,\n",
    "    ):\n",
    "\n",
    "    # ランダムに経験を取得してバッチを作成\n",
    "    batchs = random.sample(experiences, batch_size)\n",
    "\n",
    "    # Target Networkを用いて次の状態の価値を出す\n",
    "    n_states = np.asarray([e[\"n_state\"] for e in batchs])\n",
    "    n_actions = target_actor_model(n_states)\n",
    "\n",
    "    # Target Actionのノイズ\n",
    "    clipped_noise = np.clip(np.random.normal(0, target_policy_noise_stddev, n_actions.shape), -target_policy_clip_range, target_policy_clip_range)\n",
    "    n_actions = np.clip(n_actions + clipped_noise, -1, 1)\n",
    "\n",
    "    # 2つのQ値から小さいほうを採用\n",
    "    n_qvals1, n_qvals2 = target_critic_model(n_states, n_actions)\n",
    "    n_qvals = [min(q1, q2) for q1, q2 in zip(n_qvals1.numpy(), n_qvals2.numpy())]\n",
    "\n",
    "    # Qを計算 : reward if done else (reward + gamma * n_qval)\n",
    "    q_vals = np.asarray([\n",
    "        [reward] if done else [reward] + gamma * n_qval\n",
    "        for reward, done, n_qval in zip(\n",
    "            [e[\"reward\"] for e in batchs],\n",
    "            [e[\"done\"] for e in batchs],\n",
    "            n_qvals,\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # データ整形\n",
    "    states = np.asarray([e[\"state\"] for e in batchs])\n",
    "    actions = np.asarray([e[\"action\"] for e in batchs])\n",
    "\n",
    "    #--- Actorの学習\n",
    "    # Actorの学習は少し減らす\n",
    "    if all_train_count % actor_update_interval == 0:\n",
    "        with tf.GradientTape() as tape:\n",
    "            actor_actions = actor_model(states, training=True)\n",
    "            q, _ = critic_model(states, actor_actions)\n",
    "            actor_loss = -tf.reduce_mean(q)  # 最大化\n",
    "        \n",
    "        grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_model.optimizer.apply_gradients(zip(grads, actor_model.trainable_variables))\n",
    "    else:\n",
    "        actor_loss = 0\n",
    "\n",
    "    #--- Criticの学習 MSEで学習\n",
    "    with tf.GradientTape() as tape:\n",
    "        q1, q2 = critic_model(states, actions, training=True)\n",
    "        loss1 = tf.reduce_mean(tf.square(q_vals - q1))\n",
    "        loss2 = tf.reduce_mean(tf.square(q_vals - q2))\n",
    "        critic_loss = loss1 + loss2\n",
    "    \n",
    "    grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "    critic_model.optimizer.apply_gradients(zip(grads, critic_model.trainable_variables))\n",
    "\n",
    "    return actor_loss, critic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model(actor_model, target_actor_model, critic_model, target_critic_model, soft_tau):\n",
    "\n",
    "    target_actor_model.set_weights(\n",
    "        (1 - soft_tau) * np.array(target_actor_model.get_weights(), dtype=object)\n",
    "        + (soft_tau) * np.array(actor_model.get_weights(), dtype=object))\n",
    "\n",
    "    target_critic_model.set_weights(\n",
    "        (1 - soft_tau) * np.array(target_critic_model.get_weights(), dtype=object)\n",
    "        + (soft_tau) * np.array(critic_model.get_weights(), dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main():\n",
    "    # env = gym.make(\"Pendulum-v1\", render_mode = 'human')\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "    # ハイパーパラメータ\n",
    "    buffer_size = 10000  # キューの最大容量\n",
    "    warmup_size = 500    # 最低限キューに入れる数\n",
    "    train_interval = 10  # 学習間隔\n",
    "    batch_size = 32      # バッチサイズ\n",
    "    gamma = 0.9          # 割引率\n",
    "    soft_tau = 0.02      # Target network の近づく割合\n",
    "    actor_update_interval = 2         # Actorの更新間隔\n",
    "    target_policy_noise_stddev = 0.2  # Target policy ノイズの標準偏差\n",
    "    target_policy_clip_range = 0.5    # Target policy ノイズのclip範囲\n",
    "\n",
    "    # モデルの定義\n",
    "    actor_model = ActorModel(env.action_space)\n",
    "    target_actor_model = ActorModel(env.action_space)\n",
    "    critic_model = CriticModel()\n",
    "    target_critic_model = CriticModel()\n",
    "\n",
    "    # モデルは一度伝搬させないと重みが作成されない\n",
    "    dummy_state = np.random.normal(0, 0.1, size=(1,) + env.observation_space.shape)\n",
    "    actor_model(dummy_state)\n",
    "    target_actor_model(dummy_state)\n",
    "    target_actor_model.set_weights(actor_model.get_weights())\n",
    "\n",
    "    dummy_action  = np.random.normal(0, 0.1, size=(1,) + env.action_space.shape)\n",
    "    critic_model(dummy_state, dummy_action)\n",
    "    target_critic_model(dummy_state, dummy_action)\n",
    "    target_critic_model.set_weights(critic_model.get_weights())\n",
    "\n",
    "    # 収集する経験は上限を決め、古いものから削除する\n",
    "    experiences = deque(maxlen=buffer_size)\n",
    "\n",
    "    all_step_count = 0\n",
    "    all_train_count = 0\n",
    "\n",
    "    # 記録用\n",
    "    history_rewards = []\n",
    "    history_metrics = []\n",
    "    history_metrics_y = []\n",
    "\n",
    "    # 学習ループ\n",
    "    for episode in range(500):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = np.asarray(state[0])\n",
    "\n",
    "        # state = np.asarray(env.reset())\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        n_step = 0\n",
    "\n",
    "        metrics_list = []\n",
    "\n",
    "        # 1episode\n",
    "        while not done:\n",
    "            # アクションを決定\n",
    "            env_action, action = actor_model.sample_action(state, True)\n",
    "\n",
    "            # 1step進める\n",
    "            n_state, reward, done, _ = env.step(env_action)\n",
    "            # print(env.step(env_action))\n",
    "            # break\n",
    "            n_state = np.asarray(n_state)\n",
    "            n_step += 1\n",
    "            total_reward += reward\n",
    "\n",
    "            experiences.append({\n",
    "                \"state\": state,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"n_state\": n_state,\n",
    "                \"done\": done,\n",
    "            })\n",
    "            state = n_state\n",
    "            \n",
    "            if len(experiences) == warmup_size-1:\n",
    "                # pdb.set_trace()\n",
    "                print(\"train start\")\n",
    "            \n",
    "            \n",
    "            # warmup貯まったら train_interval 毎に学習する\n",
    "            if len(experiences) >= warmup_size and all_step_count % train_interval == 0:\n",
    "                # モデルの更新\n",
    "                metrics = update_model(\n",
    "                    actor_model, \n",
    "                    target_actor_model, \n",
    "                    critic_model, \n",
    "                    target_critic_model, \n",
    "                    experiences, \n",
    "                    batch_size, \n",
    "                    gamma,\n",
    "                    all_train_count,\n",
    "                    actor_update_interval,\n",
    "                    target_policy_noise_stddev,\n",
    "                    target_policy_clip_range,\n",
    "                )\n",
    "                # Soft-target\n",
    "                update_target_model(\n",
    "                    actor_model, \n",
    "                    target_actor_model, \n",
    "                    critic_model, \n",
    "                    target_critic_model, \n",
    "                    soft_tau\n",
    "                )\n",
    "                all_train_count += 1\n",
    "                metrics_list.append(metrics)\n",
    "            \n",
    "            all_step_count += 1\n",
    "\n",
    "\n",
    "        # 報酬\n",
    "        history_rewards.append(total_reward)\n",
    "\n",
    "        # メトリクス\n",
    "        if len(metrics_list) > 0:\n",
    "            history_metrics.append(np.mean(metrics_list, axis=0))  # 平均を保存\n",
    "            history_metrics_y.append(episode)\n",
    "\n",
    "        #--- print\n",
    "        interval = 50\n",
    "        if episode % interval == 0:\n",
    "            print(\"{} (min,ave,max)reward {:.1f} {:.1f} {:.1f}\".format(\n",
    "                episode,\n",
    "                min(history_rewards[-interval:]),\n",
    "                np.mean(history_rewards[-interval:]),\n",
    "                max(history_rewards[-interval:]),\n",
    "            ))\n",
    "    \n",
    "    return actor_model, history_rewards, history_metrics, history_metrics_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.9716203 , -0.23654595, -0.3255576 ], dtype=float32),\n",
       " array([-1.6045883], dtype=float32),\n",
       " array([ 0.96215534, -0.27250165, -0.7436553 ], dtype=float32),\n",
       " -0.07020339099484263,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pendulum環境の動作確認\n",
    "\n",
    "# 環境の初期化\n",
    "pendulum_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "# 初期状態の取得\n",
    "initial_observation = pendulum_env.reset()\n",
    "\n",
    "# ランダムアクションの適用\n",
    "action = pendulum_env.action_space.sample()\n",
    "next_observation, reward, done, info = pendulum_env.step(action)\n",
    "\n",
    "initial_observation, action, next_observation, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yamaguchi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (min,ave,max)reward -1561.0 -1561.0 -1561.0\n",
      "train start\n",
      "50 (min,ave,max)reward -1780.4 -1294.1 -914.6\n",
      "100 (min,ave,max)reward -1367.2 -838.0 -401.5\n",
      "150 (min,ave,max)reward -835.2 -412.9 -2.4\n",
      "200 (min,ave,max)reward -1534.5 -292.9 -1.7\n",
      "250 (min,ave,max)reward -1531.7 -264.0 -0.7\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if any GPUs are available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) == 0:\n",
    "    raise RuntimeError(\"No GPU devices found.\")\n",
    "print(f'Num GPUs Available: {len(physical_devices)}')\n",
    "\n",
    "# Enable dynamic memory allocation on GPUs\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# (Optional) Use only the first GPU\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "\n",
    "\n",
    "\n",
    "model, history_rewards, history_metrics, history_metrics_y = train_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mplot(history_rewards, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39mtight_layout()\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_rewards, label=\"reward\")\n",
    "plt.tight_layout()\n",
    "plt.xlabel('episode')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('episode')\n",
    "ax1.grid()\n",
    "ax1.plot(history_metrics_y, [m[0] for m in history_metrics], color=\"C0\", marker='.', label=\"actor_loss\")\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history_metrics_y, [m[1] for m in history_metrics], color=\"C1\", marker='.', label=\"critic_loss\")\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout()  # レイアウトの設定\n",
    "#plt.savefig('cartpole2.png') # 画像の保存\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEnv(gym.make(\"Pendulum-v1\", render_mode = 'human'), model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
