{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Assignment 5 - Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAnxu3txgrRh"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "import torch\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import pos_tag\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JJF2iZ7g87U"
      },
      "source": [
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXI_E0Xg_JG"
      },
      "source": [
        "embedder = hub.load(module_url)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdkFyMEOhEf9"
      },
      "source": [
        "def embed(sentence):\n",
        "  emb_sentence = [sentence]\n",
        "  return embedder(emb_sentence)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPtyp6X4grRj"
      },
      "source": [
        "# fileref = open('UglyWordsPETER.csv', 'r', encoding=\"utf8\")\n",
        "# ugly_words = fileref.read(header = 1)\n",
        "\n",
        "ugly_words = pd.read_csv('UglyWordsPETER.csv', header = 0)\n",
        "raw_X = ugly_words.to_numpy()\n",
        "y = raw_X[:, 0]\n",
        "raw_X = raw_X[:, 1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8zoC2togrRk",
        "outputId": "849e8876-dac8-4661-9f79-ebadd54c43c5"
      },
      "source": [
        "# First convert to lower case:\n",
        "for i in range(len(raw_X)):\n",
        "    raw_X[i] = raw_X[i].lower()\n",
        "raw_X"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['*screams in 25 different languages*',\n",
              "       \"families to sue over legionnaires: more than 40 families affected by the fatal outbreak of legionnaires' disea... http://t.co/za4axfjsvb\",\n",
              "       'pandemonium in aba as woman delivers baby without face (photos) - http://t.co/c5u9qshhnb',\n",
              "       ...,\n",
              "       \"businesses are deluged with invokces. make yours stand out with colour or shape.and it's likely to rise to the top of the pay' pile.\",\n",
              "       '#breaking411 4 police officers arrested for abusing children at police-run boot camp in san luis obispo calif. - ... http://t.co/onlvf2fyoy',\n",
              "       '@news@ refugio oil spill may have been costlier bigger than projected http://t.co/sqoa1wv4um'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIRkH5MfgrRk"
      },
      "source": [
        "# Second remove links:\n",
        "for i in range(len(raw_X)):\n",
        "    raw_X[i] = re.sub(r'http\\S+', '', raw_X[i])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BangDJuDgrRl"
      },
      "source": [
        "# results = {}\n",
        "# for i in range(len(ugly_words)): \n",
        "#     result = re.findall(r'\\s\\w*\\.{3}', ugly_words[i])\n",
        "# #     result = re.findall(r'\\s\\w*\\.\\.\\.', ugly_words[i])\n",
        "#     if result != []:\n",
        "#         results[i] = result\n",
        "# print(len(results))\n",
        "# print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXZ2ThHYgrRl"
      },
      "source": [
        "# my_list = {1: [' disea...', ' disease'], 135: [' poss...', 'possible'], [303, 1011] : [' mediterran...', ' mediterranean'],\n",
        "#            551: [' tr...', 'trapped'], 563 : [' learni...', ' learning'], 558 : [' cow...', 'cow'],\n",
        "#            582 : [' oi...', ' oil'], [584, 1611] : [' ut...', 'utterly'], 705 : [' cleav...', 'cleavage'],\n",
        "#            1013 : [' see...', 'see'], [1018, 1046, 1034] : [' sh...', 'shooting'], 1196 : [' mothe...', 'mother'],\n",
        "#            1304 : [' accepte...', ' accepted'], 1467 : [' has...', ' has'], 1512 : [' anna...', ' anna'],\n",
        "#            1572 : [' cr...', ' crash'], 1749 : [' exp...', ' experts'], 1755 : [' bang...', ' bang']}\n",
        "# len(my_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Tyq1hohKgrRm",
        "outputId": "2e0651fc-e658-4602-d733-f2b11f48b956"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOkbHn3TgrRm"
      },
      "source": [
        "for i in range(len(raw_X)):\n",
        "    raw_X[i]=\"\".join([char for char in raw_X[i] if char not in string.punctuation])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXA7I4oMgrRm"
      },
      "source": [
        "raw_X_train = raw_X[:1500]\n",
        "raw_X_test  = raw_X[1500:]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruu7xIr5grRn"
      },
      "source": [
        "y_train = y[:1500]\n",
        "y_test = y[1500:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3h7H9cQhWBQ"
      },
      "source": [
        "X_train = []\n",
        "for x in raw_X_train:\n",
        "  X_train.append(tf.reshape(embed(x), [512]))\n",
        "X_train = np.array(X_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZtFh2yZiDvQ"
      },
      "source": [
        "X_test = []\n",
        "for x in raw_X_test:\n",
        "  X_test.append(tf.reshape(embed(x), [512]))\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wtjXCiWy3wu"
      },
      "source": [
        "#Fitting a Non-naive Gaussian Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_heY_PigrRn"
      },
      "source": [
        "class GenBayes():\n",
        "    \n",
        "    def fit(self, X, y, epsilon = 1e-3):\n",
        "        N, D = X.shape\n",
        "        \n",
        "        self.likelihoods = dict()\n",
        "        self.priors = dict()\n",
        "        \n",
        "        # sometimes y is float or boolean, etc. So we make sure they are converted into ints\n",
        "        self.K = set(y.astype(int))\n",
        "\n",
        "        for k in self.K:\n",
        "            X_k = X[y == k,:]\n",
        "            N_k, D = X_k.shape\n",
        "            mu_k=X_k.mean(axis=0)\n",
        "\n",
        "            self.likelihoods[k] = {\"mean\":X_k.mean(axis=0), \"cov\":(1/(N_k-1))*np.matmul((X_k-mu_k).T,X_k-mu_k)+ epsilon*np.identity(D)}\n",
        "            self.priors[k] = len(X_k)/len(X)\n",
        "        return\n",
        "\n",
        "    def predict(self, X):\n",
        "        N, D = X.shape\n",
        "        P_hat = np.zeros((N,len(self.K)))\n",
        "\n",
        "        for k, l in self.likelihoods.items():\n",
        "            P_hat[:,k] = mvn.logpdf(X, l[\"mean\"], l[\"cov\"]) + np.log(self.priors[k])\n",
        "        return P_hat.argmax(axis = 1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bWGkyb1grRn"
      },
      "source": [
        "def accuracy(y, y_hat):\n",
        "  count = 0\n",
        "  for i in range(len(y)):\n",
        "    if y[i] == y_hat[i]:\n",
        "      count += 1\n",
        "  return count/len(y)\n",
        "  # return np.mean(y_test == y_hat)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sg9CWk9grRn",
        "outputId": "b6276dbd-98d8-4e58-d7df-d25d0e954a24"
      },
      "source": [
        "gb = GenBayes()\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "y_hat = gb.predict(X_test)\n",
        "print(f'Non-Naiv Gaussian Bayes accuracy= {(100 * accuracy(y_test, y_hat)):.2f}%')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Non-Naiv Gaussian Bayes accuracy= 88.74%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPVKCV0gzFKQ"
      },
      "source": [
        "# Training an Artificial Neura Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk9YE5GCzRdP"
      },
      "source": [
        "##Activation and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eARAC75MgrRo"
      },
      "source": [
        "def linear(H):\n",
        "  return H\n",
        "\n",
        "def ReLU(H):\n",
        "  # t avoid using max function\n",
        "  return H * (H > 0)\n",
        "\n",
        "def sigmoid(H):\n",
        "  return 1/(1 + np.exp(-H))\n",
        "\n",
        "def softmax(H):\n",
        "  eH = np.exp(H)\n",
        "  # keepdims is for keeping the dimensions.\n",
        "  return eH/eH.sum(axis = 1, keepdims = True)\n",
        "\n",
        "def cross_entropy(Y, P_hat):\n",
        "  return -(1/len(Y)) * np.sum(Y * np.log(P_hat))\n",
        "\n",
        "def OLS(Y, Y_hat):\n",
        "  return (1/(2*len(Y))) * np.sum((Y - Y_hat)**2)\n",
        "\n",
        "def one_hot_encode(y):\n",
        "  N = len(y)\n",
        "  K = len(set(y))\n",
        "\n",
        "  Y = np.zeros((N, K))\n",
        "  for i in range(N):\n",
        "    Y[i, y[i]] = 1\n",
        "\n",
        "  return Y\n",
        "\n",
        "def accuracy(y, y_hat):\n",
        "  return np.mean(y == y_hat)\n",
        "\n",
        "def R2(y, y_hat):\n",
        "  return 1 - np.sum((y - y_hat) ** 2) / np.sum((y - y.mean()) ** 2)\n",
        "\n",
        "# We define this function to generalize what we did for the two layer \n",
        "def derivative(Z, a):\n",
        "  if a == linear:\n",
        "    return 1\n",
        "\n",
        "  elif a == sigmoid:\n",
        "    return Z * (1 - Z)\n",
        "\n",
        "  elif a == np.tanh:\n",
        "    return 1 - Z * Z\n",
        "\n",
        "  elif a == ReLU:\n",
        "    return (Z > 0).astype(int)\n",
        "  else:\n",
        "    VlueError('Unknown Activation Function')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUvgDIvZzYdM"
      },
      "source": [
        "class ANN():\n",
        "\n",
        "  def __init__(self, architecture, activations = None, mode = 0):\n",
        "    self.mode = mode\n",
        "    self.architecture = architecture\n",
        "    self.activations = activations\n",
        "    self.L = len(architecture) + 1\n",
        "\n",
        "  def fit(self, X, y, eta = 1e-3, epochs = 1e3, show_curve = False):\n",
        "    epochs = int(epochs)\n",
        "\n",
        "    if self.mode:\n",
        "      Y = y\n",
        "    else:\n",
        "      Y = one_hot_encode(y)\n",
        "\n",
        "    N, D = X.shape\n",
        "    K = Y.shape[1]\n",
        "\n",
        "    # Weight and bias initialization:\n",
        "    self.W = {l : np.random.randn(M[0], M[1]) for l, M in enumerate(zip(([D] + self.architecture) , (self.architecture + [K])), 1)}\n",
        "    self.b = {l : np.random.randn(M) for l, M in enumerate(self.architecture + [K], 1)}\n",
        "\n",
        "    # Activation function loading:\n",
        "    if self.architecture is None:\n",
        "      self.a = {l : ReLU for l in range(1, self.L)}\n",
        "    else:\n",
        "      self.a = {l : act for l, act in enumerate(self.activations, 1)}\n",
        "\n",
        "    # Mode set:\n",
        "    if self.mode:\n",
        "      self.a[self.L] = linear\n",
        "    else:\n",
        "      self.a[self.L] = softmax\n",
        "\n",
        "    J = np.zeros(epochs)\n",
        "\n",
        "    # Gradient Descent:\n",
        "    for epoch in range(epochs):\n",
        "      if epoch == epochs / 4:\n",
        "        eta /=10\n",
        "      if epoch == epochs / 2:\n",
        "        eta /=10\n",
        "      if epoch == 3 * epochs / 4:\n",
        "        eta /=10\n",
        "\n",
        "      self.forward(X)\n",
        "      if self.mode:\n",
        "        J[epoch] = OLS(Y, self.Z[self.L])\n",
        "      else:\n",
        "        J[epoch] = cross_entropy(Y, self.Z[self.L])\n",
        "\n",
        "      # We now define the differential element of H:\n",
        "      dH = (1/N) * (self.Z[self.L] - Y)\n",
        "\n",
        "      # Here we assign our weight and bias upate rules (Back-propagation).\n",
        "      for l in sorted(self.W.keys(), reverse = True):\n",
        "        dW = self.Z[l - 1].T @ dH\n",
        "        db = dH.sum(axis = 0)\n",
        "\n",
        "        self.W[l] -= eta * dW\n",
        "        self.b[l] -= eta * db\n",
        "\n",
        "        if l > 1:\n",
        "          dZ = dH @ self.W[l].T\n",
        "          dH = dZ * derivative(self.Z[l-1], self.a[l - 1])\n",
        "\n",
        "    if show_curve:\n",
        "      plt.figure()\n",
        "      plt.plot(J)\n",
        "      plt.xlabel('epochs')\n",
        "      plt.ylabel('$\\mathcal{J}$')\n",
        "      plt.title('Training Curve')\n",
        "      plt.show()\n",
        "    \n",
        "  def forward(self, X):\n",
        "    self.Z = {0: X}\n",
        "\n",
        "    for l in sorted(self.W.keys()):\n",
        "      self.Z[l] = self.a[l](self.Z[l - 1] @ self.W[l] + self.b[l])\n",
        "  \n",
        "  def predict(self, X):\n",
        "    self.forward(X)\n",
        "\n",
        "    if self.mode:\n",
        "      # Returns the output of the last layer of the network which is the final output.\n",
        "      return self.Z[self.L]\n",
        "    else:\n",
        "      # This is to prform softmax function:\n",
        "      return self.Z[self.L].argmax(axis = 1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwWbaOwkzmNF"
      },
      "source": [
        "def main():\n",
        "  myAnn = ANN([8, 8, 8, 8, 8], [ReLU, np.tanh, np.tanh, np.tanh, np.tanh])\n",
        "  myAnn.fit(X_train, y_train, eta = 3e-1, epochs = 5e4, show_curve = True)\n",
        "  y_hat = myAnn.predict(X_test)\n",
        "  print(y_test.shape)\n",
        "  print(f'ANN testing accuracy: {(100 * accuracy(y_test, y_hat)):0.2f}%')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "OYlT3KPzznj3",
        "outputId": "9d35e872-27e1-40bf-c208-3ceebfd4a4ca"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(364,)\n",
            "ANN testing accuracy: 82.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW6W-T1osqP2"
      },
      "source": [
        "class Layer(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, size_in, size_out, activation):\n",
        "  # size_out defines this layer's nodes.\n",
        "    super(Layer, self).__init__()\n",
        "    # It is torch's way of injecting parameters into the class.\n",
        "    self.weights = torch.nn.Parameter(torch.randn(size_in, size_out, requires_grad = True))\n",
        "    self.bias = torch.nn.Parameter(torch.randn(1, size_out, requires_grad = True))\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, z_in):\n",
        "    \n",
        "    return self.activation(z_in@self.weights + self.bias)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IxGEMxkss50"
      },
      "source": [
        "forget = Layer(512, 15, torch.nn.Sigmoid())\n",
        "# Mean Square Error Loss\n",
        "loss_func = torch.nn.MSELoss()\n",
        "# Optimization\n",
        "opt = torch.optim.Adam(forget.parameters())"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdUqACICvoQ3"
      },
      "source": [
        "# X_train = torch.from_numpy(X_train)\n",
        "# type(X_train)\n",
        "# y_train = torch.from_numpy(y_train)\n",
        "# type(y_train)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "gNMhekzes3qA",
        "outputId": "79d39593-56d9-4553-fff3-cbf8aac775c3"
      },
      "source": [
        "print(forget.bias)\n",
        "out = forget.forward(X_train)\n",
        "print(out.shape)\n",
        "print(y_train.shape)\n",
        "loss = loss_func(out, y_train)\n",
        "# Does the backpropagation\n",
        "loss.backward()\n",
        "# Just do one epoch\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "print(forget.bias)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2838, -1.2327, -0.5805,  0.8719,  0.6260, -0.5697,  0.0698,  0.1992,\n",
            "          1.6327,  0.3334, -1.1010,  0.6734,  0.4121, -0.3924, -0.5425]],\n",
            "       requires_grad=True)\n",
            "torch.Size([1500, 15])\n",
            "(1500,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-49b7b397e312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Does the backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m                 reduction=reduction)\n\u001b[0;32m-> 2651\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n\u001b[1;32m   2653\u001b[0m                       \u001b[0;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_QrKkCYuURF"
      },
      "source": [
        "# Recurrent Networks\n",
        "class RNN(torch.nn.Module):\n",
        "  def __init__(self, size_in, size_out, size_mem):\n",
        "    super(RNN, self).__init__()\n",
        "    self.size_mem = size_mem\n",
        "    self.mem_layer = Layer(size_in + size_mem, size_mem, torch.tanh)\n",
        "    self.out_layer = Layer(size_mem, size_out, torch.sigmoid)\n",
        "\n",
        "  def forward(self, X):\n",
        "    mem = torch.zeros(1, self.size_mem)\n",
        "    y_hat = []\n",
        "    for i in range(X.shape[0]):\n",
        "      x_in = X[[i], :]\n",
        "      z_in = torch.cat([x_in, mem], dim = 1)\n",
        "      mem = self.mem_layer.forward(z_in)\n",
        "      y_hat.append(self.out_layer.forward(mem))\n",
        "\n",
        "    return torch.cat(y_hat, dim = 0)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f8xnel0uXtu"
      },
      "source": [
        "rnn = RNN(512, 15, 5)\n",
        "loss_func = torch.nn.MSELoss()\n",
        "# decides the type of gradient descient.\n",
        "opt = torch.optim.Adam(rnn.parameters())"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "m0oWBmMAua8h",
        "outputId": "45ee76ba-3eb5-4a5d-e715-f4a379824cc5"
      },
      "source": [
        "print(rnn.mem_layer.bias)\n",
        "y_hat = rnn.forward(X_train)\n",
        "loss = loss_func(y_hat, y)\n",
        "loss.backward()\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "print()\n",
        "print(rnn.mem_layer.bias)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.1948,  1.2167,  0.3311, -1.5199, -1.9711]], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-b50c9b01d9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m                 reduction=reduction)\n\u001b[0;32m-> 2651\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n\u001b[1;32m   2653\u001b[0m                       \u001b[0;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEqpujXiueg9"
      },
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, size_in, size_out, size_long, size_short):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.size_long = size_long\n",
        "    self.size_short = size_short\n",
        "\n",
        "    size_z = size_in + size_short\n",
        "    \n",
        "    # The first five are hiden layers.\n",
        "    self.forget_gate  = Layer(size_z, size_long, torch.sigmoid)\n",
        "    self.memory_gate  = Layer(size_z, size_long, torch.sigmoid)\n",
        "    self.memory_layer = Layer(size_z, size_long, torch.tanh)\n",
        "    self.recall_gate  = Layer(size_z, size_short, torch.sigmoid)\n",
        "    self.recall_layer = Layer(size_long, size_short, torch.tanh)\n",
        "    self.output_gate  = Layer(size_short, size_out, torch.sigmoid)\n",
        "\n",
        "  def forward(self, X):\n",
        "    mem_short = torch.zeros(1, self.size_short)\n",
        "    mem_long = torch.zeros(1, self.size_long)\n",
        "    y_hat = []\n",
        "    # For all the rows\n",
        "    for i in range(X.shape[0]):\n",
        "      X_t = X[[i], :]\n",
        "      z_t = torch.cat([X_t, mem_short], dim = 1)\n",
        "\n",
        "      mem_long = mem_long * self.forget_gate.forward(z_t)\n",
        "      mem_long = mem_long + (self.memory_gate.forward(z_t) * self.memory_layer.forward(z_t))\n",
        "      mem_short = self.recall_gate.forward(z_t) * self.recall_layer.forward(mem_long)\n",
        "\n",
        "      y_hat.append(self.output_gate.forward(mem_short))\n",
        "\n",
        "    return torch.cat(y_hat, dim = 0)\n",
        "\n",
        "\n",
        "  def generate(self, start, stop, random_factor):\n",
        "    y_hat = [start]\n",
        "\n",
        "    mem_long = torch.randn([1, self.size_long]) * random_factor\n",
        "    mem_short = torch.randn([1, self.size_short]) * random_factor\n",
        "\n",
        "    while (y_hat[-1] != stop).any() and len(y_hat) < 30:\n",
        "      x_t = y_hat[-1]\n",
        "      z_t = torch.cat([x_t, mem_short], dim = 1)\n",
        "      mem_long = mem_long * self.forget_gate.forward(z_t)\n",
        "      mem_long = mem_long + (self.memory_gate.forward(z_t) * self.memory_layer.forward(z_t))\n",
        "      mem_short = self.recall_gate.forward(z_t) * self.recall_layer.forward(mem_long)\n",
        "      out = self.output_gate.forward(mem_short)\n",
        "      out = torch.argmax(out, dim = 1)\n",
        "\n",
        "      y_hat.append(torch.zeros(stop.shape))\n",
        "      y_hat[-1][0, out] = 1\n",
        "    return torch.cat(y_hat, dim = 0)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQ_DKcMuiMt"
      },
      "source": [
        "def Process(name):\n",
        "  name = ''.join(['{', name, '|'])\n",
        "  out = []\n",
        "  for letter in name:\n",
        "    # Here LSTM just predicts the next letter.\n",
        "    row = torch.zeros([1, 28])\n",
        "    # 97 is the ascii code for 'a'.\n",
        "    row[0, ord(letter) - 97] = 1\n",
        "    out.append(row)\n",
        "  return torch.cat(out)\n",
        "\n",
        "def Decode(y_hat):\n",
        "  out = ''\n",
        "  for i in torch.argmax(y_hat, dim = 1):\n",
        "    out += chr(i + 97)\n",
        "  return out"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAM7f-DsuvXy"
      },
      "source": [
        "## Define our LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjF0vk47unR0"
      },
      "source": [
        "# The output and input have 28 lines. One for each alphabet character.\n",
        "# Only one of them is active at the same time.\n",
        "# We use argmax at the output to make sure that only one of them is active.\n",
        "lstm = LSTM(512, 1, 12, 12)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(lstm.parameters(), lr = 4.5e-2)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "talIsWC2uz5g",
        "outputId": "e8fbf75e-9e12-421c-eb55-27919882c104"
      },
      "source": [
        "iterations = 4600\n",
        "losses =[0]\n",
        "name = '{'\n",
        "\n",
        "for i in range(iterations):\n",
        "  name = Process(np.random.choice(data.names))\n",
        "  y_hat = lstm.forward(name)\n",
        "  # print('\\r Iteration: {} Loss:{} |{}'.format(i, loss-1, Decode(y_hat)) + '   ')\n",
        "  loss = loss_func(y_hat[:-1], torch.argmax(name, dim = 1)[1:])\n",
        "  print('\\r Iteration: {} Loss:{} |{}'.format(i, loss-1, Decode(y_hat)) + '   ')\n",
        "  loss.backward()\n",
        "  losses.append(loss.detach())\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "\n",
        "  plt.plot(losses)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-f9898dde127a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# print('\\r Iteration: {} Loss:{} |{}'.format(i, loss-1, Decode(y_hat)) + '   ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV3DNvmMu2om"
      },
      "source": [
        "for i in range(100):\n",
        "  print(Decode(lstm.generate(Process('{')[[1]], Process('')[[1]], 1)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}