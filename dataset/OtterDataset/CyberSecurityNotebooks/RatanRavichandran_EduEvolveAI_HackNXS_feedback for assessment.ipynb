{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "!pip install pypdf\n",
        "!pip install pdfplumber\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "At7Qx325Tsmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a810a0b0-5784-44b1-908d-2fbe3d89c169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.31)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.40)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.40)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.16.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.28.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW0XA7t0TOyX",
        "outputId": "0435926d-4abc-4f5d-db02-d60fa411f8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages in student profile: 2\n",
            "Number of pages in class notes: 3\n",
            "Number of text chunks in student profile: 3\n",
            "Number of text chunks in class notes: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Student Name: Jessica Lee\n",
            "Student Age: 15\n",
            "Strengths:\n",
            "- Strong analytical skills\n",
            "- Excellent verbal communication\n",
            "- Solid understanding of algebra and geometry\n",
            "- Enthusiastic about biology and enjoys hands-on experiments\n",
            "- Good comprehension skills\n",
            "- Active participant in group activities\n",
            "- Empathetic towards peers\n",
            "Academic Progress:\n",
            "- Mathematics:\n",
            "Midterm Exam: 85\n",
            "Final Exam: 88\n",
            "- Science:\n",
            "Lab Reports: 80\n",
            "Quizzes: 75\n",
            "Final Exam: 82\n",
            "- English:\n",
            "Literature Analysis: 72\n",
            "Essay Writing: 68\n",
            "Recommended Interventions (if any):\n",
            "Based on Jessica's academic progress, it is recommended that she seeks extra help in essay writing and literature analysis to improve her grades in English.\n",
            "Preferred Learning Modes:\n",
            "Jessica enjoys collaborative learning settings with interactive activities. She responds well to visual aids, hands-on experiments, and group discussions.\n",
            "Weaknesses:\n",
            "Jessica needs to work on her essay writing and literature analysis skills. She also struggles with understanding the Reconnaissance step in the cybersecurity lifecycle and has wrongly identified steps for breaking the cyber attack lifecycle.\n",
            "Future Career Aspiration:\n",
            "Jessica is interested in pursuing a career in computer science or data analytics. It is recommended that she takes courses related\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Student Name: Jessica Lee\n",
            "Student Age: 15\n",
            "Strengths:\n",
            "- Strong analytical skills\n",
            "- Excellent verbal communication\n",
            "- Solid understanding of algebra and geometry\n",
            "- Enthusiastic about biology and enjoys hands-on experiments\n",
            "- Good comprehension skills\n",
            "- Active participant in group activities\n",
            "- Empathetic towards peers\n",
            "- Interested in pursuing a career in computer science or data analytics\n",
            "- Collaborative learner who enjoys interactive activities\n",
            "Weaknesses:\n",
            "- Struggles with literature analysis and essay writing\n",
            "- Limited understanding of cybersecurity lifecycle and steps for breaking cyber attacks\n",
            "Preferred Learning Modes:\n",
            "- Visual aids\n",
            "- Hands-on experiments\n",
            "- Group discussions\n",
            "Academic Progress:\n",
            "- Lab Reports: 80\n",
            "- Quizzes: 75\n",
            "- Midterm Exam (Mathematics): 85\n",
            "- Final Exam (Mathematics): 88\n",
            "- Final Exam (Science): 82\n",
            "- Literature Analysis: 72\n",
            "- Essay Writing: 68\n",
            "- Midterm Exam (History): 80\n",
            "- Final Exam (History): 85\n",
            "Recommended Interventions (if any):\n",
            "- Provide additional support and resources for literature analysis and essay writing\n",
            "- Offer guidance and instruction on cybersecurity lifecycle and steps for breaking cyber attacks\n",
            "- Encourage participation in extr\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Student Name: Jessica Lee\n",
            "Student Age: 15\n",
            "Strengths:\n",
            "- Strong analytical skills\n",
            "- Excellent verbal communication\n",
            "- Enthusiastic about biology and enjoys hands-on experiments\n",
            "- Good comprehension skills\n",
            "- Active participant in group activities\n",
            "- Empathetic towards peers\n",
            "- Solid understanding of algebra and geometry\n",
            "Weaknesses:\n",
            "- Lack of understanding in Cybersecurity Lifecycle, specifically Reconnaissance\n",
            "- Difficulty identifying steps in breaking a cyber attack lifecycle\n",
            "Preferred Learning Modes:\n",
            "- Collaborative learning settings with interactive activities\n",
            "Academic Progress:\n",
            "Mathematics:\n",
            "- Midterm Exam: 85\n",
            "- Final Exam: 88\n",
            "Science:\n",
            "- Lab Reports: 80\n",
            "- Quizzes: 75\n",
            "- Final Exam: 82\n",
            "English:\n",
            "- Literature Analysis: 72\n",
            "- Essay Writing: 68\n",
            "- Final Exam: 78\n",
            "History:\n",
            "- Projects: 88\n",
            "- Midterm Exam: 80\n",
            "- Final Exam: 85\n",
            "Recommended Interventions (if any):\n",
            "- Providing additional resources and support to improve understanding in Cybersecurity Lifecycle, specifically Reconnaissance\n",
            "- Guiding and assisting in correctly identifying steps in breaking a cyber attack lifecycle\n",
            "- Encouraging and providing opportunities for hands-on experiments and group discussions to enhance\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "\n",
            "1. How can we effectively identify and mitigate potential cyber attacks on our business infrastructure?\n",
            "2. What strategies and tools can be used to detect and respond to suspicious activity or anomalies in our systems?\n",
            "3. How can we establish and maintain a comprehensive cyber security risk management plan to ensure resilience against cyber threats?\n"
          ]
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pdfplumber\n",
        "from pytesseract import pytesseract\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# Get your API keys from openai, you will need to create an account.\n",
        "# Here is the link to get the keys: https://platform.openai.com/account/billing/overview\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OpenAI')\n",
        "\n",
        "# Specify the file paths for the two PDF files\n",
        "student_profile_path = '/content/Student_profile_2.pdf'\n",
        "class_notes_path = '/content/Cyber-Attacks-Life-Cycle.pdf'\n",
        "\n",
        "# Process the student profile PDF\n",
        "with pdfplumber.open(student_profile_path) as student_pdf:\n",
        "    num_pages = len(student_pdf.pages)\n",
        "    print(f\"Number of pages in student profile: {num_pages}\")\n",
        "    student_raw_text = ''\n",
        "    for i in range(num_pages):\n",
        "        page = student_pdf.pages[i]\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            student_raw_text += text\n",
        "\n",
        "# Process the class notes PDF\n",
        "with pdfplumber.open(class_notes_path) as notes_pdf:\n",
        "    num_pages = len(notes_pdf.pages)\n",
        "    print(f\"Number of pages in class notes: {num_pages}\")\n",
        "    notes_raw_text = ''\n",
        "    for i in range(num_pages):\n",
        "        page = notes_pdf.pages[i]\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            notes_raw_text += text\n",
        "\n",
        "# Split the student profile text into smaller chunks\n",
        "student_text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        ")\n",
        "student_texts = student_text_splitter.split_text(student_raw_text)\n",
        "print(f\"Number of text chunks in student profile: {len(student_texts)}\")\n",
        "\n",
        "# Split the class notes text into smaller chunks\n",
        "notes_text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        ")\n",
        "notes_texts = notes_text_splitter.split_text(notes_raw_text)\n",
        "print(f\"Number of text chunks in class notes: {len(notes_texts)}\")\n",
        "\n",
        "# Set up the prompt template for generating the student profile\n",
        "student_profile_prompt_template = \"\"\"\n",
        "You are an education counselor responsible for creating a student profile based on the information provided about the student.\n",
        "\n",
        "The context about the student is as follows:\n",
        "\n",
        "{context}\n",
        "\n",
        "Based on this information, please generate a comprehensive student profile including the following:\n",
        "\n",
        "Student Name:\n",
        "Student Age:\n",
        "Strengths:\n",
        "Weaknesses:\n",
        "Preferred Learning Modes:\n",
        "Academic Progress:\n",
        "Recommended Interventions (if any):\n",
        "\n",
        "Provide a detailed profile covering all the relevant aspects of the student's academic and personal background.\n",
        "\"\"\"\n",
        "\n",
        "student_profile_prompt = PromptTemplate(template=student_profile_prompt_template, input_variables=[\"context\"])\n",
        "\n",
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create a FAISS vector store for the student profile\n",
        "student_docsearch = FAISS.from_texts(student_texts, embeddings)\n",
        "\n",
        "# Create a FAISS vector store for the class notes\n",
        "notes_docsearch = FAISS.from_texts(notes_texts, embeddings)\n",
        "\n",
        "# Set up the LLM and the chain for the student profile\n",
        "llm = OpenAI(temperature=0.7)\n",
        "student_profile_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "# Generate the student profile\n",
        "for student_text in student_texts:\n",
        "    student_query = student_profile_prompt.format(context=student_text)\n",
        "    student_docs = student_docsearch.similarity_search(student_query)\n",
        "    student_profile = student_profile_chain.run(input_documents=student_docs, question=student_query)\n",
        "    print(student_profile)\n",
        "    print(\"\\n---\\n\")\n",
        "\n",
        "# Create a prompt for generating questions based on the student's weaknesses\n",
        "weakness_prompt_template = \"\"\"\n",
        "Based on the student profile, the student's weaknesses are:\n",
        "\n",
        "{weaknesses}\n",
        "\n",
        "Generate 3 questions that could be asked in class to help address these weaknesses.\n",
        "\"\"\"\n",
        "\n",
        "weakness_prompt = PromptTemplate(template=weakness_prompt_template, input_variables=[\"weaknesses\"])\n",
        "\n",
        "# Find the student's weaknesses from the generated profile\n",
        "for student_text in student_texts:\n",
        "    student_query = student_profile_prompt.format(context=student_text)\n",
        "    student_docs = student_docsearch.similarity_search(student_query)\n",
        "    student_profile = student_profile_chain.run(input_documents=student_docs, question=student_query)\n",
        "    # Extract the weaknesses from the student profile\n",
        "    weaknesses = [line for line in student_profile.split(\"\\n\") if line.startswith(\"Weaknesses:\")]\n",
        "    if weaknesses:\n",
        "        weaknesses = weaknesses[0].split(\":\")[1].strip()\n",
        "        break\n",
        "\n",
        "# Generate questions based on the student's weaknesses\n",
        "questions_query = weakness_prompt.format(weaknesses=weaknesses)\n",
        "notes_docs = notes_docsearch.similarity_search(questions_query)\n",
        "questions = student_profile_chain.run(input_documents=notes_docs, question=questions_query)\n",
        "print(questions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "QMIMkHUPy6-3",
        "outputId": "083600d4-2182-48f2-83bc-c0358dee3907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.25.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.15.0 (from gradio)\n",
            "  Downloading gradio_client-0.15.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.6.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.10.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.15.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.16.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=a07bd4b449b9db78c8c9ffe01baa56662d8ce2e0e2029933f07c5c0415c97470\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvicorn, tomlkit, shellingham, semantic-version, ruff, python-multipart, colorama, aiofiles, starlette, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.1 ffmpy-0.3.2 gradio-4.25.0 gradio-client-0.15.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.3.5 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pdfplumber\n",
        "\n",
        "def process_pdfs(student_data_path, class_notes_path):\n",
        "    # Process the student data PDF\n",
        "    with pdfplumber.open(student_data_path) as student_pdf:\n",
        "        num_pages = len(student_pdf.pages)\n",
        "        print(f\"Number of pages in student data: {num_pages}\")\n",
        "        student_raw_text = ''\n",
        "        for i in range(num_pages):\n",
        "            page = student_pdf.pages[i]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                student_raw_text += text\n",
        "\n",
        "    # Process the class notes PDF\n",
        "    with pdfplumber.open(class_notes_path) as notes_pdf:\n",
        "        num_pages = len(notes_pdf.pages)\n",
        "        print(f\"Number of pages in class notes: {num_pages}\")\n",
        "        notes_raw_text = ''\n",
        "        for i in range(num_pages):\n",
        "            page = notes_pdf.pages[i]\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                notes_raw_text += text\n",
        "\n",
        "    # Split the student data text into smaller chunks\n",
        "    student_text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "    )\n",
        "    student_texts = student_text_splitter.split_text(student_raw_text)\n",
        "    print(f\"Number of text chunks in student data: {len(student_texts)}\")\n",
        "\n",
        "    # Split the class notes text into smaller chunks\n",
        "    notes_text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "    )\n",
        "    notes_texts = notes_text_splitter.split_text(notes_raw_text)\n",
        "    print(f\"Number of text chunks in class notes: {len(notes_texts)}\")\n",
        "\n",
        "    # Set up the prompt template for generating the student profile\n",
        "    student_profile_prompt_template = \"\"\"\n",
        "    You are an education counselor responsible for creating a student profile based on the information provided about the student.\n",
        "\n",
        "    The context about the student is as follows:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on this information, please generate a comprehensive student profile including the following:\n",
        "\n",
        "    Student Name:\n",
        "    Student Age:\n",
        "    Strengths:\n",
        "    Weaknesses:\n",
        "    Preferred Learning Modes:\n",
        "    Academic Progress:\n",
        "    Recommended Interventions (if any):\n",
        "\n",
        "    Provide a detailed profile covering all the relevant aspects of the student's academic and personal background.\n",
        "    \"\"\"\n",
        "\n",
        "    student_profile_prompt = PromptTemplate(template=student_profile_prompt_template, input_variables=[\"context\"])\n",
        "\n",
        "    # Download embeddings from OpenAI\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    # Create a FAISS vector store for the student data\n",
        "    student_docsearch = FAISS.from_texts(student_texts, embeddings)\n",
        "\n",
        "    # Create a FAISS vector store for the class notes\n",
        "    notes_docsearch = FAISS.from_texts(notes_texts, embeddings)\n",
        "\n",
        "    # Set up the LLM and the chain for the student profile\n",
        "    llm = OpenAI(temperature=0.7)\n",
        "    student_profile_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "    # Generate the student profile\n",
        "    student_profile = \"\"\n",
        "    for student_text in student_texts:\n",
        "        student_query = student_profile_prompt.format(context=student_text)\n",
        "        student_docs = student_docsearch.similarity_search(student_query)\n",
        "        student_profile += student_profile_chain.run(input_documents=student_docs, question=student_query)\n",
        "        student_profile += \"\\n---\\n\"\n",
        "\n",
        "    # Create a prompt for generating questions based on the student's weaknesses\n",
        "    weakness_prompt_template = \"\"\"\n",
        "    Based on the student profile, the student's weaknesses are:\n",
        "\n",
        "    {weaknesses}\n",
        "\n",
        "    Generate 3 questions that could be asked in class to help address these weaknesses.\n",
        "    \"\"\"\n",
        "\n",
        "    weakness_prompt = PromptTemplate(template=weakness_prompt_template, input_variables=[\"weaknesses\"])\n",
        "\n",
        "    # Find the student's weaknesses from the generated profile\n",
        "    weaknesses = \"\"\n",
        "    for student_text in student_texts:\n",
        "        student_query = student_profile_prompt.format(context=student_text)\n",
        "        student_docs = student_docsearch.similarity_search(student_query)\n",
        "        student_profile_text = student_profile_chain.run(input_documents=student_docs, question=student_query)\n",
        "        # Extract the weaknesses from the student profile\n",
        "        weakness_lines = [line for line in student_profile_text.split(\"\\n\") if line.startswith(\"Weaknesses:\")]\n",
        "        if weakness_lines:\n",
        "            weaknesses = weakness_lines[0].split(\":\")[1].strip()\n",
        "            break\n",
        "\n",
        "    # Generate questions based on the student's weaknesses\n",
        "    questions_query = weakness_prompt.format(weaknesses=weaknesses)\n",
        "    notes_docs = notes_docsearch.similarity_search(questions_query)\n",
        "    questions = student_profile_chain.run(input_documents=notes_docs, question=questions_query)\n",
        "\n",
        "    return student_profile, questions\n",
        "\n",
        "def main():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## PDF Processing Interface\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                student_data_path = gr.File(label=\"Student Data PDF\")\n",
        "            with gr.Column():\n",
        "                class_notes_path = gr.File(label=\"Class Notes PDF\")\n",
        "\n",
        "        process_button = gr.Button(\"Process PDFs\")\n",
        "        student_profile_output = gr.Textbox(label=\"Student Profile\", lines=10)\n",
        "        questions_output = gr.Textbox(label=\"Generated Questions\", lines=3)\n",
        "\n",
        "        process_button.click(\n",
        "            process_pdfs,\n",
        "            inputs=[student_data_path, class_notes_path],\n",
        "            outputs=[student_profile_output, questions_output]\n",
        "        )\n",
        "\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Si0gle_iZXwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "227b29ab-3a0d-4966-efb9-451cfe0c7c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://013905ad225e9e5ed1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://013905ad225e9e5ed1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}