{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tourism and Data Science: What is your next move?  \n",
      "\n",
      "(more suggestions on pictures and title)\n",
      "\n",
      "*\u201cThe journey is the destination.\u201d \u2015 Dan Eldon*\n",
      "\n",
      "\n",
      "<img src=\"files/taj.jpg\">\n",
      "\n",
      "(Image source: 500px)\n",
      "\n",
      "<img src=\"files/Gephi_Files/OpenOrd_With_Gephi_Modularity.png\">\n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_State.png\">\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction and Project Description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Throughout the course we have reiterated time and again the increasing amount of data being generated about the world around us. Insights about such data can lead to further understanding of the dynamics of various industries. \n",
      "\n",
      "Increasingly, geo-tagged social media data allow us to perform quantitative studies of different industries. In Hawelka et al, Geo-located Twitter as the proxy for global mobility patterns, a comprehensive study of global tourist mobility patterns was performed usign tweets over the period of a year leading to interesting insights about the global tourism industry. \n",
      "\n",
      "For this project, we aim to take such analysis to the next level through a comprehensive analysis of travel 'foot-prints' within a particular region. In this case, we choose the United States of America. We are interested in using social media data obtained through travel sites like Travel Blog to analyze the mobility patterns of tourists. \n",
      "\n",
      "In several regions across the world like Greece, Japan, Hawaii, tourism is a critical component of the local economy. Insights obtained from tourist mobility patterns and the development of a geo-spatial network of destinations can provide various stakeholders with useful knowledge regardingthe overall tourism industry in a region. \n",
      "\n",
      "Increasingly, countries like Japan are looking to 'big-data' techniques to discover behavioral patterns as potential means of optimizing outreach,  increasing the overall volume of incoming tourists, and enhancing the tourist experience. \n",
      "\n",
      "On a technical note, we aim to develop a city-by-city network of tourist flows in the US. Given such a network, different metrics can be computed which would help in the classification of various cities and regions as well as revealing different intricacies to travel in such a region. \n",
      "\n",
      "In addition to the city-by-city network, a critical part of this project is a study of traveller foot-prints/mobility traces in a given region. The mobility trace of a tourist describes the set of locations and corresponding times associated with a tourist's movement in a particular region. Through data about such traces, we can provide personalized descriptors as well as infrer probabilities of the path that a particular tourist would follow. We can also understand and potentially classify travellers based on their mobility profiles. \n",
      "\n",
      "Mobility Inference (next step prediction) and a classification of travellers is important because it can provide the host region with insights with which contextualized services can cater to different tourists. For example, if the city of New York knows that 200k people would visit between Jan - Feb and through a traveller trace classification knows the location visitation patterns of such tourists, the city can prepare adequately and work on providing relevant services to these travellers. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Process description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initially, we worked individually on our own notebooks.  During the exploration phase, we started\n",
      "to migrate our work into the main process notebook.  We decided from the beginning to follow the\n",
      "data science process outlined at the beginning of class.\n",
      "\n",
      "Project communication was over email with weekly conference calls.\n",
      "\n",
      "*Structure with interleaved/iterative approach.*  In keeping with the Data Science process, our\n",
      "approach is iterative.  Although we established a general project framework with stages, we also\n",
      "include considerations from different phases as they naturally arise in the process.  We did not\n",
      "seek to enforce artificial firewalls between phases.  For example, we include discussion about\n",
      "our thought process about things like modeling in the phases that such discussion actually occurred\n",
      "(for example, scraping).  We feel this gives a more natural and real presentation of our process."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Note on notebook outputs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to reduce the probability of merge conflicts as 4 team members collaborate on a single\n",
      "shared notebook, we run \"cell->all output->clear\" before committing.  This means that in order to\n",
      "view graphs, you need to run all cells.\n",
      "\n",
      "The notebook is intended to be able to run all cells fairly easily.  If certain cells take a long\n",
      "time to recompute, we will develop caches to be able to recover the output more quickly."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Boilerplate initialization and module imports"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following standard top code sets up our familiar defaults from the homeworks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This is from the class homeworks. It is really good for formatting graphs\n",
      "#No need to reinvent the wheel\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "import sys\n",
      "import os\n",
      "import csv \n",
      "\n",
      "import numpy as np\n",
      "import networkx as nx\n",
      "import requests\n",
      "from pattern import web\n",
      "import matplotlib.pyplot as plt\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import itertools\n",
      "import operator\n",
      "import pandas as pd\n",
      "import datetime\n",
      "from datetime import date, time\n",
      "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
      "from sklearn.datasets.samples_generator import make_blobs\n",
      "import pylab as pl\n",
      "from itertools import cycle\n",
      "\n",
      "import random\n",
      "import time\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "# We set axes.grid to True because we don't want to require the viewer to put a ruler up\n",
      "# against their monitor just to be able to get a precise quantity estimate.\n",
      "rcParams['axes.grid'] = True\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Data Procurement"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We chose initially to investigate procuring data from TravelBlog, Twitter, and tripadvisor."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.1 Description of TravelBlog and plan for scraping footprints"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TravelBlog is a website that hosts blogs of travelers describing their trips.  Each blogger has a name (\"Dirtdwellers\", for example), and posts blog entries about their travels.  Each blog entry has the following data:\n",
      "\n",
      "* Publish date\n",
      "* Title\n",
      "* Location path (\"Travel Blog \u00bb Europe \u00bb France \u00bb \u00cele-de-France \u00bb Acheres\", for example)\n",
      "* blogger name\n",
      "* blog text\n",
      "\n",
      "It would be difficult to extract regular, structured information from the blog texts.  However, because we know the location path of any blog, as well as the date and the blogger, we observed that we could crawl the site, recording rows with the date, location path, and blogger name.\n",
      "\n",
      "We could then correlate each location path with a geolocation.  This would be enough information to build per-blogger travel paths.  See the discussion below on maps for more information about\n",
      "geolocation.\n",
      "\n",
      "Potential problems include not being able to get enough data, and the risk that bloggers on travelblog.org may not be representative of all or even most travelers -- what about tourists and business travelers who don't blog on travelblog.org?\n",
      "\n",
      "Each blogger has a Maps page that gives geolocations of the blog postings.  More research needs to be done to confirm, but these appear to be attempts to automatically geolocate the blogger by IP, which may not be reliable -- even assuming the blogger is not using Tor.\n",
      "\n",
      "* Are the geolocations in the maps page generated from the IP address of the blogger?\n",
      "* Is the location path generated from the geolocation?\n",
      "\n",
      "*Julius confirmed later that the latitude and longitude values are standard entries for the location blogged about. That is to say, travelblogs.org appears to look up the latitude and longitude in a table using the location as a key.  Therefore the answers to these two questions are no and no.*\n",
      "\n",
      "For the moment we can say that this is information that we want, and we could converge to the following first scraping algorithm:\n",
      "\n",
      "    for each blogger:\n",
      "        for each (geolocation, blog id) pair:\n",
      "            scrape and store publish date, title, location path, and text along with blogger, geolocation, blog id\n",
      "\n",
      "Having identified a rough scraping plan, we look in more detail at the geolocation information\n",
      "available on a blogger's map page.  As an example, the user jdkinley's map page is here:\n",
      "\n",
      "http://www.travelblog.org/Bloggers/jdkinley/map.html\n",
      "\n",
      "Within this page is a bit of javascript with parallel arrays:\n",
      "\n",
      "    <script type='text/javascript'>\n",
      "\n",
      "    var placemarkers = '(37.7186,-77.3438),(32.2192,-80.6706), [...] ';\n",
      "    var placedata = '(264512,Getting started on the dream), [...] ';\n",
      "    var line = '';\n",
      "\n",
      "    </script>\n",
      "\n",
      "This data does not need to be collected on the first pass.  As long as first pass data has the username and the blog id, a second pass could look like the following:\n",
      "\n",
      "    for each user in the first pass collection,\n",
      "        scrape the placemarkers and placedata from that user's map page\n",
      "        join each resulting (latitude, longitude, blogid, blogtitle, userid) to existing data\n",
      "        \n",
      "This would almost certainly result in some \"holes\" (entries in original data without this additional data), but without trying this, we won't know.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.2 First scrape from TravelBlog"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned above, TravelBlog.com offers data from traveler traces within a region through their\n",
      "blogs.  The following exploratory code scrapes core data regarding traveler traces for ~53k blogs (footprints) in the USA from the last 10 years."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#receives a blog summary (DOM of its html code) and returns a dict with the blog's core data\n",
      "\n",
      "def get_data_from_blog_summary(blogSummaryDom):\n",
      "    \n",
      "    blogData = {}\n",
      "    \n",
      "    for part in blogSummaryDom.by_tag('div'): # blog summaries' html parts: header, body, footer\n",
      "        if 'class' in part.attributes:\n",
      "            classAttr = str(part.attributes['class'])\n",
      "            \n",
      "            if classAttr == \"blog_header\":\n",
      "                url = part.by_tag('a')[0].attributes['href']\n",
      "                blogData['number'] = url.split('blog-')[1].split('.')[0] #Get number\n",
      "                locationPath = url.split('blog-')[0]\n",
      "                blogData['location_path'] = locationPath[1:-1].split('/') #Get location_path\n",
      "                blogData['publication_date'] = part.by_tag('span')[0].content.split(': ')[1] #Get publication_date\n",
      "            \n",
      "            if (classAttr == \"blog_body left\") or (classAttr == \"blog_body right\"):\n",
      "                for span in part.by_tag('span'):\n",
      "                    if span.attributes['class']== 'author':\n",
      "                        blogData['blogger'] = span.by_tag('a')[0].attributes['href'].split('/')[-2] # get blogger\n",
      "                    if span.attributes['class']== 'blog_date': \n",
      "                        blogData['blog_date'] = span.content # get blog_date\n",
      "                    #not currently getting the blog content summary\n",
      "\n",
      "    return blogData\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code below performs the actual scrape.  We set the pages variable to the empty array to prevent doing another scrape from within the notebook.  This code is for historical/demonstration purposes only."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get all USA footprints/blogs\n",
      "\n",
      "blogsData = []\n",
      "pages = range(1,5)  # 5461 max; modify the range for scraping any subset\n",
      "\n",
      "pages = []\n",
      "\n",
      "for i, page in enumerate(pages):\n",
      "    \n",
      "    time.sleep(random.uniform(0,3))\n",
      "    urlPage = 'http://www.travelblog.org/North-America/United-States/blogs-page-' + str(page) + '.html'\n",
      "    pageHtml = requests.get(urlPage).text\n",
      "    domPage = web.Element(pageHtml)\n",
      "    \n",
      "    for blogSummary in domPage.by_tag('div'):\n",
      "        \n",
      "        #screen the html to get the blogs' summaries\n",
      "        if 'class' in blogSummary.attributes:\n",
      "            classAttr = str(blogSummary.attributes['class'])\n",
      "            if (classAttr == \"blog_panel summary_left \") or (classAttr == \"blog_panel summary_right \"):\n",
      "                blogsData.append( get_data_from_blog_summary(blogSummary) )\n",
      "    print i, page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The quoted code below shows is for historical interest only; it was used to create the initial\n",
      "'Travel_Blogs_USA_all.csv' file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# writes and reads the data to/from csv file\n",
      "# be careful, the first part overwrites the csv data file\n",
      "\n",
      "'''\n",
      "blog = blogsData\n",
      "with open('Travel_Blogs_USA_all.csv', 'wb+') as csvFile:\n",
      "    dict_writer = csv.DictWriter(csvFile, fieldnames=['location_path', 'publication_date', 'number', 'blog_date', 'blogger'])\n",
      "    dict_writer.writeheader()\n",
      "    dict_writer.writerows(blog)\n",
      "'''\n",
      "\n",
      "'''\n",
      "with open( 'Travel_Blogs_USA_all.csv', 'r') as csvFile:\n",
      "    #this reads the first line as the keys; we can add specific keys with: csv.DictReader( csvFile, fieldnames=<LIST HERE>, restkey=None, restval=None, )\n",
      "    csvDict = csv.DictReader( csvFile, restkey=None, restval=None, )\n",
      "    blogsDataFromCSV = [obj for obj in csvDict]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.3 Second scrape from TravelBlog (geolocations of traveler's footprints)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We seek the geolocation data for the blogs collected in the initial scrape.  We do this by first loading the csv into a Pandas file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df=pd.read_csv(\"Travel_Blogs_USA_all.csv\")\n",
      "\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then iterate through all bloggers, and for each blogger, we scrape and dump the geolocation information from their maps page into its own csv file.  We do this for several reasons, mainly if we crash or get interrupted, we don't want to have to go through the whole process again.\n",
      "\n",
      "We also cache requests for similar reasons.\n",
      "\n",
      "In the code below (disabled to prevent starting another scrape), the main loop iterates through each\n",
      "blogger, writing the geolocations of each user's blog into a csv file for that blogger.  In a later\n",
      "stage, we write a script to assemble the per-user blogs into a single .csv file.  See \"Iterative\n",
      "exploration, cleaning and generalization of the data\" below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: put John's library code in the notebook folder? so that the code is demonstrable"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import jabj.scrapers.travelblog as tb\n",
      "import os\n",
      "import pandas as pd\n",
      "import random\n",
      "import time\n",
      "\n",
      "bloggers = set(df['blogger'])\n",
      "\n",
      "# This scrape is of historical interest only.  We prevent doing another scrape by setting\n",
      "# bloggers to the empty set.\n",
      "bloggers = set()\n",
      "\n",
      "for blogger in bloggers:\n",
      "    blogger_csv_fnam = 'travelblog_per_user_maps/{}.csv'.format(blogger)\n",
      "    if os.path.exists(blogger_csv_fnam):\n",
      "        continue\n",
      "    blogger_df = tb.get_blogger_blog_coords(blogger)\n",
      "    if blogger_df:\n",
      "        blogger_df.to_csv(blogger_csv_fnam)\n",
      "    else:\n",
      "        print \"{} not found\".format(blogger)\n",
      "        \n",
      "    time.sleep(random.uniform(0,2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.4 Wrangling tourist data from tripadvisor.com"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: explain that we were exploring this possibility and why we decided to concentrate in TravelBlogs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tripadvisor.com has forums and reviews where people post data about particular cities.  Each user has a user id (for example, gatdaddy3), and a user's profile page has a map page.  The map page contains a bit of embedded javascript with geographic data.\n",
      "\n",
      "http://www.tripadvisor.com/TravelMap-a_uid.3BC5290E6AF16D4F1248DAE15634620F\n",
      "\n",
      "Users make per-location *contributions*, which can be either forum posts or reviews.  The user's map page has a list of locations for which the user has made contributions.  In order to get information about these contributions require another request.\n",
      "\n",
      "Stage 1 scrape:\n",
      "\n",
      "    for each user (within a certain range):\n",
      "        for each city that the user has made contributions to:\n",
      "            create a table entry (userid, cityid, cityname, longitude, latitude, members_citypage_url)\n",
      "            \n",
      "Getting dates associated with contributions would require a second request for each such entry to the members citypage url.  This would be done in a stage 2 scrape:\n",
      "\n",
      "    for each entry in tripadvisor_user_contributions table:\n",
      "        request member citypage\n",
      "        extract dates of contributions\n",
      "        \n",
      "Depending on the number of users we get, this scrape could take a lot more time, but it will provide us with the geolocation *and* the date.\n",
      "\n",
      "After initial scraping and exploration, we decided not to go with tripadvisor, and to focus on TravelBlogs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.5 Creation of a library"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to make scraping easier and more regular, we created a Python library for our project.  A scraper is a function that takes text input and returns and output.  The pattern becomes\n",
      "\n",
      "1.  Identify a new type of page you want to be able to scrape.\n",
      "2.  Get a sample page and put it in the test directory jabj/test/scrapers/XXXX, where XXXX is the next available number.\n",
      "3.  Update jabj/test/scrapers/db with a mapping of the number XXXX to the url.\n",
      "4.  Write a test script t_XXXX.py that contains two variables:  scraper and expected.  The top level test script will feed the input data to the scraper, and check that the output is correct.  This prevents the need to write repetitive boilerplate.\n",
      "\n",
      "In this way, we can build up a library of scrapers that will help us do higher level things like have multiple people scraping, and write functions that use scrapers in predictable and regular ways.\n",
      "\n",
      "The library is in the jabj subdirectory of the top-level project on github.  It has scrapers and\n",
      "tests for both tripadvisor and TravelBlogs.\n",
      "\n",
      "In the section \"Impediments to Collaboration\" of the paper by Kandel, Paepcke, Hellerstein, and\n",
      "Heer, the authors discuss a number of poor coding habits of data scientists.  The library reflects a\n",
      "conscious decision to follow some of the author's recommendations.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Use of requests_cache"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a Python module requests_cache that is useful for situations such as making it halfway through a scraping sequence and having your program crash for whatever reason.  The next time you run it, it runs much faster because you aren't re-requesting data you already have.  It is also much friendlier to the web servers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Data Structuring"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thinking about data storage and representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do we need an actual database for the data we collect?\n",
      "\n",
      "If our data becomes too large and unwieldy, then we should consider a database.  Until then, we should probably focus on the data collection and exploration itself, and see how far the IPython notebook + Pandas approach takes us.  It is after all what we have been using for the whole semester, so we know the tools.\n",
      "\n",
      "In order to be safe, we set up a database in the event that it becomes useful or necessary for\n",
      "later stages such as unsupervised learning."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Questions about the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to help inform the structuring of the data, we consider some of the questions we are\n",
      "seeking to answer.\n",
      "\n",
      "* What is $P(A|X)$ where $X$ a current location (or path), and $A$ is a following destination?\n",
      "* Can we predict a traveler's next stop based on where they are and have been?\n",
      "* Can we predict how long they will remain in a location given their previous itinerary?\n",
      "* Should we **not** confuse travel blogging with traveling, and instead make our questions about blogging:  \"Given that a blogger blogs about destination $X$, what is the probability they will blog about destination $Y$?\"  We could always say that based on what travel bloggers say, we think that travelers may be doing $X$, instead of conflating travel bloggers with travelers."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data structure is the Pandas data frame"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We settle on our current method of scraping data into Python data structures of some type, possibly\n",
      "Pandas, storing in .csv files which serve the function of tables in a relational framework, and\n",
      "reading the data in using ad-hoc scripts."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.1 Integrating and Reading the Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After collecting all .csv files to gather geolocation data, we joined the new .csv files with `Travel_Blogs_USA_all.csv` to create a new combined `.csv` file using a script (`assemble_maps_to_stage1.py`).  This script reads all csvs into Pandas dataframes, concatenates them, and then joins them using `pd.merge()` with the original dataframe to create the combined dataframe `Travel_Blogs_USA_combined.csv`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df=pd.read_csv(\"Travel_Blogs_USA_combined.csv\")\n",
      "\n",
      "df.describe()\n",
      "\n",
      "print df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Adding Some Indexes and Abstract Data Representations**\n",
      "\n",
      "We first perform some cleaning operations so that we can inspect some basic properties of this new dataset, and start to look at it in an abstract structural way.  Using the script `generalize-combined-csv.py`, we convert dates to datetime objects (`publication_date` and `blog_date`),\n",
      "reduce dates to timestamps (integers), and map locations and bloggernames both to integers, giving all of these mapped indices their own columns.  This allows us to temporarily strip away some information in order to view the problem in the abstract.\n",
      "\n",
      "We store the augmented data file in `Travel_Blogs_USA_combined_general.csv`.  The date now has a shape like the following."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "general_csv_fnam = 'Travel_Blogs_USA_combined_general.csv'\n",
      "general_df = pd.read_csv(general_csv_fnam)\n",
      "\n",
      "def print_properties(df):\n",
      "    properties = {}\n",
      "\n",
      "    display_cols = ['loc_id', 'pub_ts', 'blog_ts', 'blogger_id']\n",
      "    print df[display_cols].head()\n",
      "    from collections import defaultdict\n",
      "\n",
      "    print '-' * 78\n",
      "    properties[\"Number of unique locations: {}\"] = len(set(df.loc_id))\n",
      "    for k,v in properties.iteritems():\n",
      "        print k.format(v)\n",
      "\n",
      "print_properties(general_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.2 Framework for Accessing and Cleaning the CSV Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(TODO: overall explanation of what and why did we built this framework)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First we re-convert the datetimes from strings back to datetimes, so that we may\n",
      "# retrieve the year.  \n",
      "# We make a general \"thaw\" to recover our df from csv.  We will wrap this \"thaw\" function with a filtering \n",
      "# and cleaning function.\n",
      "\n",
      "def thaw_general_csv():\n",
      "    general_csv_fnam = 'Travel_Blogs_USA_combined_general.csv'\n",
      "    df = pd.read_csv(general_csv_fnam)\n",
      "    for k in ['publication_date', 'blog_date']:\n",
      "        df[k] = pd.to_datetime(df[k])\n",
      "    return df\n",
      "\n",
      "\n",
      "# Just a format converting function\n",
      "#import datetime\n",
      "def datetime_to_ts(dt):\n",
      "    \"\"\"Python has useful datetime objects that for some philosophical reason refuse \n",
      "    to offer a conversion utility to unix timestamps.  This function does that.\"\"\"\n",
      "    import calendar\n",
      "    return calendar.timegm(dt.utctimetuple())\n",
      "\n",
      "# The cleaning function. Clean-up features will be added here as we go further into the project.\n",
      "def general_df_clean(df):\n",
      "    \"\"\"A second processing stage to the thaw.\"\"\"\n",
      "    # Get rid of years before 2005.\n",
      "    ts_2005 = datetime_to_ts(datetime.datetime(2005,1,1))\n",
      "    \n",
      "    # Get rid of redundant or unneeded columns.\n",
      "    redundant_columns = ['Unnamed: 0', 'Unnamed: 0.1']\n",
      "    df = df.drop(redundant_columns, axis=1)\n",
      "    \n",
      "    return df[df.apply(lambda x: x['pub_ts'] >= ts_2005, axis=1)]\n",
      "\n",
      "def get_combined_raw_data():\n",
      "    \"\"\"Return the authoritative, agreed upon raw data.\"\"\"\n",
      "    return general_df_clean(thaw_general_csv())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our standardized framework for accessing and cleaning-up the CSV data, lets take a look at how our DataFrame looks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_combined_raw_data().head(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.3 Thinking About Data Representation for our Supervised Learning (*section 5*)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(TODOs:\n",
      "\n",
      "   - 1)check and edit this (following) two sub-sections for the final deliverable, e.g.: are there redundancies between the thoughts in the two sections?\n",
      "   - 2) add any missing parts of the text (I believe a cell was lost and couldn't recover it)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Modeling considerations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we start to look at this data, we recall that our goal is to try to predict where people are going to go next.  Perhaps we can transform the data into Markov chains in a few different ways.\n",
      "\n",
      "Option 1:  one to one correspondence between states and locations\n",
      "\n",
      "In this view, our transition matrix is from location to location.  We generate transition probabilities directly from the dataset.  For each current state $c$ and next state $s$,\n",
      "\n",
      "$$P(s|c) = \\frac{\\text{# entries where $s$ follows $c$}}{\\text{total # of entries where $c$ is current state}}$$\n",
      "\n",
      "Option 2:  one to one correspondence between prefixes and locations\n",
      "\n",
      "Under this view, the Markov states are not just locations, they are prefixes.  If we think of locations as letters and the set of all trails as a set of words, we can construct a Markov chain where the set of states is the set of all prefixes represented in the dataset.  A prefix is some first portion of a trail.  so abc representing Austin -> Boston -> Chicago is a trail with 3\n",
      "prefixes {a, ab, abc}.\n",
      "\n",
      "To be clear, we define a trail as a sequence of blog postings (which hopefully represent an actual sequence of a person's travel), but we speak here of prefixes temporarily as a thought tool to\n",
      "think of our data a little differently, to consider a possible transform into a more traditional\n",
      "Markov or automata-theoretic form.\n",
      "\n",
      "Each blogger has a sequence of posts tied to a location, and they are separated by some time period.  We could specify a time period threshold over which a trail gets broken in two, so that if someone blogs about a trip to the Grand Canyon one summer, then does nothing for 11 months, then blogs about a trip to Yellowstone, those would be considered two separate trails.\n",
      "\n",
      "This is not the only way we could view it.  We could also view each blogger as a single trail.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Modeling for Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data consists of (person, place, date) tuples.  The date forms a natural ordering on the data, so that if a date of one tuple precedes the date of another tuple for the same traveler, the second tuple will be \"reachable\" in some way from the first.\n",
      "\n",
      "From the data, we would be able to know several things:\n",
      "\n",
      "1.  The set of all places represented in a given person's blogs.\n",
      "2.  The sequence of locations represented in a given person's blogs.\n",
      "\n",
      "One second-level representation would be people as rows, place columns as boolean features.  The set of all places blogged about by all people would be the column set, and for each person/column combination, the value would be True if the person had been at that place, False otherwise.\n",
      "\n",
      "From this simple model, we would be able to train a classifier for each location pair A, B, $P(A|X)$, the probability that a person will have blogged about location A given the set of features X.  We could then do a similar machine learning application to the movies database, the most likely predictors of fresh or rotten, but with cities, the most likely locations to predict visiting city A or not visiting city A.\n",
      "\n",
      "This does not yet account for the time sequencing aspect desired, but starting to think in this way may help us build a set of modeling building blocks from which we may be able to construct such an estimator."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inference model 2:  travel paths as sequences, Markov chains"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we think of the set of all possible destinations as an alphabet $\\Sigma$, and as individual travel paths as strings over that alphabet, we can think of our problem as predicting letter $a_m$ of a word given the prefix $a_1,\\ldots,a_{m-1}$ of previous letters.\n",
      "\n",
      "We could use the input data to generate a Markov matrix of transition probabilities.  Markov chains depend only on the previous step to calculate the next step, but they are surprisingly effective in many cases.\n",
      "\n",
      "Two Markov approaches come to mind.  We could break our input data into a set of rows with two columns each, current step and next step.  We could then break that into training set, validation set and cross-validation set, and perform the normal supervised learning process to generate a classifier for next step given current step.  The result would basically be directly translatable into a Markov matrix giving transition probabilities from destination to destination.\n",
      "\n",
      "Alternatively, we could view the transition matrix as proceeding from word to word, rather than from letter to letter.  Under this view, our set of nodes in the transition matrix is the set of every prefix from our word corpus.\n",
      "\n",
      "It may be interesting to run both of these models and compare results."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How to break up travel paths, and a couple other problems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One problem we face is the fact that many of our travelers will almost certainly be taking multiple trips.  How do we identify their distinct trips, as opposed to viewing their sequence of blog posts as one long trip?\n",
      "\n",
      "We can configure a time threshold $T_{\\text{thresh}}$ such that if the time between blog posts exceeds $T_{\\text{thresh}}$, then the second blog post constitutes a new trail.\n",
      "\n",
      "Another problem is how do we know a traveler's original geolocation (home)?\n",
      "\n",
      "A third problem is how do we know that there aren't \"phantom\" destinations in trails -- destinations a blogger hasn't blogged about but that they visited?\n",
      "\n",
      "We should highlight both of these problems in our writeup as known consequences of our model, and state that we are modeling blog postings, not actual travel, and the two are not the same."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Trails from travelblogs.org are not the set of all travel trails, and may not even be representative"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we proceed further into our model, it is important to recall that this data is not a comprehensive record of travel flows.  We know that the set of trails established by bloggers on travelblogs.org is a subset of all travel flows, and almost certainly a very small subset that may not even be representative of \"real\" travel flows.  Consider the travelblogs.org dataset versus a credit card company's records of customer card swipes.  But we use this as a starting point to develop our inference model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Big Table, Part I"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a variety of reasons there emerged the concept of a single table to contain all data, which\n",
      "came to be known as \"The Big Table.\"  The Big Table meant something different to the individual\n",
      "project members, but one of the root causes of the desire for a big table was a sense that our data had become disparate, and there were not well-known ways to access the data.  We describe here the\n",
      "organization of our data.\n",
      "\n",
      "The combined raw is obtained from an *interface*, the function thaw_general_csv().\n",
      "The cleaned version of this data is obtained by passing the combined raw data to an *interface*, the\n",
      "function general_df_clean().\n",
      "\n",
      "If we think in terms of *interfaces*, we are not tied to a particular way of generating that data.\n",
      "Also, by thinking in terms of *interfaces* instead of *variables*, we help reduce the problem of\n",
      "global variables, which may change state through the notebook, leading to difficulty to reason\n",
      "about behavior.  By thinking in terms of *interfaces* rather than variables and immediate\n",
      "short-term tactical needs, we can ameliorate some of the general confusion that contribute to\n",
      "the desire for the Big Table.  These are basic software engineering concepts.\n",
      "\n",
      "In anticipation of the evolution of our concept of the Big Table, we define an *interface*, the\n",
      "function get_combined_raw_data, that can be used to return the current cleaned version of the raw\n",
      "data.  The raw data is the list of all posts, with the postprocessing to create ids for locations and dates.  We require that any changes to the combined raw data not break dependent code, and where\n",
      "it does, it will be fixed at the time of the change.\n",
      "\n",
      "It is anticipated that the Big Table will grow to include more synthetically created data, but it\n",
      "will start from the combined raw data.\n",
      "\n",
      "The function get_combined_raw_data is defined as follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Already defined on top:\n",
      "(TODO: edit the descriptions in this sections to tell a followable story, e.g.: \n",
      "include the relevant story about the interfaces from here to the appropriate section above)\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our resolution to think in terms of *interfaces* only addresses one part of the Big Table problem.\n",
      "Having defined an *interface* for the raw data, get_combined_raw_data(), we face the crucial and\n",
      "more difficult question, what, exactly, does the Big Table look like?\n",
      "\n",
      "As a first step, we make explicit the assumption that a single Big Table will suffice, and reject\n",
      "it.  Instead, we focus on what *a* Big Table might look like, and define such a table, leaving open\n",
      "the possibility that we may find that there may be more than one Big Table that suits our needs.\n",
      "\n",
      "Before we decided we needed a Big Table, we should have asked, \"What do we need a Big Table for?  What problem are we trying to solve?\"  There is not a single problem, but several:\n",
      "\n",
      "* How are we supposed to be accessing data?  This problem has been addressed above.  We use *interfaces* that return Pandas data frames.  Users of those frames don't need to worry about\n",
      "where they come from.  They only need to know what they are getting.\n",
      "\n",
      "* How are we going to use the data for unsupervised learning?  This is a different question, and\n",
      "we have agreed as a team that this has no single answer.  We will try more than one table structure\n",
      "below, but we think they will all have users or trails as rows, and features as columns.\n",
      "\n",
      "* How are we going to track our data in an organized way?  This issue is also addressed by\n",
      "programming to well-defined and well-known interfaces, and practicing good communication.\n",
      "\n",
      "In The Big Table, Part II, we will look at potential Big Tables for the purposes of supervised\n",
      "learning.  But first, we look at some other explorations and visualizations of the raw data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. Exploratory Analysis and Data Clean-Up"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.1 Some Data Clean-Up "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we get the data from the CSV file into a Data Frame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "general_df = thaw_general_csv()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How Old is our Data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first want to find the lowest and highest timestamps, i.e., the oldest and most recent blogposts; and sort our DataFrame in terms of the blog dates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_ts = min(general_df.pub_ts)\n",
      "max_ts = max(general_df.pub_ts)\n",
      "\n",
      "print \"min_ts: {}, max_ts: {}\".format(min_ts, max_ts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "general_df.sort_index(by='pub_ts', ascending=True).head(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that some timestamps are from 1970 due to absent data.  This suggests exploring the years a little more thoroughly.  Some initial inspection shows that it is probably best to throw out entries from years earlier than 2005."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Formatting function\n",
      "import matplotlib as mpl\n",
      "class Formatter(mpl.ticker.Formatter):\n",
      "    \"\"\"Matplotlib's \"design\" has a default behavior of representing everything in\n",
      "    scientific notation, for example the year 2006 on the x-axis would be represented\n",
      "    1+2.005e3.  This formatter works around that behavior.\n",
      "    \"\"\"\n",
      "    def __call__(self, x, pos=None):\n",
      "        return '%d' % x\n",
      "\n",
      "# histogram plot function\n",
      "def year_hist(df):\n",
      "    \"\"\"Create a year histogram of publication dates.\"\"\"\n",
      "    years=map(lambda x: x.year, df['publication_date'])\n",
      "    num_years = max(years) - min(years)\n",
      "    \n",
      "    plt.gca().xaxis.set_major_formatter(Formatter())\n",
      "\n",
      "    plt.xlabel('footprint year')\n",
      "    plt.ylabel('number of footprints')\n",
      "    plt.title('Histogram of the number of footprints per year')\n",
      "    \n",
      "    plt.hist(years, num_years)\n",
      "\n",
      "#We will use this function to plot pretty and big bar charts\n",
      "def plot_long_bar(diction,part, rng, figure_size, title, ylabel, xlabel):\n",
      "    d = sorted(diction.iteritems(), key=operator.itemgetter(1))\n",
      "    values = []\n",
      "    labels = []\n",
      "    new_d = []\n",
      "    \n",
      "    #top values\n",
      "    if part == 'Top':\n",
      "        a = len(d)-1\n",
      "        for i in range(rng):\n",
      "            new_d.append(d[a])\n",
      "            a = a-1\n",
      "        for a in new_d:\n",
      "            values.append(a[1])\n",
      "            labels.append(a[0])\n",
      "    #bottom values\n",
      "    else:\n",
      "        for i in range(rng):\n",
      "            new_d.append(d[i])\n",
      "        for a in new_d:\n",
      "            values.append(a[1])\n",
      "            labels.append(a[0])\n",
      "    \n",
      "    ax = plt.gca()\n",
      "    ax.tick_params(axis='x', colors = 'blue')\n",
      "    ax.tick_params(axis='y', colors = 'red')\n",
      "    s = pd.Series(values, index = labels)\n",
      "    pd.Series.plot(s, kind='bar')\n",
      "    rcParams['figure.figsize'] = (figure_size[0],figure_size[1])\n",
      "    \n",
      "    #Set descriptions:\n",
      "    plt.title(title)\n",
      "    plt.ylabel(ylabel)    \n",
      "    plt.xlabel(xlabel)\n",
      "    plt.show()\n",
      "    \n",
      "    rcParams['figure.figsize'] = (8,6)\n",
      "\n",
      "year_hist(general_df)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the histogram, we see that years before 2005 had minimal data, so we filter out years earlier\n",
      "than 2005. We use the general_df_clean function that we created before, in it we have included a filter for screening datapoints from before 2005. A histogram of the cleaned data shows a nicer\n",
      "distribution of data across years, with thousands of entries for each year.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "general_df = general_df_clean(thaw_general_csv())\n",
      "\n",
      "year_hist(general_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.2 How many places are the travellers going to?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(TODO: change this analysis to use the geneal_df structure)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some miscellaneous functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#misc functions to help with getting and processing of the data\n",
      "months = {'January': 1,'February': 2,'March': 3,'April': 4, 'May': 5 , 'June' : 6, 'July':7, 'August':8, 'September': 9, 'October':10, 'November':11, 'December' : 12 }\n",
      "\n",
      "def get_date(string):\n",
      "    parts = string.split()\n",
      "    month = int(months[parts[0]])\n",
      "    year = int(parts[2])\n",
      "    day = int(parts[1][:-2])\n",
      "    return datetime.datetime(year,month,day)\n",
      "\n",
      "def between_quotes(string):\n",
      "    value = string.find(\"'\")\n",
      "    if value == -1:\n",
      "        return string\n",
      "    value2 = string[value+1:].find(\"'\")\n",
      "    test = string[0:value+1]\n",
      "    a = len(test)\n",
      "    return string[value+1:value2+a]\n",
      "\n",
      "def get_city(location_path):\n",
      "    parts  = location_path.split(',')\n",
      "    if len(parts) < 4:\n",
      "        return None,None\n",
      "    else:\n",
      "        return between_quotes(parts[3]), between_quotes(parts[2])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the data and process\n",
      "def get_data(filename):\n",
      "    f = open(filename, 'r')\n",
      "    f.readline() #read the header line\n",
      "    data = [line.strip() for line in f.readlines()]\n",
      "    f.close()\n",
      "    return data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get usa data file\n",
      "usa_data = get_data(\"Travel_Blogs_USA_all.csv\")\n",
      "#mexico_data = get_data(\"Travel_Blogs_from_summaries_Mexico_all.csv\")\n",
      "print len(usa_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('Travel_Blogs_USA_all.csv')\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Begin Network Centric Exploration.\n",
      "\n",
      "For this section, we perform additional exploratory analysis based on the data we have. As indicated, in class, the goal of such exploratory analysis is to further help in developing adequate models. \n",
      "\n",
      "For this section, we describe the structure of the data in the following manner:\n",
      "\n",
      "\n",
      "From looking at the data, we see that the user ids are unique, so we store each user foot prints in a dictionary indexed  by the user id. \n",
      "\n",
      "They dicitonary is structured in the following manner:\n",
      "\n",
      "user_id : list of trips\n",
      "\n",
      "list of trips is a python list that has a tuple of city name, city state, and visitation date. \n",
      "\n",
      "We focus on a network centric exploration in this phase. \n",
      "To Recap:\n",
      "\n",
      "key ---> value\n",
      "blogger_id ---> list of trips\n",
      "\n",
      "list of trips = [a, b, c, ...]\n",
      "\n",
      "where a  = tuple(city_name, city_state, time of visitation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Here we build the datastructure described above to facilitate network centric\n",
      "#exploration. \n",
      "\n",
      "geo_cities = set()\n",
      "\n",
      "def structure_data(data):\n",
      "    \n",
      "    structured_data = {}\n",
      "    \n",
      "    #this is to keep track of blog post number\n",
      "    #to avoid duplicates.\n",
      "    post_number = set()\n",
      "    \n",
      "    for i in xrange(len(data)):\n",
      "        line = data[i]\n",
      "        \n",
      "        #check to make sure it is not an empty line\n",
      "        if line == '':\n",
      "            continue\n",
      "            \n",
      "        path = line[2:line.find(']')] #this gets the particular location visited\n",
      "        rest = line[line.find(']')+3:] #gets the remaining information, i.e. blog date, publication date, user id\n",
      "    \n",
      "        #check to make sure it has the information we need. \n",
      "        if path == '' or rest == '':\n",
      "            continue\n",
      "\n",
      "        rem = rest.split(',') #get the different fields\n",
      "        \n",
      "        #check that the data has the right number of fields. \n",
      "        if len(rem) < 4:\n",
      "            continue\n",
      "    \n",
      "        blog_entry_number = rem[1] #unique id for each blog entry\n",
      "        \n",
      "        #checks for duplicate records\n",
      "        if blog_entry_number in post_number:\n",
      "            continue #this is a duplicate, continue to the next line\n",
      "        \n",
      "        post_number.add(blog_entry_number) #keep track of the different blog post numbers\n",
      "        \n",
      "        visit_date = get_date(rem[2]) #parse the date and convert it python form using a function above\n",
      "        blogger_id = rem[3]  #get the unique blogger id\n",
      "        \n",
      "        #get city and state\n",
      "        visit_city , visit_state = get_city(path)\n",
      "        \n",
      "        #if data doesnt have city or state, then discard\n",
      "        if (visit_city == None) or (visit_state == None):\n",
      "            continue\n",
      "            \n",
      "        #check if user info has been added, if not initialize\n",
      "        if blogger_id not in structured_data.keys():\n",
      "            structured_data[blogger_id] = []\n",
      "\n",
      "        #add visit info, note, the same city can have multiple entries\n",
      "        #because users can return to the same city, dates would be\n",
      "        #different, here we also add the state name to the city name because \n",
      "        #there are cities with the same name across multiple states. \n",
      "        city_parameter = visit_city + ', ' + visit_state\n",
      "        geo_cities.add(city_parameter)\n",
      "        city_state_time = (visit_city,visit_state,visit_date)\n",
      "        structured_data[blogger_id].append(city_state_time)\n",
      "        \n",
      "    return structured_data\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get the data structure for the usa data set\n",
      "usa = structure_data(usa_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"There are \" + str(len(usa)) + \" unique travellers in the USA dataset\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use the usa variable below in the section \"How many places are the travellers going to?\".  First we look at some exploration of the combined TravelBlogs data, with some early modeling\n",
      "discussion."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How many places are the travellers going to?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start to visualize the data in some ways that will help us gain some intuition into questions of\n",
      "who's going where."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "city_visit_freq = {}\n",
      "no_of_unique_places_visited_per_user = []\n",
      "no_of_trips_made_per_user = []  #can have same city multiple times\n",
      "list_of_visited_city_set_for_each_user = [] #used to create edges among users\n",
      "\n",
      "#do histogram for city\n",
      "#how travellers are going to a particular city\n",
      "for person in usa.keys():\n",
      "    places = usa[person]\n",
      "    each_user_set = set()\n",
      "    \n",
      "    for place in places:\n",
      "        city_state_id = place[0] + \", \" + place[1]\n",
      "        each_user_set.add(city_state_id)\n",
      "    \n",
      "    list_of_visited_city_set_for_each_user.append(each_user_set)\n",
      "    no_of_unique_places_visited_per_user.append(len(each_user_set))\n",
      "    no_of_trips_made_per_user.append(len(places))\n",
      "    \n",
      "    #do frequency count for each city\n",
      "    for a in each_user_set:\n",
      "        if a in city_visit_freq.keys():\n",
      "            city_visit_freq[a] += 1\n",
      "        else:\n",
      "            city_visit_freq[a] = 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.xlabel('Number of Unique Cities Visited')\n",
      "plt.ylabel('Frequency')\n",
      "plt.title('Histogram of the number of unique cities visited per user')\n",
      "plt.grid(True)\n",
      "plt.axis([0, 50, 0, 6500])\n",
      "array = np.array(no_of_unique_places_visited_per_user)\n",
      "plt.hist(no_of_unique_places_visited_per_user, bins=len(np.unique(no_of_unique_places_visited_per_user)), facecolor='g', label='Dist Freshness')\n",
      "plt.show()\n",
      "\n",
      "print 'total number of \"trips\"/footprints (not counting repeated cities by traveler) '+str(sum(array))\n",
      "counts = np.bincount(array)\n",
      "print 'total number of travelers with three or more unique cities '+ str(sum(counts[3:]))+' out of ' + str(sum(counts))\n",
      "s=0\n",
      "for i in range(len(counts)):\n",
      "   if i >= 3:\n",
      "        s= s+ (i*counts[i]) \n",
      "moreThanN = array[array >= 3]\n",
      "# moreThanN = moreThanN[moreThanN <= 16]\n",
      "print 'total number of footprints in unique cities from travelers with more than 3 unique cities: ' + str(s)+ ' out of '+ str(sum(no_of_unique_places_visited_per_user))\n",
      "print 'Mean number of unique cities visited by travelers with 3 or more cities visited: ' + str(np.mean(moreThanN))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The histogram above shows that travelers from our dataset mostly visit less than 10 cities. In particular, 72% percent of these travelers visit 1-2 cities.\n",
      "\n",
      "73% of traveler footprints in our dataset belong to travelers that visited 3 or more unique cities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.xlabel('Total Number of Trips made')\n",
      "plt.ylabel('Frequency')\n",
      "plt.title('Histogram of the total number of trips made per user')\n",
      "plt.grid(True)\n",
      "plt.axis([0, 60, 0, 6000])\n",
      "array = np.array(no_of_trips_made_per_user)\n",
      "plt.hist(no_of_trips_made_per_user, bins=len(np.unique(no_of_trips_made_per_user)), facecolor='g', label='Trips')\n",
      "plt.show()\n",
      "\n",
      "print 'total number of travelers with three or more footprints '+ str(sum(counts[3:]))+ ' out of '+str(sum(counts))\n",
      "print 'total number of \"trips\"/footprints '+str(sum(array))\n",
      "counts = np.bincount(array)\n",
      "s=0\n",
      "for i in range(len(counts)):\n",
      "   if i >= 3:\n",
      "        s= s+ (i*counts[i]) \n",
      "moreThanN = array[array >= 3]\n",
      "print 'total number of footprints from travelers with more than 3 footprints: ' + str(s)+ ' of '+ str(sum(array))\n",
      "print 'Mean number of footprints by travelers with 3 or more footprints: ' + str(np.mean(moreThanN))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both histograms show similar characteristics. \n",
      "\n",
      "We notice though that a small subset of users seem to make multiple trips to the same places. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.3 Destination Rankings by Popularity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "let's look at the most and least visited cities in our data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"There are \" + str(len(city_visit_freq)) + \" unique cities in the data set\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Top 40 Cities visited in the US data Set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we present a bar chart of the most visited cities in the travelblog dataset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_long_bar(city_visit_freq, 'Top', 40, (8,6), 'City-Frequency Distribution', 'No. of Users that have visited','Cities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected we find that cities like New York City, Los-Angeles, San-Francisco, Las-Vegas, Chicago, and in the top most visited cities\n",
      "the travelblog dataset. It should also be noted that recreational places like national parks are also present in the data set. \n",
      "\n",
      "We also note the sharp drop off in the number of users visiting a particular city. NYC, LA, San-Francisco, and Las-Vegas have numerous amount \n",
      "of travellers in visiting them in this dataset. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bottom 70 Cities visited in the US data Set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we attempt to get a sense for the least visited cities in our data set. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_long_bar(city_visit_freq, 'Bottom', 70, (18,6), 'Least Visited Cities in the DataSet', 'No. of Users that have visited','Cities')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The bar chart above looks interesting because it consists of those cities that were only visited by one traveller in the entire travelblog dataset obtained.  \n",
      "\n",
      "This means that these cities wouldnot be connected to any other city in the city by city network of tourist flows.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Distribution of City Visitation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a dictionary of cities and the corresponding number of individuals visiting such city, let us attempt to look \n",
      "at the distribution of city visitation patterns. Such an histogram would provide answers to different questions.\n",
      "\n",
      "For example, are travellers visiting different cities at the same frequencies or are there a few concentrated cities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = sorted(city_visit_freq.iteritems(), key=operator.itemgetter(1))\n",
      "freq = []\n",
      "for a in d:\n",
      "    freq.append(a[1])\n",
      "plt.xlabel('Number of Cities')\n",
      "plt.ylabel('How many travellers are visiting the city')\n",
      "plt.title('Histogram of city visitation distribution')\n",
      "plt.grid(True)\n",
      "plt.axis([0, 500, 0, 3000])\n",
      "array = np.array(freq)\n",
      "plt.hist(array, bins=len(np.unique(array)), facecolor='g', label='City Visitation')\n",
      "plt.show()\n",
      "counts = np.bincount(array)\n",
      "counts[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the histogram above we see that a significant portion of this dataset visits only a few cities on a high rate. \n",
      "From our bar chart of the city popularities we can see that these cities are NYC, LA, San-Francisco, Las-Vegas, and Chicago. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4. Network Analysis\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A city-by-city network of tourist flows of the travelbloog dataset gives us an opportunity to perform quantitative network-based analysis thereby gaining an understanding of the relative importance of different cities through a computation of different measures that capture various properties about the network developed. Through centrality measures, we hope to better understand the relative importance of the different cities in our dataset. \n",
      "\n",
      "Finally, a cluster analysis of the network enables us to potentially segment the cities in groups helping to perhaps explain certain tourist patterns observed. For example, cities with high betweenness centrality in this dataset indicate that they are brigde locations across multiple cities. A clustering of the network can also present peculiarities about the nature of the cities themselves. \n",
      "\n",
      "Through an initial explore of various metrics on the network, we hope to better improve our predictions of a traveller future mobility trace. \n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.1 Generating the Network"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To build the initial network of tourist flows, we add an edge between two cities if a user as visited both cities. As we go along, we update the weights of edges across multiple cities if they share more users. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#obtain longitude and latitude information for the node attributes\n",
      "#move this up into data manipulation eventually. \n",
      "#talk about querying the google api\n",
      "infile_coord = open(\"Cities_lat_long_full_dataset\", \"r\")\n",
      "infile_coord.readline() #read the header line\n",
      "\n",
      "#build a dictionary of cities and the corresponding latitude and longitude. \n",
      "raw_lat_lon = [line.strip().split(';') for line in infile_coord.readlines()]\n",
      "city_latitude_longitude = {}\n",
      "for a in raw_lat_lon:\n",
      "    city_latitude_longitude[a[0]] = {}\n",
      "    city_latitude_longitude[a[0]]['lat'] = a[1]\n",
      "    city_latitude_longitude[a[0]]['lng'] = a[2]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create a network\n",
      "G = nx.Graph()\n",
      "\n",
      "#add all the cities/nodes to the network\n",
      "for user in usa.keys():\n",
      "    #get list of places visited for each user\n",
      "    places = usa[user]\n",
      "    #go through the list and add all the cities\n",
      "    #duplicates don't matter because networkx uses a set to store this information\n",
      "    for place in places:\n",
      "        nodeid = place[0] + ', ' + place[1] #combine the city and state name to avoid cities with similar names across states\n",
      "        G.add_node(nodeid, city = place[0], latitude = float(city_latitude_longitude[nodeid]['lat']), longitude = float(city_latitude_longitude[nodeid]['lng']), state = place[1]) \n",
      "                                               #for differentiate same name cities in different states\n",
      "#add the edges to the network\n",
      "#similar algo to the one used in class\n",
      "#Go through the set of cities a user as visited and add edges among these cities\n",
      "#updating the weights as needed\n",
      "for a in list_of_visited_city_set_for_each_user:\n",
      "    lcity = list(a)\n",
      "    edges_to_add = list(itertools.combinations(lcity,2))\n",
      "    for u in edges_to_add:\n",
      "        #check if the edges exists already and increment weight\n",
      "        if u[1] in G.neighbors(u[0]):\n",
      "            G[u[0]][u[1]]['weight'] = G[u[0]][u[1]]['weight'] + 1\n",
      "            G[u[0]][u[1]]['difference'] = 1.0/float(G[u[0]][u[1]]['weight']) #the more edges two cities have the more alike they are\n",
      "        else:\n",
      "            G.add_edge(u[0], u[1], weight = 1.0, difference = 1.0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"There are \" + str(len(G.nodes())) + \" nodes/cities in the network. \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with any network, a degree rank plot would enable us visualize the way in which the degree of each node or city changes in the network. \n",
      "\n",
      "Let us look at the degree rank plot \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "degree_sequence=sorted(nx.degree(G).values(),reverse=True) # degree sequence\n",
      "#print \"Degree sequence\", degree_sequence\n",
      "dmax=max(degree_sequence)\n",
      "\n",
      "plt.loglog(degree_sequence,'b-',marker='o')\n",
      "plt.title(\"Degree rank plot\")\n",
      "plt.ylabel(\"degree\")\n",
      "plt.xlabel(\"rank\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the network has a few nodes with very high degrees and has a relatively signicant connected component which constitute most of the network. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.xlabel('Number of Cities')\n",
      "plt.ylabel('Degree')\n",
      "plt.title('Histogram of the number of cities with a particular degree')\n",
      "plt.grid(True)\n",
      "plt.axis([0, 1000, 0, 2500])\n",
      "array = np.array(degree_sequence)\n",
      "plt.hist(array)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A histogram of the result below also communicates the same story, i.e, few nodes with very high degrees and a significant portion of the nodes with low degrees in a 'power-law' like behavior. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.2 Network Visualization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " A critical component of a network based approach or even any data analysis approach is \n",
      " to find ways in which to visualize the data so as to better inform the development of \n",
      " models regarding the behaviour being studied. \n",
      "\n",
      "Given such approach we explore different network visualization software and approaches, \n",
      "so as to better understand our data. \n",
      "\n",
      "As shown in class, when the network size is bigger than 100, it starts to become difficult \n",
      "to visualize the network, particularly in networkx. Given this situation, we looked to other\n",
      "alternatives to visualize our networks. Here we present a visual exploration of the city-by-city\n",
      "network of tourist flows in Gephi, processing, and D3js. \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "let's preprocess the network and get it in gephi. \n",
      "this takes a while, so we don't need to run it every time.  \n",
      "''' \n",
      "#We comment it out for now.\n",
      "#nx.write_gexf(G, 'Full_City_By_City_Network_With_All_Nodes.gexf')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let us now aggregate nodes with no edges\n",
      "print \"There are \" + str(len(G.nodes())) + \" Nodes in the network \"\n",
      "Nodes_With_No_Edges = []\n",
      "#remove all the nodes that have no edges\n",
      "for a in G.nodes():\n",
      "    if len(G.neighbors(a)) == 0:\n",
      "        G.remove_node(a)\n",
      "        Nodes_With_No_Edges.append(a)\n",
      "print \"There are \" +  str(len(Nodes_With_No_Edges))+ \" Nodes with no connections, leaving \" + str(len(G.nodes())) + \" Nodes.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#We comment it out for now.\n",
      "#save a list of nodes with no edges\n",
      "'''\n",
      "out_file = open(\"List_of_Nodes_With_No_Connection\", \"w\")\n",
      "for a in Nodes_With_No_Edges:\n",
      "    out_file.write(a + \"\\n\")\n",
      "    \n",
      "#preprocess a gephi network with just the nodes that are connected to one another\n",
      "nx.write_gexf(G, 'Cities_Network_With_Only_Nodes_With_Edges.gexf')\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Analysis in Gephi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As seen in class, gephi is a network exploratory and visualization tool. Gephi provides different layouts that can be applied to a network further giving it meaning. In our case, given the geo-spatial nature of the data, we can look at the distribution of nodes on a geo-spatial layout. \n",
      "\n",
      "Visualizing the network on a geo-spatial layout allows us to futher make sense of the network and see how the nodes are distributed across a particular area given the spatial dimension of the data.\n",
      "\n",
      "Each node in the network has latitude and longitude values, which are used to indicate the position of the node relative to other nodes on a layout. In this case, it allows us to get a spatial representation of the different cities in out data set. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What states comprise the cities of the city-by-city network\n",
      "\n",
      "Add more text\n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_State.png\">\n",
      "\n",
      "<img src=\"files/Gephi_Files/Legend_Color_By_State.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "<img src=\"files/Gephi_Files/Network_Degree_Chart.png\">\n",
      "\n",
      "The image above is the network the city-by-city network of how dataset with a geospatial layout. Each node is plotted on a map in \n",
      "its relative position based on the longitude and latitude information of the node. The network is also colored based on the degree of\n",
      "each node. \n",
      "\n",
      "*The darker green nodes are nodes with a higher degree*, while the lighter nodes are have a smaller degree (undirected network). We quickly \n",
      "see that all our nodes seem to be distributed all over the united states including Alaska and Hawaii. We also see that a geo-spatial layout\n",
      "might not be the best method to help segment or potentially distinguish between groups of cities. \n",
      "\n",
      "One critical question one might ask is whether applying a modularity algorithm to the network would yield some form of segmentation between \n",
      "groups of nodes. For example, will it be the case that all the nodes on the east coast would be in a specific module? Would such segmentation \n",
      "be along regional lines? We explore these questions in the remainder of this section. \n",
      "\n",
      "Before we proceed, we also wanted to visualize cities in our network that are not connected to any other cities. Based on the way the network is built, these are cities that register only one footprint on the travelblog. It indicates that only one individual is visiting all of these cities. Below we show a geo-spatial layout of all the isolated cities in our dataset.\n",
      "\n",
      "<img src=\"files/Gephi_Files/Networ_Cities_No_Edges.png\">\n",
      "\n",
      "We see that the nodes seem spread over the entire geographical area with more concentration in the east coast. We note that most of these cities are relatively obscure ones that explaining why a lot of travellers might not know about them. Examples include: Colcord, Ok; Niles, IL and others that are as obscure cities around the US. The isolation of the nodes also comes from the fact that the travellers that visited these nodes did not continue further travels to anywhere else. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Modularity\n",
      "\n",
      "Modularity is a measure of how partitioned a network's structure is. It is a measure designed to account for the division of a network into modules or communities. Networks with high modularity have dense connections between the nodes within a particular module but sparse connections between nodes in different modules. Modularity is often used in optimization methods for detecting community structure in networks.\n",
      "\n",
      "Gephi has a modularity algorithm implemented that performs community detection. In studying the structure of our network, we decided to \n",
      "apply it to the spatial network we have in order to see whether the is division on spatial lines. \n",
      "\n",
      "<img src=\"files/Gephi_Files/Full_Network_Modularity.png\">\n",
      "\n",
      "The resulting spatial network is colored according to modules, so nodes belonging to the same module have the same color. We present the same\n",
      "network but with a different set of colors below again. \n",
      "\n",
      "<img src=\"files/Gephi_Files/Full_Network_Modularity2.png\">\n",
      "\n",
      "The result obtained is shown in the pictures above. Each module in the network is colored differently. Repeated runs of the modularity algorithm in gephi shows a division of the network into between 8-20 modules on a whole. We also see that there really isn't a definite divison structurally to the network on a spatial level. For the most part, we can say nodes of the same color seem to aggregate together, however the results aren't conclusive based on the pictures above. In addition, it has been shown that modularity algorithm suffers a resolution limit and, therefore, it is can have trouble detecting small communities. Given such results, we decided to take a systematic approach to community detection and clustering of our network. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Network Clustering\n",
      "\n",
      "Another approach to segmentation would be to take a look at the network using a different layout. This approach would ignore the geospatial \n",
      "component of the network initially, given that we don't see a straighforward segmentation on spatial lines. Here we first apply a force directed layout algorithm to the network in order to move highly connected components of the network together and sparse portions of the network apart. This helps in realizing the substructure of the overall network. \n",
      "\n",
      "Force-directed algorithms help to map nodes of a graph to a 2 dimensional (in this case) space so that all the nodes that are highly connected are grouped together, and those that are sparsely connected far away from each other. This is done through assigning forces among the set of edges and nodes such that closely connected communities separate out of the entire network. In a way, this helps in potentially identifying the sets of communities that are present in the graph. \n",
      "\n",
      "Gephi has several different force directed algorithms. We decided to go with an algorithm called open ord because it is optimized for thousands of nodes and reveals interesting results about the network. The OpenOrd algorithm is uniquely fitting for this approach since it allows for a variety of parameters to be set leading to formation of different communities. \n",
      "\n",
      "Below we show a picture of the openord algorithm applied to the city by city network of tourists. We immediately notice that we can uniquely identify different cluster communities that form the overall network. This is usefual because it enables us to group cities into different groups based on their connectivity, i.e, tourist flows between them. \n",
      "\n",
      "<img src=\"files/Gephi_Files/OpenOrd Layout Gephi Full Network.png\">\n",
      "\n",
      "From the image above, we immediately see that the network is composed of a giant community on the left and more distinct communities on the right. This is useful because it enables us to identify the components of our network. To further test the gephi modularity algorithm implemented on the geo-spatial layout, we try it here as well. The results are shown below. \n",
      "\n",
      "<img src=\"files/Gephi_Files/OpenOrd_With_Modularity.png\">\n",
      "\n",
      "We immediately see that the modularity algorithm identifies to a certain extent the smaller communities part of the overall network. However, the giant component of the network is shown as consisting of several different modules. This gives us an interesting characterization of the entire picture as it shows that there are distinct communities and a major component consisting of several different nodes that could also form distinct communities. We abandon the modularity approach and now perform a quantitative clustering of the force directed network produced by Gephi. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Clustering Analysis\n",
      "\n",
      "To proceed in our analysis and arrive at a segmentation of the network, we will cluster the network. Here we describe the approach we take towards clustering the network. \n",
      "\n",
      "As indicated earlier, applying a force-directed algorithm, openord in this case, to a network projects the network to a 2d space. In projecting such a network to two dimensional space the distance between the nodes can then be thought of as a measure of similarity between those two nodes. We use this notion of distance between nodes as one of the critical measures of similarity between the nodes. \n",
      "\n",
      "In projecting the network to a 2d space, each node is assigned an (x,y) coordinate on an axes. As a result of this, the problem of clustering the network becomes that of clustering a set of 2d points on a plane. In addition, the distance between the points is used as the main clustering parameter, ie, a measure of similarity between the points. Two points are more similar if they are closer together. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Based on the work done in gephi and the pictures above, we can export a graph file\n",
      "from gephi that includes the descriptions of the nodes, so that each node has an (x,y)\n",
      "value. We take these values and then cluster them. \n",
      "\n",
      "More on clustering descriptions to come. \n",
      "'''\n",
      "#this function reads a gephi gdf file and then returns the\n",
      "#node labels and the x,y coordinates of each node for \n",
      "#clustering. \n",
      "\n",
      "def return_nodelabels_coords(filename):\n",
      "    #read in the data\n",
      "    graph_data = get_data(filename)\n",
      "    \n",
      "    #define variables.\n",
      "    graph_coords = []\n",
      "    node_labels = []\n",
      "    \n",
      "    #parse the files\n",
      "    for line in graph_data:\n",
      "        fields = line.split(',')\n",
      "        \n",
      "        #consider only the node sections\n",
      "        if len(fields) < 10:\n",
      "            continue\n",
      "    \n",
      "        node_label = fields[0] + ',' + fields[1]\n",
      "        x = float(fields[6])\n",
      "        y = float(fields[7])\n",
      "        graph_coords.append((x,y))\n",
      "        node_labels.append(node_label)\n",
      "        \n",
      "    return node_labels, graph_coords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get node_labels and graph coordinates from the graph file\n",
      "filename = \"Gephi_Files/Raw_Graph_File_For_Clustering.gdf\"\n",
      "node_labels, graph_coords = return_nodelabels_coords(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### How to Cluster?\n",
      "\n",
      "As indicated throughout the entire course, clustering is the task of grouping a set of objects in such a way that similar objects are grouped together into groups or clusters based on some similarity metric. Here the goal is to cluster a set of points based on distance. This is a well understood problem for which there are several methodologies. \n",
      "\n",
      "Before we proceed, it is important to take a moment to explain the underlying assumption in this clustering methodology. Here, we are assuming that the force directed algorithm used to layout the graph brings similar nodes to one another and disimilar nodes far apart. This is implicit because the force directed algorithm reduces the euclidean distance between sets of nodes that are highly connected in 2d space. This means that our network of city-by-city tourist flows can be clustered into groups based on this. \n",
      "\n",
      "Now we move on to clustering methodology proper. There are several methods that can be used in this case such as K-means clustering, Affinity propagation, Mean-Shift, Spectral clustering among others. We decided to go with the Mean-shift clustering methodology because it can be used to estimate the number of clusters and the geometry used is the distance between the points being clustered. \n",
      "\n",
      "Mean-Shift helps to cluster data by estimating the number of blobs and the blobs themselves in a smooth density of points matix. In this case, the matix of (x,y) coordinates of the different nodes is the points matrix. It's implementation in the scikit learn library includes a utility function which can be used to estimate the optimal bandwith for the meanshift, hence the number of clusters. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##This code borrows from the scikit learn example\n",
      "##of the mean_shift clustering methodolgy\n",
      "def mean_shift_clustering(coords,node_labels):\n",
      "    X = np.array(coords)\n",
      "\n",
      "    #The following bandwidth can be automatically detected using\n",
      "    bandwidth = estimate_bandwidth(X, quantile=0.1, n_samples=None, random_state=0)\n",
      "\n",
      "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
      "    ms.fit(X)\n",
      "    labels = ms.labels_\n",
      "    cluster_centers = ms.cluster_centers_\n",
      "\n",
      "    labels_unique = np.unique(labels)\n",
      "    n_clusters_ = len(labels_unique)\n",
      "\n",
      "    print \"The number of estimated clusters : %d\" % n_clusters_\n",
      "    print \"The total number of points clustered is  : \" + str(len(labels))\n",
      "    \n",
      "    #plot the clusters\n",
      "    pl.figure(1)\n",
      "    pl.clf()\n",
      "\n",
      "    colors = cycle('rbgymrkbgmkcmrkbgrcmykbgrcmyk')\n",
      "    for k, col in zip(range(n_clusters_), colors):\n",
      "        my_members = labels == k\n",
      "        cluster_center = cluster_centers[k]\n",
      "        pl.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
      "        pl.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,markeredgecolor='k', markersize=14)\n",
      "    pl.title('Estimated number of clusters: %d' % n_clusters_)\n",
      "    pl.show()\n",
      "    \n",
      "    #write clustering results to a file\n",
      "    outfile = open(\"City_Cluster_Number\", \"w\")\n",
      "    outfile.write('Id' + ';' + 'Cluster_No' + '\\n')\n",
      "    for i in range(len(node_labels)):\n",
      "        outfile.write(node_labels[i] + ';' + str(labels[i]) + '\\n')\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean_shift_clustering(graph_coords,node_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/Gephi_Files/OpenOrd Layout Gephi Full Network.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: The blob in each cluster represent the center of each cluster. \n",
      "\n",
      "Above we see the result of the meanshift clustering approach. A side by side comparison of both pictures indicates that the clustering captures the different seemingly distinct clusters in the network. Given such a clustering we can now examine the different clusters to see on what level our data segments. As we further talked about in class, clustering is a subtle art because there is no necessarily right answer or approach, it just depends on what produces meaningful descriptions. \n",
      "\n",
      "Looking at the new clustering methodology on a geospatial layout we see: \n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_Cluster_Geo_Layout.png\">\n",
      "\n",
      "<img src=\"files/Gephi_Files/Cluster_Legend.png\">\n",
      "\n",
      "\n",
      "\n",
      "Now we produce to look that the different cluster descriptions. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cluster Descriptions \n",
      "To Do: Give \n",
      "##### Cluster 0\n",
      " Largest connected component, to reconsider and recluster. \n",
      "\n",
      "##### Cluster 1\n",
      "'Cullman, Alabama', 'Tarpon-Springs, Florida', 'Long-Key, Florida', 'Hixson, Tennessee', 'Orange-Beach, Alabama', 'Somerset, Kentucky', 'Apalachicola, Florida', 'Fulton, Mississippi', 'Thousand-Islands, New-York', 'Fort-Myers-Beach, Florida', 'Safford, Arizona', 'Pompano-Beach, Florida', 'Steinhatchee, Florida', 'Okeechobee, Florida', 'Columbus, Mississippi', 'Cumberland-Gap, Tennessee', 'Hoquiam, Washington', 'Port-Orange, Florida', 'Key-Colony-Beach, Florida', 'Aberdeen, Mississippi', 'Artesia, New-Mexico', 'Cedar-Key, Florida', 'Weatherford, Texas', 'Miami-Springs, Florida', 'Calera, Alabama', 'Indian-Harbour-Beach, Florida', 'Clinton, Oklahoma', 'Gulf-Breeze, Florida', 'Hillsborough-River, Florida', 'Live-Oak, Florida', 'Eastport, Maine', 'Brattleboro, Vermont', 'Sanford, Florida', 'Demopolis, Alabama', 'Palatka, Florida', 'Longboat-Key, Florida', 'Navarre, Florida'\n",
      "\n",
      "##### Cluster 2\n",
      "'Dillon, Montana', 'Okemah, Oklahoma', 'Alameda, California', 'Tuskegee, Alabama', 'Stevenson, Washington', 'Fort-Huachuca, Arizona', 'Southport, North-Carolina', 'Kaneohe, Hawaii', 'Raton, New-Mexico', 'Natchitoches, Louisiana', 'Weatherford, Oklahoma', 'Dayton, Tennessee', 'Roseville, California', 'Clayton, New-Mexico', 'Dumas, Texas', 'Wytheville, Virginia', 'Manteo, North-Carolina', 'Lordsburg, New-Mexico', 'Glen-Ellyn, Illinois', 'Willcox, Arizona', 'Claremore, Oklahoma', 'Shawnee, Oklahoma', 'Hillsboro, Oregon', 'Nogales, Arizona', 'Sierra-Vista, Arizona', 'Patagonia, Arizona', 'El-Reno, Oklahoma', 'Saint-Robert, Missouri', 'Logan, West-Virginia', 'Vancouver, Washington', 'Hot-Springs, Virginia', 'Santa-Maria, California', 'Warrenville, Illinois', 'Escondido, California', 'Globe, Arizona', 'Sun-City, California', 'Columbia, Tennessee', 'Gaffney, South-Carolina'\n",
      "\n",
      "##### Cluster 3\n",
      "'Saco, Maine', 'Deer-Isle, Maine', 'Isle-au-Haut, Maine', 'Lenox, Massachusetts', 'Shelburne-Falls, Massachusetts', 'Skykomish, Washington', 'Sandwich, Massachusetts', 'Wenatchee, Washington', 'Quincy, Massachusetts', 'Castine, Maine', 'Harvard, Massachusetts', 'Lincoln, Massachusetts', 'Yachats, Oregon', 'Williamstown, Massachusetts', 'Newbury, Massachusetts', 'St-Mary, Montana', 'Topsfield, Massachusetts', 'Cornish, New-Hampshire', 'Stockbridge, Massachusetts', 'Beverly, Massachusetts', 'Stonington, Maine', 'Revere, Massachusetts', 'Lubec, Maine'\n",
      "\n",
      "##### Cluster 4\n",
      "'Glenwood, New-Mexico', 'Dickinson, North-Dakota', 'Smithville, Texas', 'Colcord, Oklahoma', 'Sheperd-s-Hill-Farm, Georgia', 'Lawton, Michigan', 'Talladega, Alabama', 'Tuolumne, California', 'Martin, Georgia', 'Newport, Tennessee', 'Ocean-Park, Washington', 'Hawkins, Texas', 'Louisville, Ohio', 'Pharr, Texas', 'Rexford, Kansas', 'Giddings, Texas', 'Jackson, Kentucky', 'Keystone-Heights, Florida'\n",
      "\n",
      "##### Cluster 5\n",
      "'Port-Isabel, Texas', 'Smithland, Kentucky', 'Bonham, Texas', 'Cestohowa, Texas', 'Cooper, Texas', 'Phelan, California', 'Ardmore, Oklahoma', 'Gothenburg, Nebraska', 'Marion, Kentucky', 'Cadiz, Kentucky', 'Paducah, Kentucky', 'Cripple-Creek, Colorado', 'Salem, Kentucky', 'Glendale, Kentucky', 'Whitesboro, Texas', 'Anna-, Texas', 'Ponca-City, Oklahoma', 'Mineola, Texas', 'Krum-, Texas', 'Hurricane-Mills, Tennessee', 'Greensburg, Indiana', 'Thackerville, Oklahoma', 'Jonesborough, Tennessee', 'Palestine, Texas'\n",
      "\n",
      "##### Cluster 6\n",
      "Beckley, West-Virginia', 'Winter-Park, Florida', 'Huntington-Beach-, South-Carolina', 'Mount-Airy, North-Carolina', 'Plant-City, Florida', 'Bushnell, Florida', 'Brunswick, Georgia', 'New-Bedford, Massachusetts', 'Tell-City, Indiana', 'Trenton, New-Jersey', 'DeLand, Florida', 'Bellows-Falls, Vermont', 'Saratoga, Wyoming', 'Pinellas-Park, Florida', 'Kingsland, Georgia', 'North-Little-Rock, Arkansas', 'Haines-City, Florida', 'Merrimack, New-Hampshire', 'Wellsville, New-York', 'Yemassee, South-Carolina', 'Conover, North-Carolina', 'Winchester, Virginia', 'Auburndale, Florida', 'Seffner, Florida', 'Ogallala, Nebraska'\n",
      "\n",
      "##### Cluster 7\n",
      "Ninilchik, Alaska', 'Morehead, Kentucky', 'Wilmot, Ohio', 'Dover, Ohio', 'Mercer, Pennsylvania', 'Elgin, Arizona', 'Wellsboro, Pennsylvania', 'Kingsville, Texas', 'Sonoita, Arizona', 'Harrodsburg, Kentucky', 'Cordova, Alaska', 'Girdwood, Alaska', 'Berlin, Ohio', 'Sandy, Utah', 'Glacier-Bay, Alaska', 'Window-Rock, Arizona', 'Aransas-Pass, Texas', 'Olive-Branch, Mississippi', 'Maysville, Kentucky', 'Owingsville, Kentucky', 'Bucksport, Maine', 'Grass-Valley, California', 'Haines, Alaska', 'Gallipolis, Ohio', 'Amado, Arizona', 'Concordia, Kansas', 'Kenai-Fjords-National-Park, Alaska', 'Slade, Kentucky'\n",
      "\n",
      "\n",
      "##### Cluster 8\n",
      "De-Funiak-Springs, Florida',  'Edmond, Oklahoma', 'Newnan, Georgia', 'Selma, Alabama', 'Delray-Beach, Florida', 'Andersonville, Georgia', 'Springfield-Plantation, Mississippi', 'Converse, Louisiana', 'Larned, Kansas', 'Liberal, Kansas', 'Port-Gibson, Mississippi', 'Patterson, Louisiana', 'Garfield, Kansas', 'Kinsley, Kansas'\n",
      "\n",
      "##### Cluster 9\n",
      "Bay-City, Michigan', 'Whitehall, Michigan', 'Chiefland, Florida', 'Muskegon, Michigan', 'Bowling-Green, Ohio', 'Greenville, Michigan', 'Madison, Florida', 'Port-Huron, Michigan', 'Mansfield, Ohio', 'Xenia, Ohio', 'Midland, Michigan', 'Saginaw, Michigan\n",
      "\n",
      "##### Cluster 10\n",
      "\u2018Corning, New-York', 'Jefferson-National-Forest, Virginia', 'Depauville, New-York', 'Troutdale, Virginia', 'Damascus, Virginia', 'Catawba, Virginia', 'Hot-Springs, Tennessee', 'Bland, Virginia', 'Enterprise, Alabama', 'Pearisburg, Virginia', 'Skyland, Virginia', 'Big-Island, Virginia'\n",
      "\n",
      "##### Cluster 11\n",
      "Fort-Benton, Montana', 'Missouri-Valley, Iowa', 'Lewistown, Montana', 'Wagner, South-Dakota', 'Chamberlain, South-Dakota', 'Park-Rapids, Minnesota', 'Mobridge, South-Dakota'\n",
      "\n",
      "##### Cluster 12\n",
      "Boyne-Falls, Michigan', 'Brighton, Colorado', 'Charlevoix, Michigan', 'Pinehurst, North-Carolina', 'Northglenn, Colorado', 'Decorah, Iowa'\n",
      "\n",
      "##### Cluster 14\n",
      "Murray, Utah', 'West-Jordan, Utah','Sulphur, Louisiana', 'Livingston, Texas'\n",
      "\n",
      "##### Cluster 13\n",
      "'Somerset, New-Jersey', 'Arlington, Massachusetts', 'Southbury, Connecticut'\n",
      "\n",
      "##### Cluster 15\n",
      "West-Lafayette, Indiana', 'Lafayette, Indiana'\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.3 Network Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(brief description of the analysis to be performed)\n",
      "\n",
      "(\n",
      "\n",
      "Explain the **meaning** of network clustering in the context of our network: communities of destinations with high internal traveler flows but low external traveler flows\n",
      "\n",
      "Explain the **relevance** of such a classification:\n",
      "\n",
      "- Relevance for **understanding demand** of transportation infrastructure and demand for integrated touristic services and packages.\n",
      "- Identification of potential **synergies and cooperation** among destinations. \n",
      "- Relevance for making **inferences of traveler \"next steps\"** using the cluster classification of the \"first steps\".\n",
      "\n",
      ")\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.1 Node Statistics and Rankings"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Edge Weight as Indicator of Transportation Demand"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Edge weight in our network represents the connection of two touristic destinations in terms of the number of common travelers that visited them. Hence, a ranking of pairwise edges can serve as an adequate source of information for uncovering transportation demand among destinations. Such a ranking can then be campared with the existing transportation infrastructure as means to identifying transportation gaps, relevant to both policy makers and businesses in the touristic and transportation industries.\n",
      "\n",
      "In this section we will rank pairwise edges, but will restrict ourselves to those connecting destinations at __x__ or more kms of distance (we will need latitutde and longitude of destinations). We will screen in such a way with the purpose of using our ranking as proxy for traveler demand for direct flights connecting touristic destinations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Through Gephi we can perform different node based calculations for our entire network. We present some of these calculations here. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import gephi file with all the information. \n",
      "network_description = pd.read_csv(\"Gephi_Files/Full_Network_Different_Descriptions [Nodes].csv\")\n",
      "network_description.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pagerank Centrality of the City-by-City Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PageRank = {}\n",
      "for i in range(len(network_description.PageRank)):\n",
      "    PageRank[network_description.Id[i]] = float(network_description.PageRank[i])\n",
      "\n",
      "plot_long_bar(PageRank, 'Top', 20, (8,6), 'Top 20 PageRank', 'PageRank','Cities')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#distribution of pagerank\n",
      "network_description.PageRank.hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To do: Include story about this picture. \n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_Cluster_Size_By_Pagerank.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Closeness Centrality of the City-by-City Network"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's look at closeness centrality for the overall network. Top 40 cities bottom 40 cities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ClosenessCentrality = {}\n",
      "for i in range(len(network_description['Closeness Centrality'])):\n",
      "    ClosenessCentrality[network_description.Id[i]] = float(network_description['Closeness Centrality'][i])\n",
      "\n",
      "plot_long_bar(ClosenessCentrality, 'Top', 20, (8,6), 'Top 20 Closeness Centrality', 'Closeness Centrality','Cities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "network_description['Closeness Centrality'].hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To do: Include story about this picture. \n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_Cluster_Size_By_Closeness_Centrality.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Betweenness of the City-by-City Network"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BetweenessCentrality = {}\n",
      "for i in range(len(network_description['Betweenness Centrality'])):\n",
      "    BetweenessCentrality[network_description.Id[i]] = float(network_description['Betweenness Centrality'][i])\n",
      "\n",
      "plot_long_bar(BetweenessCentrality, 'Top', 20, (8,6), 'Top 20 Betweenness Centrality', 'Betweenness Centrality','Cities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "network_description['Betweenness Centrality'].hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To do: Include story about this picture. \n",
      "\n",
      "<img src=\"files/Gephi_Files/Color_By_Cluster_Size_Betweeness_Centrality.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.3.2 "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "5. Supervised Learning: Inference of Travelers Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far in this project, we have collected and analyzed useful information regarding the geo-located network of turistic traces in the U.S. In this section we move forward to use the network and its data to make inferences regarding the next steps that a tourist will take in his/her travel within the U.S.\n",
      "\n",
      "Real-time estimations on the likelihood of the next destinations that tourists will visit would enable applications such as:\n",
      "\n",
      "- Providing tourists with personalized relevant information regarding transportation, acommodation, entertainment and food services.\n",
      "- Optimization of personalized and trip-aware marketing and advertisement strategies (tailored advertisement, revenue/price management, etc).\n",
      "- Provision of timely short-term demand predictions for touristic and destinations' local authorities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hence, our aim is to be able to generate estimates of the likelihood of a particular tourist visiting a particular destination, i.e.,   **$ P(\\text{Vij}|X)$**\n",
      "\n",
      "where:\n",
      "\n",
      "- **$Vij$** is the probability of turist $i$ visiting destination $j$; \n",
      "- and **$X$** is the vector of all available information regarding:\n",
      "    - prior visit/footprints that tourist $i$ has made in his/her trip in the U.S.\n",
      "    - destination $j$, \n",
      "    - tourist $i$, \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.1 Data Structure for Supervised Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Explain that in order to conduct the supervised learning analysis, we need to generate data structrures adequate for feeding inputs to the learning models (Markov Chains and bayesian classifier). Thus, the Big Table(s).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Big Table, Part II (and a first predictive model)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We resume our discussion of the Big Table, and explore construction of a Big Table for the\n",
      "purposes of supervised learning.\n",
      "\n",
      "Part of the reason we started saying \"Big Table\" is because we knew that to perform supervised\n",
      "learning, we would need to have many columns.\n",
      "\n",
      "But our question of what problem our Big Table seeks to solve is still not answered, because we\n",
      "do not yet have an appropriate supervised learning model.\n",
      "\n",
      "We return to our main question, where is a traveler going to go next.  There is potentially an\n",
      "implicit additional assumption to the question which we could make explicit by asking \"Where is\n",
      "a traveler going to go next, *and when*?\"  We make the simplifying assumption in our first model\n",
      "that we are only interested in *where* the traveler is going next.\n",
      "\n",
      "For our first model, we also make the simplifying assumption, based on the fact that most\n",
      "travelers go to a very\n",
      "small number of places, that we will not attempt to break a traveler's trails into trips.\n",
      "Each traveler has a single trail.  Because we only collected blog entries, all of which\n",
      "necessarily include a place, every traveler's trail has at least one entry.\n",
      "\n",
      "We start by thinking of each blogger's trail as a vector $v=(f_1,f_2,\\ldots,f_n)$, where $f_i$ is\n",
      "a footprint and $n$ is the length of the longest blogger trail.  We can therefore think of each\n",
      "blogger's trail as a vector in some kind of an $n$ dimensional vector space.\n",
      "This vector space does not have the necessary operations of distance, vector\n",
      "addition and subtraction, so it is not a proper vector space, but we can still think of the trails\n",
      "as $n$ dimensional vectors for the purposes of understanding our representation.\n",
      "Each $f_i$ is the set of  per-footprint columns (location path, publication date, etc.).\n",
      "\n",
      "*Null columns*. As a result of this representation, there will be at\n",
      "minimum $nk$ columns, where $k$ is the number of columns per footprint.\n",
      "Let $B_i$ be blogger $i$, and let $L(B_i)$ be\n",
      "the trail length of $B_i$.  Let $f_i[k]$ be the $k$th footprint for traveler $B_i$. For those\n",
      "travelers $B_i$ whose trail length $L(B_i) $ is shorter than $n$, we will need to devise special\n",
      "\"null\" column values for $f_i[k]$, where $L(B_i)<k<n$.  Null is a problematic and dirty concept,\n",
      "but our representation forces us into it.\n",
      "\n",
      "In order to make null column values, we will need to do something about the \"id\" type columns\n",
      "(`loc_id`, `blogger_id`, `publication_date_dt_id`, `blog_date_dt_id`) that we generated in our\n",
      "`generalize-combined-csv.py` script.  \"id\" type columns are series that start at 0.  We make\n",
      "a change to the script to start `id` numbers at 1 instead of 0, and regenerate the data.\n",
      "\n",
      "This will be the first Big Table that we create to start looking at supervised learning.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "def mk_big_table_1():\n",
      "    \"\"\"Create the first type of big table and return it.  This big table has one user\n",
      "    per row, and one column group for every footprint.\n",
      "    \"\"\"\n",
      "    df = get_combined_raw_data()\n",
      "    \n",
      "    bloggerdict = defaultdict(list)\n",
      "    \n",
      "    df = df.sort('pub_ts', ascending=True)\n",
      "    \n",
      "    # We will be building a data frame with one user per row.  Each footprint will result\n",
      "    # in another set of columns, so that we will have location_path1, publication_date1,\n",
      "    # ..., location_path2, publication_date2, ...., location_pathn, publication_daten\n",
      "    # where n is the length of the longest trail in the dataset.\n",
      "    columns = df.columns.tolist()\n",
      "    \n",
      "    for k in ['bloggername', 'blogger_id']:\n",
      "        columns.remove(k)\n",
      "    \n",
      "    for blogger, row in df.iterrows():\n",
      "        for k in columns:\n",
      "            if len(bloggerdict[row['blogger_id']]) == 0:\n",
      "                bloggerdict[row['blogger_id']] = [row['bloggername'], row['blogger_id']]\n",
      "            bloggerdict[row['blogger_id']].append(row[k])\n",
      "    # print bloggerdict.keys()\n",
      "    \n",
      "    footprint_columns = ['location_path', 'publication_date', 'blog_number', 'blog_date',\n",
      "                         'latitude', 'longitude', 'pub_ts', 'blog_ts', 'loc_id',\n",
      "                         'publication_date_id', 'blog_date_id']\n",
      "    \n",
      "    datetime_columns = ['publication_date', 'blog_date' ]\n",
      "    empty_string_columns = ['location_path', 'blog_number' ]\n",
      "    id_columns = ['loc_id', 'publication_date_id', 'blog_date_id']\n",
      "    zero_columns = ['latitude', 'longitude', 'pub_ts', 'blog_ts' ]\n",
      "\n",
      "    defaults = {}\n",
      "    for k in id_columns:\n",
      "        defaults[k] = 0\n",
      "        \n",
      "    for k in zero_columns:\n",
      "        defaults[k] = 0\n",
      "        \n",
      "    for k in empty_string_columns:\n",
      "        defaults[k] = \"\"\n",
      "\n",
      "    for k in datetime_columns:\n",
      "        defaults[k] = datetime.datetime(1970,1,1)\n",
      "        \n",
      "    # We should now have in bloggerdict a bunch of arrays of variable length.  We now\n",
      "    # normalize them by padding out the remainder with \"null\" columns.\n",
      "    \n",
      "    unique_bloggers = set(df['blogger_id'].tolist())\n",
      "    blogger_footprints = {}\n",
      "    max_footprints = 0\n",
      "    for blogger_id in unique_bloggers:\n",
      "        blogger_footprints[blogger_id] = len(df[df['blogger_id'] == blogger_id])\n",
      "        if blogger_footprints[blogger_id] > max_footprints:\n",
      "            max_footprints = blogger_footprints[blogger_id]\n",
      "        \n",
      "    for blogger_id in unique_bloggers:\n",
      "        num_nulls = max_footprints - blogger_footprints[blogger_id]\n",
      "        for i in xrange(num_nulls):\n",
      "            for k in footprint_columns:\n",
      "                bloggerdict[blogger_id].append(defaults[k])\n",
      "                \n",
      "    # assert that the lengths are all the same\n",
      "    stdlen = None\n",
      "    for k,v in bloggerdict.iteritems():\n",
      "        if stdlen is None:\n",
      "            stdlen = len(v)\n",
      "        else:\n",
      "            assert(stdlen == len(v))\n",
      "            \n",
      "    # We now create our column names array.\n",
      "    colnames = ['bloggername', 'blogger_id']\n",
      "    \n",
      "    for i in xrange(max_footprints):\n",
      "        footprint_num = i+1\n",
      "        for k in footprint_columns:\n",
      "            colnames.append('{}_{}'.format(k, footprint_num))\n",
      "    # Now we just \"transpose\" and create the new df\n",
      "    \n",
      "    newdict = defaultdict(list)\n",
      "    \n",
      "    row_order = bloggerdict.keys()\n",
      "    \n",
      "    # print row_order\n",
      "    \n",
      "    for i, colname in enumerate(colnames):\n",
      "        newdict[colname] = map(lambda x: bloggerdict[x][i], row_order)\n",
      "        \n",
      "    newdf = pd.DataFrame(newdict)\n",
      "            \n",
      "    assert(stdlen == len(colnames))\n",
      "    \n",
      "    newdf = newdf[colnames]\n",
      "\n",
      "    return newdf\n",
      "\n",
      "def create_big_table_1_csv(fnam):\n",
      "    \"\"\"Create a big table and write it to csv.\"\"\"\n",
      "    mk_big_table_1().to_csv(fnam)\n",
      "\n",
      "bt1_csv_fnam = \"big_table_1.csv\"\n",
      "# Uncomment to generate big_table_1_csv.\n",
      "\n",
      "if not os.path.exists(bt1_csv_fnam):\n",
      "    create_big_table_1_csv(bt1_csv_fnam)\n",
      "\n",
      "def thaw_big_table_1(fnam):\n",
      "    import re\n",
      "    df = pd.read_csv(fnam)\n",
      "    p = re.compile('^(publication|blog)_date_\\d+$')\n",
      "    \n",
      "    for colname in df.columns:\n",
      "        if p.match(colname):\n",
      "            df[colname] = pd.to_datetime(df[colname])\n",
      "            \n",
      "    # Get rid of redundant or unneeded columns.\n",
      "    redundant_columns = ['Unnamed: 0']\n",
      "    df = df.drop(redundant_columns, axis=1)\n",
      " \n",
      "    return df\n",
      "\n",
      "bt1_df = thaw_big_table_1(bt1_csv_fnam)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try some experiments on bt1_df here\n",
      "bt1_df.columns\n",
      "print \"OK\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.2 Benchmark Inference Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO (description): \n",
      "\n",
      "- explain why we introduce a benchmark\n",
      "- explain the benchmark we will use"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO (development):\n",
      "\n",
      "- Filter the database to get only tourists with 3 or more footprints.\n",
      "- Divide the sample into the training set and the test set.\n",
      "- Generate the vector $Yij$ : a 0|1 vector describing, for each (tourist i, destination) pair whether tourist $i$ in fact visited destination $j$\n",
      "- Generate a list of city popularities based on raw frequencies of tourist visits.\n",
      "- Make predictions on tourists' next $n$ destinations to visit based on the popularity rankings.\n",
      "- Evaluate the model performance (cross-validation).\n",
      "\n",
      "Note 1:\n",
      "In this *section 5.2* we'll introduce and construct the functions for standardized: database division into training and test sets, model performance evaluation, and cross-validation. All of which will be used in the following section(s).\n",
      "\n",
      "\n",
      "Note 2: I know have a clear idea on how to define and evaluate the precision of the models in a standardized manner, consistent with the objectives set forth in *section 5.0*. Let\u00b4s talk that on the phone."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.2 Bayesian Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section we will use a Naive Bayesian Classifier to estimate which destinations are the most likely to be visited next by a given tourist, based on the $P(\\text{Vij}|X)$ estimates to be generated. The following is the overall description of the process to be followed:\n",
      "\n",
      "- Filter the database to get only tourists with 3 or more footprints.\n",
      "- Divide the sample into the training set and the test set.\n",
      "- Generate the vector $Yij$ : a 0|1 vector describing, for each (tourist i, destination) pair whether tourist $i$ in fact visited destination $j$\n",
      "\n",
      " \n",
      "- Generate the corresponding table $X$, containing in each row the explanatory variables for each entry in $Yij$:\n",
      "    - prior visit/footprints that tourist $i$ has made in his/her trip in the U.S.\n",
      "    - destination $j$, \n",
      "    - tourist $i$, \n",
      "\n",
      "- Train the Naive Bayesian Classifier with $Yij$ and $X$.\n",
      "- Use the resulting model parameters for obtaining the $ P(\\text{Vij}|X)$\n",
      "- Evaluate the model performance\n",
      "\n",
      " \n",
      "- Identify key parameters and perform optimization search over them, with the cross-validated performance as objective function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NOTE: take a look at : http://scikit-learn.org/stable/modules/naive_bayes.html\n",
      "\n",
      "particularly at the #gaussian-naive-bayes\n",
      "\n",
      "In HW3 we used the MultinomialNB which is used mainly for text analysis (where variables xi are the frequency of a certain word in the text), our case is somewhat different because we'll be using various xi's, e.g.: destination ID of previous footprints, network cluster they belong to, their time of the year, etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Explain the Naive Bayes and its formulas (based on the description in http://scikit-learn.org/stable/modules/naive_bayes.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.3 Building a Markov Chain transition matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Returning to our previous Markov Chain discussion, we construct a\n",
      "simple Markov Chain transition matrix $M(y,\\Theta)$ in order to\n",
      "predict where a traveller will go next.  We will use this as a\n",
      "baseline to evaluate results from Supervised Learning.\n",
      "\n",
      "$$\n",
      "P(\\text{next city is }\\Theta|\\text{Current city is }y)=\n",
      "\\frac{\\text{# of times $\\Theta$ follows $y$}}\n",
      "{\\text{# of times anything follows $y$}}\n",
      "$$\n",
      "\n",
      "To construct this transition matrix $M$, which is $p\\times p$ where $p$ is\n",
      "the total number of locations, we first build a set of all\n",
      "trails.  We then take all subsequences $xy$ of length 2 and\n",
      "partition into sets by the value of $x$.  Then using this data partitioning,\n",
      "we will populate $M(x,y)$ for each pair $(x,y)$:\n",
      "\n",
      "$$M(x,y) = \\frac{|\\{ab:a=x \\text{ and } b=y\\}|}{|\\{ab:a=x\\}|}$$\n",
      "\n",
      "With Big Table 1, the initial construction of all trails is simple.  If we\n",
      "think like a C programmer, we could view Big Table 1 as an array of char\n",
      "arrays of the same allocated length, but null-terminated at different\n",
      "indices.  Each row is a blogger's trail, so to construct the trails, we\n",
      "just iterate through the rows and extract the sequence of footprints\n",
      "up to, but not including, the first null footprint (see discussion of\n",
      "null values above).\n",
      "\n",
      "The first tool we create to perform this calculation takes a big table\n",
      "and turns it into a map of blogger names to location id sequences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bt_to_trails(df):\n",
      "    \"\"\"Return a dictionary mapping bloggernames to trails, given a big table.\"\"\"\n",
      "    d = defaultdict(list)\n",
      "    colnames = set(df.columns)\n",
      "    for _, row in df.iterrows():\n",
      "        bloggername = row['bloggername']\n",
      "        for i in itertools.count(1):\n",
      "            colname = 'loc_id_{}'.format(i)\n",
      "            # The only time we will see this true \n",
      "            if colname not in colnames:\n",
      "                break\n",
      "            curr_loc_id = row[colname]\n",
      "            # We have constructed our IDs so that they start with 1, giving us a \"null id\"\n",
      "            # value of 0.  See discussion of nulls above.  We here use the null location\n",
      "            # id as our \"trail terminator\".\n",
      "            if curr_loc_id == 0:\n",
      "                break\n",
      "            d[bloggername].append(curr_loc_id)\n",
      "    return d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We take the opportunity to exploit the iterative nature of the Data Science process by going\n",
      "back to do a little more exploration to confirm and revisit some of our earlier results.\n",
      "\n",
      "We use our new function to make a histogram of trail lengths, which\n",
      "we create with a utility.  We expect to see the same histogram\n",
      "\"Total number of trips made per user\" above, and we see that this is the case. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dict_of_lists_to_list_len_hist(dict_of_lists, xlabel, ylabel, title,\n",
      "                                   cutoff_len_hi=None, bins=None):\n",
      "    \"\"\"Given a dictionary of lists, show a histogram of the list lengths.\"\"\"\n",
      "    lengths = map(len, dict_of_lists.values())\n",
      "    \n",
      "    if bins is None:\n",
      "        bins = len(set(lengths))\n",
      "    \n",
      "    if cutoff_len_hi is not None:\n",
      "        lengths = filter(lambda x: x < cutoff_len_hi, lengths)\n",
      "\n",
      "    plt.xlabel(xlabel)\n",
      "    plt.ylabel(ylabel)\n",
      "    plt.title(title)\n",
      "    \n",
      "    plt.hist(lengths, bins)\n",
      "    \n",
      "trails_dict = bt_to_trails(bt1_df)\n",
      "    \n",
      "dict_of_lists_to_list_len_hist(trails_dict, \"trail length\", \"number of trails\",\n",
      "                               \"Histogram of per-user trail lengths\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We already knew that very few users make more than 50 trips.  In order to get a more detailed\n",
      "view of the histogram of those users who make 50 trips or fewer, we draw another histogram.\n",
      "We add a parameter 'cutoff_len_hi' to our function to support this new use case, and making\n",
      "another helper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def explore_trail_lengths_cutoff(trails_dict, cutoff=None, bins=None):\n",
      "    dict_of_lists_to_list_len_hist(trails_dict, \"trail length\", \"number of trails\",\n",
      "                                   \"Histogram of per-user trail lengths < {}\".format(cutoff),\n",
      "                                   cutoff_len_hi = cutoff)\n",
      "    \n",
      "explore_trail_lengths_cutoff(trails_dict, cutoff=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By setting our cutoff even further, we get an even better idea of the distribution in the low end."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "explore_trail_lengths_cutoff(trails_dict, cutoff=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Virtual source and sink locations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our group calls, we discussed the issue that we don't know where travelers are starting\n",
      "from because we don't know their home town.  But we can form a simplifying assumption that\n",
      "all trails start from the same location, some specially designated location node $source$.\n",
      "This gives us a way to model entry into the Markov matrix, as well as allowing all of the travelers\n",
      "with only one destination to fully participate in the model.  We also need a way to model a\n",
      "user not going\n",
      "anywhere else (terminating their trail).  We designate a special sink node $sink$ for that,\n",
      "with $M(sink,p)=1$ for $p=sink$ and $0$ for $p\\ne sink$.\n",
      "\n",
      "We will use the null location id (0) to model the sink $sink$, and `1+max(loc_id)`\n",
      "to model the origin node $start$.\n",
      "\n",
      "First we set up some helper functions to convert back and forth between ID numbers and\n",
      "strings for both locations and bloggers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up some lookup tables, with some special nodes for source and sink.\n",
      "\n",
      "# We do a lot here to work around not having a relational database.  Relational\n",
      "# database practitioners would say that we should be using a relational database,\n",
      "# but we decided as a team to stay with a paradigm we are all familiar with.\n",
      "\n",
      "_loc_id_to_location_path = {}\n",
      "_location_path_to_loc_id = {}\n",
      "_bloggername_to_blogger_id = {}\n",
      "_blogger_id_to_bloggername = {}\n",
      "\n",
      "def load_idmaps():\n",
      "    \"\"\"Populate the lookup tables.\"\"\"\n",
      "    if any(_loc_id_to_location_path):\n",
      "        return True\n",
      "    df = thaw_general_csv()\n",
      "    for _, row in df.iterrows():\n",
      "        _loc_id_to_location_path[row['loc_id']] = row['location_path']\n",
      "        _location_path_to_loc_id[row['location_path']] = row['loc_id']\n",
      "        _bloggername_to_blogger_id[row['bloggername']] = row['blogger_id']\n",
      "        _blogger_id_to_bloggername[row['blogger_id']] = row['bloggername']\n",
      "\n",
      "    assert(\"SOURCE\" not in _location_path_to_loc_id)\n",
      "    assert(\"SINK\" not in _location_path_to_loc_id) \n",
      "    assert(0 not in _loc_id_to_location_path)\n",
      "    source_id = df['loc_id'].max() + 1\n",
      "    assert(source_id not in _loc_id_to_location_path)\n",
      "\n",
      "    _loc_id_to_location_path[source_id] = \"SOURCE\"\n",
      "    _loc_id_to_location_path[0] = \"SINK\"\n",
      "    _location_path_to_loc_id[\"SOURCE\"] = source_id\n",
      "    _location_path_to_loc_id[\"SINK\"] = 0\n",
      "    \n",
      "    return True\n",
      "\n",
      "def loc_id_to_location_path(loc_id):\n",
      "    load_idmaps()\n",
      "    return _loc_id_to_location_path[loc_id]\n",
      "\n",
      "def location_path_to_loc_id(location_path):\n",
      "    load_idmaps()\n",
      "    return _location_path_to_loc_id[location_path]\n",
      "\n",
      "def bloggername_to_blogger_id(bloggername):\n",
      "    load_idmaps()\n",
      "    return _bloggername_to_blogger_id[bloggername]\n",
      "\n",
      "def blogger_id_to_bloggername(blogger_id):\n",
      "    load_idmaps()\n",
      "    return _blogger_id_to_bloggername[blogger_id]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we build our actual Markov transition matrix.  It will be a 2D array, $n+2$ by $n+2$, one\n",
      "entry for each location plus our two special source and sink nodes.\n",
      "\n",
      "We iterate through every pair of locations $a$ and $b$ such that some traveler has gone from $a$ to\n",
      "$b$, and build up a map structure.  We then sum the relevant number of occurrences, and construct\n",
      "the Markov matrix.  In the function that builds the transition matrix, we sanity check the\n",
      "transition probabilities to make sure it is actually a Markov matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def locid_colname(i):\n",
      "    return 'loc_id_{}'.format(i)\n",
      "\n",
      "def get_sinkid():\n",
      "    return location_path_to_loc_id(\"SINK\")\n",
      "\n",
      "\n",
      "def build_transition_dict(df):\n",
      "    \"\"\"Build a transition dict from a big table.\"\"\"\n",
      "    \n",
      "    def second_level_dict():\n",
      "        return defaultdict(int)\n",
      "    \n",
      "    tdict = defaultdict(second_level_dict)\n",
      "    \n",
      "    colnames = set(df.columns)\n",
      "    \n",
      "    sink_locid = get_sinkid()\n",
      "    source_locid = location_path_to_loc_id(\"SOURCE\")\n",
      "    \n",
      "    for _, row in df.iterrows():\n",
      "        for i in itertools.count(1):\n",
      "            \n",
      "            colname = locid_colname(i)\n",
      "            prev_colname = locid_colname(i-1)\n",
      "            \n",
      "            # 3 possibilities.  First is that we have \"run off the end\".  This\n",
      "            # will only happen if we are processing the user with the longest\n",
      "            # trail.  We add one from the last location to the sink node.\n",
      "            if colname not in colnames:\n",
      "                tdict[row[prev_colname]][sink_locid] += 1\n",
      "                break\n",
      "            # Second possibility is that we are at the beginning.  We add one from\n",
      "            # the source node to the current location.\n",
      "            elif i == 1:\n",
      "                tdict[source_locid][row[colname]] += 1\n",
      "            # Third possibility is that we are in a \"normal\" footprint or have reached\n",
      "            # the end, as signaled by the sink node (which is ID 0).  In either case\n",
      "            # we add one to the appropriate counter, and if we are at the end, we\n",
      "            # break.\n",
      "            else:\n",
      "                tdict[row[prev_colname]][row[colname]] += 1\n",
      "                if row[colname] == sink_locid:\n",
      "                    break\n",
      "    return tdict\n",
      "        \n",
      "def bt_to_trails(df):\n",
      "    \"\"\"Return a dictionary mapping bloggernames to trails, given a big table.\"\"\"\n",
      "    d = defaultdict(list)\n",
      "    colnames = set(df.columns)\n",
      "    for _, row in df.iterrows():\n",
      "        bloggername = row['bloggername']\n",
      "        for i in itertools.count(1):\n",
      "            colname = 'loc_id_{}'.format(i)\n",
      "            # The only time we will see this true \n",
      "            if colname not in colnames:\n",
      "                break\n",
      "            curr_loc_id = row[colname]\n",
      "            # We have constructed our IDs so that they start with 1, giving us a \"null id\"\n",
      "            # value of 0.  See discussion of nulls above.  We here use the null location\n",
      "            # id as our \"trail terminator\".\n",
      "            if curr_loc_id == 0:\n",
      "                break\n",
      "            d[bloggername].append(curr_loc_id)\n",
      "    return d\n",
      "\n",
      "def build_transition_table(tdict):\n",
      "    # We rely on the fact that the sink id is 0 and the source id is the highest ID.\n",
      "    # We instantiate a 2D array of dimen source_id+1 by source_id+1\n",
      "    source_id = location_path_to_loc_id(\"SOURCE\")\n",
      "    dimen = source_id+1\n",
      "    \n",
      "    m = np.zeros((dimen, dimen))\n",
      "    \n",
      "    for i in xrange(dimen):\n",
      "        idict = tdict[i]\n",
      "        tot_idict = float(sum(tdict[i].values()))\n",
      "        \n",
      "        for j in xrange(dimen):\n",
      "            if tot_idict == 0:\n",
      "                p = 0\n",
      "            else:\n",
      "                p = tdict[i][j] / tot_idict\n",
      "            m[i][j] = p\n",
      "            \n",
      "        m[0][0] = 1\n",
      "        \n",
      "    sinkid = get_sinkid()\n",
      "        \n",
      "    for i in xrange(dimen):\n",
      "        s = np.sum(m[i])\n",
      "        assert(s < 1.00001)\n",
      "        if s <= 0.99999:\n",
      "            # There are some location IDs for places that got thrown out (such as\n",
      "            # blog entries from before the cutoff date whose places are not represented\n",
      "            # in later entries).  Give them a transition probability of 1 to the\n",
      "            # sink.  Uncomment the following line to see what these places are.\n",
      "            # print loc_id_to_location_path(i)\n",
      "            assert(s == 0.0)\n",
      "            m[i][sinkid] = 1\n",
      "            continue\n",
      "        assert(s > 0.99999)\n",
      "            \n",
      "    return m\n",
      "\n",
      "transition_dict = build_transition_dict(bt1_df)\n",
      "\n",
      "markov_table = build_transition_table(transition_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "6. Concluding Remarks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}