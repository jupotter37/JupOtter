{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "def get_latest_papers(category, num_papers=3):\n",
    "    search = arxiv.Search(\n",
    "    query=category,\n",
    "    max_results=num_papers,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "    results = list(search.results())\n",
    "    papers = []\n",
    "    for res in results:\n",
    "        papers.append({\n",
    "            \"title\": res.title,\n",
    "            \"authors\": [author.name for author in res.authors],\n",
    "            \"date\": res.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"abstract\": res.summary,\n",
    "            \"categories\": res.categories,\n",
    "            \"journal\": res.journal_ref,\n",
    "            \"url\": res.pdf_url\n",
    "        })\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2783/993974508.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Images that Sound: Composing Images and Sounds on a Single Canvas', 'authors': ['Ziyang Chen', 'Daniel Geng', 'Andrew Owens'], 'date': '2024-05-20', 'abstract': 'Spectrograms are 2D representations of sound that look very different from\\nthe images found in our visual world. And natural images, when played as\\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\\nto synthesize spectrograms that simultaneously look like natural images and\\nsound like natural audio. We call these spectrograms images that sound. Our\\napproach is simple and zero-shot, and it leverages pre-trained text-to-image\\nand text-to-spectrogram diffusion models that operate in a shared latent space.\\nDuring the reverse process, we denoise noisy latents with both the audio and\\nimage diffusion models in parallel, resulting in a sample that is likely under\\nboth models. Through quantitative evaluations and perceptual studies, we find\\nthat our method successfully generates spectrograms that align with a desired\\naudio prompt while also taking the visual appearance of a desired image prompt.\\nPlease see our project page for video results:\\nhttps://ificl.github.io/images-that-sound/', 'categories': ['cs.CV', 'cs.LG', 'cs.MM', 'cs.SD', 'eess.AS'], 'journal': None, 'url': 'http://arxiv.org/pdf/2405.12221v1'}, {'title': 'Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo', 'authors': ['Tianqi Liu', 'Guangcong Wang', 'Shoukang Hu', 'Liao Shen', 'Xinyi Ye', 'Yuhang Zang', 'Zhiguo Cao', 'Wei Li', 'Ziwei Liu'], 'date': '2024-05-20', 'abstract': 'We present MVSGaussian, a new generalizable 3D Gaussian representation\\napproach derived from Multi-View Stereo (MVS) that can efficiently reconstruct\\nunseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware\\nGaussian representations and decode them into Gaussian parameters. 2) To\\nfurther enhance performance, we propose a hybrid Gaussian rendering that\\nintegrates an efficient volume rendering design for novel view synthesis. 3) To\\nsupport fast fine-tuning for specific scenes, we introduce a multi-view\\ngeometric consistent aggregation strategy to effectively aggregate the point\\nclouds generated by the generalizable model, serving as the initialization for\\nper-scene optimization. Compared with previous generalizable NeRF-based\\nmethods, which typically require minutes of fine-tuning and seconds of\\nrendering per image, MVSGaussian achieves real-time rendering with better\\nsynthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian\\nachieves better view synthesis with less training computational cost. Extensive\\nexperiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples\\ndatasets validate that MVSGaussian attains state-of-the-art performance with\\nconvincing generalizability, real-time rendering speed, and fast per-scene\\noptimization.', 'categories': ['cs.CV'], 'journal': None, 'url': 'http://arxiv.org/pdf/2405.12218v1'}, {'title': 'Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning', 'authors': ['Guanglin Zhou', 'Zhongyi Han', 'Shiming Chen', 'Biwei Huang', 'Liming Zhu', 'Salman Khan', 'Xin Gao', 'Lina Yao'], 'date': '2024-05-20', 'abstract': \"Recent studies indicate that large multimodal models (LMMs) are highly robust\\nagainst natural distribution shifts, often surpassing previous baselines.\\nDespite this, domain-specific adaptation is still necessary, particularly in\\nspecialized areas like healthcare. Due to the impracticality of fine-tuning\\nLMMs given their vast parameter space, this work investigates in-context\\nlearning (ICL) as an effective alternative for enhancing LMMs' adaptability. We\\nfind that the success of ICL heavily relies on the choice of demonstration,\\nmirroring challenges seen in large language models but introducing unique\\ncomplexities for LMMs facing distribution shifts. Our study addresses this by\\nevaluating an unsupervised ICL method, TopKNearestPR, which selects in-context\\nexamples through a nearest example search based on feature similarity. We\\nuncover that its effectiveness is limited by the deficiencies of pre-trained\\nvision encoders under distribution shift scenarios. To address these\\nchallenges, we propose InvariantSelectPR, a novel method leveraging\\nClass-conditioned Contrastive Invariance (CCI) for more robust demonstration\\nselection. Specifically, CCI enhances pre-trained vision encoders by improving\\ntheir discriminative capabilities across different classes and ensuring\\ninvariance to domain-specific variations. This enhancement allows the encoders\\nto effectively identify and retrieve the most informative examples, which are\\nthen used to guide LMMs in adapting to new query samples under varying\\ndistributions. Our experiments show that InvariantSelectPR substantially\\nimproves the adaptability of LMMs, achieving significant performance gains on\\nbenchmark datasets, with a 34.2%$\\\\uparrow$ accuracy increase in 7-shot on\\nCamelyon17 and 16.9%$\\\\uparrow$ increase in 7-shot on HAM10000 compared to the\\nbaseline zero-shot performance.\", 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'journal': None, 'url': 'http://arxiv.org/pdf/2405.12217v1'}]\n"
     ]
    }
   ],
   "source": [
    "print(get_latest_papers(category=\"cat:cs.CV\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "def get_relevant_passage(query, top_k=5):\n",
    "    results = []\n",
    "    q_emb = embedding_model.encode(query)\n",
    "    res = index.query(vector=q_emb.tolist(), top_k=top_k, include_metadata=True).to_dict()\n",
    "    ## get the titles and the urls\n",
    "    for r in res['matches']:\n",
    "        title = r['metadata']['title']\n",
    "        url = r['metadata']['url']\n",
    "        results.append({'title': title, 'url': url})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcategories\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Artificial Intelligence\n",
       "1                               Hardware Architecture\n",
       "2                            Computational Complexity\n",
       "3     Computational Engineering, Finance, and Science\n",
       "4                              Computational Geometry\n",
       "5                            Computation and Language\n",
       "6                           Cryptography and Security\n",
       "7             Computer Vision and Pattern Recognition\n",
       "8                               Computers and Society\n",
       "9                                           Databases\n",
       "10       Distributed, Parallel, and Cluster Computing\n",
       "11                                  Digital Libraries\n",
       "12                               Discrete Mathematics\n",
       "13                     Data Structures and Algorithms\n",
       "14                              Emerging Technologies\n",
       "15               Formal Languages and Automata Theory\n",
       "16                                 General Literature\n",
       "17                                           Graphics\n",
       "18                   Computer Science and Game Theory\n",
       "19                         Human-Computer Interaction\n",
       "20                              Information Retrieval\n",
       "21                                 Information Theory\n",
       "22                                   Machine Learning\n",
       "23                          Logic in Computer Science\n",
       "24                                 Multiagent Systems\n",
       "25                                         Multimedia\n",
       "26                              Mathematical Software\n",
       "27                                 Numerical Analysis\n",
       "28                  Neural and Evolutionary Computing\n",
       "29               Networking and Internet Architecture\n",
       "30                             Other Computer Science\n",
       "31                                  Operating Systems\n",
       "32                                        Performance\n",
       "33                              Programming Languages\n",
       "34                                           Robotics\n",
       "35                               Symbolic Computation\n",
       "36                                              Sound\n",
       "37                               Software Engineering\n",
       "38                    Social and Information Networks\n",
       "39                                Systems and Control\n",
       "Name: Subcategory name, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Category'] == 'Computer Science']['Subcategory name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {}\n",
    "for cat in categories:\n",
    "    cats[cat] = df[df['Category'] == cat]['Subcategory name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cats['Biology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory abrv</th>\n",
       "      <th>Subcategory name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cs.AR</td>\n",
       "      <td>Hardware Architecture</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cs.CC</td>\n",
       "      <td>Computational Complexity</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering, Finance, and Science</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cs.CG</td>\n",
       "      <td>Computational Geometry</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subcategory abrv                                 Subcategory name  \\\n",
       "0            cs.AI                          Artificial Intelligence   \n",
       "1            cs.AR                            Hardware Architecture   \n",
       "2            cs.CC                         Computational Complexity   \n",
       "3            cs.CE  Computational Engineering, Finance, and Science   \n",
       "4            cs.CG                           Computational Geometry   \n",
       "\n",
       "           Category  \n",
       "0  Computer Science  \n",
       "1  Computer Science  \n",
       "2  Computer Science  \n",
       "3  Computer Science  \n",
       "4  Computer Science  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('arxiv_categories.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df.groupby('Category')['Subcategory name'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " # categories = {\n",
    "    # \"Computer Science\": [\"Machine Learning\", \"Web Development\", \"Artificial Intelligence\", \"Data Science\", \"Cybersecurity\"],\n",
    "    # \"Business\": [\"Accounting\", \"Marketing\", \"Project Management\", \"Entrepreneurship\"],\n",
    "    # \"Health\": [\"Nutrition\", \"Fitness\", \"Mental Health\", \"Pharmacy\"],\n",
    "    # \"Entertainment\": [\"Movies\", \"Music\", \"Gaming\", \"Literature\"]\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs.AI', 'cs.AR']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = ['Artificial Intelligence', 'Cybersecurity', 'Hardware Architecture']\n",
    "df[df['Subcategory name'].isin(lst)]['subcategory abrv'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 21:48:12.566 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/codespace/.local/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper found.\n"
     ]
    }
   ],
   "source": [
    "def get_paper_by_id(paper_id):\n",
    "    # Create an arxiv client\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Create a search object\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "\n",
    "    # Execute the search and retrieve the results\n",
    "    results = list(client.results(search))\n",
    "\n",
    "    # Check if results are found\n",
    "    if results:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "paper_id = '2102.09342'  # Replace with your desired arxiv ID\n",
    "paper = get_paper_by_id(paper_id)\n",
    "\n",
    "if paper:\n",
    "    print(\"Paper found.\")\n",
    "else:\n",
    "    print(\"Paper not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FedMood: Federated Learning on Mobile Health Data for Mood Detection'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4564/2536030040.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "query='cs.AI',\n",
    "max_results=5,\n",
    "sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "sort_order=arxiv.SortOrder.Descending,\n",
    ")\n",
    "results = list(search.results())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author\n",
      "Link\n",
      "MissingFieldError\n",
      "authors\n",
      "categories\n",
      "comment\n",
      "doi\n",
      "download_pdf\n",
      "download_source\n",
      "entry_id\n",
      "get_short_id\n",
      "journal_ref\n",
      "links\n",
      "pdf_url\n",
      "primary_category\n",
      "published\n",
      "summary\n",
      "title\n",
      "updated\n"
     ]
    }
   ],
   "source": [
    "for m in dir(results[0]):\n",
    "    if not m.startswith('_'):\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [4, 5, 6]\n",
    "t2 = [1, 2, 3]\n",
    "t3 = []\n",
    "# make t3 = [4, 5, 6, 1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat import load_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://arxiv.org/pdf/2405.12177v1'\n",
    "res = load_paper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
