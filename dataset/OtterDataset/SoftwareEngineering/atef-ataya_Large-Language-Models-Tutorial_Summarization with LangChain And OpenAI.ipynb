{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd0e1c8",
   "metadata": {},
   "source": [
    "### Summarization with LangChain and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43ffd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt -q\n",
    "# !pip - installs the packages in the base environment\n",
    "# pip - installs the packages in the virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdc492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de48022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/ds/anaconda3/lib/python3.10/site-packages (from pdfminer.six) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/ds/anaconda3/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Users/ds/anaconda3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20221105\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain --upgrade -q\n",
    "# For working with unstructured pdf load, install the below dependencies\n",
    "# !pip install unstructured\n",
    "# !pip install pdf2image\n",
    "# !pip install pdfminer\n",
    "!pip install pdfminer.six "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088272f",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a7666db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bccaf984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c80586",
   "metadata": {},
   "source": [
    "### Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a09e1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a01ee054",
   "metadata": {},
   "outputs": [],
   "source": [
    "textMessage=\"\"\"\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to \\  \n",
    "refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able \\\n",
    "to access some window of past messages directly. A more complex system will need to have a world model that it is \\\n",
    "constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "We call this ability to store information about past interactions \"memory\". LangChain provides a lot of utilities \\\n",
    "for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain.\n",
    "A memory system needs to support two basic actions: reading and writing. Recall that every chain defines some \\\n",
    "core execution logic that expects certain inputs. Some of these inputs come directly from the user, but some of \\\n",
    "these inputs can come from memory. A chain will interact with its memory system twice in a given run.\n",
    "AFTER receiving the initial user inputs but BEFORE executing the core logic, a chain will READ from its \\\n",
    "memory system and augment the user inputs.\n",
    "AFTER executing the core logic but BEFORE returning the answer, a chain will WRITE the inputs and outputs \\\n",
    "of the current run to memory, so that they can be referred to in future runs.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a higly skilled writer, and your expertiese lies in the field of copywriting'),\n",
    "    HumanMessage(content=f'Please provide a short and concise summary of the following text:\\n TEXT:{textMessage}')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b74547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(textMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b681591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the field of conversational interfaces, it is crucial for a system to be able to refer back to previous information in a conversation. This can be achieved through a memory system, which allows the system to store and access past interactions. LangChain offers utilities for adding memory to a system, which can be used independently or seamlessly integrated into a chain. The memory system supports two basic actions: reading and writing. Before executing the core logic, the system reads from memory to augment user inputs, and after executing the core logic, it writes the inputs and outputs to memory for future reference.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "summary = llm(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369574c",
   "metadata": {},
   "source": [
    "### Summarization with LangChain Using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44ba11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "025e9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Compose a concise and a brief summary of the following text:\n",
    "TEXT: `{text}`\n",
    "Translate the summary to {language}.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text', 'language'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "178aa850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt.format(text=textMessage, language='German'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54059147",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edade657",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary= chain.run({'text':textMessage, 'language':'arabic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb637124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "معظم تطبيقات LLM لديها واجهة محادثة. واحدة من المكونات الأساسية للمحادثة هي القدرة على الإشارة إلى المعلومات التي تم طرحها في وقت سابق في المحادثة. على الأقل، يجب أن يكون لدى نظام محادثة القدرة على الوصول إلى نافذة من الرسائل السابقة مباشرة. سيحتاج النظام المعقد أكثر إلى وجود نموذج عالم يتم تحديثه باستمرار، مما يتيح له القيام بأشياء مثل الحفاظ على معلومات حول الكيانات وعلاقاتها.\n",
      "نسمي هذه القدرة على تخزين المعلومات حول التفاعلات السابقة \"الذاكرة\". يوفر LangChain العديد من الأدوات لإضافة الذاكرة إلى النظام. يمكن استخدام هذه الأدوات بمفردها أو دمجها بسلاسة في سلسلة.\n",
      "يحتاج نظام الذاكرة إلى دعم إجراءين أساسيين: القراءة والكتابة. تذكر أن كل سلسلة تحدد بعض المنطق التنفيذي الأساسي الذي يتوقع بعض المدخلات المحددة. بعض هذه المدخلات تأتي مباشرة من المستخدم، ولكن بعض هذه المدخلات يمكن أن تأتي من الذاكرة. ستتفاعل السلسلة مع نظام الذاكرة مرتين في تشغيل معين.\n",
      "بعد استلام المدخلات الأولية من المستخدم ولكن قبل تنفيذ المنطق الأساسي، ستقوم السلسلة بقراءة من نظام الذاكرة وتعزيز المدخلات من المستخدم.\n",
      "بعد تنفيذ المنطق الأساسي ولكن قبل إرجاع الإجابة، ستقوم السلسلة بكتابة المدخلات والمخرجات للتشغيل الحالي في الذاكرة، حتى يمكن الرجوع إليها في التشغيلات المستقبلية.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cd551",
   "metadata": {},
   "source": [
    "### Summarization with LangChain using StuffDcoumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1168d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4afbb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./CNNs.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "docs = [Document(page_content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98ef7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Compose a concise and a brief summary of the following text:\n",
    "TEXT: `{text}`\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ad4036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c10d3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Neural Networks (CNNs) are a type of neural network architecture commonly used for image recognition tasks. They are designed to encode image-specific features into the network, making them well-suited for image-focused tasks. CNNs operate by scanning images and categorizing them based on the features they detect. They are used in various applications such as self-driving cars, face recognition, and computer vision. CNNs address the challenge of working with large input images by implementing the convolution operation, which is a fundamental building block of CNNs. Unlike other neural networks, CNNs exploit knowledge about the specific type of input, allowing for a simpler network architecture.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4272209",
   "metadata": {},
   "source": [
    "### Summarization with LangChain using Map_Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf4be27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./CNNs.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0673813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98aa220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e211bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3dde9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b56a0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional Neural Networks (CNNs) are widely used in image recognition tasks due to their ability to encode image-specific features and simplify neural network architecture. They operate similarly to the brain, categorizing objects based on detected features. CNNs are crucial in computer vision applications like self-driving cars and face recognition as they accurately classify and detect objects in images. However, working with large images can lead to overfitting and require significant computational resources. CNNs handle image data using the convolution operation and can be used for tasks like image classification and emotion detection. They analyze images as arrays of pixel values and exploit knowledge about the input type to simplify the network architecture.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a47a5148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a0fd615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.combine_document_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0d992",
   "metadata": {},
   "source": [
    "### Summarization using the refine CombineDocumentChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6458fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "841734e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e5c5f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ds/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ds/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredPDFLoader('./GNNs in Anomaly Detection.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df34bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "Abstract Anomaly detection is an important task, which tackles the problem of dis- covering “different from normal” signals or patterns by analyzing a massive amount of data, thereby identifying and preventing major faults. Anomaly detection is ap- plied to numerous high-impact applications in areas such as cyber-security, ﬁnance, e-commerce, social network, industrial monitoring, and many more mission-critical tasks. While multiple techniques have been developed in past decades in address- ing unstructured collections of multi-dimensional data, graph-structure-aware tech- niques have recently attracted considerable attention. A number of novel techniques have been developed for anomaly detection by leveraging the graph structure. Re- cently, graph neural networks (GNNs), as a powerful deep-learning-based graph rep- resentation technique, has demonstrated superiority in leveraging the graph structure and been used in anomaly detection. In this chapter, we provide a general, compre- hensive, and structured overview of the existing works that apply GNNs in anomaly detection.\n",
      "\n",
      "26.1 Introduction\n",
      "\n",
      "In the era of machine learning, sometimes, what stands out in the data is more important and interesting than the normal. This branch of task is called anomaly detection, which concentrates on discovering “different from normal” signals or patterns by analyzing a massive amount of data, thereby identifying and prevent- ing major faults. This task plays a key on in several high-impact domains, such as cyber-security (network intrusion or network failure detection, malicious program\n",
      "\n",
      "Shen Wang Department of Computer Science, University of Illinois at Chicago, e-mail: swang224@uic. edu\n",
      "\n",
      "Philip S. Yu Department of Computer Science, University of Illinois at Chicago, e-mail: psyu@uic.edu\n",
      "\n",
      "557\n",
      "\n",
      "558\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "detection), ﬁnance (credit card fraud detection, malicious account detection, cashout user detection, loan fraud detection), e-commerce (reviews spam detection), social network (key player detection, anomaly user detection, real money trading detec- tion), and industrial monitoring (fault detection).\n",
      "\n",
      "In the past decades, many techniques have been developed for anomaly detec- tion by leveraging the graph structure, a.k.a. graph-based anomaly detection. Unlike non-graph anomaly detection, they further take the inter-dependency among each data instance into consideration, where data instances in a wide range of disciplines, such as physics, biology, social sciences, and information systems, are inherently re- lated to one another. Compare to the non-graph-based method, the performance of the graph-based method is greatly improved. Here, we provide an illustrative ex- ample of malicious program detection in the cyber-security domain in Figure 26.1. In a phishing email attack as shown in Figure 26.1, to steal sensitive data from the database of a computer/server, the attacker exploits a known venerability of Mi- crosoft Ofﬁce by sending a phishing email attached with a malicious .doc ﬁle to one of the IT staff of the enterprise. When the IT staff member opens the attached .doc ﬁle through the browser, a piece of a malicious macro is triggered. This ma- licious macro creates and executes a malware executable, which pretends to be an open-source Java runtime (Java.exe). This malware then opens a backdoor to the ad- versary, subsequently allowing the adversary to read and dump data from the target database via the affected computer. In this case, signature-based or behavior-based malware detection approaches generally do not work well in detecting the mali- cious program in our example. As the adversary can make the malicious program from scratch with binary obfuscation, signature-based approaches would fail due to the lack of known malicious signatures. Behavior-based approaches may not be effective unless the malware sample has previously been used to train the detection model. It might be possible to detect the malicious program using existing host- level anomaly detection techniques. These host-based anomaly detection methods can locally extract patterns from process events as the discriminators of abnormal behavior. However, such detection is based on observations of single operations, and it sacriﬁces the false positive rate to detect the malicious program. For exam- ple, the host-level anomaly detection can detect the fake “Java.exe” by capturing the database read. However, a Java-based SQL client may also exhibit the same opera- tion. If we simply detect the database read, we may also classify normal Java-based SQL clients as abnormal program instances and generate false positives. In the en- terprise environment, too many false positives can lead to the alert fatigue problem, causing cyber-analysts to fail to catch up with attacks. To accurately separate the database read of the malicious Java from the real Java instances, we need to con- sider the higher semantic-level context of the two Java instances. As shown in Figure ??, malicious Java is a very simple program and directly accesses the database. On the contrary, a real Java instance has to load a set of .DLL ﬁles in addition to the database read. By comparing the behavior graph of the fake Java instance with the normal ones, we can ﬁnd that it is abnormal and precisely report it as a malicious program instance. Thus, leveraging the graph helps to identify the anomaly data instances.\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "559\n",
      "\n",
      "Whole Execution Environment\n",
      "\n",
      "Malicious Java\n",
      "\n",
      "Real Java\n",
      "\n",
      "Web Service\n",
      "\n",
      "RPCRT4.DLL\n",
      "\n",
      "MSVCRT.DLL\n",
      "\n",
      "NTDLL.DLL\n",
      "\n",
      "IT Laptop\n",
      "\n",
      "Database\n",
      "\n",
      "Database\n",
      "\n",
      "dump\n",
      "\n",
      "create execute\n",
      "\n",
      "phishing email\n",
      "\n",
      ".doc\n",
      "\n",
      "USP10.DLL\n",
      "\n",
      "USER32.DLL\n",
      "\n",
      "Java.exe\n",
      "\n",
      "Figure 26.1: An illustrative example of malicious program detection in the cyber- security domain. The left side shows an example of a phishing email attack: the hacker creates and executes a malware executable, which pretends to be an open- source Java runtime (Java.exe); this malware then opens a backdoor to the adversary, subsequently allowing the adversary to read and dump data from the target database via the affected computer. The right side demonstrates the behavior graph of the malicious Java.exe vs. normal Java runtime.\n",
      "\n",
      "Speciﬁcally, the beneﬁt of graph-based method is four-folded:\n",
      "\n",
      "Inter-dependent Property – Data instances in a wide range of disciplines, such as physics, biology, social sciences, and information systems, are inherently related to one another and can form a graph. These graph structures can provide additional side information to identify the anomalies in addition to the attributes of each data instance.\n",
      "\n",
      "Relational Property – The anomaly data instances sometimes can exhibit them- selves as relational, e.g., in the fraud domain, the context of the anomaly data instance has a high probability of being abnormal; the anomaly data instance is closely related to a group of data instances. If we identify one anomaly data instance in the graph, some other anomaly data instances based on it can be detected.\n",
      "\n",
      "Fruitful Data Structure – The graph is a data structure encoding fruitful in- formation. The graph consists of nodes and edges, enabling the incorporation of node and edge attributes/types for anomaly data instance identiﬁcation. Be- sides, multiple paths exist between each pair of data instances, which allows the relation extraction in different ranges.\n",
      "\n",
      "Robust Data Structure – The graph is a more adversarially robust data structure, e.g., attacker or fraudster usually can only attack or fraud the speciﬁc data in- stance or its context and have a limited global view of the whole graph. In this case, the anomaly data instance is harder to ﬁt into the graph as well as possible.\n",
      "\n",
      "Recent years have witnessed a growing interest in developing deep-learning- based algorithms on the graph, including unsupervised methods (Grover and Leskovec, 2016; Liao et al, 2018; Perozzi et al, 2014) and supervised methods (Wang et al, 2016, 2017e; Hamilton et al, 2017b; Kipf and Welling, 2017b; Veliˇckovi´c et al, 2018). Among these deep-learning-based algorithms on the graph, the graph neural networks (GNNs) (Hamilton et al, 2017b; Kipf and Welling, 2017b; Veliˇckovi´c et al,\n",
      "\n",
      "560\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "2018), as powerful deep graph representation learning techniques, have demon- strated superiority in leveraging the graph structure. The basic idea is to aggregate information from local neighborhoods in order to combine the content feature and graph structures to learn the new graph representation. In particular, GCN (Kipf and Welling, 2017b) leverages the “graph convolution” operation to aggregate the feature of one-hop neighbors and propagate multiple-hop information via the iter- ative “graph convolution”. GraphSage (Hamilton et al, 2017b) develops the graph neural network in an inductive setting, which performs neighborhood sampling and aggregation to generate new node representation efﬁciently. GAT (Veliˇckovi´c et al, 2018) further incorporates attention mechanism into GCN to perform the attentional aggregation of the neighborhoods. Given the importance of graph-based anomaly detection and the success of graph neural networks, both academia and industry are interested in applying GNNs to tackle the problem of anomaly detection. In re- cent years, some researchers have successfully applied GNNs in several important anomaly detection tasks. In this book chapter, we summarize different GNN-based anomaly detection approaches and provide taxonomies for them according to var- ious criteria. Despite the more than 10+ papers published in the last three years, several challenges remain unsolved until now, which we summarize and introduce in this chapter as below.\n",
      "\n",
      "Issues Unlike GNNs applications in other domains, the GNNs applications in anomaly detection have several unique issues, which comes from data, task, and model. We brieﬂy discuss and summarize them to provide a comprehensive understanding of the difﬁculties of the problems.\n",
      "\n",
      "Pipeline There are various GNN-based anomaly detection works. It is challeng- ing and time-consuming to understand the big pictures of all these works. To facilitate an easy understanding of existing research on this line, we summarize the general pipeline of GNN-based anomaly detection approaches.\n",
      "\n",
      "Taxonomy There are already several works in the domain of GNN-based anomaly detection. Compared with other GNN applications, GNN-based anomaly detec- tion is more complicated due to unique challenges and problem deﬁnitions. To provide a quick understanding of the similarity and differences between exist- ing works, we list some representative works and summarize novel elaborated taxonomies according to various criteria.\n",
      "\n",
      "Case Studies We provide the case studies of some representative GNN-based\n",
      "\n",
      "anomaly detection approaches.\n",
      "\n",
      "The rest of this chapter is organized as follows. Section 26.2 discusses and sum- marizes the issues of the GNN-based anomaly detection. Section 26.3 provides the uniﬁed pipeline of the GNN-based anomaly detection. Section 26.4 provides the taxonomies of existing GNN-based anomaly detection approaches. Section 26.5 provides the case studies of some representative GNN-based anomaly detection ap- proaches. In the last section, we provide the discussion and future directions.\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "561\n",
      "\n",
      "26.2 Issues\n",
      "\n",
      "In this section, we provide a brief discussion and summary of the issues in GNN- based anomaly detection. In particular, we group them into three: (i) data-speciﬁc issues, (ii) task-speciﬁc issues, and (iii) model-speciﬁc issues.\n",
      "\n",
      "26.2.1 Data-speciﬁc issues\n",
      "\n",
      "As the anomaly detection systems usually work with real-world data, they demon- strate high volume, high dimensionality, high heterogeneity, high complexity, and dynamic property.\n",
      "\n",
      "High Volume – With the advance of information storage, it is much easier to collect large amounts of data. For example, in an e-commerce platform like Xianyu, there are over 1 billion second-hand goods published by over ten millions users; in an enterprise network monitoring system, the system event data collected from a single computer system in one day can easily reach 20 GB, and the number of events related to one speciﬁc program can easily reach thousands. It is prohibitively expensive to perform the analytic task on such massive data in terms of both time and space.\n",
      "\n",
      "High Dimensionality – Also, beneﬁt from the advance of the information stor- age, rich amount of information is collected. It results in high dimensionality of the attributes for each data instance. For example, in an e-commerce platform like Xianyu, different types of attributes are collected for each data instance, such as user demographics, interests, roles, as well as different types of relations; in an en- terprise network monitoring system, each collected system event is associated with hundreds of attributes, including information of involved system entities and their relationships, which causes the curse of dimensionality.\n",
      "\n",
      "High Heterogeneity – As rich types of information are collected, it results in high heterogeneity of the attributes for each data instance: the feature of each data instance can be multi-view or multi-sourced. For example, in an e-commerce plat- form like Xianyu, multiple types of data are collected from the user, such as personal proﬁle, purchase history, explore history, and so on. Nevertheless, multi-view data like social relations and user attributes have different statistical properties. Such het- erogeneity poses a great challenge to integrate multi-view data.\n",
      "\n",
      "High Complexity – As we can collect more and more information, the collected data is complex in content: it can be categorical or numerical, which increases the difﬁculty of leveraging all the contents jointly.\n",
      "\n",
      "Dynamic Property – The data collection is usually conducted every day or con- tinuously. For example, billions of credit card transactions are performed every day; billions of click-through traces of web users are generated each day. This kind of data can be thought of as streaming data, and it demonstrates dynamic property.\n",
      "\n",
      "The above data-speciﬁc issues are general and apply to all kinds of data. So we also need to discuss the graph-data-speciﬁc issues, including relational prop-\n",
      "\n",
      "562\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "erty, graph heterogeneity, graph dynamic, variety of deﬁnitions, lack of intrinsic distance/similarity metrics, and search space size.\n",
      "\n",
      "Relational Property – The relational property of the data makes it challenging to quantify the anomalousness of graph objects. While in traditional outlier detection, the objects or data instances are treated as independent and identically distributed (i.i.d.) from each other, the data instances in graph data are pair-wise correlated. Thus, the “spreading activation” of anomalousness or “guilt by associations” needs to be carefully accounted for. For example, the cash-out users not only have ab- normal features, but also behavior abnormally in interaction relations. They may simultaneously have many transactions and fund transfer interactions with particu- lar merchants, which is hard to be exploited by traditional feature extraction.\n",
      "\n",
      "Graph Heterogeneity – Similar to the general data-speciﬁc issues of high het- erogeneity, the graph instance type, and relation type are usually heterogeneous. For example, in a computer system graph, there are three types of entities: process (P), ﬁle (F), and INETSocket (I) and multiple types of relations: a process forking F), a process connecting to another process (P ! ! an Internet socket (P I), and so on. Due to the heterogeneity of entities (nodes) and dependencies (edges) in a heterogeneous graph, the diversities between differ- ent dependencies vary dramatically, signiﬁcantly increasing the difﬁculty of jointly leveraging these nodes and edges.\n",
      "\n",
      "P), a process accessing a ﬁle (P\n",
      "\n",
      "!\n",
      "\n",
      "Graph Dynamic – As the data are collected periodically or continuously, the constructed graph also demonstrates the dynamic property. It is challenging to de- tect the anomaly due to its dynamic nature. Some anomalous operations show some explicit patterns but try to hide them in a large graph, while others are with implicit patterns. Take an explicit anomaly pattern in a recommender system as an exam- ple. As anomalous users usually control multiple accounts to promote the target items, the edges between these accounts and items may compose a dense subgraph, which emerges in a short time period. In addition, although the accounts which involve the anomaly perform anomalous operations sometimes, these accounts per- form normally most of the time, which hides their long-term anomalous behavior and increases the difﬁculty of detection.\n",
      "\n",
      "Variety of Deﬁnitions – The deﬁnitions of anomalies in graphs are much more diverse than in traditional outlier detection, given the rich representation of graphs. For example, novel types of anomalies related to graph substructures are of interest for many applications, e.g., money-laundering rings in trading networks.\n",
      "\n",
      "Lack of Intrinsic Distance/Similarity Metrics – The intrinsic distance/similarity metrics are not clear. For example, in real computer systems, given two programs with thousands of system events related to them, it is a difﬁcult task to measure their distance/similarity.\n",
      "\n",
      "Search Space Size – The main issue associated with more complex anomalies such as graph substructures is that the search space is huge, as in many graph- theoretical problems associated with graph search. The enumeration of possible substructures is combinatorial, making the problem of ﬁnding out the anomalies a much harder task. This search space is enlarged even more when the graphs are attributed as the possibilities span both the graph structure and the attribute space.\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "563\n",
      "\n",
      "As a result, the graph-based anomaly detection algorithms need to be designed not only for effectiveness but also for efﬁciency and scalability.\n",
      "\n",
      "26.2.2 Task-speciﬁc Issues\n",
      "\n",
      "Due to the unique characteristics of the anomaly detection task, the issues also come from the problems, including labels quantity and quality, class imbalance and asym- metric error, and novel anomalies.\n",
      "\n",
      "Labels Quantity and Quality – The major issue of anomaly detection is that the data often has no or very few class labels. It is unknown which data is abnormal or normal. Usually, it is costly and time-consuming to obtain ground-truth labels from the domain expert. Moreover, due to the complexity of the data, the produced label may be noisy and biased. Therefore, this issue limits the performance of the supervised machine learning algorithm. What is more, the lack of true clean labels, i.e., ground truth data, also makes the evaluation of anomaly detection techniques challenging.\n",
      "\n",
      "Class Imbalance and Asymmetric Error – Since the anomalies are rare and only a small fraction of the data is excepted to be abnormal, the data is extremely imbal- anced. Moreover, the cost of mislabeling a good data instance versus a bad instance may change depending on the application and further could be hard to estimate beforehand. For example, mis-predicting a cash-out fraudster as a normal user is es- sentially harmful to the whole ﬁnancial system or even the national security, while mis-predicting a normal user as a fraudster could cause customer loss ﬁdelity. There- fore, the class imbalance and asymmetric error affect the machine-learning-based method seriously.\n",
      "\n",
      "Novel Anomalies – In some domain, such as fraud detection or malware detec- tion, the anomalies are created by the human. They are created by analyzing the detection system and designed to be disguised as a normal instance to bypass the detection. As a result, not only should the algorithms be adaptive to changing and growing data over time, they should also be adaptive to and be able to detect novel anomalies in the face of adversaries.\n",
      "\n",
      "26.2.3 Model-speciﬁc Issues\n",
      "\n",
      "Apart from data-speciﬁc and task-speciﬁc issues, it is also challenging to apply the graph neural network directly to anomaly detection task sdue to its unique model properties, such as homogeneous focus and vulnerability.\n",
      "\n",
      "Homogeneous Focus – Most graph neural network models are designed for ho- mogeneous graph, which considers a single type of nodes and edges. In many real- world applications, data can be naturally represented as heterogeneous graphs. How- ever, traditional GNNs treat different features equally. All the features are mapped\n",
      "\n",
      "564\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "and propagated together to get the representations of nodes. Considering that the role of each node is just a one-dimensional feature in the high dimensional feature space, there exist more features that are not related to the role, e.g., age, gender, and education. Thus the representation of applicants with neighbors of different roles has no distinction in representation space after neighbor aggregation, which causes the traditional GNNs to fail.\n",
      "\n",
      "Vulnerability – Recently theoretical studies prove the limitations and vulnerabil- ities of GNNs, when graphs have noisy nodes and edges. Therefore, a small change to the node features may cause a dramatic performance drop and failing to tackle the camouﬂage, where fraudsters would sabotage the GNN-based fraud detectors.\n",
      "\n",
      "26.3 Pipeline\n",
      "\n",
      "In this section, we introduce the standard pipeline of the GNN-based anomaly detec- tion. Typically, GNN-based anomaly detection methods consist of three important components, including graph construction and transformation, graph representation learning, and prediction.\n",
      "\n",
      "26.3.1 Graph Construction and Transformation\n",
      "\n",
      "As discussed in the previous section, a real-world anomaly detection system has some data-speciﬁc issues. Therefore, it requires data analysis on the raw data to ad- dress them. Then the graph can be constructed to capture the complex interactions and eliminate the data redundancies. Based on the type of the data instance and relations, the graph can be constructed as a homogeneous graph or heterogeneous graph, where a homogeneous graph only has a single-typed data instance and rela- tion, and a heterogeneous graph has multi-typed data instances and relations. Based on the availability of the timestamp, the graph can be constructed as a static graph or a dynamic graph, where a static graph refers to the graph that has ﬁxed nodes and edges, and a dynamic graph refers to the graph that has nodes and/or edges change over time. Based on the availability of the node and/or edge attributes, the constructed graph can be a plain graph or an attributed graph, where the plain graph only contains the structure information and the attributed graph has attributes on nodes and/or edges.\n",
      "\n",
      "When the constructed graph is heterogeneous, simply aggregating neighbors can- not capture the semantic and structural correlations among different types of enti- ties. To address the graph heterogeneity issue, a graph transformation is performed to transform the heterogeneous graph to a multi-channel graph guided by the meta- paths, where a meta-path (Sun et al, 2011) is a path that connects entity types via a sequence of relations over a heterogeneous network. For example, in a computer I), with each system, a meta-path can be the system events (P\n",
      "\n",
      "P, P\n",
      "\n",
      "F, and P\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "565\n",
      "\n",
      "one deﬁning a unique relationship between two entities. The multi-channel graph is a graph with each channel constructed via a certain type of meta-path. Formally, given a heterogeneous graph G with a set of meta-paths M = , the transformed multi-channel network ˆG is deﬁned as follows:\n",
      "\n",
      "M1, ..., M M { |\n",
      "\n",
      "|}\n",
      "\n",
      "ˆG =\n",
      "\n",
      "Gi = (Vi, Ei, Ai), i = 1, 2, ...,\n",
      "\n",
      ") }\n",
      "\n",
      "(26.1)\n",
      "\n",
      "Gi| {\n",
      "\n",
      "M\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "where Ei denotes the homogeneous links between the entities in Vi, which are con- nected through the meta-path Mi. Each channel graph Gi is associated with an adja- cency matrix Ai. indicates the number of meta-paths. Notice that the potential meta-paths induced from the heterogeneous network can be inﬁnite, but not every- one is relevant and useful for the speciﬁc task of interest. Fortunately, there are some algorithms (Chen and Sun, 2017) proposed recently for automatically selecting the meta-paths for particular tasks.\n",
      "\n",
      "M |\n",
      "\n",
      "|\n",
      "\n",
      "26.3.2 Graph Representation Learning\n",
      "\n",
      "After the graph is constructed and transformed, graph representation learning is conducted to get the proper new representation of the graph. Generally GNNs are built by stacking seven types of basic operations, including neural aggrega- tor function AGG(), linear mapping function MAPlinear(), nonlinear mapping func- tion MAPnonlinear(), multilayer perceptron function MLP(), feature concatenation CONCAT (), attentional feature fusion COMBatt , and readout function Readout(). Among these operations, linear mapping function, nonlinear mapping function, mul- tilayer perceptron function, feature concatenation, and attentional feature fusion are typical operations used in traditional deep learning algorithms. Their formal de- scriptions are described as follows:\n",
      "\n",
      "Linear Mapping Function MAPlinear():\n",
      "\n",
      "MAPlinear(x) = Wx\n",
      "\n",
      "(26.2)\n",
      "\n",
      "where x is the input feature vector, and W is the trainable weight matrix.\n",
      "\n",
      "Nonlinear Mapping Function MAPnonlinear():\n",
      "\n",
      "MAPnonlinear(x) = s (Wx)\n",
      "\n",
      "(26.3)\n",
      "\n",
      "where x is the input feature vector, W is the trainable weight matrix, and s () repre- sents the non-linear activation function.\n",
      "\n",
      "Multilayer Perceptron Function MLP():\n",
      "\n",
      "MLP(x) = s (Wk\n",
      "\n",
      "s (W1x))\n",
      "\n",
      "(26.4)\n",
      "\n",
      "· ·\n",
      "\n",
      "566\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "where x is the input feature vector, Wi with i = 1, ..., k is the trainable weight ma- trix, k indicates the number of layers, and s () represents the non-linear activation function.\n",
      "\n",
      "Feature Concatenation CONCAT ():\n",
      "\n",
      "CONCAT (x1,\n",
      "\n",
      "xn) = [x1,\n",
      "\n",
      "xn]\n",
      "\n",
      "(26.5)\n",
      "\n",
      "· ·\n",
      "\n",
      "· ·\n",
      "\n",
      "where n indicates the number of the features. Attentional Feature Fusion COMBatt ():\n",
      "\n",
      "n Â i=1\n",
      "\n",
      "COMBatt (x1,\n",
      "\n",
      "xn) =\n",
      "\n",
      "so f tmax(xi)xi\n",
      "\n",
      "(26.6)\n",
      "\n",
      "· ·\n",
      "\n",
      "exp(MAP(xi)) j=1 exp(MAP(x j))\n",
      "\n",
      "so f tmax(xi) =\n",
      "\n",
      "(26.7)\n",
      "\n",
      "Ân\n",
      "\n",
      "where MAP() can be linear or nonlinear.\n",
      "\n",
      "Different from traditional deep learning algorithm, the GNNs have its unique operation–neural aggregation function AGG(). Based on the level of object to ag- gregate, it can be categorized into three speciﬁc types: node-wise neural aggregator AGGnode(), layer-wise neural aggregator AGGlayer(), and path-wise neural aggrega- tor AGGpath().\n",
      "\n",
      "Node-wise Neural Aggregator AGGnode() is the GNN module that aims to aggre-\n",
      "\n",
      "gate the node neighborhoods, which can be described as follows,\n",
      "\n",
      "(i)(k) h v\n",
      "\n",
      "(i)(k = AGGnode(h v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k h u {\n",
      "\n",
      "1)\n",
      "\n",
      "(26.8)\n",
      "\n",
      ",\n",
      "\n",
      ")\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2\n",
      "\n",
      "(i)(k) is the layer indicator, h v\n",
      "\n",
      "1, 2, ...K\n",
      "\n",
      "where i is meta-path (relation) indicator, k is the feature vector of node v for relation Mi at the k-th layer, N i v indicates the neighbourhoods of node v under the relation Mi. Based on the way the the node neighborhoods are aggregated, typically, the node-level neural aggregator can be GCN AGGGCN() (Kipf and Welling, 2017b), GAT AGGGAT () (Veliˇckovi´c et al, 2018) or Message-Passing AGGMPNN() (Gilmer et al, 2017). For the GCN and GAT, the formulations can be described by Equation 8. While for the Message-Passing, the edges are also used during the node-level aggregation. Formally, it can be de- scribed as follows,\n",
      "\n",
      "2{\n",
      "\n",
      "}\n",
      "\n",
      "(i)(k) v\n",
      "\n",
      "(i)(k = AGGnode(h v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k h v {\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k , h u\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k vu\n",
      "\n",
      "1)\n",
      "\n",
      "(26.9)\n",
      "\n",
      ",\n",
      "\n",
      ", h\n",
      "\n",
      ")\n",
      "\n",
      "h\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2\n",
      "\n",
      "(i)(k where h vu bor node u, and bor node and the corresponding edge between them.\n",
      "\n",
      "1)\n",
      "\n",
      "denotes the edge embedding between the target node v and its neigh- indicates a fusion function to combine the target node, its neigh-\n",
      "\n",
      "\u0000\n",
      "\n",
      "{}\n",
      "\n",
      "Layer-wise Neural Aggregator AGGlayer() is the GNN module that aims to ag- gregate the context information from different hops. For example, if layer num- ber k = 2, the GNN gets 1-hop neighborhood information, and if layer number\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "567\n",
      "\n",
      "k = K + 1, the GNN gets K-hop neighborhood information. The larger the k is, the more global information the GNN obtains. Formally, this function can be described as follows,\n",
      "\n",
      "(i)(k) l v\n",
      "\n",
      "(i)(k = AGGlayer(l v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k) , h v\n",
      "\n",
      ")\n",
      "\n",
      "(26.10)\n",
      "\n",
      "\u0000\n",
      "\n",
      "(i)(k) v\n",
      "\n",
      "is the aggregated representation of (k\n",
      "\n",
      "1)\n",
      "\n",
      "hop neighborhood node v\n",
      "\n",
      "where l for relation Mi at the k-th layer.\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "Path-wise Neural Aggregator AGGlayer() is the GNN module that aims to ag- gregate the context information from different relations. Generally, the relation can be described by meta-path (Sun et al, 2011) based contextual search. Formally, this function can be described as follows,\n",
      "\n",
      "(i)(K) (i) v = l v\n",
      "\n",
      "(26.11)\n",
      "\n",
      "p\n",
      "\n",
      "(1) v\n",
      "\n",
      "( M , ...p | v\n",
      "\n",
      ") |\n",
      "\n",
      "pv = AGGpath(p\n",
      "\n",
      ")\n",
      "\n",
      "(26.12)\n",
      "\n",
      "(i) where p v\n",
      "\n",
      "is the aggregated ﬁnal layer representation of node v for relation Mi.\n",
      "\n",
      "Then the ﬁnal node representation is described by the fusion representation from\n",
      "\n",
      "different meta-paths (relations) as follows,\n",
      "\n",
      "( f inal) h v\n",
      "\n",
      "= pv\n",
      "\n",
      "(26.13)\n",
      "\n",
      "Based on the task, we can also compute the graph representation by performing readout function Readout() to aggregate all the nodes’ ﬁnal representations, which can be described as follows,\n",
      "\n",
      "( f inal) v1\n",
      "\n",
      "( f inal) vV\n",
      "\n",
      "g = Readout(h\n",
      "\n",
      ", ...h\n",
      "\n",
      ")\n",
      "\n",
      "(26.14)\n",
      "\n",
      "Typically, we can obtain different levels of graph representations, including node- level, edge-level, and graph-level. The node-level and edge-level representation are the most preliminary representations, which can be learned via graph neural net- work. The graph-level representation is a higher-level representation, which can be obtained by performing the readout function to the node-level and edge-level repre- sentations. Based on the target of the task, the speciﬁc level of graph representations is fed to the next stage.\n",
      "\n",
      "26.3.3 Prediction\n",
      "\n",
      "After the graph representation is learned, they are fed to the prediction stage. De- pends on the task and the target label, there are two types of prediction: classiﬁcation and matching. In the classiﬁcation-based prediction, it assumes that enough labeled anomaly data instances are provided. A good classiﬁer can be trained to identify\n",
      "\n",
      "568\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "if the given graph target is abnormal or not. As mentioned in the issues section, there might be no or few anomaly data instances. In this case, the matching-based prediction is usually used. If there are very few anomaly samples, we learn the rep- resentation of them, and when the candidate sample is similar to one of the anomaly samples, an alarm is triggered. If there is no anomaly sample, we learn the represen- tation of the normal data instance. When the candidate sample is not similar to any of the normal samples, an alarm is triggered.\n",
      "\n",
      "26.4 Taxonomy\n",
      "\n",
      "In this section, we provide the taxonomies of existing GNN-based anomaly detec- tion approaches. Due to the variety of graph data and anomalies, the GNN-based anomaly detection can have multiple taxonomies. Here we provided four types of taxonomy in order to give a quickly understand of the similarity and difference between existing works, including static/dynamic graph taxonomy, homogeneous/ heterogeneous graph taxonomy, plain/attributed graph taxonomy, object taxonomy, and task taxonomy.\n",
      "\n",
      "In task taxonomy, the exiting works can be categorized into GNN-based anomaly detection in ﬁnancial networks, GNN-based anomaly detection in computer net- works, GNN-based anomaly detection in telecom networks, GNN-based anomaly detection in social networks, GNN-based anomaly detection in opinion networks, and GNN-based anomaly detection in sensor networks.\n",
      "\n",
      "In anomaly taxonomy, the existing works can be categorized into node-level anomaly detection, edge-level anomaly detection, and graph-level anomaly detec- tion.\n",
      "\n",
      "In static/dynamic graph taxonomy, the existing works can be categorized into static GNN-based anomaly detection and dynamic GNN-based anomaly detection. In homogeneous/heterogeneous graph taxonomy, the exiting works can be categorized into homogeneous GNN-based anomaly detection and heterogeneous GNN-based anomaly detection.\n",
      "\n",
      "In plain/attributed graph taxonomy, the exiting works can be categorized into plain GNN-based anomaly detection and attributed GNN-based anomaly detection. In object taxonomy, the exiting works can be categorized into: classiﬁcation-\n",
      "\n",
      "based approach and matching-based approach.\n",
      "\n",
      "We present our taxonomy with more details in Table 1.\n",
      "\n",
      "26.5 Case Studies\n",
      "\n",
      "In this section, we provide the case studies to give the details of some representative GNN-based anomaly detection approaches.\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "569\n",
      "\n",
      "Table 26.1: Summary of GNN-based anomaly detection approaches.\n",
      "\n",
      "Static Dynamic\n",
      "\n",
      "Homogeneous Heterogeneous\n",
      "\n",
      "Plain Attributed\n",
      "\n",
      "Approach\n",
      "\n",
      "Year Venue\n",
      "\n",
      "Task\n",
      "\n",
      "Anomaly\n",
      "\n",
      "Model\n",
      "\n",
      "Object\n",
      "\n",
      "GCN, Attention(path) GCN, Attention( f eature,path) GCN, Attention(path) GCN, Attention(node,layer,path) GCN, GRUatt GCN, Attention(node,path) GAT, Attention(path,view) MPNN, Attention(message) GCN, Attention(path)\n",
      "\n",
      "Malicious Account Detection\n",
      "\n",
      "GEM (Liu et al, 2018f)\n",
      "\n",
      "2018 CIKM\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "Cashout User Detection\n",
      "\n",
      "HACUD (Hu et al, 2019b)\n",
      "\n",
      "2019 AAAI\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "Malicious Program Detection\n",
      "\n",
      "DeepHGNN (Wang et al, 2019h) 2019 SDM\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "Malicious Program Detection Malicious Connection Detection Malicious Account Detection\n",
      "\n",
      "MatchGNet (Wang et al, 2019i). 2019 IJCAI\n",
      "\n",
      "Graph\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Matching\n",
      "\n",
      "AddGraph (Zheng et al, 2019)\n",
      "\n",
      "2019 IJCAI\n",
      "\n",
      "Edge\n",
      "\n",
      "Dynamic Homogeneous\n",
      "\n",
      "Plain\n",
      "\n",
      "Matching\n",
      "\n",
      "Classiﬁcation, Matching\n",
      "\n",
      "SemiGNN (Wang et al, 2019b)\n",
      "\n",
      "2019 ICDM\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Real Money Trading Detection\n",
      "\n",
      "MVAN (Tao et al, 2019)\n",
      "\n",
      "2019 KDD\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "Spam Detection\n",
      "\n",
      "GAS (Li et al, 2019a)\n",
      "\n",
      "2019 CIKM\n",
      "\n",
      "Edge\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "Key Player Detection Anomaly User Detection Fraud Detection\n",
      "\n",
      "iDetective (Zhang et al, 2019a)\n",
      "\n",
      "2019 CIKM\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "GAL (Zhao et al, 2020f)\n",
      "\n",
      "2020 CIKM\n",
      "\n",
      "Node\n",
      "\n",
      "Static\n",
      "\n",
      "Homogeneous Attributed\n",
      "\n",
      "GCN/GAT\n",
      "\n",
      "Matching\n",
      "\n",
      "GCN, Attention(node)\n",
      "\n",
      "CARE-GNN (Dou et al, 2020)\n",
      "\n",
      "2020 CIKM\n",
      "\n",
      "Node\n",
      "\n",
      "Static Heterogeneous Attributed\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "26.5.1 Case Study 1: Graph Embeddings for Malicious Accounts\n",
      "\n",
      "Detection\n",
      "\n",
      "Graph embeddings for malicious accounts detection (GEM) (Liu et al, 2018f) is the ﬁrst attempt to apply the GNN to anomaly detection. The aim of GEM is to detect the malicious account at Alipay pay, a mobile cashless payment platform.\n",
      "\n",
      "The graph constructed from the raw data is static and heterogeneous. The con- strued graph G = (V , E ) consists of 7 types of nodes, including account typed nodes (U) and 6 types of device typed nodes (phone number (PN), User Machine ID (UMID), MAC address (MACA), International Mobile Subscriber Identity (IMSI), Alipay Device ID (APDID) and a random number generated via IMSI and IMEI (TID), such that V = U T ID. To over- [ come the heterogeneous graph challenge and make GNN applicable to the graph, through graph transformation, GEM constructs a 6-channel graph ˆG = Gi = (Vi, Ei, Ai), i = 1, 2, ..., = 6. In particular, 6 types of edges are specif- M ically modeled to capture the edge heterogeneity, e.g., account connects phone num- UMID), account connects MAC ad- ber (U IMSI), account connects Alipay dress (U Device ID (U T ID). As the activity attributes are constructed, the constructed graph is an attributed graph. After the graphs are constructed and transformed, GEM performs a graph convolutional net- work to aggregate the neighborhood on each channel graph. As each channel graph is treated as a homogeneous graph corresponding to a speciﬁc relation, GNN can be directly applied to each channel graph.\n",
      "\n",
      "PN\n",
      "\n",
      "UMID\n",
      "\n",
      "MACA\n",
      "\n",
      "IMSI\n",
      "\n",
      "APDID\n",
      "\n",
      "[\n",
      "\n",
      "[\n",
      "\n",
      "[\n",
      "\n",
      "[\n",
      "\n",
      "[\n",
      "\n",
      "Gi| {\n",
      "\n",
      "with\n",
      "\n",
      "M\n",
      "\n",
      "|\n",
      "\n",
      "|}\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "PN), account connects UMID (U\n",
      "\n",
      "!\n",
      "\n",
      "! MACA), account connects IMSI (U\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "APDID) and account connects TID (U\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "570\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "During the graph representation learning stage, the node aggregated representa- (i)(k) is computed by performing a GCN aggregator AGGGCN(). To get the path tion h v aggregated representation, it adopts the attentionally feature fusion to fuse the node aggregated representation obtained in each channel graph G i. Besides, an activity feature for each node is constructed, and it adds the linear mapping of this activity feature to the attentional feature fusion of the path aggregated representations. For- mally, the GNN operations can be described as follow. Node-wise aggregation:\n",
      "\n",
      "(i)(k) h v\n",
      "\n",
      "(i)(k = AGGnode(h v (i)(k = AGGGCN(h v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k u\n",
      "\n",
      "1)\n",
      "\n",
      ",\n",
      "\n",
      ")\n",
      "\n",
      "h {\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2\n",
      "\n",
      "(26.15)\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k h u\n",
      "\n",
      "1)\n",
      "\n",
      ",\n",
      "\n",
      ")\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2\n",
      "\n",
      "{\n",
      "\n",
      "Path-wise aggregation:\n",
      "\n",
      "(1)(k)) (k) v = MAPlinear(xv) +COMBatt (h p v\n",
      "\n",
      "( M , ..., h | v\n",
      "\n",
      ")(k) |\n",
      "\n",
      "(26.16)\n",
      "\n",
      ")\n",
      "\n",
      "Layer-wise aggregation:\n",
      "\n",
      "(K) (K) v = p l v\n",
      "\n",
      "(26.17)\n",
      "\n",
      "Final node representation:\n",
      "\n",
      "( f inal) h v\n",
      "\n",
      "(K) = l v\n",
      "\n",
      "(26.18)\n",
      "\n",
      "where K indicates the number of the layers.\n",
      "\n",
      "The object of GEM is classiﬁcation. It feeds the learned account node embedding\n",
      "\n",
      "to a standard logistic loss function.\n",
      "\n",
      "26.5.2 Case Study 2: Hierarchical Attention Mechanism based\n",
      "\n",
      "Cash-out User Detection\n",
      "\n",
      "Hierarchical attention mechanism based cash-out user detection (HACUD) (Hu et al, 2019b) applied the GNN to the fraud user detection at Credit Payment Ser- vices platform, where the fraud user performs the cash-out fraud, that pursues cash gains with illegal or insincere intent.\n",
      "\n",
      "HACUD also constructs a static heterogeneous graph from the raw data. Specif- ically, it consists of multiple types of nodes (i.e., User (U), Merchant (M), Device (D)) with rich attributes and relations (i.e., fund transfer relation between users and transaction relation between users and merchants). Different from the way GEM deal with the graph heterogeneity issues, during the graph transformation stage, HACUD only models the user nodes and considers two speciﬁc types of meta-paths (relations) between pairwise of users, including User-(fund transfer)-User (UU) and User-(transaction)-Merchant-(transaction)-User (UMU) and constructs a 2-channel U. graph, such that\n",
      "\n",
      "ˆG =\n",
      "\n",
      "Gi = (Vi, Ei, Ai), i = 1, ...,\n",
      "\n",
      "= 2 and Vi 2\n",
      "\n",
      "with\n",
      "\n",
      "Gi| {\n",
      "\n",
      "M |\n",
      "\n",
      "M\n",
      "\n",
      "|}\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "571\n",
      "\n",
      "The two selected meta-paths capture different semantics. For example, the UU path connects users having fund transfers from one to another, while the UMU connects users having transactions with the same merchants. Then each channel graph is ho- mogeneous and can work with GNN directly. As the user attributes are available, the constructed graph is attributed.\n",
      "\n",
      "In the graph representation stage, the node-wise aggregation is performed to each channel graph via a convolutional graph network. Different from GEM (Liu et al, 2018f), it adds and joins the user feature xv to the aggregated node representation in an attentional way. Then the node-wise aggregation extends to a 3-step proce- dure, including (a) initial node-wise aggregation, (b) feature fusion, and (c) feature (i) attention. After the initial aggregated node representation ˜h is computed vis GCN v AGGGCN(), it is fused with user feature xv through a feature fusion. Next, it per- forms the feature attention. Since only 1-hop neighborhoods are considered, there is (i) no layer-wise aggregation, and the ﬁnal node-wise aggregated representations h v are fed to the path-wise aggregation directly. Formally, it can be described as fol- lows, Node-wise aggregation:\n",
      "\n",
      "(a)Initial node-wise aggregation:\n",
      "\n",
      "(i) ˜h v = AGGnode(h = AGGGNN(h\n",
      "\n",
      "(i) (i) v , h u }u { 2 (i) (i) v , h u }u { 2\n",
      "\n",
      ")\n",
      "\n",
      "N i v\n",
      "\n",
      "(26.19)\n",
      "\n",
      ")\n",
      "\n",
      "N i v\n",
      "\n",
      "(b)Feature fusion:\n",
      "\n",
      "(i) v = MAPnonlinear(CONCAT (MAPlinear( ˜h f\n",
      "\n",
      "(i) v ), MAPlinear(xv)))\n",
      "\n",
      "(26.20)\n",
      "\n",
      "(c)Feature attention:\n",
      "\n",
      "(i) v = MAPnonlinear(MAPnonlinear(CONCAT (MAPlinear(xv), f\n",
      "\n",
      "(i) v ))\n",
      "\n",
      "(26.21)\n",
      "\n",
      "a\n",
      "\n",
      "(i) v = so f tmax(a\n",
      "\n",
      "(i) v )\n",
      "\n",
      "(i) v\n",
      "\n",
      "(26.22)\n",
      "\n",
      "h\n",
      "\n",
      "f\n",
      "\n",
      "K\n",
      "\n",
      "Path-wise aggregation:\n",
      "\n",
      "(0) pv = AGGpath(h v\n",
      "\n",
      "(1) v ) (1) v )\n",
      "\n",
      ", h\n",
      "\n",
      "(26.23)\n",
      "\n",
      "(0) = COMBatt (h v\n",
      "\n",
      ", h\n",
      "\n",
      "Final node representation:\n",
      "\n",
      "( f inal) h v\n",
      "\n",
      "= MLP(pv)\n",
      "\n",
      "(26.24)\n",
      "\n",
      "where there is no layer indicator k.\n",
      "\n",
      "denotes the element-wise product. As only one-hop information is used,\n",
      "\n",
      "J\n",
      "\n",
      "572\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "As same as GEM, the object of HACUD is classiﬁcation. It feeds the learned\n",
      "\n",
      "user node embedding to a standard logistic loss function.\n",
      "\n",
      "26.5.3 Case Study 3: Attentional Heterogeneous Graph Neural\n",
      "\n",
      "Networks for Malicious Program Detection\n",
      "\n",
      "Attentional heterogeneous graph neural network for malicious program detection (DeepHGNN) (Wang et al, 2019h) applied the GNN to the malicious program de- tection in a computer system of an enterprise network.\n",
      "\n",
      "The raw data is a large volume of system behavioral data with rich informa- tion on program/process level events. A static heterogeneous graph is constructed to model the program behaviors. Formally, given the program event data across many machines within a time window (e.g., 1 day), a heterogeneous graph G = (V , E ) is constructed for the target program. V denotes a set of nodes, with each one repre- senting an entity of three types: process (P), ﬁle (F), and INETSocket (I). Namely, V = P I. E denotes a set of edges (vs, vd, r) between the source entity vs and destination entity vd with relation r. To address the heterogeneous graph challenges, it takes three types of relations, including: (1) a process forking another process (P F), and (3) a process connecting to an In- ternet socket (P I). Similar to GEM, DeepHGNN designs a graph transformation module to transform the heterogeneous graph to a 3-channel graph guided by above ˆG = three meta-paths (relations), such that Gi| |} { = 3 and Vi = V . The attributes are constructed for each node. Since the with process node, ﬁle node, and INETSocket node has quite different attributes, the (i)(gstat) graph statistic features x v\n",
      "\n",
      "F\n",
      "\n",
      "[\n",
      "\n",
      "[\n",
      "\n",
      "P), (2) a process accessing a ﬁle (P\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "Gi = (Vi, Ei, Ai), i = 1, 2, ...,\n",
      "\n",
      "M\n",
      "\n",
      "|\n",
      "\n",
      "M\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "are constructed and act as the node attributes.\n",
      "\n",
      "Similar to the GEM and HACUD, DeepHGNN also adopts the graph convo- lutional network AGGGCN() for node-wise aggregation. Three layers are used in order to capture program behavior within 3-hop contexts. Different from GEM and HACUD, DeepHGNN uses the graph statistic node attributes as the initialization of the node representation for each channel graph. After the three node-wise aggre- gation and layer-wise aggregation, the node representations from different channel graphs are fused via the attentional feature fusion as GEM and HACUD. Formally, it can be described as follows, Node-wise aggregation:\n",
      "\n",
      "(i)(0) h v\n",
      "\n",
      "(i)(gstat) = x v\n",
      "\n",
      "(26.25)\n",
      "\n",
      "(i)(k) h v\n",
      "\n",
      "(i)(k = AGGnode(h v (i)(k = AGGGNN(h v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k h u { ,\n",
      "\n",
      "1)\n",
      "\n",
      ",\n",
      "\n",
      ")\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2 1)\n",
      "\n",
      "(26.26)\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k h u {\n",
      "\n",
      ")\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i v\n",
      "\n",
      "}u 2\n",
      "\n",
      "Layer-wise aggregation:\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "573\n",
      "\n",
      "(i)(k) l v\n",
      "\n",
      "(i)(k) = h v\n",
      "\n",
      "(26.27)\n",
      "\n",
      "Path-wise aggregation:\n",
      "\n",
      "(1)(K)) v\n",
      "\n",
      "( M | v\n",
      "\n",
      ")(K) |\n",
      "\n",
      "(26.28)\n",
      "\n",
      "pv = COMBatt (l\n",
      "\n",
      ", ..., l\n",
      "\n",
      ")\n",
      "\n",
      "Final node representation:\n",
      "\n",
      "( f inal) h v\n",
      "\n",
      "= pv\n",
      "\n",
      "(26.29)\n",
      "\n",
      "The object of DeepHGNN is classiﬁcation. However, it is different from GEM and HACUD, which simply build single classiﬁers for all the samples. DeepHGNN formulates the problem of program reidentiﬁcation in malicious program detection. The graph representation learning aims to learn the representation of the normal target program, and each target program learns a unique classiﬁer. Given a target program with corresponding event data during a time window U = and a claimed name/ID, the system checks whether it belongs to the claimed name/ID. If it matches the behavior pattern of the claimed name/ID, the predicted label should be +1; otherwise, it should be\n",
      "\n",
      "e1, e2, ... } {\n",
      "\n",
      "1.\n",
      "\n",
      "\u0000\n",
      "\n",
      "26.5.4 Case Study 4: Graph Matching Framework to Learn the Program Representation and Similarity Metric via Graph Neural Networks for Unknown Malicious Program Detection\n",
      "\n",
      "Graph matching framework to learn the program representation and similarity met- ric via graph neural network (MatchGNet) (Wang et al, 2019i) is another GNN- based anomaly detection approach for malicious program detection in a computer system of an enterprise network. MatchGNet is different from DeepHGNN in ﬁve aspects: (1) after the graph transformation, the resulted channel graph only keep the target type node – process node, which is similar to HACUD, (2) the raw program attributes are used as the program node representation initialization, (3) the GNN aggregation is conducted hierarchically in node-wise, layer-wise, and path-wise, (4) the anomaly target is the subgraph of the target program (5) the ﬁnal graph repre- sentation is fed to a similarity learning framework with contrastive loss to deal with the unknown anomaly.\n",
      "\n",
      "It follows a similar style to construct the static heterogeneous graph from system behavioral data. In the graph transformation, it adopts three meta-paths (relations): P), two processes accessing the same ﬁle a process forking another process (P P) with (P each one deﬁning a unique relationship between two processes. Based on them, a ˆG = 3-channel graph is constructed from the the heterogeneous graph, such that P. Then the GNN can be with Gi| {\n",
      "\n",
      "! P), and two processes opening the same internet socket (P\n",
      "\n",
      "F\n",
      "\n",
      "I\n",
      "\n",
      "!\n",
      "\n",
      "!\n",
      "\n",
      "Gi = (Vi, Ei, Ai), i = 1, ...,\n",
      "\n",
      "= 3 and Vi 2\n",
      "\n",
      "M |\n",
      "\n",
      "M |\n",
      "\n",
      "|}\n",
      "\n",
      "|\n",
      "\n",
      "574\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "directly applied to each channel graph. As only process typed nodes are available, we use the raw attributes of these process xv as the node representation initialization. During the graph representation stage, a hierarchical attentional graph neural network is designed, including node-wise attentional neural aggregator, layer-wise dense-connected neural aggregator, and path-wise attentional neural aggregator. In particular, the node-wise attentional neural aggregator aims to generate node em- beddings by selectively aggregating the entities in each channel graph based on ran- dom walk scores a i (u). Layer-wise dense-connected neural aggregator aggregates the node embeddings generated from different layers towards a dense-connected node embedding. Path-wise attentional neural aggregator performs attentional feature fu- sion of the layer-wise dense-connected representations. In the end, the ﬁnal node representation is used as the graph representation. Formally, it can be described as follows, Node-wise aggregation:\n",
      "\n",
      "(i)(0) h v\n",
      "\n",
      "(26.30)\n",
      "\n",
      "= xv\n",
      "\n",
      "(i)(k) h v\n",
      "\n",
      "(i)(k v\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k u\n",
      "\n",
      "1)\n",
      "\n",
      "= AGGnode(h h { (i)(k = MLP((1 + e (k))h v\n",
      "\n",
      ",\n",
      "\n",
      ")\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "N i }u v 2 a i + Â N i u v 2\n",
      "\n",
      "(26.31)\n",
      "\n",
      "1)\n",
      "\n",
      "(i)(k u\n",
      "\n",
      "1)\n",
      "\n",
      ")\n",
      "\n",
      "(u)(:)h\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "Layer-wise aggregation:\n",
      "\n",
      "(i)(k) v\n",
      "\n",
      "(i)(0) = AGGlayer(h v\n",
      "\n",
      "(i)(1) v\n",
      "\n",
      "(i)(k) , ...l v\n",
      "\n",
      ", l\n",
      "\n",
      ")\n",
      "\n",
      "l\n",
      "\n",
      "(26.32)\n",
      "\n",
      "(i)(0) v\n",
      "\n",
      "(i)(1) v\n",
      "\n",
      "(i)(k) v\n",
      "\n",
      "= MLP(CONCAT (h\n",
      "\n",
      "; ...l\n",
      "\n",
      "))\n",
      "\n",
      "; l\n",
      "\n",
      "Path-wise aggregation:\n",
      "\n",
      "(i)(K)) v\n",
      "\n",
      "( M , ..., l | v\n",
      "\n",
      ")(K) |\n",
      "\n",
      "(26.33)\n",
      "\n",
      "pv = COMBatt (l\n",
      "\n",
      ")\n",
      "\n",
      "Final node representation:\n",
      "\n",
      "( f inal) h v\n",
      "\n",
      "(26.34)\n",
      "\n",
      "= pv\n",
      "\n",
      "Final graph representation:\n",
      "\n",
      "( f inal) hGv = h v\n",
      "\n",
      "(26.35)\n",
      "\n",
      "where k indicates the number of layers, and e is a small number. Different from GEM, HACUD, and DeepHGNN, the object of MatchGNet is matching. The ﬁnal graph representation is fed to a similarity learning framework with contrastive loss to deal with the unknown anomaly. During the training, P pairs of program graph 1, 2, ...P snapshots (Gi(1), Gi(2)), i are collected with corresponding ground truth } 2{ +1, pairing information yi 2{ . If the pair of graph snapshots belong to the same program, the ground truth label is yi = +1; otherwise, its ground truth label is yi = 1. For each pair of program snapshots, a cosine score function is used to\n",
      "\n",
      "1 }\n",
      "\n",
      "\u0000\n",
      "\n",
      "\u0000\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "575\n",
      "\n",
      "measure the similarity of the two program embeddings, and the output is deﬁned as follows:\n",
      "\n",
      "Sim(Gi(1), Gi(2)) = cos((hGi(1) , hGi(2)))\n",
      "\n",
      "hGi(1) · hGi(1)|| · ||\n",
      "\n",
      "hGi(2) hGi(2) ||\n",
      "\n",
      "(26.36)\n",
      "\n",
      "=\n",
      "\n",
      "||\n",
      "\n",
      "Correspondingly, our objective function can be formulated as:\n",
      "\n",
      "P Â i=1\n",
      "\n",
      "yi)2\n",
      "\n",
      "` =\n",
      "\n",
      "(Sim(Gi(1), Gi(2))\n",
      "\n",
      "(26.37)\n",
      "\n",
      "\u0000\n",
      "\n",
      "26.5.5 Case Study 5: Anomaly Detection in Dynamic Graph Using\n",
      "\n",
      "Attention-based Temporal GCN\n",
      "\n",
      "Anomaly detection in dynamic graph using attention-based temporal GCN (Add- Graph) (Zheng et al, 2019) is the ﬁrst work that applies the GNN to solve the prob- lem of anomaly edge detection in the dynamic graph. It focuses on the modeling of the dynamic graph via GNN and performs anomaly connection detection in telecom networks and social networks. The graphs are constructed from the edge stream data, and the constructed graphs are dynamic, homogeneous, and plain.\n",
      "\n",
      "The basic idea is to build a framework to describe the normal edges by using all possible features in the graph snapshots in the training phase, including structural, content, and temporal features. Then at the prediction stage, the matching objective is used similar to MatchGNet. In particular, AddGraph applies GCN AGGGCN() to compute the new current state of a node ct v by aggregating its neighborhoods in the current snapshot graph, which can be described as follows,\n",
      "\n",
      "v = AGGGCN(ht ct 1 \u0000 v\n",
      "\n",
      ")\n",
      "\n",
      "(26.38)\n",
      "\n",
      "As the state of a node ct v can be computed by aggregating the neighboring hidden states in the previous timestamp t 1, the node hidden states in a short window w can be obtained and combined to get the short-term embedding st v. In particular, an attentional feature fusion is used to combine these node hidden states in a short window, as follows,\n",
      "\n",
      "\u0000\n",
      "\n",
      ", ..., ht 1 \u0000 v Then short-term embedding st v and current state ct v are fed to GRU, a classic recur- rent neural network, to compute the current hidden state that encoding the dynamics within the graph. This stage can be described as follows:\n",
      "\n",
      "v = COMBatt (ht st \u0000 v\n",
      "\n",
      "w\n",
      "\n",
      ")\n",
      "\n",
      "(26.39)\n",
      "\n",
      "v = GRU(ct ht\n",
      "\n",
      "v, st v)\n",
      "\n",
      "(26.40)\n",
      "\n",
      "576\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "The object of AddGraph is matching. The hidden state of the nodes at each times- tamp are used to calculate the anomalous probabilities of an existing edge and a negative sampled edge, and then feed them to a margin loss.\n",
      "\n",
      "26.5.6 Case Study 6: GCN-based Anti-Spam for Spam Review\n",
      "\n",
      "Detection\n",
      "\n",
      "GCN-based anti-spam (GAS) (Li et al, 2019a) applies the GNN in the spam re- view detection at the e-commerce platform Xianyu. Similar to previous works, the constructed graph is static, heterogeneous and attributed, such that G = (U , I , E ). There are two types of nodes: user nodes U and item nodes I . The edges E are a set of comments. Different from previous works, the edges E are the anomalies tar- gets. Moreover, as each edge represents a sentence, edge modeling is complicated, and the number of edge types increases dramatically. To better capture the edge representation, the message-passing-like GNN is used. The edge-wise aggregation is proposed by concatenation of previous representation of the edge itself hk 1 and \u0000 iu 1 corresponding user node representation hk 1 To get \u0000 \u0000 u the initial attributes of edge, the word2vec word embedding for each word in the comments of the edges is extracted via the embedding function pre-training on a million-scale comment dataset. Then the word embedding of each words in an edge of comments w0, w1, ...wn is fed to TextCNN() function to get the comments em- bedding h0 iu, which is used as the initial attributes of edge. Then the edge-wise ag- gregation is deﬁned as: Edge-wise aggregation:\n",
      "\n",
      ", item node representation hk i\n",
      "\n",
      "h0 iu = TextCNN(w0, w1, ...wn)\n",
      "\n",
      "(26.41)\n",
      "\n",
      "iu = MAPnonlinear(CONCAT (hk hk 1 \u0000 iu\n",
      "\n",
      ", hk i\n",
      "\n",
      ", hk 1 \u0000 u\n",
      "\n",
      "1\n",
      "\n",
      "(26.42)\n",
      "\n",
      "))\n",
      "\n",
      "\u0000\n",
      "\n",
      "On the other hand, the node-wise aggregation also needs to take the edges into con- sideration. The node-wise aggregation is performed by attention feature fusion of the target node and its connected edge followed by a non-linear mapping, which can be described with (a) user node-wise aggregation, and (b) item node-wise aggrega- tion as follows: Node-wise aggregation:\n",
      "\n",
      "(a)User node-wise aggregation:\n",
      "\n",
      "u = CONCAT (MAPlinear(hk hk 1 \u0000 u\n",
      "\n",
      "), MAPnonlinear(COMBatt (hk 1 \u0000 u\n",
      "\n",
      ", hk ,CONCAT (hk 1 1 \u0000 \u0000 i iu (26.43)\n",
      "\n",
      ")))\n",
      "\n",
      "(b)Item node-wise aggregation:\n",
      "\n",
      "26 Graph Neural Networks in Anomaly Detection\n",
      "\n",
      "577\n",
      "\n",
      ", hk ,CONCAT (hk 1 \u0000 \u0000 u iu (26.44) where k is the layer indicator. The ﬁnal edge representation is computed by con- catenation of the raw edge embedding h0 iu, corresponding new user node embedding hK i as follows: Final edge representation:\n",
      "\n",
      "i = CONCAT (MAPlinear(hk hk i\n",
      "\n",
      "), MAPnonlinear(COMBatt (hk i\n",
      "\n",
      "1 \u0000\n",
      "\n",
      "1 \u0000\n",
      "\n",
      "1\n",
      "\n",
      ")))\n",
      "\n",
      "iu, new edge embedding hK u , and corresponding new item node embedding hK\n",
      "\n",
      "f inal iu = CONCAT (h0 h\n",
      "\n",
      "vu, hK\n",
      "\n",
      "vu, hK\n",
      "\n",
      "u , hK i )\n",
      "\n",
      "(26.45)\n",
      "\n",
      "The object of GAS is classiﬁcation, and the ﬁnal edge representation is fed to a standard logistic loss function.\n",
      "\n",
      "26.6 Future Directions\n",
      "\n",
      "GNNs on anomaly detection is an important research direction, which leverages multi-source, multi-view features extracted from both content and structure for anomaly sample analysis and detection. It plays a key role in numerous high-impact applications in areas such as cyber-security, ﬁnance, e-commerce, social network, industrial monitoring, and many more mission-critical tasks. Due to the multiple is- sues from data, model and task, it still needs a lot of effort in the ﬁeld. The future works are mainly lying in two perspectives: anomaly analysis and machine learning. From an anomaly analysis perspective, there are still a lot of research questions. How to deﬁne and identify the anomalies in the graph in the different tasks? How to effectively convert the large-scale raw data to the graph? How to effectively leverage the attributes? How to model the dynamic during the graph construction? How to keep the heterogeneity during the graph construction? Recently, due to the data- speciﬁc and task-speciﬁc issues, the applications of GNN-based anomaly detection are still limited. There is still a lot of potential scenarios that can be applied.\n",
      "\n",
      "From a machine learning perspective, lots of issues need to be addressed. How to model the graph? How to represent the graph? How to leverage the context? How to fuse the content and structure features? Which part of the structure to capture, local or global? How to provide the model explainability? How to protect the model from adversarial attacks? How to overcome the time-space scalability bottleneck. Recently, lots of contributions have been made from the machine learning perspec- tive. However, due to the unique characteristics of the anomaly detection problem, which GNNs to use and how to apply GNNs are still critical questions. Further work will also beneﬁt from the new ﬁndings and new models in the graph machine learning community.\n",
      "\n",
      "578\n",
      "\n",
      "Shen Wang, Philip S. Yu\n",
      "\n",
      "Editor’s Notes: Graph neural networks for anomaly detection can be con- sidered as a downstream task of graph representation learning, where the long-term challenges in anomaly detection are coupled with the vulnera- bility of graph neural networks such as scalability discussed in Chapter 6 and robustness discussed in Chapter 8. Graph neural networks for anomaly detection also further beneﬁts a wide range of downstream tasks in various interesting, important, yet usually challenging areas such as anomaly detec- tion in dynamic networks, spam review detection for recommender system, and malware program detection, which are highly relevant to the topics in- troduced in Chapters 15, 19, and 22.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b525b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8f620db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3e0c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b2cbbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 13079\n",
      "Embedding Cost in USD: 0.026158\n"
     ]
    }
   ],
   "source": [
    "# If you want to calculate the cost\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.002:.6f}')\n",
    "    \n",
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7d9d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain (\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "output_summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a563f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chapter provides an overview of the use of graph neural networks (GNNs) in anomaly detection, focusing on the unique challenges in this domain. It discusses existing works that apply GNNs in anomaly detection and highlights the benefits of using graph-based methods. The chapter also presents a summary of the general pipeline and taxonomies of GNN-based anomaly detection approaches. It addresses data-specific, task-specific, and model-specific issues that arise in anomaly detection. The standard pipeline of GNN-based anomaly detection is described, including graph construction and transformation, graph representation learning, and prediction. The chapter concludes with case studies of representative GNN-based anomaly detection approaches, including attentional heterogeneous graph neural networks for malicious program detection and graph matching framework to learn the program representation and similarity metric for unknown malicious program detection. Additionally, the chapter provides a detailed taxonomy of existing GNN-based anomaly detection approaches, categorizing them based on various factors such as graph type, attribute type, and task type. The chapter also discusses the framework for describing normal edges using various features in the graph snapshots during the training phase and the matching objective used in the prediction stage. It further presents a case study on GCN-based anti-spam for spam review detection, highlighting the use of GNNs in a static, heterogeneous, and attributed graph. The chapter concludes with future directions for research in GNN-based anomaly detection, addressing both anomaly analysis and machine learning perspectives.\n"
     ]
    }
   ],
   "source": [
    "print(output_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6efd17",
   "metadata": {},
   "source": [
    "### Summarizing with LangChain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "352ceeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.utilities import WikipediaAPIWrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "77969a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e18604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
    "wikipedia = WikipediaAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf573112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool (\n",
    "        name='Wikipedia',\n",
    "        func=wikipedia.run,\n",
    "        description='Using wikipedia for getting information'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34287e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8f52417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find information about clean architecture in software design.\n",
      "Action: Wikipedia\n",
      "Action Input: Clean architecture\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Hexagonal architecture (software)\n",
      "Summary: The hexagonal architecture, or ports and adapters architecture, is an architectural pattern used in software design. It aims at creating loosely coupled application components that can be easily connected to their software environment by means of ports and adapters. This makes components exchangeable at any level and facilitates test automation.\n",
      "\n",
      "Page: Single-responsibility principle\n",
      "Summary: The single-responsibility principle (SRP) is a computer programming principle that states that \"A module should be responsible to one, and only one, actor.\" The term actor refers to a group (consisting of one or more stakeholders or users) that requires a change in the module.\n",
      "Robert C. Martin, the originator of the term, expresses the principle as, \"A class should have only one reason to change\". Because of confusion around the word \"reason\" he has also clarified saying that the \"principle is about people.\" In some of his talks, he also argues that the principle is, in particular, about roles or actors. For example, while they might be the same person, the role of an accountant is different from a database administrator. Hence, each module should be responsible for each role.\n",
      "\n",
      "Page: SOLID\n",
      "Summary: In software engineering, SOLID is a mnemonic acronym for five design principles intended to make object-oriented designs more understandable, flexible, and maintainable. The principles are a subset of many principles promoted by American software engineer and instructor Robert C. Martin, first introduced in his 2000 paper Design Principles and Design Patterns discussing software rot.: 2–3 The SOLID ideas are\n",
      "\n",
      "The Single-responsibility principle: \"There should never be more than one reason for a class to change.\" In other words, every class should have only one responsibility.\n",
      "The Open–closed principle: \"Software entities ... should be open for extension, but closed for modification.\"\n",
      "The Liskov substitution principle: \"Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\" See also design by contract.\n",
      "The Interface segregation principle: \"Clients should not be forced to depend upon interfaces that they do not use.\"\n",
      "The Dependency inversion principle: \"Depend upon abstractions, [not] concretions.\"The SOLID acronym was introduced later, around 2004, by Michael Feathers.Although the SOLID principles apply to any object-oriented design, they can also form a core philosophy for methodologies such as agile development or adaptive software development.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Clean architecture in software design is an architectural pattern that aims to create loosely coupled application components that can be easily connected to their software environment. It promotes the use of ports and adapters to make components exchangeable at any level and facilitate test automation. It is based on principles such as the single-responsibility principle, the open-closed principle, the Liskov substitution principle, the interface segregation principle, and the dependency inversion principle.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_executor.run('Please help me by providing a short summary of what is clean architecture in software design?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "096eab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean architecture in software design is an architectural pattern that aims to create loosely coupled application components that can be easily connected to their software environment. It promotes the use of ports and adapters to make components exchangeable at any level and facilitate test automation. It is based on principles such as the single-responsibility principle, the open-closed principle, the Liskov substitution principle, the interface segregation principle, and the dependency inversion principle.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad5b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
