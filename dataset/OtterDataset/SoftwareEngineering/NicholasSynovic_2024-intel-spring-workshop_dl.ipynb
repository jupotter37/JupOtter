{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A DL Model From Scratch\n",
    "\n",
    "> A demonstration of DL pipelines and how to develop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "This notebook is to help those attending my 2024 Intel oneAPI Workshop on reusing and extending deep learning models.\n",
    "\n",
    "This notebook assumes that you have reviewed the [Machine Learning notebook](ml.ipynb) already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Pipelines\n",
    "\n",
    "Machine learning and deep learning pipelines are **very similar**!\n",
    "\n",
    "The biggest difference is the amount of software engineering involved to get started with deep learning.\n",
    "\n",
    "Thankfully, this is changing!\n",
    "\n",
    "![ML pipeline stages image](images/ml_pipeline_stages.png)\n",
    "\n",
    "- Yibo Wang, Ying Wang, Tingwei Zhang, Yue Yu, Shing-Chi Cheung, Hai Yu, and Zhiliang Zhu. 2023. *Can Machine Learning Pipelines Be Better Configured*? In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2023). Association for Computing Machinery, New York, NY, USA, 463â€“475. [https://doi.org/10.1145/3611643.3616352](https://doi.org/10.1145/3611643.3616352)\n",
    "\n",
    "ML pipelines expand the concept of a data and ETL pipelines by including *feedback loops*, *feature engineering*, and ML specific stages including *training*, *evaluation*, and *deployment*.\n",
    "\n",
    "Furthermore, Machine Learning Operations (MLOps) (an extension of DevOps practices aimed at machine and deep learning) now takes into consideration the state of the model post-deployment and how to update the model to continously match the requirements.\n",
    "\n",
    "While MLOps is an interesting and exciting topic, **it is not covered** in this workshop.\n",
    "\n",
    "However, please take a look at [Intel's MLOps Professional course](https://www.intel.com/content/www/us/en/developer/certification/mlops.html) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback Loops\n",
    "\n",
    "Machine learning is not strictly an engineering task, but also a scientific one.\n",
    "\n",
    "This is to say that when you train a model on a dataset, you may not get the best result the first time.\n",
    "\n",
    "Different model architectures, implemntations, hyper-parameters, and features may result in better or worse models.\n",
    "\n",
    "Thus, while you need to be a software engineer to build a machine learning pipeline, you also need to be a computer scientist and explore different model configurations to identify which best suits your needs.\n",
    "\n",
    "For this workshop, we are less interested in the software engineering aspect, and more interested in the computer science one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "Feature engineering is the act of taking a data source and undergoing a *data pipeline* to **extract** relevant features of the data to train a ML model on.\n",
    "\n",
    "Given that we have to extract relevant features, an ETL pipeline is a good first choice for designing a data pipeline for a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Specific Stages\n",
    "\n",
    "**Training** is the process of taking your engineered data and processing it with an algorithm that updates its underlying weights continously as the data is passed through it (this is the core of ML and DL).\n",
    "\n",
    "**Evaluation** is the process of taking labelled testing data and passing them into your trained ML model and computing metrics such as accuracy, precision, and recall.\n",
    "\n",
    "**Deployment** is the process of actually deploying your model to within an application for users to provide completely unseen data to your model.\n",
    "\n",
    "There is enough academic and professional literature on all three of these stages to fill several volumes, so for conciseness, I will not expand on the intricies of these here until relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time To Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install progress ucimlrepo numpy pandas \"unidist[all]\" \"modin[all]\" scikit-learn scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download (Extract) Dataset From UCI Machine Learning Repository\n",
    "\n",
    "> Wine dataset hosted on the *UCI Machine Learning Repository* ([source](https://archive.ics.uci.edu/dataset/109/wine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo  import fetch_ucirepo\n",
    "from ucimlrepo.dotdict import dotdict\n",
    "from pandas import DataFrame\n",
    "from numpy import ndarray\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# Random state value to use for consitency\n",
    "RANDOM_STATE: int = 42\n",
    "\n",
    "# Download dataset\n",
    "wine: dotdict = fetch_ucirepo(id=109)\n",
    "\n",
    "# Extract dataset as DataFrame\n",
    "wineDF: DataFrame = wine[\"data\"][\"original\"]\n",
    "\n",
    "# Get column names of Wine dataset\n",
    "columns: List[str] = wineDF.columns.to_list()\n",
    "\n",
    "# Get number of rows of Wine dataset\n",
    "rowCount: int = wineDF.shape[0]\n",
    "\n",
    "# Create Wine features dataframe (excludes class labels)\n",
    "wineFeaturesDF: DataFrame = wineDF.drop(labels=\"class\", axis=1, inplace=False)\n",
    "\n",
    "# Convert DataFrames to NDArrays for ease of use\n",
    "wineFeaturesNDArrary: ndarray = wineFeaturesDF.to_numpy()\n",
    "wineNDArray: ndarray = wineDF.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset By Standardizing It\n",
    "\n",
    "Currently our dataset has values that are much larger and much smaller than one another.\n",
    "\n",
    "We have to standardize the data within this dataset prior to performing any machine learning on it.\n",
    "\n",
    "As we are manually adjusting the features of the dataset, our actions are henceforth called *feature engineering*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler preprocessor to remove the mean and scale to unit variance\n",
    "scaler: StandardScaler = StandardScaler()\n",
    "scaledWineFeaturesNDArray: ndarray = scaler.fit_transform(X=wineFeaturesNDArrary)\n",
    "\n",
    "# Take the scaled data and create a complete DataFrame object representing the data\n",
    "scaledWineDF: DataFrame = DataFrame(data=scaledWineFeaturesNDArray)\n",
    "scaledWineDF[\"class\"] = wineDF[\"class\"]\n",
    "scaledWineDF.columns = columns\n",
    "\n",
    "# Create NDArray for ease of use\n",
    "scaledWineNDArray = scaledWineDF.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Training, Validation, and Testing Datasets From Original Dataset\n",
    "\n",
    "We will generate these datasets from our scaled data\n",
    "\n",
    "- **Training** dataset will consist of a unique 50% of our original data and will be used for *training* the model.\n",
    "- **Validation** dataset will consist of a unique 25% of our original data and will be used for *validating* our training process as we train the model.\n",
    "- **Testing** datasets will consist of the remaining unique 25% of our original data and will be used for testing the final version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Intel Extension for Scikit-learn\n",
    "from sklearnex.model_selection import train_test_split\n",
    "from modin.pandas import DataFrame as Modin_DataFrame\n",
    "\n",
    "# Generate training and temporary data splits\n",
    "training: ndarray\n",
    "temp: ndarray\n",
    "validation: ndarray\n",
    "testing: ndarray\n",
    "training, temp = train_test_split(scaledWineNDArray, test_size=0.5, train_size=0.5, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "# From the temporary data split, generate the validation and testing splits\n",
    "validation, testing = train_test_split(temp, test_size=0.5, train_size=0.5, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "# Convert ndarrays to Modin DataFrames for ease of use\n",
    "trainingDF: Modin_DataFrame = Modin_DataFrame(data=training, columns=columns)\n",
    "validationDF: Modin_DataFrame = Modin_DataFrame(data=validation, columns=columns)\n",
    "testingDF: Modin_DataFrame = Modin_DataFrame(data=testing, columns=columns)\n",
    "\n",
    "# Print out Modin DataFrames\n",
    "# print(trainingDF)\n",
    "# print(validationDF)\n",
    "# print(testingDF)\n",
    "\n",
    "# Print out size stats of training, validation, and testing DataFrames\n",
    "print(f\"Training data size w.r.t original: {trainingDF.shape[0] / rowCount}\")\n",
    "print(f\"Validation data size w.r.t original: {validationDF.shape[0] / rowCount}\")\n",
    "print(f\"Testing data size w.r.t original: {testingDF.shape[0] / rowCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset Into Model For Training\n",
    "\n",
    "Now that we have created our training, validation, and testing datasets, we know need to train our machine learning model.\n",
    "\n",
    "For our particular task, we will be classifying wines given their features.\n",
    "\n",
    "For this task, we are going to leverage Support Vector Machines (SVMs) as they are fast and efficient algorithms for classifying data.\n",
    "\n",
    "Other models that we could have used were Decision Trees, Multilayer Perceptrons, or K-Nearest Neighbors\n",
    "\n",
    "In addition to training our models, we need to figure out which model configuration is the best.\n",
    "\n",
    "To do so, we will leverage our validation dataset by generating metrics for each model we train.\n",
    "\n",
    "Thus, the best model (as measured by accuracy in our case) will be returned to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Intel Extension for Scikit-learn\n",
    "from sklearnex.svm import SVC\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from progress.bar import Bar\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def trainModels(hyper: ParameterGrid)   ->  SVC:\n",
    "    bestModel: Tuple[int, float, SVC] | None = None\n",
    "\n",
    "    # Get training features and classes\n",
    "    features: ndarray = trainingDF.drop(labels=\"class\", axis=1, inplace=False).to_numpy()\n",
    "    classes: ndarray = trainingDF[\"class\"].to_numpy()\n",
    "\n",
    "    # Get validation features and classes\n",
    "    validationFeatures: ndarray = validationDF.drop(labels=\"class\", axis=1, inplace=False).to_numpy()\n",
    "    validationClasses: ndarray = validationDF[\"class\"].to_numpy()\n",
    "\n",
    "    # For each hyperparameter combination, train a model and evaluate on the validation dataset\n",
    "    for idx in range(hyper.__len__()):\n",
    "        # Instantiate the model with hyperparameters\n",
    "        svm: SVC = SVC(**hyper[idx])\n",
    "\n",
    "        # Train the model on the training data\n",
    "        svm.fit(X=features, y=classes)\n",
    "\n",
    "        # Make predications on the validation data\n",
    "        pred: ndarray = svm.predict(X=validationFeatures)\n",
    "\n",
    "        # Compute accuracy of the model on the validation data\n",
    "        accuracy: float = accuracy_score(y_true=validationClasses, y_pred=pred)\n",
    "\n",
    "        # Store the best model, its accuracy, and hyperparameter configuration index in a tuple if it is the best model\n",
    "        if bestModel is None:\n",
    "            bestModel = (idx, accuracy, svm)\n",
    "            print(f\"Best model is config {idx} w/ {accuracy} accuracy on validation data\")\n",
    "        else:\n",
    "            if bestModel[1] < accuracy:\n",
    "                bestModel = (idx, accuracy, svm)\n",
    "                print(f\"Best model is config {idx} w/ {accuracy} accuracy on validation data\")\n",
    "\n",
    "    return bestModel\n",
    "\n",
    "# Define hyper-parameters\n",
    "hyper: ParameterGrid = ParameterGrid(param_grid={\n",
    "    \"C\": [0.01, 0.05, 0.1, 0.2, 0.4, 0.8, 1],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"degree\": [2, 3, 4, 5, 10, 15, 20],\n",
    "    \"probability\": [True],\n",
    "    \"max_iter\": [10, 20, 30, 40, 50, 100],\n",
    "})\n",
    "\n",
    "idx: int\n",
    "accuracy: float\n",
    "svm: SVC\n",
    "idx, accuracy, svm = trainModels(hyper=hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Testing Dataset Into Model For Evaluation\n",
    "\n",
    "Now that we have trained our SVM model, we now need to evaluate it against the training dataset.\n",
    "\n",
    "At this point, the machine learning model has not yet seen this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing features and classes\n",
    "features: ndarray = testingDF.drop(labels=\"class\", axis=1, inplace=False).to_numpy()\n",
    "classes: ndarray = testingDF[\"class\"].to_numpy()\n",
    "\n",
    "# Generate predictions on testing data\n",
    "pred: ndarray = svm.predict(X=features)\n",
    "\n",
    "# Compute accuracy on the generated data\n",
    "accuracy: float = accuracy_score(y_pred=pred, y_true=classes)\n",
    "\n",
    "print(f\"The best model's accuracy on the training dataset is {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
